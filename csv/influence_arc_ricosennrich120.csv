2010.amta-papers.14,W06-2810,0,0.0170237,"Missing"
2010.amta-papers.14,P91-1022,0,0.439489,"Missing"
2010.amta-papers.14,E06-1032,0,0.0217106,"Similarity Score BLEU has been developed as an automatic means to measure the translation quality of MT systems by comparing the system translation with one or more reference translations (Papineni et al., 2002). This is done by measuring the token n-gram precision of the system translation (hypothesis) for all n-gram levels up to 4, and combining the n-gram precisions using the geometric mean. The score of hypotheses shorter than the reference is reduced by a brevity penalty. BLEU has been criticised as a measure of translation quality, and it is not considered reliable on a sentence level (Callison-Burch and Osborne, 2006). On the other hand, judging the quality of a translation is a much harder task than deciding whether two sentences are possible translations of each other. We found that BLEU is very sensitive to misalignments, often yielding a score of 0 if two unrelated sentences are compared, which means that it is capable of discriminating between aligned and unaligned sentence pairs. Still, we made a number of modifications to the scoring implementation to fit it to our needs. Usually, BLEU is measured on up to 4-grams, motivated by the fact that n-gram scores of higher order are a good measure of a tran"
2010.amta-papers.14,P93-1002,0,0.212456,"approaches. 2 Related Work Overviews of sentence alignment algorithms are provided in (Manning and Sch¨utze, 1999; Singh and Husain, 2005). Most widespread methods are based on a comparison of sentence length, lexical correspondences, or a combination of the two. Length-based algorithms have first been proposed by Brown, Lai and Mercer (1991) (word count), and Gale and Church (1993) (character count). The Gale and Church algorithm is still widely used today, for instance to align Europarl (Koehn, 2005). Kay and R¨oscheisen (1993) introduce an alignment algorithm based on word correspondences. Chen (1993) constructs a word-to-word translation model during alignment, using it to estimate the probability of an alignment. Moore (2002) and Varga et al. (2005) describe a two-pass algorithm, using a length-based approach for a first alignment. The first alignment subsequently serves as training data for a translation model, which is then used in a complex similarity score. The two approaches differ in that Varga et al. (2005) use a dictionary-based translation model, with a dictionary that can be manually expanded, while Moore (2002) works with a IBM-1 translation model, following Chen (1993). Diffe"
2010.amta-papers.14,W04-3208,0,0.0114444,"two approaches differ in that Varga et al. (2005) use a dictionary-based translation model, with a dictionary that can be manually expanded, while Moore (2002) works with a IBM-1 translation model, following Chen (1993). Different degrees of textual and metatextual structure open different possibilities for sentence alignment. Tiedemann (2007) shows that movie subtitles are a highly attractive text type for sentence alignment because the texts can be aligned on the basis of time stamps. Sentence alignment is also performed on comparable corpora for which no parallel structure can be assumed (Fung and Cheung, 2004; Adafre and de Rijke, 2006; Yasuda and Sumita, 2008). Adafre and de Rijke (2006) describe an MTbased approach to find corresponding sentences in Wikipedia based on sentence similarity. Our approach is based on the same basic idea of first automatically translating one of the to-be-aligned language portions, and then measuring the similarity between this translation and the other language portion. 3 The Parallel Corpus Our sentence aligner has been developed to align the parallel part of the Text+Berg corpus, a corpus consisting of the yearbooks of the Swiss Alpine Club from 1864–19821 (Volk e"
2010.amta-papers.14,J93-1004,0,0.910396,"core. The approach requires an MT system with a reasonable performance for the language pair, but no other language-specific resources. It is thus fairly language-independent, and as we will show, more robust to textual noise than other approaches. 2 Related Work Overviews of sentence alignment algorithms are provided in (Manning and Sch¨utze, 1999; Singh and Husain, 2005). Most widespread methods are based on a comparison of sentence length, lexical correspondences, or a combination of the two. Length-based algorithms have first been proposed by Brown, Lai and Mercer (1991) (word count), and Gale and Church (1993) (character count). The Gale and Church algorithm is still widely used today, for instance to align Europarl (Koehn, 2005). Kay and R¨oscheisen (1993) introduce an alignment algorithm based on word correspondences. Chen (1993) constructs a word-to-word translation model during alignment, using it to estimate the probability of an alignment. Moore (2002) and Varga et al. (2005) describe a two-pass algorithm, using a length-based approach for a first alignment. The first alignment subsequently serves as training data for a translation model, which is then used in a complex similarity score. The"
2010.amta-papers.14,J93-1006,0,0.337859,"Missing"
2010.amta-papers.14,P07-2045,0,0.0147103,"the quality of the sentence alignment to correlate with the quality of the MT systems used, this should also give an indication as to how good translations need to be for this approach to work. A second question is how SMT performance scales with sentence alignment performance, and whether high-precision or high-recall alignment is preferable. 6.1 Method Except for the Google translations, which were obtained online through Google Translate, all systems were trained using the instructions for building a baseline system by the 2010 ACL Workshop on SMT.11 The SMT systems are built using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). The systems we trained are different from the baseline system described above in that we used various training sets, and did not train a recaser, using a BLEU implementation that ignores case differences. Table 5 describes the training data used for the different MT systems. Translation model (TM) training data is a subset of language model (LM) training data: only sentences with a valid alignment and fewer than 40 words in length are used to train the translation model. Europarl-K uses a sample of 1000 (1K) units of training data; all o"
2010.amta-papers.14,2005.mtsummit-papers.11,0,0.111691,"es. It is thus fairly language-independent, and as we will show, more robust to textual noise than other approaches. 2 Related Work Overviews of sentence alignment algorithms are provided in (Manning and Sch¨utze, 1999; Singh and Husain, 2005). Most widespread methods are based on a comparison of sentence length, lexical correspondences, or a combination of the two. Length-based algorithms have first been proposed by Brown, Lai and Mercer (1991) (word count), and Gale and Church (1993) (character count). The Gale and Church algorithm is still widely used today, for instance to align Europarl (Koehn, 2005). Kay and R¨oscheisen (1993) introduce an alignment algorithm based on word correspondences. Chen (1993) constructs a word-to-word translation model during alignment, using it to estimate the probability of an alignment. Moore (2002) and Varga et al. (2005) describe a two-pass algorithm, using a length-based approach for a first alignment. The first alignment subsequently serves as training data for a translation model, which is then used in a complex similarity score. The two approaches differ in that Varga et al. (2005) use a dictionary-based translation model, with a dictionary that can be"
2010.amta-papers.14,moore-2002-fast,0,0.758294,"ain, 2005). Most widespread methods are based on a comparison of sentence length, lexical correspondences, or a combination of the two. Length-based algorithms have first been proposed by Brown, Lai and Mercer (1991) (word count), and Gale and Church (1993) (character count). The Gale and Church algorithm is still widely used today, for instance to align Europarl (Koehn, 2005). Kay and R¨oscheisen (1993) introduce an alignment algorithm based on word correspondences. Chen (1993) constructs a word-to-word translation model during alignment, using it to estimate the probability of an alignment. Moore (2002) and Varga et al. (2005) describe a two-pass algorithm, using a length-based approach for a first alignment. The first alignment subsequently serves as training data for a translation model, which is then used in a complex similarity score. The two approaches differ in that Varga et al. (2005) use a dictionary-based translation model, with a dictionary that can be manually expanded, while Moore (2002) works with a IBM-1 translation model, following Chen (1993). Different degrees of textual and metatextual structure open different possibilities for sentence alignment. Tiedemann (2007) shows tha"
2010.amta-papers.14,J03-1002,0,0.00754503,"the quality of the MT systems used, this should also give an indication as to how good translations need to be for this approach to work. A second question is how SMT performance scales with sentence alignment performance, and whether high-precision or high-recall alignment is preferable. 6.1 Method Except for the Google translations, which were obtained online through Google Translate, all systems were trained using the instructions for building a baseline system by the 2010 ACL Workshop on SMT.11 The SMT systems are built using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). The systems we trained are different from the baseline system described above in that we used various training sets, and did not train a recaser, using a BLEU implementation that ignores case differences. Table 5 describes the training data used for the different MT systems. Translation model (TM) training data is a subset of language model (LM) training data: only sentences with a valid alignment and fewer than 40 words in length are used to train the translation model. Europarl-K uses a sample of 1000 (1K) units of training data; all other systems use all the data available for TM training"
2010.amta-papers.14,P02-1040,0,0.0994516,"t between two hard delimiters (including the beginning and end of file), and is composed of two steps. First, a set of anchor points is identified using BLEU as a similarity score between the translated source text and the target text. In a second step, the sentences between these anchor points are either aligned using BLEU-based heuristics or the length-based algorithm by Gale and Church. 4.1 Using BLEU as Similarity Score BLEU has been developed as an automatic means to measure the translation quality of MT systems by comparing the system translation with one or more reference translations (Papineni et al., 2002). This is done by measuring the token n-gram precision of the system translation (hypothesis) for all n-gram levels up to 4, and combining the n-gram precisions using the geometric mean. The score of hypotheses shorter than the reference is reduced by a brevity penalty. BLEU has been criticised as a measure of translation quality, and it is not considered reliable on a sentence level (Callison-Burch and Osborne, 2006). On the other hand, judging the quality of a translation is a much harder task than deciding whether two sentences are possible translations of each other. We found that BLEU is"
2010.amta-papers.14,W05-0816,0,0.105957,"agraph boundaries and 1to-many alignments. Since we found the performance of existing sentence alignment tools unsatisfactory, we developed an alignment algorithm based on automatic translations of the to-be-aligned texts and BLEU as a similarity score. The approach requires an MT system with a reasonable performance for the language pair, but no other language-specific resources. It is thus fairly language-independent, and as we will show, more robust to textual noise than other approaches. 2 Related Work Overviews of sentence alignment algorithms are provided in (Manning and Sch¨utze, 1999; Singh and Husain, 2005). Most widespread methods are based on a comparison of sentence length, lexical correspondences, or a combination of the two. Length-based algorithms have first been proposed by Brown, Lai and Mercer (1991) (word count), and Gale and Church (1993) (character count). The Gale and Church algorithm is still widely used today, for instance to align Europarl (Koehn, 2005). Kay and R¨oscheisen (1993) introduce an alignment algorithm based on word correspondences. Chen (1993) constructs a word-to-word translation model during alignment, using it to estimate the probability of an alignment. Moore (200"
2010.amta-papers.14,volk-etal-2010-challenges,1,0.536873,", 2004; Adafre and de Rijke, 2006; Yasuda and Sumita, 2008). Adafre and de Rijke (2006) describe an MTbased approach to find corresponding sentences in Wikipedia based on sentence similarity. Our approach is based on the same basic idea of first automatically translating one of the to-be-aligned language portions, and then measuring the similarity between this translation and the other language portion. 3 The Parallel Corpus Our sentence aligner has been developed to align the parallel part of the Text+Berg corpus, a corpus consisting of the yearbooks of the Swiss Alpine Club from 1864–19821 (Volk et al., 2010). Since 1957, the yearbooks are published in both French and German, with most articles being translated between the two languages. Currently, the parallel part of the corpus spans 138 000 sentences with 2.3/2.6 million tokens (German and French, respectively). Some examples from the 1911 yearbook illustrate the diversity. There are the typical reports on mountain expeditions: “Klettereien in der Gruppe der Engelh¨orner” (English: Climbing in the Engelh¨orner group) or “Aus den Hochregionen des Kaukasus” (English: From the high regions of the Caucasus). But the 1911 book also contains scientif"
2010.jec-1.7,2006.tc-1.9,0,0.0884004,"Missing"
2010.jec-1.7,W09-4610,1,0.847215,"prior translations (cf. the average of 57.3 in table 1) is probably due to the fact that subtitles are shorter and grammatically simpler than Europarl and Acquis sentences. 4.4 Linguistic Information in SMT for Subtitles The results reported in tables 1 and 2 are based on a purely statistical MT system. No linguistic knowledge was included. We wondered whether linguistic features such as Part-of-Speech tags or number information (singular vs. plural) could improve our system. We therefore ran a series of experiments to check this hypothesis using factored SMT for Swedish - Danish translation. Hardmeier and Volk (2009) describe these experiments in detail. Here we summarize the main findings. When we used a large training corpus of around 900,000 subtitles or 10 million tokens per language, the gains from adding linguistic information were generally small. Minor improvements were observed when using additional language models operating on part-of-speech tags and tags from morphological analysis. A technique called analytical translation, which enables the SMT system to back off to separate translation of lemmas and morphological tags (provided by Eckhard Bick’s tools) when the main phrase table does not pro"
2010.jec-1.7,2005.mtsummit-papers.11,0,0.0414572,"Missing"
2010.jec-1.7,2006.tc-1.10,0,0.787235,"Missing"
2010.jec-1.7,J04-4002,0,0.115149,"Missing"
2010.jec-1.7,2005.mtsummit-papers.19,0,0.123956,"Missing"
2010.jec-1.7,2001.mtsummit-papers.68,0,0.0877422,"Missing"
2010.jec-1.7,prokopidis-etal-2008-condensing,0,0.090432,"and (D´ıaz-Cintas and Remael, 2007). Gottlieb (2001) and Pedersen (2007) describe the peculiarities of subtitling in Scandinavia, Nagel et al. (2009) in other European countries. 3 Approaches to the Automatic Translation of Film Subtitles In this section we describe other projects on the automatic translation of subtitles.2 We assume subtitles in one language as input and aim at producing an automatic translation of these subtitles into another language. In this paper we do not deal with the conversion of the film transcript into subtitles which requires shortening the original dialogue (cf. (Prokopidis et al., 2008)). We distinguish between rulebased, example-based, and statistical approaches. 3.1 Rule-based MT of Film Subtitles Popowich et al. (2000) provide a detailed account of a MT system tailored towards the translation of English subtitles into Spanish. Their approach is based on a MT paradigm which relies heavily on lexical resources but is otherwise similar to the transfer-based approach. A unification-based parser analyzes the 2 Throughout this paper we focus on TV subtitles, but in this section we deliberately use the term “film subtitles” in a general sense covering both TV and movie subtitles"
2010.jec-1.7,2009.mtsummit-papers.16,0,0.0275594,"lation suggestions. This takes too much time. Suppressing Bad Translations An issue that has followed us throughout the project is the suppression of (presumably) bad translations. While good machine translations considerably increase the productivity of the post-editors, editing bad translations is tedious and frequently slower than translating from scratch. To take away some of this burden from the post-editors, we experimented with a Machine Learning component to predict confidence scores for the individual subtitles output by our Machine Translation systems. Closely following the work by (Specia et al., 2009), we prepared a data set of 4,000 machine-translated subtitles, manually annotated for translation quality on a 1-4 scale by the post-editors. We extracted around 70 features based on the MT input and output, their similarity and the similarity between the input and the MT training data. Then we trained a Partial Least Squares regressor to predict quality scores for unseen subtitles. Like (Specia et al., 2009), we used Inductive Confidence Machines to calibrate the acceptance threshold of our translation quality filter. We found that a confidence filter with the features proposed by Specia et"
2010.jec-1.7,2007.mtsummit-papers.66,1,0.472329,"rpus they have used for evaluating the MT system). This summary indicates that work on the automatic translation of film subtitles with Statistical MT is limited because of the lack of freely available high-quality training data. Our own efforts are based on large proprietary subtitle data and have resulted in mature MT systems. We will report on them in the following section. 4 Our MT Systems for TV Subtitles We have built Machine Translation systems for translating film subtitles from Swedish to Danish and to Norwegian in a commercial setting. Some of this work has been described earlier by Volk and Harder (2007) and Volk (2008). Most films are originally in English and receive Swedish subtitles based on the English video and audio (sometimes accompanied by an English transcript). The creation of the Swedish subtitle is a manual process done by specially trained subtitlers following company-specific guidelines. In particular, the subtitlers set the time codes (beginning and end time) for each subtitle. They use an in-house tool which allows them to link the subtitle to specific frames in the video. The Danish translator subsequently has access to the original English video and audio but also to the Sw"
2010.jec-1.7,P02-1040,0,\N,Missing
2011.eamt-1.14,W05-0909,0,0.0125582,"the other hand, use alternative decoding paths and avoid the problem of tuning by using the same set of weights for both phrase tables. For our experiments, we will use alternative decoding paths, keeping the search space under control by not adding any features, and never using more than one dynamic phrase table. In the online learning experiments, we will follow (Hardt and Elming, 2010) in using the same set of weights for both phrase tables. 4 Evaluation Data and tools used for our experiments are described in section 3.1. For the evaluation, we use BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), applying bootstrap resampling to test for statistical significance (Riezler and Maxwell, 2005). After establishing the baseline performance of our in-domain SMT system and the two external systems (i.e. Personal Translator 14 and Google Translate), we describe three experiments. The first is a re-implementation of the multiengine approach described in (Chen et al., 2009), the second one of online learning by (Hardt and Elming, 2010), and the third a combination of the two. In the first two experiments, we want to address the following research questions: Combination of Phrase Tables (Chen et"
2011.eamt-1.14,W10-1739,0,0.199011,"ly degrade as the dynamic phrase table becomes smaller. We propose to mitigate the problem by grounding the MLEbased scoring of the dynamic phrase table in the frequency counts of both the dynamic and the primary phrase table. Since our implementation can be used for both system combination and online learning, we will test the effect of scoring on both approaches. We implemented a system combination architecture similar to that described in (Chen et al., 2007). While some approaches treat all systems as black boxes, needing only the 1-best output from each system and a language model, (e.g. (Barrault, 2010; Heafield and Lavie, 2010)), the combined system described by (Chen et al., 2007) is an extension of an existing SMT system. The combination is achieved by taking a vanilla SMT system and adding a second, dynamic phrase table to the existing primary one. (Chen et al., 2007) propose that the dynamic phrase table be trained online on the translation output of several rule-based translation systems. (Chen et al., 2009) expand on this concept by allowing for the inclusion of arbitrary translation systems. We think this distinction into a primary system and several secondary ones is attractive for"
2011.eamt-1.14,D07-1103,0,0.0587736,"anslation approach, we use the rule-based Personal Translator 142 , and Google Translate3 . While Google Translate is a statistical system, it promises to be more robust to data sparseness than our in-domain system because the Google system is trained on significantly more training data.4 We build the SMT systems using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and MGIZA++ (Gao and Vogel, 2008). In terms of configuration, we adhere to the instructions for building a baseline system by the 2010 ACL Workshop on SMT.5 Additionally, we prune the primary phrase table using the approach by (Johnson et al., 2007). 3.2 Scoring Alignment We compute the word alignment of the primary corpus using the default configuration in Moses, but saving all models to disk. We then force an alignment of all dynamic corpora on the basis of these models with MGIZA++. Since we do not focus on word alignment in this paper, we only compute the alignments once for each dynamic corpus, re-using the alignment when we build phrase tables from parts of the corpus. This allows us to rule out alignment differences as the reason for variation in performance. 2 http://www.linguatec.net/products/tr/pt http://translate.google.com 4"
2011.eamt-1.14,E06-1032,0,0.214786,"Missing"
2011.eamt-1.14,P07-2045,0,0.0101472,"approach of taking an existing SMT system and extending it with a dynamic phrase table seems better suited than a black box combination of systems, in which we cannot know how wellevidenced different translation options are. As external SMT systems for the multi-engine translation approach, we use the rule-based Personal Translator 142 , and Google Translate3 . While Google Translate is a statistical system, it promises to be more robust to data sparseness than our in-domain system because the Google system is trained on significantly more training data.4 We build the SMT systems using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and MGIZA++ (Gao and Vogel, 2008). In terms of configuration, we adhere to the instructions for building a baseline system by the 2010 ACL Workshop on SMT.5 Additionally, we prune the primary phrase table using the approach by (Johnson et al., 2007). 3.2 Scoring Alignment We compute the word alignment of the primary corpus using the default configuration in Moses, but saving all models to disk. We then force an alignment of all dynamic corpora on the basis of these models with MGIZA++. Since we do not focus on word alignment in this paper, we only compute the alignment"
2011.eamt-1.14,W10-1703,0,0.0322283,"post-edited by the user, constitute the dynamic corpus. The aim of this study is to: a) evaluate both approaches in an independent reimplementation and on a different corpus; b) implement and evaluate an alternative scoring procedure that promises further performance gains; c) show the feasibility of combining multi-engine MT and online learning in a single framework. 2 Related Work System combination for Machine Translation is an active research field. The last two Workshops on Machine Translation (WMT) included a system combination task; an overview is given in (Callison-Burch et al., 2009; Callison-Burch et al., 2010). The effectiveness of system combination strongly depends on the relative performance of the systems being combined. In the 2009 WMT, (Callison-Burch et al., 2009) conclude that “In general, system combinations performed as well as the best individual systems, but not statistically significantly better than them.” A possible reason for this failure to improve on individual systems is given in the following year: “This year we excluded Google translations from the systems used in system combination. In last year’s evaluation, the large margin between Google and many of the other systems meant"
2011.eamt-1.14,J03-1002,0,0.00559769,"Missing"
2011.eamt-1.14,W07-0726,0,0.311725,"rning. 1 Introduction Two recent trends in Machine Translation are multi-engine MT, and online learning. In multiengine MT, the aim is to combine the strengths of different MT systems to perform better than any single system. Online learning is of high interest in the field of interactive MT; In order to increase translation performance and user satisfaction, it is beneficial to consider previous post-edits made by the user of the system. Both approaches can be implemented within the phrase-based SMT framework by adding a second, dynamic phrase table. This architecture was first described in (Chen et al., 2007), who built a dynamic phrase table trained on translation hypothec 2011 European Association for Machine Translation. ses by external MT systems. The online learning system described in (Hardt and Elming, 2010) uses a similar architecture, with the difference that, rather than translations by external systems, previous translations, post-edited by the user, constitute the dynamic corpus. The aim of this study is to: a) evaluate both approaches in an independent reimplementation and on a different corpus; b) implement and evaluate an alternative scoring procedure that promises further performan"
2011.eamt-1.14,P02-1040,0,0.080818,"eses. (Hardt and Elming, 2010), on the other hand, use alternative decoding paths and avoid the problem of tuning by using the same set of weights for both phrase tables. For our experiments, we will use alternative decoding paths, keeping the search space under control by not adding any features, and never using more than one dynamic phrase table. In the online learning experiments, we will follow (Hardt and Elming, 2010) in using the same set of weights for both phrase tables. 4 Evaluation Data and tools used for our experiments are described in section 3.1. For the evaluation, we use BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), applying bootstrap resampling to test for statistical significance (Riezler and Maxwell, 2005). After establishing the baseline performance of our in-domain SMT system and the two external systems (i.e. Personal Translator 14 and Google Translate), we describe three experiments. The first is a re-implementation of the multiengine approach described in (Chen et al., 2009), the second one of online learning by (Hardt and Elming, 2010), and the third a combination of the two. In the first two experiments, we want to address the following research questions:"
2011.eamt-1.14,W05-0908,0,0.129799,"e set of weights for both phrase tables. For our experiments, we will use alternative decoding paths, keeping the search space under control by not adding any features, and never using more than one dynamic phrase table. In the online learning experiments, we will follow (Hardt and Elming, 2010) in using the same set of weights for both phrase tables. 4 Evaluation Data and tools used for our experiments are described in section 3.1. For the evaluation, we use BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), applying bootstrap resampling to test for statistical significance (Riezler and Maxwell, 2005). After establishing the baseline performance of our in-domain SMT system and the two external systems (i.e. Personal Translator 14 and Google Translate), we describe three experiments. The first is a re-implementation of the multiengine approach described in (Chen et al., 2009), the second one of online learning by (Hardt and Elming, 2010), and the third a combination of the two. In the first two experiments, we want to address the following research questions: Combination of Phrase Tables (Chen et al., 2009) decided against using the primary and dynamic phrase table as alternative decodings"
2011.eamt-1.14,W09-0405,0,0.341468,"ombinations perform better than their component systems more often than last year.” (Callison-Burch et al., 2010) Mikel L. Forcada, Heidi Depraetere, Vincent Vandeghinste (eds.) Proceedings of the 15th Conference of the European Association for Machine Translation, p. 8996 Leuven, Belgium, May 2011 a combination that avoids duplicate phrase pairs, giving priority to the primary phrase table. In contrast to word alignment and phrase table combination, little attention has been paid to the issue of obtaining translation probabilities for the dynamic phrase tables. (Hardt and Elming, 2010) and (Chen et al., 2009) report that they use standard Moses procedures, i.e. Maximum Likelihood Estimation (MLE) for phrase translation probabilities, and lexical weights that are based on the word translation probabilities estimated by MLE.1 Since MLE is unreliable for low sample sizes, we expect the performance of systems that include a dynamic phrase table to seriously degrade as the dynamic phrase table becomes smaller. We propose to mitigate the problem by grounding the MLEbased scoring of the dynamic phrase table in the frequency counts of both the dynamic and the primary phrase table. Since our implementation"
2011.eamt-1.14,W08-0509,0,0.0582928,"t with a dynamic phrase table seems better suited than a black box combination of systems, in which we cannot know how wellevidenced different translation options are. As external SMT systems for the multi-engine translation approach, we use the rule-based Personal Translator 142 , and Google Translate3 . While Google Translate is a statistical system, it promises to be more robust to data sparseness than our in-domain system because the Google system is trained on significantly more training data.4 We build the SMT systems using Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and MGIZA++ (Gao and Vogel, 2008). In terms of configuration, we adhere to the instructions for building a baseline system by the 2010 ACL Workshop on SMT.5 Additionally, we prune the primary phrase table using the approach by (Johnson et al., 2007). 3.2 Scoring Alignment We compute the word alignment of the primary corpus using the default configuration in Moses, but saving all models to disk. We then force an alignment of all dynamic corpora on the basis of these models with MGIZA++. Since we do not focus on word alignment in this paper, we only compute the alignments once for each dynamic corpus, re-using the alignment whe"
2011.eamt-1.14,volk-etal-2010-challenges,0,0.0476014,"Missing"
2011.eamt-1.14,2010.amta-papers.21,0,0.0726759,"n any single system. Online learning is of high interest in the field of interactive MT; In order to increase translation performance and user satisfaction, it is beneficial to consider previous post-edits made by the user of the system. Both approaches can be implemented within the phrase-based SMT framework by adding a second, dynamic phrase table. This architecture was first described in (Chen et al., 2007), who built a dynamic phrase table trained on translation hypothec 2011 European Association for Machine Translation. ses by external MT systems. The online learning system described in (Hardt and Elming, 2010) uses a similar architecture, with the difference that, rather than translations by external systems, previous translations, post-edited by the user, constitute the dynamic corpus. The aim of this study is to: a) evaluate both approaches in an independent reimplementation and on a different corpus; b) implement and evaluate an alternative scoring procedure that promises further performance gains; c) show the feasibility of combining multi-engine MT and online learning in a single framework. 2 Related Work System combination for Machine Translation is an active research field. The last two Work"
2011.eamt-1.14,W09-0401,0,\N,Missing
2011.eamt-1.14,N03-1017,0,\N,Missing
2012.eamt-1.43,2010.amta-papers.16,0,0.365128,"ning or the test data is homogeneous. They cluster the training text into k clusters, and use unsupervised domain selection to translate each test set sentence by a cluster-specific model. Finch and Sumita (2008) distinguish between two classes of sentences: questions and declaratives (i.e. non-questions). They split the training corpus automatically according to a simple rule (does the target sentence end with ’?’), and for decoding use a linear interpolation of the classspecific and a general model, the interpolation weight depending on the class membership of each sentence. Banerjee et al. (2010) focus on a scenario in which the domains of the training texts are known, whereas the test sets are a mix of two domains. They use a sentence-level classifier to translate each sentence with a domain-specific SMT system. K m (x, x0 ) = m X X n=1 K m (x, x0 ) = u∈Σn pP m X X fx (u) f 0 (u) pP x v∈Σn fx (v) v∈Σn fx0 (v) (1) fx (u) f 0 (u) P x v∈Σn fx (v) v∈Σn fx0 (v) (2) s P n=1 u∈Σn Eck, Vogel and Waibel (2004) use information retrieval techniques to find the sentences in a parallel corpus that are closest to the translation input, then use the corresponding target sentences to build a languag"
2012.eamt-1.43,P11-2031,0,0.0254487,"ding to statistical significance tests, as described by (Johnson et al., 2007). We use two different development sets, one for domain adaptation (through perplexity optimization) and one for MERT, in order to rule out that MERT gives too much weight to the language and translation model which are optimized on the same dataset. We measure translation performance through B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003). We perform significance testing with MultEval (Clark et al., 2011), which uses approximate randomization to account for optimizer instability. Note that there are other causes of instability unaccounted 4 http://www.statmt.org/wmt11/baseline. html 189 distance k entropy mean stdev no smoothing WSK 10 0.727 LM 10 0.439 LM 100 0.344 exponential smoothing WSK 10 0.263 LM 10 0.112 LM 100 0.064 itr. (avg) 0.022 0.034 0.008 21.4 20.2 38.8 0.048 0.016 0.013 13.8 10.4 9.0 Table 2: Entropy comparison between clustering with different distance functions (with or without smoothing), and different numbers of clusters (k). Mean, standard deviation, and average number of"
2012.eamt-1.43,W11-2107,0,0.0176899,"., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. One exception is that we additionally filter the phrase table according to statistical significance tests, as described by (Johnson et al., 2007). We use two different development sets, one for domain adaptation (through perplexity optimization) and one for MERT, in order to rule out that MERT gives too much weight to the language and translation model which are optimized on the same dataset. We measure translation performance through B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003). We perform significance testing with MultEval (Clark et al., 2011), which uses approximate randomization to account for optimizer instability. Note that there are other causes of instability unaccounted 4 http://www.statmt.org/wmt11/baseline. html 189 distance k entropy mean stdev no smoothing WSK 10 0.727 LM 10 0.439 LM 100 0.344 exponential smoothing WSK 10 0.263 LM 10 0.112 LM 100 0.064 itr. (avg) 0.022 0.034 0.008 21.4 20.2 38.8 0.048 0.016 0.013 13.8 10.4 9.0 Table 2: Entropy compa"
2012.eamt-1.43,2005.mtsummit-papers.11,0,0.130916,"Missing"
2012.eamt-1.43,J03-1002,0,0.0282005,"mization for language model and translation model adaptation, which has been demonstrated to be a successful technique in SMT (Foster and Kuhn, 2007). For translation model interpolation, we use the approach described in (Sennrich, 2012), optimizing each translation model feature separately on a parallel development set. The optimization itself is convex, which means that we can easily apply it to a high number of clusters. The biggest risk is that the weight vector will be overfitted if we optimize it for a high number of small models. Finally, we set new log-linear SMT weights through MERT (Och and Ney, 2003) for each experiment. 5 Experiments The main questions that we want to answer in our experiments are: 1. How well does unsupervised clustering split a heterogeneous training text according to its domains? How are the results affected by different distance functions and smoothing? 188 Data set Alpine (in-domain) Europarl JRC Acquis OpenSubtitles v2 Total train Dev (perplexity) Dev (MERT) Test sentences 200k 1 500k 1 100k 2 300k 5 100k 1424 1000 991 words (fr) 4 400k 44 000k 24 000k 18 000k 90 400k 33 000 20 000 21 000 Table 1: Parallel data sets for German – French translation task. 2. How much"
2012.eamt-1.43,P02-1040,0,0.085581,". The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. One exception is that we additionally filter the phrase table according to statistical significance tests, as described by (Johnson et al., 2007). We use two different development sets, one for domain adaptation (through perplexity optimization) and one for MERT, in order to rule out that MERT gives too much weight to the language and translation model which are optimized on the same dataset. We measure translation performance through B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003). We perform significance testing with MultEval (Clark et al., 2011), which uses approximate randomization to account for optimizer instability. Note that there are other causes of instability unaccounted 4 http://www.statmt.org/wmt11/baseline. html 189 distance k entropy mean stdev no smoothing WSK 10 0.727 LM 10 0.439 LM 100 0.344 exponential smoothing WSK 10 0.263 LM 10 0.112 LM 100 0.064 itr. (avg) 0.022 0.034 0.008 21.4 20.2 38.8 0.048 0.016"
2012.eamt-1.43,eck-etal-2004-language,0,0.0248691,"’?’), and for decoding use a linear interpolation of the classspecific and a general model, the interpolation weight depending on the class membership of each sentence. Banerjee et al. (2010) focus on a scenario in which the domains of the training texts are known, whereas the test sets are a mix of two domains. They use a sentence-level classifier to translate each sentence with a domain-specific SMT system. K m (x, x0 ) = m X X n=1 K m (x, x0 ) = u∈Σn pP m X X fx (u) f 0 (u) pP x v∈Σn fx (v) v∈Σn fx0 (v) (1) fx (u) f 0 (u) P x v∈Σn fx (v) v∈Σn fx0 (v) (2) s P n=1 u∈Σn Eck, Vogel and Waibel (2004) use information retrieval techniques to find the sentences in a parallel corpus that are closest to the translation input, then use the corresponding target sentences to build a language model. Their approach is similar to that of Yamamoto and Sumita (2008) in that both try to adapt models in a fully unsupervised manner. The main difference is that Yamamoto and Sumita (2008) compute the clusters (and the cluster-specific models) offline, and only do cluster prediction online, whereas in (Eck et al., 2004), the whole adaptation process, i.e. selecting a subset of training data, training a mode"
2012.eamt-1.43,E12-1055,1,0.936251,"l training data to obtain a given number of clusters. Secondly, we apply domain adaptation c 2012 European Association for Machine Translation. 185 Related Work The general idea in domain adaptation is to obtain models that are specifically optimized for best performance in one domain, with a potentially negative effect on its performance for other domains. The classical domain adaptation scenario consists of a (small) in-domain corpus, a (large) out-ofdomain corpus, and in-domain development and test sets. Mixture-modeling approaches such as (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012) fall into this category. We will here give an overview of adaptation techniques that assume less prior knowledge about the training set and/or target domains. Yamamoto and Sumita (2008) operate without any predetermined domains, and without assuming that either the training or the test data is homogeneous. They cluster the training text into k clusters, and use unsupervised domain selection to translate each test set sentence by a cluster-specific model. Finch and Sumita (2008) distinguish between two classes of sentences: questions and declaratives (i.e. non-questions). They split the traini"
2012.eamt-1.43,W08-0334,0,0.0176909,"in-domain development and test sets. Mixture-modeling approaches such as (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012) fall into this category. We will here give an overview of adaptation techniques that assume less prior knowledge about the training set and/or target domains. Yamamoto and Sumita (2008) operate without any predetermined domains, and without assuming that either the training or the test data is homogeneous. They cluster the training text into k clusters, and use unsupervised domain selection to translate each test set sentence by a cluster-specific model. Finch and Sumita (2008) distinguish between two classes of sentences: questions and declaratives (i.e. non-questions). They split the training corpus automatically according to a simple rule (does the target sentence end with ’?’), and for decoding use a linear interpolation of the classspecific and a general model, the interpolation weight depending on the class membership of each sentence. Banerjee et al. (2010) focus on a scenario in which the domains of the training texts are known, whereas the test sets are a mix of two domains. They use a sentence-level classifier to translate each sentence with a domain-speci"
2012.eamt-1.43,steinberger-etal-2006-jrc,0,0.0979809,"Missing"
2012.eamt-1.43,W07-0717,0,0.460698,"ustering on the parallel training data to obtain a given number of clusters. Secondly, we apply domain adaptation c 2012 European Association for Machine Translation. 185 Related Work The general idea in domain adaptation is to obtain models that are specifically optimized for best performance in one domain, with a potentially negative effect on its performance for other domains. The classical domain adaptation scenario consists of a (small) in-domain corpus, a (large) out-ofdomain corpus, and in-domain development and test sets. Mixture-modeling approaches such as (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012) fall into this category. We will here give an overview of adaptation techniques that assume less prior knowledge about the training set and/or target domains. Yamamoto and Sumita (2008) operate without any predetermined domains, and without assuming that either the training or the test data is homogeneous. They cluster the training text into k clusters, and use unsupervised domain selection to translate each test set sentence by a cluster-specific model. Finch and Sumita (2008) distinguish between two classes of sentences: questions and declaratives (i.e. non-questions). They"
2012.eamt-1.43,D07-1103,0,0.0194938,"entences from one corpus, its entropy is 0. The baseline is a uniform distribution, which corresponds to an entropy of 1.698 (with the data sets from table 1). The second evaluation is a translation task. In terms of tools and techniques used, we mostly adhere to the work flow described for the WMT 2011 baseline system4 . The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. One exception is that we additionally filter the phrase table according to statistical significance tests, as described by (Johnson et al., 2007). We use two different development sets, one for domain adaptation (through perplexity optimization) and one for MERT, in order to rule out that MERT gives too much weight to the language and translation model which are optimized on the same dataset. We measure translation performance through B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003). We perform significance testing with MultEval (Clark et al., 2011), which uses approximate randomization to account for optim"
2012.eamt-1.43,W07-0733,0,0.199989,"we perform unsupervised clustering on the parallel training data to obtain a given number of clusters. Secondly, we apply domain adaptation c 2012 European Association for Machine Translation. 185 Related Work The general idea in domain adaptation is to obtain models that are specifically optimized for best performance in one domain, with a potentially negative effect on its performance for other domains. The classical domain adaptation scenario consists of a (small) in-domain corpus, a (large) out-ofdomain corpus, and in-domain development and test sets. Mixture-modeling approaches such as (Koehn and Schroeder, 2007; Foster and Kuhn, 2007; Sennrich, 2012) fall into this category. We will here give an overview of adaptation techniques that assume less prior knowledge about the training set and/or target domains. Yamamoto and Sumita (2008) operate without any predetermined domains, and without assuming that either the training or the test data is homogeneous. They cluster the training text into k clusters, and use unsupervised domain selection to translate each test set sentence by a cluster-specific model. Finch and Sumita (2008) distinguish between two classes of sentences: questions and declaratives (i."
2012.eamt-1.43,P07-2045,0,0.00984258,"which sentence i originally belonged. pc (orig(i)) is the probability that a sentence in cluster c is originally from corpus orig(i), estimated through relative frequency. k X X 1 H(X) = − log2 pc (orig(i)) N (5) c=0 i∈c If a cluster only contains sentences from one corpus, its entropy is 0. The baseline is a uniform distribution, which corresponds to an entropy of 1.698 (with the data sets from table 1). The second evaluation is a translation task. In terms of tools and techniques used, we mostly adhere to the work flow described for the WMT 2011 baseline system4 . The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. One exception is that we additionally filter the phrase table according to statistical significance tests, as described by (Johnson et al., 2007). We use two different development sets, one for domain adaptation (through perplexity optimization) and one for MERT, in order to rule out that MERT gives too much weight to the language and translation model which are optimized on the same dataset. We measure translation performance through B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowsk"
2012.eamt-1.43,volk-etal-2010-challenges,0,0.035786,"Missing"
2012.eamt-1.43,D07-1054,0,\N,Missing
2013.mtsummit-posters.2,W11-2103,0,0.016486,"xtracting the set of occurrences for each terminal sequence, and using the intersection of these sets as occurrences of the full rule. For a rule X → h a b X1 c , x X1 y z i, cs is thus the number of source sentences in which a b and c occur, ct the number of target sentences in which x and y z occur, and cst the number of sentence pairs in which a b and c occur in the source sentence, x and y z in the target sentence. 6 Evaluation 6.1 Data and Methods We perform the evaluation on the language pairs French–English and German–English, with training data mostly from the shared task of WMT 2011 (Callison-Burch et al., 2011). For both language pairs, we use Europarl and News-Commentary as parallel data sets. Language models are trained on the respective target language sides of Europarl, News-Commentary, and the monolingual News 210 system newstest2011 baseline FLEX cross-domain baseline FLEX DE–EN B LEU METEOR EN–DE B LEU METEOR FR–EN B LEU METEOR EN–FR B LEU METEOR 21.0 21.2 28.6 28.7 15.6 15.8 36.2 36.4 28.6 28.8 33.8 33.8 30.3 30.3 51.2 51.1 29.1 29.4 28.9 29.1 27.0 27.1 42.0 42.2 25.5 26.4 32.2 32.6 22.2 22.6 44.1 44.5 Table 3: SMT results on newstest2011 and cross-domain test sets. Phrase-based models. Data"
2013.mtsummit-posters.2,N10-1029,0,0.0202103,"’s flexibility, i.e. whether it occurs in many contexts or is restricted to fixed expressions. Note that the aim is not to penalize MWEs themselves, which may be flexible in terms of their contexts, but only phrases that are part of a larger MWE. In contrast to other related work on MWEs in SMT, our approach is unsupervised and language independent. 2 Related Work The fact that word-based translation techniques are inadequate to deal with MWEs, which are by definition non-compositional, has led to approaches that extract MWEs in order to improve bilingual resources (e.g. (Smadja et al., 1996; Carpuat and Diab, 2010)). Using contextual information to disambiguate translations is an equally wellresearched topic (Carpuat and Wu, 2007; Chiang et al., 2009). One can even argue that the success of phrase-based SMT (Koehn et al., 2003) compared Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 207–214. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. to word-based approaches is in large part due to the existence of MWEs in natural"
2013.mtsummit-posters.2,D07-1007,0,0.0238354,"to penalize MWEs themselves, which may be flexible in terms of their contexts, but only phrases that are part of a larger MWE. In contrast to other related work on MWEs in SMT, our approach is unsupervised and language independent. 2 Related Work The fact that word-based translation techniques are inadequate to deal with MWEs, which are by definition non-compositional, has led to approaches that extract MWEs in order to improve bilingual resources (e.g. (Smadja et al., 1996; Carpuat and Diab, 2010)). Using contextual information to disambiguate translations is an equally wellresearched topic (Carpuat and Wu, 2007; Chiang et al., 2009). One can even argue that the success of phrase-based SMT (Koehn et al., 2003) compared Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 207–214. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. to word-based approaches is in large part due to the existence of MWEs in natural language. Our work is not concerned with improving the translation of MWEs themselves, but with preventing an overgen"
2013.mtsummit-posters.2,N09-1025,0,0.0642968,"Missing"
2013.mtsummit-posters.2,P05-1033,0,0.0918154,"same aim as the joining of MWEs that Lambert et al. (2006) describe, namely to prevent overgeneralisation of phrase pairs learned from MWEs to new contexts. It has the advantage of being languageindependent and requiring no additional resources. Additionally, it does not need to make a hard classification into MWEs and others, and thus does not 2 The examples are from the models described in section 6.1. 209 suffer from an increase in data sparseness. 4.1 Variants for Hierarchical Phrase-based Models We can extend the notion of a phrase pair’s flexibility to hierarchical phrase-based models (Chiang, 2005). However, we argue that a naive transfer of the approach to hierarchical phrase-based systems is incomplete. Because subphrases are allowed in hierarchical rules, there are additional ways in which a rule can be inflexible. Consider these three rules that might be learned from occurrences of the phrase pair (of course, bien sûr). 1. X → h course , sûr i 2. X → h X1 course , X1 sûr i 3. X → h and X1 course , et X1 sûr i Each of these examples is a poor generalisation, and should ideally be penalized in the model. In all cases, we only expect the translation of course into sûr if course is prec"
2013.mtsummit-posters.2,P11-2031,0,0.0241422,"onal German– English data, we used JRC-Acquis, a collection of legislative texts (Steinberger et al., 2006), and OpenSubtitles v2, a parallel corpus extracted from film subtitles5 (Tiedemann, 2009). The respective data sizes are listed in table 2. We train all systems with Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). We measure translation performance through B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and using MultEval (Clark et al., 2011) to account for optimizer instability. Results marked in bold are statistically significantly better than the baseline according to significance testing in MultEval (p < 0.05). We evaluate each system with two test sets. The first test set is newstest2011 from WMT 2011, with the system optimized on news-test2008; as second test set, we use patent abstracts for FR–EN6 , and help desk tickets provided to us by the software company Finnova for DE–EN. The reason for this is that we expect idiomaticity to be more of a problem if training and test set are dissimilar, since MWEs may be domain-specifi"
2013.mtsummit-posters.2,W11-2107,0,0.0223406,"for minimal perplexity on newstest2008. For French–English, we additionally used the 109 corpus and United Nations corpus as parallel data sets. As additional German– English data, we used JRC-Acquis, a collection of legislative texts (Steinberger et al., 2006), and OpenSubtitles v2, a parallel corpus extracted from film subtitles5 (Tiedemann, 2009). The respective data sizes are listed in table 2. We train all systems with Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). We measure translation performance through B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and using MultEval (Clark et al., 2011) to account for optimizer instability. Results marked in bold are statistically significantly better than the baseline according to significance testing in MultEval (p < 0.05). We evaluate each system with two test sets. The first test set is newstest2011 from WMT 2011, with the system optimized on news-test2008; as second test set, we use patent abstracts for FR–EN6 , and help desk tickets provided to us by the software company Finnova for DE–EN. T"
2013.mtsummit-posters.2,D07-1103,0,0.0169559,"t sx be defined as the full subphrase, or, if a rule contains multiple subphrases, the concatenation of all subphrases.4 If a rule does not have a subphrase, we let pflex_sub (t|s) be 1, so that this feature is without cost for rules without subphrases. We also add pflex_sub (s|t), which is defined analogously. 5 Filtering Hierarchical Rule Tables In preliminary experiments, we found that some of the differences between the baseline system and the experimental ones were due to spurious phrase or rule pairs whose probability estimates were unduly high. Thus, we use significance test filtering (Johnson et al., 2007) for phrase tables, which, as the authors note, has a similar effect as smoothing, since both pruning and smoothing penalizes infrequent phrase pairs. We extend their approach to hierarchical rule tables. Since (Johnson et al., 2007) do not base the significance test on alignment counts, but co-occurrence counts in the parallel corpus, we decided on an approximative 4 We insert a delimiter between two subphrases to distinguish between X1 = ’a b’, X2 = ’c’ and X1 = ’a’, X2 = ’b c’. method to count the number of occurrences of hierarchical rules, which can be implemented with a suffix array. Thr"
2013.mtsummit-posters.2,N03-1017,0,0.106541,"are part of a larger MWE. In contrast to other related work on MWEs in SMT, our approach is unsupervised and language independent. 2 Related Work The fact that word-based translation techniques are inadequate to deal with MWEs, which are by definition non-compositional, has led to approaches that extract MWEs in order to improve bilingual resources (e.g. (Smadja et al., 1996; Carpuat and Diab, 2010)). Using contextual information to disambiguate translations is an equally wellresearched topic (Carpuat and Wu, 2007; Chiang et al., 2009). One can even argue that the success of phrase-based SMT (Koehn et al., 2003) compared Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 207–214. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. to word-based approaches is in large part due to the existence of MWEs in natural language. Our work is not concerned with improving the translation of MWEs themselves, but with preventing an overgeneralisation of translations learned from MWEs. In other words, our aim is not to improve the transla"
2013.mtsummit-posters.2,P07-2045,0,0.00973535,"0k 1200k 4650k 110 000k 25 000k 52 000k 3300k 48 000k 25 800k 35 400k 2 650 000k 610 000k 920 000k Table 2: Training data used in evaluation. data set, interpolated for minimal perplexity on newstest2008. For French–English, we additionally used the 109 corpus and United Nations corpus as parallel data sets. As additional German– English data, we used JRC-Acquis, a collection of legislative texts (Steinberger et al., 2006), and OpenSubtitles v2, a parallel corpus extracted from film subtitles5 (Tiedemann, 2009). The respective data sizes are listed in table 2. We train all systems with Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). We measure translation performance through B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and using MultEval (Clark et al., 2011) to account for optimizer instability. Results marked in bold are statistically significantly better than the baseline according to significance testing in MultEval (p < 0.05). We evaluate each system with two test sets. The first test set is newstest2011 from WMT 2011, with the s"
2013.mtsummit-posters.2,W06-2402,0,0.0378073,"ce, September 2–6, 2013), p. 207–214. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. to word-based approaches is in large part due to the existence of MWEs in natural language. Our work is not concerned with improving the translation of MWEs themselves, but with preventing an overgeneralisation of translations learned from MWEs. In other words, our aim is not to improve the translation of words in known contexts, but picking a better translation if a word occurs in a new context. It shares the aim with the work by Lambert and Banchs (2006), who convert MWEs into single tokens in a preprocessing step, thus preventing MWE sub-segments from being extracted. In order to identify MWEs in the source text, they exploit asymmetries in word alignment, lemmatisation and PoS-tagging. They found that the positive effect of suppressing wrong phrase pairs in some instances was counterbalanced by increased data sparseness, especially because some word sequences were erroneously identified as MWEs. Pal et al. (2011) follow the same idea for the language pair English–Bengali, with different MWE extraction techniques. 3 Learning Translations in"
2013.mtsummit-posters.2,J03-1002,0,0.00507398,"800k 35 400k 2 650 000k 610 000k 920 000k Table 2: Training data used in evaluation. data set, interpolated for minimal perplexity on newstest2008. For French–English, we additionally used the 109 corpus and United Nations corpus as parallel data sets. As additional German– English data, we used JRC-Acquis, a collection of legislative texts (Steinberger et al., 2006), and OpenSubtitles v2, a parallel corpus extracted from film subtitles5 (Tiedemann, 2009). The respective data sizes are listed in table 2. We train all systems with Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). We measure translation performance through B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and using MultEval (Clark et al., 2011) to account for optimizer instability. Results marked in bold are statistically significantly better than the baseline according to significance testing in MultEval (p < 0.05). We evaluate each system with two test sets. The first test set is newstest2011 from WMT 2011, with the system optimized on news-test2008; as second test set, w"
2013.mtsummit-posters.2,2011.mtsummit-papers.23,0,0.0304111,"in known contexts, but picking a better translation if a word occurs in a new context. It shares the aim with the work by Lambert and Banchs (2006), who convert MWEs into single tokens in a preprocessing step, thus preventing MWE sub-segments from being extracted. In order to identify MWEs in the source text, they exploit asymmetries in word alignment, lemmatisation and PoS-tagging. They found that the positive effect of suppressing wrong phrase pairs in some instances was counterbalanced by increased data sparseness, especially because some word sequences were erroneously identified as MWEs. Pal et al. (2011) follow the same idea for the language pair English–Bengali, with different MWE extraction techniques. 3 Learning Translations in SMT To illustrate why wrong translations are learned from MWEs, let us consider the common SMT training process. In (hierarchical) phrase-based SMT, translations are extracted from a wordaligned corpus. This extraction is performed by heuristics that extract phrase pairs which are consistent with word alignment, specifically, so that no word in the source phrase is aligned to a word outside the target phrase, and vice versa. For MWEs, this means that phrase pair ext"
2013.mtsummit-posters.2,P02-1040,0,0.0918197,"in evaluation. data set, interpolated for minimal perplexity on newstest2008. For French–English, we additionally used the 109 corpus and United Nations corpus as parallel data sets. As additional German– English data, we used JRC-Acquis, a collection of legislative texts (Steinberger et al., 2006), and OpenSubtitles v2, a parallel corpus extracted from film subtitles5 (Tiedemann, 2009). The respective data sizes are listed in table 2. We train all systems with Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). We measure translation performance through B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and using MultEval (Clark et al., 2011) to account for optimizer instability. Results marked in bold are statistically significantly better than the baseline according to significance testing in MultEval (p < 0.05). We evaluate each system with two test sets. The first test set is newstest2011 from WMT 2011, with the system optimized on news-test2008; as second test set, we use patent abstracts for FR–EN6 , and help desk tickets provided to us b"
2013.mtsummit-posters.2,2011.mtsummit-plenaries.5,0,0.0399213,"Missing"
2013.mtsummit-posters.2,J96-1001,0,0.77965,"measure a phrase pair’s flexibility, i.e. whether it occurs in many contexts or is restricted to fixed expressions. Note that the aim is not to penalize MWEs themselves, which may be flexible in terms of their contexts, but only phrases that are part of a larger MWE. In contrast to other related work on MWEs in SMT, our approach is unsupervised and language independent. 2 Related Work The fact that word-based translation techniques are inadequate to deal with MWEs, which are by definition non-compositional, has led to approaches that extract MWEs in order to improve bilingual resources (e.g. (Smadja et al., 1996; Carpuat and Diab, 2010)). Using contextual information to disambiguate translations is an equally wellresearched topic (Carpuat and Wu, 2007; Chiang et al., 2009). One can even argue that the success of phrase-based SMT (Koehn et al., 2003) compared Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 207–214. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. to word-based approaches is in large part due to the exis"
2013.mtsummit-posters.2,steinberger-etal-2006-jrc,0,0.0228998,"ed Nations 109 corpus EN–DE News-commentary Europarl JRC-Acquis OpenSubtitles v2 News (EN) News (FR) News (DE) sentences words (EN) 110k 1830k 11 800k 21 400k 2900k 50 600k 300 000k 551 000k 140k 1740k 1200k 4650k 110 000k 25 000k 52 000k 3300k 48 000k 25 800k 35 400k 2 650 000k 610 000k 920 000k Table 2: Training data used in evaluation. data set, interpolated for minimal perplexity on newstest2008. For French–English, we additionally used the 109 corpus and United Nations corpus as parallel data sets. As additional German– English data, we used JRC-Acquis, a collection of legislative texts (Steinberger et al., 2006), and OpenSubtitles v2, a parallel corpus extracted from film subtitles5 (Tiedemann, 2009). The respective data sizes are listed in table 2. We train all systems with Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). We measure translation performance through B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and using MultEval (Clark et al., 2011) to account for optimizer instability. Results marked in bold are statistically signific"
2014.eamt-1.37,P11-2031,0,0.0492909,"Missing"
2014.eamt-1.37,W10-1734,0,0.013883,"ting The German language has a productive compounding system, which increases vocabulary size and exacerbates the data sparsity effect. Many compounds are domain-specific and are unlikely to be learned from larger general-domain corpora. Compound splitting, however, has the potential to also work on our in-domain texts. We evaluate two methods of compound splitting. Koehn and Knight (2003) describe a purely data-driven approach, in which frequency statistics are collected from the unsplit corpus, and words are split so that the geometric mean of the word frequencies of its parts is maximized. Fritzinger and Fraser (2010) describe a hybrid approach, which uses the same corpus-driven selection method to choose the best split of a word among multiple candidates, but instead of considering all character sequences to be potential parts, they only consider those splits that are validated by a finite-state morphology tool. The motivation for using the finite-state morphology is to prevent linguistically implausible splittings such as Testsets → Test ETS. We use the Zmorge morphology (Sennrich and Kunz, 2014), which combines the SMOR grammar (Schmid et al., 2004) with a lexicon extracted from Wiktionary.2 With this h"
2014.eamt-1.37,E03-1076,0,0.292035,"xample: Ger: siehe auch ecl kd042 de crm basis MP-MAR-11, kapitel 9.2.1.1 Eng: see also ecl kd042 de crm basis MP-MAR-11, chapter 9.2.1.1 c 2014 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. 159 While these technical tokens need no translation, our baseline system also suffers from a large number of out-of-vocabulary tokens (OOVs) that should be translated. The concatenative morphology of German compounds is a classical problem for machine translation, as it leads to an increased vocabulary and exacerbates data sparsity (Koehn and Knight, 2003). In our case the problem is inflated due to the domain-specific compound terms like Tabellenattribute (table attribute) or Nachbuchungen (subsequent postings): many of these are not seen in the smaller in-domain parallel corpus and they are too specific to be present in general-domain corpora. Technical tokens like URLs and alphanumeric IDs do not require translation and should be transferred into the output verbatim. However, since they are also unknown to the translation system, they still present a number of problems. They are often broken by tokenization and not restored properly by subse"
2014.eamt-1.37,P07-2045,0,0.00544158,"Missing"
2014.eamt-1.37,2005.mtsummit-papers.11,0,0.08957,"Missing"
2014.eamt-1.37,schmid-etal-2004-smor,0,0.0197712,"he word frequencies of its parts is maximized. Fritzinger and Fraser (2010) describe a hybrid approach, which uses the same corpus-driven selection method to choose the best split of a word among multiple candidates, but instead of considering all character sequences to be potential parts, they only consider those splits that are validated by a finite-state morphology tool. The motivation for using the finite-state morphology is to prevent linguistically implausible splittings such as Testsets → Test ETS. We use the Zmorge morphology (Sennrich and Kunz, 2014), which combines the SMOR grammar (Schmid et al., 2004) with a lexicon extracted from Wiktionary.2 With this hybrid approach, we only consider nouns for compound splitting; with the datadriven approach on the other hand we have no control over which word classes are split. 2 http://www.wiktionary.org 161 Source: Reference: Masking: Reduction: erweiterung tabellen TX VL und TXTSVL . extension of tables TX VL and TXTSVL . extension of tables TX VL TXTSVL and . extension of tables TX VL and TXTSVL . Table 2: An example of the effect of reducing: the correct order of technical tokens is preserved. 5 Experiments and Results We evaluated our experiments"
2014.eamt-1.37,sennrich-kunz-2014-zmorge,1,0.82568,"corpus, and words are split so that the geometric mean of the word frequencies of its parts is maximized. Fritzinger and Fraser (2010) describe a hybrid approach, which uses the same corpus-driven selection method to choose the best split of a word among multiple candidates, but instead of considering all character sequences to be potential parts, they only consider those splits that are validated by a finite-state morphology tool. The motivation for using the finite-state morphology is to prevent linguistically implausible splittings such as Testsets → Test ETS. We use the Zmorge morphology (Sennrich and Kunz, 2014), which combines the SMOR grammar (Schmid et al., 2004) with a lexicon extracted from Wiktionary.2 With this hybrid approach, we only consider nouns for compound splitting; with the datadriven approach on the other hand we have no control over which word classes are split. 2 http://www.wiktionary.org 161 Source: Reference: Masking: Reduction: erweiterung tabellen TX VL und TXTSVL . extension of tables TX VL and TXTSVL . extension of tables TX VL TXTSVL and . extension of tables TX VL and TXTSVL . Table 2: An example of the effect of reducing: the correct order of technical tokens is preserved."
2014.eamt-1.37,E12-1055,1,0.900155,"Missing"
2014.eamt-1.37,tiedemann-2012-parallel,0,0.0568003,"Missing"
2014.eamt-1.37,steinberger-etal-2006-jrc,0,\N,Missing
2014.eamt-1.37,W13-2201,0,\N,Missing
2020.acl-main.148,N19-1121,0,0.0492882,"Missing"
2020.acl-main.148,C18-1263,0,0.0463148,"al. (2017) exploited character-level inputs and adopted a shared encoder for many-to-one translation. Ha et al. (2016) and Johnson et al. (2017) further successfully trained a single NMT model for multilingual translation with a target language symbol guiding the translation direction. This approach serves as our baseline. Still, this paradigm forces different languages into one joint representation space, neglecting their linguistic diversity. Several subsequent studies have explored different strategies to mitigate this representation bottleneck, ranging from reorganizing parameter sharing (Blackwood et al., 2018; Sachan and Neubig, 2018; Lu et al., 2018; Wang et al., 2019c; Vázquez et al., 2019), designing language-specific parameter generators (Platanios et al., 2018), decoupling multilingual word encodings (Wang et al., 2019b) to language clustering (Tan et al., 2019). Our languagespecific modeling continues in this direction, but with a special focus on broadening normalization layers and encoder outputs. Multilingual NMT allows us to perform zeroshot translation, although the quality is not guaranteed (Firat et al., 2016b; Johnson et al., 2017). We observe that multilingual NMT often translates i"
2020.acl-main.148,P15-1166,0,0.154129,"Missing"
2020.acl-main.148,W18-6319,0,0.0621887,"ces already in OPUS-100. 6 6.1 Experiments Setup We perform one-to-many (English-X) and manyto-many (English-X ∪ X-English) translation on OPUS-100 (|T |is 100). We apply byte pair encoding (BPE) (Sennrich et al., 2016b; Kudo and Richardson, 2018) to handle multilingual words with a joint vocabulary size of 64k. We randomly 7 For efficiency, we only use 200 sentences per language pair for validation in our multilingual experiments. shuffle the training set to mix instances of different language pairs. We adopt BLEU (Papineni et al., 2002) for translation evaluation with the toolkit SacreBLEU (Post, 2018)8 . We employ the langdetect library9 to detect the language of translations, and measure the translation-language accuracy for zero-shot cases. Rather than providing numbers for each language pair, we report average BLEU over all 94 language pairs with test sets (BLEU94 ). We also show the win ratio (WR), counting the proportion where our approach outperforms its baseline. Apart from multilingual NMT, our baselines also involve bilingual NMT and pivot-based translation (only for zero-shot comparison). We select four typologically different target languages (German/De, Chinese/Zh, Breton/Br, T"
2020.acl-main.148,W18-6327,0,0.0797739,"racter-level inputs and adopted a shared encoder for many-to-one translation. Ha et al. (2016) and Johnson et al. (2017) further successfully trained a single NMT model for multilingual translation with a target language symbol guiding the translation direction. This approach serves as our baseline. Still, this paradigm forces different languages into one joint representation space, neglecting their linguistic diversity. Several subsequent studies have explored different strategies to mitigate this representation bottleneck, ranging from reorganizing parameter sharing (Blackwood et al., 2018; Sachan and Neubig, 2018; Lu et al., 2018; Wang et al., 2019c; Vázquez et al., 2019), designing language-specific parameter generators (Platanios et al., 2018), decoupling multilingual word encodings (Wang et al., 2019b) to language clustering (Tan et al., 2019). Our languagespecific modeling continues in this direction, but with a special focus on broadening normalization layers and encoder outputs. Multilingual NMT allows us to perform zeroshot translation, although the quality is not guaranteed (Firat et al., 2016b; Johnson et al., 2017). We observe that multilingual NMT often translates into the wrong target lang"
2020.acl-main.148,P16-1009,1,0.831948,"erformance, particularly compared to pivot-based models. Without access to parallel training data for zero-shot language pairs, multilingual models easily fall into the trap of offtarget translation where a model ignores the given target information and translates into a wrong language as shown in Table 1. To avoid such a trap, we propose the random online backtranslation (ROB T) algorithm. ROB T finetunes a pretrained multilingual NMT model for unseen training language pairs with pseudo parallel batches generated by back-translating the target-side training data.2 We perform backtranslation (Sennrich et al., 2016a) into randomly picked intermediate languages to ensure good coverage of ∼10,000 zero-shot directions. Although backtranslation has been successfully applied to zero-shot translation (Firat et al., 2016b; Gu et al., 2019; Lakew et al., 2019), whether it works in the massively multilingual set-up remained an open question and we investigate it in our work. For experiments, we collect OPUS-100, a massively multilingual dataset sampled from OPUS (Tiedemann, 2012). OPUS-100 consists of 55M English-centric sentence pairs covering 100 languages. As far as we know, no similar dataset is publicly ava"
2020.acl-main.148,P16-1162,1,0.656287,"erformance, particularly compared to pivot-based models. Without access to parallel training data for zero-shot language pairs, multilingual models easily fall into the trap of offtarget translation where a model ignores the given target information and translates into a wrong language as shown in Table 1. To avoid such a trap, we propose the random online backtranslation (ROB T) algorithm. ROB T finetunes a pretrained multilingual NMT model for unseen training language pairs with pseudo parallel batches generated by back-translating the target-side training data.2 We perform backtranslation (Sennrich et al., 2016a) into randomly picked intermediate languages to ensure good coverage of ∼10,000 zero-shot directions. Although backtranslation has been successfully applied to zero-shot translation (Firat et al., 2016b; Gu et al., 2019; Lakew et al., 2019), whether it works in the massively multilingual set-up remained an open question and we investigate it in our work. For experiments, we collect OPUS-100, a massively multilingual dataset sampled from OPUS (Tiedemann, 2012). OPUS-100 consists of 55M English-centric sentence pairs covering 100 languages. As far as we know, no similar dataset is publicly ava"
2020.acl-main.148,D19-1089,0,0.0451194,"Missing"
2020.acl-main.148,tiedemann-2012-parallel,0,0.463207,"uage pairs with pseudo parallel batches generated by back-translating the target-side training data.2 We perform backtranslation (Sennrich et al., 2016a) into randomly picked intermediate languages to ensure good coverage of ∼10,000 zero-shot directions. Although backtranslation has been successfully applied to zero-shot translation (Firat et al., 2016b; Gu et al., 2019; Lakew et al., 2019), whether it works in the massively multilingual set-up remained an open question and we investigate it in our work. For experiments, we collect OPUS-100, a massively multilingual dataset sampled from OPUS (Tiedemann, 2012). OPUS-100 consists of 55M English-centric sentence pairs covering 100 languages. As far as we know, no similar dataset is publicly available.3 We have released OPUS100 to facilitate future research.4 We adopt the Transformer model (Vaswani et al., 2017) and evaluate our approach under one-to-many and manyto-many translation settings. Our main findings are summarized as follows: • Increasing the capacity of multilingual NMT yields large improvements and narrows the performance gap with bilingual models. Lowresource translation benefits more from the increased capacity. • Language-specific mode"
2020.acl-main.148,W19-4305,0,0.0366729,"ne translation. Ha et al. (2016) and Johnson et al. (2017) further successfully trained a single NMT model for multilingual translation with a target language symbol guiding the translation direction. This approach serves as our baseline. Still, this paradigm forces different languages into one joint representation space, neglecting their linguistic diversity. Several subsequent studies have explored different strategies to mitigate this representation bottleneck, ranging from reorganizing parameter sharing (Blackwood et al., 2018; Sachan and Neubig, 2018; Lu et al., 2018; Wang et al., 2019c; Vázquez et al., 2019), designing language-specific parameter generators (Platanios et al., 2018), decoupling multilingual word encodings (Wang et al., 2019b) to language clustering (Tan et al., 2019). Our languagespecific modeling continues in this direction, but with a special focus on broadening normalization layers and encoder outputs. Multilingual NMT allows us to perform zeroshot translation, although the quality is not guaranteed (Firat et al., 2016b; Johnson et al., 2017). We observe that multilingual NMT often translates into the wrong target language on zero-shot directions (Table 1), resonating with the"
2020.acl-main.148,P19-1176,0,0.333792,"s and seek solutions that are capable of overcoming this capacity bottleneck. We propose language-aware layer normalization and linear transformation to relax the representation constraint in multilingual NMT models. The linear transformation is inserted in-between the encoder and the decoder so as to facilitate the induction of language-specific translation correspon1628 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628–1639 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dences. We also investigate deep NMT architectures (Wang et al., 2019a; Zhang et al., 2019) aiming at further reducing the performance gap with bilingual methods. Another pitfall of massively multilingual NMT is its poor zero-shot performance, particularly compared to pivot-based models. Without access to parallel training data for zero-shot language pairs, multilingual models easily fall into the trap of offtarget translation where a model ignores the given target information and translates into a wrong language as shown in Table 1. To avoid such a trap, we propose the random online backtranslation (ROB T) algorithm. ROB T finetunes a pretrained multilingual N"
2020.acl-main.148,P19-1117,0,0.469076,"s and seek solutions that are capable of overcoming this capacity bottleneck. We propose language-aware layer normalization and linear transformation to relax the representation constraint in multilingual NMT models. The linear transformation is inserted in-between the encoder and the decoder so as to facilitate the induction of language-specific translation correspon1628 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628–1639 c July 5 - 10, 2020. 2020 Association for Computational Linguistics dences. We also investigate deep NMT architectures (Wang et al., 2019a; Zhang et al., 2019) aiming at further reducing the performance gap with bilingual methods. Another pitfall of massively multilingual NMT is its poor zero-shot performance, particularly compared to pivot-based models. Without access to parallel training data for zero-shot language pairs, multilingual models easily fall into the trap of offtarget translation where a model ignores the given target information and translates into a wrong language as shown in Table 1. To avoid such a trap, we propose the random online backtranslation (ROB T) algorithm. ROB T finetunes a pretrained multilingual N"
2020.acl-main.148,D19-1083,1,0.91379,"Missing"
2020.acl-main.148,D16-1050,1,0.707318,"lation for the first time. We release OPUS-100, a multilingual dataset from OPUS including 100 languages with around 55M sentence pairs for future study. Our experiments on this dataset show that the proposed approaches substantially increase translation performance, narrowing the performance gap with bilingual NMT models and pivot-based methods. In the future, we will develop lightweight alternatives to L A LT to reduce the number of model parameters. We will also exploit novel strategies to break the upper bound of ROB T and obtain larger zero-shot improvements, such as generative modeling (Zhang et al., 2016; Su et al., 2018; García et al., 2020; Zheng et al., 2020). Acknowledgments This project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreements 825460 (ELITR) and 825299 (GoURMET). This project has received support from Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland. Rico Sennrich acknowledges support of the Swiss National Science Foundation (MUTAMUR; no. 176727). References Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Confer"
2020.acl-main.148,2020.wmt-1.63,0,0.014697,"gual dataset from OPUS including 100 languages with around 55M sentence pairs for future study. Our experiments on this dataset show that the proposed approaches substantially increase translation performance, narrowing the performance gap with bilingual NMT models and pivot-based methods. In the future, we will develop lightweight alternatives to L A LT to reduce the number of model parameters. We will also exploit novel strategies to break the upper bound of ROB T and obtain larger zero-shot improvements, such as generative modeling (Zhang et al., 2016; Su et al., 2018; García et al., 2020; Zheng et al., 2020). Acknowledgments This project has received funding from the European Union’s Horizon 2020 Research and Innovation Programme under Grant Agreements 825460 (ELITR) and 825299 (GoURMET). This project has received support from Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland. Rico Sennrich acknowledges support of the Swiss National Science Foundation (MUTAMUR; no. 176727). References Roee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for C"
2020.acl-main.148,P02-1040,0,\N,Missing
2020.acl-main.148,Q17-1026,0,\N,Missing
2020.acl-main.148,D18-1039,0,\N,Missing
2020.acl-main.148,D18-1338,0,\N,Missing
2020.acl-main.148,W19-5301,0,\N,Missing
2020.acl-main.148,D19-1165,0,\N,Missing
2020.acl-main.148,2020.findings-emnlp.283,0,\N,Missing
2020.acl-main.326,J96-2004,0,0.560427,"MLE) (Wu et al., 2019) DynamicConv (MLE) (Wu et al., 2019) 32.2 32.8 (+0.6) 34.4 35.2 MLE MRT 34.7 35.2 (+0.5) 4 Table 2: Results for IWSLT’14 DE→EN with MLE and MRT (in brackets, improvement over MLE). gual (German/English) high school or University programs. We collected ∼3600 annotations in total, spread over 12 configurations. We ask annotators to evaluate translations according to fluency and adequacy. For fluency, the annotator classifies a translation as fluent, partially fluent or not fluent; for adequacy, as adequate, partially adequate or inadequate. We report kappa coefficient (K) (Carletta, 1996) for inter-annotator and intra-annotator agreement in Table 1, and assess statistical significance with Fisher’s exact test (two-tailed). 3.4 domain performance, which is a first indicator that exposure bias may be more problematic under domain shift. Our OOD results lag slightly behind those of M¨uller et al. (2019), but note that the techniques employed by them, namely reconstruction (Tu et al., 2017; Niu et al., 2019), subword regularization (Kudo, 2018), and noisy channel modelling (Li and Jurafsky, 2016) are orthogonal to MRT. We leave the combination of these approaches to future work. R"
2020.acl-main.326,2014.iwslt-evaluation.1,0,0.0353988,"Missing"
2020.acl-main.326,N18-1033,0,0.235222,"distribution: X X R(θ) = P (˜ y|x; θ) ∆ (˜ y, y) (2) (x,y)∈D y ˜∈Y(x) in which the loss ∆ (˜ y, y) indicates the discrepancy between the gold translation y and the model prediction y ˜. Due to the intractable search space, the posterior distribution Y(x) is approximated by a subspace S(x) by sampling a certain number of candidate translations, and normalizing: P˜ (˜ y|x; θ, α) = P P (˜ y|x; θ)α α 0 y0 ∈S(x) P (y |x; θ) (3) where α is a hyperparameter to control the sharpness of the subspace. Based on preliminary results, we use random sampling to generate candidate translations, and following Edunov et al. (2018), do not add the reference translation to the subspace. 3 3.1 For DE→EN, data comes from OPUS (Lison and Tiedemann, 2016), and is comprised of five domains: medical, IT, law, koran and subtitles. We use medical for training and development, and report results on an in-domain test set and the four other domains (out-of-domain; OOD). German→Romansh (DE→RM) is a low-resource language pair where robustness to domain shift is of practical relevance. The training data is from the Allegra corpus (Scherrer and Cartoni, 2012) (law domain) with 100 000 sentence pairs. The test domain are blogs, using da"
2020.acl-main.326,W18-6322,0,0.0231189,"uence-level training objectives that reduce or eliminate exposure bias. Furthermore, we believe that a better understanding of the links between exposure bias and well-known translation problems will help practitioners decide when sequence-level training objectives are especially promising, for example in settings where the test domain is unknown, or where hallucinations are a common problem. Acknowledgments Table 6: Average OOD BLEU and proportion of hallucinations with different beam sizes k. DE→EN OPUS. 9 The beam search problem has previously been linked to length bias (Yang et al., 2018; Murray and Chiang, 2018) and the copy mode (Ott et al., 2018). We consider hallucinations another result of using large search spaces with MLE models. Chaojun Wang was supported by the UK Engineering and Physical Sciences Research Council (EPSRC) fellowship grant EP/S001271/1 (MTStretch). Rico Sennrich acknowledges support of the Swiss National Science Foundation (MUTAMUR; no. 176727). This project has received support from Samsung Electronics Polska sp. z o.o. - Samsung R&D Institute Poland. 3548 References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to al"
2020.acl-main.326,N19-1043,0,0.019223,", the annotator classifies a translation as fluent, partially fluent or not fluent; for adequacy, as adequate, partially adequate or inadequate. We report kappa coefficient (K) (Carletta, 1996) for inter-annotator and intra-annotator agreement in Table 1, and assess statistical significance with Fisher’s exact test (two-tailed). 3.4 domain performance, which is a first indicator that exposure bias may be more problematic under domain shift. Our OOD results lag slightly behind those of M¨uller et al. (2019), but note that the techniques employed by them, namely reconstruction (Tu et al., 2017; Niu et al., 2019), subword regularization (Kudo, 2018), and noisy channel modelling (Li and Jurafsky, 2016) are orthogonal to MRT. We leave the combination of these approaches to future work. Results Table 2 shows results for IWSLT’14. We compare to results by Edunov et al. (2018), who use a convolutional architecture (Gehring et al., 2017), and Wu et al. (2019), who report results with Transfomerbase and dynamic convolution. With 34.7 BLEU, our baseline is competitive. We observe an improvement of 0.5 BLEU from MRT, comparable to Edunov et al. (2018), although we start from a stronger baseline (+2.5 BLEU). Ta"
2020.acl-main.326,P03-1021,0,0.267176,"e-level objectives is likely to be helpful. We further empirically confirm the link between exposure bias and the ‘beam search problem’, i.e. the fact that translation quality does not increase consistently with beam size (Koehn and Knowles, 2017; Ott et al., 2018; Stahlberg and Byrne, 2019). We base our experiments on German→English IWSLT’14, and two datasets used to investigate domain robustness by M¨uller et al. (2019): a selection of corpora from OPUS (Lison and Tiedemann, 2016) for German→English, and a low-resource German→Romansh scenario. We experiment with Minimum Risk Training (MRT) (Och, 2003; Shen et al., 2016), a training objective which inherently avoids exposure bias. Our experiments show that MRT indeed improves quality more in out-of-domain settings, and reduces the amount of hallucination. Our analysis of translation uncertainty also shows how the MLE baseline over-estimates the probability of random translations at all but the initial time steps, and how MRT mitigates this problem. Finally, we show that the beam search problem is reduced by MRT. 3544 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3544–3552 c July 5 - 10, 2020"
2020.acl-main.326,P02-1040,0,0.106558,"ansformer baseline (Vaswani et al., 2017), we first conduct experiments on IWSLT’14 German→English (DE→EN) (Cettolo et al., 2014), which consists of 180 000 sentence pairs. We follow previous work for data splits (Ranzato et al., 2016; Edunov et al., 2018). For experiments with domain shift, we use data sets and preprocessing as M¨uller et al. (2019)2 . 1 This is equivalent to maximizing the likelihood of the data, hence Maximum Likelihood Estimation (MLE). 2 https://github.com/ZurichNLP/ domain-robustness Model Evaluation For comparison to previous work, we report lowercased, tokenised BLEU (Papineni et al., 2002) with multi-bleu.perl for IWSLT’14, and cased, detokenised BLEU with SacreBLEU (Post, 2018)5 otherwise. For settings with domain shift, we report average and standard deviation of 3 independent training runs to account for optimizer instability. The manual evaluation was performed by two native speakers of German who completed bilin3 https://www.suedostschweiz.ch/blogs/ convivenza 4 Code available at https: //github.com/zippotju/ Exposure-Bias-Hallucination-Domain-Shift 5 Signature: BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.2 3545 inter-annotator annotation P (A) P (E) fluency adequacy 0.66 0.82 0."
2020.acl-main.326,W18-6319,0,0.0269796,"→EN) (Cettolo et al., 2014), which consists of 180 000 sentence pairs. We follow previous work for data splits (Ranzato et al., 2016; Edunov et al., 2018). For experiments with domain shift, we use data sets and preprocessing as M¨uller et al. (2019)2 . 1 This is equivalent to maximizing the likelihood of the data, hence Maximum Likelihood Estimation (MLE). 2 https://github.com/ZurichNLP/ domain-robustness Model Evaluation For comparison to previous work, we report lowercased, tokenised BLEU (Papineni et al., 2002) with multi-bleu.perl for IWSLT’14, and cased, detokenised BLEU with SacreBLEU (Post, 2018)5 otherwise. For settings with domain shift, we report average and standard deviation of 3 independent training runs to account for optimizer instability. The manual evaluation was performed by two native speakers of German who completed bilin3 https://www.suedostschweiz.ch/blogs/ convivenza 4 Code available at https: //github.com/zippotju/ Exposure-Bias-Hallucination-Domain-Shift 5 Signature: BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.2 3545 inter-annotator annotation P (A) P (E) fluency adequacy 0.66 0.82 0.38 0.61 K 0.44 0.54 intra-annotator P (A) P (E) 0.87 0.93 0.42 0.66 K 0.77 0.79 Table 1: In"
2020.acl-main.326,scherrer-cartoni-2012-trilingual,0,0.0111954,"ry results, we use random sampling to generate candidate translations, and following Edunov et al. (2018), do not add the reference translation to the subspace. 3 3.1 For DE→EN, data comes from OPUS (Lison and Tiedemann, 2016), and is comprised of five domains: medical, IT, law, koran and subtitles. We use medical for training and development, and report results on an in-domain test set and the four other domains (out-of-domain; OOD). German→Romansh (DE→RM) is a low-resource language pair where robustness to domain shift is of practical relevance. The training data is from the Allegra corpus (Scherrer and Cartoni, 2012) (law domain) with 100 000 sentence pairs. The test domain are blogs, using data from Convivenza3 . We have access to 2000 sentences for development and testing, respectively, in each domain. We tokenise and truecase data sets with Moses (Koehn et al., 2007), and use shared BPE with 32 000 units (Sennrich et al., 2016). 3.2 We implement4 MRT in the Nematus toolkit (Sennrich et al., 2017). All our experiments use the Transformer architecture (Vaswani et al., 2017). Following Edunov et al. (2018), we use 1-BLEUsmooth (Lin and Och, 2004) as the MRT loss. Models are pre-trained with the token-leve"
2020.acl-main.326,E17-3017,1,0.878077,"Missing"
2020.acl-main.326,D16-1137,0,0.0638775,"Missing"
2020.acl-main.326,P16-1162,1,0.465726,"ing and development, and report results on an in-domain test set and the four other domains (out-of-domain; OOD). German→Romansh (DE→RM) is a low-resource language pair where robustness to domain shift is of practical relevance. The training data is from the Allegra corpus (Scherrer and Cartoni, 2012) (law domain) with 100 000 sentence pairs. The test domain are blogs, using data from Convivenza3 . We have access to 2000 sentences for development and testing, respectively, in each domain. We tokenise and truecase data sets with Moses (Koehn et al., 2007), and use shared BPE with 32 000 units (Sennrich et al., 2016). 3.2 We implement4 MRT in the Nematus toolkit (Sennrich et al., 2017). All our experiments use the Transformer architecture (Vaswani et al., 2017). Following Edunov et al. (2018), we use 1-BLEUsmooth (Lin and Och, 2004) as the MRT loss. Models are pre-trained with the token-level objective MLE and then fine-tuned with MRT. Hyperparameters mostly follow previous work (Edunov et al., 2018; M¨uller et al., 2019); for MRT, we conduct limited hyperparameter search on the IWSLT’14 development set, including learning rate, batch size, and the sharpness parameter α. We set the number of candidate tra"
2020.acl-main.326,D18-1396,0,0.044388,"Missing"
2020.acl-main.326,P16-1159,0,0.201961,"put (hallucinations), and their misleading nature makes them particularly problematic. We hypothesise that exposure bias (Ranzato et al., 2016), a discrepancy between training and inference, makes this problem worse. Specifically, training with teacher forcing only exposes the model to gold history, while previous predictions during inference may be erroneous. Thus, the model trained with teacher forcing may over-rely on previously predicted words, which would exacerbate error propagation. Previous work has sought to reduce exposure bias in training (Bengio et al., 2015; Ranzato et al., 2016; Shen et al., 2016; Wiseman and Rush, 2016; Zhang et al., 2019). However, the relevance of error propagation is under debate: Wu et al. (2018) argue that its role is overstated in literature, and that linguistic features explain some of the accuracy drop at higher time steps. Previous work has established a link between domain shift and hallucination in NMT (Koehn and Knowles, 2017; M¨uller et al., 2019). In this paper, we will aim to also establish an empirical link between hallucination and exposure bias. Such a link will deepen our understanding of the hallucination problem, but also has practical relevance,"
2020.acl-main.326,D18-1342,0,0.0857885,"Missing"
2020.acl-main.326,D19-1331,0,0.0290652,"ination in NMT (Koehn and Knowles, 2017; M¨uller et al., 2019). In this paper, we will aim to also establish an empirical link between hallucination and exposure bias. Such a link will deepen our understanding of the hallucination problem, but also has practical relevance, e.g. to help predicting in which settings the use of sequence-level objectives is likely to be helpful. We further empirically confirm the link between exposure bias and the ‘beam search problem’, i.e. the fact that translation quality does not increase consistently with beam size (Koehn and Knowles, 2017; Ott et al., 2018; Stahlberg and Byrne, 2019). We base our experiments on German→English IWSLT’14, and two datasets used to investigate domain robustness by M¨uller et al. (2019): a selection of corpora from OPUS (Lison and Tiedemann, 2016) for German→English, and a low-resource German→Romansh scenario. We experiment with Minimum Risk Training (MRT) (Och, 2003; Shen et al., 2016), a training objective which inherently avoids exposure bias. Our experiments show that MRT indeed improves quality more in out-of-domain settings, and reduces the amount of hallucination. Our analysis of translation uncertainty also shows how the MLE baseline ov"
2020.acl-main.326,P19-1426,0,0.0628481,"Missing"
2020.acl-main.326,C04-1072,0,\N,Missing
2020.acl-main.326,L16-1147,0,\N,Missing
2020.acl-main.688,D18-1399,0,0.0595878,"Missing"
2020.acl-main.688,W17-4715,1,0.894071,"Missing"
2020.acl-main.688,Y17-1038,0,0.206211,"rmed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation systems. Typically, ou"
2020.acl-main.688,P19-1120,0,0.0716333,"Missing"
2020.acl-main.688,D18-1101,0,0.0453867,"Missing"
2020.acl-main.688,W18-6325,0,0.190356,"an off-the-shelf transfer learning baseline and simplified versions of the transfer learning scheme. If a simplified version recovers some of the quality gains of full transfer learning, it suggests that the simplified version has captured some of the information being transferred. Since information may be transferred redundantly, our claims are limited to sufficiency rather than exclusivity. Transferring word embeddings is not straightforward since languages have different vocabularies. Zoph et al. (2016) claimed that vocabulary alignment is not necessary, while Nguyen and Chiang (2017) and Kocmi and Bojar (2018) suggest a joint vocabulary. We find that the vocabulary has to be aligned before transferring the embedding to achieve a substantial improvement. Transfer learning without the embedding or with vocabulary mismatches is still possible, but with lower quality. Conversely, transferring only the word embeddings can be worse than transferring nothing at all. A rudimentary model of machine translation consists of alignment and token mapping. We hypothesize that these capabilities are transferred across languages. To test this, we experiment with transferring from auto-encoders that learn purely dia"
2020.acl-main.688,J82-2005,0,0.629768,"Missing"
2020.acl-main.688,N18-1032,0,0.136034,"transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation systems. Typically, our experiments have a baseline that was trained from scratch"
2020.acl-main.688,D18-1398,0,0.118764,"transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation systems. Typically, our experiments have a baseline that was trained from scratch"
2020.acl-main.688,I17-2050,0,0.475249,"sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation systems. Typically, our experiments have a baseline that was tra"
2020.acl-main.688,Q17-1024,0,0.0607087,"Missing"
2020.acl-main.688,N19-1119,0,0.0382838,"Missing"
2020.acl-main.688,P18-4020,1,0.876608,"Missing"
2020.acl-main.688,W18-6319,0,0.0194858,"task (Bojar et al., 2017). This data consists of 207k pairs of sentences. Similar to Id→En, we add a back-translation corpus from News Crawl 2015. Our total training data consists of 415k sentence pairs. For all language pairs, we use byte-pair encoding (Sennrich et al., 2016b) to tokenise words into subword units. 3.3 Training Setup We use a standard transformer-base architecture with six encoder and six decoder layers for all experiments with the default hyperparameters (Vaswani et al., 2017). Training and decoding use Marian (Junczys-Dowmunt et al., 2018), while evaluation uses SacreBLEU (Post, 2018). 1 http://www.panl10n.net/english/ OutputsIndonesia2.htm Results Parent En→De En→Ru De→En Ru→En High-Resource Datasets We use German-English and Russian-English datasets for our parent models. Our GermanEnglish dataset is taken from the WMT17 news translation task (Bojar et al., 2017). Our RussianEnglish is taken from the WMT18 task (Bojar et al., 2018). For both pairs, we preprocess the input with byte-pair encoding (Sennrich et al., 2016b). 3.2 3.4 My→En 4.0 17.5 17.8 17.3 17.1 BLEU Id→En 20.6 27.5 27.4 26.3 26.8 Tr→En 19.0 20.2 20.3 20.1 20.6 Table 1: Transfer learning performance across d"
2020.acl-main.688,E17-2025,0,0.0320856,"omputational Linguistics, pages 7701–7710 c July 5 - 10, 2020. 2020 Association for Computational Linguistics period, though only real language parents yielded faster training. 2 Related Work Transfer learning has been successfully used in lowresource scenarios for NMT. Zoph et al. (2016) gain 5 BLEU points in Uzbek–English by transferring from French–English. Their style of transfer learning copies the entire model, including word embeddings, ignoring the vocabulary mismatch between parent and child. They used separate embeddings for source and target language words, whereas tied embeddings (Press and Wolf, 2017; Vaswani et al., 2017) have since become the de-facto standard in low-resource NMT. Tied embeddings provide us with the opportunity to revisit some of their findings. In Section 5, we find an English–English copy model does work as a parent with tied embeddings, whereas Zoph et al. (2016) reported no gains from a copy model with untied embeddings. Methods to cope with vocabulary mismatch have improved since Zoph et al. (2016). Kocmi and Bojar (2018) suggest that a shared vocabulary between the parent language and the child is beneficial, though this requires knowledge of the child languages w"
2020.acl-main.688,N18-2084,0,0.0237937,"ngs, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation systems. Typically, our experiments hav"
2020.acl-main.688,P16-1009,1,0.917615,"out the experimental setup used for the rest of the paper. 3.1 Low-Resource Datasets We use the following datasets: Burmese–English: For our My→En parallel data, we used 18k parallel sentences from the Asian Language Treebank (ALT) Project (Ding et al., 2018, 2019) collected from news articles. Indonesian–English: Id→En parallel data consists of 22k news-related sentences, which are taken from the PAN Localization BPPT corpus.1 This dataset does not have a test/validation split. Hence we randomly sample 2000 sentences to use as test and validation sets. We augment our data by backtranslating (Sennrich et al., 2016a) News Crawl from 2015. Our total training set (including the back-translated sentences) consists of 88k pairs of sentences. Turkish–English: Tr→En data comes from the WMT17 news translation task (Bojar et al., 2017). This data consists of 207k pairs of sentences. Similar to Id→En, we add a back-translation corpus from News Crawl 2015. Our total training data consists of 415k sentence pairs. For all language pairs, we use byte-pair encoding (Sennrich et al., 2016b) to tokenise words into subword units. 3.3 Training Setup We use a standard transformer-base architecture with six encoder and six"
2020.acl-main.688,P16-1162,1,0.714863,"out the experimental setup used for the rest of the paper. 3.1 Low-Resource Datasets We use the following datasets: Burmese–English: For our My→En parallel data, we used 18k parallel sentences from the Asian Language Treebank (ALT) Project (Ding et al., 2018, 2019) collected from news articles. Indonesian–English: Id→En parallel data consists of 22k news-related sentences, which are taken from the PAN Localization BPPT corpus.1 This dataset does not have a test/validation split. Hence we randomly sample 2000 sentences to use as test and validation sets. We augment our data by backtranslating (Sennrich et al., 2016a) News Crawl from 2015. Our total training set (including the back-translated sentences) consists of 88k pairs of sentences. Turkish–English: Tr→En data comes from the WMT17 news translation task (Bojar et al., 2017). This data consists of 207k pairs of sentences. Similar to Id→En, we add a back-translation corpus from News Crawl 2015. Our total training data consists of 415k sentence pairs. For all language pairs, we use byte-pair encoding (Sennrich et al., 2016b) to tokenise words into subword units. 3.3 Training Setup We use a standard transformer-base architecture with six encoder and six"
2020.acl-main.688,P18-1072,0,0.0509496,"Missing"
2020.acl-main.688,D16-1163,0,0.335754,"arning can be performed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warmup phase when training transformer models in high resource language pairs. 1 Introduction Transfer learning is a common method for lowresource neural machine translation (NMT) (Zoph et al., 2016; Dabre et al., 2017; Qi et al., 2018; Nguyen and Chiang, 2017; Gu et al., 2018b). However, it is unclear what settings make transfer learning successful and what knowledge is being transferred. Understanding why transfer learning is successful can improve best practices while also opening the door to investigating ways to gain similar benefits without requiring parent models. In this paper, we perform several ablation studies on transfer learning in order to understand what information is being transferred. We apply a black box methodology by measuring the quality of end-to-end translation sy"
2020.amta-research.14,D16-1162,0,0.0356454,"Missing"
2020.amta-research.14,W18-1820,0,0.0222739,"Missing"
2020.amta-research.14,Q17-1024,0,0.0840171,"Missing"
2020.amta-research.14,D16-1139,0,0.0324645,"rnot et al. 2016), where the student shares the network architecture with the teacher, with the purpose not being model compression, but improving the model’s generalization to samples outside of its training set, and specifically robustness against adversarial examples (Szegedy et al., 2013; Goodfellow et al., 2014). Defensive distillation has been shown to be effective at improving robustness to adversarial examples in image recognition. In this work, we apply it to NMT and test its effect on domain robustness, which Papernot et al. (2016) hint at but do not empirically test. As proposed by Kim and Rush (2016), we train the student model on teacher translations produced with beam search, instead of training on soft prediction labels. 4.3 Reconstruction Reconstruction (Tu et al., 2017) is a change to the model architecture that addresses the problem of adequacy. The authors propose to extend encoder-decoder models with a reconstructor component that learns to reconstruct the source sentence from decoder states. The reconstructor has two uses: as a training objective, it forces the decoder representations to retain information that will be useful for reconstruction; during inference, it can provide s"
2020.amta-research.14,kobus-etal-2017-domain,0,0.0133011,"arn models that generalize well to unseen data distributions, including data from other domains. We will refer to this property of showing good generalization to unseen domains as domain robustness. We consider domain robustness a desirable property of NLP systems, along with other types of robustness, such as robustness against adversarial examples (Goodfellow et al. 2015) or typos in the input (Belinkov and Bisk, 2018). While domain adaptation with small amounts of parallel or monolingual in-domain data has proven very effective for NMT (e.g. Luong and Manning, 2015; Sennrich et al., 2016a; Kobus et al., 2017; Li et al., 2019), the target domain(s) may be unknown when a system is built, and there are language pairs for which training data is only available for limited domains. Hence, domain robustness of systems without any domain adaptation is not only of theoretical interest, but also relevant in practice. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 151 SRC Aber geh subtil dabei vor. REF But be subtle about it. HYP Pharmacokinetic parameters are not significantly affected in patients with rena"
2020.amta-research.14,P07-2045,0,0.0131706,"Missing"
2020.amta-research.14,W17-3204,0,0.538972,"adequacy or in-domain robustness in terms of their effectiveness at improving domain robustness. In experiments on German→English OPUS data, and German→Romansh (a low-resource setting) we find that several methods improve domain robustness. While those methods do lead to higher BLEU scores overall, they only slightly increase the adequacy of translations compared to SMT. 1 Introduction Even though neural models have improved the state-of-the-art in machine translation considerably in recent years, they still underperform in specific conditions. One such condition is out-of-domain translation. Koehn and Knowles (2017) found that neural machine translation (NMT) systems perform poorly in such settings and that their poor performance cannot be explained solely by the fact that out-of-domain translation is difficult: non-neural, statistical machine translation (SMT) systems were superior at this task. For this reason, Koehn and Knowles (2017) identified translation of out-of-domain text as a key challenge for NMT. Catastrophic failure to translate out-of-domain text can be viewed as overfitting to the training domain, i.e. systems learn idiosyncrasies of the domain rather than more general features. Our goal"
2020.amta-research.14,P18-1007,0,0.135676,"or simply as n-best list reranking. We adopt the latter, since n-best list reranking was shown to have equal or better performance than more computationally costly methods that score partial hypotheses during beam search (Yee et al., 2019). 5 Experimental Setup for Proposed Methods This section describes how we preprocessed data and trained the models described in Section 4. Unless stated otherwise, the data is preprocessed in the same way as for the baselines (see Section 3.1). Subword Regularization Models We integrate subword regularization in Sockeye, using the Python library provided by Kudo (2018)4 . The training data is not segmented with BPE in this case. Instead, the training tool is given truecased data, and new segmentations are sampled before each training epoch. We use the following hyperparameters: we set the smoothing parameter α to 0.1 and use an n-best size of 64. For the validation and test data we use 1-best segmentation. Defensive Distillation Models We use our baseline Transformer model as the teacher model. We translate the original training set with beam size 10. The student is trained on the translations of the teacher model using the same hyperparameters and being in"
2020.amta-research.14,W18-6459,0,0.0975086,"Missing"
2020.amta-research.14,L16-1147,0,0.0382666,"Missing"
2020.amta-research.14,2015.iwslt-evaluation.11,0,0.0177166,"her than more general features. Our goal is to learn models that generalize well to unseen data distributions, including data from other domains. We will refer to this property of showing good generalization to unseen domains as domain robustness. We consider domain robustness a desirable property of NLP systems, along with other types of robustness, such as robustness against adversarial examples (Goodfellow et al. 2015) or typos in the input (Belinkov and Bisk, 2018). While domain adaptation with small amounts of parallel or monolingual in-domain data has proven very effective for NMT (e.g. Luong and Manning, 2015; Sennrich et al., 2016a; Kobus et al., 2017; Li et al., 2019), the target domain(s) may be unknown when a system is built, and there are language pairs for which training data is only available for limited domains. Hence, domain robustness of systems without any domain adaptation is not only of theoretical interest, but also relevant in practice. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 151 SRC Aber geh subtil dabei vor. REF But be subtle about it. HYP Pharmacokinetic parameters are not"
2020.amta-research.14,N18-1031,0,0.0593566,"Missing"
2020.amta-research.14,N19-1043,0,0.0563317,"dence for NMT systems occasionally falling into a hallucination mode where translations are grammatically correct but unrelated to the source sentence (Arthur et al., 2016; Koehn and Knowles, 2017; Nguyen and Chiang, 2018). Our manual evaluation shows that hallucination is more pronounced in out-of-domain translation. We therefore expect methods that alleviate the hallucination problem to indirectly improve domain robustness. As a means to reduce hallucination, we experiment with several techniques and assess their effectiveness in improving domain robustness: reconstruction (Tu et al., 2017; Niu et al., 2019), subword regularization (Kudo, 2018), neural noisy channel models (Li and Jurafsky, 2016; Yee et al., 2019), and defensive distillation (Papernot et al., 2016), as well as combinations of these techniques. The main contributions of this paper are: • we perform an analysis of SMT and NMT systems that confirms that while in-domain BLEU increased, domain robustness remains a major problem even with state-of-the-art Transformer architectures. Our comparison of SMT and NMT shows differences in how performance degrades in unseen domains: SMT mostly suffers in terms of fluency, while NMT tends to pr"
2020.amta-research.14,N19-4009,0,0.0201896,"t for the new loss and a lower initial learning rate. For testing we select the model with the lowest validation perplexity. We use the reconstruction loss only for training, not for reranking. Noisy Channel Reranking For each hypothesis, we store an n-best list of 50. We produce the following scores: p(y|x) (usual translation score), p(x|y) (translation score in reverse direction) and p(y) (language model score in target language). p(y|x) and p(x|y) are computed with the same model since it is bi-directional. In order to produce p(y) scores we train a Transformer language model with fairseq (Ott et al., 2019) using standard settings. We impose a large penalty of −100 for hypotheses that contain subwords not found in the target side training data. The final hypothesis score for reranking is computed as a weighted multiplication: score(x, y) = p(y|x)λtf ∗ p(x|y)λtb ∗ p(y)λlm 4 https://github.com/google/sentencepiece 5 https://github.com/ZurichNLP/sockeye/tree/domain-robustness Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 158 DE→EN DE→RM λtf λtb λlm λtf λtb λlm (5) Multilingual (6) Reconstruction 0."
2020.amta-research.14,P02-1040,0,0.109033,"of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 153 SMT NMT in-domain medical 58.4 61.5 out-of-domain IT koran law subtitles 21.4 1.4 19.8 4.7 17.1 1.1 25.3 3.4 average (out-of-domain) 11.8 11.7 SMT NMT in-domain law 45.2 52.5 out-of-domain blogs 15.5 18.9 (b) (a) Table 2: BLEU scores of (a) baseline DE→EN systems trained on medical data, (b) baseline DE→RM systems trained on law data. We always test on several domains, including the training domain. We use a beam size of 10 to translate test data. We report case-sensitive BLEU (Papineni et al., 2002) scores on detokenized text, computed with SacreBLEU (Post, 2018)2 . 3.2 Analysis of Baseline Systems Tables 2a and 2b show automatic evaluation results for all our baseline models. Neural models achieve good performance on the respective in-domain test sets (61.5 BLEU on medical for DE→EN; 52.5 BLEU on law for DE→RM), but on out-of-domain text, translation quality is clearly diminished, with an average BLEU of roughly 12 (DE→EN) and 19 (DE→RM). The following analysis will focus on our DE→EN baseline systems. Compared to results reported by Koehn and Knowles (2017), NMT has improved markedly s"
2020.amta-research.14,W18-6319,0,0.0233353,"20, Volume 1: MT Research Track Page 153 SMT NMT in-domain medical 58.4 61.5 out-of-domain IT koran law subtitles 21.4 1.4 19.8 4.7 17.1 1.1 25.3 3.4 average (out-of-domain) 11.8 11.7 SMT NMT in-domain law 45.2 52.5 out-of-domain blogs 15.5 18.9 (b) (a) Table 2: BLEU scores of (a) baseline DE→EN systems trained on medical data, (b) baseline DE→RM systems trained on law data. We always test on several domains, including the training domain. We use a beam size of 10 to translate test data. We report case-sensitive BLEU (Papineni et al., 2002) scores on detokenized text, computed with SacreBLEU (Post, 2018)2 . 3.2 Analysis of Baseline Systems Tables 2a and 2b show automatic evaluation results for all our baseline models. Neural models achieve good performance on the respective in-domain test sets (61.5 BLEU on medical for DE→EN; 52.5 BLEU on law for DE→RM), but on out-of-domain text, translation quality is clearly diminished, with an average BLEU of roughly 12 (DE→EN) and 19 (DE→RM). The following analysis will focus on our DE→EN baseline systems. Compared to results reported by Koehn and Knowles (2017), NMT has improved markedly since their study was conducted, and is now on par with SMT in out"
2020.amta-research.14,scherrer-cartoni-2012-trilingual,0,0.0118285,"s, the medical domain serves as the training domain, while the remaining four domains are used for testing. 2.2 German→Romansh To complement our DE→EN experiments, we also train systems for DE→RM. Romansh is a Romance language that, with an estimated 40 000 native speakers, is low-resource, but has some parallel resources thanks to its status as an official Swiss language. Domain robustness is of particular relevance in low-resource settings since training data is typically only available for few domains. Our training data consists of 100 000 sentence pairs, specifically the Allegra corpus by Scherrer and Cartoni (2012) which contains mostly law text, and an in-house collection of government press releases. As test domain (unseen during training), we use blog posts from Convivenza1 . From both data sets we randomly select 2000 consecutive sentence pairs as test sets. 3 State-of-the-art Models Exhibit Low Domain Robustness In this section, we establish that current NMT systems exhibit low domain robustness by analyzing our baseline systems automatically and manually. 3.1 Experimental Setup for Baseline Models We use Moses scripts for punctuation normalization and tokenization. We apply truecasing trained on i"
2020.amta-research.14,P16-1009,1,0.903613,"tures. Our goal is to learn models that generalize well to unseen data distributions, including data from other domains. We will refer to this property of showing good generalization to unseen domains as domain robustness. We consider domain robustness a desirable property of NLP systems, along with other types of robustness, such as robustness against adversarial examples (Goodfellow et al. 2015) or typos in the input (Belinkov and Bisk, 2018). While domain adaptation with small amounts of parallel or monolingual in-domain data has proven very effective for NMT (e.g. Luong and Manning, 2015; Sennrich et al., 2016a; Kobus et al., 2017; Li et al., 2019), the target domain(s) may be unknown when a system is built, and there are language pairs for which training data is only available for limited domains. Hence, domain robustness of systems without any domain adaptation is not only of theoretical interest, but also relevant in practice. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 151 SRC Aber geh subtil dabei vor. REF But be subtle about it. HYP Pharmacokinetic parameters are not significantly affected"
2020.amta-research.14,P16-1162,1,0.727363,"tures. Our goal is to learn models that generalize well to unseen data distributions, including data from other domains. We will refer to this property of showing good generalization to unseen domains as domain robustness. We consider domain robustness a desirable property of NLP systems, along with other types of robustness, such as robustness against adversarial examples (Goodfellow et al. 2015) or typos in the input (Belinkov and Bisk, 2018). While domain adaptation with small amounts of parallel or monolingual in-domain data has proven very effective for NMT (e.g. Luong and Manning, 2015; Sennrich et al., 2016a; Kobus et al., 2017; Li et al., 2019), the target domain(s) may be unknown when a system is built, and there are language pairs for which training data is only available for limited domains. Hence, domain robustness of systems without any domain adaptation is not only of theoretical interest, but also relevant in practice. Proceedings of the 14th Conference of the Association for Machine Translation in the Americas October 6 - 9, 2020, Volume 1: MT Research Track Page 151 SRC Aber geh subtil dabei vor. REF But be subtle about it. HYP Pharmacokinetic parameters are not significantly affected"
2020.amta-research.14,D19-1571,0,0.0712498,"rrect but unrelated to the source sentence (Arthur et al., 2016; Koehn and Knowles, 2017; Nguyen and Chiang, 2018). Our manual evaluation shows that hallucination is more pronounced in out-of-domain translation. We therefore expect methods that alleviate the hallucination problem to indirectly improve domain robustness. As a means to reduce hallucination, we experiment with several techniques and assess their effectiveness in improving domain robustness: reconstruction (Tu et al., 2017; Niu et al., 2019), subword regularization (Kudo, 2018), neural noisy channel models (Li and Jurafsky, 2016; Yee et al., 2019), and defensive distillation (Papernot et al., 2016), as well as combinations of these techniques. The main contributions of this paper are: • we perform an analysis of SMT and NMT systems that confirms that while in-domain BLEU increased, domain robustness remains a major problem even with state-of-the-art Transformer architectures. Our comparison of SMT and NMT shows differences in how performance degrades in unseen domains: SMT mostly suffers in terms of fluency, while NMT tends to produce more fluent, but less adequate translations (hallucinations). • we test several techniques related to"
2020.coling-main.375,P17-1080,0,0.0801062,"odels have been shown to perform better than BPE-based models (Cherry et al., 2018). In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism. Previous studies have tried to interpret and understand NMT models by interpreting attention weights (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018; Tang et al., 2019), using gradients (He et al., 2019), applying layer-wise relevance propagation (Ding et al., 2017), probing classification tasks (Belinkov et al., 2017b; Belinkov et al., 2017a; Belinkov et al., 2020; Poliak et al., 2018; Tang et al., 2019), and more intrinsic analysis (Ghader and Monz, 2019; Voita et al., 2019). However, only Belinkov et al. (2017a; Belinkov et al. (2020) have probed character-based representations. Belinkov et al. (2017a) have only explored character-aware word-level representations, while we investigate fully character-level representations, which are also studied in Belinkov et al. (2020). We apply more composition methods to explore how CHAR models learn linguistic knowledge and how attention extracts features directly"
2020.coling-main.375,I17-1001,0,0.0436194,"Missing"
2020.coling-main.375,D18-1461,0,0.248477,"e translation significantly in recent years (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). However, it is still unclear how NMT models work due to the black-box nature of neural networks. Better understandings of NMT models could guide us in improving NMT systems. Currently most of the studies towards understanding NMT models only take into account subword-based (e.g. BPE-based) models. Deeper character-based (CHAR) models have been shown to perform better than BPE-based models (Cherry et al., 2018). In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism. Previous studies have tried to interpret and understand NMT models by interpreting attention weights (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018; Tang et al., 2019), using gradients (He et al., 2019), applying layer-wise relevance propagation (Ding et al., 2017), probing classification tasks (Belinkov et al., 2017b; Belinkov et al., 2017a; Belinkov et al., 2020; Poliak et al"
2020.coling-main.375,D14-1179,0,0.0633082,"Missing"
2020.coling-main.375,W19-4828,0,0.0391358,"Missing"
2020.coling-main.375,N19-1423,0,0.0464375,"Missing"
2020.coling-main.375,P17-1106,0,0.0173559,"BPE-based) models. Deeper character-based (CHAR) models have been shown to perform better than BPE-based models (Cherry et al., 2018). In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism. Previous studies have tried to interpret and understand NMT models by interpreting attention weights (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018; Tang et al., 2019), using gradients (He et al., 2019), applying layer-wise relevance propagation (Ding et al., 2017), probing classification tasks (Belinkov et al., 2017b; Belinkov et al., 2017a; Belinkov et al., 2020; Poliak et al., 2018; Tang et al., 2019), and more intrinsic analysis (Ghader and Monz, 2019; Voita et al., 2019). However, only Belinkov et al. (2017a; Belinkov et al. (2020) have probed character-based representations. Belinkov et al. (2017a) have only explored character-aware word-level representations, while we investigate fully character-level representations, which are also studied in Belinkov et al. (2020). We apply more composition methods to explore how CHAR models learn linguistic kn"
2020.coling-main.375,N19-1154,0,0.0364743,"Missing"
2020.coling-main.375,I17-1004,0,0.0171175,"tandings of NMT models could guide us in improving NMT systems. Currently most of the studies towards understanding NMT models only take into account subword-based (e.g. BPE-based) models. Deeper character-based (CHAR) models have been shown to perform better than BPE-based models (Cherry et al., 2018). In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism. Previous studies have tried to interpret and understand NMT models by interpreting attention weights (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018; Tang et al., 2019), using gradients (He et al., 2019), applying layer-wise relevance propagation (Ding et al., 2017), probing classification tasks (Belinkov et al., 2017b; Belinkov et al., 2017a; Belinkov et al., 2020; Poliak et al., 2018; Tang et al., 2019), and more intrinsic analysis (Ghader and Monz, 2019; Voita et al., 2019). However, only Belinkov et al. (2017a; Belinkov et al. (2020) have probed character-based representations. Belinkov et al. (2017a) have only explored character-aware word-level representations, while we investigate fu"
2020.coling-main.375,W19-6611,0,0.0171359,"sm of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism. Previous studies have tried to interpret and understand NMT models by interpreting attention weights (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018; Tang et al., 2019), using gradients (He et al., 2019), applying layer-wise relevance propagation (Ding et al., 2017), probing classification tasks (Belinkov et al., 2017b; Belinkov et al., 2017a; Belinkov et al., 2020; Poliak et al., 2018; Tang et al., 2019), and more intrinsic analysis (Ghader and Monz, 2019; Voita et al., 2019). However, only Belinkov et al. (2017a; Belinkov et al. (2020) have probed character-based representations. Belinkov et al. (2017a) have only explored character-aware word-level representations, while we investigate fully character-level representations, which are also studied in Belinkov et al. (2020). We apply more composition methods to explore how CHAR models learn linguistic knowledge and how attention extracts features directly from characters. Probing classification tasks (Belinkov et al., 2017a) have emerged as a popular method to interpret the internal representat"
2020.coling-main.375,D19-1088,0,0.0187335,"anding NMT models only take into account subword-based (e.g. BPE-based) models. Deeper character-based (CHAR) models have been shown to perform better than BPE-based models (Cherry et al., 2018). In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism. Previous studies have tried to interpret and understand NMT models by interpreting attention weights (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018; Tang et al., 2019), using gradients (He et al., 2019), applying layer-wise relevance propagation (Ding et al., 2017), probing classification tasks (Belinkov et al., 2017b; Belinkov et al., 2017a; Belinkov et al., 2020; Poliak et al., 2018; Tang et al., 2019), and more intrinsic analysis (Ghader and Monz, 2019; Voita et al., 2019). However, only Belinkov et al. (2017a; Belinkov et al. (2020) have probed character-based representations. Belinkov et al. (2017a) have only explored character-aware word-level representations, while we investigate fully character-level representations, which are also studied in Belinkov et al. (2020). We apply more com"
2020.coling-main.375,2020.eamt-1.50,0,0.0534137,"Missing"
2020.coling-main.375,D13-1176,0,0.0535546,"ning linguistic knowledge. In addition, character-based models need more layers to encode word senses which explains why only deeper models outperform subword-based models. The attention distribution pattern shows that separators attract a lot of attention and we explore a sparse word-level attention to enforce character hidden states to capture the full word-level information. Experimental results show that the word-level attention with a single head results in 1.2 BLEU points drop. 1 Introduction Neural machine translation (NMT) has boosted machine translation significantly in recent years (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). However, it is still unclear how NMT models work due to the black-box nature of neural networks. Better understandings of NMT models could guide us in improving NMT systems. Currently most of the studies towards understanding NMT models only take into account subword-based (e.g. BPE-based) models. Deeper character-based (CHAR) models have been shown to perform better than BPE-based models (Cherry et al., 2018). In this paper, we try to investigate the working mech"
2020.coling-main.375,D15-1166,0,0.411527,"nses which explains why only deeper models outperform subword-based models. The attention distribution pattern shows that separators attract a lot of attention and we explore a sparse word-level attention to enforce character hidden states to capture the full word-level information. Experimental results show that the word-level attention with a single head results in 1.2 BLEU points drop. 1 Introduction Neural machine translation (NMT) has boosted machine translation significantly in recent years (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Gehring et al., 2017; Vaswani et al., 2017). However, it is still unclear how NMT models work due to the black-box nature of neural networks. Better understandings of NMT models could guide us in improving NMT systems. Currently most of the studies towards understanding NMT models only take into account subword-based (e.g. BPE-based) models. Deeper character-based (CHAR) models have been shown to perform better than BPE-based models (Cherry et al., 2018). In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and"
2020.coling-main.375,P02-1040,0,0.10895,"tag. The detailed statistics are provided in Table 1. 2.2 Experimental Settings NMT models We use the Sockeye (Hieber et al., 2017) toolkit to train NMT models. The encoder is a stack of 1 bidirectional RNN and 6 unidirectional RNNs, and the decoder has 8 unidirectional RNNs. We choose long short-term memory (LSTM) RNN unit (Hochreiter and Schmidhuber, 1997). The size of embeddings and hidden units is 512. We tie the source, target, and output embeddings. The beam size is 8 during inference. We employ the models that have the best perplexity on the validation set for evaluation. BLEU scores (Papineni et al., 2002) are computed by sacrebleu (Post, 2018). For CHAR models, we add separators between any two tokens including punctuation marks, and input character sequences to the model directly. The character vocabulary size is 379. For the BPE-based model, we learn a joint BPE model with 32K subwords (Sennrich et al., 2016). As Cherry et al. (2018) have shown that the depth is crucial to the success of CHAR models, we train a 4-layer CHAR model to study the effect of depth. Probing classifiers These probing classifiers are feed-forward neural networks with only one hidden layer, using ReLU non-linear activ"
2020.coling-main.375,N18-2082,0,0.0505931,"Missing"
2020.coling-main.375,W18-6319,0,0.0115204,"e 1. 2.2 Experimental Settings NMT models We use the Sockeye (Hieber et al., 2017) toolkit to train NMT models. The encoder is a stack of 1 bidirectional RNN and 6 unidirectional RNNs, and the decoder has 8 unidirectional RNNs. We choose long short-term memory (LSTM) RNN unit (Hochreiter and Schmidhuber, 1997). The size of embeddings and hidden units is 512. We tie the source, target, and output embeddings. The beam size is 8 during inference. We employ the models that have the best perplexity on the validation set for evaluation. BLEU scores (Papineni et al., 2002) are computed by sacrebleu (Post, 2018). For CHAR models, we add separators between any two tokens including punctuation marks, and input character sequences to the model directly. The character vocabulary size is 379. For the BPE-based model, we learn a joint BPE model with 32K subwords (Sennrich et al., 2016). As Cherry et al. (2018) have shown that the depth is crucial to the success of CHAR models, we train a 4-layer CHAR model to study the effect of depth. Probing classifiers These probing classifiers are feed-forward neural networks with only one hidden layer, using ReLU non-linear activation. The size of the hidden layer is"
2020.coling-main.375,W18-5431,0,0.0167659,"could guide us in improving NMT systems. Currently most of the studies towards understanding NMT models only take into account subword-based (e.g. BPE-based) models. Deeper character-based (CHAR) models have been shown to perform better than BPE-based models (Cherry et al., 2018). In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism. Previous studies have tried to interpret and understand NMT models by interpreting attention weights (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018; Tang et al., 2019), using gradients (He et al., 2019), applying layer-wise relevance propagation (Ding et al., 2017), probing classification tasks (Belinkov et al., 2017b; Belinkov et al., 2017a; Belinkov et al., 2020; Poliak et al., 2018; Tang et al., 2019), and more intrinsic analysis (Ghader and Monz, 2019; Voita et al., 2019). However, only Belinkov et al. (2017a; Belinkov et al. (2020) have probed character-based representations. Belinkov et al. (2017a) have only explored character-aware word-level representations, while we investigate fully character-level representa"
2020.coling-main.375,W19-5354,0,0.049152,"to the classifier and the output are different from the classifiers in the morphological probing tasks. Instead, the representations of an ambiguous word and its candidate translation are both fed into the classifier and then the classifier predicts whether the candidate translation is correct or not. 2.1 Data We train NMT models on the WMT15 shared task data (Bojar et al., 2015) for FI→EN to be able to compare with Cherry et al. (2018). There are about 2.1M sentence pairs in the training set after preprocessing with Moses scripts. For the WSD probing task, we use the FI–EN part of the MuCoW (Raganato et al., 2019) test set, which is a multilingual test suite for WSD in the WMT19 shared task. It has 2,117 annotated sentences. Each annotation provides the ambiguous Finnish word, the domain of the sentence, and a set of translation candidates of the ambiguous word including both correct and incorrect translations. For each ambiguous word from an annotation, we generate multiple instances that are labeled with one translation candidate and a binary value indicating whether it corresponds to the correct sense. 1,000/1,000 instances are randomly selected as the development/test sets, and the remaining 6,325"
2020.coling-main.375,P16-1162,1,0.45213,"LSTM) RNN unit (Hochreiter and Schmidhuber, 1997). The size of embeddings and hidden units is 512. We tie the source, target, and output embeddings. The beam size is 8 during inference. We employ the models that have the best perplexity on the validation set for evaluation. BLEU scores (Papineni et al., 2002) are computed by sacrebleu (Post, 2018). For CHAR models, we add separators between any two tokens including punctuation marks, and input character sequences to the model directly. The character vocabulary size is 379. For the BPE-based model, we learn a joint BPE model with 32K subwords (Sennrich et al., 2016). As Cherry et al. (2018) have shown that the depth is crucial to the success of CHAR models, we train a 4-layer CHAR model to study the effect of depth. Probing classifiers These probing classifiers are feed-forward neural networks with only one hidden layer, using ReLU non-linear activation. The size of the hidden layer is set to 512. We use the Adam learning algorithm (Kingma and Ba, 2015). The classifiers are trained using a cross-entropy loss. Each classifier is trained for 180/100 epochs in the WSD/morphological probing tasks and the one that performs best on the development set is selec"
2020.coling-main.375,W18-6304,1,0.813362,"T systems. Currently most of the studies towards understanding NMT models only take into account subword-based (e.g. BPE-based) models. Deeper character-based (CHAR) models have been shown to perform better than BPE-based models (Cherry et al., 2018). In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism. Previous studies have tried to interpret and understand NMT models by interpreting attention weights (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018; Tang et al., 2019), using gradients (He et al., 2019), applying layer-wise relevance propagation (Ding et al., 2017), probing classification tasks (Belinkov et al., 2017b; Belinkov et al., 2017a; Belinkov et al., 2020; Poliak et al., 2018; Tang et al., 2019), and more intrinsic analysis (Ghader and Monz, 2019; Voita et al., 2019). However, only Belinkov et al. (2017a; Belinkov et al. (2020) have probed character-based representations. Belinkov et al. (2017a) have only explored character-aware word-level representations, while we investigate fully character-level representations, which are al"
2020.coling-main.375,D19-1149,1,0.827874,"y most of the studies towards understanding NMT models only take into account subword-based (e.g. BPE-based) models. Deeper character-based (CHAR) models have been shown to perform better than BPE-based models (Cherry et al., 2018). In this paper, we try to investigate the working mechanism of CHAR models. We explore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism. Previous studies have tried to interpret and understand NMT models by interpreting attention weights (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018; Tang et al., 2019), using gradients (He et al., 2019), applying layer-wise relevance propagation (Ding et al., 2017), probing classification tasks (Belinkov et al., 2017b; Belinkov et al., 2017a; Belinkov et al., 2020; Poliak et al., 2018; Tang et al., 2019), and more intrinsic analysis (Ghader and Monz, 2019; Voita et al., 2019). However, only Belinkov et al. (2017a; Belinkov et al. (2020) have probed character-based representations. Belinkov et al. (2017a) have only explored character-aware word-level representations, while we investigate fully character-level representations, which are also studied in Belink"
2020.coling-main.375,D19-1448,1,0.842371,"xplore the ability of CHAR models to learn word senses and morphological inflections and the attention mechanism. Previous studies have tried to interpret and understand NMT models by interpreting attention weights (Ghader and Monz, 2017; Raganato and Tiedemann, 2018; Tang et al., 2018; Tang et al., 2019), using gradients (He et al., 2019), applying layer-wise relevance propagation (Ding et al., 2017), probing classification tasks (Belinkov et al., 2017b; Belinkov et al., 2017a; Belinkov et al., 2020; Poliak et al., 2018; Tang et al., 2019), and more intrinsic analysis (Ghader and Monz, 2019; Voita et al., 2019). However, only Belinkov et al. (2017a; Belinkov et al. (2020) have probed character-based representations. Belinkov et al. (2017a) have only explored character-aware word-level representations, while we investigate fully character-level representations, which are also studied in Belinkov et al. (2020). We apply more composition methods to explore how CHAR models learn linguistic knowledge and how attention extracts features directly from characters. Probing classification tasks (Belinkov et al., 2017a) have emerged as a popular method to interpret the internal representations from neural netw"
2020.eamt-1.53,D19-1081,1,0.842597,"erpreter would not be available for capacity reasons. The 43 languages are 24 EU official languages and 19 others, spoken between Morocco and Kazachstan. With our other user partner, alfatraining, we connect our system with an online meeting platform, alfaview®. 2.2 Other Research Topics The most visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascade"
2020.eamt-1.53,P19-1116,1,0.81532,"erpreter would not be available for capacity reasons. The 43 languages are 24 EU official languages and 19 others, spoken between Morocco and Kazachstan. With our other user partner, alfatraining, we connect our system with an online meeting platform, alfaview®. 2.2 Other Research Topics The most visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascade"
2020.eamt-1.53,W19-5355,1,0.883897,"Missing"
2020.eamt-1.53,D19-1083,1,0.761049,"st visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascaded and fully end-to-end neural spoken language translation (Pham et al., 2019; Nguyen et al., 2019; Nguyen et al., 2020) and co-organize shared tasks at WMT and IWSLT. 2.3 Automatic Minuting The last objective of our project is an automatic system for structured summaries of meetings. It is a ch"
2020.emnlp-main.415,D11-1038,1,0.750876,"all experiments we assume that English is the high-resource language and German is WikiLarge WMT19 GeoLino Wikipedia Source EnglishC EnglishC — — Target EnglishS GermanC GermanS EnglishS Size 300K 6.0M 200K 1.4M Table 2: Training data used in our experiments; monolingual corpora shown under Target; indices are shorthands for Complex and Simple language. the low-resource language. Simplification data in English is taken from WikiLarge (Zhang and Lapata, 2017), a fairly large corpus which consists of a mixture of three automatically-collated Wikipedia simplification datasets (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). EnglishGerman bilingual data is taken from the WMT19 news translation task. Complex monolingual nonparallel data uses one side of the WMT19 translation data. Simple English non-parallel data uses sentences extracted from simple Wikipedia, a simplified version of Wikipedia. Simple German nonparallel data uses sentences scraped from GEOLino (Hancke et al., 2012), a German general-interest magazine for children aged between 8–14. Test Set We evaluated our model on two German simplification datasets, each targeting different users. TextComplexityDE (Naderi et al., 2019) consists"
2020.emnlp-main.415,Q15-1021,0,0.155218,"et al., 2015; Sutskever et al., 2014). In contrast to traditional methods, which target individual aspects of the simplification task, such as sentence splitting (Carroll et al. 1999; Chandrasekar et al. 1996, inter alia) or the substitution of complex words with simpler ones (Devlin, 1999; Kaji et al., 2002), neural models have no special purpose mechanisms for ensuring how to best simplify text. They rely on representation learning to implicitly capture simplification rewrites from data, i.e., examples of complex-simple sentence pairs. While large-scale parallel datasets exist for English (Xu et al., 2015; Zhang and Lapata, 2017) and Spanish (Agrawal and Carpuat, 2019), there is a limited amount of simplification data for other languages. For example, Klaper et al. (2013) automatically aligned 7,000 complex-simple German sentences,1 and Brunato et al. (2015) released 1,000 complex-simple Italian sentences. But datadriven approaches to simplification, in particular popular neural models, require significantly more training data to achieve good performance, making these datasets better suited for testing or development purposes. Unsupervised approaches (Surya et al., 2019; Artetxe et al., 2018)"
2020.emnlp-main.415,Q16-1029,0,0.116949,"s. As we wished to do equally well with all tasks we select a minibatch from a task with a probability inversely proportional to the training loss of the task. One model was selected using the average FRE-BLEU score across both development sets. All text was preprocessed using the UDPipe tokEvaluation As there is no single agreed-upon metric for simplification (Alva-Manchego et al., 2020; Sulem et al., 2018), we evaluate model output using a combination of four automaticallygenerated scores.5 These metrics have been previously shown to correlate with human judgments of simplification quality (Xu et al., 2016) and essentially quantify: a) whether the output is similar to the gold standard reference (Target-based, T); b) whether the output is similar to the source (Sourcebased, S); and c) whether the output is simple on its own, with no regard to preserving the meaning of the original sentence (Readability-based, R). We indicate the type of each metric using superscripts. BLEUT (Papineni et al., 2002) assesses the degree to which generated simplifications agree with the gold standard references.6 I-BLEUT,S (Sun and Zhou, 2012) combines self-BLEU and BLEU to reward systems with high overlap with the"
2020.emnlp-main.415,D17-1062,1,0.905926,"d and pivot-based methods. 1 Introduction Sentence simplification aims to reduce the linguistic complexity of a text whilst retaining most of its meaning. It has been the subject of several modeling efforts in recent years due to its relevance to various applications (Siddharthan, 2014; Shardlow, 2014). Examples include the development of reading aids for individuals with autism (Evans et al., 2014), aphasia (Carroll et al., 1999), dyslexia (Rello et al., 2013), and population groups with low-literacy skills (Watanabe et al., 2009), such as children and non-native speakers. Modern approaches (Zhang and Lapata, 2017; Mallinson and Lapata, 2019; Nishihara et al., 2019; Dong et al., 2019) view the simplification task as monolingual text-to-text rewriting and employ the very successful encoder-decoder neural architecture (Bahdanau et al., 2015; Sutskever et al., 2014). In contrast to traditional methods, which target individual aspects of the simplification task, such as sentence splitting (Carroll et al. 1999; Chandrasekar et al. 1996, inter alia) or the substitution of complex words with simpler ones (Devlin, 1999; Kaji et al., 2002), neural models have no special purpose mechanisms for ensuring how to be"
2020.emnlp-main.415,N18-1063,0,0.100233,"out set to 0.1. The networks were optimized using Adam (Kingma and Ba, 2014). Multi-tasking was performed by alternating batches of different tasks. Tasks varied in dataset sizes and had different difficulties. As we wished to do equally well with all tasks we select a minibatch from a task with a probability inversely proportional to the training loss of the task. One model was selected using the average FRE-BLEU score across both development sets. All text was preprocessed using the UDPipe tokEvaluation As there is no single agreed-upon metric for simplification (Alva-Manchego et al., 2020; Sulem et al., 2018), we evaluate model output using a combination of four automaticallygenerated scores.5 These metrics have been previously shown to correlate with human judgments of simplification quality (Xu et al., 2016) and essentially quantify: a) whether the output is similar to the gold standard reference (Target-based, T); b) whether the output is similar to the source (Sourcebased, S); and c) whether the output is simple on its own, with no regard to preserving the meaning of the original sentence (Readability-based, R). We indicate the type of each metric using superscripts. BLEUT (Papineni et al., 20"
2020.emnlp-main.415,D18-1355,0,0.0272328,"tasets for training their model. Palmero Aprosio et al. (2019) explore different ways to to incorporate non-parallel simplification data to expand small scale training data, including autoencoding and backtranslation. Translation data, in the form of paraphrases, has also been incorporated into simplification models leading to significant improvements. Guo et al. (2018) use multi-task learning to augment the limited amount of simplification training data. In addition to training on complex-simple sentence pairs, their model employs paraphrases, created automatically using machine translation. Zhao et al. (2018) augment a Transformer-based simplification model with lexical rules obtained from Simple PPDB (Pavlick and Callison-Burch, 2016), a database of paraphrase rules, automatically annotated with simplicity scores. Unlike previous approaches, we do not train models to create training data, either via backtranslation or extracting paraphrases. Instead, our model is able to train directly on existing datasets, saving computation power and time. In the future, it would be interesting to explore whether additional datasets or tasks improve simplification performance. Crosslingual Generation Cross-ling"
2020.emnlp-main.415,P12-2008,0,0.0304857,"reviously shown to correlate with human judgments of simplification quality (Xu et al., 2016) and essentially quantify: a) whether the output is similar to the gold standard reference (Target-based, T); b) whether the output is similar to the source (Sourcebased, S); and c) whether the output is simple on its own, with no regard to preserving the meaning of the original sentence (Readability-based, R). We indicate the type of each metric using superscripts. BLEUT (Papineni et al., 2002) assesses the degree to which generated simplifications agree with the gold standard references.6 I-BLEUT,S (Sun and Zhou, 2012) combines self-BLEU and BLEU to reward systems with high overlap with the reference, and penalize those with high overlap to the source. Self-BLEU computes the BLEU score between the output and the source. It allows us to examine whether the models are making trivial changes to the input. Following Xu et al. (2016), we set the parameter which balances the contribution of the two metrics to α = 0.9. SARIT,S (Xu et al., 2016) is calculated using the average of three rewrite operation scores: addition, copying, and deletion. It rewards addition operations when the system’s output is not in the in"
2020.emnlp-main.415,P19-1198,0,0.255011,"atasets exist for English (Xu et al., 2015; Zhang and Lapata, 2017) and Spanish (Agrawal and Carpuat, 2019), there is a limited amount of simplification data for other languages. For example, Klaper et al. (2013) automatically aligned 7,000 complex-simple German sentences,1 and Brunato et al. (2015) released 1,000 complex-simple Italian sentences. But datadriven approaches to simplification, in particular popular neural models, require significantly more training data to achieve good performance, making these datasets better suited for testing or development purposes. Unsupervised approaches (Surya et al., 2019; Artetxe et al., 2018) which forgo the use of parallel corpora are an appealing solution to overcoming the paucity of data. However, in this paper we argue that better simplification models can be obtained by taking advantage of existing complex-simple data in a high-resource language, and bilingual data in a low-resource language (i.e., a language for which no parallel simplification corpus exists). Drawing inspiration from the success of machine translation (Firat et al., 2016b; Blackwood et al., 2018; Johnson et al., 2017), we propose a modeling framework which transfers simplification kno"
2020.emnlp-main.415,C10-1152,0,0.0534272,"ed in Table 2. For all experiments we assume that English is the high-resource language and German is WikiLarge WMT19 GeoLino Wikipedia Source EnglishC EnglishC — — Target EnglishS GermanC GermanS EnglishS Size 300K 6.0M 200K 1.4M Table 2: Training data used in our experiments; monolingual corpora shown under Target; indices are shorthands for Complex and Simple language. the low-resource language. Simplification data in English is taken from WikiLarge (Zhang and Lapata, 2017), a fairly large corpus which consists of a mixture of three automatically-collated Wikipedia simplification datasets (Zhu et al., 2010; Woodsend and Lapata, 2011; Kauchak, 2013). EnglishGerman bilingual data is taken from the WMT19 news translation task. Complex monolingual nonparallel data uses one side of the WMT19 translation data. Simple English non-parallel data uses sentences extracted from simple Wikipedia, a simplified version of Wikipedia. Simple German nonparallel data uses sentences scraped from GEOLino (Hancke et al., 2012), a German general-interest magazine for children aged between 8–14. Test Set We evaluated our model on two German simplification datasets, each targeting different users. TextComplexityDE (Nad"
2020.emnlp-main.616,N19-1314,0,0.0185138,"training (Belinkov et al., 2019). Adversarial attacks have recently been extended as an effective model analysis tool from vision to language tasks (Samanta and Mehta, 2017; Alzantot et al., 2018; Glockner et al., 2018; Zhang et al., 2019), including NMT (Cheng et al., 2018, 2019), where the focus so far has been on strategies requiring direct access to the victim model’s loss gradient or output distribution. Recent surveys suggested that state-of-the-art attacks often yield ungrammatical and meaning-destroying samples, thus diminishing their usefulness for the evaluation of model robustness (Michel et al., 2019; Morris et al., 2020). Targeted attacks on WSD abilities of translation models have so far remained unexplored. 6 Conclusion We conducted an initial investigation into leveraging data artifacts for the prediction of WSD errors in machine translation and proposed a simple adversarial attack strategy based on the presented insights. Our results show that WSD is not yet a solved problem in NMT, and while the general performance of popular model architectures is high, we can identify or create sentences where models are more likely to fail due to data biases. The effectiveness of our methods owes"
2020.emnlp-main.616,2020.findings-emnlp.341,0,0.0228425,"al., 2019). Adversarial attacks have recently been extended as an effective model analysis tool from vision to language tasks (Samanta and Mehta, 2017; Alzantot et al., 2018; Glockner et al., 2018; Zhang et al., 2019), including NMT (Cheng et al., 2018, 2019), where the focus so far has been on strategies requiring direct access to the victim model’s loss gradient or output distribution. Recent surveys suggested that state-of-the-art attacks often yield ungrammatical and meaning-destroying samples, thus diminishing their usefulness for the evaluation of model robustness (Michel et al., 2019; Morris et al., 2020). Targeted attacks on WSD abilities of translation models have so far remained unexplored. 6 Conclusion We conducted an initial investigation into leveraging data artifacts for the prediction of WSD errors in machine translation and proposed a simple adversarial attack strategy based on the presented insights. Our results show that WSD is not yet a solved problem in NMT, and while the general performance of popular model architectures is high, we can identify or create sentences where models are more likely to fail due to data biases. The effectiveness of our methods owes to neural models stru"
2020.emnlp-main.616,P16-1162,1,0.257586,"Missing"
2020.emnlp-main.616,P19-1164,0,0.0214994,"resolution capabilities of translation models (Rios et al., 2017; Liu et al., 2018). To our knowledge, no study so far has examined the interaction between training data artifacts and WSD performance in detail. Dataset artifacts, on the other hand, have previously been shown to enable models to make correct predictions based on incorrect or insufficient information (McCoy et al., 2019; Gururangan et al., 2018) by over-relying on spurious correlations present in the training data. Within NMT, models were found to exhibit gender-bias, reinforcing harmful stereotypes (Vanmassenhove et al., 2018; Stanovsky et al., 2019). As a response, strategies have been proposed for de-biasing the training data (Li and Vasconcelos, 2019; Le Bras et al., 2020), as well as for making models more robust to data biases through adversarial training (Belinkov et al., 2019). Adversarial attacks have recently been extended as an effective model analysis tool from vision to language tasks (Samanta and Mehta, 2017; Alzantot et al., 2018; Glockner et al., 2018; Zhang et al., 2019), including NMT (Cheng et al., 2018, 2019), where the focus so far has been on strategies requiring direct access to the victim model’s loss gradient or ou"
2020.emnlp-main.616,D19-1149,1,0.864039,"Missing"
2020.emnlp-main.616,P10-1023,0,0.0259456,"errors, while minimizing the models’ ability to disambiguate based on syntactic cues. English homographs are collected from web resources4 , excluding those that do not satisfy the above criteria. Refer to appendix A.2 for the full homograph list. We next compile a parallel lexicon of homograph translations, prioritizing a high coverage of all possible senses. Similar to (Raganato et al., 2019), 2 http://opus.nlpl.eu http://statmt.org/wmt19 4 http://7esl.com/homographs http://en.wikipedia.org/wiki/List_of_ English_homographs 3 we obtain sense-specific translations from crosslingual BabelNet (Navigli and Ponzetto, 2010) synsets. Since BabelNet entries vary in their granularity, we iteratively merge related synsets as long as they have at least three German translations in common or share at least one definition.5 This leaves us with multiple sense clusters of semantically related German translations per homograph. To further improve the quality of the lexicon, we manually clean and extend each homograph entry to address the noise inherent in BabelNet and its incomplete coverage.6 Appendix A.7 provides examples of the final sense clusters. In order to identify sentence contexts specific to each homograph sens"
2020.emnlp-main.616,D18-1334,0,0.0235428,"9), or to improve ambiguity resolution capabilities of translation models (Rios et al., 2017; Liu et al., 2018). To our knowledge, no study so far has examined the interaction between training data artifacts and WSD performance in detail. Dataset artifacts, on the other hand, have previously been shown to enable models to make correct predictions based on incorrect or insufficient information (McCoy et al., 2019; Gururangan et al., 2018) by over-relying on spurious correlations present in the training data. Within NMT, models were found to exhibit gender-bias, reinforcing harmful stereotypes (Vanmassenhove et al., 2018; Stanovsky et al., 2019). As a response, strategies have been proposed for de-biasing the training data (Li and Vasconcelos, 2019; Le Bras et al., 2020), as well as for making models more robust to data biases through adversarial training (Belinkov et al., 2019). Adversarial attacks have recently been extended as an effective model analysis tool from vision to language tasks (Samanta and Mehta, 2017; Alzantot et al., 2018; Glockner et al., 2018; Zhang et al., 2019), including NMT (Cheng et al., 2018, 2019), where the focus so far has been on strategies requiring direct access to the victim mo"
2020.emnlp-main.616,N19-4009,0,0.0258583,"Missing"
2020.emnlp-main.616,W18-6319,0,0.0121862,"ctors identified for the different senses of the homograph spring in the OS18 training set. Intuitively, if an NMT model disproportionately relies on simple surface-level correlations when resolving lexical ambiguity, it is more likely to make WSD errors when translating sentences that contain 5 A manual inspection found the clusters to be meaningful. The lexicon is released as part of our experimental code: http://github.com/demelin/detecting_ wsd_biases_for_nmt. 6 7636 season water source device summer winter come hot water find like back thing regime and hyper-parameter choices. SacreBLEU (Post, 2018) scores reported in Table 2 indicate that all models are reasonably competent. WMT Table 1: Examples of attractors for spring. strong attractors towards a wrong sense. To test this, we collect attractors from the extracted parallel sentences, quantifying their disambiguation bias (DB) using two metrics: Raw co-occurrence frequency (FREQ) and positive point-wise mutual information (PPMI) between attractors and homograph senses. FREQ is defined in Eqn.1, while Eqn.2 describes PPMI, with w ∈ V denoting an attractor term in the source vocabulary7 , and sc ∈ SC denoting a sense cluster in the set o"
2020.findings-emnlp.223,2020.acl-main.688,1,0.936518,"that romanization on the target side can also be effective when combined with a learned deromanization model. Introduction Neural Machine Translation (NMT) has opened up new opportunities in transfer learning from highresource to low-resource language pairs (Zoph et al., 2016; Kocmi and Bojar, 2018; Lakew et al., 2018). While transfer learning has shown great promise, the transfer between languages with different scripts brings additional challenges. For a successful transfer of the embedding layer, both the parent and the child model should use the same or a partially overlapping vocabulary (Aji et al., 2020). It is common to merge the two vocabularies by aligning identical subwords and randomly assigning the remaining subwords from the child vocabulary to positions in the parent vocabulary (Lakew et al., 2018, 2019; Kocmi and Bojar, 2020). This works well for transfer between languages that use the same script, but if the child language is written in an unseen script, most vocabulary positions are replaced by random subwords. This 2 Related Work Initial work on transfer learning for NMT has assumed that the child language is known in advance and that the parent and child model can use a shared vo"
2020.findings-emnlp.223,W19-5308,0,0.181318,"h uroman and uconv: 她到塔皓湖去了 uroman: ta dao ta hao hu qu le uconv: t¯a d`ao tˇa h`ao h´u q`u le “She went to Lake Tahoe.” character-level overlap between English and romanized Arabic, Russian and Chinese with chrF scores (Popovi´c, 2015) and find they are much higher for uroman (9.6, 18.8 and 13.3) compared to uconv (6.8, 18.1 and 7.2 respectively). 4 Romanization is not necessarily reversible with simple rules due to information loss. Therefore, previous work on romanized machine translation has focused on source-side romanization only (Du and Way, 2017; Wang et al., 2018; Aqlan et al., 2019; Briakou and Carpuat, 2019; Gheini and May, 2019). We argue that romanization can also be applied on the target side, followed by an additional deromanization step. This step can be performed by a character-based Transformer (Vaswani et al., 2017) that takes data romanized with uroman or uconv as input and is trained to map it back to the original script. We provide more details on our deromanization systems in Appendix A.2. 5 5.1 2 https://github.com/isi-nlp/uroman https://linux.die.net/man/1/uconv Experimental Setup Data We use OPUS-100 (Zhang et al., 2020)3 , an English-centric dataset that includes parallel data fo"
2020.findings-emnlp.223,P18-4003,0,0.0711276,"romanization. Based on experiments on a diverse set of low-resource languages, we show that romanization is helpful for model transfer to related languages with different scripts. 3 Romanization Romanization describes the process of mapping characters in various scripts to Latin script. This mapping is not always reversible. The goal is to approximate the pronunciation of the text in the original script. However, depending on the romanization tool, more or less information encoded in the original script is lost. We compare two tools for mapping our translation input to Latin script: uroman1 (Hermjakob et al., 2018) is a tool for universal romanization that can romanize almost all character sets. It is unidirectional; mappings from Latin script back to other scripts are not available. uconv2 is a command-line tool similar to iconv that can be used for transliteration. It preserves more information from the original script, which is expressed with diacritics. uconv is bi-directional for a limited number of script pairs. Below is an example of the same Chinese sentence romanized with uroman and uconv: 她到塔皓湖去了 uroman: ta dao ta hao hu qu le uconv: t¯a d`ao tˇa h`ao h´u q`u le “She went to Lake Tahoe.” chara"
2020.findings-emnlp.223,I17-2050,0,0.0291886,"mmon to merge the two vocabularies by aligning identical subwords and randomly assigning the remaining subwords from the child vocabulary to positions in the parent vocabulary (Lakew et al., 2018, 2019; Kocmi and Bojar, 2020). This works well for transfer between languages that use the same script, but if the child language is written in an unseen script, most vocabulary positions are replaced by random subwords. This 2 Related Work Initial work on transfer learning for NMT has assumed that the child language is known in advance and that the parent and child model can use a shared vocabulary (Nguyen and Chiang, 2017; Kocmi and Bojar, 2018). Lakew et al. (2018) argue that this is not feasible in most real-life scenarios and propose using a dynamic vocabulary. Most studies have since opted to replace unused parts of the parent vocabulary with unseen subwords from the child vocabulary (Lakew et al., 2019; Kocmi and Bojar, 2020); others use various methods to align embedding spaces (Gu et al., 2018; Kim et al., 2019). Recently, Aji et al. (2020) showed that transfer of the embedding layer is only beneficial if there is an overlap between the parent and child vocabulary such that embeddings for identical subw"
2020.findings-emnlp.223,P19-1120,0,0.0355698,"Missing"
2020.findings-emnlp.223,P02-1040,0,0.105883,"Missing"
2020.findings-emnlp.223,W15-3049,0,0.0840122,"Missing"
2020.findings-emnlp.223,W18-6319,0,0.0153215,"n, other languages in this set they are closely related to (considering lexical similarity) and number of X↔EN sentence pairs. (*) means artificial low-resource settings were created. (c) Written in Latin script but closely related to a pretraining language in non-Latin script, e.g. Maltese is related to Arabic and written in Latin script. Our selection of low-resource languages covers a wide range of language families and training data sizes. Table 1 gives an overview of the selected languages. 5.2 Model Descriptions We use nematus4 (Sennrich et al., 2017) to train our models and SacreBLEU5 (Post, 2018) to evaluate them. We compute statistical significance with paired bootstrap resampling (Koehn, 2004) using a significance level of 0.05 (sampling 1,000 times with replacement from our 2,000 test sentences). Our subword vocabularies are computed with byte pair encoding (Sennrich et al., 2016) using the SentencePiece implementation (Kudo and Richardson, 2018). We use a character coverage of 0.9995 to ensure the resulting models do not consist of mostly single characters. Bilingual Baselines: We follow the recommended setup for low-resource translation in Sennrich and Zhang (2019) to train our b"
2020.findings-emnlp.223,E17-2025,0,0.0543891,"Missing"
2020.findings-emnlp.223,2020.acl-main.148,1,0.832399,"and Way, 2017; Wang et al., 2018; Aqlan et al., 2019; Briakou and Carpuat, 2019; Gheini and May, 2019). We argue that romanization can also be applied on the target side, followed by an additional deromanization step. This step can be performed by a character-based Transformer (Vaswani et al., 2017) that takes data romanized with uroman or uconv as input and is trained to map it back to the original script. We provide more details on our deromanization systems in Appendix A.2. 5 5.1 2 https://github.com/isi-nlp/uroman https://linux.die.net/man/1/uconv Experimental Setup Data We use OPUS-100 (Zhang et al., 2020)3 , an English-centric dataset that includes parallel data for 100 languages. It provides up to 1 million sentence pairs for every X-EN language pair as well as 2,000 sentence pairs for development and testing each. There is no overlap between any of the data splits across any of the languages, i.e. every English sentence occurs only once. We pretrain our multilingual models on 5 highresource languages that cover a range of different scripts {AR, DE, FR, RU, ZH} ↔ EN. For our transfer learning experiments, we choose 7 additional languages that are either: (a) Not closely related to any of the"
2020.findings-emnlp.223,D16-1163,0,0.0200012,"showing that this can be a successful strategy when coupled with a simple deromanization model. 1 - We show that romanized MT is not generally optimal, but can improve transfer between related languages that use different scripts. - We study information loss from different romanization tools and its effect on MT quality. - We demonstrate that romanization on the target side can also be effective when combined with a learned deromanization model. Introduction Neural Machine Translation (NMT) has opened up new opportunities in transfer learning from highresource to low-resource language pairs (Zoph et al., 2016; Kocmi and Bojar, 2018; Lakew et al., 2018). While transfer learning has shown great promise, the transfer between languages with different scripts brings additional challenges. For a successful transfer of the embedding layer, both the parent and the child model should use the same or a partially overlapping vocabulary (Aji et al., 2020). It is common to merge the two vocabularies by aligning identical subwords and randomly assigning the remaining subwords from the child vocabulary to positions in the parent vocabulary (Lakew et al., 2018, 2019; Kocmi and Bojar, 2020). This works well for tr"
2020.findings-emnlp.223,E17-3017,1,0.898262,"Missing"
2020.findings-emnlp.223,P16-1162,1,0.274995,". Maltese is related to Arabic and written in Latin script. Our selection of low-resource languages covers a wide range of language families and training data sizes. Table 1 gives an overview of the selected languages. 5.2 Model Descriptions We use nematus4 (Sennrich et al., 2017) to train our models and SacreBLEU5 (Post, 2018) to evaluate them. We compute statistical significance with paired bootstrap resampling (Koehn, 2004) using a significance level of 0.05 (sampling 1,000 times with replacement from our 2,000 test sentences). Our subword vocabularies are computed with byte pair encoding (Sennrich et al., 2016) using the SentencePiece implementation (Kudo and Richardson, 2018). We use a character coverage of 0.9995 to ensure the resulting models do not consist of mostly single characters. Bilingual Baselines: We follow the recommended setup for low-resource translation in Sennrich and Zhang (2019) to train our bilingual baselines for the low-resource pairs (original script). For our bilingual low-resource models, we use language-specific vocabularies of size 2,000. 4 https://github.com/EdinburghNLP/ nematus 5 BLEU+case.mixed+lang.XX-XX+numrefs.1 +smooth.exp+tok.13a+version.1.4.2 Pretrained multiling"
2020.findings-emnlp.223,P19-1021,1,0.822099,"n our models and SacreBLEU5 (Post, 2018) to evaluate them. We compute statistical significance with paired bootstrap resampling (Koehn, 2004) using a significance level of 0.05 (sampling 1,000 times with replacement from our 2,000 test sentences). Our subword vocabularies are computed with byte pair encoding (Sennrich et al., 2016) using the SentencePiece implementation (Kudo and Richardson, 2018). We use a character coverage of 0.9995 to ensure the resulting models do not consist of mostly single characters. Bilingual Baselines: We follow the recommended setup for low-resource translation in Sennrich and Zhang (2019) to train our bilingual baselines for the low-resource pairs (original script). For our bilingual low-resource models, we use language-specific vocabularies of size 2,000. 4 https://github.com/EdinburghNLP/ nematus 5 BLEU+case.mixed+lang.XX-XX+numrefs.1 +smooth.exp+tok.13a+version.1.4.2 Pretrained multilingual models: We pretrain three multilingual standard Transformer Base machine translation models (Vaswani et al., 2017): One keeps the original, non-Latin script for Arabic, Russian and Chinese (orig). The others (uroman and uconv) apply the respective romanization to these parent languages."
2020.findings-emnlp.223,Y18-3017,0,0.0157283,"the same Chinese sentence romanized with uroman and uconv: 她到塔皓湖去了 uroman: ta dao ta hao hu qu le uconv: t¯a d`ao tˇa h`ao h´u q`u le “She went to Lake Tahoe.” character-level overlap between English and romanized Arabic, Russian and Chinese with chrF scores (Popovi´c, 2015) and find they are much higher for uroman (9.6, 18.8 and 13.3) compared to uconv (6.8, 18.1 and 7.2 respectively). 4 Romanization is not necessarily reversible with simple rules due to information loss. Therefore, previous work on romanized machine translation has focused on source-side romanization only (Du and Way, 2017; Wang et al., 2018; Aqlan et al., 2019; Briakou and Carpuat, 2019; Gheini and May, 2019). We argue that romanization can also be applied on the target side, followed by an additional deromanization step. This step can be performed by a character-based Transformer (Vaswani et al., 2017) that takes data romanized with uroman or uconv as input and is trained to map it back to the original script. We provide more details on our deromanization systems in Appendix A.2. 5 5.1 2 https://github.com/isi-nlp/uroman https://linux.die.net/man/1/uconv Experimental Setup Data We use OPUS-100 (Zhang et al., 2020)3 , an English"
2020.findings-emnlp.230,N18-1008,0,0.0196101,"ng studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018). This led researchers to explore solutions ranging from efficient neural architecture design (Karita et al.,"
2020.findings-emnlp.230,N19-1006,0,0.0749518,"mproves model convergence; feature selection makes training more stable. Compared to other models, the curve of ST with AFS is much smoother, suggesting its better regularization effect. We then investigate the effect of training data size, and show the results in Figure 7. Overall, we do not observe higher data efficiency by feature selection on low-resource settings. But instead, our results suggest that feature selection delivers larger performance improvement when more training data is available. With respect to data efficiency, ASR pretraining seems to be more important (Figure 7, left) (Bansal et al., 2019; Stoian et al., 2020). Com2538 10 17.5 # Selected Features 20 BLEU BLEU 15.0 12.5 10.0 7.5 18 16 ST + ASR-PT ST + Fixed Rate ST+AFSt ST + AFSt,f 14 ST ST + ASR-PT 5.0 2.5 50000 12 50000 6 4 2 100000 150000 200000 # Training Samples MuST-C En-De. We split the original training data into non-overlapped five subsets, and train different models with accumulated subsets. Results are reported on the test set. Note that we perform ASR pretraining on the original dataset. λ = 0.5, k = 6. 2500 Frequency 6000 4000 2000 0.5 1.0 1.5 Attention Weight 2.0 0.6 AFSt AFSt,f Fixed Rate 1.5 2.0 0 10 20 30 40 50"
2020.findings-emnlp.230,P19-1284,1,0.874511,"Missing"
2020.findings-emnlp.230,D19-5304,0,0.0916558,"in tokenization and letter case. To ease future cross-paper comparison, we provide SacreBLEU (Post, 2018)4 for our models. 5 Related Work Speech Translation Pioneering studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms casc"
2020.findings-emnlp.230,P18-1163,0,0.0603518,"LEU (Post, 2018)4 for our models. 5 Related Work Speech Translation Pioneering studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018). This led researchers"
2020.findings-emnlp.230,D19-1223,0,0.0367555,"Missing"
2020.findings-emnlp.230,N19-1202,0,0.284852,"Missing"
2020.findings-emnlp.230,N16-1109,0,0.62118,"−0.4 play P L is EY1 IH1 Z not NAA1 just JH IH0 S T CH child ’s AY1 L DT S games G EY1 M Z Figure 1: Example illustrating our motivation. We plot the amplitude and frequency spectrum of an audio segment (top), paired with its time-aligned words and phonemes (bottom). Information inside an audio stream is not uniformly distributed. We propose to dynamically capture speech features corresponding to informative signals (red rectangles) to improve ST. Introduction End-to-end (E2E) speech translation (ST), a paradigm that directly maps audio to a foreign text, has been gaining popularity recently (Duong et al., 2016; B´erard et al., 2016; Bansal et al., 2018; Di Gangi et al., 2019; Wang et al., 2019). Based on the attentional encoder-decoder framework (Bahdanau et al., 2015), it optimizes model parameters under direct translation supervision. This end-toend paradigm avoids the problem of error propagation that is inherent in cascade models where an automatic speech recognition (ASR) model and 1 We release our source code at https://github. com/bzhangGo/zero. a machine translation (MT) model are chained together. Nonetheless, previous work still reports that E2E ST delivers inferior performance compared t"
2020.findings-emnlp.230,L18-1001,0,0.276493,"hat correspond to git > 0, and pass them similarly as done with word embeddings to the ST encoder. We employ sinusoidal positional encoding to distinguish features at different positions. Except for the input to the ST encoder, our E2E ST follows the standard encoder-decoder translation model (MST in Eq. 10) and is optimized with LMLE alone as in Eq. 9. Intuitively, AFS bridges the gap between ASR output and MT input by selecting transcriptaligned speech features. 4 Experiments Datasets and Preprocessing We experiment with two benchmarks: the Augmented LibriSpeech dataset (LibriSpeech En-Fr) (Kocabiyikoglu et al., 2018) and the multilingual MuST-C dataset (MuSTC) (Di Gangi et al., 2019). LibriSpeech En-Fr is 3 Other candidate gating models, like linear mapping upon mean-pooled encoder outputs, delivered worse performance in our preliminary experiments. collected by aligning e-books in French with English utterances of LibriSpeech, further augmented with French translations offered by Google Translate. We use the 100 hours clean training set for training, including 47K utterances to train ASR models and double the size for ST models after concatenation with the Google translations. We report results on the te"
2020.findings-emnlp.230,P07-2045,0,0.0118427,"mon test set, whose size ranges from 2502 (Es) to 2641 (De) utterances. For all datasets, we extract 40-dimensional logMel filterbanks with a step size of 10ms and window size of 25ms as the acoustic features. We expand these features with their first and second-order derivatives, and stabilize them using mean subtraction and variance normalization. We stack the features corresponding to three consecutive frames without overlapping to the left, resulting in the final 360-dimensional acoustic input. For transcriptions and translations, we tokenize and truecase all the text using Moses scripts (Koehn et al., 2007). We train subword models (Sennrich et al., 2016) on each dataset with a joint vocabulary size of 16K to handle rare words, and share the model for ASR, MT and ST. We train all models without removing punctuation. Model Settings and Baselines We adopt the Transformer architecture (Vaswani et al., 2017) for all tasks, including MASR (Eq. 6), MAFS (Eq. 8) and MST (Eq. 10). The encoder and decoder consist of 6 identical layers, each including a self-attention sublayer, a cross-attention sublayer (decoder alone) and a feedforward sublayer. We employ the base setting for experiments: hidden size d"
2020.findings-emnlp.230,P02-1040,0,0.106095,"Missing"
2020.findings-emnlp.230,W18-6319,0,0.0501918,"Missing"
2020.findings-emnlp.230,P16-1162,1,0.24597,"to 2641 (De) utterances. For all datasets, we extract 40-dimensional logMel filterbanks with a step size of 10ms and window size of 25ms as the acoustic features. We expand these features with their first and second-order derivatives, and stabilize them using mean subtraction and variance normalization. We stack the features corresponding to three consecutive frames without overlapping to the left, resulting in the final 360-dimensional acoustic input. For transcriptions and translations, we tokenize and truecase all the text using Moses scripts (Koehn et al., 2007). We train subword models (Sennrich et al., 2016) on each dataset with a joint vocabulary size of 16K to handle rare words, and share the model for ASR, MT and ST. We train all models without removing punctuation. Model Settings and Baselines We adopt the Transformer architecture (Vaswani et al., 2017) for all tasks, including MASR (Eq. 6), MAFS (Eq. 8) and MST (Eq. 10). The encoder and decoder consist of 6 identical layers, each including a self-attention sublayer, a cross-attention sublayer (decoder alone) and a feedforward sublayer. We employ the base setting for experiments: hidden size d = 512, attention head 8 and feedforward size 2048"
2020.findings-emnlp.230,Q19-1020,0,0.0126597,"f separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018). This led researchers to explore solutions ranging from efficient neural architecture design (Karita et al., 2019; Di Gangi et al.,"
2020.findings-emnlp.230,E14-1065,0,0.0202094,"ison, we provide SacreBLEU (Post, 2018)4 for our models. 5 Related Work Speech Translation Pioneering studies on ST used a cascade of separately trained ASR and MT systems (Ney, 1999). Despite its simplicity, this approach inevitably suffers from mistakes made by ASR models, and is error prone. Research in this direction often focuses on strategies capable of mitigating the mismatch between ASR output and MT input, such as representing ASR outputs with lattices (Saleem et al., 2004; Mathias and Byrne, 2006; Zhang et al., 2019a; Beck et al., 2019), injecting synthetic ASR errors for robust MT (Tsvetkov et al., 2014; Cheng et al., 2018) and differentiable cascade modeling (Kano et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019). In contrast to cascading, another option is to perform direct speech-to-text translation. Duong et al. (2016) and B´erard et al. (2016) employ the attentional encoder-decoder model (Bahdanau et al., 2015) for E2E ST without accessing any intermediate transcriptions. E2E ST opens the way to bridging the modality gap directly, but it is data-hungry, sample-inefficient and often underperforms cascade models especially in low-resource settings (Bansal et al., 2018)."
2020.findings-emnlp.230,2020.acl-main.344,0,0.240923,"ription. To improve speech encoding, we apply logarithmic penalty on attention to enforce short-range dependency (Di Gangi et al., 2019) and use trainable positional embedding with a maximum length of 2048. Apart from LMLE , we augment the training objective with the connectionist temporal classification (Graves et al., 2006, CTC) loss LCTC as in Eq. 5. Note η = 1 − γ. The CTC loss is applied to the encoder outputs, guiding them to align with their corresponding transcription (sub)words and improving the encoder’s robustness (Karita et al., 2019). Following previous work (Karita et al., 2019; Wang et al., 2020), we set γ to 0.3. (7)  3. Train ST model with pretrained and frozen ASR and AFS submodules until convergence:  ing encoder and decoder respectively. F (·) denotes the AFS approach, and F E means freezing the ASR encoder and the AFS module during training. Note that our framework puts no constraint on the architecture of the encoder and decoder in any task, although we adopt the multi-head dot-product attention network (Vaswani et al., 2017) for our experiments. Note that our model only requires pair-wise training corpora, (X, Y ) for ASR, and (X, Z) for ST. Lt0 (X) = n X i=1 1 − p(git = 0|"
2020.findings-emnlp.230,2021.findings-acl.255,1,0.952716,"Missing"
2020.findings-emnlp.230,P19-1649,0,0.290045,"zero. a machine translation (MT) model are chained together. Nonetheless, previous work still reports that E2E ST delivers inferior performance compared to cascade methods (Niehues et al., 2019). We study one reason for the difficulty of training E2E ST models, namely the uneven spread of information in the speech signal, as visualized in Figure 1, and the consequent difficulty of extracting informative features. Features corresponding to uninformative signals, such as pauses or noise, increase the input length and bring in unmanageable noise for ST. This increases the difficulty of learning (Zhang et al., 2019b; Na et al., 2019) and reduces translation performance. In this paper, we propose adaptive feature selection (AFS) for ST to explicitly eliminate uninformative features. Figure 2 shows the overall architecture. We employ a pretrained ASR encoder to induce contextual speech features, followed by an ST encoder bridging the gap between speech and translation modalities. AFS is inserted in-between them to select a subset of features for ST encoding (see red rectangles in Figure 1). To ensure that the selected features are well-aligned to transcriptions, we pretrain AFS on ASR. AFS estimates the i"
2020.iwltp-1.7,2012.eamt-1.60,0,0.030274,"(Ha et al., 2016; Ha et al., 2017; Johnson et al., 2017) where a single system is able to translate from and to multiple languages. This approach has many advantages: • It leverages the large availability of multi-way, multilingual corpora in European languages such as the corpus of European Parliament documents and speeches’ transcription (Europarl) (Koehn, 2005), the collection of legislative texts of the European Union (JRCAcquis) (Steinberger et al., 2006) or the texts extracted from the document of European Constitution (EUconst) as well as the WIT3 corpus extracted from TED talks (TED) (Cettolo et al., 2012). • It uses the multilingual information to help improve the translation of the language pairs which are considered as low-resource languages in some domains. Our research has shown that our multilingual translation system maintains parity with the translation quality of systems trained on individual language pairs on the same small amount of data. • In practice, having a small number of multilingual systems to cover all language pairs significantly reduces the development and deployment efforts compared with having one system for each pair. 5. While each of the components (ASR, punctuation, M"
2020.iwltp-1.7,2015.iwslt-papers.8,1,0.871161,"Missing"
2020.iwltp-1.7,Q17-1024,0,0.0913848,"Missing"
2020.iwltp-1.7,2005.mtsummit-papers.11,0,0.143289,"f that sentence is displayed and never changed again by the update mechanism, to under 5 seconds. Machine Translation System With the ultimate goal of featuring a translation system for all EUROSAI languages, we opt for the multilingual approach (Ha et al., 2016; Ha et al., 2017; Johnson et al., 2017) where a single system is able to translate from and to multiple languages. This approach has many advantages: • It leverages the large availability of multi-way, multilingual corpora in European languages such as the corpus of European Parliament documents and speeches’ transcription (Europarl) (Koehn, 2005), the collection of legislative texts of the European Union (JRCAcquis) (Steinberger et al., 2006) or the texts extracted from the document of European Constitution (EUconst) as well as the WIT3 corpus extracted from TED talks (TED) (Cettolo et al., 2012). • It uses the multilingual information to help improve the translation of the language pairs which are considered as low-resource languages in some domains. Our research has shown that our multilingual translation system maintains parity with the translation quality of systems trained on individual language pairs on the same small amount of"
2020.iwltp-1.7,W19-5337,1,0.607584,"al conferences and remote conferencing) is described in the respective sections below. Our multilingual systems are based on the neural sequenceto-sequence with attention framework (Bahdanau et al., 2014) and shares the internal representation across languages (Pham et al., 2017). At present, we have one manyto-many Transformer model (Vaswani et al., 2017) providing translation between all pairings of 36 languages, along with several specialized models focused on subsets of languages, in particular the project’s primary languages of English, Czech, and German, see i.a. (Popel and Bojar, 2018; Popel et al., 2019). The resulting multilingual models after training can be used immediately in deployment or can go through a language adaptation step. This language adaptation is simply continuing training the multilingual model on the data of a specific language pair for a few epochs in order to improve the individual translation performance. While we need to do this language adaptation for every single language pair in our system, it is a trivial job since we could automate the process with the same settings and it takes only a little of time and computing resources to reach decent performances. 4.2. Practi"
2020.iwltp-1.7,steinberger-etal-2006-jrc,0,0.163597,"Missing"
2020.wmt-1.62,P19-1122,0,0.296453,"great speedup, albeit at a high cost to generation quality. Follow-up studies often seek solutions to recover the performance (Libovick´y and Helcl, 2018; Guo et al., 2019; Shao et al., 2020; Ghazvininejad et al., 2020; Ran et al., 2020), but also reveal the trade-off between the quality and speed in terms of autoregressiveness. This motivates researchers to discover the optimal balance by resorting to semi-autoregressive modeling (Wang et al., 2018; Stern et al., 2018), iterative refinement (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009) and recently gained popularity in neural machine translation (Liu et al., 3"
2020.wmt-1.62,D09-1117,0,0.0391274,"n et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009) and recently gained popularity in neural machine translation (Liu et al., 3 Autoregressive Transformer Transformer (Vaswani et al., 2017), the state-of-theart neural sequence generation model, follows the autoregressive factorization as in Eq. 1. To handle the dependency of target word yt on previous target words y<t , Transformer relies on a masked selfattention network in the decoder: ! l Kl T Q √ + M Vl (3) ATT(Yl , M) = f d where Ql , Kl , Vl = Wql Yl , Wkl Yl , Wvl Yl ∈ Rn×d , f (·) denotes softmax operation, d is model dimension and l is layer depth. Wq , Wk , Wv ∈ Rd×d are trainable pa"
2020.wmt-1.62,D19-1633,0,0.059964,"tion where target words are predicted independently, leading to great speedup, albeit at a high cost to generation quality. Follow-up studies often seek solutions to recover the performance (Libovick´y and Helcl, 2018; Guo et al., 2019; Shao et al., 2020; Ghazvininejad et al., 2020; Ran et al., 2020), but also reveal the trade-off between the quality and speed in terms of autoregressiveness. This motivates researchers to discover the optimal balance by resorting to semi-autoregressive modeling (Wang et al., 2018; Stern et al., 2018), iterative refinement (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009) and recently gained"
2020.wmt-1.62,Q19-1042,0,0.332934,"2020), but also reveal the trade-off between the quality and speed in terms of autoregressiveness. This motivates researchers to discover the optimal balance by resorting to semi-autoregressive modeling (Wang et al., 2018; Stern et al., 2018), iterative refinement (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009) and recently gained popularity in neural machine translation (Liu et al., 3 Autoregressive Transformer Transformer (Vaswani et al., 2017), the state-of-theart neural sequence generation model, follows the autoregressive factorization as in Eq. 1. To handle the dependency of target word yt on previous t"
2020.wmt-1.62,D19-5601,0,0.018062,"In contrast to Zhou et al. (2019), we only make minimal changes to the standard Transformer decoder, which benefits efficiency during training and inference, and makes our method easy to implement. We also find improvements in both decoding speed and translation quality compared to (Wang et al., 2018; Zhou et al., 2019). Related Work Efforts on fast sequence generation come along with the rapid development of encoder-decoder models (Vaswani et al., 2017). A straightforward way is to reduce the amount of computation. Methods in this category range from teacherstudent model (Kim and Rush, 2016; Hayashi et al., 2019), constrained softmax prediction (Hu et al., 2015), beam search cube pruning (Zhang et al., 2018c), float-point quantization (Wu et al., 2016; Bhandare et al., 2019), model pruning (See et al., 2016), to simplified decoder architectures, such as lightweight recurrent models (Zhang et al., 2018b; Zhang and Sennrich, 2019; Kim et al., 2019), average attention network (Zhang et al., 2018a), merged attention network (Zhang et al., 2019a), dynamic convolution (Wu et al., 2019), and hybrid attentions (Shazeer, 2019; Wang et al., 2019), .etc. Nonetheless, the above methods still suffer from the infer"
2020.wmt-1.62,2015.mtsummit-papers.23,0,0.0459221,"changes to the standard Transformer decoder, which benefits efficiency during training and inference, and makes our method easy to implement. We also find improvements in both decoding speed and translation quality compared to (Wang et al., 2018; Zhou et al., 2019). Related Work Efforts on fast sequence generation come along with the rapid development of encoder-decoder models (Vaswani et al., 2017). A straightforward way is to reduce the amount of computation. Methods in this category range from teacherstudent model (Kim and Rush, 2016; Hayashi et al., 2019), constrained softmax prediction (Hu et al., 2015), beam search cube pruning (Zhang et al., 2018c), float-point quantization (Wu et al., 2016; Bhandare et al., 2019), model pruning (See et al., 2016), to simplified decoder architectures, such as lightweight recurrent models (Zhang et al., 2018b; Zhang and Sennrich, 2019; Kim et al., 2019), average attention network (Zhang et al., 2018a), merged attention network (Zhang et al., 2019a), dynamic convolution (Wu et al., 2019), and hybrid attentions (Shazeer, 2019; Wang et al., 2019), .etc. Nonetheless, the above methods still suffer from the inference bottleneck caused by the sequential nature of"
2020.wmt-1.62,P02-1040,0,0.106272,"Missing"
2020.wmt-1.62,D16-1139,0,0.311654,"ropose a modified beam search algorithm. 504 We extensively experiment on five machine translation tasks and two document summarization tasks, with an in-depth analysis studying the impact of batch size, beam size and sequence length on the decoding speed. We close our analysis by examining the capacity of our model in handling long-range dependencies. On these tasks, IBDecoder yields ∼2× speedup against Transformer at inference, and reaches 4×–11× after pairing it with SA. Still, the overall generation quality is comparable. When we pair our method with sequence-level knowledge distillation (Kim and Rush, 2016), we outperform a Transformer baseline on 6 out of 7 tasks. Our contributions are summarized below: • We propose IBDecoder, following a bidirectional factorization of the conditional probability, for fast sequence generation. IBDecoder retains the training efficiency and is easy to implement. • We extend IBDecoder to enable multi-word simultaneous generation by investigating integration with IMDecoder and SA. Results show that IBDecoder + SA performs better than IMDecoder. • We propose a modified beam search algorithm to support step-wise parallel generation. • On several sequence generation b"
2020.wmt-1.62,W18-6319,0,0.0117233,"beam search algorithm as in Algorithm 1. For each partial hyExperiments Setup We test our model on machine translation (MT) and document summarization. We train MT models on five different language pairs: WMT14 English-German (En-De, Bojar et al., 2014), WMT14 English-French (En-Fr, Bojar et al., 2014), WMT16 Romanian-English (Ro-En, Bojar et al., 2016), WMT18 English-Russian (EnRu, Bojar et al., 2018) and WAT17 Small-NMT English-Japanese (En-Ja, Nakazawa et al., 2017). Translation quality is measured by BLEU (Papineni et al., 2002), and we report detokenized BLEU using the toolkit sacreBLEU (Post, 2018)4 except for En-Ja. Following Gu et al. (2019b), we segment Japanese text with KyTea5 and compute tokenized BLEU. We train document summarization models on two benchmark datasets: the non-anonymized version of the CNN/Daily Mail dataset (CDMail, Hermann et al., 2015) and the Annotated English Gigaword (Gigaword, Rush et al., 2015). We evaluate the summarization quality using ROUGEL (Lin, 2004). We provide details of data preprocessing and model settings in Appendix A. We perform thorough analysis of our model on WMT14 En-De. We also report results improved by knowledge distillation (KD, Kim an"
2020.wmt-1.62,2020.acl-main.277,0,0.0288259,"Zhang et al., 2019a), dynamic convolution (Wu et al., 2019), and hybrid attentions (Shazeer, 2019; Wang et al., 2019), .etc. Nonetheless, the above methods still suffer from the inference bottleneck caused by the sequential nature of autoregressive models. Instead, Gu et al. (2018) propose non-autoregressive generation where target words are predicted independently, leading to great speedup, albeit at a high cost to generation quality. Follow-up studies often seek solutions to recover the performance (Libovick´y and Helcl, 2018; Guo et al., 2019; Shao et al., 2020; Ghazvininejad et al., 2020; Ran et al., 2020), but also reveal the trade-off between the quality and speed in terms of autoregressiveness. This motivates researchers to discover the optimal balance by resorting to semi-autoregressive modeling (Wang et al., 2018; Stern et al., 2018), iterative refinement (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et"
2020.wmt-1.62,D18-1149,0,0.019092,"018) propose non-autoregressive generation where target words are predicted independently, leading to great speedup, albeit at a high cost to generation quality. Follow-up studies often seek solutions to recover the performance (Libovick´y and Helcl, 2018; Guo et al., 2019; Shao et al., 2020; Ghazvininejad et al., 2020; Ran et al., 2020), but also reveal the trade-off between the quality and speed in terms of autoregressiveness. This motivates researchers to discover the optimal balance by resorting to semi-autoregressive modeling (Wang et al., 2018; Stern et al., 2018), iterative refinement (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita,"
2020.wmt-1.62,D15-1044,0,0.0498232,"sh (Ro-En, Bojar et al., 2016), WMT18 English-Russian (EnRu, Bojar et al., 2018) and WAT17 Small-NMT English-Japanese (En-Ja, Nakazawa et al., 2017). Translation quality is measured by BLEU (Papineni et al., 2002), and we report detokenized BLEU using the toolkit sacreBLEU (Post, 2018)4 except for En-Ja. Following Gu et al. (2019b), we segment Japanese text with KyTea5 and compute tokenized BLEU. We train document summarization models on two benchmark datasets: the non-anonymized version of the CNN/Daily Mail dataset (CDMail, Hermann et al., 2015) and the Annotated English Gigaword (Gigaword, Rush et al., 2015). We evaluate the summarization quality using ROUGEL (Lin, 2004). We provide details of data preprocessing and model settings in Appendix A. We perform thorough analysis of our model on WMT14 En-De. We also report results improved by knowledge distillation (KD, Kim and Rush, 2016). 507 4 5 Signature BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.3 http://www.phontron.com/kytea/ ID Model B h c BLEU↑ +KD↑ Latency↓ Speedup↑ Train↑ 1 Transformer 4 1 1 1 26.9 26.0 27.3 26.8 387 294 1.00× 1.32× 1.00× 2 IBDecoder 4 1 2 1 26.2 25.0 27.1 26.8 204 166 1.90× 2.33× 0.98× 3 2 + SA 4 1 2 2 23.0 21.7 26.3 26.0 117 89"
2020.wmt-1.62,D18-1336,0,0.0462902,"Missing"
2020.wmt-1.62,K16-1029,0,0.0688829,"Missing"
2020.wmt-1.62,W04-1013,0,0.0540656,"., 2018) and WAT17 Small-NMT English-Japanese (En-Ja, Nakazawa et al., 2017). Translation quality is measured by BLEU (Papineni et al., 2002), and we report detokenized BLEU using the toolkit sacreBLEU (Post, 2018)4 except for En-Ja. Following Gu et al. (2019b), we segment Japanese text with KyTea5 and compute tokenized BLEU. We train document summarization models on two benchmark datasets: the non-anonymized version of the CNN/Daily Mail dataset (CDMail, Hermann et al., 2015) and the Annotated English Gigaword (Gigaword, Rush et al., 2015). We evaluate the summarization quality using ROUGEL (Lin, 2004). We provide details of data preprocessing and model settings in Appendix A. We perform thorough analysis of our model on WMT14 En-De. We also report results improved by knowledge distillation (KD, Kim and Rush, 2016). 507 4 5 Signature BLEU+c.mixed+#.1+s.exp+tok.13a+v.1.4.3 http://www.phontron.com/kytea/ ID Model B h c BLEU↑ +KD↑ Latency↓ Speedup↑ Train↑ 1 Transformer 4 1 1 1 26.9 26.0 27.3 26.8 387 294 1.00× 1.32× 1.00× 2 IBDecoder 4 1 2 1 26.2 25.0 27.1 26.8 204 166 1.90× 2.33× 0.98× 3 2 + SA 4 1 2 2 23.0 21.7 26.3 26.0 117 89 3.31× 4.35× 0.98× 4 IMDecoder 4 1 4 1 21.5 19.7 24.6 24.1 102 85"
2020.wmt-1.62,E17-2060,1,0.847341,"erformance drop w/ and w/o KD. Further analysis reveals that, as the dependency between predicted target words weakens, our model suffers from more serious over-translation issue, yielding larger OTEM (Yang et al., 2018). Although n-gram deduplication slightly improves quality8 , it does not explain the whole performance drop, echoing with Wang et al. (2018). We recommend using c = 2 for a good balance. In addition, the reduction of OTEM by KD in Figure 4 partially clarifies its improvement on quality. Analysis on Long-range Dependency We adopt the subject-verb agreement task from Lingeval97 (Sennrich, 2017) for analysis. We can see from the results in Figure 5 that IBDecoder 8 we only applied deduplication for results in Figure 4. Comparison to Previous Work Results in Table 5 show that our model outperforms SynST (Akoury et al., 2019) in quality, and slightly surpasses the Levenshtein Transformer (Gu et al., 2019b) in speed. Particularly, our model (27.50† /2.33×) surpasses SAT (Wang et al., 2018) (26.09† /2.07×) and SBSG (Zhou et al., 2019) (27.22† /1.61×) in terms of both quality and speed. Our model doesn’t heavily rely on extra linguistic knowledge (Akoury et al., 2019), neither requires co"
2020.wmt-1.62,N16-1046,0,0.0374675,"Missing"
2020.wmt-1.62,W17-5701,0,0.0491763,"Missing"
2020.wmt-1.62,P16-1162,1,0.259035,"al probability, for fast sequence generation. IBDecoder retains the training efficiency and is easy to implement. • We extend IBDecoder to enable multi-word simultaneous generation by investigating integration with IMDecoder and SA. Results show that IBDecoder + SA performs better than IMDecoder. • We propose a modified beam search algorithm to support step-wise parallel generation. • On several sequence generation benchmarks, IBDecoder yields ∼2× speedup against Transformer at inference, and reaches 4×–11× after pairing it with SA. Still, the overall generation quality is comparable. 2 2016; Sennrich et al., 2016a; Zhang et al., 2019c,b; Zheng et al., 2019). Unfortunately, these methods either design complex neural decoders, which hurts training efficiency, and/or perform the left-to-right and right-to-left inference separately followed by rescoring, which slows down decoding. By contrast, our model speeds up inference while maintaining training speed. Our work is closely related to SA (Wang et al., 2018) and synchronous bidirectional generation (Zhou et al., 2019). IBDecoder extends SA to incorporate information from different directions. In contrast to Zhou et al. (2019), we only make minimal change"
2020.wmt-1.62,Q19-1002,0,0.0134224,"in parallel are in fact likely to be inter-dependent. We hypothesize that there are groups of words that are less likely to be strongly inter-dependent than neighbouring words, which will allow for better parallelization. Inspired by bidirectional modeling (Zhang et al., 2019b, 2020), we resort to an alternative probabilistic factorization: dn/2e BD p (y|x) = Y −→ ←−−  − p → yt , ← y− t0 |y<t , y>t0 , x , (2) t=1 Introduction Neural sequence generation aided by encoderdecoder models (Bahdanau et al., 2015; Vaswani et al., 2017) has achieved great success in recent years (Bojar et al., 2018; Song et al., 2019; Raffel et al., 2019; Karita et al., 2019), but still suffers from slow inference. One crucial bottleneck 1 Source code is released at https://github.com/ bzhangGo/zero. Introducing an independence assumption between t and t0 = n − t + 1 allows for parallel word pre−−−−−−−→ ←−−−−−−− diction from both the left-to-right and right-to-left directions. Based on this factorization, Zhou et al. (2019) propose synchronous bidirectional translation using a dedicated interactive decoder, and report quality improvements compared to left-toright semi-autoregressive decoding (Wang et al., 2018, SA) in tra"
2020.wmt-1.62,D18-1044,0,0.033297,"Missing"
2020.wmt-1.62,P19-1149,1,0.831445,"19). Related Work Efforts on fast sequence generation come along with the rapid development of encoder-decoder models (Vaswani et al., 2017). A straightforward way is to reduce the amount of computation. Methods in this category range from teacherstudent model (Kim and Rush, 2016; Hayashi et al., 2019), constrained softmax prediction (Hu et al., 2015), beam search cube pruning (Zhang et al., 2018c), float-point quantization (Wu et al., 2016; Bhandare et al., 2019), model pruning (See et al., 2016), to simplified decoder architectures, such as lightweight recurrent models (Zhang et al., 2018b; Zhang and Sennrich, 2019; Kim et al., 2019), average attention network (Zhang et al., 2018a), merged attention network (Zhang et al., 2019a), dynamic convolution (Wu et al., 2019), and hybrid attentions (Shazeer, 2019; Wang et al., 2019), .etc. Nonetheless, the above methods still suffer from the inference bottleneck caused by the sequential nature of autoregressive models. Instead, Gu et al. (2018) propose non-autoregressive generation where target words are predicted independently, leading to great speedup, albeit at a high cost to generation quality. Follow-up studies often seek solutions to recover the performanc"
2020.wmt-1.62,D19-1083,1,0.894205,"Missing"
2020.wmt-1.62,P18-1166,1,0.934783,"bidirectional decoder (IBDecoder) for fast generation. As shown in Figure 1a, we interleave target words from the left-to-right and right-to-left directions and separate their positions to support reusing any standard unidirectional decoders, such as the Transformer decoder (Vaswani et al., 2017). We reorganize the self-attention mask to enable inter- and intra-direction interaction (Figure 1c) following SA. Unlike SA, we show through experiments that distant tokens from different directions are less inter-dependent, providing a guarantee for better performance. Compared to previous studies (Zhang et al., 2018d, 2019b, 2020; Zhou et al., 2019), our approach has no extra model parameters and brings in little overhead at training and decoding. IBDecoder is speedup-bounded at 2×. To push this ceiling up, we explore strategies for multiword simultaneous generation, including multidirectional decoding (IMDecoder, Figure 1d) and SA (Figure 1b). The former extends Eq. 2 by inserting more generation directions, while the latter allows each direction to produce multiple target words (Wang et al., 2018). These strategies offer us a chance to aggressively improve the decoding speed albeit at the risk of degen"
2020.wmt-1.62,D18-1459,1,0.940765,"bidirectional decoder (IBDecoder) for fast generation. As shown in Figure 1a, we interleave target words from the left-to-right and right-to-left directions and separate their positions to support reusing any standard unidirectional decoders, such as the Transformer decoder (Vaswani et al., 2017). We reorganize the self-attention mask to enable inter- and intra-direction interaction (Figure 1c) following SA. Unlike SA, we show through experiments that distant tokens from different directions are less inter-dependent, providing a guarantee for better performance. Compared to previous studies (Zhang et al., 2018d, 2019b, 2020; Zhou et al., 2019), our approach has no extra model parameters and brings in little overhead at training and decoding. IBDecoder is speedup-bounded at 2×. To push this ceiling up, we explore strategies for multiword simultaneous generation, including multidirectional decoding (IMDecoder, Figure 1d) and SA (Figure 1b). The former extends Eq. 2 by inserting more generation directions, while the latter allows each direction to produce multiple target words (Wang et al., 2018). These strategies offer us a chance to aggressively improve the decoding speed albeit at the risk of degen"
2020.wmt-1.62,C02-1050,0,0.216363,"ent (Lee et al., 2018; Stern et al., 2019; Ghazvininejad et al., 2019) or in-between (Kaiser et al., 2018; Akoury et al., 2019). We hypothesize that generation order affects the felicity of independence assumptions made in semiautoregressive modelling. Unlike generation with flexible orders (Emelianenko et al., 2019; Stern et al., 2019; Gu et al., 2019a), we employ deterministic generation order for model simplicity and training efficiency, specifically focusing on bidirectional decoding. The study of bidirectional modeling dates back to the era of phase-based statistical machine translation (Watanabe and Sumita, 2002; Finch and Sumita, 2009) and recently gained popularity in neural machine translation (Liu et al., 3 Autoregressive Transformer Transformer (Vaswani et al., 2017), the state-of-theart neural sequence generation model, follows the autoregressive factorization as in Eq. 1. To handle the dependency of target word yt on previous target words y<t , Transformer relies on a masked selfattention network in the decoder: ! l Kl T Q √ + M Vl (3) ATT(Yl , M) = f d where Ql , Kl , Vl = Wql Yl , Wkl Yl , Wvl Yl ∈ Rn×d , f (·) denotes softmax operation, d is model dimension and l is layer depth. Wq , Wk , W"
2020.wmt-1.62,D18-1460,0,0.28935,"bidirectional decoder (IBDecoder) for fast generation. As shown in Figure 1a, we interleave target words from the left-to-right and right-to-left directions and separate their positions to support reusing any standard unidirectional decoders, such as the Transformer decoder (Vaswani et al., 2017). We reorganize the self-attention mask to enable inter- and intra-direction interaction (Figure 1c) following SA. Unlike SA, we show through experiments that distant tokens from different directions are less inter-dependent, providing a guarantee for better performance. Compared to previous studies (Zhang et al., 2018d, 2019b, 2020; Zhou et al., 2019), our approach has no extra model parameters and brings in little overhead at training and decoding. IBDecoder is speedup-bounded at 2×. To push this ceiling up, we explore strategies for multiword simultaneous generation, including multidirectional decoding (IMDecoder, Figure 1d) and SA (Figure 1b). The former extends Eq. 2 by inserting more generation directions, while the latter allows each direction to produce multiple target words (Wang et al., 2018). These strategies offer us a chance to aggressively improve the decoding speed albeit at the risk of degen"
2020.wmt-1.62,D19-1086,0,0.0159326,"BDecoder retains the training efficiency and is easy to implement. • We extend IBDecoder to enable multi-word simultaneous generation by investigating integration with IMDecoder and SA. Results show that IBDecoder + SA performs better than IMDecoder. • We propose a modified beam search algorithm to support step-wise parallel generation. • On several sequence generation benchmarks, IBDecoder yields ∼2× speedup against Transformer at inference, and reaches 4×–11× after pairing it with SA. Still, the overall generation quality is comparable. 2 2016; Sennrich et al., 2016a; Zhang et al., 2019c,b; Zheng et al., 2019). Unfortunately, these methods either design complex neural decoders, which hurts training efficiency, and/or perform the left-to-right and right-to-left inference separately followed by rescoring, which slows down decoding. By contrast, our model speeds up inference while maintaining training speed. Our work is closely related to SA (Wang et al., 2018) and synchronous bidirectional generation (Zhou et al., 2019). IBDecoder extends SA to incorporate information from different directions. In contrast to Zhou et al. (2019), we only make minimal changes to the standard Transformer decoder, which"
2020.wmt-1.64,2012.eamt-1.60,0,0.0229909,"., 2019; Zhang et al., 2020). Ha et al. (2016) do not share embeddings across languages, but use language-specific codes. We will show that both strategies cause errors. In terms of data settings, the number of languages involved in multilingual models has increased from 3–4 (Firat et al., 2016; Johnson et al., 2017) to over 100 (Aharoni et al., 2019). The most popular setup are English-centric datasets, where the model is trained on translations between English and a number of other languages. A multiway parallel corpus between 5 languages has been provided for the IWSLT17 multilingual task (Cettolo et al., 2012). Results on this dataset show strong zero-shot generalization, close or even exceeding the supervised condition (Lakew et al., 2017), but multi-way parallel corpora are only available in small amounts and specific domains, so we investigate alternatives to English-centric models that do not rely on multi-way parallelism. 20 10 0 0 5 10 15 20 Updates (*10k) 25 30 cs-de de-cs de-fi fi-de fi-fr fr-fi fr-cs cs-fr fr-de de-fr de-en en-de cs-en en-cs fi-en en-fi fr-en en-fr Figure 1: Baseline BLEU scores on test set as a function of training time. Dashed lines: trained pairs; solid lines: zero-shot"
2020.wmt-1.64,D19-5610,0,0.0847042,"iple language pairs with a standard encoder-decoder architecture, all parameters in the network are shared for all languages, including the vocabulary. An artificial target language token determines the output language. We prefix this special token to the source sentence as in Johnson et al. (2017). The major advantage of this model lies in its simplicity, since it does not require changing the architecture or training objective. Several recent studies have explored approaches to improve generalization to zero-shot language pairs, for example through semi-supervised training (Gu et al., 2019; Currey and Heafield, 2019; Zhang et al., 2020) or alignment of encoder representations (Arivazhagan et al., 2019). Our study is concerned with data conditions that enable zero-shot generalization for multilingual NMT, specifically preprocessing and data settings. While initial work used separate encoders and decoders for different languages (Firat et al., 2016), sharing of encoder and decoder parameters was established by Johnson et al. (2017); Ha et al. (2016) and has since been widely adopted. Johnson et al. (2017) use a shared subword segmentation model across languages, and this strategy is followed by later work"
2020.wmt-1.64,D16-1026,0,0.0371038,"Missing"
2020.wmt-1.64,P19-1121,0,0.397393,"ffect Zero-Shot Neural Machine Translation 1 ¨ Annette Rios1 and Mathias Muller and Rico Sennrich1,2 1 Department of Computational Linguistics, University of Zurich 2 School of Informatics, University of Edinburgh Abstract translation is guided only by special tags to indicate the desired output language (Johnson et al., 2017; Ha et al., 2016). While this capability is attractive because it is an alternative to building N 2 dedicated translation systems to serve N languages, performance on zero-shot pairs tends to lag behind pivot translation. Recent papers, such as Arivazhagan et al. (2019), Gu et al. (2019) and Zhang et al. (2020), have suggested training techniques to improve the generalization to unseen language pairs, but performance varies considerably across settings. In this paper, we examine in detail the behavior of the multilingual model proposed by Johnson et al. (2017) on zero-shot translation directions. Our experiments show the following: Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It"
2020.wmt-1.64,W18-1820,0,0.0284978,"irs; solid lines: zero-shot pairs. English↔{French,Czech,German,Finnish} from WMT (Barrault et al., 2019). For all zero-shot language pairs, we sample test sets from OPUS (Tiedemann, 2012), see Table 1 for details. To indicate the target language, we prefix a language tag on the source side (e.g. <2en>). Following Johnson et al. (2017), we segment all data with a byte-pair encoding model trained jointly on the training data in all five languages (Sennrich et al., 2016), with a threshold of 32k BPE operations. All our systems are base Transformers (Vaswani et al., 2017) implemented in Sockeye (Hieber et al., 2018), trained with early stopping based on BLEU on a development set that consists in equal parts of parallel sentences from all trained translation directions. See Appendix A and B for training details. 4 Baseline Experiments BLEU1 on zero-shot pairs is relatively unstable, see Fig. 1: while BLEU on the trained pairs increases steadily during training (dashed lines), performance on unseen language pairs fluctuates considerably, as also observed by Aharoni et al. (2019). Furthermore, multiple training runs result in relatively large differences in BLEU on the zero-shot directions. Across three tra"
2020.wmt-1.64,Q17-1024,0,0.0392922,"Missing"
2020.wmt-1.64,P18-1007,0,0.0509714,"Missing"
2020.wmt-1.64,W18-6319,0,0.0299817,"LEU on the trained pairs increases steadily during training (dashed lines), performance on unseen language pairs fluctuates considerably, as also observed by Aharoni et al. (2019). Furthermore, multiple training runs result in relatively large differences in BLEU on the zero-shot directions. Across three training runs, average BLEU varies up to 0.24 points on trained language pairs (standard deviation: 0.12), but up to 6.28 BLEU on zero-shot pairs (standard deviation: 3.14) – see Table 2 for full results. We suspect that this fluctuation is due to the fact that the model is not op1 SacreBLEU (Post, 2018): +s.exp+t.13a+v.1.2.21. 529 BLEU+c.mixed+#.1 corpora training dev test 5M 5M 5M 4.35M* 250 250 250 250 2000 2000 2000 2000 350k 343k** 200 200 2000 2000 Language Pairs with English: de↔en cs↔en fr↔en fi↔en Commoncrawl, Europarl-v9, Wikititles-v1 Europarl-v9, CzEng1.7 Commoncrawl, Europarl-v7 Europarl-v9, Wikititles-v1, Paracrawl-v3 Multi-Bridge Pairs: fr↔fi cs↔de Rapid2016 Rapid2016, NewsCommentary, GlobalVoices Zero-shot test sets: de↔fi de↔fr cs↔fr * ** Rapid2016 Rapid2016, NewsCommentary, GlobalVoices Rapid2016, NewsCommentary, GlobalVoices 2000 2000 2000 oversampled to 5M oversampled to 3"
2020.wmt-1.64,2020.acl-main.170,0,0.055452,"Missing"
2020.wmt-1.64,P16-1162,1,0.336579,"r de-en en-de cs-en en-cs fi-en en-fi fr-en en-fr Figure 1: Baseline BLEU scores on test set as a function of training time. Dashed lines: trained pairs; solid lines: zero-shot pairs. English↔{French,Czech,German,Finnish} from WMT (Barrault et al., 2019). For all zero-shot language pairs, we sample test sets from OPUS (Tiedemann, 2012), see Table 1 for details. To indicate the target language, we prefix a language tag on the source side (e.g. <2en>). Following Johnson et al. (2017), we segment all data with a byte-pair encoding model trained jointly on the training data in all five languages (Sennrich et al., 2016), with a threshold of 32k BPE operations. All our systems are base Transformers (Vaswani et al., 2017) implemented in Sockeye (Hieber et al., 2018), trained with early stopping based on BLEU on a development set that consists in equal parts of parallel sentences from all trained translation directions. See Appendix A and B for training details. 4 Baseline Experiments BLEU1 on zero-shot pairs is relatively unstable, see Fig. 1: while BLEU on the trained pairs increases steadily during training (dashed lines), performance on unseen language pairs fluctuates considerably, as also observed by Ahar"
2020.wmt-1.64,tiedemann-2012-parallel,0,0.14848,"way parallel corpora are only available in small amounts and specific domains, so we investigate alternatives to English-centric models that do not rely on multi-way parallelism. 20 10 0 0 5 10 15 20 Updates (*10k) 25 30 cs-de de-cs de-fi fi-de fi-fr fr-fi fr-cs cs-fr fr-de de-fr de-en en-de cs-en en-cs fi-en en-fi fr-en en-fr Figure 1: Baseline BLEU scores on test set as a function of training time. Dashed lines: trained pairs; solid lines: zero-shot pairs. English↔{French,Czech,German,Finnish} from WMT (Barrault et al., 2019). For all zero-shot language pairs, we sample test sets from OPUS (Tiedemann, 2012), see Table 1 for details. To indicate the target language, we prefix a language tag on the source side (e.g. <2en>). Following Johnson et al. (2017), we segment all data with a byte-pair encoding model trained jointly on the training data in all five languages (Sennrich et al., 2016), with a threshold of 32k BPE operations. All our systems are base Transformers (Vaswani et al., 2017) implemented in Sockeye (Hieber et al., 2018), trained with early stopping based on BLEU on a development set that consists in equal parts of parallel sentences from all trained translation directions. See Appendi"
2020.wmt-1.64,2020.acl-main.148,1,0.831695,"l Machine Translation 1 ¨ Annette Rios1 and Mathias Muller and Rico Sennrich1,2 1 Department of Computational Linguistics, University of Zurich 2 School of Informatics, University of Edinburgh Abstract translation is guided only by special tags to indicate the desired output language (Johnson et al., 2017; Ha et al., 2016). While this capability is attractive because it is an alternative to building N 2 dedicated translation systems to serve N languages, performance on zero-shot pairs tends to lag behind pivot translation. Recent papers, such as Arivazhagan et al. (2019), Gu et al. (2019) and Zhang et al. (2020), have suggested training techniques to improve the generalization to unseen language pairs, but performance varies considerably across settings. In this paper, we examine in detail the behavior of the multilingual model proposed by Johnson et al. (2017) on zero-shot translation directions. Our experiments show the following: Zero-shot neural machine translation is an attractive goal because of the high cost of obtaining data and building translation systems for new translation directions. However, previous papers have reported mixed success in zero-shot translation. It is hard to predict in w"
2021.acl-long.200,N18-1118,1,0.930138,"translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving conte"
2021.acl-long.200,2020.iwdp-1.3,0,0.0248615,"). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policie"
2021.acl-long.200,2020.autosimtrans-1.5,0,0.0295807,"rk is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST"
2021.acl-long.200,N19-1202,0,0.0590194,"Missing"
2021.acl-long.200,N13-1073,0,0.0189828,".29 62.08 Table 1: Case-sensitive tokenized BLEU and APT for different models and settings on MuST-C En-De test set. Numbers in bracket denote document-based BLEU. lp: the length penalty for beam search decoding. “w/o Cyn ”: models that are trained without target-side context. Best results are highlighted in bold. Note C = 2, λ = 0.5 and lp = 0.6 by default. erwise specified. We use APT (Miculicich Werlen and Popescu-Belis, 2017), the accuracy of pronoun translation, as an approximate proxy for documentlevel evaluation. Word alignment required by APT is automatically extracted via fast align (Dyer et al., 2013) with the strategy “grow-diag-final-and”. 5.2 Results on MuST-C En-De Does context improve translation? Yes, but the decoding method matters for context-aware ST. Table 1 summarizes the results. Our model with IMED outperforms Baseline by +0.48 BLEU (significant at p &lt; 0.05)6 and +1.79 APT (1→5), clearly showing the benefits from contextual modeling. Although SWBD-Cons yields worse sentencebased BLEU (-0.27, 1→4), it still beats Baseline in document-based BLEU (+0.58) and pronoun translation (+0.17 APT). The reason behind this inferior BLEU partially lies in misaligned translation (see Table 8"
2021.acl-long.200,D14-1140,0,0.0403613,"Missing"
2021.acl-long.200,W18-6435,0,0.0268638,"Missing"
2021.acl-long.200,D12-1108,0,0.0351747,"gs in textual MT (Miculicich et al., 2018; Huo et al., 2020). In addition, context also improves the translation of homophones. • ST models with contexts suffer less from (artificial) audio segmentation errors. • Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2"
2021.acl-long.200,2020.wmt-1.71,0,0.109365,"textaware ST model here for both types of translation – that’s why we call it in-model ensemble. We adopt Transformer (Vaswani et al., 2017) for experiments with the MuST-C dataset (Di Gangi et al., 2019). We study the impact of context on translation in different settings. Our results demonstrate the effectiveness of contextual modeling. Our main findings are summarized below: • Incorporating context improves overall translation quality (+0.18-2.61 BLEU) and benefits pronoun translation across different language pairs, resonating with previous findings in textual MT (Miculicich et al., 2018; Huo et al., 2020). In addition, context also improves the translation of homophones. • ST models with contexts suffer less from (artificial) audio segmentation errors. • Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rap"
2021.acl-long.200,2020.emnlp-main.206,0,0.0710008,"Missing"
2021.acl-long.200,2020.emnlp-main.175,0,0.0284592,"Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future wor"
2021.acl-long.200,D19-6503,0,0.0268863,"Missing"
2021.acl-long.200,C18-1050,0,0.0644363,"Missing"
2021.acl-long.200,D18-1512,1,0.89201,"Missing"
2021.acl-long.200,2020.eamt-1.24,0,0.178603,"fforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoS´anchez et al., 2020; Zhang an"
2021.acl-long.200,2020.acl-main.321,0,0.0156967,"ioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sent"
2021.acl-long.200,D18-1325,0,0.111413,"that we use the same contextaware ST model here for both types of translation – that’s why we call it in-model ensemble. We adopt Transformer (Vaswani et al., 2017) for experiments with the MuST-C dataset (Di Gangi et al., 2019). We study the impact of context on translation in different settings. Our results demonstrate the effectiveness of contextual modeling. Our main findings are summarized below: • Incorporating context improves overall translation quality (+0.18-2.61 BLEU) and benefits pronoun translation across different language pairs, resonating with previous findings in textual MT (Miculicich et al., 2018; Huo et al., 2020). In addition, context also improves the translation of homophones. • ST models with contexts suffer less from (artificial) audio segmentation errors. • Contextual modeling improves translation quality and reduces latency and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang,"
2021.acl-long.200,W17-4802,0,0.0152439,") 22.97 (28.29) 22.94 (28.11) 61.89 63.51 62.76 11 12 3 w/o Cyn 5 w/o Cyn 21.12 (26.17) 20.72 (25.43) 59.51 58.18 13 14 3 w/o Baseline Initial. 5 w/o Baseline Initial. 21.75 (27.15) 21.97 (27.20) 62.29 62.08 Table 1: Case-sensitive tokenized BLEU and APT for different models and settings on MuST-C En-De test set. Numbers in bracket denote document-based BLEU. lp: the length penalty for beam search decoding. “w/o Cyn ”: models that are trained without target-side context. Best results are highlighted in bold. Note C = 2, λ = 0.5 and lp = 0.6 by default. erwise specified. We use APT (Miculicich Werlen and Popescu-Belis, 2017), the accuracy of pronoun translation, as an approximate proxy for documentlevel evaluation. Word alignment required by APT is automatically extracted via fast align (Dyer et al., 2013) with the strategy “grow-diag-final-and”. 5.2 Results on MuST-C En-De Does context improve translation? Yes, but the decoding method matters for context-aware ST. Table 1 summarizes the results. Our model with IMED outperforms Baseline by +0.48 BLEU (significant at p &lt; 0.05)6 and +1.79 APT (1→5), clearly showing the benefits from contextual modeling. Although SWBD-Cons yields worse sentencebased BLEU (-0.27, 1→4"
2021.acl-long.200,P02-1040,0,0.109788,"Missing"
2021.acl-long.200,W18-6319,0,0.0237995,"Missing"
2021.acl-long.200,N13-1023,0,0.194178,"nd Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoS´anchez et al., 2020; Zhang and Zhang, 2020; Arivazhagan et al., 2020b). Very recently, Ma et al. (2020b) proposed a memory augmented Transformer encoder for streaming ST, where the previous audio features are summarized into a growing continuous memory to improve the model’s context awareness. Despite its success, this method ignores the target-side context, which turns out to have significant positive impact on ST in our experiments. Our study still relies on oracle sentence segmentation of the audio. The most related work to ours is (Gaido et al., 2020), which also investigated con"
2021.acl-long.200,W17-4702,1,0.848263,"re ST. yn denotes the n-th target sentence in a document; xn denotes the speech encodings extracted from the n-th audio segment. We use dashed gray box to indicate the concatenation operation. “&lt;s>”: sentence separator symbol. Document-level context often offers extra informative clues that could improve the understanding of individual sentences. Such clues have been proven effective for textual machine translation (MT), particularly in handling translation errors specific to discourse phenomena, such as inaccurate coreference of pronouns (Guillou, 2016) and mistranslation of ambiguous words (Rios et al., 2017). Besides, ensuring consistency in translation is virtually impossible without document-level context as well (Voita et al., 2019). Analogous to MT, speech translation (ST) also suffers from these translation issues, and super-sentential context could in fact be more valuable to ST because 1) homophones and acoustic noise bring additional ambiguity to ST, and 2) a common use case in ST is simultaneous translation, where the system has to output translations of sentence fragments, and may have to predict future input to account for word order differences between the source and target language ("
2021.acl-long.200,P16-1162,1,0.404175,"Missing"
2021.acl-long.200,W17-4811,0,0.0254226,"and flicker for simultaneous translation under re-translation strategy (Arivazhagan et al., 2020a). 2 Related Work Our work is inspired by pioneer studies on contextaware textual MT. Context beyond the current sentence carries information whose importance for translation cohesion and coherence has long been posited (Hardmeier et al., 2012; Xiong and Zhang, 2013). With the rapid development of neural MT and also available document-level textual datasets, research in this direction gained great popularity. Recent efforts often focus on either advanced contextual neural architecture development (Tiedemann and Scherrer, 2017; Kuang et al., 2018; Miculicich et al., 2018; Zhang et al., 2018, 2020c; Kang et al., 2020; Chen et al., 2020; Ma et al., 2020a; Zheng et al., 2020) and/or improved analysis and evaluation targeted at specific discourse phenomena (Bawden et al., 2018; L¨aubli et al., 2018; Guillou et al., 2018; Voita et al., 2019; Kim et al., 2019; Cai and Xiong, 2020). We follow this research line, and adapt the concatenation-based contextual model (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Lopes et al., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicat"
2021.acl-long.200,P19-1116,1,0.896393,"Missing"
2021.acl-long.200,P18-1117,1,0.841621,"wer this question by studying the impact of incorrect context on our model. We replace the correct source context with some random audio segments from the same document, and randomly select the target context from previous translations during decoding. Intuitively, the performance of our model should be intact if it ignores the context. Note that we trained our model with correct contexts but test it with random contexts here. Results in Table 2 show that the randomized context, either source- or target-side, hurts the performance of our model in both BLEU and APT, similar to the findings in (Voita et al., 2018), and the translation of pronouns suffers more (> -1.6 APT). Compared to SWBD, the incorrect context has more negative impact on IMED, resulting in worse performance than Baseline (Table 1), although IMED also uses sentence-level translation. We ascribe this to the target prefix constraint in IMED which makes translation errors at early decoding much easier to propagate. We observe that the incorrect target context acts similarly to its source counterpart under IMED, albeit its selection scope is much smaller (only limited to the translated segments), and combining both contexts leads to a sli"
2021.acl-long.200,2020.acl-main.344,0,0.0208695,"ty to ST, and 2) a common use case in ST is simultaneous translation, where the system has to output translations of sentence fragments, and may have to predict future input to account for word order differences between the source and target language (Grissom II et al., 2014). Both for ambiguity from the acoustic signal, and operating on small sentence fragments, we hypothesize that access to extra context2 will be beneficial. Although recent studies on ST have achieved promising results with end-to-end (E2E) models (Anastasopoulos and Chiang, 2018; Di Gangi et al., 2019; Zhang et al., 2020a; Wang et al., 2020; Dong et al., 2020), nevertheless, they mainly focus on sentence-level translation. One practical challenge when scaling up sentence-level E2E ST to the document-level is the encoding of very long audio segments, which can easily hit the computational bottleneck, especially with Transformers (Vaswani et al., 2017). So far, the research question of whether and how contextual information benefits E2E ST has received little attention. In this paper, we answer this question through extensive experiments by exploring a concatenation1 Source code is available at https://github.com/ bzhangGo/zero. 2"
2021.acl-long.200,2020.findings-emnlp.230,1,0.889467,"ng additional ambiguity to ST, and 2) a common use case in ST is simultaneous translation, where the system has to output translations of sentence fragments, and may have to predict future input to account for word order differences between the source and target language (Grissom II et al., 2014). Both for ambiguity from the acoustic signal, and operating on small sentence fragments, we hypothesize that access to extra context2 will be beneficial. Although recent studies on ST have achieved promising results with end-to-end (E2E) models (Anastasopoulos and Chiang, 2018; Di Gangi et al., 2019; Zhang et al., 2020a; Wang et al., 2020; Dong et al., 2020), nevertheless, they mainly focus on sentence-level translation. One practical challenge when scaling up sentence-level E2E ST to the document-level is the encoding of very long audio segments, which can easily hit the computational bottleneck, especially with Transformers (Vaswani et al., 2017). So far, the research question of whether and how contextual information benefits E2E ST has received little attention. In this paper, we answer this question through extensive experiments by exploring a concatenation1 Source code is available at https://github.c"
2021.acl-long.200,2021.findings-acl.255,1,0.835314,"Missing"
2021.acl-long.200,D18-1049,0,0.0359043,"Missing"
2021.acl-long.200,2020.emnlp-main.81,0,0.0507902,"Missing"
2021.acl-long.200,2020.autosimtrans-1.1,0,0.196645,"., 2020) to ST. Our main interest lies in exploring the impact of context on ST. Developing dedicated contextual models for ST is beyond the scope of this study, which we leave to future work. Context-aware ST extends the sentence-level ST towards streaming ST which allows models to access unlimited previous audio inputs. Instead of improving contextual modeling, many studies on streaming ST aim at developing better sentence/word segmentation policies to avoid segmentation errors that greatly hurt translation (Matusov et al., 2007; Rangarajan Sridhar et al., 2013; IranzoS´anchez et al., 2020; Zhang and Zhang, 2020; Arivazhagan et al., 2020b). Very recently, Ma et al. (2020b) proposed a memory augmented Transformer encoder for streaming ST, where the previous audio features are summarized into a growing continuous memory to improve the model’s context awareness. Despite its success, this method ignores the target-side context, which turns out to have significant positive impact on ST in our experiments. Our study still relies on oracle sentence segmentation of the audio. The most related work to ours is (Gaido et al., 2020), which also investigated contextualized translation and showed that contextaware"
2021.acl-long.200,2021.acl-demo.7,0,0.0628604,"Missing"
2021.acl-long.22,W14-3346,0,0.0272679,"BLEU is a precisionbased metric known to prefer shorter translations on the sentence level (Nakov et al., 2012). chrf-2 and meteor emphasize recall more, and the resulting MBR translations overestimate the true length of translations.2 On the other hand, chrf-0.5, a CHRF variant with a bias for precision, leads to the shortest translations overall. We test whether we can reduce length biases by symmetrizing our utility functions u as follows: Table 1: Utility functions used with MBR. The smoothed variants of BLEU correspond to the ones implemented in SacreBLEU (Post, 2018) and are defined in Chen and Cherry (2014). MBR also depends on samples, so we repeat each MBR experiment twice to show the resulting variance. We also vary the number of samples used with MBR, from 5 to 100 in increments of 5. Finally, we produce MBR translations with different utility functions. All of the utility functions are sentence-level variants of our evaluation metrics: BLEU, CHRF or METEOR. See Table 1 for an overview of utility functions. If not stated otherwise, MBR results are based on 100 samples and use chrf-1 as the utility function. 5 usym (si , sj ) = H(u(si , sj ), u(sj , si )) Length bias We evaluate MBR decoding"
2021.acl-long.22,W14-3348,0,0.0159994,"disable label smoothing so as to get unbiased samples. 4.3 Decoding and evaluation In all experiments, we compare beam search to MBR decoding and in most cases also to single samples. For beam search, we always use a beam size of 5. Single samples are drawn at least 100 times to show the resulting variance. If not stated otherwise, all results presented are on a test set held out from the training data, i.e. are certainly in-domain, which avoids any unintended out-of-domain effects. We evaluate automatic translation quality with BLEU (Papineni et al., 2002), CHRF (Popovi´c, 2016) and METEOR (Denkowski and Lavie, 2014). We compute BLEU and CHRF with SacreBLEU (Post, 2018). See Appendix B for details. 261 Figure 1: CHRF1 scores of MBR decoding on two test corpora: the standard Tatoeba test set (out-of-domain) and a test set of held-out training data (in-domain). Plots show the difference between MBR and beam search, as a function of the number of samples used for MBR. smoothed? α β γ δ bleu bleu-floor bleu-add-k bleu-exp 7 3 3 3 - - - - chrf-0.5 chrf-1 chrf-2 chrf-3 7 7 7 7 - 0.5 1.0 2.0 3.0 - - meteor meteor-0.5 7 7 0.85 0.50 0.2 0.2 0.6 0.6 0.75 0.75 that MBR does not suffer from the beam search curse wher"
2021.acl-long.22,2020.amta-research.14,1,0.840313,"Missing"
2021.acl-long.22,C12-1121,0,0.0290141,"in terms of mean length of translations, beam search underestimates the true length of translations, even when hypotheses are normalized. Hypotheses generated by sampling better match the reference length. This is in line with the findings of Eikema and Aziz (2020). For MBR decoding, it is clear that the choice of utility function has an impact on the mean length of the resulting translations. For instance, employing sentence-level BLEU as the utility function leads to translations that are too short. BLEU is a precisionbased metric known to prefer shorter translations on the sentence level (Nakov et al., 2012). chrf-2 and meteor emphasize recall more, and the resulting MBR translations overestimate the true length of translations.2 On the other hand, chrf-0.5, a CHRF variant with a bias for precision, leads to the shortest translations overall. We test whether we can reduce length biases by symmetrizing our utility functions u as follows: Table 1: Utility functions used with MBR. The smoothed variants of BLEU correspond to the ones implemented in SacreBLEU (Post, 2018) and are defined in Chen and Cherry (2014). MBR also depends on samples, so we repeat each MBR experiment twice to show the resultin"
2021.acl-long.22,2020.emnlp-main.97,0,0.0216494,"rating very frequent words (Ott et al., 2018), or being susceptible to copy noise in the training data (Khayrallah and Koehn, 2018). In out-of-domain translation, hallucinations (translations that are fluent but unrelated to the source) are common (Koehn and Knowles, 2017; Lee et al., 2018; M¨uller et al., 2020). Previous work has addressed these problems with decoding heuristics such as length normalization (Wu et al., 2016), data cleaning (JunczysDowmunt, 2018; Ba˜no´ n et al., 2020) or model regularization (Bengio et al., 2015; Shen et al., 2016; Wiseman and Rush, 2016; Zhang et al., 2019; Ng et al., 2020). Recently, Eikema and Aziz (2020) have highlighted the role of the decision rule, namely searching for the highest-scoring translation, and have argued that it is at least partially to blame for some of these biases and shortcomings. They found that sampling from an NMT model is faithful to the training data statistics, while beam search is not. They recommend the field look into alternative inference algorithms based on unbiased samples, such as Minimum Bayes Risk (MBR) decoding. We believe MBR has potential to overcome several known biases of NMT. More precisely, if a bias can be understood"
2021.acl-long.22,P02-1040,0,0.110019,"e of the training set. Following Eikema and Aziz (2020) we disable label smoothing so as to get unbiased samples. 4.3 Decoding and evaluation In all experiments, we compare beam search to MBR decoding and in most cases also to single samples. For beam search, we always use a beam size of 5. Single samples are drawn at least 100 times to show the resulting variance. If not stated otherwise, all results presented are on a test set held out from the training data, i.e. are certainly in-domain, which avoids any unintended out-of-domain effects. We evaluate automatic translation quality with BLEU (Papineni et al., 2002), CHRF (Popovi´c, 2016) and METEOR (Denkowski and Lavie, 2014). We compute BLEU and CHRF with SacreBLEU (Post, 2018). See Appendix B for details. 261 Figure 1: CHRF1 scores of MBR decoding on two test corpora: the standard Tatoeba test set (out-of-domain) and a test set of held-out training data (in-domain). Plots show the difference between MBR and beam search, as a function of the number of samples used for MBR. smoothed? α β γ δ bleu bleu-floor bleu-add-k bleu-exp 7 3 3 3 - - - - chrf-0.5 chrf-1 chrf-2 chrf-3 7 7 7 7 - 0.5 1.0 2.0 3.0 - - meteor meteor-0.5 7 7 0.85 0.50 0.2 0.2 0.6 0.6 0.75"
2021.acl-long.22,W16-2341,0,0.121278,"Missing"
2021.acl-long.22,W18-6319,0,0.146624,"and evaluation In all experiments, we compare beam search to MBR decoding and in most cases also to single samples. For beam search, we always use a beam size of 5. Single samples are drawn at least 100 times to show the resulting variance. If not stated otherwise, all results presented are on a test set held out from the training data, i.e. are certainly in-domain, which avoids any unintended out-of-domain effects. We evaluate automatic translation quality with BLEU (Papineni et al., 2002), CHRF (Popovi´c, 2016) and METEOR (Denkowski and Lavie, 2014). We compute BLEU and CHRF with SacreBLEU (Post, 2018). See Appendix B for details. 261 Figure 1: CHRF1 scores of MBR decoding on two test corpora: the standard Tatoeba test set (out-of-domain) and a test set of held-out training data (in-domain). Plots show the difference between MBR and beam search, as a function of the number of samples used for MBR. smoothed? α β γ δ bleu bleu-floor bleu-add-k bleu-exp 7 3 3 3 - - - - chrf-0.5 chrf-1 chrf-2 chrf-3 7 7 7 7 - 0.5 1.0 2.0 3.0 - - meteor meteor-0.5 7 7 0.85 0.50 0.2 0.2 0.6 0.6 0.75 0.75 that MBR does not suffer from the beam search curse where single pathological hypotheses in a large beam can j"
2021.acl-long.22,2020.emnlp-main.213,0,0.035983,"of NMT, such as spurious copying, or hallucinations under domain shift. The mechanism by which MBR achieves such robustness is that copies or hallucinated hypotheses in a pool of samples are assigned low utility and never selected as the final translation. In our experiments, MBR did not generally outperform beam search according to automatic metrics, but we still deem it a promising alternative to MAP decoding due to its robustness. For future work, we are interested in exploring more sophisticated similarity metrics to be used as utility functions, including trainable metrics such as COMET (Rei et al., 2020), and investigating how these utility functions affect the overall quality and biases of translations. 10 Note on reproducibility We will not only release the source code used to train our models (as is common in NLP papers at the moment), but a complete pipeline of code that can be run on any instance in a fully automated fashion. This will allow to reproduce our results, including the graphs and tables shown in this paper, in a consistent way with minimal changes. We encourage the community to attempt to reproduce our results and publish the results. 266 Acknowledgements This work has receiv"
2021.acl-long.22,2020.eamt-1.61,0,0.0201156,". For one additional experiment on out-of-domain robustness we use data from M¨uller et al. (2020). This data set is German-English and defines 5 different domains of text (medical, it, koran, law and subtitles). Following M¨uller et al. (2020) we train our model on the medical domain, and use data in other domains to test domain robustness. We hold out a random sample of the training data for testing purposes. The size of this sample varies between 1k and 5k sentences, depending on the overall size of the training data. 4.2 Models Our preprocessing and model settings are inspired by OPUS-MT (Tiedemann and Thottingal, 2020). We use Sentencepiece (Kudo, 2018) with subword regularization as the only preprocessing step, which takes care of both tokenization and subword segmentation. The desired number of pieces in the vocabulary varies with the size of the data set. We train NMT models with Sockeye 2 (Domhan et al., 2020). The models are standard Transformer models (Vaswani et al., 2017), except that some settings (such as word batch size and dropout rate) vary with the size of the training set. Following Eikema and Aziz (2020) we disable label smoothing so as to get unbiased samples. 4.3 Decoding and evaluation In"
2021.acl-long.22,2020.wmt-1.139,0,0.0131265,"sentence or hallucinations (translations that are fluent, but unrelated to the input) can be avoided with MBR if they are not common in a pool of samples. Finally, we study the skewedness of token frequencies in translations. Eikema and Aziz (2020) study lexical biases in NMT models, showing that model samples have higher agreement with the training distribution than MAP output. We investigate whether this is also true for MBR decoding, focusing on the well-known bias towards frequent tokens. 4 4.1 Experimental Setup Data We use data for a number of language pairs from the Tatoeba Challenge (Tiedemann, 2020). Individual language pairs are fairly different in terms of language families, scripts and training set sizes. See Appendix A for details about our data sets. For one additional experiment on out-of-domain robustness we use data from M¨uller et al. (2020). This data set is German-English and defines 5 different domains of text (medical, it, koran, law and subtitles). Following M¨uller et al. (2020) we train our model on the medical domain, and use data in other domains to test domain robustness. We hold out a random sample of the training data for testing purposes. The size of this sample var"
2021.acl-long.22,D08-1065,0,0.0738379,"ining does not constrain a model to be characterized well by its mode only. If the mode is inadequate, then obviously that is problematic for a mode-seeking procedure such as beam search, and MAP inference in general. In fact, MAP decoding should be used only if the mode of the output distribution can be trusted (Smith, 2011). An alternative is a decision rule that considers how different a translation is from other likely translations. 2.3 Minimum Bayes Risk Decoding MBR decoding was used in speech recognition (Goel and Byrne, 2000) and statistical machine translation (Kumar and Byrne, 2004; Tromble et al., 2008). More recently, MBR was also used to improve beam search decoding in NMT (Stahlberg et al., 2017; Shu and Nakayama, 2017; Blain et al., 2017). Eikema and Aziz (2020) are the first to test a variant of MBR that operates on samples instead of an nbest list generated by beam search. We give here a simplified, accessible definition of MBR in the context of NMT. Essentially, the goal of MBR is to find not the most probable trans260 lation, but the one that minimizes the expected risk for a given loss function and the true posterior distribution. In practice, the set of all possible candidate trans"
2021.acl-long.22,P16-1159,0,0.0287652,"9), underestimating the probability of rare words and over-generating very frequent words (Ott et al., 2018), or being susceptible to copy noise in the training data (Khayrallah and Koehn, 2018). In out-of-domain translation, hallucinations (translations that are fluent but unrelated to the source) are common (Koehn and Knowles, 2017; Lee et al., 2018; M¨uller et al., 2020). Previous work has addressed these problems with decoding heuristics such as length normalization (Wu et al., 2016), data cleaning (JunczysDowmunt, 2018; Ba˜no´ n et al., 2020) or model regularization (Bengio et al., 2015; Shen et al., 2016; Wiseman and Rush, 2016; Zhang et al., 2019; Ng et al., 2020). Recently, Eikema and Aziz (2020) have highlighted the role of the decision rule, namely searching for the highest-scoring translation, and have argued that it is at least partially to blame for some of these biases and shortcomings. They found that sampling from an NMT model is faithful to the training data statistics, while beam search is not. They recommend the field look into alternative inference algorithms based on unbiased samples, such as Minimum Bayes Risk (MBR) decoding. We believe MBR has potential to overcome several kn"
2021.acl-long.22,P19-1426,0,0.0379771,"Missing"
2021.acl-long.91,P17-1080,0,0.0267114,"utions when conditioning on different types of prefixes, when varying the training objective or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open."
2021.acl-long.91,W19-5361,0,0.0226197,"rol the influence of source and target leads to improvement for both RNN (Tu et al., 2017; Wang et al., 2018) and Transfomer (Li et al., 2020) models. A more popular example is a model’s tendency to generate hallucinations (fluent but inadequate translations); it is usually attributed to the inappropriately strong influence of target context. Several works observed that, when hallucinating, a model fails to properly use source: it produces a deficient attention matrix, where almost all the probability mass is concentrated on uninformative source tokens (EOS and punctuation) (Lee et al., 2018; Berard et al., 2019). We argue that a natural way to estimate how the source and target contexts contribute to generation is to apply Layerwise Relevance Propagation (LRP) (Bach et al., 2015) to NMT models. LRP redistributes the information used for a prediction between all input elements keeping the total contribution constant. This ‘conservation principle’ makes relevance propagation unique: differently from other methods estimating influence of individual tokens (Alvarez-Melis and Jaakkola, 2017; He 1126 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Intern"
2021.acl-long.91,P17-1106,0,0.080576,"yer l by summing all messages from neurons at layer (l + 1): X (l,l+1) (l) Ri = Ri←j . (3) j Layer-wise relevance propagation is a framework which decomposes the prediction of a deep neural network computed over an instance, e.g. an image or sentence, into relevance scores for single input dimensions of the sample such as subpixels of an image or neurons of input token embeddings. The Equations (2) and (3) define the propagation of relevance from layer l+1 to layer l. The only thing that is missing is specific formulas for computing the 2 Previous work applying one of the LRP variants to NMT (Ding et al., 2017; Voita et al., 2019) do not describe extensions beyond the original LRP rules (Bach et al., 2015). 1127 (l,l+1) (l,l+1) messages Ri←j . Usually, the message Ri←j has the following structure: (l,l+1) Ri←j (l+1) = vij Rj , X vij = 1. (4) i Several versions of LRP satisfying equation (4) (and, therefore, the conservation principle) have been introduced: LRP-ε, LRP-αβ and LRPγ (Bach et al., 2015; Binder et al., 2016; Montavon et al., 2019). We use LRP-αβ (Bach et al., 2015; Binder et al., 2016), which defines relevances at each step in such a way that they are positive. Rule for relevance propaga"
2021.acl-long.91,N18-1033,0,0.0334514,"Missing"
2021.acl-long.91,I17-1004,0,0.0187699,"ss is non-monotonic with several stages of different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding"
2021.acl-long.91,D19-1088,0,0.0815687,"for the model 6 Random prefixes come from the same evaluation set, but with shuffled target sentences. 1130 to rely on the source more and to be more confident when choosing relevant source tokens. Reference vs random prefixes. Results for random sentence prefixes are given in Figures 3c, 3d. The reaction to random prefixes helps us study the self-recovery ability of NMT models. Previous work has found that models can fall into a hallucination mode where “the decoder ignores context from the encoder and samples from its language mode” (Koehn and Knowles, 2017; Lee et al., 2018). In contrast, He et al. (2019b) found that a language model is able to recover from artificially distorted history input and generate reasonable samples. Our results show evidence for both. At the beginning of the generation process, the model tends to rely more on the source context when given a random prefix compared to the reference prefix, indicating a self-recovery mode. However, when the prefix becomes longer, the model choice shifts towards ignoring the source and relying more on the target: Figure 3c shows a large drop of source influence for later positions. Figure 3d also shows that with a random prefix, the ent"
2021.acl-long.91,kocmi-bojar-2017-curriculum,0,0.0180957,"that earlier positions are the ones that change most actively: at these positions, we see the largest decrease at the first stage and the largest following increase at the subsequent stages. If we look at how accuracy for each position changes in training (Figure 10), we see that at the end of the first stage, early tokens have the highest accuracy.7 This is not surprising: one could expect early positions to train faster because they are observed more frequently in training. Previously such intuition motivated the usage of sentence length as one of the criteria for curriculum learning (e.g., Kocmi and Bojar (2017)). 7.1 Relation to Previous Work Interestingly, our stages in Figure 7 agree with the ones found by Frankle et al. (2020) for ResNet-20 trained on CIFAR-10 when investigating, among other things, the lottery ticket hypothesis (Frankle and Carbin, 2019). Their stages were defined based on the changes in gradient magnitude, in the weight space, in the performance, and in the effectiveness of rewinding in search of the ‘winning’ subnetwork (for more details on the lottery ticket hypothesis 7 Accuracy is the proportion of cases where the correct token is the most probable choice. 1133 not using so"
2021.acl-long.91,W17-3204,0,0.0232433,"experiments, these simpler model-generated prefixes allow for the model 6 Random prefixes come from the same evaluation set, but with shuffled target sentences. 1130 to rely on the source more and to be more confident when choosing relevant source tokens. Reference vs random prefixes. Results for random sentence prefixes are given in Figures 3c, 3d. The reaction to random prefixes helps us study the self-recovery ability of NMT models. Previous work has found that models can fall into a hallucination mode where “the decoder ignores context from the encoder and samples from its language mode” (Koehn and Knowles, 2017; Lee et al., 2018). In contrast, He et al. (2019b) found that a language model is able to recover from artificially distorted history input and generate reasonable samples. Our results show evidence for both. At the beginning of the generation process, the model tends to rely more on the source context when given a random prefix compared to the reference prefix, indicating a self-recovery mode. However, when the prefix becomes longer, the model choice shifts towards ignoring the source and relying more on the target: Figure 3c shows a large drop of source influence for later positions. Figure"
2021.acl-long.91,2020.acl-main.757,0,0.0321549,"he generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT models often fail to effectively control information flow from source and target contexts. For example, adding context gates to dynamically control the influence of source and target leads to improvement for both RNN (Tu et al., 2017; Wang et al., 2018) and Transfomer (Li et al., 2020) models. A more popular example is a model’s tendency to generate hallucinations (fluent but inadequate translations); it is usually attributed to the inappropriately strong influence of target context. Several works observed that, when hallucinating, a model fails to properly use source: it produces a deficient attention matrix, where almost all the probability mass is concentrated on uninformative source tokens (EOS and punctuation) (Lee et al., 2018; Berard et al., 2019). We argue that a natural way to estimate how the source and target contexts contribute to generation is to apply Layerwis"
2021.acl-long.91,W18-5431,0,0.036824,"1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT models often fail to effectively"
2021.acl-long.91,W17-4702,1,0.826388,"the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly"
2021.acl-long.91,E17-2060,1,0.800681,"or the amount of training data, and during the training process. We find that models trained with more data tend to rely on source information more and to have more sharp token contributions; the training process is non-monotonic with several stages of different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source"
2021.acl-long.91,P16-1162,1,0.434072,"Missing"
2021.acl-long.91,P16-1159,0,0.0171603,"get history, and show that Minimum Risk Training (MRT), which does not suffer from exposure bias, reduces hallucinations. However, they did not directly measure this overreliance on target history. Our method is able to directly test whether there is indeed an over-reliance on the target history with MLE-trained models, and more robust inclusion of source context with MRT. We also consider a simpler heuristic, word dropout, which we hypothesize to have a similar effect. (a) (b) Figure 4: Contribution of source (a) and entropy of source (b) with model-generated prefixes. Minimum Risk Training (Shen et al., 2016) is a sentence-level objective that inherently avoids exposure bias. It minimises the expected loss (‘risk’) with respect to the posterior distribution: X X R(θ) = P (˜ y |x, θ)∆(˜ y , y), (x,y) y˜∈Y(x) where Y(x) is a set of candidate translations for x, ∆(˜ y , y) is the discrepancy between the model prediction y˜ and the gold translation y (e.g., a negative smoothed sentence-level BLEU). More details on the method can be found in Shen et al. (2016) or Edunov et al. (2018); training details for our models are in the appendix. Word Dropout is a simple data augmentation technique. During train"
2021.acl-long.91,W11-2102,0,0.0996528,"anguage.6 As in previous experiments, we evaluate relevance for top-1 logit predicted by the model. Reference vs model prefixes. When feeding model-generated prefixes, the model uses source more (Figure 3(a)) and has more focused source contributions (lower entropy in Figure 3(b)) than when generating the reference. This may be because model-generated translations are ‘easier’ than references. For example, beam search translations contain fewer rare tokens (Burlot and Yvon, 2018; Ott et al., 2018), are simpler syntactically (Burlot and Yvon, 2018) and, according to the fuzzy reordering score (Talbot et al., 2011), model translations have significantly less reordering compared to the real parallel sentences (Zhou et al., 2020). As we see from our experiments, these simpler model-generated prefixes allow for the model 6 Random prefixes come from the same evaluation set, but with shuffled target sentences. 1130 to rely on the source more and to be more confident when choosing relevant source tokens. Reference vs random prefixes. Results for random sentence prefixes are given in Figures 3c, 3d. The reaction to random prefixes helps us study the self-recovery ability of NMT models. Previous work has found"
2021.acl-long.91,D18-1458,1,0.849335,"different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT mo"
2021.acl-long.91,W18-6304,1,0.8452,"different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT mo"
2021.acl-long.91,Q17-1007,0,0.0170471,"edictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT models often fail to effectively control information flow from source and target contexts. For example, adding context gates to dynamically control the influence of source and target leads to improvement for both RNN (Tu et al., 2017; Wang et al., 2018) and Transfomer (Li et al., 2020) models. A more popular example is a model’s tendency to generate hallucinations (fluent but inadequate translations); it is usually attributed to the inappropriately strong influence of target context. Several works observed that, when hallucinating, a model fails to properly use source: it produces a deficient attention matrix, where almost all the probability mass is concentrated on uninformative source tokens (EOS and punctuation) (Lee et al., 2018; Berard et al., 2019). We argue that a natural way to estimate how the source and target c"
2021.acl-long.91,P18-1117,1,0.828772,"h several stages of different nature.1 1 Introduction With the success of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work"
2021.acl-long.91,P19-1580,1,0.901558,"s of neural approaches to natural language processing, analysis of NLP models has become an important and active topic of research. In NMT, approaches to analysis include probing for linguistic structure (Belinkov et al., 2017; Conneau et al., 2018), evaluating via contrastive translation pairs (Sennrich, 2017; Burlot and Yvon, 2017; Rios Gonzales et al., 2017; Tang 1 We release the code at https://github.com/ lena-voita/the-story-of-heads. et al., 2018), inspecting model components, such as attention (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; Raganato and Tiedemann, 2018; Voita et al., 2019) or neurons (Dalvi et al., 2019; Bau et al., 2019), among others. Unfortunately, although a lot of work on model analysis has been done, a question of how the NMT predictions are formed remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT models often fail to effectively control information"
2021.acl-long.91,2020.acl-main.326,1,0.919069,"longer, the model choice shifts towards ignoring the source and relying more on the target: Figure 3c shows a large drop of source influence for later positions. Figure 3d also shows that with a random prefix, the entropy of source contributions is high and is roughly constant. 5 Exposure Bias and Source Contributions The results in the previous section agree with some observations made in previous work studying selfrecovery and hallucinations. In this section, we illustrate more explicitly how our methodology can be used to shed light on the effects of exposure bias and training objectives. Wang and Sennrich (2020) empirically link the hallucination mode to exposure bias (Ranzato et al., 2016), i.e. the mismatch between the gold history seen at training time, and the (potentially erroneous) model-generated prefixes at test time. The authors hypothesize that exposure bias leads to an over-reliance on target history, and show that Minimum Risk Training (MRT), which does not suffer from exposure bias, reduces hallucinations. However, they did not directly measure this overreliance on target history. Our method is able to directly test whether there is indeed an over-reliance on the target history with MLE-"
2021.acl-long.91,C18-1124,0,0.0223897,"med remains largely open. Namely, the generation of a target token is defined by two types of context, source and target, but there is no method which explicitly evaluates the relative contribution of source and target to a given prediction. The ability to measure this relative contribution is important for model understanding since previous work showed that NMT models often fail to effectively control information flow from source and target contexts. For example, adding context gates to dynamically control the influence of source and target leads to improvement for both RNN (Tu et al., 2017; Wang et al., 2018) and Transfomer (Li et al., 2020) models. A more popular example is a model’s tendency to generate hallucinations (fluent but inadequate translations); it is usually attributed to the inappropriately strong influence of target context. Several works observed that, when hallucinating, a model fails to properly use source: it produces a deficient attention matrix, where almost all the probability mass is concentrated on uninformative source tokens (EOS and punctuation) (Lee et al., 2018; Berard et al., 2019). We argue that a natural way to estimate how the source and target contexts contribute t"
2021.blackboxnlp-1.5,2020.findings-emnlp.345,0,0.0148118,"to et al., 2016). With regard to syntactic evaluation of language models, Newman et al. (2021) point out that contrastive evaluation and evaluation of systematicity across a paradigm do not necessarily describe a model’s likely behavior. They propose to analyze the complete search space, which, however, is difficult to implement in many use cases. We pursue a different strategy and create minimal pairs that are more similar to sequences the model will likely generate at deployment time. 3 3. Polarity affix deletion: NMT systems sometimes omit negation affixes, changing the polarity of a word (Hossain et al., 2020). Specifically, English–German models sometimes omit the negation prefix un- from German words (Sennrich, 2017). 4. Clause omission: NMT systems sometimes omit a clause from the translated sentence (Tu et al., 2016). 3.1 Test Set Creation For each of the four hypotheses, we create an English–German contrastive test set. For vague language, polarity affix deletion and clause omission, we use the newstest datasets 2009–2016 as a data source. For hypercorrection, we combine five data sources: newstest 2009–2019 as well as OpenSubtitles2016 (Lison and Tiedemann, Experiments In previous work, contr"
2021.blackboxnlp-1.5,W19-5301,0,0.0507793,"Missing"
2021.blackboxnlp-1.5,D16-1139,0,0.127807,"results. As a main factor we identify the distributional discrepancy of contrastive evaluation datasets: Minimal pairs are usually derived from human-written references, but when deployed, a model is conditioned on its own output. To measure the effect of this factor on evaluation, we focus on neural machine translation (NMT) systems. Our approach is to test implausible research hypotheses in addition to plausible ones. We find that distributional discrepancy increases the number of false positives regarding implausible hypotheses. They particularly occur when evaluating distilled NMT models (Kim and Rush, 2016), indicating that in such models, ranking behavior on noisy sequences diverges from generative behavior. We also propose a way to reduce the distributional discrepancy of minimal pairs. Our experiments show that false positives can be largely avoided by using machine-generated text instead of human-written text. This inspires us to release DistilLingEval, a variant of the LingEval97 English–German MT evaluation suite (Sennrich, 2017) that uses MT-generated references. We recommend that future efforts to create contrastive datasets for the evaluation of language generation models minimize distr"
2021.blackboxnlp-1.5,Q16-1037,0,0.121174,"contrastive evaluation can lead to false positives. Secondly, test data should be chosen such as to minimize distributional discrepancy between evaluation time and deployment time. For a good approximation of deployment-time decoding, we recommend that minimal pairs are created based on machine-generated text, as opposed to humanwritten references. We present a contrastive evaluation suite for English–German MT that implements this recommendation.1 1 Introduction Contrastive evaluation is one of the most widely used evaluation techniques for generative language models, both for causal models (Linzen et al., 2016) and sequence-to-sequence models (Sennrich, 2017). Various phenomena have been analyzed using this technique, including syntax (Marvin and Linzen 2018; among others), word sense disambiguation (Rios et al. 2017; among others), document coherence (Bawden et al. 2018; Beyer et al. 2021; among others), and grammatical acceptability in general (Warstadt et al., 2020; Xiang et al., 2021). Contrastive evaluation allows for a targeted, automated evaluation of generative models but is restricted to a specific behavioral interface, namely the ranking of pre-defined minimal pairs. However, most models i"
2021.blackboxnlp-1.5,N18-1118,1,0.917423,"are created based on machine-generated text, as opposed to humanwritten references. We present a contrastive evaluation suite for English–German MT that implements this recommendation.1 1 Introduction Contrastive evaluation is one of the most widely used evaluation techniques for generative language models, both for causal models (Linzen et al., 2016) and sequence-to-sequence models (Sennrich, 2017). Various phenomena have been analyzed using this technique, including syntax (Marvin and Linzen 2018; among others), word sense disambiguation (Rios et al. 2017; among others), document coherence (Bawden et al. 2018; Beyer et al. 2021; among others), and grammatical acceptability in general (Warstadt et al., 2020; Xiang et al., 2021). Contrastive evaluation allows for a targeted, automated evaluation of generative models but is restricted to a specific behavioral interface, namely the ranking of pre-defined minimal pairs. However, most models in application areas such as translation or conversation are deployed to produce 1-best sequences, exposing a different behavioral interface to users. While this limitation of contrastive 2 2.1 Background and Related Work Contrastive Evaluation Contrastive evaluatio"
2021.blackboxnlp-1.5,W17-4702,1,0.907316,"yment-time decoding, we recommend that minimal pairs are created based on machine-generated text, as opposed to humanwritten references. We present a contrastive evaluation suite for English–German MT that implements this recommendation.1 1 Introduction Contrastive evaluation is one of the most widely used evaluation techniques for generative language models, both for causal models (Linzen et al., 2016) and sequence-to-sequence models (Sennrich, 2017). Various phenomena have been analyzed using this technique, including syntax (Marvin and Linzen 2018; among others), word sense disambiguation (Rios et al. 2017; among others), document coherence (Bawden et al. 2018; Beyer et al. 2021; among others), and grammatical acceptability in general (Warstadt et al., 2020; Xiang et al., 2021). Contrastive evaluation allows for a targeted, automated evaluation of generative models but is restricted to a specific behavioral interface, namely the ranking of pre-defined minimal pairs. However, most models in application areas such as translation or conversation are deployed to produce 1-best sequences, exposing a different behavioral interface to users. While this limitation of contrastive 2 2.1 Background and Re"
2021.blackboxnlp-1.5,2020.lrec-1.457,0,0.0148767,"eisen? German (contrastive): Und selbst wenn man das für den Menschen beweisen könnte: Vague language Contrastive variants are created by replacing a random noun in each reference with an uninflected Ding ‘thing’, which is a common replacement noun in spoken German (Vogel, 2020): 3.2 Human-Written References The above test sets are derived from naturally occurring parallel text, which is common practice when creating contrastive datasets for MT (Sennrich, 2017; Rios et al., 2017; Bawden et al., 2018; Müller et al., 2018; Voita et al., 2019; Raganato et al., 2019; Sugiyama and Yoshinaga, 2019; Nagata and Morishita, 2020; Shimazu et al., 2020; Lopes et al., 2020; He et al., 2020; Stojanovski et al., 2020). However, comparisons have shown that human-written references are different from machine translations in that they contain more noise and have more linguistic diversity (Zhang et al., 2018; Vanmassenhove et al., 2019). We propose to measure the “distance” between a pre-defined target sequence and the 1-best translation Yˆ generated by an MT system as the difference in log-scores (according to Equation 2) that the system assigns to the two sequences. Furthermore, we define the distributional discrepancy of a"
2021.blackboxnlp-1.5,E17-2060,1,0.376431,"ributional discrepancy increases the number of false positives regarding implausible hypotheses. They particularly occur when evaluating distilled NMT models (Kim and Rush, 2016), indicating that in such models, ranking behavior on noisy sequences diverges from generative behavior. We also propose a way to reduce the distributional discrepancy of minimal pairs. Our experiments show that false positives can be largely avoided by using machine-generated text instead of human-written text. This inspires us to release DistilLingEval, a variant of the LingEval97 English–German MT evaluation suite (Sennrich, 2017) that uses MT-generated references. We recommend that future efforts to create contrastive datasets for the evaluation of language generation models minimize distributional discrepancy between evaluation and deployment. Due to the possibility of false positives, linguistic conclusions about knowledge or abilities of models should be corroborated by additional evidence from a more natural setting. Minimal sentence pairs are frequently used to analyze the behavior of language models. It is often assumed that model behavior on contrastive pairs is predictive of model behavior at large. We argue t"
2021.blackboxnlp-1.5,2021.naacl-main.290,0,0.0429492,"score(Y |X) = 1 X log pθ (yi |X, y<i ) |Y | (2) i=0 2.2 Limitations of Forced Choice Since a limited set of variants is scored, contrastive evaluation presents the model with a forced choice. In fact, scoring a pre-defined sequence is related to teacher forcing, i.e., the conditioning of a model on a ground truth prefix during training. Whenever an application involves unconstrained generation, a discrepancy between evaluation and deployment arises that is comparable to the exposure bias of cross-entropy training (Ranzato et al., 2016). With regard to syntactic evaluation of language models, Newman et al. (2021) point out that contrastive evaluation and evaluation of systematicity across a paradigm do not necessarily describe a model’s likely behavior. They propose to analyze the complete search space, which, however, is difficult to implement in many use cases. We pursue a different strategy and create minimal pairs that are more similar to sequences the model will likely generate at deployment time. 3 3. Polarity affix deletion: NMT systems sometimes omit negation affixes, changing the polarity of a word (Hossain et al., 2020). Specifically, English–German models sometimes omit the negation prefix"
2021.blackboxnlp-1.5,W19-5333,0,0.0141912,"uced to a degree, we see it is a desired bias, since our goal is to reduce distributional discrepancy between a test set and the generative behavior of an evaluated system. 3.4 Experimental Setting We evaluate two types of NMT systems: 1. T RANSFORMER: Transformer models of size ‘big’ (Vaswani et al., 2017). 2. D ISTILLED: Transformer models of size ‘small’ distilled from (1) using sequence-level knowledge distillation (Kim and Rush, 2016). Training For both types, we trained three models with different random seeds. To train the T RANS FORMER models, we used similar data and configuration as Ng et al. (2019), using Fairseq (Ott et al., 2019). We used the English–German parallel training data from the WMT19 news translation task (Barrault et al., 2019). Sentences longer than 250 tokens and pairs with a length ratio larger than 1.5 were filtered, resulting in 42.9M sentence pairs used for training and distillation. We selected the Dataset Bias We discuss two kinds of bias that might be introduced. First of all, by only including machine references that can be classified automatically as either correct or incorrect based on the human references, the distribution of the machine61 Error type best chec"
2021.blackboxnlp-1.5,2021.acl-long.330,0,0.0140107,"en the two behavioral interfaces is especially high when human-written text is used to create minimal pairs. Using machinegenerated text largely reduces the gap. We recommend that human-written minimal pairs are mainly used for assessing the robustness of models, but that for predicting the generative behavior of language models, machine-generated minimal pairs are used. Broader Impact For language generation systems to be deployed, they should behave according to specified principles in a robust way. Typical requirements are linguistic acceptability, avoidance of undesirable societal biases (Sheng et al., 2021), and the avoidance of harmful speech acts. Contrastive evaluation is one of several methods that can help predict the behavior of language generation systems. However, to our knowledge the method has been mainly used to evaluate linguistic acceptability, and less to evaluate ethically sensitive aspects of generation. It is crucial that evaluation methods have a high predictiveness regarding the behavior of a deployed system. On the one hand, lack of sensitivity can lead to unforeseen negative impact. On the other hand, lack of specificity – which we address in this paper – reduces the usefuln"
2021.blackboxnlp-1.5,N19-4009,0,0.016601,"desired bias, since our goal is to reduce distributional discrepancy between a test set and the generative behavior of an evaluated system. 3.4 Experimental Setting We evaluate two types of NMT systems: 1. T RANSFORMER: Transformer models of size ‘big’ (Vaswani et al., 2017). 2. D ISTILLED: Transformer models of size ‘small’ distilled from (1) using sequence-level knowledge distillation (Kim and Rush, 2016). Training For both types, we trained three models with different random seeds. To train the T RANS FORMER models, we used similar data and configuration as Ng et al. (2019), using Fairseq (Ott et al., 2019). We used the English–German parallel training data from the WMT19 news translation task (Barrault et al., 2019). Sentences longer than 250 tokens and pairs with a length ratio larger than 1.5 were filtered, resulting in 42.9M sentence pairs used for training and distillation. We selected the Dataset Bias We discuss two kinds of bias that might be introduced. First of all, by only including machine references that can be classified automatically as either correct or incorrect based on the human references, the distribution of the machine61 Error type best checkpoint with respect to BLEU based"
2021.blackboxnlp-1.5,2020.lrec-1.447,0,0.0215825,"Und selbst wenn man das für den Menschen beweisen könnte: Vague language Contrastive variants are created by replacing a random noun in each reference with an uninflected Ding ‘thing’, which is a common replacement noun in spoken German (Vogel, 2020): 3.2 Human-Written References The above test sets are derived from naturally occurring parallel text, which is common practice when creating contrastive datasets for MT (Sennrich, 2017; Rios et al., 2017; Bawden et al., 2018; Müller et al., 2018; Voita et al., 2019; Raganato et al., 2019; Sugiyama and Yoshinaga, 2019; Nagata and Morishita, 2020; Shimazu et al., 2020; Lopes et al., 2020; He et al., 2020; Stojanovski et al., 2020). However, comparisons have shown that human-written references are different from machine translations in that they contain more noise and have more linguistic diversity (Zhang et al., 2018; Vanmassenhove et al., 2019). We propose to measure the “distance” between a pre-defined target sequence and the 1-best translation Yˆ generated by an MT system as the difference in log-scores (according to Equation 2) that the system assigns to the two sequences. Furthermore, we define the distributional discrepancy of a contrastive evaluatio"
2021.blackboxnlp-1.5,2020.coling-main.417,0,0.120712,"Vague language Contrastive variants are created by replacing a random noun in each reference with an uninflected Ding ‘thing’, which is a common replacement noun in spoken German (Vogel, 2020): 3.2 Human-Written References The above test sets are derived from naturally occurring parallel text, which is common practice when creating contrastive datasets for MT (Sennrich, 2017; Rios et al., 2017; Bawden et al., 2018; Müller et al., 2018; Voita et al., 2019; Raganato et al., 2019; Sugiyama and Yoshinaga, 2019; Nagata and Morishita, 2020; Shimazu et al., 2020; Lopes et al., 2020; He et al., 2020; Stojanovski et al., 2020). However, comparisons have shown that human-written references are different from machine translations in that they contain more noise and have more linguistic diversity (Zhang et al., 2018; Vanmassenhove et al., 2019). We propose to measure the “distance” between a pre-defined target sequence and the 1-best translation Yˆ generated by an MT system as the difference in log-scores (according to Equation 2) that the system assigns to the two sequences. Furthermore, we define the distributional discrepancy of a contrastive evaluation dataset as the mean difference in scores between the 1-best tr"
2021.blackboxnlp-1.5,2020.acl-demos.14,0,0.023937,"Missing"
2021.blackboxnlp-1.5,D19-6504,0,0.0202699,"wollte man es bei Ratten nachweisen? German (contrastive): Und selbst wenn man das für den Menschen beweisen könnte: Vague language Contrastive variants are created by replacing a random noun in each reference with an uninflected Ding ‘thing’, which is a common replacement noun in spoken German (Vogel, 2020): 3.2 Human-Written References The above test sets are derived from naturally occurring parallel text, which is common practice when creating contrastive datasets for MT (Sennrich, 2017; Rios et al., 2017; Bawden et al., 2018; Müller et al., 2018; Voita et al., 2019; Raganato et al., 2019; Sugiyama and Yoshinaga, 2019; Nagata and Morishita, 2020; Shimazu et al., 2020; Lopes et al., 2020; He et al., 2020; Stojanovski et al., 2020). However, comparisons have shown that human-written references are different from machine translations in that they contain more noise and have more linguistic diversity (Zhang et al., 2018; Vanmassenhove et al., 2019). We propose to measure the “distance” between a pre-defined target sequence and the 1-best translation Yˆ generated by an MT system as the difference in log-scores (according to Equation 2) that the system assigns to the two sequences. Furthermore, we define the dis"
2021.blackboxnlp-1.5,W19-5354,0,0.0167187,"n beweisen könnte: Wie wollte man es bei Ratten nachweisen? German (contrastive): Und selbst wenn man das für den Menschen beweisen könnte: Vague language Contrastive variants are created by replacing a random noun in each reference with an uninflected Ding ‘thing’, which is a common replacement noun in spoken German (Vogel, 2020): 3.2 Human-Written References The above test sets are derived from naturally occurring parallel text, which is common practice when creating contrastive datasets for MT (Sennrich, 2017; Rios et al., 2017; Bawden et al., 2018; Müller et al., 2018; Voita et al., 2019; Raganato et al., 2019; Sugiyama and Yoshinaga, 2019; Nagata and Morishita, 2020; Shimazu et al., 2020; Lopes et al., 2020; He et al., 2020; Stojanovski et al., 2020). However, comparisons have shown that human-written references are different from machine translations in that they contain more noise and have more linguistic diversity (Zhang et al., 2018; Vanmassenhove et al., 2019). We propose to measure the “distance” between a pre-defined target sequence and the 1-best translation Yˆ generated by an MT system as the difference in log-scores (according to Equation 2) that the system assigns to the two sequences."
2021.blackboxnlp-1.5,tiedemann-2012-parallel,0,0.0173055,"ion and clause omission, we use the newstest datasets 2009–2016 as a data source. For hypercorrection, we combine five data sources: newstest 2009–2019 as well as OpenSubtitles2016 (Lison and Tiedemann, Experiments In previous work, contrastive evaluation has commonly been used to test plausible research hypotheses, for example the hypothesis that RNNs can predict long-distance number agreement (Gulordava et al., 2018), or the hypothesis that word dropout 59 2016), TED2020 (Reimers and Gurevych, 2020), QED (Abdelali et al., 2014) and JW300 (Agi´c and Vuli´c, 2019), which are provided by OPUS (Tiedemann, 2012). German (correct): Und selbst wenn man das für den Menschen beweisen könnte: Wie wollte man es bei Ratten nachweisen? German (contrastive): Und selbst wenn man das für den Menschen beweisen könnte: Vague language Contrastive variants are created by replacing a random noun in each reference with an uninflected Ding ‘thing’, which is a common replacement noun in spoken German (Vogel, 2020): 3.2 Human-Written References The above test sets are derived from naturally occurring parallel text, which is common practice when creating contrastive datasets for MT (Sennrich, 2017; Rios et al., 2017; Baw"
2021.blackboxnlp-1.5,2020.emnlp-main.365,0,0.0150756,"r each of the four hypotheses, we create an English–German contrastive test set. For vague language, polarity affix deletion and clause omission, we use the newstest datasets 2009–2016 as a data source. For hypercorrection, we combine five data sources: newstest 2009–2019 as well as OpenSubtitles2016 (Lison and Tiedemann, Experiments In previous work, contrastive evaluation has commonly been used to test plausible research hypotheses, for example the hypothesis that RNNs can predict long-distance number agreement (Gulordava et al., 2018), or the hypothesis that word dropout 59 2016), TED2020 (Reimers and Gurevych, 2020), QED (Abdelali et al., 2014) and JW300 (Agi´c and Vuli´c, 2019), which are provided by OPUS (Tiedemann, 2012). German (correct): Und selbst wenn man das für den Menschen beweisen könnte: Wie wollte man es bei Ratten nachweisen? German (contrastive): Und selbst wenn man das für den Menschen beweisen könnte: Vague language Contrastive variants are created by replacing a random noun in each reference with an uninflected Ding ‘thing’, which is a common replacement noun in spoken German (Vogel, 2020): 3.2 Human-Written References The above test sets are derived from naturally occurring parallel te"
2021.blackboxnlp-1.5,W19-6622,0,0.0185935,"ences The above test sets are derived from naturally occurring parallel text, which is common practice when creating contrastive datasets for MT (Sennrich, 2017; Rios et al., 2017; Bawden et al., 2018; Müller et al., 2018; Voita et al., 2019; Raganato et al., 2019; Sugiyama and Yoshinaga, 2019; Nagata and Morishita, 2020; Shimazu et al., 2020; Lopes et al., 2020; He et al., 2020; Stojanovski et al., 2020). However, comparisons have shown that human-written references are different from machine translations in that they contain more noise and have more linguistic diversity (Zhang et al., 2018; Vanmassenhove et al., 2019). We propose to measure the “distance” between a pre-defined target sequence and the 1-best translation Yˆ generated by an MT system as the difference in log-scores (according to Equation 2) that the system assigns to the two sequences. Furthermore, we define the distributional discrepancy of a contrastive evaluation dataset as the mean difference in scores between the 1-best translation and the preferred variant: English: Prague Stock Market falls to minus by the end of the trading day German (correct): Die Prager Börse stürzt gegen Geschäftsschluss ins Minus. German (contrastive): Die Prager"
2021.blackboxnlp-1.5,P19-1116,1,0.890409,"Missing"
2021.blackboxnlp-1.5,2021.eacl-main.242,0,0.0235826,"suite for English–German MT that implements this recommendation.1 1 Introduction Contrastive evaluation is one of the most widely used evaluation techniques for generative language models, both for causal models (Linzen et al., 2016) and sequence-to-sequence models (Sennrich, 2017). Various phenomena have been analyzed using this technique, including syntax (Marvin and Linzen 2018; among others), word sense disambiguation (Rios et al. 2017; among others), document coherence (Bawden et al. 2018; Beyer et al. 2021; among others), and grammatical acceptability in general (Warstadt et al., 2020; Xiang et al., 2021). Contrastive evaluation allows for a targeted, automated evaluation of generative models but is restricted to a specific behavioral interface, namely the ranking of pre-defined minimal pairs. However, most models in application areas such as translation or conversation are deployed to produce 1-best sequences, exposing a different behavioral interface to users. While this limitation of contrastive 2 2.1 Background and Related Work Contrastive Evaluation Contrastive evaluation compares the probability scores that a model assigns to two minimally dif1 https://github.com/ZurichNLP/ distil-lingev"
2021.eacl-demos.32,P19-1126,0,0.361622,"Missing"
2021.eacl-demos.32,D14-1140,0,0.362092,"Missing"
2021.eacl-demos.32,2020.iwslt-1.27,0,0.152889,"among the research partners in our project, e.g. Hindi. The scientific motivation for our efforts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quali"
2021.eacl-demos.32,E17-1099,0,0.269847,"rts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quality of the output translations using BLEU and WER metrics. Zheng et al. (2019) propose"
2021.eacl-demos.32,N12-1048,0,0.134304,"m the secured networks of the labs so it usually does not run into firewall issues. tions of the EU and nearby countries. Experimentally, we include also other languages based on available systems among the research partners in our project, e.g. Hindi. The scientific motivation for our efforts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speec"
2021.eacl-demos.32,2020.eamt-1.53,1,0.493793,"Missing"
2021.eacl-demos.32,C18-2020,1,0.876378,"Missing"
2021.eacl-demos.32,2020.iwslt-1.25,1,0.823962,"Missing"
2021.eacl-demos.32,N16-3017,1,0.800534,"Missing"
2021.eacl-demos.32,2020.acl-main.148,1,0.8252,"tem in end-to-end fashion and face engineering problems and technical issues on all layers from sound acquisition through network connections, worker configuration to subtitle presentation. • We are currently running a user study with nonGerman speakers watching German videos with our online subtitles, see Section 7.1. We aim to measure the comprehension loss caused by different subtitling options, latency or flicker. 42 languages (Johnson et al., 2017). The models are mostly Transformers (Vaswani et al., 2017) but we improve their performance in massively multilingual setting by extra depth (Zhang et al., 2020). 5.3 Interplay of ASR and MT Connecting ASR and MT systems is not straightforward because MT systems assume input in the form of complete sentences. We follow the strategy of Niehues et al. (2016), first inserting punctuation into the stream of tokens coming from ASR (Tilk and Alum¨ae, 2016), breaking it up at full stops and sending individual sentences to MT, either as unfinished sentence prefixes, or complete sentences. We are using re-translation, as ASR or punctuation updates are received. Currently, the main problem is that punctuation prediction does not have access to the sound any mor"
2021.eacl-demos.32,D19-1137,0,0.29802,"product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quality of the output translations using BLEU and WER metrics. Zheng et al. (2019) proposed a new approach with a delay-based heuristic. The model decides to read more input (or wait for it) or wri"
2021.eacl-demos.32,2020.findings-emnlp.349,0,0.0269839,"text available, the user does not see sufficient number of words to let the brain “make up” or reconstruct the original meaning from pieces. The short-term memory of recently processed text does not seem to be sufficient for this type recovery, while seeing the words in larger context gives the user a better chance. The last step in an SLT system is the delivery of the translated content to the user. Our goal stops at the textual representation, i.e. we do not include speech synthesis and delivery of the sound, which would bring yet another set of design decisions and open problems, see e.g. Zheng et al. (2020). We experiment with two different views for our text output, both implemented as web applications. The “subtitle view” is optimized toward minimal use of screen space. Only two lines of text are available which leaves room either for e.g. a streamed video of the session or the slides, or for many languages displayed at once, if the screen is intended for a multi-lingual audience. The “paragraph view” provides more textual context to the user. 7.1 Subtitle View The subtitle view offers a simple interface with a HLS stream of the video or slides and one or more subtitles streams. Section 7.1 pr"
2021.eacl-demos.32,W19-5337,1,0.807801,"latency and hypotheses updates, as in KIT Lecture Translator (M¨uller et al., 2016). We use the hybrid ASR models based on Janus from KIT Lecture Translator, for German and English, as well as recent neural sequence-to-sequence ASR models trained on the same data (Nguyen et al., 2020). For Czech ASR, we use a Kaldi hybrid model trained on a Corpus of Czech Parliament Plenary Hearings (Kratochv´ıl et al., 2019). Czech sequence-to-sequence ASR is a work in progress. 5.2 MT Systems in ELITR We use bilingual NMT models for some high resource and well-studied language pairs e.g. for English-Czech (Popel et al., 2019; Wetesko et al., 2019). For other targets, we use multi-target models, e.g. an English-centric universal model for ELITR Flexible Architecture We always strive for the best performance for each considered language pair. With the perpetual com272 Index Name auto-iwslt2020-antrecorp(ASR) auto-iwslt2020-antrecorp(MT) auto-iwslt2020-antrecorp(MT) auto-asr-english-auditing(ASR) auto-asr-english-auditing(MT) auto-asr-english-auditing(MT) auto-iwslt2020-khanacademy(ASR) Worker en-EU-lecture KIT-s2s rb-EU fromEN-en to 41 rb-EU fromEN-en to 41 en-EU-lecture KIT-s2s rb-EU fromEN-en to 41 rb-EU fromEN-e"
2021.emnlp-main.523,2020.emnlp-main.42,0,0.0859537,"reflects linguistic properties. native solution in this direction is to develop sparsified softmax alternatives, such as sparse- In machine translation, the encoder-decoder attenmax (Martins and Astudillo, 2016; Malaviya et al., tion captures the source-target word alignment to 2018), entmax (Peters et al., 2019; Correia et al., a certain degree (Ghader and Monz, 2017), with recent work further strengthening this via specific 2019), fusedmax (Niculae and Blondel, 2017), and induction methods (Ding et al., 2019; Kobayashi 2 Note that sparsified softmax variants also use some form et al., 2020; Chen et al., 2020). We apply analyof ReLU to achieve sparsity, but they stick to the probabilistic constraint which demands extra complexity. sis techniques from previous work to analyze our 6508 models. 3 Background: Attention in Transformer Many variants of attention mechanism have been developed since its first proposal (Bahdanau et al., 2015; Luong et al., 2015). In this paper, we focus on the one used by Transformer, namely multihead scaled dot-product attention (MHATT), in an encoder-decoder setup. Given query inputs X ∈ Rn×d and a sequence of context items Y ∈ Rm×d , each head in MHATT summarizes query-r"
2021.emnlp-main.523,D15-1166,0,0.0686734,"Ghader and Monz, 2017), with recent work further strengthening this via specific 2019), fusedmax (Niculae and Blondel, 2017), and induction methods (Ding et al., 2019; Kobayashi 2 Note that sparsified softmax variants also use some form et al., 2020; Chen et al., 2020). We apply analyof ReLU to achieve sparsity, but they stick to the probabilistic constraint which demands extra complexity. sis techniques from previous work to analyze our 6508 models. 3 Background: Attention in Transformer Many variants of attention mechanism have been developed since its first proposal (Bahdanau et al., 2015; Luong et al., 2015). In this paper, we focus on the one used by Transformer, namely multihead scaled dot-product attention (MHATT), in an encoder-decoder setup. Given query inputs X ∈ Rn×d and a sequence of context items Y ∈ Rm×d , each head in MHATT summarizes query-relevant context information as follows: SMATT (X, Y) = αV, with α = Softmax f Q, KT  , (1) with Q = XWq ; K, V = YWk , YWv , where n and m are the query and context length, respectively; d and dh are the model and head dimension, respectively; W∗ ∈ Rd×dh denotes trainable model parameters. α ∈ Rn×m is the attention weight, which estimates the deg"
2021.emnlp-main.523,P18-2059,0,0.0360198,"Missing"
2021.emnlp-main.523,P00-1056,0,0.642798,"ayer attention (bottom figures). Results in Table 5 further show that the behaviour of ReLA-g is more alignment-like than the baselines we consider. Head Diversity We evaluate head diversity with a generalization of Jensen-Shannon divergence following (Correia et al., 2019) to reflect disCross Attention vs. Word Alignment We ex- agreements between heads. For ReLA-g, we reperiment with the publicly available De-En eval- normalize its attention scores via softmax, and reuation set9 and evaluate the alignment quality gard the null attention as a special one-hot distribuwith alignment error rate (Och and Ney, 2000, tion putting all probability mass to a dummy zero AER). We study normal attention and shifted atten- vector, i.e. entropy of 0. tion following previous work (Chen et al., 2020; Figure 5 shows the results. We observe that the Kobayashi et al., 2020). The former explores at- heads of the encoder self-attention exhibit much tention weights corresponding to decoder outputs higher disagreement than those of the other two (i.e. α in Eq. 1 and 3); the latter, by contrast, skips attention types for all sparsified attention models. the weights at the first decoding step, i.e. α[1 :], to Overall, head"
2021.emnlp-main.523,P02-1040,0,0.109315,"aky rectified linear unit (Xu et al., 2015). Baseline: Transformer. “-”: optimization failed, where training loss didn’t decrease. Higher BLEU indicates better result. 2018, Zh-En, 25M), and WMT16 RomanianEnglish (Bojar et al., 2016, Ro-En, 608K). We evaluate on the official test set from the corresponding year (e.g. newstest2014 for WMT14), and regard the previous year’s test set as the development set (e.g. newstest2013 for WMT14). We preprocess all datasets using the byte pair encoding algorithm (Sennrich et al., 2016) with 32K merging operations. We report detokenized case-sensitive BLEU (Papineni et al., 2002) implemented by SacreBLEU (Post, 2018),5 and also show tokenized case-sensitive BLEU with multi-bleu.perl for ablation studies. Model Configuration We use the Transformer base setting for experiments: model dimension d = 512, head number H = 8, head dimension dh = 64, 6 layers and FFN size of 2048 (Vaswani et al., 2017). We apply dropout to the residual connections and attention weights, with a rate of 0.1. We tune model parameters using Adam (Kingma and Ba, 2015, β1 = 0.9, β2 = 0.98) with label smoothing of 0.1. We schedule the learning rate following Vaswani et al. (2017) with a warmup step"
2021.emnlp-main.523,P19-1146,0,0.737498,"tes the analysis of the information flow in the model, and has led researchers to study sparse alternatives, which often lead to improved model performance and/or interpretabil1 Introduction ity (Correia et al., 2019). Efforts in this category Attention models (Bahdanau et al., 2015) have include designing fixed sparsity patterns (Raganato been hugely successful recently, with Trans- et al., 2020; Child et al., 2019) and creating sparsiformer (Vaswani et al., 2017) in particular, advanc- fied softmax variants (Martins and Astudillo, 2016; ing state of the art on various tasks, such as machine Peters et al., 2019). However, these methods also translation (Bojar et al., 2018), document summa- have drawbacks. Fixed sparsity patterns lack flexrization (Liu and Lapata, 2019) and speech process- ibility and generalize poorly across tasks. Sparing (Chiu et al., 2018), and delivering a large im- sified softmax variants often depend on complex pact on a broad range of NLP tasks via large-scale inference algorithms (e.g., requiring the sorting self-supervised pretraining (Devlin et al., 2019). operation), which reduces their efficiency. 1 In this paper, we propose rectified linear attenSource code is available"
2021.emnlp-main.523,W18-6319,0,0.0286511,"line: Transformer. “-”: optimization failed, where training loss didn’t decrease. Higher BLEU indicates better result. 2018, Zh-En, 25M), and WMT16 RomanianEnglish (Bojar et al., 2016, Ro-En, 608K). We evaluate on the official test set from the corresponding year (e.g. newstest2014 for WMT14), and regard the previous year’s test set as the development set (e.g. newstest2013 for WMT14). We preprocess all datasets using the byte pair encoding algorithm (Sennrich et al., 2016) with 32K merging operations. We report detokenized case-sensitive BLEU (Papineni et al., 2002) implemented by SacreBLEU (Post, 2018),5 and also show tokenized case-sensitive BLEU with multi-bleu.perl for ablation studies. Model Configuration We use the Transformer base setting for experiments: model dimension d = 512, head number H = 8, head dimension dh = 64, 6 layers and FFN size of 2048 (Vaswani et al., 2017). We apply dropout to the residual connections and attention weights, with a rate of 0.1. We tune model parameters using Adam (Kingma and Ba, 2015, β1 = 0.9, β2 = 0.98) with label smoothing of 0.1. We schedule the learning rate following Vaswani et al. (2017) with a warmup step of 4K. Each training batch contains ar"
2021.emnlp-main.523,2020.findings-emnlp.49,0,0.01865,"eries, even allowing for null attention (all attention scores are zero) for some queries. We provide experimental results for ReLA with Transformer on five machine translation tasks, along with an in-depth analysis on WMT14 English-German task. Our contributions are summarized below: hashing/clustering-based variants (Roy et al., 2020; Kitaev et al., 2020). These models often require dedicated algorithms for forward and backward propagation, at the cost of a significant computational overhead. Another strategy is to manually define sparse patterns inspired by task-specific attention analysis. Raganato et al. (2020) corroborated the feasibility of fixed patterns for Transformer encoder in translation. Child et al. (2019) introduced local and strided patterns to scale SMATT up to very long inputs. Unlike data-driven approaches, whether these patterns could generalize to different tasks and settings is still an open question. • We propose ReLA, a drop-in SMATT alternative, that learns sparse attention automatically with high flexibility and efficiency. A different type of linear attention model is proposed by Katharopoulos et al. (2020) and Choromanski et al. (2020), who aim at reducing the O(n2 ) complexi"
2021.emnlp-main.523,P16-1162,1,0.360181,"s on WMT14 En-De. GeLU: Gaussian error linear unit (Hendrycks and Gimpel, 2016); Leaky ReLU: leaky rectified linear unit (Xu et al., 2015). Baseline: Transformer. “-”: optimization failed, where training loss didn’t decrease. Higher BLEU indicates better result. 2018, Zh-En, 25M), and WMT16 RomanianEnglish (Bojar et al., 2016, Ro-En, 608K). We evaluate on the official test set from the corresponding year (e.g. newstest2014 for WMT14), and regard the previous year’s test set as the development set (e.g. newstest2013 for WMT14). We preprocess all datasets using the byte pair encoding algorithm (Sennrich et al., 2016) with 32K merging operations. We report detokenized case-sensitive BLEU (Papineni et al., 2002) implemented by SacreBLEU (Post, 2018),5 and also show tokenized case-sensitive BLEU with multi-bleu.perl for ablation studies. Model Configuration We use the Transformer base setting for experiments: model dimension d = 512, head number H = 8, head dimension dh = 64, 6 layers and FFN size of 2048 (Vaswani et al., 2017). We apply dropout to the residual connections and attention weights, with a rate of 0.1. We tune model parameters using Adam (Kingma and Ba, 2015, β1 = 0.9, β2 = 0.98) with label smoo"
2021.emnlp-main.523,P18-1117,1,0.846856,"y ensuring the sparse property of the attention weight α ∈ Rn×m . Besides, ReLA allows for null attention, where it assigns zero scores to all context items (i.e. some rows of α are zero vectors), effectively switching off the corresponding attention head for certain queries. Nevertheless, the outputs of ReLU in Eq. 3 are often of different scales and varied variance, causing gradient instability and also optimization failure. Stabilization with Normalization A common strategy in deep learning to stabilize neuron activations is to apply layer normalization LN(·) (Ba 3 As an anecdotal example, Voita et al. (2018) performed an analysis of attention to previous sentences in MT, and found that the model has learned to generally attend to the end-ofsentence symbol as a way to ignore context. While this might be an effective strategy for instances where context matters little, this reduces the interpretability of attention. We argue that the use of the softmax function in SMATT (Eq. 1) has two undesirable consequences: 6509 et al., 2016). We follow this strategy and normalize each representation z ∈ Rdh in the attention outputs (αV) with root mean square layer normalization (Zhang and Sennrich, 2019, RMSNo"
2021.emnlp-main.523,P19-1580,1,0.83075,"ose of the other two (i.e. α in Eq. 1 and 3); the latter, by contrast, skips attention types for all sparsified attention models. the weights at the first decoding step, i.e. α[1 :], to Overall, heads in ReLA-g are in less agreement offset the left padding to the decoder inputs made than with the sparsified softmax alternatives, in for auto-regressive generation in Transformer. most cases across different attention types. This Figure 4 shows the results. Regardless of the indicates that ReLA-g is capable of inducing heads attention type (normal or shifted), attention re- with different roles (Voita et al., 2019). 9 Note we convert the attention scores of ReLA-g https://www-i6.informatik.rwth-aachen. de/goldAlignment/ into categorical distribution via softmax for diver6513 JS Divergence Encoder Attention Decoder Attention Cross Attention softmax sparsemax 1.5-entmax ReLA-g 1.0 0.5 1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6 Layer Index Figure 5: Jensen-Shannon (JS) Divergence over heads at each layer for different attention models and types on the WMT14 En-De test set. Higher JS Divergence indicates higher head diversity. Encoder Attention Decoder Attention Cross Attention 0.6 Null Rate 0.5 0.4 0.3 0.2 0.1 1"
2021.emnlp-main.523,D19-1002,0,0.039206,"Missing"
2021.emnlp-main.667,W18-6315,0,0.344008,"ining 1/3 as a held-out set for analysis (see Section 3.3). More details on hyperparameters, preprocessing, and training can be found in the appendix. 3.2 Target-Side LM Scores For each of the models, we train 2-, 3-, 4- and 5gram KenLM (Heafield, 2011)2 language models on target sides of the corresponding training data (segmented with BPE). We report KenLM scores for the translations of the development sets. 3.3 Monotonicity of Alignments To measure how the relative ordering of words in the source and its translation changes during training, we use two different scores used in previous work (Burlot and Yvon, 2018; Zhou et al., 2020). We evaluate the scores for two permutations of the source: the trivial monotonic alignment and the alignment inferred for the generated translation. Fuzzy Reordering Score (Talbot et al., 2011) counts the number of chunks of contiguously aligned words and, intuitively, it is based on the number of times a reader would need to jump in order to read one reordering in the order proposed by the other. The score is between 0 and 1, where a larger score indicates more monotonic alignments. Kendall tau distance (Kendall, 1938) is also called bubble-sort distance since it is equi"
2021.emnlp-main.667,2016.amta-researchers.10,0,0.017897,"translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesting in light of a recent work analyzing"
2021.emnlp-main.667,2020.emnlp-main.42,0,0.0125496,"er resources due to only partial training of the teachers. 7 Additional Related Work Other work connecting neural and traditional approaches include modeling modifications, such as modeling coverage and/or fertility (Tu et al., 2016; Mi et al., 2016a; Cohn et al., 2016; Feng et al., 2016) and several other modifications (Zhang et al., 2017b; Stahlberg et al., 2017; Huang et al., 2018), analysis of the relation between attention and word alignments (Ghader and Monz, 2017), and word alignment induction from NMT models (Li et al., 2019; Garg et al., 2019; Song et al., 2020b; Zenkel et al., 2020; Chen et al., 2020). Previous analysis of NMT learning dynamics include analyzing how the trainable parameters affect an NMT model (Zhu et al., 2020) and looking at the speed of learning specific discourse phenomena in context-aware NMT (Voita et al., 2019b,a). 8 Conclusions We analyze how NMT acquires different competencies during training and look at the competencies related to three core SMT components. We find 6 We used the code and the data from https:// that NMT first focuses on learning target-side langithub.com/pytorch/fairseq/tree/master/ examples/nonautoregressive_translation. guage modeling, then impr"
2021.emnlp-main.667,N16-1102,0,0.0223665,"lysis can be useful in the settings where data complexity matters and, therefore, limit ourselves to only using different teacher checkpoints. Future work, however, can investigate possible combinations with other approaches. For example, to further improve quality, our method can be combined with the Born-Again networks while still requiring fewer resources due to only partial training of the teachers. 7 Additional Related Work Other work connecting neural and traditional approaches include modeling modifications, such as modeling coverage and/or fertility (Tu et al., 2016; Mi et al., 2016a; Cohn et al., 2016; Feng et al., 2016) and several other modifications (Zhang et al., 2017b; Stahlberg et al., 2017; Huang et al., 2018), analysis of the relation between attention and word alignments (Ghader and Monz, 2017), and word alignment induction from NMT models (Li et al., 2019; Garg et al., 2019; Song et al., 2020b; Zenkel et al., 2020; Chen et al., 2020). Previous analysis of NMT learning dynamics include analyzing how the trainable parameters affect an NMT model (Zhu et al., 2020) and looking at the speed of learning specific discourse phenomena in context-aware NMT (Voita et al., 2019b,a). 8 Conclu"
2021.emnlp-main.667,W11-2123,0,0.0230619,"En-Ru – 2.5m sentence pairs (parallel training data excluding UN and Paracrawl). Since our observations are similar for both languages, in the main text we show figures for one of them and in the appendix – for the other. vocabulary of about 50k tokens for LM-style models. For each experiment, we randomly choose 2/3 of the dataset for training and use the remaining 1/3 as a held-out set for analysis (see Section 3.3). More details on hyperparameters, preprocessing, and training can be found in the appendix. 3.2 Target-Side LM Scores For each of the models, we train 2-, 3-, 4- and 5gram KenLM (Heafield, 2011)2 language models on target sides of the corresponding training data (segmented with BPE). We report KenLM scores for the translations of the development sets. 3.3 Monotonicity of Alignments To measure how the relative ordering of words in the source and its translation changes during training, we use two different scores used in previous work (Burlot and Yvon, 2018; Zhou et al., 2020). We evaluate the scores for two permutations of the source: the trivial monotonic alignment and the alignment inferred for the generated translation. Fuzzy Reordering Score (Talbot et al., 2011) counts the numbe"
2021.emnlp-main.667,D16-1139,0,0.0235228,"eously. This is possible only with an underlying assumption that the output tokens are independent from each other, which is unrealistic for natural language. Fortunately, while this independence assumption is unrealistic for real references, it might be more plausible for simpler sequences, e.g. artificially generated translations. That is why targets for NAT models are usually not references but beam search translations of the standard autoregressive NMT (which, as we already mentioned above, are simpler than references in many aspects). This is called sequence-level knowledge distillation (Kim and Rush, 2016), and it is currently one of the de-facto standard parts of the NAT training pipelines (Gu et al. (2018); Lee et al. (2018); Ghazvininejad et al. (2019) to name a few). Recently Zhou et al. (2020) showed that the quality of a NAT model strongly depends on the We showed that during a large part of the training, complexity of the distilled data, and changing this the translation quality (e.g., BLEU) changes little, complexity can improve the model. Since distilled but the alignments become less monotonic. Intu- data consists of translations from a standard auitively, the translations become more"
2021.emnlp-main.667,D18-1149,0,0.020969,"unrealistic for natural language. Fortunately, while this independence assumption is unrealistic for real references, it might be more plausible for simpler sequences, e.g. artificially generated translations. That is why targets for NAT models are usually not references but beam search translations of the standard autoregressive NMT (which, as we already mentioned above, are simpler than references in many aspects). This is called sequence-level knowledge distillation (Kim and Rush, 2016), and it is currently one of the de-facto standard parts of the NAT training pipelines (Gu et al. (2018); Lee et al. (2018); Ghazvininejad et al. (2019) to name a few). Recently Zhou et al. (2020) showed that the quality of a NAT model strongly depends on the We showed that during a large part of the training, complexity of the distilled data, and changing this the translation quality (e.g., BLEU) changes little, complexity can improve the model. Since distilled but the alignments become less monotonic. Intu- data consists of translations from a standard auitively, the translations become more complicated toregressive teacher, our analysis gives a very simwhile their quality remains roughly the same. ple way of mo"
2021.emnlp-main.667,P19-1124,0,0.0181308,"method can be combined with the Born-Again networks while still requiring fewer resources due to only partial training of the teachers. 7 Additional Related Work Other work connecting neural and traditional approaches include modeling modifications, such as modeling coverage and/or fertility (Tu et al., 2016; Mi et al., 2016a; Cohn et al., 2016; Feng et al., 2016) and several other modifications (Zhang et al., 2017b; Stahlberg et al., 2017; Huang et al., 2018), analysis of the relation between attention and word alignments (Ghader and Monz, 2017), and word alignment induction from NMT models (Li et al., 2019; Garg et al., 2019; Song et al., 2020b; Zenkel et al., 2020; Chen et al., 2020). Previous analysis of NMT learning dynamics include analyzing how the trainable parameters affect an NMT model (Zhu et al., 2020) and looking at the speed of learning specific discourse phenomena in context-aware NMT (Voita et al., 2019b,a). 8 Conclusions We analyze how NMT acquires different competencies during training and look at the competencies related to three core SMT components. We find 6 We used the code and the data from https:// that NMT first focuses on learning target-side langithub.com/pytorch/fairse"
2021.emnlp-main.667,C16-1291,0,0.0204142,"ve neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesting in light of a re"
2021.emnlp-main.667,D16-1096,0,0.114135,"la nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesti"
2021.emnlp-main.667,D16-1249,0,0.102271,"la nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesti"
2021.emnlp-main.667,N19-4009,0,0.0257403,"g Following previous work (Zhou et al., 2020), we train the same NAT model on their preprocessed dataset6 and vary only distilled targets. Model. The model is the re-implemented by Zhou et al. (2020) version of the vanilla NAT by Gu et al. (2018). For more details, see appendix. Dataset. The dataset is WMT14 English-German (En-De) with newstest2013 as the validation set and newstest2014 as the test set, and BPE vocabulary of 37,000. We use the preprocessed dataset and the vocabularies released by Zhou et al. (2020). Distilled targets. The teacher is the standard Transformer-base from fairseq (Ott et al., 2019). For the baseline distilled dataset, we use the fully converged model (in this case, the model after 200k updates). For other datasets, we use earlier checkpoints. Evaluation. We average the last 10 checkpoints. 6.3 Experiments Figure 8c shows the BLEU scores for NAT models trained with distilled data obtained from different teacher’s checkpoints; the baseline is the fully converged model (200k iterations). We see that by taking an earlier checkpoint, after 40k iterations, we improve NAT quality by 1.1 BLEU. For this checkpoint, the teacher’s BLEU score is not much lower than that of the fina"
2021.emnlp-main.667,D19-5626,0,0.0145721,"petences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesting in light of a recent work analyzing how NMT balances the two different types of context: the source and pre"
2021.emnlp-main.667,P16-1162,1,0.275811,"ere 0 indicates the monotonic alignment. The main difference between the scores is that the first one takes into account only the number of jumps, while the second also considers their distance. For a formal description of the scores and their differences, see the appendix. Our setting. For each of the considered model checkpoints, we obtain datasets where the sources come from the held-out 1/3 of the original dataset, and targets are their translations. For these datasets, we infer alignments using fast_align (Dyer et al., 2013)3 . Preprocessing. The data is lowercased and encoded using BPE (Sennrich et al., 2016). We use separate source and target vocabularies of about 32k tokens for encoder-decoder models, and a joint 8480 2 3 https://github.com/kpu/kenlm https://github.com/clab/fast_align (a) (b) Figure 2: (a) KenLM scores (horizontal dashed lines are the scores for the references); (b) proportion of tokens of different frequency ranks in model translations. En-Ru. Figure 3: Translations at different steps during training. En-De. 4 Transformer Training Stages In this section, we discuss the standard encoderdecoder Transformer. In the next section, we mention differences with several other models. We"
2021.emnlp-main.667,W18-6321,0,0.0237275,"be used to improve vanilla nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is esp"
2021.emnlp-main.667,E17-2058,0,0.0612973,"Missing"
2021.emnlp-main.667,W11-2102,0,0.0290503,", 4- and 5gram KenLM (Heafield, 2011)2 language models on target sides of the corresponding training data (segmented with BPE). We report KenLM scores for the translations of the development sets. 3.3 Monotonicity of Alignments To measure how the relative ordering of words in the source and its translation changes during training, we use two different scores used in previous work (Burlot and Yvon, 2018; Zhou et al., 2020). We evaluate the scores for two permutations of the source: the trivial monotonic alignment and the alignment inferred for the generated translation. Fuzzy Reordering Score (Talbot et al., 2011) counts the number of chunks of contiguously aligned words and, intuitively, it is based on the number of times a reader would need to jump in order to read one reordering in the order proposed by the other. The score is between 0 and 1, where a larger score indicates more monotonic alignments. Kendall tau distance (Kendall, 1938) is also called bubble-sort distance since it is equivalent to the number of swaps that the bubble sort algorithm would take to place one list in the same order as the other list. We evaluate the normalized distance: it is between 0 and 1, where 0 indicates the monoto"
2021.emnlp-main.667,W19-6627,0,0.0318272,"Missing"
2021.emnlp-main.667,P16-1008,0,0.0292613,"ts mainly to illustrate how our analysis can be useful in the settings where data complexity matters and, therefore, limit ourselves to only using different teacher checkpoints. Future work, however, can investigate possible combinations with other approaches. For example, to further improve quality, our method can be combined with the Born-Again networks while still requiring fewer resources due to only partial training of the teachers. 7 Additional Related Work Other work connecting neural and traditional approaches include modeling modifications, such as modeling coverage and/or fertility (Tu et al., 2016; Mi et al., 2016a; Cohn et al., 2016; Feng et al., 2016) and several other modifications (Zhang et al., 2017b; Stahlberg et al., 2017; Huang et al., 2018), analysis of the relation between attention and word alignments (Ghader and Monz, 2017), and word alignment induction from NMT models (Li et al., 2019; Garg et al., 2019; Song et al., 2020b; Zenkel et al., 2020; Chen et al., 2020). Previous analysis of NMT learning dynamics include analyzing how the trainable parameters affect an NMT model (Zhu et al., 2020) and looking at the speed of learning specific discourse phenomena in context-aware"
2021.emnlp-main.667,D19-1081,1,0.900622,"Missing"
2021.emnlp-main.667,P19-1116,1,0.896375,"Missing"
2021.emnlp-main.667,2021.acl-long.91,1,0.927702,"thers), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve everything at the same rate? Does it learn word-by-word translation first and more complicated patterns later, or is there a different behavior? This is especially interesting in light of a recent work analyzing how NMT balances the two different types of context: the source and prefix of the target sentence (Voita et al., 2021). As it turns out, changes in NMT training are non-monotonic and form several distinct stages (e.g., stages changing direction from decreasing influence of source to increasing), which hints that the NMT training consists of stages with qualitatively different changes. In the last couple of decades, the two main machine translation paradigms have been statistical and neural MT. Statistical MT (SMT) decomposes the translation task into several components (e.g., lexical translation probabilities, alignment probabilities, target-side language model, etc.) which are learned separately and then com"
2021.emnlp-main.667,D17-1149,0,0.0184086,"veral models and language pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source),"
2021.emnlp-main.667,2020.acl-main.146,0,0.0201941,"e still requiring fewer resources due to only partial training of the teachers. 7 Additional Related Work Other work connecting neural and traditional approaches include modeling modifications, such as modeling coverage and/or fertility (Tu et al., 2016; Mi et al., 2016a; Cohn et al., 2016; Feng et al., 2016) and several other modifications (Zhang et al., 2017b; Stahlberg et al., 2017; Huang et al., 2018), analysis of the relation between attention and word alignments (Ghader and Monz, 2017), and word alignment induction from NMT models (Li et al., 2019; Garg et al., 2019; Song et al., 2020b; Zenkel et al., 2020; Chen et al., 2020). Previous analysis of NMT learning dynamics include analyzing how the trainable parameters affect an NMT model (Zhu et al., 2020) and looking at the speed of learning specific discourse phenomena in context-aware NMT (Voita et al., 2019b,a). 8 Conclusions We analyze how NMT acquires different competencies during training and look at the competencies related to three core SMT components. We find 6 We used the code and the data from https:// that NMT first focuses on learning target-side langithub.com/pytorch/fairseq/tree/master/ examples/nonautoregressive_translation. guage"
2021.emnlp-main.667,P17-1139,0,0.117211,"nguage pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve e"
2021.emnlp-main.667,I17-1016,0,0.0954765,"nguage pairs. Additionally, we explain how such an understanding of the training process can be useful in practice and, as an example, show how it can be used to improve vanilla nonautoregressive neural machine translation by guiding teacher model selection. 1 Introduction competences within the same network over the course of training. Even though previous work shows how to improve some of the competences in NMT, e.g., by using lexical translation probabilities, phrase memories, target-side LM, alignment information (Arthur et al., 2016; He et al., 2016; Tang et al., 2016; Wang et al., 2017; Zhang et al., 2017a; Dahlmann et al., 2017; Gülçehre et al., 2015; Gülçehre et al., 2017; He et al., 2016; Sriram et al., 2017; Dahlmann et al., 2017; Stahlberg et al., 2018; Mi et al., 2016b; Liu et al., 2016; Chen et al., 2016; Alkhouli et al., 2016; Alkhouli and Ney, 2017; Park and Tsvetkov, 2019; Song et al., 2020a among others), it is still not clear how and when NMT acquires these competences during training. For example, are there any stages where NMT focuses on different aspects of translation, e.g., fluency (agreement on the target side) or adequacy (i.e. connection to the source), or does it improve e"
2021.emnlp-main.670,2020.acl-main.747,0,0.24782,"analogue and demonstrates roughly comparable performance across all examined languages. EN-DE BASE LARGE Experimental Setup Our investigation seeks to answer two questions: 1. To what extent can MLLMs solve Winograd schemas in different languages? and 2. Does commonsense knowledge actively transfer across languages? Should the latter be the case, it could substantially reduce the need for language-specific commonsense knowledge bases that usually require significant human effort to construct and expand (Speer et al., 2017). Our experiments focus on the XLM-RoBERTa (XLM-R) model introduced in (Conneau et al., 2020). Structurally similar to the decoder of a transformer NMT model, XLMR is trained on monolingual as well as parallel data covering 100 diverse languages, to induce language-agnostic representations in a shared semantic space. Intuitively, sharing representations across languages should facilitate commonsense 15 As with bias values, initial trigger importance scores are re-computed on test sets used in few-shot experiments. Finetuning has a limited effect on EN-RU which had the highest initial importance scores. Results EN 0.53 0.62 DE 0.53 0.61 EN-FR EN 0.54 0.63 FR 0.53 0.6 EN-RU EN 0.52 0.62"
2021.emnlp-main.670,D18-1269,0,0.0297936,"ral CoR datasets have been proposed in the past, including (Guillou and Hardmeier, 2016; Bawden et al., 2018; Müller et al., 2018; Stojanovski et al., 2020). Among those, that of (Stojanovski et al., 2020) is most relevant to our work. While it contains samples that require world knowledge to resolve coreference, they are constructed from a fixed set of templates and remain limited to EN-DE. In contrast, Wino-X encompasses multiple target languages, while offering greater linguistic and thematic diversity. Finally, while cross-lingual transfer in MLLMs has received much attention in the past (Conneau et al., 2018, 2020; Hu et al., 2020; Liang et al., 2020), research on CSR in multiple languages remains limited, with (He et al., 2020) being the only relevant machine translation study known to us. Concurrent to our work, (Lin et al., 2021) examine whether MLLMs can perform multilingual CSR on tasks unrelated to Winograd schemas. 8524 19 Excluding samples found in each test set from training. 6 Conclusion and Outlook In this work, we introduced Wino-X, a dataset containing cross-lingual and multilingual Winograd schemas. Based on this resource, we showed that NMT models struggle to correctly resolve core"
2021.emnlp-main.670,2020.findings-emnlp.117,0,0.0300397,"ngual schemas suitable for evaluating translation models requires replacing the gap with the ambiguous pronoun it, which is not possible for the social domain. Consequently, we focus our attention on the physical subset of WinoGrande that contains 19,260 unique samples (9,630 schemas), with each sample representing a single instance of a monolingual, English schema. 2.1 Sample Formats Wino-X includes samples in two formats - one for the evaluation of translation models and another for the evaluation of MLLMs. In both cases the dataset assumes a contrastive evaluation setup (Rios et al., 2017; Gardner et al., 2020), whereby evaluated models are used to rank two minimally different alternatives. Models are scored according to how frequently they rank the correct alternative above the incorrect one. For the evaluation of NMT models, we replace the gap token with the ambiguous it in each sample, and pair the result with two contrastive translations. The translated it agrees in gender with a different antecedent in each case. For the purpose of our investigation, we focus on German, French, and Russian as morphologically rich, high-resource target languages. In the following, we refer to this set of cross-l"
2021.emnlp-main.670,2020.emnlp-main.484,0,0.0186539,"st, including (Guillou and Hardmeier, 2016; Bawden et al., 2018; Müller et al., 2018; Stojanovski et al., 2020). Among those, that of (Stojanovski et al., 2020) is most relevant to our work. While it contains samples that require world knowledge to resolve coreference, they are constructed from a fixed set of templates and remain limited to EN-DE. In contrast, Wino-X encompasses multiple target languages, while offering greater linguistic and thematic diversity. Finally, while cross-lingual transfer in MLLMs has received much attention in the past (Conneau et al., 2018, 2020; Hu et al., 2020; Liang et al., 2020), research on CSR in multiple languages remains limited, with (He et al., 2020) being the only relevant machine translation study known to us. Concurrent to our work, (Lin et al., 2021) examine whether MLLMs can perform multilingual CSR on tasks unrelated to Winograd schemas. 8524 19 Excluding samples found in each test set from training. 6 Conclusion and Outlook In this work, we introduced Wino-X, a dataset containing cross-lingual and multilingual Winograd schemas. Based on this resource, we showed that NMT models struggle to correctly resolve coreference that presupposes commonsense knowled"
2021.emnlp-main.670,W18-6301,0,0.0370991,"Missing"
2021.emnlp-main.670,W18-6319,0,0.0121217,"perty # Parameters (M) # Training pairs (M) Test BLEU BASE 65.5 39.7 29.9 BIG 363.5 538.7* 36.2 EN-FR mBART 610.9 42.6 25.6 BASE 67.7 140.6 40.2 BIG 313.1 36 41.1 EN-RU mBART 610.9 36.8 36 BASE 72.5 34.3 21.3 BIG 317.9 162* 25.5 mBART 610.9 13.9 20.6 Table 2: Overview of the evaluated NMT models. Training size estimates were taken from corresponding publications (Ott et al., 2018; Ng et al., 2019; Tang et al., 2020). * denotes inclusion of back-translated parallel data. For mBART50, training size does not include monolingual data used in pre-training. BLEU scores were computed with SacreBLEU (Post, 2018). to allow for a fair evaluation of models trained on sentence-level data. To reduce dataset artifacts in Wino-X, both instances of a schema are removed if a single one of them is filtered-out. To obtain contrastive translations, the gap token is replaced with one of its fillers (which serve as the antecedents of it) before passing the sample through a translation engine. For all target languages, translations are obtained via the Google Translate API2 , due to its relative domain generality. Afterwards, the previously inserted filler is replaced with a pronoun of the same grammatical gender,"
2021.emnlp-main.673,W18-6402,0,0.0177105,"mine models’ reliance on the visual modality, we calculate the performance degradation when randomly sampled images are fed. This is also termed as image awareness (Elliott, 2018). 2 https://github.com/multi30k/dataset/ blob/master/scripts/feature-extractor 3 8558 See § 6. 4 Results The results of our experiments are shown in Tab. 2. 4.1 Multi30K EN→DE Test2016 We found our MMT models provide little to no improvement over the text-only Transformer. Moreover, the impact of feeding MMT systems with incongruent images is negligible. Our observations conform with previous work (Lala et al., 2018; Barrault et al., 2018; Wu et al., 2021), namely that visual signals are not utilized. Multisense We also evaluate models trained on Multi30K on the Multisense test set (Gella et al., 2019). Similarly, no substantial difference is observed whether congruent or incongruent images are used. This suggests that it is not just a matter of the Test2016 test set containing too little textual ambiguity, but that the model has not learned to incorporate the visual information necessary for Multisense.4 4.2 Multi30K TR→EN Our experiments on the TR→EN version of Multi30K that we created do not show any substantial improvement"
2021.emnlp-main.673,K16-1002,0,0.0216966,"seudo-parallel data for training our Turkish-English MMT models. For validation Word Dropout. Inspired by Caglayan et al. and testing we randomly sample 1000 sentences and use the remaining for training. We refer to (2019), we degrade the textual inputs to eliminate this processed dataset as Ambiguous Captions crucial information. We use a simplified approach that requires no manual annotation, randomly re- (AmbigCaps). placing tokens in the source sentence with a speFor comparison, we also create a cial UNK token, subject to a dropout probability p Turkish→English version of Multi30k by back(Bowman et al., 2016). translating the English side. Tab. 1 summarizes the 8557 Concatenation. We implement a different approach to combine textual and visual features. The flattened and projected ‘res4_relu’ features Hres4_relu are directly concatenated with the Transformer encoder representations Htext as follows: characteristics of the two corpora. Dataset Multi30k AmbigCaps # Sen 31,014 91,601 # Words (EN) 369,048 1,253,400 # Gen. PROs 4,181 109,440 Table 1: Statistical properties (numbers of sentences, words, and gender pronouns in English) of the Multi30k and Ambiguous Captions datasets used in our experimen"
2021.emnlp-main.673,N19-1422,0,0.0677025,"Missing"
2021.emnlp-main.673,P17-1175,0,0.0349096,"Missing"
2021.emnlp-main.673,D18-1329,0,0.233635,"whereas Lala et al. (2018); Baronly neural machine translation (NMT) counrault et al. (2018); Raunak et al. (2019) observed terparts when visual context is available. Howthat the multimodal integration did not make a big ever, recent studies have also shown that the perdifference quantitatively or qualitatively. Followformance of MMT models is only marginally impacted when the associated image is replaced ing experimental work showed that replacing the with an unrelated image or noise, which sugimages in image-caption pairs with incongruent imgests that the visual context might not be exages (Elliott, 2018) or even random noise (Wu et al., ploited by the model at all. We hypothesize that 2021) might still result in similar performance of this might be caused by the nature of the commultimodal models. In light of these results, Wu monly used evaluation benchmark, also known et al. (2021) suggested that gains in quality might as Multi30K, where the translations of image merely be due to a regularization effect and the captions were prepared without actually showing the images to human translators. In this images may not actually be exploited by models paper, we present a qualitative study that exd"
2021.emnlp-main.673,W17-4718,0,0.0178206,"ribed by the text. We also implement another simple model to demonstrate the applicability of our approaches. Transformer. For text-only baseline, we use a variant of the Transformer that has 4 encoder layers, 4 decoder layers, and 4 attention heads in each layer. The dimensions of input/output layers and inner feed-forward layers are also reduced to 128 and 256 respectively. This configuration has been shown to be effective on Multi30K dataset (Wu et al., 2021). The MMT models below follow the same configuration. Visual Features. Image features are extracted with the code snippet provided by Elliott et al. (2017),2 which uses a ResNet-50 (He et al., 2016) pre-trained on ImageNet (Deng et al., 2009) as image encoder. The ‘res4_relu’ features ∈ R1024×14×14 and average pooled features ∈ R2048 are extracted. Gated Fusion. Gated fusion model (Wu et al., 2021) learns a gate vector λ, and combines textual representation and image representations as follows: H = Htext + λ Havg , (1) where Htext is the output of the Transformer encoder, Havg is the average pooled visual features after projection and broadcasting, and denotes the Hadamard product. H is then fed into the Transformer decoder as in NMT. H = [Htext"
2021.emnlp-main.673,W16-3210,0,0.0370052,"Missing"
2021.emnlp-main.673,2021.acl-long.505,0,0.0773091,"Missing"
2021.emnlp-main.673,P02-1040,0,0.109512,"m (Kingma and Ba, 2015) with β1 = 0.9 and β2 = 0.98. Maximum number of tokens in a mini-batch is 4096. Learning rate warms up from 1e − 7 to 0.005 in 2000 steps, then decays based on the the inverse square root of the update number. A dropout (Srivastava et al., 2014) of 0.3 and label-smoothing of 0.1 are applied. The models are trained with earlystopping (patience=10) and the last ten checkpoints are averaged for inference. We use beam search with beam size 5. We use the toolkit FAIRSEQ (Ott et al., 2019) for our implementation. 3.4 Metrics BLEU. We compute the cumulative 4-gram BLEU scores (Papineni et al., 2002) to evaluate the overall quality of translation. Gender Accuracy. Since we are most concerned with the gender ambiguity in the texts, we introduce gender accuracy as an additional metric. We first extract gender pronouns from the sentence. If the sentence contains at least one of the male pronouns [‘he’, ‘him’, ‘his’, ‘himself’], it is classified as ‘male’; if it contains at least one of the female pronouns [‘she’, ‘her’, ‘hers’, ‘herself’], it is classified as ‘female’; if it contains both male and female pronouns or neither, it is classified as ‘undetermined’. We only consider the first two"
2021.emnlp-main.673,N19-1200,0,0.0616489,"are only pos∗ Work done while at the University of Zürich. sible in the presence of images. Caglayan et al. 1 Our code and data are available (2019) degrade the Multi30K dataset to hide away at: https://github.com/jiaodali/ vision-matters-when-it-should. crucial information in the source sentence, includ8556 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8556–8562 c November 7–11, 2021. 2021 Association for Computational Linguistics ing color, head nouns, and suffixes. Similarly, Wu et al. (2021) mask high-frequency words in Multi30K. Multisense (Gella et al., 2019) collects sentences whose verbs have cross-lingual sense ambiguities. However, due to the high cost of data collection, datasets of such kind are often limited in size. MultiSubs (Wang et al., 2021) is another related dataset. which is primarily used for lexical translation because the images are retrieved to align with text fragments rather than whole sentences. In this work, we propose two methods to necessitate the visual context — back-translation from a gender-neutral language (e.g. Turkish) and word dropout in the source sentence. They are simple and cheap to implement, allowing them to"
2021.emnlp-main.673,W19-8620,0,0.0162429,"d: Sanity Checking Multimodal Machine Translation Models Jiaoda Li1 Duygu Ataman2∗ Rico Sennrich3,4 1 ETH Zürich New York University 3 University of Zürich 4 University of Edinburgh jiaoda.li@inf.ethz.ch ataman@nyu.edu 2 sennrich@cl.uzh.ch Abstract models (Calixto et al., 2017; Helcl et al., 2018; Ive et al., 2019; Lin et al., 2020; Yin et al., 2020) have Multimodal machine translation (MMT) sysbeen proposed which showed improvements over tems have been shown to outperform their texttext-only models, whereas Lala et al. (2018); Baronly neural machine translation (NMT) counrault et al. (2018); Raunak et al. (2019) observed terparts when visual context is available. Howthat the multimodal integration did not make a big ever, recent studies have also shown that the perdifference quantitatively or qualitatively. Followformance of MMT models is only marginally impacted when the associated image is replaced ing experimental work showed that replacing the with an unrelated image or noise, which sugimages in image-caption pairs with incongruent imgests that the visual context might not be exages (Elliott, 2018) or even random noise (Wu et al., ploited by the model at all. We hypothesize that 2021) might still"
2021.emnlp-main.673,W18-6441,0,0.0324615,"Missing"
2021.emnlp-main.673,P19-1653,0,0.0411533,"Missing"
2021.emnlp-main.673,W18-6442,0,0.0169349,"e Awareness. To examine models’ reliance on the visual modality, we calculate the performance degradation when randomly sampled images are fed. This is also termed as image awareness (Elliott, 2018). 2 https://github.com/multi30k/dataset/ blob/master/scripts/feature-extractor 3 8558 See § 6. 4 Results The results of our experiments are shown in Tab. 2. 4.1 Multi30K EN→DE Test2016 We found our MMT models provide little to no improvement over the text-only Transformer. Moreover, the impact of feeding MMT systems with incongruent images is negligible. Our observations conform with previous work (Lala et al., 2018; Barrault et al., 2018; Wu et al., 2021), namely that visual signals are not utilized. Multisense We also evaluate models trained on Multi30K on the Multisense test set (Gella et al., 2019). Similarly, no substantial difference is observed whether congruent or incongruent images are used. This suggests that it is not just a matter of the Test2016 test set containing too little textual ambiguity, but that the model has not learned to incorporate the visual information necessary for Multisense.4 4.2 Multi30K TR→EN Our experiments on the TR→EN version of Multi30K that we created do not show any"
2021.emnlp-main.673,N19-4009,0,0.0206307,"avoids confounding the two modalities. 3.3 Implementation Details We follow (Wu et al., 2021) and use Adam (Kingma and Ba, 2015) with β1 = 0.9 and β2 = 0.98. Maximum number of tokens in a mini-batch is 4096. Learning rate warms up from 1e − 7 to 0.005 in 2000 steps, then decays based on the the inverse square root of the update number. A dropout (Srivastava et al., 2014) of 0.3 and label-smoothing of 0.1 are applied. The models are trained with earlystopping (patience=10) and the last ten checkpoints are averaged for inference. We use beam search with beam size 5. We use the toolkit FAIRSEQ (Ott et al., 2019) for our implementation. 3.4 Metrics BLEU. We compute the cumulative 4-gram BLEU scores (Papineni et al., 2002) to evaluate the overall quality of translation. Gender Accuracy. Since we are most concerned with the gender ambiguity in the texts, we introduce gender accuracy as an additional metric. We first extract gender pronouns from the sentence. If the sentence contains at least one of the male pronouns [‘he’, ‘him’, ‘his’, ‘himself’], it is classified as ‘male’; if it contains at least one of the female pronouns [‘she’, ‘her’, ‘hers’, ‘herself’], it is classified as ‘female’; if it contain"
2021.emnlp-main.673,P16-1009,1,0.750578,"uch larger datasets. We test the methods on two MMT architectures and find that they indeed make the model more reliant on the images. 2 Method In this section, we elaborate two methods to conceal important information in the source textual inputs that can be recovered with the aid of visual inputs. Back-Translation. Rather than trying to create reference translations that make use of visual signals for disambiguation, we treat original image captions as the target side and automatically produce ambiguous source sentences. While such back-translations are generally used for data augmentation (Sennrich et al., 2016), we rely fully on this data for training and testing. We focus on gender ambiguity, which can be easily created by translating from a language with natural gender (English) into a gender-neutral language (Turkish). In Turkish, there is no distinction between gender pronouns (e.g. “he” and “she” are both translated into “o”). We use a commercial translation system (Google Translate) to translate the image description in English to Turkish. The task is then to translate from Turkish back into English. An example is shown in Fig. 1. Film character sitting at his chair and reading a letter with f"
2021.emnlp-main.673,P18-1238,0,0.0217947,"m character sitting in his chair and reading a letter with fireplace and Christmas tree in the background. MMT Model Film character sitting in her chair and reading a letter with fireplace and Christmas tree in the background. Figure 1: An example for back-translation. The image caption is translated into Turkish using a text-only translation system. Then a MMT model is trained to translate it back into English. When an incongruent image is fed into the model, the gender pronoun “his” is mistranslated. 3 3.1 Experimental Setup Data Collection As our starting point, we use Conceptual Captions (Sharma et al., 2018), which contains 3.3M images with captions. The captions in the dataset have already been processed to replace named entities with hypernyms such as ’person’ or profession names such as ’actor’. In order to create a gender-ambiguous dataset we further filter out any sentences containing nouns with information about the gender of the entity (e.g. woman/man, lady/gentleman, king/queen, etc.) and also remove sentences with professions which are only used in a single gender-specific context (e.g. ‘football player’, which is always used with the male pronoun in the dataset). We then automatically t"
2021.emnlp-main.673,W16-2346,0,0.0459617,"Missing"
2021.emnlp-main.673,2021.acl-long.480,0,0.422461,"y translations. A number of MMT datasets where correct translations are only pos∗ Work done while at the University of Zürich. sible in the presence of images. Caglayan et al. 1 Our code and data are available (2019) degrade the Multi30K dataset to hide away at: https://github.com/jiaodali/ vision-matters-when-it-should. crucial information in the source sentence, includ8556 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8556–8562 c November 7–11, 2021. 2021 Association for Computational Linguistics ing color, head nouns, and suffixes. Similarly, Wu et al. (2021) mask high-frequency words in Multi30K. Multisense (Gella et al., 2019) collects sentences whose verbs have cross-lingual sense ambiguities. However, due to the high cost of data collection, datasets of such kind are often limited in size. MultiSubs (Wang et al., 2021) is another related dataset. which is primarily used for lexical translation because the images are retrieved to align with text fragments rather than whole sentences. In this work, we propose two methods to necessitate the visual context — back-translation from a gender-neutral language (e.g. Turkish) and word dropout in the sou"
2021.emnlp-main.673,2020.acl-main.273,0,0.0355258,"Missing"
2021.emnlp-main.673,Q14-1006,0,0.0262281,"in model performance and methods to highlight the importance of visual whether its tendency to ignore visual information signals in the datasets which demonstrate imin the input could be a consequence of the nature provements in reliance of models on the source images. Our findings suggest the research on of the dataset. The most widely-used dataset for effective MMT architectures is currently imMMT is Multi30K (Elliott et al., 2016, 2017; Barpaired by the lack of suitable datasets and carerault et al., 2018), which extends the Flickr30K ful consideration must be taken in creation of dataset (Young et al., 2014) to German, French, future MMT datasets, for which we also proand Czech translations. Captions were translated vide useful insights.1 without access to images, and it is posited that this heavily biases MMT models towards only relying 1 Introduction on textual input (Elliott, 2018). MMT models may Multimodal machine translation (MMT) aims to well be capable of using visual signals, but will only improve machine translation by resolving certain learn to do so if the visual context provides inforcontextual ambiguities with the aid of other modal- mation beyond the text. For instance, the English"
2021.emnlp-main.673,D17-1323,0,0.0483416,"Missing"
2021.emnlp-main.803,2020.acl-main.418,0,0.0614226,"Missing"
2021.emnlp-main.803,W18-6319,0,0.0168508,"rent random seeds. Each teacher was then used to train an individual student per size. We repeated this procedure with the ‘SOTA’ teachers. We used Adam with an initial learning rate of 5e-4, FP16 training, label smoothing with  = 0.1, and a dropout of 0.3. We trained with a token batch size of 16k, and we selected the best checkpoint with respect to BLEU based on the newstest sets from the preceding years. For decoding, beam search with size 5 was used. Evaluation To evaluate general translation performance we used the WMT19 testset and computed BLEU (Papineni et al., 2002) using SacreBLEU (Post, 2018).6 To evaluate disambiguation accuracy we used the MuCoW and WinoMT test sets. In the case of MuCoW, the WMT19 version of the complete translation-based data was used (Raganato et al., 2019). Details of those datasets are reported in Table A2. Finally, we used the ensembles by Ng et al. (2019) as an evaluator for contrastive conditioning and applied categorywise weighting (Section 3.4). Metrics Since our goal is to quantify overgeneralization bias, a metric is required that captures overgeneralization based on disambiguation testis identical to the third one, so we only used three as teachers."
2021.emnlp-main.803,2020.wmt-1.39,0,0.148597,"to an exacerbation of gender bias in MT (Vanmassenhove et al., 2019; Roberts et al., 2020). In the case of WSD, Rios et al. (2017) have found that neural MT systems handle frequent word senses well but perform poorly on rare word senses. The influence of sense distribution on WSD has been further examined by Tang et al. (2018), Raganato et al. (2020) and Emelin et al. (2020). With regard to gendered occupations, Stanovsky et al. (2019) show that MT translates stereotypes more reliably, and WinoMT or similar datasets have subsequently been used to quantify bias in various translation settings (Kocmi et al. 2020; Costa-jussà et al. 2020a,b; Tomalin et al. 2021; Choubey et al. 2021; Renduchintala and Williams 2021; among others). 2.4 Effects of Knowledge Distillation Overgeneralization bears some resemblance to compression, which is significant in the context of knowledge distillation (Hinton et al., 2015). The process of sequence-level knowledge distillation (SeqKD) can be described as follows (Kim and Rush, 2016): 1. A generative model is trained, to be used as an intermediate teacher; 2. The teacher re-generates the target side of the training data; 3. A student model, which is usually smaller, is"
2021.emnlp-main.803,W17-1601,0,0.0170644,"goes undetected by common evaluation metrics. While we focus on how it affects the accuracy of translations, overgeneralization bias does not just have a technical dimension but also a social one (Hovy and Spruit, 2016; Sheng et al., 2021), especially with regard to sensitive categories such as gender. Therefore, our findings could also inform a socio-political discussion of model compression, provided that such a discussion is normatively well-founded (Blodgett et al., 2020). Our preliminary data analysis (Section 4.1) is based on gender as a variable, which warrants some ethical reflection (Larson, 2017). Our analysis is based on a very large collection of English occupation nouns and their translations into German. We categorize the notional gender of the German translations as ‘male’ or ‘female’ in cases where grammatical gender is a valid indication. While the automatic inference of gender is discouraged in many research contexts, we believe that our approach is adequate, since in this case, rather than the personal gender of human subjects, the notional gender of nouns is inferred (McConnell-Ginet, 2014). However, gender-neutral or alternative ways of expressing gender are not separately"
2021.emnlp-main.803,2021.ccl-1.108,0,0.0319765,"Missing"
2021.emnlp-main.803,2021.acl-long.330,0,0.0116531,"ng assumptions nor deeper expertise of a tar- as the anonymous reviewers for helpful feedback. get language are strictly required to perform con- We also thank the ACL Rolling Review team for trastive conditioning (even though in this paper, their effort in organizing a pilot run. 10254 Broader Impact We use the term ‘bias’ to describe a behavioral tendency of NLP systems that goes undetected by common evaluation metrics. While we focus on how it affects the accuracy of translations, overgeneralization bias does not just have a technical dimension but also a social one (Hovy and Spruit, 2016; Sheng et al., 2021), especially with regard to sensitive categories such as gender. Therefore, our findings could also inform a socio-political discussion of model compression, provided that such a discussion is normatively well-founded (Blodgett et al., 2020). Our preliminary data analysis (Section 4.1) is based on gender as a variable, which warrants some ethical reflection (Larson, 2017). Our analysis is based on a very large collection of English occupation nouns and their translations into German. We categorize the notional gender of the German translations as ‘male’ or ‘female’ in cases where grammatical g"
2021.emnlp-main.803,2021.naacl-main.189,0,0.0320712,"knowledge distillation (SeqKD) can be described as follows (Kim and Rush, 2016): 1. A generative model is trained, to be used as an intermediate teacher; 2. The teacher re-generates the target side of the training data; 3. A student model, which is usually smaller, is trained on the new data. been characterized as less noisy and more deterministic than the original target (Gu et al., 2018), as having a reduced fertility and distortion (Zhang et al., 2018), reduced lexical diversity (Xu et al., 2021), and as being less complex while preserving faithfulness (Zhou et al., 2020). Concurrent work (Silva et al., 2021) studies distillation in the context of masked language models, showing that distilled models have a more pronounced bias according to standard metrics. Ding et al. (2021) examine SeqKD in non-autoregressive MT, where it is shown to decrease translation accuracy with respect to rare words in word-aligned parallel data. Finally, Renduchintala et al. (2021) show that MT models optimized for speed have an increased gender bias, analyzing techniques that are complementary to distillation, namely reduction of beam size, shallow decoders, efficient attention and quantization. In this paper, we perfo"
2021.emnlp-main.803,2020.wmt-1.73,0,0.0608052,"Missing"
2021.emnlp-main.803,P19-1164,0,0.169139,"ords makes trans- sider a model that translates the English source lations inadequate and can even constitute a form ‘doctor’ into German as ‘Ärztin’ (female doctor). of bias when it occurs systematically. However, This translation will receive a better score when detecting disambiguation errors in machine transla- conditioned on the source ‘female doctor’ than on tion (MT) is a non-trivial task. Previous work has ‘male doctor’, indicating that it is a feminine form. focused on automatic post-hoc analysis of transla- Given sufficient disambiguation cues, the whitetions (Raganato et al., 2019; Stanovsky et al., 2019), box translation model can thus serve as an evaluator but rules of what makes a disambiguation correct for the original translation. or incorrect tend to be imprecise. While contrastive Since the contrastive sources are written in the evaluation (Sennrich, 2017; Rios et al., 2017) elim- source language, contrastive conditioning does not inates the need for post-hoc analysis by scoring rely on references in the target language. This pre-defined pairs of hypotheses, such probability makes it easier to scale the evaluation across mulestimates cannot be obtained from black-box sys- tiple target l"
2021.emnlp-main.803,W18-6304,1,0.810711,"n arise if a system sees mostly positive examples. More recently, overgeneralization has been discussed as a category of social impact of NLP systems (Hovy and Spruit, 2016), and it has been hypothesized that overgeneralization of the training data leads to a loss of lexical diversity and to an exacerbation of gender bias in MT (Vanmassenhove et al., 2019; Roberts et al., 2020). In the case of WSD, Rios et al. (2017) have found that neural MT systems handle frequent word senses well but perform poorly on rare word senses. The influence of sense distribution on WSD has been further examined by Tang et al. (2018), Raganato et al. (2020) and Emelin et al. (2020). With regard to gendered occupations, Stanovsky et al. (2019) show that MT translates stereotypes more reliably, and WinoMT or similar datasets have subsequently been used to quantify bias in various translation settings (Kocmi et al. 2020; Costa-jussà et al. 2020a,b; Tomalin et al. 2021; Choubey et al. 2021; Renduchintala and Williams 2021; among others). 2.4 Effects of Knowledge Distillation Overgeneralization bears some resemblance to compression, which is significant in the context of knowledge distillation (Hinton et al., 2015). The proces"
2021.emnlp-main.803,W19-6622,0,0.0282258,"pattern- Carbonell et al. (1983) describe overgeneralization matching and scoring approaches. In a pattern- as a tendency to learn concepts that extend not only matching evaluation, translations are searched for to positive but also to negative examples, which 10247 can arise if a system sees mostly positive examples. More recently, overgeneralization has been discussed as a category of social impact of NLP systems (Hovy and Spruit, 2016), and it has been hypothesized that overgeneralization of the training data leads to a loss of lexical diversity and to an exacerbation of gender bias in MT (Vanmassenhove et al., 2019; Roberts et al., 2020). In the case of WSD, Rios et al. (2017) have found that neural MT systems handle frequent word senses well but perform poorly on rare word senses. The influence of sense distribution on WSD has been further examined by Tang et al. (2018), Raganato et al. (2020) and Emelin et al. (2020). With regard to gendered occupations, Stanovsky et al. (2019) show that MT translates stereotypes more reliably, and WinoMT or similar datasets have subsequently been used to quantify bias in various translation settings (Kocmi et al. 2020; Costa-jussà et al. 2020a,b; Tomalin et al. 2021;"
2021.emnlp-main.803,H05-1097,0,0.0998364,"ord sense disambiguation in general, as represented by the MuCoW test suite (Raganato et al., 2019). The second type is the special case of gendered occupation names, for which the WinoMT challenge set has been released (Stanovsky et al., 2019). For both types of disambiguation, our results based on contrastive conditioning confirm that models trained via SeqKD tend to have a more pronounced overgeneralization bias than other models with a comparable BLEU score. 2 2.1 Background and Related Work Evaluation of Disambiguation in MT phrases that are known to be correct or incorrect. For example, Vickrey et al. (2005) create a test set from ambiguous source words in a parallel corpus, and Raganato et al. (2019, 2020) use this approach to assemble a large-scale benchmark (MuCoW) with multiple translation variants for ambiguous words. However, it is usually not feasible to create an exhaustive list of all translation variants. Scoring-based evaluation alleviates this problem by directly comparing probabilities for pre-defined contrastive translation variants as estimated by the model (Sennrich, 2017; Rios et al., 2017). An example of a contrastive translation pair for WSD is presented in the left part of Fig"
2021.emnlp-main.803,2021.findings-acl.385,0,0.0290643,"ch is significant in the context of knowledge distillation (Hinton et al., 2015). The process of sequence-level knowledge distillation (SeqKD) can be described as follows (Kim and Rush, 2016): 1. A generative model is trained, to be used as an intermediate teacher; 2. The teacher re-generates the target side of the training data; 3. A student model, which is usually smaller, is trained on the new data. been characterized as less noisy and more deterministic than the original target (Gu et al., 2018), as having a reduced fertility and distortion (Zhang et al., 2018), reduced lexical diversity (Xu et al., 2021), and as being less complex while preserving faithfulness (Zhou et al., 2020). Concurrent work (Silva et al., 2021) studies distillation in the context of masked language models, showing that distilled models have a more pronounced bias according to standard metrics. Ding et al. (2021) examine SeqKD in non-autoregressive MT, where it is shown to decrease translation accuracy with respect to rare words in word-aligned parallel data. Finally, Renduchintala et al. (2021) show that MT models optimized for speed have an increased gender bias, analyzing techniques that are complementary to distillat"
2021.emnlp-main.803,N18-2003,0,0.0232415,"Gendered Occupations The translation of gendered occupation nouns can be seen as a special case of WSD. When translating occupations from a language that does not tend to mark their gender into a language that does, gender has to be either inferred from the context, e.g. from anaphoric pronouns, or expressed neutrally. Such a challenge arises when translating from English into German, Russian or other morphologically rich languages. WinoMT (Stanovsky et al., 2019) is a challenge set for this phenomenon, which combines several datasets for gender coreference in English (Rudinger et al., 2018; Zhao et al., 2018). See Figure 2 for an example in contrastive format. In the context of translation, word sense disambiguation (WSD; Navigli, 2009) can be formally defined as follows: Let us assume that every instance of a content word w conveys one out of a set {s1 , s2 , . . . } of senses. Then a WSD error occurs if a source instance wi is translated into a target word that does not convey the sense of wi but another sense of the word w (Popescu-Belis, 2019). Automated approaches for evaluating MT sys- 2.3 Overgeneralization Bias tems on WSD can be grouped into pattern- Carbonell et al. (1983) describe overg"
2021.findings-acl.255,P19-1284,1,0.915029,"017). In this work, we hypothesize that encoder outputs are compressible and we can force Seq2Seq model to route information through their subset. Figure 2 illustrates our intuition as well as the difference with existing work (Vaswani et al., 2017; Correia et al., 2019). Instead of dynamically sparsifying attention weights for individual decoder steps (Correia et al., 2019), we aim at detecting uninformative source encodings and dropping them to shorten the encoding sequence before generation. To this end, we build on recent work on sparsifying weights (Louizos et al., 2018) and activations (Bastings et al., 2019) of neural networks. Specifically, we insert a differentiable neural sparsity layer (L0 D ROP) in-between the encoder and the decoder. The layer can be regarded as providing a multiplicative scalar gate for every encoder output. The gate is a random variable and, unlike standard attention, can be exactly zero, effectively masking out the corresponding source encodings. The sparsity is promoted by introducing an extra term to the learning objective, i.e. an expected value of the sparsity-inducing L0 penalty. By varying the coefficient for the regularizer, we can obtain different levels of spars"
2021.findings-acl.255,D18-1461,0,0.0148816,"ch differs from the linear-time model of Wang et al. (2019), although both can accelerate decoding. Our study of rule-based sparsity patterns is in line with the sparse Transformer (Child et al., 2019) though we also explore the use of external linguistic information (POS tag) in our sparsification rules, and focus on encoder outputs instead of self-attention. Character-based translation gained increasing popularity due to its capability of handling out-ofvocabulary issues while avoiding tokenization and subword segmentation (Ling et al., 2015; Costajuss`a and Fonollosa, 2016; Sennrich, 2017; Cherry et al., 2018). Recent efforts often focus on closing the performance gap against its subword-level counterpart (Libovick´y and Fraser, 2020; Gao et al., 2020), but little study explores solutions to improve its inefficient inference resulted from the long character sequences. In this respect, Cherry et al. (2018) proposed to use conditional computation to dynamically compress encoder states. Similar to our results, they also observed a trade-off between the translation quality and the degree of compression. 3 Background: Transformer We take Transformer (Vaswani et al., 2017) as our testbed. Transformer use"
2021.findings-acl.255,P16-2058,0,0.0496229,"Missing"
2021.findings-acl.255,D19-1361,0,0.0221361,"codings with rulebased sparse patterns is feasible, and confirms information-theoretic expectations, although rule-based patterns do not generalize well across tasks. 2 Related Work Approaches to compression in Seq2Seq models fall into the category of model parameter compression (See et al., 2016), sequential knowledge dis2889 tillation (Kim and Rush, 2016) or sparse attention induction that ranges from modeling hard attention (Wu et al., 2018) to developing differentiable sparse softmax functions or regularizing attention weights for sparsity (Niculae and Blondel, 2017; Correia et al., 2019; Cui et al., 2019; Zhang et al., 2019). Unfortunately, the success of all these studies builds upon the access to all source encodings in training and decoding. Learning which encoder outputs to prune in Seq2Seq models, to the best of our knowledge, has never been investigated before. Sukhbaatar et al. (2019) learn attention spans in self-attention and discard information from states outside of the span; this method is not directly applicable to encoder-decoder attention. We use the differentiable L0 -relaxation which was first introduced by Louizos et al. (2018) in the context of pruning individual neural net"
2021.findings-acl.255,2020.acl-main.145,0,0.0174848,"line with the sparse Transformer (Child et al., 2019) though we also explore the use of external linguistic information (POS tag) in our sparsification rules, and focus on encoder outputs instead of self-attention. Character-based translation gained increasing popularity due to its capability of handling out-ofvocabulary issues while avoiding tokenization and subword segmentation (Ling et al., 2015; Costajuss`a and Fonollosa, 2016; Sennrich, 2017; Cherry et al., 2018). Recent efforts often focus on closing the performance gap against its subword-level counterpart (Libovick´y and Fraser, 2020; Gao et al., 2020), but little study explores solutions to improve its inefficient inference resulted from the long character sequences. In this respect, Cherry et al. (2018) proposed to use conditional computation to dynamically compress encoder states. Similar to our results, they also observed a trade-off between the translation quality and the degree of compression. 3 Background: Transformer We take Transformer (Vaswani et al., 2017) as our testbed. Transformer uses the dot-product attention network as its backbone to handle intra- and intersequence dependencies:   QKT √ V, (1) ATT(H, M) = AV = S M d wher"
2021.findings-acl.255,D18-1443,0,0.063663,"Missing"
2021.findings-acl.255,D16-1139,0,0.0135276,", verbs and nouns). • L0 D ROP can improve decoding efficiency particularly for lengthy source inputs. We achieve a decoding speedup of up to 1.65× on document summarization tasks and 1.20× on character-based machine translation task. • Filtering out source encodings with rulebased sparse patterns is feasible, and confirms information-theoretic expectations, although rule-based patterns do not generalize well across tasks. 2 Related Work Approaches to compression in Seq2Seq models fall into the category of model parameter compression (See et al., 2016), sequential knowledge dis2889 tillation (Kim and Rush, 2016) or sparse attention induction that ranges from modeling hard attention (Wu et al., 2018) to developing differentiable sparse softmax functions or regularizing attention weights for sparsity (Niculae and Blondel, 2017; Correia et al., 2019; Cui et al., 2019; Zhang et al., 2019). Unfortunately, the success of all these studies builds upon the access to all source encodings in training and decoding. Learning which encoder outputs to prune in Seq2Seq models, to the best of our knowledge, has never been investigated before. Sukhbaatar et al. (2019) learn attention spans in self-attention and disca"
2021.findings-acl.255,D19-1223,0,0.188797,"Missing"
2021.findings-acl.255,Q17-1026,0,0.0281559,"tention weight, and the white blocks denote an attention weight of 0. The source words whose encoding is pruned by L0 D ROP (receiving zero weight) are highlighted in red. across words, for example, it is negatively correlated with event frequency (Shannon, 1948; Zipf, 1949). When moving from word-level to characterlevel processing, the notion of encoding information and computing attention on the level of characters also seems excessive. Previous work has proposed hierarchical architectures where characterlevel encodings are compressed into word-level or span-level states (Ling et al., 2015; Lee et al., 2017). In this work, we hypothesize that encoder outputs are compressible and we can force Seq2Seq model to route information through their subset. Figure 2 illustrates our intuition as well as the difference with existing work (Vaswani et al., 2017; Correia et al., 2019). Instead of dynamically sparsifying attention weights for individual decoder steps (Correia et al., 2019), we aim at detecting uninformative source encodings and dropping them to shorten the encoding sequence before generation. To this end, we build on recent work on sparsifying weights (Louizos et al., 2018) and activations (Bast"
2021.findings-acl.255,2020.emnlp-main.203,0,0.0279484,"Missing"
2021.findings-acl.255,W04-1013,0,0.117013,"rtened source sequence X across decoder layers and steps. L0 D ROP changes the dependency of the encoder-decoder attention on source sequence from O(N M ) to O(N 0 M ), and allows for efficiency gains even with moderate sparsity, especially for large L, N and M . 5 Experimental Setup We study L0 D ROP on machine translation tasks (WMT14 English-German (En-De) (Bojar et al., 2014) and WMT18 Chinese-English (Zh-En) (Bojar et al., 2018)5 ) and document summarization tasks (CNN/Daily Mail (Hermann et al., 2015)6 and WikiSum (Liu et al., 2018)7 ). We adopt BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004) to evaluate the translation and summarization quality, respectively. Other details, including model settings, are given in the Appendix A. For characterbased translation, we employ the same model architecture and hyperparameters for training and decoding as specified in Appendix A, except that we adopt larger-batch training (∼85K characters) and encourage longer sequence decoding (length penalty of 1.0). 6 Results and Analysis How much can encoder outputs be sparsified? We answer this question by analyzing the impact of pruning source encodings on the generation quality. We first train a base"
2021.findings-acl.255,P19-1500,0,0.0448246,"Missing"
2021.findings-acl.255,P02-1040,0,0.117752,"eriments. ¯ L is reused that the shortened source sequence X across decoder layers and steps. L0 D ROP changes the dependency of the encoder-decoder attention on source sequence from O(N M ) to O(N 0 M ), and allows for efficiency gains even with moderate sparsity, especially for large L, N and M . 5 Experimental Setup We study L0 D ROP on machine translation tasks (WMT14 English-German (En-De) (Bojar et al., 2014) and WMT18 Chinese-English (Zh-En) (Bojar et al., 2018)5 ) and document summarization tasks (CNN/Daily Mail (Hermann et al., 2015)6 and WikiSum (Liu et al., 2018)7 ). We adopt BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004) to evaluate the translation and summarization quality, respectively. Other details, including model settings, are given in the Appendix A. For characterbased translation, we employ the same model architecture and hyperparameters for training and decoding as specified in Appendix A, except that we adopt larger-batch training (∼85K characters) and encourage longer sequence decoding (length penalty of 1.0). 6 Results and Analysis How much can encoder outputs be sparsified? We answer this question by analyzing the impact of pruning source encodings on the generation qualit"
2021.findings-acl.255,W18-6319,0,0.0512902,"Missing"
2021.findings-acl.255,K16-1029,0,0.0534137,"Missing"
2021.findings-acl.255,E17-2060,1,0.833484,"e-dependent, which differs from the linear-time model of Wang et al. (2019), although both can accelerate decoding. Our study of rule-based sparsity patterns is in line with the sparse Transformer (Child et al., 2019) though we also explore the use of external linguistic information (POS tag) in our sparsification rules, and focus on encoder outputs instead of self-attention. Character-based translation gained increasing popularity due to its capability of handling out-ofvocabulary issues while avoiding tokenization and subword segmentation (Ling et al., 2015; Costajuss`a and Fonollosa, 2016; Sennrich, 2017; Cherry et al., 2018). Recent efforts often focus on closing the performance gap against its subword-level counterpart (Libovick´y and Fraser, 2020; Gao et al., 2020), but little study explores solutions to improve its inefficient inference resulted from the long character sequences. In this respect, Cherry et al. (2018) proposed to use conditional computation to dynamically compress encoder states. Similar to our results, they also observed a trade-off between the translation quality and the degree of compression. 3 Background: Transformer We take Transformer (Vaswani et al., 2017) as our te"
2021.findings-acl.255,P16-1162,1,0.505932,"Missing"
2021.findings-acl.255,P19-1032,0,0.0272384,"et al., 2016), sequential knowledge dis2889 tillation (Kim and Rush, 2016) or sparse attention induction that ranges from modeling hard attention (Wu et al., 2018) to developing differentiable sparse softmax functions or regularizing attention weights for sparsity (Niculae and Blondel, 2017; Correia et al., 2019; Cui et al., 2019; Zhang et al., 2019). Unfortunately, the success of all these studies builds upon the access to all source encodings in training and decoding. Learning which encoder outputs to prune in Seq2Seq models, to the best of our knowledge, has never been investigated before. Sukhbaatar et al. (2019) learn attention spans in self-attention and discard information from states outside of the span; this method is not directly applicable to encoder-decoder attention. We use the differentiable L0 -relaxation which was first introduced by Louizos et al. (2018) in the context of pruning individual neural network parameters. It was previously used to prune heads in multi-head attention (Voita et al., 2019). Our work is more similar in spirit to Bastings et al. (2019) where they used the L0 relaxations to construct interpretable classifiers, i.e. models that can reveal which words they rely on whe"
2021.findings-acl.255,N03-1033,0,0.194987,"efforts. By scheduling λ linearly with training steps, we can train models with L0 D ROP (Eq. (12)) from scratch, and obtain a BLEU score of 27.03 (λ = 0.2, warmup step of 200K) on WMT14 En-De (using subwords), comparable to using finetuning (27.04). What types of source encoding are required for generation? Our goal here it to understand encodings of which types of tokens are retained. For each source encoding, we regard the POS of its corresponding word as its type. We take WMT14 EnDe as our benchmark, where we annotate POS for source sentences in the test set using the Stanford POS tagger (Toutanova et al., 2003). We handle subwords separately by labeling its first piece as BPEH while the others as BPEO, regardless of the POS of its unsegmented form. We group different POS tags into 6 categories for the sake of analysis: BPEH, BPEO, function words, content words, punctuation and the rest.9 Figure 5 shows how the sparsity rate of each encoding type changes as a function of the overall sparsity rate. We find that L0 D ROP first choose to eliminate the encoding of punctuation, followed by that of function words. These words often signal structural and grammatical relationships that, while important to bu"
2021.findings-acl.255,P19-1580,1,0.823602,"on the access to all source encodings in training and decoding. Learning which encoder outputs to prune in Seq2Seq models, to the best of our knowledge, has never been investigated before. Sukhbaatar et al. (2019) learn attention spans in self-attention and discard information from states outside of the span; this method is not directly applicable to encoder-decoder attention. We use the differentiable L0 -relaxation which was first introduced by Louizos et al. (2018) in the context of pruning individual neural network parameters. It was previously used to prune heads in multi-head attention (Voita et al., 2019). Our work is more similar in spirit to Bastings et al. (2019) where they used the L0 relaxations to construct interpretable classifiers, i.e. models that can reveal which words they rely on when predicting a class. In their approach, the information from dropped words is lost rather than rerouted into the states of retained words, as desirable for interpretability but problematic in the text generation set-up. The number of the source encodings selected by L0 D ROP is sentence-dependent, which differs from the linear-time model of Wang et al. (2019), although both can accelerate decoding. Our"
2021.findings-acl.255,D19-1074,0,0.0231365,"ed to prune heads in multi-head attention (Voita et al., 2019). Our work is more similar in spirit to Bastings et al. (2019) where they used the L0 relaxations to construct interpretable classifiers, i.e. models that can reveal which words they rely on when predicting a class. In their approach, the information from dropped words is lost rather than rerouted into the states of retained words, as desirable for interpretability but problematic in the text generation set-up. The number of the source encodings selected by L0 D ROP is sentence-dependent, which differs from the linear-time model of Wang et al. (2019), although both can accelerate decoding. Our study of rule-based sparsity patterns is in line with the sparse Transformer (Child et al., 2019) though we also explore the use of external linguistic information (POS tag) in our sparsification rules, and focus on encoder outputs instead of self-attention. Character-based translation gained increasing popularity due to its capability of handling out-ofvocabulary issues while avoiding tokenization and subword segmentation (Ling et al., 2015; Costajuss`a and Fonollosa, 2016; Sennrich, 2017; Cherry et al., 2018). Recent efforts often focus on closing"
2021.findings-acl.255,D18-1473,0,0.050686,"Missing"
2021.findings-emnlp.60,P17-1183,0,0.042562,"Missing"
2021.findings-emnlp.60,P18-2049,0,0.0155191,"of adopting a strategy that inadvertently disadvantages certain languages.1 1 Introduction Data-driven subword-level segmentation of text (Sennrich et al., 2016; Kudo, 2018) is a well-known and widely used text representation strategy in the natural language processing (NLP) community. While subword segmentation largely solves the open vocabulary problem, previous research has shown that models often break down in outof-domain contexts (El Boukkouri et al., 2020), when encountering spelling errors (Belinkov and Bisk, 2018; Pruthi et al., 2019), when translating morphologically-rich languages (Ataman and Federico, 2018) and in multilingual scenarios (Chung et al., 2020; Wang et al., 2021). The reason for this is that even slight deviations from the text seen 1 Test suite and code available at https://github. com/ZurichNLP/segtest when learning a segmentation model can result in entirely different segmentations and often aggressively over-segmented text. Given the rich morphological diversity across natural languages, it is especially interesting to investigate the suitability of subword segmentation to represent different morphological phenomena. For example, reduplication is a non-concatenative morphologica"
2021.findings-emnlp.60,P17-1080,0,0.0237759,"generalise to non-concatenative morphological phenomena such as reduplication or vowel harmony, even in high-resource settings. Previous work that evaluated how well morphology or morpho-syntax is captured by sequence-tosequence models either used contrastive test sets to evaluate whether models assign a higher probability to sentences e.g. with correct subject-verb agreement (Sennrich, 2017; Marvin and Linzen, 2018; Warstadt et al., 2020) or probing classifiers to evaluate how well morphological features such as case, number or gender can be predicted from the models’ hidden representations (Belinkov et al., 2017; Vylomova et al., 2017; Dalvi et al., 2017; Bisazza and Tump, 2018; Belinkov et al., 2020). Isolated morphological analysis and reinflection have long been of interest to the NLP community, Our work is similar to Burlot and Yvon (2017) with yearly shared tasks (Kurimo et al., 2010; Vylo- who also evaluate morphological competence mova et al., 2020) that result in dedicated architec- based on the output of machine translation models tures that perform well for many languages (Aha- rather than probabilities or hidden states. However, roni and Goldberg, 2017; Makarov and Clematide, instead of mo"
2021.findings-emnlp.60,2020.cl-1.1,0,0.0224831,"mony, even in high-resource settings. Previous work that evaluated how well morphology or morpho-syntax is captured by sequence-tosequence models either used contrastive test sets to evaluate whether models assign a higher probability to sentences e.g. with correct subject-verb agreement (Sennrich, 2017; Marvin and Linzen, 2018; Warstadt et al., 2020) or probing classifiers to evaluate how well morphological features such as case, number or gender can be predicted from the models’ hidden representations (Belinkov et al., 2017; Vylomova et al., 2017; Dalvi et al., 2017; Bisazza and Tump, 2018; Belinkov et al., 2020). Isolated morphological analysis and reinflection have long been of interest to the NLP community, Our work is similar to Burlot and Yvon (2017) with yearly shared tasks (Kurimo et al., 2010; Vylo- who also evaluate morphological competence mova et al., 2020) that result in dedicated architec- based on the output of machine translation models tures that perform well for many languages (Aha- rather than probabilities or hidden states. However, roni and Goldberg, 2017; Makarov and Clematide, instead of morphological features, we are interested 2018; Wu and Cotterell, 2019; Rios et al., 2021). i"
2021.findings-emnlp.60,D18-1313,0,0.0199445,"duplication or vowel harmony, even in high-resource settings. Previous work that evaluated how well morphology or morpho-syntax is captured by sequence-tosequence models either used contrastive test sets to evaluate whether models assign a higher probability to sentences e.g. with correct subject-verb agreement (Sennrich, 2017; Marvin and Linzen, 2018; Warstadt et al., 2020) or probing classifiers to evaluate how well morphological features such as case, number or gender can be predicted from the models’ hidden representations (Belinkov et al., 2017; Vylomova et al., 2017; Dalvi et al., 2017; Bisazza and Tump, 2018; Belinkov et al., 2020). Isolated morphological analysis and reinflection have long been of interest to the NLP community, Our work is similar to Burlot and Yvon (2017) with yearly shared tasks (Kurimo et al., 2010; Vylo- who also evaluate morphological competence mova et al., 2020) that result in dedicated architec- based on the output of machine translation models tures that perform well for many languages (Aha- rather than probabilities or hidden states. However, roni and Goldberg, 2017; Makarov and Clematide, instead of morphological features, we are interested 2018; Wu and Cotterell, 201"
2021.findings-emnlp.60,W10-2211,0,0.041216,"ether models assign a higher probability to sentences e.g. with correct subject-verb agreement (Sennrich, 2017; Marvin and Linzen, 2018; Warstadt et al., 2020) or probing classifiers to evaluate how well morphological features such as case, number or gender can be predicted from the models’ hidden representations (Belinkov et al., 2017; Vylomova et al., 2017; Dalvi et al., 2017; Bisazza and Tump, 2018; Belinkov et al., 2020). Isolated morphological analysis and reinflection have long been of interest to the NLP community, Our work is similar to Burlot and Yvon (2017) with yearly shared tasks (Kurimo et al., 2010; Vylo- who also evaluate morphological competence mova et al., 2020) that result in dedicated architec- based on the output of machine translation models tures that perform well for many languages (Aha- rather than probabilities or hidden states. However, roni and Goldberg, 2017; Makarov and Clematide, instead of morphological features, we are interested 2018; Wu and Cotterell, 2019; Rios et al., 2021). in evaluating how well different types of morphoHowever, despite the large morphological diver- logical phenomena can be learned by sequence-tosity of natural languages, many approaches for se"
2021.findings-emnlp.60,2020.emnlp-main.203,0,0.0231618,"randomly dropped. Like other dropout methods (Srivastava et al., 2014; Zhou et al., 2020), BPE-dropout has a regularising effect and allows the model to generalise better to text segmented into smaller units. We also expect it to help generalisation across different occurrences of the same morpheme that would be segmented differently with deterministic subword segmentation. 4 A character-level model which is finetuned on the model with 500 merge-operations. This finetuning strategy allows training reasonably well-performing character-level models without the need for very deep architectures (Libovický and Fraser, 2020). We train Transformer Base machine translation models (Vaswani et al., 2017) with the nematus9 (Sennrich et al., 2017) framework. We train the first three models for 700k updates and choose the best checkpoint based on the BLEU score. This is evaluated on a dev set without synthetic morphological phenomena using SacreBLEU10 (Post, 2018). For the character-level model, we start the finetuning from the best checkpoint in the first 400k updates of the subword model with 500 merges. The characterlevel model is then finetuned for an additional 550k updates and we choose the best checkpoint based o"
2021.findings-emnlp.60,C18-1008,0,0.041571,"Missing"
2021.findings-emnlp.60,D18-1151,0,0.0151062,"ere to morpheme boundaries which can limit the generalisation to rare or unseen words and can lead to performance loss. Furthermore, it is unclear if models trained with subword segmentation can learn to generalise to non-concatenative morphological phenomena such as reduplication or vowel harmony, even in high-resource settings. Previous work that evaluated how well morphology or morpho-syntax is captured by sequence-tosequence models either used contrastive test sets to evaluate whether models assign a higher probability to sentences e.g. with correct subject-verb agreement (Sennrich, 2017; Marvin and Linzen, 2018; Warstadt et al., 2020) or probing classifiers to evaluate how well morphological features such as case, number or gender can be predicted from the models’ hidden representations (Belinkov et al., 2017; Vylomova et al., 2017; Dalvi et al., 2017; Bisazza and Tump, 2018; Belinkov et al., 2020). Isolated morphological analysis and reinflection have long been of interest to the NLP community, Our work is similar to Burlot and Yvon (2017) with yearly shared tasks (Kurimo et al., 2010; Vylo- who also evaluate morphological competence mova et al., 2020) that result in dedicated architec- based on th"
2021.findings-emnlp.60,W18-6319,0,0.0125723,"subword segmentation. 4 A character-level model which is finetuned on the model with 500 merge-operations. This finetuning strategy allows training reasonably well-performing character-level models without the need for very deep architectures (Libovický and Fraser, 2020). We train Transformer Base machine translation models (Vaswani et al., 2017) with the nematus9 (Sennrich et al., 2017) framework. We train the first three models for 700k updates and choose the best checkpoint based on the BLEU score. This is evaluated on a dev set without synthetic morphological phenomena using SacreBLEU10 (Post, 2018). For the character-level model, we start the finetuning from the best checkpoint in the first 400k updates of the subword model with 500 merges. The characterlevel model is then finetuned for an additional 550k updates and we choose the best checkpoint based on BLEU as for the other models. Our subword vocabularies are computed with byte pair encoding (Sennrich et al., 2016) using the SentencePiece implementation (Kudo and Richardson, 2018). We use a character coverage of 0.9999 to ensure the vocabulary for the model with 500 9 github.com/EdinburghNLP/nematus 10 BLEU+case.mixed+lang.deen+numr"
2021.findings-emnlp.60,E17-2025,0,0.0454688,"Missing"
2021.findings-emnlp.60,2020.acl-main.170,0,0.0354442,"t results with synthetically balanced test sets in Appendix A.3. 5.4 Model Description We train four neural machine translation models on our modified training data: 1 A subword-level BPE model (Sennrich et al., 8 We compare dev BLEU per checkpoint for models without and with added synthetic morphological phenomena and observe only an average absolute difference of 0.25 with 32k merges and 0.1 with 500 merges. 693 2016) with 32k merge-operations as a baseline and representation of current state-of-the-art models. 2 A subword-level BPE model with 32k merge-operations, trained with BPE-dropout (Provilkov et al., 2020). With BPE-dropout, the training data is resegmented after every epoch and at each merge step, some merges are randomly dropped. Like other dropout methods (Srivastava et al., 2014; Zhou et al., 2020), BPE-dropout has a regularising effect and allows the model to generalise better to text segmented into smaller units. We also expect it to help generalisation across different occurrences of the same morpheme that would be segmented differently with deterministic subword segmentation. 4 A character-level model which is finetuned on the model with 500 merge-operations. This finetuning strategy al"
2021.findings-emnlp.60,P19-1561,0,0.0230048,"ted on a range of typologically diverse languages to minimise the risk of adopting a strategy that inadvertently disadvantages certain languages.1 1 Introduction Data-driven subword-level segmentation of text (Sennrich et al., 2016; Kudo, 2018) is a well-known and widely used text representation strategy in the natural language processing (NLP) community. While subword segmentation largely solves the open vocabulary problem, previous research has shown that models often break down in outof-domain contexts (El Boukkouri et al., 2020), when encountering spelling errors (Belinkov and Bisk, 2018; Pruthi et al., 2019), when translating morphologically-rich languages (Ataman and Federico, 2018) and in multilingual scenarios (Chung et al., 2020; Wang et al., 2021). The reason for this is that even slight deviations from the text seen 1 Test suite and code available at https://github. com/ZurichNLP/segtest when learning a segmentation model can result in entirely different segmentations and often aggressively over-segmented text. Given the rich morphological diversity across natural languages, it is especially interesting to investigate the suitability of subword segmentation to represent different morphologi"
2021.findings-emnlp.60,2021.naacl-main.354,1,0.709055,"elinkov et al., 2020). Isolated morphological analysis and reinflection have long been of interest to the NLP community, Our work is similar to Burlot and Yvon (2017) with yearly shared tasks (Kurimo et al., 2010; Vylo- who also evaluate morphological competence mova et al., 2020) that result in dedicated architec- based on the output of machine translation models tures that perform well for many languages (Aha- rather than probabilities or hidden states. However, roni and Goldberg, 2017; Makarov and Clematide, instead of morphological features, we are interested 2018; Wu and Cotterell, 2019; Rios et al., 2021). in evaluating how well different types of morphoHowever, despite the large morphological diver- logical phenomena can be learned by sequence-tosity of natural languages, many approaches for sequence models, especially with a focus on their sentence-level sequence-to-sequence tasks are of- textual representation. Closely related is work by ten only tested on a subset of (morphologically Vania and Lopez (2017) who compare language similar) languages and then adopted without much model perplexities for different segmentation stratequestioning (Bender, 2011). gies on morphologically diverse lang"
2021.findings-emnlp.60,2020.acl-main.240,0,0.0239555,"Missing"
2021.findings-emnlp.60,2021.emnlp-main.576,0,0.0374069,"translate concatenative morphology. Generally, we expect that our controlled setting - where there is a one-to-one correspondence between artificial morphemes - is an idealised scenario and that models In the future, we are interested in evaluating likely perform worse in real-life settings with more a wider variety of text representation strategies, ambiguity and noise. However, our results do show including tokenisation-free input such as CANINE a clear gap between the models’ competence for (Clark et al., 2021) or visual text representations non-concatenative and concatenative morphology. (Salesky et al., 2021), although these are limited to Considering this performance gap and our reasons the source side. We would also like to investigate for evaluating in a semi-synthetic setup (see Sec- the effects of out-of-domain contexts where we tion 4), we think that our test suite offers a targeted expect more rare word stems. way to compare how well novel text representation strategies can learn non-concatenative phenomena. For vowel harmony, there is one factor in our set- Acknowledgements ting that may slightly increase its difficulty: only a few patterns in our data set exhibit vowel harmony. We thank o"
2021.findings-emnlp.60,E17-2060,1,0.908024,"uch as affixes, subword-level segmentations often do not adhere to morpheme boundaries which can hurt the performance of these models. For nonconcatenative morphology, it is still unclear to what extent subword-level models can learn to generalise to rare or unseen words. We believe these challenges are exciting opportunities to work on better text representations for cross-lingual NLP but currently, there is a lack of targeted evaluation environments. Most previous work evaluates very specific morphological or morpho-syntactic functions such as number, case, gender or subject-verb agreement (Sennrich, 2017; Burlot and Yvon, 2017; Warstadt et al., 2020) rather than evaluating how well different types of morphological phenomena can be learned. To address this issue, we design a test suite that can be used to evaluate how well a range of morphological phenomena can be learned with sentencelevel sequence-to-sequence models. We focus on 2 The distinction between non-concatenative and concatenative morphological phenomena is often a topic of debate in Linguistics. We follow Lieber and Štekauer (2014). 3 See reduplication feature in WALS (Rubino, 2013). 689 Findings of the Association for Computationa"
2021.findings-emnlp.60,E17-3017,1,0.884798,"Missing"
2021.findings-emnlp.60,P16-1162,1,0.733725,"subword- and character-level can translate these morphological phenomena. We find that learning to analyse and generate morphologically complex surface representations is still challenging, especially for nonconcatenative morphological phenomena like reduplication or vowel harmony and for rare word stems. Based on our results, we recommend that novel text representation strategies be tested on a range of typologically diverse languages to minimise the risk of adopting a strategy that inadvertently disadvantages certain languages.1 1 Introduction Data-driven subword-level segmentation of text (Sennrich et al., 2016; Kudo, 2018) is a well-known and widely used text representation strategy in the natural language processing (NLP) community. While subword segmentation largely solves the open vocabulary problem, previous research has shown that models often break down in outof-domain contexts (El Boukkouri et al., 2020), when encountering spelling errors (Belinkov and Bisk, 2018; Pruthi et al., 2019), when translating morphologically-rich languages (Ataman and Federico, 2018) and in multilingual scenarios (Chung et al., 2020; Wang et al., 2021). The reason for this is that even slight deviations from the te"
2021.findings-emnlp.60,W17-4704,0,0.0534434,"Missing"
2021.findings-emnlp.60,P17-1184,0,0.0200426,"(Aha- rather than probabilities or hidden states. However, roni and Goldberg, 2017; Makarov and Clematide, instead of morphological features, we are interested 2018; Wu and Cotterell, 2019; Rios et al., 2021). in evaluating how well different types of morphoHowever, despite the large morphological diver- logical phenomena can be learned by sequence-tosity of natural languages, many approaches for sequence models, especially with a focus on their sentence-level sequence-to-sequence tasks are of- textual representation. Closely related is work by ten only tested on a subset of (morphologically Vania and Lopez (2017) who compare language similar) languages and then adopted without much model perplexities for different segmentation stratequestioning (Bender, 2011). gies on morphologically diverse languages and by 690 Klein and Tsarfaty (2020) who show that multilingual BERT (Devlin et al., 2019) subwords do not reflect the morphological structure of a nonconcatenative language like Hebrew well. Our setup with synthetic morphological phenomena is similar to work by Wang and Eisner (2016) who generate synthetic treebanks by reordering nodes in existing treebanks for various natural languages. Instead of simp"
2021.findings-emnlp.60,Q16-1035,0,0.0246609,"sequence tasks are of- textual representation. Closely related is work by ten only tested on a subset of (morphologically Vania and Lopez (2017) who compare language similar) languages and then adopted without much model perplexities for different segmentation stratequestioning (Bender, 2011). gies on morphologically diverse languages and by 690 Klein and Tsarfaty (2020) who show that multilingual BERT (Devlin et al., 2019) subwords do not reflect the morphological structure of a nonconcatenative language like Hebrew well. Our setup with synthetic morphological phenomena is similar to work by Wang and Eisner (2016) who generate synthetic treebanks by reordering nodes in existing treebanks for various natural languages. Instead of simply reordering components, we need to apply a more complex preprocessing to generate synthetic morphological phenomena that fit the natural context. We discuss this preprocessing in more detail in Section 5.2. 3 Reduplication: Reduplication is another nonconcatenative morphological process in which the whole word (full reduplication) or a part of a word (partial reduplication) is repeated exactly or with a slight change. In some cases, the repetition can also occur twice (tr"
2021.findings-emnlp.60,2021.naacl-main.39,0,0.0252296,"Introduction Data-driven subword-level segmentation of text (Sennrich et al., 2016; Kudo, 2018) is a well-known and widely used text representation strategy in the natural language processing (NLP) community. While subword segmentation largely solves the open vocabulary problem, previous research has shown that models often break down in outof-domain contexts (El Boukkouri et al., 2020), when encountering spelling errors (Belinkov and Bisk, 2018; Pruthi et al., 2019), when translating morphologically-rich languages (Ataman and Federico, 2018) and in multilingual scenarios (Chung et al., 2020; Wang et al., 2021). The reason for this is that even slight deviations from the text seen 1 Test suite and code available at https://github. com/ZurichNLP/segtest when learning a segmentation model can result in entirely different segmentations and often aggressively over-segmented text. Given the rich morphological diversity across natural languages, it is especially interesting to investigate the suitability of subword segmentation to represent different morphological phenomena. For example, reduplication is a non-concatenative morphological phenomenon2 that is common across the world’s languages, but is marg"
2021.findings-emnlp.60,P19-1148,0,0.0134621,"isazza and Tump, 2018; Belinkov et al., 2020). Isolated morphological analysis and reinflection have long been of interest to the NLP community, Our work is similar to Burlot and Yvon (2017) with yearly shared tasks (Kurimo et al., 2010; Vylo- who also evaluate morphological competence mova et al., 2020) that result in dedicated architec- based on the output of machine translation models tures that perform well for many languages (Aha- rather than probabilities or hidden states. However, roni and Goldberg, 2017; Makarov and Clematide, instead of morphological features, we are interested 2018; Wu and Cotterell, 2019; Rios et al., 2021). in evaluating how well different types of morphoHowever, despite the large morphological diver- logical phenomena can be learned by sequence-tosity of natural languages, many approaches for sequence models, especially with a focus on their sentence-level sequence-to-sequence tasks are of- textual representation. Closely related is work by ten only tested on a subset of (morphologically Vania and Lopez (2017) who compare language similar) languages and then adopted without much model perplexities for different segmentation stratequestioning (Bender, 2011). gies on morpholo"
2021.findings-emnlp.60,2020.findings-emnlp.178,0,0.0124026,"t al., 8 We compare dev BLEU per checkpoint for models without and with added synthetic morphological phenomena and observe only an average absolute difference of 0.25 with 32k merges and 0.1 with 500 merges. 693 2016) with 32k merge-operations as a baseline and representation of current state-of-the-art models. 2 A subword-level BPE model with 32k merge-operations, trained with BPE-dropout (Provilkov et al., 2020). With BPE-dropout, the training data is resegmented after every epoch and at each merge step, some merges are randomly dropped. Like other dropout methods (Srivastava et al., 2014; Zhou et al., 2020), BPE-dropout has a regularising effect and allows the model to generalise better to text segmented into smaller units. We also expect it to help generalisation across different occurrences of the same morpheme that would be segmented differently with deterministic subword segmentation. 4 A character-level model which is finetuned on the model with 500 merge-operations. This finetuning strategy allows training reasonably well-performing character-level models without the need for very deep architectures (Libovický and Fraser, 2020). We train Transformer Base machine translation models (Vaswani"
2021.findings-emnlp.60,W17-4115,0,0.0208536,"tenative morphological phenomena such as reduplication or vowel harmony, even in high-resource settings. Previous work that evaluated how well morphology or morpho-syntax is captured by sequence-tosequence models either used contrastive test sets to evaluate whether models assign a higher probability to sentences e.g. with correct subject-verb agreement (Sennrich, 2017; Marvin and Linzen, 2018; Warstadt et al., 2020) or probing classifiers to evaluate how well morphological features such as case, number or gender can be predicted from the models’ hidden representations (Belinkov et al., 2017; Vylomova et al., 2017; Dalvi et al., 2017; Bisazza and Tump, 2018; Belinkov et al., 2020). Isolated morphological analysis and reinflection have long been of interest to the NLP community, Our work is similar to Burlot and Yvon (2017) with yearly shared tasks (Kurimo et al., 2010; Vylo- who also evaluate morphological competence mova et al., 2020) that result in dedicated architec- based on the output of machine translation models tures that perform well for many languages (Aha- rather than probabilities or hidden states. However, roni and Goldberg, 2017; Makarov and Clematide, instead of morphological features, w"
2021.iwslt-1.19,P18-1163,0,0.0185638,".9 44.5 26.4 29.5 30.1 27.0 34.5 23.0 31.1 31.41 Cascading model: base ASR model + 24-layer MT model 33.3 26.8 28.6 39.9 23.7 26.9 26.8 23.6 30.0 19.7 26.7 27.82 Single E2E Model: multilingual ST model + 6 layers, big Transformer 35.0 29.9 31.9 44.1 25.5 28.8 29.0 26.2 33.3 22.4 30.1 30.56 Table 5: SacreBLEU↑ for our submissions to the IWSLT2021 multilingual ST task. didn’t bias our MT model towards ASR outputs, and the mismatch between gold transcripts and ASR outputs often hurts cascading performance. Recent advances on avoiding such error propagation might deliver better cascading results (Cheng et al., 2018; Zhang et al., 2019b; Cheng et al., 2019; Sperber et al., 2019). Our final submission is an ensemble of 6 E2E multilingual ST models, which reaches an average BLEU of 31.34. Apart from the ensemble, we also increase the decoding length penalty from 0.6 to 0.9, which performs slightly better. 5 Submission Results The IWSLT2021 task prepares a held-out test set for the final evaluation. We submitted three systems: one cascading system, one E2E single model (w/ big ST Transformer) and one ensemble model. Results are shown in Table 5: our E2E multilingual ST model outperforms its cascading counte"
2021.iwslt-1.19,D16-1026,0,0.034598,"Missing"
2021.iwslt-1.19,2021.eacl-main.57,0,0.062349,"Missing"
2021.iwslt-1.19,2020.iwslt-1.8,0,0.61276,"Missing"
2021.iwslt-1.19,D16-1139,0,0.0276814,". RMSNorm yields comparable performance to LayerNorm in a series of experiments (Zhang and Sennrich, 2019b) and show great scalability in large-scale pretraining (Narang et al., 2021). We apply RMSNorm to the ST encoder and decoder, which benefits the training of deep and big Transformers. 2.6 Data Augmentation Data augmentation (DA) is an effective strategy for low-resource tasks by increasing the training corpus with pseudo-labelled samples (Sennrich et al., 2016a; Zhang and Zong, 2016). Methods for generating such samples vary greatly, and we adopt the one following knowledge distillation (Kim and Rush, 2016). Note, prior to our study, knowledge distillation has already been successfully applied to ST tasks (Liu et al., 2019; Gaido et al., 2020). We regard the multilingual MT as the teacher since textbased translation is much easier than and almost upper-bounds the speech-based counterpart (Zhang et al., 2020a), and transfer its knowledge into our multilingual ST (student). Concretely, we first train a multilingual MT model and then use it to translate each source transcript into all possible ST directions, including the zero-shot ones, based on beam search algorithm. We directly concatenate the g"
2021.iwslt-1.19,P07-2045,0,0.0081875,"2 2.7 Multi-Task Learning Multi-task learning aims at improving task performance by jointly modeling different tasks within one framework. Particularly, when tasks are of high correlation, they tend to benefit each other and deEs Fr Pt It a step size of 10ms and window size of 25ms as the acoustic features, followed by feature expansion via second-order derivatives and mean-variance normalization. The final acoustic input is 360dimensional, a concatenation of the features corresponding to three consecutive and non-overlapping frames. We tokenize and truecase all text data using Moses scripts (Koehn et al., 2007). We adopt subword processing (Sennrich et al., 2016b) with 8K merging operations (Sennrich and Zhang, 2019) on these texts to handle rare words. Note we use different subword models (but with the same vocabulary size) for ST, ASR and MT. Target Languages Speech En Es Fr Pt It 36K/102K 30K/116K 31K/90K -/50K 102K/21K/116K -/90K -/50K 3.6K/102K 116K/-/90K -/50K 21K/102K 13K/116K 90K/-/50K 5.6K/102K -/116K -/90K 50K/- Table 1: Statistics for ST training data used for the IWSLT2021 multilingual ST task. “-”: denotes no data available. “a/b”: “a” denotes genuine data while “b” is for augmented dat"
2021.iwslt-1.19,2021.emnlp-main.465,0,0.0430042,"izes network activations and improves model performance (Ba et al., 2016), but raises non-negligible computational overheads reducing net efficiency, particularly to recurrent models (Zhang and Sennrich, 2019a). To overcome such overhead, Zhang and Sennrich (2019b) propose root mean square layer normalization (RMSNorm) which relies on root mean square statistic alone to regularize activations and is a drop-in replacement to LayerNorm. RMSNorm yields comparable performance to LayerNorm in a series of experiments (Zhang and Sennrich, 2019b) and show great scalability in large-scale pretraining (Narang et al., 2021). We apply RMSNorm to the ST encoder and decoder, which benefits the training of deep and big Transformers. 2.6 Data Augmentation Data augmentation (DA) is an effective strategy for low-resource tasks by increasing the training corpus with pseudo-labelled samples (Sennrich et al., 2016a; Zhang and Zong, 2016). Methods for generating such samples vary greatly, and we adopt the one following knowledge distillation (Kim and Rush, 2016). Note, prior to our study, knowledge distillation has already been successfully applied to ST tasks (Liu et al., 2019; Gaido et al., 2020). We regard the multiling"
2021.iwslt-1.19,P19-1146,0,0.0167511,"the ASR and ST encoder to the source and target language, respectively. Following multilingual translation (Johnson et al., 2017; Gangi et al., 2019; Inaguma et al., 2019), we adopt language embedding (such as “[en], [es]”) but add it to the inputs rather than appending an extra token. 2.4 Sparsified Linear Attention Attention, as the key component in Transformer, takes the main responsibility to capture token-wise dependencies. However, not all tokens are semantically correlated, inspiring follow-up studies on sparsified attention that could explicitly zero-out some attention probabilities (Peters et al., 2019; Zhang et al., 2021b). Recently, Zhang et al. (2021b) propose rectified linear attention (ReLA) which directly induces sparse structures by enforcing ReLU activation on the attention logits. ReLA has achieved comparable performance on several MT tasks with the advantage of high computational efficiency against the sparsified softmax models (Peters et al., 2019). Results on MT show that ReLA delivers better performance when applied to Transformer decoder (Zhang et al., 2021b). We follow this practice and apply it to the ST decoder. Our study also demonstrates that ReLA generalizes well to ST."
2021.iwslt-1.19,W18-6319,0,0.0187622,"33.0 104.3 57.8 39.49 Ablation Study + 6 layers w/o LS layer 18.0 19.5 23.2 21.6 40.8 35.2 97.8 62.6 39.84 Table 3: WER↓ for ASR on Multilingual TEDx testsets. ? : results reported by Salesky et al. (2021). Best results are highlighted in bold. model used for evaluation is averaged over the last 5 checkpoints. Note, while the training data size varies across languages, we follow the original data distribution and adopt no specific sampling strategies for all multilingual experiments. Evaluation We evaluate translation quality using tokenized case-sensitive (Sacre)BLEU (Papineni et al., 2002; Post, 2018), and report WER for ASR performance without punctuation on lowercased text. In ST experiments, we observe some repeated translations decreasing BLEU. We automatically post-process translations by removing repeated chunks of up to 10 words. 4 4.1 Results ers), even though the dataset is small. Note the benefit from increased depth diminishes as the depth goes larger (+0.55 average BLEU, 12 layers → 24 layers). We find that language-specific modeling slightly improves translation performance (+0.12 average BLEU). Such improvement seems uninteresting particularly compared to the significant gain"
2021.iwslt-1.19,P19-1179,0,0.0540098,"Missing"
2021.iwslt-1.19,P16-1009,1,0.893004,"quare layer normalization (RMSNorm) which relies on root mean square statistic alone to regularize activations and is a drop-in replacement to LayerNorm. RMSNorm yields comparable performance to LayerNorm in a series of experiments (Zhang and Sennrich, 2019b) and show great scalability in large-scale pretraining (Narang et al., 2021). We apply RMSNorm to the ST encoder and decoder, which benefits the training of deep and big Transformers. 2.6 Data Augmentation Data augmentation (DA) is an effective strategy for low-resource tasks by increasing the training corpus with pseudo-labelled samples (Sennrich et al., 2016a; Zhang and Zong, 2016). Methods for generating such samples vary greatly, and we adopt the one following knowledge distillation (Kim and Rush, 2016). Note, prior to our study, knowledge distillation has already been successfully applied to ST tasks (Liu et al., 2019; Gaido et al., 2020). We regard the multilingual MT as the teacher since textbased translation is much easier than and almost upper-bounds the speech-based counterpart (Zhang et al., 2020a), and transfer its knowledge into our multilingual ST (student). Concretely, we first train a multilingual MT model and then use it to transla"
2021.iwslt-1.19,P16-1162,1,0.589548,"quare layer normalization (RMSNorm) which relies on root mean square statistic alone to regularize activations and is a drop-in replacement to LayerNorm. RMSNorm yields comparable performance to LayerNorm in a series of experiments (Zhang and Sennrich, 2019b) and show great scalability in large-scale pretraining (Narang et al., 2021). We apply RMSNorm to the ST encoder and decoder, which benefits the training of deep and big Transformers. 2.6 Data Augmentation Data augmentation (DA) is an effective strategy for low-resource tasks by increasing the training corpus with pseudo-labelled samples (Sennrich et al., 2016a; Zhang and Zong, 2016). Methods for generating such samples vary greatly, and we adopt the one following knowledge distillation (Kim and Rush, 2016). Note, prior to our study, knowledge distillation has already been successfully applied to ST tasks (Liu et al., 2019; Gaido et al., 2020). We regard the multilingual MT as the teacher since textbased translation is much easier than and almost upper-bounds the speech-based counterpart (Zhang et al., 2020a), and transfer its knowledge into our multilingual ST (student). Concretely, we first train a multilingual MT model and then use it to transla"
2021.iwslt-1.19,P19-1021,1,0.900233,"ifferent tasks within one framework. Particularly, when tasks are of high correlation, they tend to benefit each other and deEs Fr Pt It a step size of 10ms and window size of 25ms as the acoustic features, followed by feature expansion via second-order derivatives and mean-variance normalization. The final acoustic input is 360dimensional, a concatenation of the features corresponding to three consecutive and non-overlapping frames. We tokenize and truecase all text data using Moses scripts (Koehn et al., 2007). We adopt subword processing (Sennrich et al., 2016b) with 8K merging operations (Sennrich and Zhang, 2019) on these texts to handle rare words. Note we use different subword models (but with the same vocabulary size) for ST, ASR and MT. Target Languages Speech En Es Fr Pt It 36K/102K 30K/116K 31K/90K -/50K 102K/21K/116K -/90K -/50K 3.6K/102K 116K/-/90K -/50K 21K/102K 13K/116K 90K/-/50K 5.6K/102K -/116K -/90K 50K/- Table 1: Statistics for ST training data used for the IWSLT2021 multilingual ST task. “-”: denotes no data available. “a/b”: “a” denotes genuine data while “b” is for augmented data. liver positive knowledge transfer. With datasets of different tasks combined, this also partially allevia"
2021.iwslt-1.19,P19-1115,0,0.0244186,"del: base ASR model + 24-layer MT model 33.3 26.8 28.6 39.9 23.7 26.9 26.8 23.6 30.0 19.7 26.7 27.82 Single E2E Model: multilingual ST model + 6 layers, big Transformer 35.0 29.9 31.9 44.1 25.5 28.8 29.0 26.2 33.3 22.4 30.1 30.56 Table 5: SacreBLEU↑ for our submissions to the IWSLT2021 multilingual ST task. didn’t bias our MT model towards ASR outputs, and the mismatch between gold transcripts and ASR outputs often hurts cascading performance. Recent advances on avoiding such error propagation might deliver better cascading results (Cheng et al., 2018; Zhang et al., 2019b; Cheng et al., 2019; Sperber et al., 2019). Our final submission is an ensemble of 6 E2E multilingual ST models, which reaches an average BLEU of 31.34. Apart from the ensemble, we also increase the decoding length penalty from 0.6 to 0.9, which performs slightly better. 5 Submission Results The IWSLT2021 task prepares a held-out test set for the final evaluation. We submitted three systems: one cascading system, one E2E single model (w/ big ST Transformer) and one ensemble model. Results are shown in Table 5: our E2E multilingual ST model outperforms its cascading counterpart, and the ensemble model reaches the best performance. Our"
2021.iwslt-1.19,P19-1176,0,0.0181902,"p Transformer, Zhang et al. (2019a) propose depth-scaled initialization (DSInit) that only requires changing parameter initialization without any architectural modification. DSInit successfully helps to train up to 30-layer Transformer, substantially improving bilingual and also massively multilingual translation (Zhang et al., 2019a, 2020c). We adopt this strategy for all deep Transformer experiments. Apart from DS-Init, researchers also find that changing the post-norm structure to its pre-norm alternative improves Transformer’s robustness to deep modeling, albeit slightly reducing quality (Wang et al., 2019; Zhang et al., 2019a). We 161 keep using post-norm Transformer for most modules but apply the pre-norm structure to the ASR encoder to stabilize the encoding of speeches from different languages. 2.3 Language-Specific Modeling Analogous to multi-task learning, multilingual translation benefits from inter-task transfer learning but suffers from task interference. How to balance between shared modeling and languagespecific (LS) modeling so as to maximize the transfer effect and avoid the interference remains challenging. A recent study suggests that scheduling language-specific modeling to top"
2021.iwslt-1.19,P19-1149,1,0.931096,"ive feature selection (AFS) framework (Zhang et al., 2020a,b) as shown in Figure 1. AFS is capable of filtering out uninformative speech features contributing little to ASR, effectively reducing speech redundancy and improving ST performance (Zhang et al., 2020a). We adapt AFS to multilingual ST, and further incorporate several techniques that encourage transfer learning and larger capacity modeling, ranging from language-specific modeling, multi-task learning, deep and big Transformer, sparsified linear attention (ReLA) (Zhang et al., 2021b) to root mean square layer normalization (RMSNorm) (Zhang and Sennrich, 2019b). Inspired by Zhang et al. (2020c), we convert the zero-shot translation problem into a zero-resource one via data augmentation with multilingual MT models. 160 Proceedings of the 18th International Conference on Spoken Language Translation, pages 160–168 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics a) ASR Pretraining w/ Adaptive Feature Selection b) ST Training with AFS-Filtered Features [es] mapping AFS ASR Decoder Post-norm transformer decoder Thank you. Tgt LS Layer [en] mapping ST Encoder Src LS Layer Post-norm Transformer w/ RMSNorm ASR"
2021.iwslt-1.19,2020.findings-emnlp.230,1,0.112983,"sing machine translation models for ST which converts the zero-shot problem into a zero-resource one. Experimental results show that these methods deliver substantial improvements, surpassing the official baseline by > 15 average BLEU and outperforming our cascading system by > 2 average BLEU. Our final submission achieves competitive performance (runner up).1 1 Introduction Although end-to-end (E2E) speech translation (ST) has achieved great success in recent years, outperforming its cascading counterpart and delivering state-of-the-art performance on several benchmarks (Ansari et al., 2020; Zhang et al., 2020a; Zhao et al., 2020), it still suffers from the relatively low amounts of dedicated speech-to-translation parallel training data (Salesky et al., 2021). In textbased machine translation (MT), one solution to lack of training data is to jointly perform multilingual translation with the benefit of transferring knowledge across similar languages and to lowresource directions, and even enabling zero-shot 1 Source code and pretrained models are available at https://github.com/bzhangGo/zero. translation, i.e. direct translation between language pairs unseen in training (Firat et al., 2016; Johnson"
2021.iwslt-1.19,D19-1083,1,0.877463,"Missing"
2021.iwslt-1.19,2021.emnlp-main.523,1,0.916572,"ransformer (Vaswani et al., 2017) as the backbone, and follows the adaptive feature selection (AFS) framework (Zhang et al., 2020a,b) as shown in Figure 1. AFS is capable of filtering out uninformative speech features contributing little to ASR, effectively reducing speech redundancy and improving ST performance (Zhang et al., 2020a). We adapt AFS to multilingual ST, and further incorporate several techniques that encourage transfer learning and larger capacity modeling, ranging from language-specific modeling, multi-task learning, deep and big Transformer, sparsified linear attention (ReLA) (Zhang et al., 2021b) to root mean square layer normalization (RMSNorm) (Zhang and Sennrich, 2019b). Inspired by Zhang et al. (2020c), we convert the zero-shot translation problem into a zero-resource one via data augmentation with multilingual MT models. 160 Proceedings of the 18th International Conference on Spoken Language Translation, pages 160–168 Bangkok, Thailand (Online), August 5–6, 2021. ©2021 Association for Computational Linguistics a) ASR Pretraining w/ Adaptive Feature Selection b) ST Training with AFS-Filtered Features [es] mapping AFS ASR Decoder Post-norm transformer decoder Thank you. Tgt LS La"
2021.iwslt-1.19,2020.acl-main.148,1,0.0816458,"sing machine translation models for ST which converts the zero-shot problem into a zero-resource one. Experimental results show that these methods deliver substantial improvements, surpassing the official baseline by > 15 average BLEU and outperforming our cascading system by > 2 average BLEU. Our final submission achieves competitive performance (runner up).1 1 Introduction Although end-to-end (E2E) speech translation (ST) has achieved great success in recent years, outperforming its cascading counterpart and delivering state-of-the-art performance on several benchmarks (Ansari et al., 2020; Zhang et al., 2020a; Zhao et al., 2020), it still suffers from the relatively low amounts of dedicated speech-to-translation parallel training data (Salesky et al., 2021). In textbased machine translation (MT), one solution to lack of training data is to jointly perform multilingual translation with the benefit of transferring knowledge across similar languages and to lowresource directions, and even enabling zero-shot 1 Source code and pretrained models are available at https://github.com/bzhangGo/zero. translation, i.e. direct translation between language pairs unseen in training (Firat et al., 2016; Johnson"
2021.iwslt-1.19,D18-1459,1,0.846548,"data while “b” is for augmented data. liver positive knowledge transfer. With datasets of different tasks combined, this also partially alleviates data scarcity. We adopt multi-task learning by augmenting translation tasks with transcription tasks. We incorporate the ASR tasks for multilingual ST, and auto-encoding tasks (transcript-to-transcript in the same language) for multilingual MT. 3 Experimental Settings In this section, we explain the used datasets, model architectures, optimization details and evaluation metrics in our experiments. All implementations are based on the zero2 toolkit (Zhang et al., 2018). Data We participate in the constrained setting, where only the provided data, i.e. Multilingual TEDx (Salesky et al., 2021), is permitted. Multilingual TEDx collects audios from TEDx talks in 8 source languages (Spanish/Es, French/Fr, Portuguese/Pt, Italian/It, Russian/Ru, Greek/El, Arabic/Ar, German/De) paired with their manual transcriptions, covering translations into 5 target languages (English/En, Es, Fr, Pt, It). It contains supervised training data for 13 ST directions, three of which (Pt-Es, It-En, It-Es) are masked-out for zero-shot evaluation. ASR training data is given for all 8 s"
2021.iwslt-1.19,D16-1160,0,0.0228485,"n (RMSNorm) which relies on root mean square statistic alone to regularize activations and is a drop-in replacement to LayerNorm. RMSNorm yields comparable performance to LayerNorm in a series of experiments (Zhang and Sennrich, 2019b) and show great scalability in large-scale pretraining (Narang et al., 2021). We apply RMSNorm to the ST encoder and decoder, which benefits the training of deep and big Transformers. 2.6 Data Augmentation Data augmentation (DA) is an effective strategy for low-resource tasks by increasing the training corpus with pseudo-labelled samples (Sennrich et al., 2016a; Zhang and Zong, 2016). Methods for generating such samples vary greatly, and we adopt the one following knowledge distillation (Kim and Rush, 2016). Note, prior to our study, knowledge distillation has already been successfully applied to ST tasks (Liu et al., 2019; Gaido et al., 2020). We regard the multilingual MT as the teacher since textbased translation is much easier than and almost upper-bounds the speech-based counterpart (Zhang et al., 2020a), and transfer its knowledge into our multilingual ST (student). Concretely, we first train a multilingual MT model and then use it to translate each source transcrip"
2021.iwslt-1.19,P19-1649,0,0.0235709,".1 27.0 34.5 23.0 31.1 31.41 Cascading model: base ASR model + 24-layer MT model 33.3 26.8 28.6 39.9 23.7 26.9 26.8 23.6 30.0 19.7 26.7 27.82 Single E2E Model: multilingual ST model + 6 layers, big Transformer 35.0 29.9 31.9 44.1 25.5 28.8 29.0 26.2 33.3 22.4 30.1 30.56 Table 5: SacreBLEU↑ for our submissions to the IWSLT2021 multilingual ST task. didn’t bias our MT model towards ASR outputs, and the mismatch between gold transcripts and ASR outputs often hurts cascading performance. Recent advances on avoiding such error propagation might deliver better cascading results (Cheng et al., 2018; Zhang et al., 2019b; Cheng et al., 2019; Sperber et al., 2019). Our final submission is an ensemble of 6 E2E multilingual ST models, which reaches an average BLEU of 31.34. Apart from the ensemble, we also increase the decoding length penalty from 0.6 to 0.9, which performs slightly better. 5 Submission Results The IWSLT2021 task prepares a held-out test set for the final evaluation. We submitted three systems: one cascading system, one E2E single model (w/ big ST Transformer) and one ensemble model. Results are shown in Table 5: our E2E multilingual ST model outperforms its cascading counterpart, and the ensem"
2021.naacl-main.354,2020.acl-main.688,1,0.768312,"s to a similar improvement to previous works on hard monotonic attention for RNNs, whereas for transformer models, the results are mixed: Biasing all attention heads towards monotonicity may limit the representation power of multihead attention in a way Many sequence-to-sequence tasks in natural language processing are roughly monotonic in the alignment between source and target sequence, and previous work has focused on learning monotonic attention behavior either through specialized attention functions (Aharoni and Goldberg, 2017; Raffel et al., 2017; Wu and Cotterell, 2019) or pretraining (Aji et al., 2020). However, it is non-trivial to port specialized attention functions to different models, and recently, Yolchuyeva et al. (2019); Wu et al. (2021) found that a transformer model (Vaswani et al., 2017) outperforms previous work on monotone tasks such as grapheme-to-phoneme conversion, despite having no mechanism that biases the model towards monotonicity. In the transformer, it is less straightforward to what extent individual encoder states, especially 1 in deeper layers, still represent distinct source inCode and scripts available at: https://github. puts after passing through several self-at"
2021.naacl-main.354,2020.coling-main.304,0,0.0371299,"rence of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4474–4488 June 6–11, 2021. ©2021 Association for Computational Linguistics that is harmful even for monotonic sequence-tosequence tasks. However, for some tasks, we see small improvements when limiting monotonicity to only a subset of heads. 2 Related Work Recently, the transformer architecture (Vaswani et al., 2017) has outperformed RNNs in lowresource settings for character-level transduction tasks (Yolchuyeva et al., 2019; Wu et al., 2021) and neural machine translation (Araabi and Monz, 2020). While there has been some work on extending the methods of Raffel et al. (2017); Chiu and Raffel (2018); Arivazhagan et al. (2019) to multihead attention (Ma et al., 2020; Liu et al., 2020), we are not aware of any work that studied monotonicity in transformers for monotonic tasks, such as grapheme-to-phoneme conversion, transliteration, or morphological inflection. Attention models (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) are a very powerful and flexible mechanism to learn the relationship between source and target sequences, but the flexibility might come at the co"
2021.naacl-main.354,P19-1126,0,0.0402083,"Missing"
2021.naacl-main.354,N16-1102,0,0.0202388,"20), we are not aware of any work that studied monotonicity in transformers for monotonic tasks, such as grapheme-to-phoneme conversion, transliteration, or morphological inflection. Attention models (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) are a very powerful and flexible mechanism to learn the relationship between source and target sequences, but the flexibility might come at the cost of making the relationship harder to learn. Previous work has shown that their performance can be improved by To this end, we propose a model-agnostic monointroducing inductive biases. Cohn et al. (2016) tonicity loss that can seamlessly be integrated into introduce various structural alignment biases into RNNs as well as the transformer. Our monotonica neural machine translation model, including a ity loss captures how monotone the soft attention positional bias. While this bias is motivated by the behaves during training, while two hyperparamefact that a given token in the source often aligns ters allow us to control how much monotonicity is with a target token at a similar relative position, it enforced. By encouraging monotonicity through does not explicitly encourage monotonicity. a loss"
2021.naacl-main.354,D18-1317,0,0.0379864,"Missing"
2021.naacl-main.354,D15-1166,0,0.390037,"2017) has outperformed RNNs in lowresource settings for character-level transduction tasks (Yolchuyeva et al., 2019; Wu et al., 2021) and neural machine translation (Araabi and Monz, 2020). While there has been some work on extending the methods of Raffel et al. (2017); Chiu and Raffel (2018); Arivazhagan et al. (2019) to multihead attention (Ma et al., 2020; Liu et al., 2020), we are not aware of any work that studied monotonicity in transformers for monotonic tasks, such as grapheme-to-phoneme conversion, transliteration, or morphological inflection. Attention models (Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) are a very powerful and flexible mechanism to learn the relationship between source and target sequences, but the flexibility might come at the cost of making the relationship harder to learn. Previous work has shown that their performance can be improved by To this end, we propose a model-agnostic monointroducing inductive biases. Cohn et al. (2016) tonicity loss that can seamlessly be integrated into introduce various structural alignment biases into RNNs as well as the transformer. Our monotonica neural machine translation model, including a ity loss captures how mon"
2021.naacl-main.354,P02-1040,0,0.108775,"Missing"
2021.naacl-main.354,W18-6319,0,0.060472,"Missing"
2021.naacl-main.354,P16-1162,1,0.385666,"et al. (2021) in transformers (smaller feed-forward dimensions of 512). We early stop training as for grapheme-to-phoneme conversion. We evaluate our models following Zhang et al. (2015) and compute word-level accuracy (ACC) and character-level mean F-score (MFS). The formula for MFS is in Appendix A.1. Dialect Normalization es isch aber als Kompliment gmeint gsi es war aber als Kompliment gemeint it was however as compliment meant Figure 3: Swiss-German to German dialect normalization example with verb reordering. illustrated in Figure 3. The models are trained on subwords obtained via BPE (Sennrich et al., 2016), created with subwordnmt computing 2000 merges. We treat this as a lowresource machine translation task, and thus follow hyperparameters by Sennrich and Zhang (2019) for the RNN models, while the transformer models are trained according to Araabi and Monz (2020). We evaluate our models with BLEU (Papineni et al., 2002).8 4.2 Results In addition to task-specific evaluation metrics, we use the loss function to score the monotonicity of the attention on the test set for all models (reported as LM ON O ). Furthermore, we report the percentage of decoding states for which the average source attent"
2021.naacl-main.354,P19-1021,1,0.902381,"Missing"
2021.naacl-main.354,2020.findings-emnlp.178,0,0.179067,"n Appendix A.2. The monotonic loss function is controlled by a hyperparameter for the margin (δ), and an additional scaling factor for the loss itself (λ). Preliminary experiments have shown that the monotonicity loss has an undesirable interaction with attention dropout, which is commonly used in transformer models. Randomly dropping attention connections during training makes it harder to reliably avoid a decrease in the mean attended position, favoring a degenerate local optimum where attention resides constantly on the first (or last) encoder state. To avoid this problem, we use DropHead (Zhou et al., 2020) instead, which has a similar regularizing effect as attention dropout, but does not interact with the monotonicity loss. In addition to the standard evaluation metrics used in each task, we provide the monotonicity loss on the test set and the percentage of target tokens for which the average source attention position has increased (by some margin). We perform experiments on three word-level and one sentence-level sequence-to-sequence tasks: Grapheme-to-Phoneme Conversion For grapheme-to-phoneme conversion, we use NETtalk (Sejnowski and Rosenberg, 1987)4 and CMUdict,5 two datasets for English"
2021.naacl-main.354,P19-1580,1,0.764205,"e margin δ can influence the monotonicity loss with some examples. In equation 4, costs are later summed over the target sequence. In practice, we normalize the cost by the number of tokens in a batch for training stability, as is typically done for the cross-entropy loss. If a model has multiple attention mechanisms, e.g. attention in multiple layers, or multihead attention, we separately compute the loss for each attention mechanism, then average the losses. We can also just apply the loss to a subset of attention mechanisms, allowing different attention heads to learn specialized behavior (Voita et al., 2019). 3 Making the cost relative to the source sequence length ensures that the worst-case cost per timestep is independent of source sequence length. 4476 4 4.1 vanilla Experiments V 0 Models and Data We implement the loss function in sockeye (Hieber et al., 2018), and experiment with RNN and transformer models. We list the specific baseline settings for each task in Appendix A.2. The monotonic loss function is controlled by a hyperparameter for the margin (δ), and an additional scaling factor for the loss itself (λ). Preliminary experiments have shown that the monotonicity loss has an undesirabl"
2021.naacl-main.354,W18-2404,0,0.0417157,"Missing"
2021.naacl-main.354,P19-1148,0,0.472008,"different sequence-to-sequence models and tasks. To this end, we introduce a loss function that measures and rewards monotonic behavior of the attention mechanism.1 We perform experiments and analysis on a variety of sequence-to-sequence tasks where we expect the alignment between source and target to be highly monotonic, such as grapheme-to-phoneme conversion, transliteration, morphological inflection, and dialect normalization and compare our results to previous work that successfully applied hard monotonic attention to recurrent sequence-tosequence models for these tasks (Wu et al., 2018a; Wu and Cotterell, 2019). Our results show that a monotonicity bias learned through a loss function is capable of making the soft attention between source and target highly monotonic both in RNNs and the transformer. We find that this leads to a similar improvement to previous works on hard monotonic attention for RNNs, whereas for transformer models, the results are mixed: Biasing all attention heads towards monotonicity may limit the representation power of multihead attention in a way Many sequence-to-sequence tasks in natural language processing are roughly monotonic in the alignment between source and target seq"
2021.naacl-main.354,2021.eacl-main.163,0,0.340181,"ll attention heads towards monotonicity may limit the representation power of multihead attention in a way Many sequence-to-sequence tasks in natural language processing are roughly monotonic in the alignment between source and target sequence, and previous work has focused on learning monotonic attention behavior either through specialized attention functions (Aharoni and Goldberg, 2017; Raffel et al., 2017; Wu and Cotterell, 2019) or pretraining (Aji et al., 2020). However, it is non-trivial to port specialized attention functions to different models, and recently, Yolchuyeva et al. (2019); Wu et al. (2021) found that a transformer model (Vaswani et al., 2017) outperforms previous work on monotone tasks such as grapheme-to-phoneme conversion, despite having no mechanism that biases the model towards monotonicity. In the transformer, it is less straightforward to what extent individual encoder states, especially 1 in deeper layers, still represent distinct source inCode and scripts available at: https://github. puts after passing through several self-attention com/ZurichNLP/monotonicity_loss 4474 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational"
2021.naacl-main.354,D18-1473,0,0.0386955,"Missing"
2021.nodalida-main.34,W17-4717,0,0.0287335,"Missing"
2021.nodalida-main.34,W17-4773,0,0.0378549,"agreement errors, mistranslations of proper names (e.g., Lena as Sarah), or the incorrect use or omission of subjunctive mood in conditional sentences. 7 Related Work Our work draws on two strands of research: automatic post-editing and context-aware MT. Automatic post-editing has a long history in MT (Knight and Chander, 1994), with regular shared tasks (Bojar et al., 2015, 2016, 2017). Neural multi-source APE systems as first proposed by Pal et al. (2016) and Junczys-Dowmunt and Grundkiewicz (2016), some of them including source language information (Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2017; Libovický and Helcl, 2017), have come to dominate APE. We take inspiration from the top-performing systems at the WMT19 shared task for architectures and training/decoding tricks (Chatterjee et al., 2019), and make heavy use of synthetic training data (Sennrich et al., 2016a; Junczys-Dowmunt and Grundkiewicz, 2016; Freitag et al., 2019). Neural context-aware MT can be achieved by integrating context into the main translation model (Jean et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia). Two-stage models with a sentence-level first pass and document-level second pas"
2021.nodalida-main.34,P07-2045,0,0.00804036,"ts the consistent transliteration of proper names into Cyrillic script. These two sets are independent of source context by design, as the model is only evaluated on the generation of consistent repetitions of a form it has committed to, regardless of its adequacy in the context. The “Ellipsis VP” set targets elliptic verb phrases, where Russian requires the production of a lexical verb form not found in English. The “Ellipsis inflection” set tests the generation of noun inflections in sentences where the governing verb has been elided. The training data is tokenised and truecased with Moses (Koehn et al., 2007), and encoded using byte-pair encoding (Sennrich et al., 2016b) with source and target vocabularies of 32000 tokens. Like Voita et al. (2019a), we report lowercased, tokenised BLEU (Papineni et al., 2002) with multibleu.perl from the Moses toolkit. 4 Model The sentence-level baselines (EN→RU) and model used for RU→EN back-translation are Transformer base models (Vaswani et al., 2017). For document-level APE, DocRepair is a Transformer base model that operates on groups of adjacent sentences, mapping from mt to ref. We use the Nematus toolkit (Sennrich et al., 2017) for DocRepair and our implem"
2021.nodalida-main.34,P17-2031,0,0.0186668,"nslations of proper names (e.g., Lena as Sarah), or the incorrect use or omission of subjunctive mood in conditional sentences. 7 Related Work Our work draws on two strands of research: automatic post-editing and context-aware MT. Automatic post-editing has a long history in MT (Knight and Chander, 1994), with regular shared tasks (Bojar et al., 2015, 2016, 2017). Neural multi-source APE systems as first proposed by Pal et al. (2016) and Junczys-Dowmunt and Grundkiewicz (2016), some of them including source language information (Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2017; Libovický and Helcl, 2017), have come to dominate APE. We take inspiration from the top-performing systems at the WMT19 shared task for architectures and training/decoding tricks (Chatterjee et al., 2019), and make heavy use of synthetic training data (Sennrich et al., 2016a; Junczys-Dowmunt and Grundkiewicz, 2016; Freitag et al., 2019). Neural context-aware MT can be achieved by integrating context into the main translation model (Jean et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia). Two-stage models with a sentence-level first pass and document-level second pass have been explored for sce"
2021.nodalida-main.34,C04-1072,0,0.21142,"to achieve this. A conservativeness penalty (Junczys-Dowmunt and Grundkiewicz, 2016), denoted c, penalises the score of each prediction that is not in src or mt. Formally, let Vc = Vsrc ∪ Vmt be the subset of the full vocabulary V that occurs in an input segment. Given a |V |-sized vector of candidates ht at time step t, the score of each candidate v is defined as:  ht (v) − c if v ∈ V Vc ht (v) = (1) ht (v) otherwise. Second, similar to Lopes et al. (2019), we apply a data weighting strategy during training. We assign each training sample a weight that is defined as BLEUsmooth (mt, ref ) (Lin and Och, 2004) to upweight samples that require little post-editing. 3 Data and Preprocessing We use all of the English-to-Russian data released by Voita et al. (2019a)1 , including: (1) 6M context1 https://github.com/lena-voita/goodtranslation-wrong-in-context Model Deixis Lex.c. Ell.infl. Ell.VP BLEU Results reported by Voita et al. (2019a): Baseline 50.0 45.9 53.0 DocRepair 91.8 80.6 86.4 28.4 75.2 32.41 34.60 Our experiments: DocRepair DocRepair (+P) Transference Transference (+P) 69.0 71.8 73.0 82.8 32.69 32.38 30.56 32.53 88.6 87.6 86.8 87.8 70.5 67.6 62.9 65.4 83.8 82.2 81.6 84.8 Experiments marked +"
2021.nodalida-main.34,L18-1275,0,0.0122857,"U Results reported by Voita et al. (2019a): Baseline 50.0 45.9 53.0 DocRepair 91.8 80.6 86.4 28.4 75.2 32.41 34.60 Our experiments: DocRepair DocRepair (+P) Transference Transference (+P) 69.0 71.8 73.0 82.8 32.69 32.38 30.56 32.53 88.6 87.6 86.8 87.8 70.5 67.6 62.9 65.4 83.8 82.2 81.6 84.8 Experiments marked +P use the ParData corpus. Table 1: BLEU score on general test set and accuracy on contrastive test sets (deixis, lexical consistency, ellipsis (inflection), and VP ellipsis). agnostic and 1.5M context-aware (4 consecutive sentences in each sample) data from the OpenSubtitles2018 corpus (Lison et al., 2018); (2) Russian monolingual data in 30M groups of 4 consecutive sentences gathered by Voita et al. (2019a). We reuse the synthetic training data for APE generated by Voita et al. (2019a), treating Russian monolingual data as ref, a sentence-level English backtranslation as src, and the Russian roundtrip translation as mt. The evaluation data consists of general test sets extracted from the training data and four contrastive test sets to evaluate specific contextual phenomena. The four contrastive test sets have a narrow focus on specific discourse-level phenomena. The “Deixis” set targets consis"
2021.nodalida-main.34,W19-5413,0,0.0188685,"entence-level main model, we need to control how aggressively APE can modify mt to prevent over-correction. We adopt two strategies from the APE literature to achieve this. A conservativeness penalty (Junczys-Dowmunt and Grundkiewicz, 2016), denoted c, penalises the score of each prediction that is not in src or mt. Formally, let Vc = Vsrc ∪ Vmt be the subset of the full vocabulary V that occurs in an input segment. Given a |V |-sized vector of candidates ht at time step t, the score of each candidate v is defined as:  ht (v) − c if v ∈ V Vc ht (v) = (1) ht (v) otherwise. Second, similar to Lopes et al. (2019), we apply a data weighting strategy during training. We assign each training sample a weight that is defined as BLEUsmooth (mt, ref ) (Lin and Och, 2004) to upweight samples that require little post-editing. 3 Data and Preprocessing We use all of the English-to-Russian data released by Voita et al. (2019a)1 , including: (1) 6M context1 https://github.com/lena-voita/goodtranslation-wrong-in-context Model Deixis Lex.c. Ell.infl. Ell.VP BLEU Results reported by Voita et al. (2019a): Baseline 50.0 45.9 53.0 DocRepair 91.8 80.6 86.4 28.4 75.2 32.41 34.60 Our experiments: DocRepair DocRepair (+P) T"
2021.nodalida-main.34,D18-1325,0,0.0207678,"g on source text for automatic post-editing. They also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably. 1 Introduction Neural machine translation (NMT) has significantly improved the state of the art in MT (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) on the sentence level. However, accurate translation requires looking at larger units than individual sentences (Hardmeier, 2014), and context-aware NMT has recently become a popular research direction (Miculicich et al., 2018; Scherrer et al., 2019; Junczys-Dowmunt, 2019). One approach to discourse-level processing in NMT is automatic post-editing of the output of a sentence-level system. DocRepair (Voita et al., 2019a) is a monolingual sequence-to-sequence model to correct inconsistencies in groups of adjacent sentence-level translations, showing improvements for specific discourse-level phenomena such as the generation of inflections in elliptic sentences. The hypotheses explored in this work are (1) that the coherence of the translation can be further improved by exploiting context in the source language, and ("
2021.nodalida-main.34,P16-2046,0,0.0455034,"Missing"
2021.nodalida-main.34,W19-5414,0,0.0303983,"Missing"
2021.nodalida-main.34,P02-1040,0,0.113028,"tions of a form it has committed to, regardless of its adequacy in the context. The “Ellipsis VP” set targets elliptic verb phrases, where Russian requires the production of a lexical verb form not found in English. The “Ellipsis inflection” set tests the generation of noun inflections in sentences where the governing verb has been elided. The training data is tokenised and truecased with Moses (Koehn et al., 2007), and encoded using byte-pair encoding (Sennrich et al., 2016b) with source and target vocabularies of 32000 tokens. Like Voita et al. (2019a), we report lowercased, tokenised BLEU (Papineni et al., 2002) with multibleu.perl from the Moses toolkit. 4 Model The sentence-level baselines (EN→RU) and model used for RU→EN back-translation are Transformer base models (Vaswani et al., 2017). For document-level APE, DocRepair is a Transformer base model that operates on groups of adjacent sentences, mapping from mt to ref. We use the Nematus toolkit (Sennrich et al., 2017) for DocRepair and our implementation of the Transference architecture, using the same configuration as Pal et al. (2019).2 Detailed hyperparameters are listed in Appendix A. We train our document-level models on the 30M pairs of syn"
2021.nodalida-main.34,D19-6506,0,0.0178,"matic post-editing. They also highlight blind spots in automatic methods for targeted evaluation and demonstrate the need for human assessment to evaluate document-level translation quality reliably. 1 Introduction Neural machine translation (NMT) has significantly improved the state of the art in MT (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) on the sentence level. However, accurate translation requires looking at larger units than individual sentences (Hardmeier, 2014), and context-aware NMT has recently become a popular research direction (Miculicich et al., 2018; Scherrer et al., 2019; Junczys-Dowmunt, 2019). One approach to discourse-level processing in NMT is automatic post-editing of the output of a sentence-level system. DocRepair (Voita et al., 2019a) is a monolingual sequence-to-sequence model to correct inconsistencies in groups of adjacent sentence-level translations, showing improvements for specific discourse-level phenomena such as the generation of inflections in elliptic sentences. The hypotheses explored in this work are (1) that the coherence of the translation can be further improved by exploiting context in the source language, and (2) that the omission of"
2021.nodalida-main.34,E17-3017,1,0.843703,"Missing"
2021.nodalida-main.34,P16-1009,1,0.922375,"llic script. These two sets are independent of source context by design, as the model is only evaluated on the generation of consistent repetitions of a form it has committed to, regardless of its adequacy in the context. The “Ellipsis VP” set targets elliptic verb phrases, where Russian requires the production of a lexical verb form not found in English. The “Ellipsis inflection” set tests the generation of noun inflections in sentences where the governing verb has been elided. The training data is tokenised and truecased with Moses (Koehn et al., 2007), and encoded using byte-pair encoding (Sennrich et al., 2016b) with source and target vocabularies of 32000 tokens. Like Voita et al. (2019a), we report lowercased, tokenised BLEU (Papineni et al., 2002) with multibleu.perl from the Moses toolkit. 4 Model The sentence-level baselines (EN→RU) and model used for RU→EN back-translation are Transformer base models (Vaswani et al., 2017). For document-level APE, DocRepair is a Transformer base model that operates on groups of adjacent sentences, mapping from mt to ref. We use the Nematus toolkit (Sennrich et al., 2017) for DocRepair and our implementation of the Transference architecture, using the same con"
2021.nodalida-main.34,P16-1162,1,0.369121,"llic script. These two sets are independent of source context by design, as the model is only evaluated on the generation of consistent repetitions of a form it has committed to, regardless of its adequacy in the context. The “Ellipsis VP” set targets elliptic verb phrases, where Russian requires the production of a lexical verb form not found in English. The “Ellipsis inflection” set tests the generation of noun inflections in sentences where the governing verb has been elided. The training data is tokenised and truecased with Moses (Koehn et al., 2007), and encoded using byte-pair encoding (Sennrich et al., 2016b) with source and target vocabularies of 32000 tokens. Like Voita et al. (2019a), we report lowercased, tokenised BLEU (Papineni et al., 2002) with multibleu.perl from the Moses toolkit. 4 Model The sentence-level baselines (EN→RU) and model used for RU→EN back-translation are Transformer base models (Vaswani et al., 2017). For document-level APE, DocRepair is a Transformer base model that operates on groups of adjacent sentences, mapping from mt to ref. We use the Nematus toolkit (Sennrich et al., 2017) for DocRepair and our implementation of the Transference architecture, using the same con"
2021.nodalida-main.34,W17-4811,0,0.0197597,"nt and Grundkiewicz (2016), some of them including source language information (Junczys-Dowmunt and Grundkiewicz, 2017; Chatterjee et al., 2017; Libovický and Helcl, 2017), have come to dominate APE. We take inspiration from the top-performing systems at the WMT19 shared task for architectures and training/decoding tricks (Chatterjee et al., 2019), and make heavy use of synthetic training data (Sennrich et al., 2016a; Junczys-Dowmunt and Grundkiewicz, 2016; Freitag et al., 2019). Neural context-aware MT can be achieved by integrating context into the main translation model (Jean et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia). Two-stage models with a sentence-level first pass and document-level second pass have been explored for scenarios with asymmetric training data. Voita et al. (2019b) introduces a two-pass model where, unlike in APE, the second-pass is tightly integrated with the first-pass model, reusing its hidden representations. Apart from Voita et al. (2019a), the model closest to ours is by Junczys-Dowmunt (2019), who explored document-level APE, but only manually evaluated its efficacy as part of a large model ensemble. 8 Conclusion Our human evaluation shows that mono"
2021.nodalida-main.34,D19-1081,1,0.849733,"Missing"
2021.nodalida-main.34,P19-1116,1,0.890439,"Missing"
D15-1248,P11-2031,0,0.0111348,"y in parallel text &lt; 5). For comparison with the state-of-the-art, we train a full system on our restructured representation, which incorporates all models and settings of our WMT 2015 submission system (Williams et al., 2015).7 Note that our WMT 2015 submission 5 We use the last position in the clause as default location, but put the particle before any subordinated and coordinated clauses, which occur in the Nachfeld (the ‘final field’ in topological field theory). 6 We use mteval-v13a.pl for comparability to official WMT results; all significance values reported are obtained with MultEval (Clark et al., 2011). 7 In contrast to our other systems in this paper, RDLM is trained on all monolingual data for the full system, and two models are added: a 5-gram Neural Network language model newstest2014 20.7 21.3 21.4 20.9 22.0 22.1 22.6 newstest2015 22.0 22.4 22.8 22.7 23.4 23.8 24.4 Table 2: English–German translation results (B LEU). Average of three optimization runs. system reference baseline +head binarization +split compounds +particle verbs compound 2841 845 798 1850 1992 sep. 553 96 157 160 333 particle verb pref. zu-infix 1195 176 847 71 858 106 877 94 953 169 Table 3: Number of compounds [that"
D15-1248,E12-1068,0,0.015318,"t.1 We also split linking elements, and represent them as a postmodifier of each non-final segment, including the empty string (&quot;&quot;). We use the same representation for noun compounds and adjective compounds. An example of the original2 and the proposed compound representation is shown in Figure 1. Importantly, the head of the compound is also the parent of the determiners and attributes in the noun phrase, which makes a bigram dependency language model sufficient to enforce agreement. Since we model morphosyntactic agreement within the main translation step, and not in a separate step as in (Fraser et al., 2012), we deem it useful that inflection is marked at the head of the compound. Consequently, we do not split off inflectional or derivational morphemes. For German particle verbs, we define a common representation that abstracts away from the various surface realizations (see Table 1). Separated 1 We follow prior work in leaving frequent words or subwords unsplit, which has a disambiguating effect. With more aggressive splitting, frequency information could be used for the structural disambiguation of internal structure. 2 The original dependency trees follow the annotation guidelines by Foth (200"
D15-1248,W10-1734,0,0.0465186,"normally a premodifying particle, appears as an infix in particle verbs. Table 1 shows an illustrating example. 2 root obja det subj sie erheben eine Handgepäckgebühr PPER VVFIN ART NN they charge a carry-on bag fee obja det A Dependency Representation of Compounds and Particle Verbs root mod subj The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; Nießen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally: ’(city part) project)’) and Stadt(teilprojekt) ’city sub-project’. We opt for a left-branching representation by default.1 We also split linking elements, and represent them"
D15-1248,P06-1121,0,0.444314,"mber and gender is enforced between eine ’a’ and Gebühr ’fee’, and selectional preference between erheben ’charge’ and Gebühr ’fee’. A flat representation, as is common in phrase-based SMT, does not encode these relationships, but a dependency representation does so through dependency links. In this paper, we investigate a dependency representation of morphologically segmented words for SMT. Our representation encodes syntactic and morphological structure jointly, allowing a single model to learn the translation of both. Specifically, we work with a string-to-tree model with GHKM-style rules (Galley et al., 2006), and a relational dependency language model (Sennrich, 2015). We focus on the representation of German syntax and morphology in an English-to-German system, and two morphologically complex word classes in German that are challenging for translation, compounds and particle verbs. German makes heavy use of compounding, and compounds such as Abwasserbehandlungsanlage ‘waste water treatment plant’ are translated into complex noun phrases in other languages, such as French station d’épuration des eaux résiduaires. German particle verbs are difficult to model because their surface realization diffe"
D15-1248,W14-4018,0,0.27665,"etical drawback. With head binarization, we find substantial improvements from compound splitting of 0.7–1.1 B LEU. On newstest2014, the improvement is almost twice of that reported in related work (Williams et al., 2014), which also uses a hierarchical representation of compounds, albeit one that does not allow for dependency modelling. Examples of correct, unseen compounds generated include Staubsauger|roboter ’vacuum cleaner robot’, Gravitation|s|wellen ’gravitational waves’, and NPD|-|verbot|s|verfahren ’NPD banning process’.8 (Vaswani et al., 2013), and soft source-syntactic constraints (Huck et al., 2014). 8 Note that Staubsauger, despite being a compound, is not 2084 Particle verb restructuring yields additional gains of 0.1–0.4 B LEU. One reason for the smaller effect of particle verb restructuring is that the difficult cases – separated particle verbs and those with infixation – are rarer than compounds, with 2841 rare compounds [that would be split by our compound splitter] in the reference texts, in contrast to 553 separated particle verbs, and 176 particle verbs with infixation, as Table 3 illustrates. If we only evaluate the sentences containing a particle verb with zu-infix in the refe"
D15-1248,E03-1076,0,0.529645,"lation quality of 1.4–1.8 B LEU in the WMT English–German translation task. 1 finite (sub.) bare infinitive to/zu-infinitive Table 1: Surface realizations of particle verb weggehen ’walk away’. they charge a carry-on bag fee. Introduction When translating between two languages that differ in their degree of morphological synthesis, syntactic structures in one language may be realized as morphological structures in the other. Machine Translation models that treat words as atomic units have poor learning capabilities for such translation units, and morphological segmentations are commonly used (Koehn and Knight, 2003). Like words in a sentence, the morphemes of a word have a hierarchical structure that is relevant in translation. For instance, compounds in Germanic languages are head-final, and the head is the segment that determines agreement within the noun phrase, and is relevant for selectional preferences of verbs. 1. sie erheben eine Hand|gepäck|gebühr. English/German example he walks away quickly er geht schnell weg [...] because he walks away quickly [...] weil er schnell weggeht he can walk away quickly er kann schnell weggehen he promises to walk away quickly er verspricht, schnell wegzugehen In"
D15-1248,P07-2045,0,0.00473569,"leave it to future research to determine if our limitation to projective trees affects translation quality, and how to produce nonprojective trees. 5 SMT experiments 5.1 Data and Models We train English–German string-to-tree SMT systems on the training data of the shared translation task of the Workshop on Statistical Machine Translation (WMT) 2015. The data set consists of 4.2 million sentence pairs of parallel data, and 160 million sentences of monolingual German data. We base our systems on that of Williams et al. (2014). It is a string-to-tree GHKM translation system implemented in Moses (Koehn et al., 2007), and using the dependency annotation by ParZu (Sennrich et al., 2013). Additionally, our baseline system contains a dependency language model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive B LEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5). For comparison with the state-of-the-art, we"
D15-1248,P11-1140,0,0.0502404,"Missing"
D15-1248,C00-2162,0,0.676827,"er 2015. 2015 Association for Computational Linguistics. main clauses, but prefixed to the verb in subordinated clauses, or when the verb is non-finite. The infinitive marker zu ’to’, which is normally a premodifying particle, appears as an infix in particle verbs. Table 1 shows an illustrating example. 2 root obja det subj sie erheben eine Handgepäckgebühr PPER VVFIN ART NN they charge a carry-on bag fee obja det A Dependency Representation of Compounds and Particle Verbs root mod subj The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; Nießen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stad"
D15-1248,2001.mtsummit-papers.45,0,0.074245,"st-modifiers, as erheben and gepäck do in Figure 3. 2083 1. non-finite verbs are concatenated with the particle, and zu-markers are infixed. 2. finite verbs that head a subordinated clause (identified by its dependency label) are concatenated with the particle. 3. finite verbs that head a main clause have the system baseline +split compounds +particle verbs head binarization +split compounds +particle verbs full system particle moved to the right clause bracket.5 Previous work on particle verb translation into German proposed to predict the position of particles with an n-gram language model (Nießen and Ney, 2001). Our rules have the advantage that they are informed by the syntax of the sentence and consider the finiteness of the verb. Our rules only produce projective trees. Verb particles may also appear in positions that violate projectivity, and we leave it to future research to determine if our limitation to projective trees affects translation quality, and how to produce nonprojective trees. 5 SMT experiments 5.1 Data and Models We train English–German string-to-tree SMT systems on the training data of the shared translation task of the Workshop on Statistical Machine Translation (WMT) 2015. The"
D15-1248,schmid-etal-2004-smor,0,0.0162513,"ing example. 2 root obja det subj sie erheben eine Handgepäckgebühr PPER VVFIN ART NN they charge a carry-on bag fee obja det A Dependency Representation of Compounds and Particle Verbs root mod subj The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; Nießen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally: ’(city part) project)’) and Stadt(teilprojekt) ’city sub-project’. We opt for a left-branching representation by default.1 We also split linking elements, and represent them as a postmodifier of each non-final segment, including the empty string (&quot;&quot;). We use the s"
D15-1248,sennrich-kunz-2014-zmorge,1,0.84052,"bja det subj sie erheben eine Handgepäckgebühr PPER VVFIN ART NN they charge a carry-on bag fee obja det A Dependency Representation of Compounds and Particle Verbs root mod subj The main focus of research on compound splitting has been on the splitting algorithm (Popovic et al., 2006; Nießen and Ney, 2000; Weller et al., 2014; Macherey et al., 2011). Our focus is not the splitting algorithm, but the representation of compounds. For splitting, we use an approach similar to (Fritzinger and Fraser, 2010), with segmentation candidates identified by a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014), and statistical evidence from the training corpus to select a split (Koehn and Knight, 2003). German compounds are head-final, and premodifiers can be added recursively. Compounds are structurally ambiguous if there is more than one modifier. Consider the distinction between (Stadtteil)projekt (literally: ’(city part) project)’) and Stadt(teilprojekt) ’city sub-project’. We opt for a left-branching representation by default.1 We also split linking elements, and represent them as a postmodifier of each non-final segment, including the empty string (&quot;&quot;). We use the same representation for nou"
D15-1248,R13-1079,1,0.651787,"ctive trees affects translation quality, and how to produce nonprojective trees. 5 SMT experiments 5.1 Data and Models We train English–German string-to-tree SMT systems on the training data of the shared translation task of the Workshop on Statistical Machine Translation (WMT) 2015. The data set consists of 4.2 million sentence pairs of parallel data, and 160 million sentences of monolingual German data. We base our systems on that of Williams et al. (2014). It is a string-to-tree GHKM translation system implemented in Moses (Koehn et al., 2007), and using the dependency annotation by ParZu (Sennrich et al., 2013). Additionally, our baseline system contains a dependency language model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive B LEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5). For comparison with the state-of-the-art, we train a full system on our restructured representation, which incorpor"
D15-1248,Q15-1013,1,0.873797,"selectional preference between erheben ’charge’ and Gebühr ’fee’. A flat representation, as is common in phrase-based SMT, does not encode these relationships, but a dependency representation does so through dependency links. In this paper, we investigate a dependency representation of morphologically segmented words for SMT. Our representation encodes syntactic and morphological structure jointly, allowing a single model to learn the translation of both. Specifically, we work with a string-to-tree model with GHKM-style rules (Galley et al., 2006), and a relational dependency language model (Sennrich, 2015). We focus on the representation of German syntax and morphology in an English-to-German system, and two morphologically complex word classes in German that are challenging for translation, compounds and particle verbs. German makes heavy use of compounding, and compounds such as Abwasserbehandlungsanlage ‘waste water treatment plant’ are translated into complex noun phrases in other languages, such as French station d’épuration des eaux résiduaires. German particle verbs are difficult to model because their surface realization differs depending on the finiteness of the verb and the type of cl"
D15-1248,D13-1140,0,0.0683442,"d representation, and that binarization compensates this theoretical drawback. With head binarization, we find substantial improvements from compound splitting of 0.7–1.1 B LEU. On newstest2014, the improvement is almost twice of that reported in related work (Williams et al., 2014), which also uses a hierarchical representation of compounds, albeit one that does not allow for dependency modelling. Examples of correct, unseen compounds generated include Staubsauger|roboter ’vacuum cleaner robot’, Gravitation|s|wellen ’gravitational waves’, and NPD|-|verbot|s|verfahren ’NPD banning process’.8 (Vaswani et al., 2013), and soft source-syntactic constraints (Huck et al., 2014). 8 Note that Staubsauger, despite being a compound, is not 2084 Particle verb restructuring yields additional gains of 0.1–0.4 B LEU. One reason for the smaller effect of particle verb restructuring is that the difficult cases – separated particle verbs and those with infixation – are rarer than compounds, with 2841 rare compounds [that would be split by our compound splitter] in the reference texts, in contrast to 553 separated particle verbs, and 176 particle verbs with infixation, as Table 3 illustrates. If we only evaluate the sen"
D15-1248,D07-1078,0,0.15192,"Missing"
D15-1248,W14-3324,1,0.817402,"ith infixed zu-marker. verb particles are reordered to be the closest premodifier of the verb. Prefixed particles and the zuinfix are identified by the finite-state-morphology, and split from the verb so that the particle is the closest, the zu marker the next-closest premodifier of the verb, as shown in Figure 2. Agreement, selectional preferences, and other phenomena involve the verb and its dependents, and the proposed representation retains these dependency links, but reduces data sparsity from affixation and avoids discontinuity of the verb and its particle. 3 Tree Binarization We follow Williams et al. (2014) and map dependency trees into a constituency representation, which allows for the extraction of GHKM-style translation rules (Galley et al., 2006). This conversion is lossless, and we can still apply a de2082 pendency language model (RDLM). Figure 3 (a) shows the constituency representation of the example in Figure 1. Our model should not only be able to produce new words productively, but also to memorize words it has observed during training. Looking at the compound Handgepäckgebühr in Figure 3 (a), we can see that it does not form a constituent, and cannot be extracted with GHKM extraction"
D15-1248,W15-3024,1,0.880521,"ge model (RDLM) (Sennrich, 2015), trained on the target-side of the parallel training data. We report case-sensitive B LEU scores on the newstest2014/5 test sets from WMT, averaged over 3 optimization runs of k-batch MIRA (Cherry and Foster, 2012) on a subset of newstest2008-12.6 We split all particle verbs and hyphenated compounds, but other compounds are only split if they are rare (frequency in parallel text &lt; 5). For comparison with the state-of-the-art, we train a full system on our restructured representation, which incorporates all models and settings of our WMT 2015 submission system (Williams et al., 2015).7 Note that our WMT 2015 submission 5 We use the last position in the clause as default location, but put the particle before any subordinated and coordinated clauses, which occur in the Nachfeld (the ‘final field’ in topological field theory). 6 We use mteval-v13a.pl for comparability to official WMT results; all significance values reported are obtained with MultEval (Clark et al., 2011). 7 In contrast to our other systems in this paper, RDLM is trained on all monolingual data for the full system, and two models are added: a 5-gram Neural Network language model newstest2014 20.7 21.3 21.4 2"
D15-1248,J10-4005,0,\N,Missing
D15-1248,W14-5709,0,\N,Missing
D15-1248,W15-3014,0,\N,Missing
D15-1248,N12-1047,0,\N,Missing
D17-1156,2012.eamt-1.60,0,0.0287279,"he corresponding columns of the out-of-domain parameter matrices. This can be alternatively seen as learning matrices of parameter differences between in-domain and out-of-domain models with standard dropout, starting from a zero initialization at the beginning of fine-tuning. Therefore, equation 2 becomes ˆ + ∆W · M∆W,i,j ) · hi,j vi,j = (W (3) ˆ is the fixed out-of-domain parameter where W matrix and ∆W is the parameter difference matrix to be learned and M∆W,i,j is a Bayesian dropout mask. 1490 Evaluation 31 We evaluate transfer learning on test sets from the IWSLT shared translation task (Cettolo et al., 2012). 3.1 Data and Methods Test sets consist of transcripts of TED talks and their translations; small amounts of in-domain training data are also provided. For English-toGerman we use IWSLT 2015 training data, while for English-to-Russian we use IWSLT 2014 training data. For the out-of-domain systems, we use training data from the WMT shared translation task,2 which is considered permissible for IWSLT tasks, including back-translations of monolingual training data (Sennrich et al., 2016b), i.e., automatic translations of data available only in target language “back” into the source language.3 . W"
D17-1156,P07-1033,0,0.223933,"Missing"
D17-1156,W04-3250,0,0.204806,"Missing"
D17-1156,2015.iwslt-evaluation.11,0,0.0702048,"rvised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. Owing to the incremental nature of stochastic gradient-based training algorithms, a simple yet effective approach to transfer learning for neural networks is fine-tuning (Hinton and Salakhutdinov, 2006; Mesnil et al., 2012; Yosinski et al., 2014): to continue training an existing model which was trained on out-of-domain data with indomain training data. This strategy was also found to be very effective for neural machine translation (Luong and Manning, 2015; Sennrich et al., 2016b). In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for English→German and English→Russian. We also investigate the amounts of in-domain training data needed for d"
D17-1156,D15-1123,0,0.0184422,"xt is available for the specific application domain, and it is We investigate techniques where domain adaptation starts from a pre-trained out-domain model, and only needs to process the in-domain corpus. Since we do not need to process the large out-domain corpus during adaptation, this is suitable for scenarios where adaptation must be performed quickly or where the original outdomain corpus is not available. Other works consider techniques that jointly train on the outdomain and in-domain corpora, distinguishing them using specific input features (Daume III, 2007; Finkel and Manning, 2009; Wuebker et al., 2015). These techniques are largely orthogonal to 1489 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1489–1494 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ours1 and can be used in combination. In fact, Chu et al. (2017) successfully apply fine-tuning in combination with joint training. inal out-of-domain model was also trained with dropout. 2 L2-norm regularization is widely used for machine learning and statistical models. For linear models, it corresponds to imposing a diagonal Gaussian prior with zero"
D17-1156,W16-2323,1,\N,Missing
D17-1156,E17-3017,1,\N,Missing
D17-1156,P16-1162,1,\N,Missing
D17-1156,P16-1009,1,\N,Missing
D17-1156,N09-1068,0,\N,Missing
D17-1303,S14-2010,0,0.036251,"S2 GT Pred Black bird standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual se"
D17-1303,S15-2045,0,0.0326146,"d standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual semantic embeddings (Ki"
D17-1303,S12-1051,0,0.0318735,"3 84.6 84.5 91.5 S1 S2 GT Pred Black bird standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et a"
D17-1303,J75-4040,0,0.427596,"Missing"
D17-1303,P11-1020,0,0.0528035,"izing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual semantic embeddings (Kiros et al., 2015), and order embeddings (Vendrov et al., 2016). The shared task baseline is computed based on"
D17-1303,W16-3210,0,0.233939,"Missing"
D17-1303,D16-1026,0,0.275329,"sis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016). Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space. However, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space. The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat et al., 2016) and to learn multilingual multimodal representations (Rajendran et al., 2016; Calixto et al., 2017). Rajendran et al. (2016) propose a 2839 Proceedings of the 2017 Conference on Empirical Metho"
D17-1303,D15-1070,0,0.209515,"We introduce a new pairwise ranking loss function which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to han"
D17-1303,P16-1227,0,0.0697283,"evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015"
D17-1303,P16-1168,0,0.2997,"Missing"
D17-1303,N16-1021,0,0.155414,"which can handle both symmetric and asymmetric similarity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation"
D17-1303,W10-0721,0,0.0735935,"man is folding paper. A woman is slicing a 0.6 0.6 pepper. Table 4: Results on Semantic Textual Similarity Image datasets (Pearson’s r × 100 ). Our systems that performed better than best reported shared task scores are in bold. tences (image descriptions in this case). We evaluate on video task from STS-2012 and image tasks from STS-2014, STS-2015 (Agirre et al. 2012, Agirre et al. 2014, Agirre et al. 2015). The video descriptions in the STS-2012 task are from the MSR video description corpus (Chen and Dolan, 2011) and the image descriptions in STS2014 and 2015 are from UIUC PASCAL dataset (Rashtchian et al., 2010). In Table 4, we present the Pearson correlation coefficients of our model predicted scores with the gold-standard similarity scores provided as part of the STS image/video description tasks. We compare with the best reported scores for the STS shared tasks, achieved by MLMME (Calixto et al., 2017), paraphrastic sentence embeddings (Wieting et al., 2017), visual semantic embeddings (Kiros et al., 2015), and order embeddings (Vendrov et al., 2016). The shared task baseline is computed based on word overlap and is high for both the 2014 and the 2015 dataset, indicating that there is substantial"
D17-1303,Q14-1017,0,0.0524692,"the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy Most work on learning a joint space for images and their descriptions is based on Canonical Correlation Analysis (CCA) or neural variants of CCA over representations of image and its descriptions (Hodosh et al., 2013; Andrew et al., 2013; Yan and Mikolajczyk, 2015; Gong et al., 2014; Chandar et al., 2016). Besides CCA, a few others learn a visual-semantic or multimodal embedding space of image descriptions and representations by optimizing a ranking cost function (Kiros et al., 2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space. However, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space. The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat"
D17-1303,W16-2346,0,0.0614214,"larity between the two modalities. We evaluate our models on image-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015"
D17-1303,D17-1026,0,0.134783,"odal embeddings, irrespective of the language. Our results also show that the asymmetric scoring function can help learn better embeddings. In Table 3 we present a few examples where P IVOT-A SYM and PARALLEL -A SYM models performed better on both the languages compared to baseline order embedding model even using descriptions of very different lengths as queries. 4.2 Semantic Textual Similarity Results In the semantic textual similarity task (STS), we use the textual embeddings from our model to compute the similarity between a pair of sen2842 Model Shared Task Baseline STS Best System GRAN (Wieting et al., 2017) MLMME (Calixto et al., 2017) VSE (Kiros et al., 2015) OE (Vendrov et al., 2016) P IVOT-S YM PARALLEL -S YM P IVOT-A SYM PARALLEL -A SYM VF − − − VGG19 VGG19 VGG19 VGG19 VGG19 VGG19 VGG19 2012 2014 2015 29.9 51.3 60.4 87.3 83.4 86.4 83.7 84.5 85.0 − 72.7 79.7 80.6 82.7 89.6 82.2 84.1 90.8 80.5 81.8 89.2 82.0 81.4 90.4 83.1 83.8 90.3 84.6 84.5 91.5 S1 S2 GT Pred Black bird standing on Blue bird standing on 1.0 4.2 concrete. green grass. Two zebras are playing. Zebras are socializing. 4.2 1.2 Three goats are being Three goats are chased 4.6 4.5 rounded up by a dog. by a dog A man is folding pape"
D17-1303,P07-1108,0,0.0352625,"2015; Socher et al., 2014; Ma et al., 2015; Vendrov et al., 2016) or by aligning image regions (objects) and segments of the description (Karpathy et al., 2014; Plummer et al., 2015) in a common space. Recently Lin and Parikh (2016) have leveraged visual question answering models to encode images and descriptions into the same space. However, all of this work is targeted at monolingual descriptions, i.e., mapping images and descriptions in a single language onto a joint embedding space. The idea of pivoting or bridging is not new and language pivoting is well explored for machine translation (Wu and Wang, 2007; Firat et al., 2016) and to learn multilingual multimodal representations (Rajendran et al., 2016; Calixto et al., 2017). Rajendran et al. (2016) propose a 2839 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2839–2845 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Two men playing soccer on a field Joint space translations of each other, i.e., they are not parallel, although they describe the same image. 3 zwei männer kämpfen um einen fussball Figure 1: Our multilingual multimodal model with image as pi"
D17-1303,P17-2066,0,0.114081,"mage-description ranking for German and English, and on semantic textual similarity of image descriptions in English. In both cases we achieve state-of-the-art performance. 1 Previous work on image description generation or learning a joint space for images and text has mostly focused on English due to the availability of English datasets. Recently there have been attempts to create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015). There has also been a"
D17-1303,Q14-1006,0,0.831264,"create image descriptions and models for other languages (Funaki and Nakayama, 2015; Elliott et al., 2016; Rajendran et al., 2016; Miyazaki and Shimizu, 2016; Specia et al., 2016; Li et al., 2016; Hitschler et al., 2016; Yoshikawa et al., 2017). Introduction In recent years there has been a significant amount of research in language and vision tasks which require the joint modeling of texts and images. Examples include text-based image retrieval, image description and visual question answering. An increasing number of large image description datasets has become available (Hodosh et al., 2013; Young et al., 2014; Lin et al., 2014) and various systems have been proposed to handle the image description task as a generation problem (Bernardi et al., 2016; Mao et al., 2015; Vinyals et al., 2015; Fang et al., 2015). There has also been a great deal of work on sentence-based image search or cross-modal retrieval where the objective is to learn a joint space for images and text (Hodosh et al., 2013; Frome et al., 2013; Karpathy Most work on learning a joint space for images and their descriptions is based on Canonical Correlation Analysis (CCA) or neural variants of CCA over representations of image and its"
D17-1303,W10-0707,0,\N,Missing
D18-1267,P11-1049,0,0.0673364,"ual corpus is available and performs arbitrary rewrites without access to compression specific data. We release1 M OSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres. 1 Introduction Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how"
D18-1267,2012.eamt-1.60,0,0.019139,"ssion model on three languages initialized with varying compression rates3 (see Section 4 for details on how the models were trained and tested). The compression rate (CR) is used to determine length parameter of Equation (8): Ty0 = Tx ·CR 1. Europarl, the European Parliament Proceedings Parallel Corpus (Koehn, 2005), has been used extensively in machine translation research; it contains the minutes of the European parliament and is a spoken corpus of formulaic nature; speakers take part in debating various issues concerning EU policy (e.g., taxation, environment). 2. The TED parallel Corpus (Cettolo et al., 2012) contains transcripts in multiple languages of short talks devoted to spreading powerful ideas on a variety of topics ranging from science to business and global issues. 3. The EU bookshop corpus (Skadin¸sˇ et al., 2014) contains publications from European institutions covering a variety of topics such as refugees, gender equality, and travel. 4. The News Commentary Parallel Corpus contains articles downloaded from Project Syndicate, an international media organization that publishes commentary on global topics (e.g., economics, world affairs). (11) The figure shows how the output length varie"
D18-1267,D14-1082,0,0.0194995,"ource sentence so that it matches the compression ratio of the validation set. 5 Results M OSS Evaluation We assessed model performance using three automatic metrics which represent different aspects of the compression task and have been found to correlate well with human judgments (Toutanova et al., 2016; Clarke and Lapata, 2006). These include a recall metric based on skip bi-grams, any pair of words in a sequence allowing for gaps of size four6 (RS-R); a recall metric based on bi-grams of dependency tree triples (D2-R); and bi-gram ROUGE (R2-F1). We used the Stanford neural network parser (Chen and Manning, 2014) to obtain dependency triples. Table 3(a) reports results on English with a model which controls the output length (L ) and uses either a single pivot (SP; K = 1) or multiple pivots (MP; K = 10). We experimented with French (fr) or German (de) as pivot languages. All pivot-based models perform compression in a single step (see Section 2.3). Dual-step compres5 We used our own implementation of ABS and LenInit which on DUC-2004 obtained ROUGE scores similar to those published in Rush et al. (2015) and Kikuchi et al. (2016). 6 We add a begin-of-sentence marker at the start of the candidate and re"
D18-1267,P16-1185,0,0.0629603,"Missing"
D18-1267,N16-1012,0,0.423667,"iting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 2016). Filippova et al. (2015) focus on deletion-based sentence compression which they model as a sequence labeling problem using a recurrent neural network with long short-term memory units (LSTM; Hochreiter and Schmidhuber 1997). Rush et al. (2015) capture the full gamut of rewrite operations drawing insights from encoderdecoder models recently proposed for machine translation (Bahdanau et al., 2015). Neural network-based approaches are datadriven, relying on the ability of recurrent architectures to learn continuous features without recourse to preprocessing tools or synta"
D18-1267,P06-1048,1,0.66521,"or, we also translated German or French into English, compressed it with ABS and LenInit trained on the Gigaword corpus, and then translated the compressions back to French or German. Finally, we include a prefix (Pfix) baseline which does not perform any rewriting but simply truncates the source sentence so that it matches the compression ratio of the validation set. 5 Results M OSS Evaluation We assessed model performance using three automatic metrics which represent different aspects of the compression task and have been found to correlate well with human judgments (Toutanova et al., 2016; Clarke and Lapata, 2006). These include a recall metric based on skip bi-grams, any pair of words in a sequence allowing for gaps of size four6 (RS-R); a recall metric based on bi-grams of dependency tree triples (D2-R); and bi-gram ROUGE (R2-F1). We used the Stanford neural network parser (Chen and Manning, 2014) to obtain dependency triples. Table 3(a) reports results on English with a model which controls the output length (L ) and uses either a single pivot (SP; K = 1) or multiple pivots (MP; K = 10). We experimented with French (fr) or German (de) as pivot languages. All pivot-based models perform compression in"
D18-1267,D15-1042,0,0.0906937,"Missing"
D18-1267,D13-1155,0,0.0471947,"ains manual compressions for single and multiple sentences (about 26,000 pairs of source and compressed texts). 2 Rush et al. (2015) use approximately four million training instances and Filippova et al. (2015) two million. 2453 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2453–2464 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Since large scale compression datasets do not occur naturally, they must be somehow approximated, e.g., by pairing headlines with the first sentence of a news article (Filippova and Altun, 2013; Rush et al., 2015). As a result, the training corpus construction process must be repeated and reconfigured for new languages and domains (e.g., many headline-first sentence pairs are spurious and need to be filtered using language and domain specific heuristics). And although it may be easy to automatically obtain large scale training data in the news domain, it is not clear how such data can be sourced for many other genres with different writing conventions. Our work addresses the paucity of data for sentence compression models. We argue that multilingual corpora are a rich source for lea"
D18-1267,D16-1026,0,0.0360291,"Missing"
D18-1267,N07-1023,0,0.0342067,"tion (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 20"
D18-1267,C12-1067,0,0.0168628,"peakers of the respective language who process a document at a time. We obtain five compressions per document leading to 2,000 long-short sentence pairs per language. Like previous related resources (Clarke and Lapata, 2008; Cohn and Lapata, 2013; de Loupy et al., 2010) our corpus is curated manually, however it differs from Toutanova et al. (2016) in that it contains compressions for individual sentences, not documents. There has been relatively little interest in compressing languages other than English. A few models have been proposed for Japanese (Hori and Furui, 2004; Hirao et al., 2009; Harashima and Kurohashi, 2012), including a neural network model (Hasegawa et al., 2017) which repurposes Filippova and Altun’s (2013) data construction method for Japanese. There is a compression corpus available for French (de Loupy et al., 2010), however, we are not aware of any modeling work on this language. Overall, there are no standardized datasets in languages other than English, either for training or testing. Our contributions in this work are three-fold: a novel application of bilingual pivoting to sentence compression; corroborated by empirical results showing that our model scales across languages and text ge"
D18-1267,P17-2044,0,0.0126444,"e. We obtain five compressions per document leading to 2,000 long-short sentence pairs per language. Like previous related resources (Clarke and Lapata, 2008; Cohn and Lapata, 2013; de Loupy et al., 2010) our corpus is curated manually, however it differs from Toutanova et al. (2016) in that it contains compressions for individual sentences, not documents. There has been relatively little interest in compressing languages other than English. A few models have been proposed for Japanese (Hori and Furui, 2004; Hirao et al., 2009; Harashima and Kurohashi, 2012), including a neural network model (Hasegawa et al., 2017) which repurposes Filippova and Altun’s (2013) data construction method for Japanese. There is a compression corpus available for French (de Loupy et al., 2010), however, we are not aware of any modeling work on this language. Overall, there are no standardized datasets in languages other than English, either for training or testing. Our contributions in this work are three-fold: a novel application of bilingual pivoting to sentence compression; corroborated by empirical results showing that our model scales across languages and text genres without additional supervision over and above what is"
D18-1267,P09-1093,0,0.0294704,"mpressed by native speakers of the respective language who process a document at a time. We obtain five compressions per document leading to 2,000 long-short sentence pairs per language. Like previous related resources (Clarke and Lapata, 2008; Cohn and Lapata, 2013; de Loupy et al., 2010) our corpus is curated manually, however it differs from Toutanova et al. (2016) in that it contains compressions for individual sentences, not documents. There has been relatively little interest in compressing languages other than English. A few models have been proposed for Japanese (Hori and Furui, 2004; Hirao et al., 2009; Harashima and Kurohashi, 2012), including a neural network model (Hasegawa et al., 2017) which repurposes Filippova and Altun’s (2013) data construction method for Japanese. There is a compression corpus available for French (de Loupy et al., 2010), however, we are not aware of any modeling work on this language. Overall, there are no standardized datasets in languages other than English, either for training or testing. Our contributions in this work are three-fold: a novel application of bilingual pivoting to sentence compression; corroborated by empirical results showing that our model sca"
D18-1267,A00-1043,0,0.0392667,"Our model can be trained for any language as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data. We release1 M OSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres. 1 Introduction Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeo"
D18-1267,D16-1140,0,0.0802519,"and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 2016). Filippova et al. (2015) focus on deletion-based sentence compression which they model as a sequence labeling problem using a recurrent neural network with long short-term memory units (LSTM; Hochreiter and Schmidhuber 1997). Rush et al. (2015) capture the full gamut of rewrite operations drawing insights from encoderdecoder models recently proposed for machine translation (Bahdanau et al., 2015). Neural network-based approaches are datadriven, relying on the ability of recurrent architectures to learn continuous features without recourse to preprocessing tools or syntactic information (e.g.,"
D18-1267,E17-1083,1,0.936309,"guage and domain specific heuristics). And although it may be easy to automatically obtain large scale training data in the news domain, it is not clear how such data can be sourced for many other genres with different writing conventions. Our work addresses the paucity of data for sentence compression models. We argue that multilingual corpora are a rich source for learning a variety of rewrite rules across languages and that existing neural machine translation (NMT) models (Sutskever et al. 2014; Bahdanau et al. 2015) can be easily adapted to the compression task through bilingual pivoting (Mallinson et al., 2017) coupled with methods which decode the output sequence to a desired length (e.g., subject to language and genre requirements). We obtain compressions by translating a source string into a foreign language and then back-translating it into the source while controlling the translation length (Kikuchi et al., 2016). Our model can be trained for any language as long as a bilingual corpus is available, and can perform arbitrary rewrites while taking advantage of multiple pivots if these exist.We also demonstrate that models trained on multilingual data perform well out-of-domain. Although our appro"
D18-1267,E06-1038,0,0.0336769,"he most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural"
D18-1267,W11-1610,0,0.054439,"Missing"
D18-1267,2005.mtsummit-papers.11,0,0.0843565,"pressions in succession reduces both length and content conservatively and as a result produces more meaningful text. In Figure 1 we illustrate how the pivot-based model sketched above can successfully control the output of the generated compressions. We show the output of a single-step compression model on three languages initialized with varying compression rates3 (see Section 4 for details on how the models were trained and tested). The compression rate (CR) is used to determine length parameter of Equation (8): Ty0 = Tx ·CR 1. Europarl, the European Parliament Proceedings Parallel Corpus (Koehn, 2005), has been used extensively in machine translation research; it contains the minutes of the European parliament and is a spoken corpus of formulaic nature; speakers take part in debating various issues concerning EU policy (e.g., taxation, environment). 2. The TED parallel Corpus (Cettolo et al., 2012) contains transcripts in multiple languages of short talks devoted to spreading powerful ideas on a variety of topics ranging from science to business and global issues. 3. The EU bookshop corpus (Skadin¸sˇ et al., 2014) contains publications from European institutions covering a variety of topic"
D18-1267,P07-2045,0,0.00441645,"up to four encoder-decoder NMT models in our experiments (BLEU scores4 shown in parentheses): English→French (27.03), French→English (29.14), English→German (28.3), and German→English (31.19). German training/test data was taken from the WMT16 shared task and French from the WMT14 shared task. The training data was 4.2 million and 39 million sentence pairs for en-de, and en-fr, respectively. We also used back-translated monolingual training data, from the news domain, (Sennrich et al., 2016a) in training for the German systems. The data was pre-processed using standard scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, using byte pair encoding (BPE; Sennrich et al. 2016b). The BPE operations are shared between language directions. We experimented with various model variants using one or multiple pivots. The compression rate (see Equation (8)) was tuned experimentally on the validation set which consists of one document from each domain (20 source sentences; 100 compression-pairs). Compression rates varied from 0.55 to 0.85 and were broadly comparable to those shown in Table 2. 4 BLEU scores were calculated using mteval-v13a.pl. Comparison Systems We compared our m"
D18-1267,W04-1013,0,0.0134269,"y short summary (75 bytes) for a document. The evaluation set consists of 500 source documents (from the New York Times and Associated Press Wire services) each paired with four humanwritten (reference) summaries. We follow previous work (Rush et al., 2015; Chopra et al., 2016) in compressing the first sentence of the document and presenting this as the summary. To make the evaluation unbiased to length, the output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries. Our results are shown in Table 7. To compare with existing methods, we also report ROUGE (Lin, 2004) unigram and bigram overlap (Lin, 2004) and the longest common subsequence (ROUGE -L).9 We employed a dual step compression model (see Section 2) as preliminary experiments showed that it was superior to singlestage variants. We compared single and multiple pivot models against existing ABS and ABS+ (Rush et al., 2015), two encoder-decoder models trained on the English Gigaword. ABS+ applies minimum error rate (MERT) training as a copy7 Our ABS implementation obtains R1-R 25.03, R2-R 8.40, and RL-R: 22.35 8 Our LenInit implementation obtains R1-R 29.26, R2-R 9.56, and RL-R 25.70 9 We used ROUG"
D18-1267,W15-1818,0,0.0195803,"sion specific data. We release1 M OSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres. 1 Introduction Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the comp"
D18-1267,N03-1026,0,0.0657919,"uce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, ther"
D18-1267,D15-1044,0,0.129804,"nce compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in applying neural network models to natural language generation tasks, including sentence compression (Rush et al., 2015; Filippova et al., 2015; Chopra et al., 2016; Kikuchi et al., 2016). Filippova et al. (2015) focus on deletion-based sentence compression which they model as a sequence labeling problem using a recurrent neural network with long short-term memory units (LSTM; Hochreiter and Schmidhuber 1997). Rush et al. (2015) capture the full gamut of rewrite operations drawing insights from encoderdecoder models recently proposed for machine translation (Bahdanau et al., 2015). Neural network-based approaches are datadriven, relying on the ability of recurrent architectures to learn continuous features wit"
D18-1267,E17-3017,1,0.884756,"Missing"
D18-1267,P16-1009,1,0.824345,"ssion rate (CR), TER scores, and number of insertions (Ins), deletions (Del), substitutions (Sub), and shifts (Shft). We used up to four encoder-decoder NMT models in our experiments (BLEU scores4 shown in parentheses): English→French (27.03), French→English (29.14), English→German (28.3), and German→English (31.19). German training/test data was taken from the WMT16 shared task and French from the WMT14 shared task. The training data was 4.2 million and 39 million sentence pairs for en-de, and en-fr, respectively. We also used back-translated monolingual training data, from the news domain, (Sennrich et al., 2016a) in training for the German systems. The data was pre-processed using standard scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, using byte pair encoding (BPE; Sennrich et al. 2016b). The BPE operations are shared between language directions. We experimented with various model variants using one or multiple pivots. The compression rate (see Equation (8)) was tuned experimentally on the validation set which consists of one document from each domain (20 source sentences; 100 compression-pairs). Compression rates varied from 0.55 to 0.85 and were broadly co"
D18-1267,P16-1162,1,0.22348,"ssion rate (CR), TER scores, and number of insertions (Ins), deletions (Del), substitutions (Sub), and shifts (Shft). We used up to four encoder-decoder NMT models in our experiments (BLEU scores4 shown in parentheses): English→French (27.03), French→English (29.14), English→German (28.3), and German→English (31.19). German training/test data was taken from the WMT16 shared task and French from the WMT14 shared task. The training data was 4.2 million and 39 million sentence pairs for en-de, and en-fr, respectively. We also used back-translated monolingual training data, from the news domain, (Sennrich et al., 2016a) in training for the German systems. The data was pre-processed using standard scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, using byte pair encoding (BPE; Sennrich et al. 2016b). The BPE operations are shared between language directions. We experimented with various model variants using one or multiple pivots. The compression rate (see Equation (8)) was tuned experimentally on the validation set which consists of one document from each domain (20 source sentences; 100 compression-pairs). Compression rates varied from 0.55 to 0.85 and were broadly co"
D18-1267,skadins-etal-2014-billions,0,0.0607245,"Missing"
D18-1267,D17-1062,1,0.872488,"Missing"
D18-1267,2006.amta-papers.25,0,0.0562624,"ee column SL), TED contains the shortest sentences, while the other two corpora are somewhere in-between. We also observe that crowdworkers compress the least when it comes to TED (see column CR), which is not surprising given the brevity of the utterances. Overall, French speakers seem more conservative when shortening sentences compared to English and German. In general, compression rates are genre dependent, they range from 0.58 (for English Europarl) to 0.84 (for German TED). We also examined the degree to which crowdworkers paraphrase the source sentence using Translation Edit Rate (TER; Snover et al., 2006), a measure commonly used to automatically evaluate the quality of machine translation output. We used TER to compute the (average) number of edits required to change a long sentence to shorter output. We also report the number of edits by type, i.e., the number of insertions, substitutions, deletions, and shifts needed (on average) to convert long to short sentences. We observe that crowdworkers perform a fair amount of rewriting across corpora and languages. The most frequent rewrite operations are deletions followed by substitutions, shifts, and insertions. 4 Experimental Setup Neural Machi"
D18-1267,D16-1033,0,0.198601,"to achieve good performance, they require large amounts of training data, in the region of millions of long-short sentence pairs.2 Existing compression datasets are several orders of magnitude smaller. For example, the ZiffDavis corpus (Knight and Marcu, 2002) contains 1,067 sentences and originated from a collection of news articles on computer products. Clarke and Lapata (2008) create two manual corpora sampled from written (1,433 sentences) and spoken sources (1,370 sentences). Cohn and Lapata (2013) elicit manual compressions for 625 sentences taken from newspaper articles. More recently, Toutanova et al. (2016) crowdsource a larger corpus which contains manual compressions for single and multiple sentences (about 26,000 pairs of source and compressed texts). 2 Rush et al. (2015) use approximately four million training instances and Filippova et al. (2015) two million. 2453 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2453–2464 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Since large scale compression datasets do not occur naturally, they must be somehow approximated, e.g., by pairing headlines with"
D18-1267,P05-1036,0,0.0660819,"gle sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://github. com/Jmallins/MOSS the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words. More recently, there has been much interest in"
D18-1267,N07-1061,0,0.0258365,"d language at hand. We determine the target length experimentally based on a small validation set. 2.3 Pivoting Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is no translation path from the source language to the target by taking advantage of paths through an intermediate language. The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) and more recently in neural MT systems (Firat et al., 2016). We use pivoting to provide a path from a source English sentence, via an intermediate foreign language, to English in a compressed form. We propose to extend Mallinson et al.’s (2017) approach to multi-pivoting, where a sentence x is translated to K-best foreign pivots, Fx = { f1 , ..., fK }. The probability of generating compression y = y1 ...yTy is decomposed as: Fx → − ← − P(y|x) = ∑ P(y |f ; θ ) · P( f |x; θ ) (9) f which we approximate as the tokenwise weighted average of the pivots: j=1 2.2 Length Control Ty Fx To be able to p"
D18-1267,W04-1015,0,0.0804665,"es without access to compression specific data. We release1 M OSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres. 1 Introduction Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013). Irrespective of how 1 Publicly available for download at https://gith"
D18-1267,P10-1058,1,0.778912,"anguage as long as a bilingual corpus is available and performs arbitrary rewrites without access to compression specific data. We release1 M OSS, a new parallel Multilingual Compression dataset for English, German, and French which can be used to evaluate compression models across languages and genres. 1 Introduction Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency. The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001). The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lap"
D18-1267,P07-1108,0,0.0475611,"e domain, genre, and language at hand. We determine the target length experimentally based on a small validation set. 2.3 Pivoting Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is no translation path from the source language to the target by taking advantage of paths through an intermediate language. The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) and more recently in neural MT systems (Firat et al., 2016). We use pivoting to provide a path from a source English sentence, via an intermediate foreign language, to English in a compressed form. We propose to extend Mallinson et al.’s (2017) approach to multi-pivoting, where a sentence x is translated to K-best foreign pivots, Fx = { f1 , ..., fK }. The probability of generating compression y = y1 ...yTy is decomposed as: Fx → − ← − P(y|x) = ∑ P(y |f ; θ ) · P( f |x; θ ) (9) f which we approximate as the tokenwise weighted average of the pivots: j=1 2.2 Length C"
D18-1458,2017.lilt-15.3,0,0.0656005,"in Transformers impacts their ability to capture long-distance dependencies. Specifically, many-headed multi-head attention is essential for modeling longdistance phenomena with only self-attention. • We empirically show that Transformers excel at WSD, indicating that they are strong semantic feature extractors. 2 Related work Yin et al. (2017) are the first to compare CNNs, LSTMs and GRUs on several NLP tasks. They find that CNNs are better at tasks related to semantics, while RNNs are better at syntax-related tasks, especially for longer sentences. Based on the work of Linzen et al. (2016), Bernardy and Lappin (2017) find that RNNs perform better than CNNs on a subject-verb agreement task, which is a good proxy for how well long-range dependencies are captured. Tran et al. (2018) find that a Transformer language model performs worse than an RNN language model on a subject-verb agreement task. They, too, note that this is especially true as the distance between subject and verb grows, even if RNNs resulted in a higher perplexity on the validation set. This result of Tran et al. (2018) is clearly in contrast to the general finding that Transformers are better than RNNs for NMT tasks. Bai et al. (2018) evalu"
D18-1458,P18-1008,0,0.040533,"Bernardy and Lappin, 2017; Sennrich, 2017; Tran et al., 2018). Thus, we also use this task to evaluate different NMT architectures on long-range dependencies. 4.1 Experimental Settings Different architectures are hard to compare fairly because many factors affect performance. We aim to create a level playing field for the comparison by training with the same toolkit, Sockeye (Hieber et al., 2017) which is based on MXNet (Chen et al., 2015). In addition, different hyperparameters and training techniques (such as label smoothing or layer normalization) have been found to affect the performance (Chen et al., 2018). We apply the same hyperparameters and techniques for all architectures except the parameters of each specific architecture. Since the best hyperparameters for different architectures may be diverse, we verify our hyperparameter choice by comparing our results to those published previously. Our models achieve similar performance to that reported by Hieber et al. (2017) with the best available settings. In addition, we extend Sockeye with an interface that enables scoring of existing translations, which is required for contrastive evaluation. All the models are trained with 2 GPUs. During trai"
D18-1458,D14-1179,0,0.0317728,"Missing"
D18-1458,P18-1167,0,0.029996,"onal language modeling. For WSD, it is unclear whether the most important component is the encoder, the decoder, or both. Following the hypothesis that Transformer encoders excel as semantic feature extractors, we train a hybrid encoder-decoder model (TransRNN) with a Transformer encoder and an RNN decoder. The results (in Table 5) show that TransRNN performs better than RNNS2S, but worse than the pure Transformer, both in terms of BLEU and WSD accuracy. This indicates that WSD is not only done in the encoder, but that the decoder also affects WSD performance. We note that Chen et al. (2018); Domhan (2018) introduce the techniques in Transformers into RNN-based models, with reportedly higher BLEU. Thus, it would be interesting to see if the same result holds true with their architectures. 6 Conclusion In this paper, we evaluate three popular NMT architectures, RNNS2S, ConvS2S, and Transformers, on subject-verb agreement and WSD by scoring contrastive translation pairs. We test the theoretical claims that shorter path lengths make models better capture long-range dependencies. Our experimental results show that: 6 https://github.com/a-rios/ContraWSD/ tree/master/baselines 4270 • There is no evid"
D18-1458,E17-3017,1,0.917028,"ng distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation. 1 Annette Rios2 Introduction Different architectures have been shown to be effective for neural machine translation (NMT), ranging from recurrent architectures (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015) to convolutional (Kalchbrenner and Blunsom, 2013; Gehring et al., 2017) and, most recently, fully selfattentional (Transformer) models (Vaswani et al., 2017). Since comparisons (Gehring et al., 2017; Vaswani et al., 2017; Hieber et al., 2017) are mainly carried out via BLEU (Papineni et al., ∗ Work carried out during a visit to the machine translation group at the University of Edinburgh. 2002), it is inherently difficult to attribute gains in BLEU to architectural properties. Recurrent neural networks (RNNs) (Elman, 1990) can easily deal with variable-length input sentences and thus are a natural choice for the encoder and decoder of NMT systems. Modern variants of RNNs, such as GRUs (Cho et al., 2014) and LSTMs (Hochreiter and Schmidhuber, 1997), address the difficulty of training recurrent networks with long-range dependencies."
D18-1458,P18-4020,0,0.0225771,"Missing"
D18-1458,D13-1176,0,0.0862241,"on networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation. 1 Annette Rios2 Introduction Different architectures have been shown to be effective for neural machine translation (NMT), ranging from recurrent architectures (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015) to convolutional (Kalchbrenner and Blunsom, 2013; Gehring et al., 2017) and, most recently, fully selfattentional (Transformer) models (Vaswani et al., 2017). Since comparisons (Gehring et al., 2017; Vaswani et al., 2017; Hieber et al., 2017) are mainly carried out via BLEU (Papineni et al., ∗ Work carried out during a visit to the machine translation group at the University of Edinburgh. 2002), it is inherently difficult to attribute gains in BLEU to architectural properties. Recurrent neural networks (RNNs) (Elman, 1990) ca"
D18-1458,Q16-1037,0,0.260112,"er of attention heads in Transformers impacts their ability to capture long-distance dependencies. Specifically, many-headed multi-head attention is essential for modeling longdistance phenomena with only self-attention. • We empirically show that Transformers excel at WSD, indicating that they are strong semantic feature extractors. 2 Related work Yin et al. (2017) are the first to compare CNNs, LSTMs and GRUs on several NLP tasks. They find that CNNs are better at tasks related to semantics, while RNNs are better at syntax-related tasks, especially for longer sentences. Based on the work of Linzen et al. (2016), Bernardy and Lappin (2017) find that RNNs perform better than CNNs on a subject-verb agreement task, which is a good proxy for how well long-range dependencies are captured. Tran et al. (2018) find that a Transformer language model performs worse than an RNN language model on a subject-verb agreement task. They, too, note that this is especially true as the distance between subject and verb grows, even if RNNs resulted in a higher perplexity on the validation set. This result of Tran et al. (2018) is clearly in contrast to the general finding that Transformers are better than RNNs for NMT ta"
D18-1458,D15-1166,0,0.715198,"pendencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation. 1 Annette Rios2 Introduction Different architectures have been shown to be effective for neural machine translation (NMT), ranging from recurrent architectures (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015) to convolutional (Kalchbrenner and Blunsom, 2013; Gehring et al., 2017) and, most recently, fully selfattentional (Transformer) models (Vaswani et al., 2017). Since comparisons (Gehring et al., 2017; Vaswani et al., 2017; Hieber et al., 2017) are mainly carried out via BLEU (Papineni et al., ∗ Work carried out during a visit to the machine translation group at the University of Edinburgh. 2002), it is inherently difficult to attribute gains in BLEU to architectural properties. Recurrent neural networks (RNNs) (Elman, 1990) can easily deal with variable-length input sentences and thus are a na"
D18-1458,W17-4710,1,0.849111,"ccuracy uni-directional LSTMs, and the decoder is a stack of 8 uni-directional LSTMs. The size of embeddings and hidden states is 512. We apply layer normalization and label smoothing (0.1) in all models. We tie the source and target embeddings. The dropout rate of embeddings and Transformer blocks is set to 0.1. The dropout rate of RNNs and CNNs is 0.2. The kernel size of CNNs is 3. Transformers have an 8-head attention mechanism. To test the robustness of our findings, we also test a different style of RNN architecture, from a different toolkit. We evaluate bi-deep transitional RNNs (Miceli Barone et al., 2017) which are state-of-art RNNs in machine translation. We use the bi-deep RNN-based model (RNN-bideep) implemented in Marian (Junczys-Dowmunt et al., 2018). Different from the previous settings, we use the Adam optimizer with β1 = 0.9, β2 = 0.98,  = 10−9 . The initial learning rate is 0.0003. We tie target embeddings and output embeddings. Both the encoder and decoder have 4 layers of LSTM units, only the encoder layers are bi-directional. LSTM units consist of several cells (deep transition): 4 in the first layer of the decoder, 2 cells everywhere else. We use training data from the WMT17 shar"
D18-1458,P02-1040,0,0.104071,"Missing"
D18-1458,W18-6319,0,0.0166964,"Different from the previous settings, we use the Adam optimizer with β1 = 0.9, β2 = 0.98,  = 10−9 . The initial learning rate is 0.0003. We tie target embeddings and output embeddings. Both the encoder and decoder have 4 layers of LSTM units, only the encoder layers are bi-directional. LSTM units consist of several cells (deep transition): 4 in the first layer of the decoder, 2 cells everywhere else. We use training data from the WMT17 shared task.2 We use newstest2013 as the validation set, and use newstest2014 and newstest2017 as the test sets. All BLEU scores are computed with SacreBLEU (Post, 2018). There are about 5.9 million sentence pairs in the training set after preprocessing with Moses scripts. We learn a joint BPE model with 32,000 subword units (Sennrich et al., 2016). We employ the model that has the best perplexity on the validation set for the evaluation. 0.8 0.75 RNNS2S 0.7 RNN-bideep 0.65 ConvS2S 0.6 Transformer 0.55 Overall Results 0.5 Table 2 reports the BLEU scores on newstest2014 and newstest2017, the perplexity on the validation set, and the accuracy on long-range dependencies.3 Transformer achieves the highest accuracy on this task and the highest BLEU scores on both"
D18-1458,W17-4702,1,0.849861,"hitectures: models based on RNNs (referred to as RNNS2S in the remainder of the paper), based on CNNs (referred to as ConvS2S) and self-attentional models (referred to as Transformers). Motivated by the aforementioned theoretical claims regarding path length and semantic feature extraction, we evaluate their performance on a subject-verb agreement task (that requires modeling long-range dependencies) and a word sense disambiguation (WSD) task (that requires extracting semantic features). Both tasks build on test sets of contrastive translation pairs, Lingeval97 (Sennrich, 2017) and ContraWSD (Rios et al., 2017). The main contributions of this paper can be summarized as follows: • We test the theoretical claims that architectures with shorter paths through networks are better at capturing long-range dependencies. Our experimental results on modeling subject-verb agreement over long distances do not show any evidence that Transformers or CNNs are superior to RNNs in this regard. • We empirically show that the number of attention heads in Transformers impacts their ability to capture long-distance dependencies. Specifically, many-headed multi-head attention is essential for modeling longdistance phenom"
D18-1458,E17-2060,1,0.937012,"luate all three popular NMT architectures: models based on RNNs (referred to as RNNS2S in the remainder of the paper), based on CNNs (referred to as ConvS2S) and self-attentional models (referred to as Transformers). Motivated by the aforementioned theoretical claims regarding path length and semantic feature extraction, we evaluate their performance on a subject-verb agreement task (that requires modeling long-range dependencies) and a word sense disambiguation (WSD) task (that requires extracting semantic features). Both tasks build on test sets of contrastive translation pairs, Lingeval97 (Sennrich, 2017) and ContraWSD (Rios et al., 2017). The main contributions of this paper can be summarized as follows: • We test the theoretical claims that architectures with shorter paths through networks are better at capturing long-range dependencies. Our experimental results on modeling subject-verb agreement over long distances do not show any evidence that Transformers or CNNs are superior to RNNs in this regard. • We empirically show that the number of attention heads in Transformers impacts their ability to capture long-distance dependencies. Specifically, many-headed multi-head attention is essentia"
D18-1458,P16-1162,1,0.452352,"tput embeddings. Both the encoder and decoder have 4 layers of LSTM units, only the encoder layers are bi-directional. LSTM units consist of several cells (deep transition): 4 in the first layer of the decoder, 2 cells everywhere else. We use training data from the WMT17 shared task.2 We use newstest2013 as the validation set, and use newstest2014 and newstest2017 as the test sets. All BLEU scores are computed with SacreBLEU (Post, 2018). There are about 5.9 million sentence pairs in the training set after preprocessing with Moses scripts. We learn a joint BPE model with 32,000 subword units (Sennrich et al., 2016). We employ the model that has the best perplexity on the validation set for the evaluation. 0.8 0.75 RNNS2S 0.7 RNN-bideep 0.65 ConvS2S 0.6 Transformer 0.55 Overall Results 0.5 Table 2 reports the BLEU scores on newstest2014 and newstest2017, the perplexity on the validation set, and the accuracy on long-range dependencies.3 Transformer achieves the highest accuracy on this task and the highest BLEU scores on both newstest2014 and newstest2017. Compared to RNNS2S, ConvS2S has slightly better results regarding BLEU scores, but a much lower accuracy on long-range dependencies. The RNN-bideep mo"
D18-1458,C18-1112,1,0.849005,"ask. They, too, note that this is especially true as the distance between subject and verb grows, even if RNNs resulted in a higher perplexity on the validation set. This result of Tran et al. (2018) is clearly in contrast to the general finding that Transformers are better than RNNs for NMT tasks. Bai et al. (2018) evaluate CNNs and LSTMs on several sequence modeling tasks. They conclude that CNNs are better than RNNs for sequence modeling. However, their CNN models perform much worse than the state-of-art LSTM models on some sequence modeling tasks, as they themselves state in the appendix. Tang et al. (2018) evaluate different RNN architectures and Transformer models on the task of historical spelling normalization which translates a historical spelling into its modern form. They find that Transformer models surpass RNN models only in high-resource conditions. In contrast to previous studies, we focus on the machine translation task, where architecture comparisons so far are mostly based on BLEU. 3 Background 3.1 NMT Architectures We evaluate three different NMT architectures: RNN-based models, CNN-based models, and Transformer-based models. All of them have a bipartite structure in the sense tha"
D18-1458,tiedemann-2012-parallel,0,0.0240104,"Missing"
D18-1458,D18-1503,0,0.324674,"ith only self-attention. • We empirically show that Transformers excel at WSD, indicating that they are strong semantic feature extractors. 2 Related work Yin et al. (2017) are the first to compare CNNs, LSTMs and GRUs on several NLP tasks. They find that CNNs are better at tasks related to semantics, while RNNs are better at syntax-related tasks, especially for longer sentences. Based on the work of Linzen et al. (2016), Bernardy and Lappin (2017) find that RNNs perform better than CNNs on a subject-verb agreement task, which is a good proxy for how well long-range dependencies are captured. Tran et al. (2018) find that a Transformer language model performs worse than an RNN language model on a subject-verb agreement task. They, too, note that this is especially true as the distance between subject and verb grows, even if RNNs resulted in a higher perplexity on the validation set. This result of Tran et al. (2018) is clearly in contrast to the general finding that Transformers are better than RNNs for NMT tasks. Bai et al. (2018) evaluate CNNs and LSTMs on several sequence modeling tasks. They conclude that CNNs are better than RNNs for sequence modeling. However, their CNN models perform much wors"
D18-1512,1999.mtsummit-1.31,0,0.490937,"Missing"
D18-1512,D09-1030,0,0.050412,"wn the source and a candidate text, and asked: How accurately does the above candidate text convey the semantics of the source text? In doing so, they have translations produced by humans and machines rated independently, and parity is assumed if the mean score of the former does not significantly differ from the mean score of the latter. Raters To optimise cost, machine translation quality is typically assessed by means of crowdsourcing. Combined ratings of bilingual crowd workers have been shown to be more reliable than automatic metrics and “very similar” to ratings produced by “experts”2 (Callison-Burch, 2009). Graham et al. (2017) compare crowdsourced to “expert” ratings on machine translations from WMT 2012, concluding that, with proper quality control, “machine translation systems can indeed be evaluated by the crowd alone.” However, it is unclear whether this finding carries over to translations produced by NMT systems where, due to increased fluency, errors are more difficult to identify (Castilho et al., 2017a), and concurrent work by Toral et al. (2018) highlights the importance of expert translators for MT evaluation. Experimental Unit Machine translation evaluation is predominantly perform"
D18-1512,W07-0718,0,0.226812,"Missing"
D18-1512,2004.iwslt-evaluation.1,0,0.110253,"Missing"
D18-1512,W16-4616,0,0.0181594,"indings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs. 1 Martin Volk1 Introduction Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has become the de-facto standard in machine translation, outperforming earlier phrasebased approaches in many data settings and shared translation tasks (Luong and Manning, 2015; Sennrich et al., 2016; Cromieres et al., 2016). Some recent results suggest that neural machine translation “approaches the accuracy achieved by average bilingual human translators [on some test sets]” (Wu et al., 2016), or even that its “translation quality is at human parity when compared to professional human translators” (Hassan et al., 2018). Claims of human parity in machine translation are certainly extraordinary, and require extraordinary evidence.1 Laudably, Hassan et al. (2018) have released their data publicly to allow external validation of their claims. Their claims are further strengthened by the fact that they follow best p"
D18-1512,W13-2305,0,0.0482831,"79), and we identify three aspects with particular relevance to assessing human parity: granularity of measurement (ordinal vs. interval scales), raters (experts vs. crowd workers), and experimental unit (sentence vs. document). 2.1 Related Work Granularity of Measurement Callison-Burch et al. (2007) show that ranking (Which of these translations is better?) leads to better inter-rater agreement than absolute judgement on 5-point Likert scales (How good is this translation?) but gives no insight about how much a candidate translation differs from a (presumably perfect) reference. To this end, Graham et al. (2013) suggest the use of continuous scales for direct assessment of translation quality. Implemented as a slider between 0 (Not at all) and 100 (Perfectly), their method yields scores on a 100-point interval scale in practice (Bojar et al., 2016b, 2017), with each raters’ rating being standardised to increase homogeneity. Hassan et al. (2018) use source-based direct assessment to avoid bias towards reference translations. In the shared task evaluation by Cettolo et al. (2017), raters are shown the source and a candidate text, and asked: How accurately does the above candidate text convey the semant"
D18-1512,D13-1176,0,0.0344536,"cols, contrasting the evaluation of single sentences and entire documents. In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs. 1 Martin Volk1 Introduction Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has become the de-facto standard in machine translation, outperforming earlier phrasebased approaches in many data settings and shared translation tasks (Luong and Manning, 2015; Sennrich et al., 2016; Cromieres et al., 2016). Some recent results suggest that neural machine translation “approaches the accuracy achieved by average bilingual human translators [on some test sets]” (Wu et al., 2016), or even that its “translation quality is at human parity when compared to professional human translators” (Hassan et al., 2018). Claims of human parity"
D18-1512,W18-6312,0,0.0736235,"f bilingual crowd workers have been shown to be more reliable than automatic metrics and “very similar” to ratings produced by “experts”2 (Callison-Burch, 2009). Graham et al. (2017) compare crowdsourced to “expert” ratings on machine translations from WMT 2012, concluding that, with proper quality control, “machine translation systems can indeed be evaluated by the crowd alone.” However, it is unclear whether this finding carries over to translations produced by NMT systems where, due to increased fluency, errors are more difficult to identify (Castilho et al., 2017a), and concurrent work by Toral et al. (2018) highlights the importance of expert translators for MT evaluation. Experimental Unit Machine translation evaluation is predominantly performed on single sentences, presented to raters in random order (e. g., Bojar et al., 2017; Cettolo et al., 2017). There are two main reasons for this. The first is cost: if raters assess entire documents, obtaining the same number of data points in an evaluation campaign multiplies the cost by the average number of sentences per document. The second is experimental validity. When comparing systems that produce sentences without considering documentlevel cont"
D18-1512,2015.iwslt-evaluation.11,0,0.0466284,"cuments as compared to isolated sentences. Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs. 1 Martin Volk1 Introduction Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has become the de-facto standard in machine translation, outperforming earlier phrasebased approaches in many data settings and shared translation tasks (Luong and Manning, 2015; Sennrich et al., 2016; Cromieres et al., 2016). Some recent results suggest that neural machine translation “approaches the accuracy achieved by average bilingual human translators [on some test sets]” (Wu et al., 2016), or even that its “translation quality is at human parity when compared to professional human translators” (Hassan et al., 2018). Claims of human parity in machine translation are certainly extraordinary, and require extraordinary evidence.1 Laudably, Hassan et al. (2018) have released their data publicly to allow external validation of their claims. Their claims are further"
D18-1512,W16-2323,1,\N,Missing
D18-1512,W17-4739,1,\N,Missing
D19-1081,W16-2378,0,0.027085,"in Figure 6. Figure 5 shows how consistency scores are changing in training.6 For deixis, the model achieves the final quality quite quickly; for the rest, it needs a large number of training steps to converge. 8 (b) Figure 5: Consistency scores progression in training. 8.1 Automatic post-editing Our model can be regarded as an automatic postediting system – a system designed to fix systematic MT errors that is decoupled from the main MT system. Automatic post-editing has a long history, including rule-based (Knight and Chander, 1994), statistical (Simard et al., 2007) and neural approaches (Junczys-Dowmunt and Grundkiewicz, 2016; Pal et al., 2016; Freitag et al., 2019). In terms of architectures, modern approaches use neural sequence-to-sequence models, either multi-source architectures that consider both the original source and the baseline translation (Junczys-Dowmunt and Grundkiewicz, 2016; Pal et al., 2016), or monolingual repair systems, as in Freitag et al. (2019), which is concurrent work to ours. True post-editing datasets are typically small and expensive to create (Specia et al., 2017), hence synthetic training data has been created that uses original monolingual data as output for the sequence-to-sequence"
D19-1081,C18-1050,0,0.280125,"rsity of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use monolingual data to imitate typical inconsistencies between context-agnostic translations of isolated sente"
D19-1081,D18-1512,1,0.752714,"Missing"
D19-1081,N18-1118,1,0.881056,"ennrich4,3 Ivan Titov3,2 1 3 Yandex, Russia 2 University of Amsterdam, Netherlands University of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use monolingual data to"
D19-1081,L18-1275,0,0.0705509,"t statistics are shown in Table 1. The suites for deixis and lexical cohesion are split into development and test sets, with 500 examples from each used for validation purposes and the rest for testing. Convergence of both consistency scores on these development sets and BLEU score on a general development set are used as early stopping criteria in models training. For ellipsis, there is no dedicated development set, so we evaluate on all the ellipsis data and do not use it for development. 3.2 4 4.1 Experimental Setup Data preprocessing We use the publicly available OpenSubtitles2018 corpus (Lison et al., 2018) for English and Russian. For a fair comparison with previous work, we train the baseline MT system on the data released by Voita et al. (2019). Namely, our MT system is trained on 6m instances. These are sentence pairs with a relative time overlap of subtitle frames between source and target language subtitles of at least 0.9. We gathered 30m groups of 4 consecutive sentences as our monolingual data. We used only documents not containing groups of sentences from general development and test sets as well as from contrastive test sets. The main results we report Phenomena overview Deixis Deicti"
D19-1081,P18-1118,0,0.262537,"ia 2 University of Amsterdam, Netherlands University of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use monolingual data to imitate typical inconsistencies between conte"
D19-1081,P12-1000,0,0.240227,"Missing"
D19-1081,P19-1116,1,0.829458,"Missing"
D19-1081,D18-1325,0,0.340288,"Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use monolingual data to imitate typical inconsistencies between context-agnostic translations of isolated sentences. The DocRepair model"
D19-1081,P18-1117,1,0.894452,"3,2 1 3 Yandex, Russia 2 University of Amsterdam, Netherlands University of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use monolingual data to imitate typical inco"
D19-1081,P16-2046,0,0.0681467,"Missing"
D19-1081,D17-1301,0,0.21286,"Neural Machine Translation Elena Voita1,2 Rico Sennrich4,3 Ivan Titov3,2 1 3 Yandex, Russia 2 University of Amsterdam, Netherlands University of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sen"
D19-1081,P16-1009,1,0.944061,"ered 30m groups of 4 consecutive sentences as our monolingual data. We used only documents not containing groups of sentences from general development and test sets as well as from contrastive test sets. The main results we report Phenomena overview Deixis Deictic words or phrases, are referential expressions whose denotation depends on context. 879 are for the model trained on all 30m fragments. We use the tokenization provided by the corpus and use multi-bleu.perl3 on lowercased data to compute BLEU score. We use beam search with a beam of 4. Sentences were encoded using byte-pair encoding (Sennrich et al., 2016b), with source and target vocabularies of about 32000 tokens. Translation pairs were batched together by approximate sequence length. Each training batch contained a set of translation pairs containing approximately 150004 source tokens. It has been shown that Transformer’s performance depends heavily on batch size (Popel and Bojar, 2018), and we chose a large batch size to ensure the best performance. In training context-aware models, for early stopping we use both convergence in BLEU score on the general development set and scores on the consistency development sets. After training, we aver"
D19-1081,P16-1162,1,0.737112,"ered 30m groups of 4 consecutive sentences as our monolingual data. We used only documents not containing groups of sentences from general development and test sets as well as from contrastive test sets. The main results we report Phenomena overview Deixis Deictic words or phrases, are referential expressions whose denotation depends on context. 879 are for the model trained on all 30m fragments. We use the tokenization provided by the corpus and use multi-bleu.perl3 on lowercased data to compute BLEU score. We use beam search with a beam of 4. Sentences were encoded using byte-pair encoding (Sennrich et al., 2016b), with source and target vocabularies of about 32000 tokens. Translation pairs were batched together by approximate sequence length. Each training batch contained a set of translation pairs containing approximately 150004 source tokens. It has been shown that Transformer’s performance depends heavily on batch size (Popel and Bojar, 2018), and we chose a large batch size to ensure the best performance. In training context-aware models, for early stopping we use both convergence in BLEU score on the general development set and scores on the consistency development sets. After training, we aver"
D19-1081,W17-4811,0,0.157059,"nslation Elena Voita1,2 Rico Sennrich4,3 Ivan Titov3,2 1 3 Yandex, Russia 2 University of Amsterdam, Netherlands University of Edinburgh, Scotland 4 University of Zurich, Switzerland lena-voita@yandex-team.ru sennrich@cl.uzh.ch ititov@inf.ed.ac.uk Abstract human over machine translation (Läubli et al., 2018). These findings emphasize the need to shift towards context-aware machine translation both from modeling and evaluation perspective. Most previous work on context-aware NMT assumed that either all the bilingual data is available at the document level (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Kuang et al., 2018; Miculicich et al., 2018) or at least its fraction (Voita et al., 2019). But in practical scenarios, document-level parallel data is often scarce, which is one of the challenges when building a contextaware system. We introduce an approach to context-aware machine translation using only monolingual document-level data. In our setting, a separate monolingual sequence-to-sequence model (DocRepair) is used to correct sentence-level translations of adjacent sentences. The key idea is to use"
D19-1083,D14-1179,0,0.0233531,"Missing"
D19-1083,D18-1338,0,0.701433,"as convolutional networks (Gehring et al., 2017) and selfattention networks (Vaswani et al., 2017). The Transformer represents the current SOTA in NMT. It heavily relies on the combination of residual connections (He et al., 2015) and layer normalization (Ba et al., 2016) for convergence. Nevertheless, simply extending this model with more layers results in gradient vanishing due to the interaction of RC and LN (see Section 4). Recent work has proposed methods to train deeper 899 Transformer models, including a rescheduling of RC and LN (Vaswani et al., 2018), the transparent attention model (Bapna et al., 2018) and the stochastic residual connection (Pham et al., 2019). In contrast to these work, we identify the large output variance of RC as the source of gradient vanishing, and employ scaled initialization to mitigate it without any structure adjustment. The effect of careful initialization on boosting convergence was also investigated and verified in previous work (Zhang et al., 2019; Child et al., 2019; Devlin et al., 2019; Radford et al., 2018). The merged attention network falls into the category of simplifying the Transformer so as to shorten training and/or decoding time. Methods to improve"
D19-1083,N19-1423,0,0.520782,"ayers. Solid lines indicate the vanilla Transformer, and dashed lines denote our proposed method. During back-propagation, gradients in Transformer gradually vanish from high layers to low layers. Introduction The capability of deep neural models of handling complex dependencies has benefited various artificial intelligence tasks, such as image recognition where test error was reduced by scaling VGG nets (Simonyan and Zisserman, 2015) up to hundreds of convolutional layers (He et al., 2015). In NLP, deep self-attention networks have enabled large-scale pretrained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) to boost state-of-the-art (SOTA) performance on downstream applications. By contrast, though neural machine translation (NMT) gained encouraging improvement when shifting from a shallow architecture (Bahdanau et al., 2015) to deeper ones (Zhou et al., 2016; Wu et al., 2016; Zhang et al., 2018; Chen et al., 2018), the Transformer (Vaswani et al., 2017), a currently SOTA architecture, achieves best results with merely 6 encoder and decoder layers, and no gains were reported by Vaswani et al. (2017) from 1 Source code for reproduction is available at https:// githu"
D19-1083,D17-1151,0,0.023642,"he encoder-decoder attention into a single sublayer, allowing for parallel computation. • We conduct extensive experiments and verify that deep Transformers with DS-Init and MAtt improve translation quality while preserving decoding efficiency. 2 Related Work Our work aims at improving translation quality by increasing model depth. Compared with the single-layer NMT system (Bahdanau et al., 2015), deep NMT models are typically more capable of handling complex language variations and translation relationships via stacking multiple encoder and decoder layers (Zhou et al., 2016; Wu et al., 2016; Britz et al., 2017; Chen et al., 2018), and/or multiple attention layers (Zhang et al., 2018). One common problem for the training of deep neural models are vanishing or exploding gradients. Existing methods mainly focus on developing novel network architectures so as to stabilize gradient back-propagation, such as the fast-forward connection (Zhou et al., 2016), the linear associative unit (Wang et al., 2017), or gated recurrent network variants (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001; Cho et al., 2014; Di Gangi and Federico, 2018). In contrast to the above recurrent network based NMT mod"
D19-1083,P18-1008,0,0.11932,"recognition where test error was reduced by scaling VGG nets (Simonyan and Zisserman, 2015) up to hundreds of convolutional layers (He et al., 2015). In NLP, deep self-attention networks have enabled large-scale pretrained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) to boost state-of-the-art (SOTA) performance on downstream applications. By contrast, though neural machine translation (NMT) gained encouraging improvement when shifting from a shallow architecture (Bahdanau et al., 2015) to deeper ones (Zhou et al., 2016; Wu et al., 2016; Zhang et al., 2018; Chen et al., 2018), the Transformer (Vaswani et al., 2017), a currently SOTA architecture, achieves best results with merely 6 encoder and decoder layers, and no gains were reported by Vaswani et al. (2017) from 1 Source code for reproduction is available at https:// github.com/bzhangGo/zero 898 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 898–909, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Our contributions are summarized as follows: further incr"
D19-1083,W18-2716,0,0.0383618,"Missing"
D19-1083,W18-6301,0,0.0786423,"Missing"
D19-1083,P02-1040,0,0.104645,"putation except for two linear projections: h i SA AN(Sl−1 ) = Ma (Sl−1 Wv ) Wo , (16) 902 Dataset WMT14 En-De WMT14 En-Fr WMT18 En-Fi WMT18 Zh-En IWSLT14 De-En #Src 116M 1045M 73M 510M 3.0M #Tgt 110M 1189M 54M 576M 3.2M #Sent 4.5M 36M 3.3M 25M 159K #BPE 32K 32K 32K 32K 30K evaluate our model on all WMT14-18 test sets for WMT14 En-De translation. The settings for IWSLT14 De-En are as in Ranzato et al. (2016), with 7584 sentence pairs for development, and the concatenated dev sets for IWSLT 2014 as test set (tst2010, tst2011, tst2012, dev2010, dev2012). We report tokenized case-sensitive BLEU (Papineni et al., 2002) for WMT14 En-De and WMT14 En-Fr, and provide detokenized casesensitive BLEU for WMT14 En-De, WMT18 EnFi and Zh-En with sacreBLEU (Post, 2018)5 . We also report chrF score for En-Fi translation which was found correlated better with human evaluation (Bojar et al., 2018). Following previous work (Wu et al., 2019), we evaluate IWSLT14 DeEn with tokenized case-insensitive BLEU. Table 2: Statistics for different training datasets. #Src and #Tgt denote the number of source and target tokens respectively. #Sent: the number of bilingual sentences. #BPE: the number of merge operations in BPE. M: milli"
D19-1083,P17-1013,0,0.0287158,"015), deep NMT models are typically more capable of handling complex language variations and translation relationships via stacking multiple encoder and decoder layers (Zhou et al., 2016; Wu et al., 2016; Britz et al., 2017; Chen et al., 2018), and/or multiple attention layers (Zhang et al., 2018). One common problem for the training of deep neural models are vanishing or exploding gradients. Existing methods mainly focus on developing novel network architectures so as to stabilize gradient back-propagation, such as the fast-forward connection (Zhou et al., 2016), the linear associative unit (Wang et al., 2017), or gated recurrent network variants (Hochreiter and Schmidhuber, 1997; Gers and Schmidhuber, 2001; Cho et al., 2014; Di Gangi and Federico, 2018). In contrast to the above recurrent network based NMT models, recent work focuses on feed-forward alternatives with more smooth gradient flow, such as convolutional networks (Gehring et al., 2017) and selfattention networks (Vaswani et al., 2017). The Transformer represents the current SOTA in NMT. It heavily relies on the combination of residual connections (He et al., 2015) and layer normalization (Ba et al., 2016) for convergence. Nevertheless,"
D19-1083,P19-1176,0,0.171746,"ly. Deep Transformer with our method outperforms its 6-layer counterpart by over 0.4 points on newstest2014 and around 0.1 point on newstest2014∼newstest2018. Our model outperforms the transparent model (Bapna et al., 2018) (+1.58 BLEU), an approach for the deep encoder. Our model performs on par with current SOTA, the dynamic convolution model (DCNN) (Wu et al., 2019). In particular, though DCNN achieves encouraging performance on newstest2014, it falls behind the baseline on other test sets. By contrast, our model obtains more consistent performance improvements. In work concurrent to ours, Wang et al. (2019) discuss how the placement of layer normalization affects deep Transformers, and compare the original post-norm (which we consider our baseline) and a pre-norm layout (which we call T2T). Their results also show that pre-norm allows training of deeper Transformers. Our results show that deep post-norm Transformers are also trainable with appropriate initialization, and tend to give slightly better results. 27.75 27.50 10 15 20 25 30 Model Depth Figure 3: Test BLEU score on newstest2014 with respect to model depth for Transformer+DS-Init+MAtt. Model Vaswani et al. (2017) Chen et al. (2018) Ott"
D19-1083,W18-6319,0,0.030342,"4 De-En #Src 116M 1045M 73M 510M 3.0M #Tgt 110M 1189M 54M 576M 3.2M #Sent 4.5M 36M 3.3M 25M 159K #BPE 32K 32K 32K 32K 30K evaluate our model on all WMT14-18 test sets for WMT14 En-De translation. The settings for IWSLT14 De-En are as in Ranzato et al. (2016), with 7584 sentence pairs for development, and the concatenated dev sets for IWSLT 2014 as test set (tst2010, tst2011, tst2012, dev2010, dev2012). We report tokenized case-sensitive BLEU (Papineni et al., 2002) for WMT14 En-De and WMT14 En-Fr, and provide detokenized casesensitive BLEU for WMT14 En-De, WMT18 EnFi and Zh-En with sacreBLEU (Post, 2018)5 . We also report chrF score for En-Fi translation which was found correlated better with human evaluation (Bojar et al., 2018). Following previous work (Wu et al., 2019), we evaluate IWSLT14 DeEn with tokenized case-insensitive BLEU. Table 2: Statistics for different training datasets. #Src and #Tgt denote the number of source and target tokens respectively. #Sent: the number of bilingual sentences. #BPE: the number of merge operations in BPE. M: million, K: thousand. where Ma denotes the average mask matrix for parallel computation (Zhang et al., 2018). This new model is then combined with"
D19-1083,P16-1162,1,0.515604,"0.6. Decoding is implemented with cache to save redundant computations. Other settings for specific translation tasks are explained in the individual subsections. Experiments 7.1 Model Settings Datasets and Evaluation We take WMT14 English-German translation (En-De) (Bojar et al., 2014) as our benchmark for model analysis, and examine the generalization of our approach on four other tasks: WMT14 English-French (En-Fr), IWSLT14 German-English (De-En) (Cettolo et al., 2014), WMT18 English-Finnish (En-Fi) and WMT18 Chinese-English (Zh-En) (Bojar et al., 2018). Byte pair encoding algorithm (BPE) (Sennrich et al., 2016) is used in preprocessing to handle low frequency words. Statistics of different datasets are listed in Table 2. Except for IWSLT14 De-En task, we collect subword units independently on the source and target side of training data. We directly use the preprocessed training data from the WMT18 website3 for En-Fi and Zh-En tasks, and use newstest2017 as our development set, newstest2018 as our test set. Our training data for WMT14 EnDe and WMT14 En-Fr is identical to previous setups (Vaswani et al., 2017; Wu et al., 2019). We use newstest2013 as development set for WMT14 En-De and newstest2012+20"
D19-1083,P18-1166,1,0.692132,"asks, such as image recognition where test error was reduced by scaling VGG nets (Simonyan and Zisserman, 2015) up to hundreds of convolutional layers (He et al., 2015). In NLP, deep self-attention networks have enabled large-scale pretrained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) to boost state-of-the-art (SOTA) performance on downstream applications. By contrast, though neural machine translation (NMT) gained encouraging improvement when shifting from a shallow architecture (Bahdanau et al., 2015) to deeper ones (Zhou et al., 2016; Wu et al., 2016; Zhang et al., 2018; Chen et al., 2018), the Transformer (Vaswani et al., 2017), a currently SOTA architecture, achieves best results with merely 6 encoder and decoder layers, and no gains were reported by Vaswani et al. (2017) from 1 Source code for reproduction is available at https:// github.com/bzhangGo/zero 898 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 898–909, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Our contributions are summarized as f"
D19-1083,W18-1819,0,0.402365,"rward alternatives with more smooth gradient flow, such as convolutional networks (Gehring et al., 2017) and selfattention networks (Vaswani et al., 2017). The Transformer represents the current SOTA in NMT. It heavily relies on the combination of residual connections (He et al., 2015) and layer normalization (Ba et al., 2016) for convergence. Nevertheless, simply extending this model with more layers results in gradient vanishing due to the interaction of RC and LN (see Section 4). Recent work has proposed methods to train deeper 899 Transformer models, including a rescheduling of RC and LN (Vaswani et al., 2018), the transparent attention model (Bapna et al., 2018) and the stochastic residual connection (Pham et al., 2019). In contrast to these work, we identify the large output variance of RC as the source of gradient vanishing, and employ scaled initialization to mitigate it without any structure adjustment. The effect of careful initialization on boosting convergence was also investigated and verified in previous work (Zhang et al., 2019; Child et al., 2019; Devlin et al., 2019; Radford et al., 2018). The merged attention network falls into the category of simplifying the Transformer so as to shor"
D19-1083,D16-1050,1,0.869028,"ayer. Experimental results show that deep models trained with these techniques clearly outperform a vanilla Transformer with 6 layers in terms of BLEU, and outperforms other solutions to train deep Transformers (Bapna et al., 2018; Vaswani et al., 2018). Thanks to the more efficient merged attention sublayer, we achieve these quality improvements while matching the decoding speed of the baseline model. In the future, we would like to extend our model to other sequence-to-sequence tasks, such as summarization and dialogue generation, as well as adapt the idea to other generative architectures (Zhang et al., 2016, 2018). We have trained models with up to 30 layers each for the encoder and decoder, and while training was successful and improved over shallower counterparts, gains are relatively small beyond 12 layers. An open question is whether there are other structural issues that limit the benefits of increasing the depth of the Transformer architecture, or whether the benefit of very deep models is greater for other tasks and dataset. 0.6 0.4 0.2 0.0 (b) Decoder 1.2 Transformer Transformer Transformer Transformer Gradient Norm 1.0 L1 L18 + DS-Init L1 + DS-Init L18 0.8 0.6 0.4 0.2 0.0 0 10 20 30 40"
D19-1083,Q16-1027,0,0.0956212,"ed various artificial intelligence tasks, such as image recognition where test error was reduced by scaling VGG nets (Simonyan and Zisserman, 2015) up to hundreds of convolutional layers (He et al., 2015). In NLP, deep self-attention networks have enabled large-scale pretrained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al., 2018) to boost state-of-the-art (SOTA) performance on downstream applications. By contrast, though neural machine translation (NMT) gained encouraging improvement when shifting from a shallow architecture (Bahdanau et al., 2015) to deeper ones (Zhou et al., 2016; Wu et al., 2016; Zhang et al., 2018; Chen et al., 2018), the Transformer (Vaswani et al., 2017), a currently SOTA architecture, achieves best results with merely 6 encoder and decoder layers, and no gains were reported by Vaswani et al. (2017) from 1 Source code for reproduction is available at https:// github.com/bzhangGo/zero 898 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 898–909, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics"
D19-1149,P17-1080,0,0.0303653,"18), extracted from ContraWSD (Rios et al., 2017), for both German→English (DE→EN) and German→French (DE→FR). The classifier is fed a representation of ambiguous nouns and a word sense (represented as the embedding of a translation candidate), and has to predict whether the two match. We can learn the role that encoders play in encoding information relevant for WSD by comparing different representations: word embeddings and encoder hidden states at different layers. We extract encoder hidden states from both RNNbased (RNNS2S) (Luong et al., 2015) and Transformer (Vaswani et al., 2017) models. Belinkov et al. (2017a,b) have shown that the higher layers are better at learning semantics. We hypothesize that the hidden states in higher layers incorporate more relevant information for WSD than those in lower layers. In addition to encoders, we also probe how much do decoder hidden states contribute to the WSD classification task. Recently, the distributions of attention mechanisms have been used for interpreting NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018b; Voita et al., 2019; Tang et al., 2019). We further investigate the attention weights and attention entropy of self-attentio"
D19-1149,I17-1001,0,0.057937,"Missing"
D19-1149,2005.mtsummit-papers.11,0,0.0517875,"ention distributions. We use attention entropy (Ghader and Monz, 2017) to measure the concentration. EAt (xt ) = − |x| X examples are used for training. We train 10 times with different seeds for each classifier and apply average accuracy. Table 1 lists the detailed statistics of the data. More experimental details are provided in the Appendix. At(xi , xt ) log At(xi , xt ) (1) i=1 For NMT models, we use the Sockeye (Hieber et al., 2017) toolkit to train RNNS2Ss and Transformers. DE→EN training data is from the WMT17 shared task (Bojar et al., 2017). DE→FR training data is from Europarl (v7) (Koehn, 2005) and News Commentary (v11) cleaned by Rios et al. (2017).3 In ContraWSD, each ambiguous noun has a small number of translation candidates. The average number of word senses per noun is 2.4 and 2.3 in DE→EN and DE→FR, respectively. We generate instances that are labelled with one candidate and a binary value indicating whether it corresponds to the correct sense. we get 50,792 and 43,268 instances in DE→EN and DE→FR, respectively. 5K/5K examples are randomly selected as the test/development set. The remaining We use TreeTagger (Schmid, 1995) to identify nouns. (1) If a query word is split into"
D19-1149,W17-3204,0,0.0226269,"umber of senses. Lexical ambiguities: number of sentences containing an ambiguous word. Instances: number of instances generated for WSD classification. 3.1 Results Table 2 provides the BLEU scores and the WSD accuracy on test sets, using different representations to represent ambiguous nouns. ENC denotes encoder hidden states; DEC means decoder hidden states. DE→EN Experiments 1 DE→FR NMT training data Here xi denotes the ith source token, xt is the current source token, and At(xi , xt ) represents the attention weight from xt to xi . We merge subwords after encoding, following the method in Koehn and Knowles (2017).2 Each self-attention layer has multiple heads and we average the attention weights from all the heads. In theory, sentential information is more important for ambiguous words that need to be disambiguated than non-ambiguous words. From the perspective of attention weights, for ambiguous words, we hypothesize that self-attention distributes more attention to the context words to capture the relevant sentential information, compared to words in general. From the perspective of attention entropy, we hypothesize that self-attention focuses on the related context words rather than the entire sent"
D19-1149,D15-1166,0,0.47884,"uous noun. We find that encoder hidden states outperform word embeddings significantly which indicates that encoders adequately encode relevant information for disambiguation into hidden states. In contrast to encoders, the effect of decoder is different in models with different architectures. Moreover, the attention weights and attention entropy show that self-attention can detect ambiguous nouns and distribute more attention to the context. 1 Introduction Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have access to the whole source sentence for the prediction of each word, which intuitively allows them to perform word sense disambiguation (WSD) better than previous phrase-based methods, and Rios et al. (2018) have confirmed this empirically. However, it is still unclear which component dominates the ability to disambiguate word senses. We explore the ability of NMT encoders and decoders to disambiguate word senses by evaluating hidden states and investigating the self-attention distributions. Marvin and Koehn (2018) find that the hidden states in higher encoder layers do not perform disam"
D19-1149,W18-1812,0,0.168414,"unsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have access to the whole source sentence for the prediction of each word, which intuitively allows them to perform word sense disambiguation (WSD) better than previous phrase-based methods, and Rios et al. (2018) have confirmed this empirically. However, it is still unclear which component dominates the ability to disambiguate word senses. We explore the ability of NMT encoders and decoders to disambiguate word senses by evaluating hidden states and investigating the self-attention distributions. Marvin and Koehn (2018) find that the hidden states in higher encoder layers do not perform disambiguation better than those in lower layers and conclude that encoders do not encode enough relevant context for disambiguation. However, their results are based on small data sets, and we wish to revisit this question with larger-scale data sets. Tang et al. (2018b) speculate that encoders have encoded the relevant information for WSD into hidden states before decoding but without any experimental tests. In this paper, we first train a classifier for WSD, on a much larger data set than Marvin and Koehn (2018), extracted"
D19-1149,W18-6319,0,0.0622041,"Missing"
D19-1149,D14-1179,0,0.115535,"Missing"
D19-1149,N18-1119,0,0.0178907,"so that ambiguous nouns have different representations in different source sentences. We can learn to what extent relevant information for WSD is encoded by encoders by comparing to the baseline. Decoders To explore the role of decoders, we feed the decoder hidden state at the time step predicting the translation of the ambiguous noun, and the word embedding of the current translate candidate into the classifier. The decoder hidden state is extracted from the last decoder layer. To get these hidden states. we force NMT models to generate the reference translations using constrained decoding (Post and Vilar, 2018). Since decoders are crucial in NMT, we assume that the decoder hidden states incorporate more relevant information for WSD from the decoder side. Thus, we hypothesize that using decoder hidden states can achieve better WSD performance. 2.2 Attention Distribution The attention weights can be viewed as the degree of contribution to the current word representation, which provides a way to interpret NMT models. Tang et al. (2018a) have shown that Transformers with self-attention are better at WSD than RNNs. However, the working mechanism of self-attention has not been explored. We try to use the"
D19-1149,I17-1004,0,0.135022,"hidden states at different layers. We extract encoder hidden states from both RNNbased (RNNS2S) (Luong et al., 2015) and Transformer (Vaswani et al., 2017) models. Belinkov et al. (2017a,b) have shown that the higher layers are better at learning semantics. We hypothesize that the hidden states in higher layers incorporate more relevant information for WSD than those in lower layers. In addition to encoders, we also probe how much do decoder hidden states contribute to the WSD classification task. Recently, the distributions of attention mechanisms have been used for interpreting NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018b; Voita et al., 2019; Tang et al., 2019). We further investigate the attention weights and attention entropy of self-attention in encoders to explore how self-attention incorporates relevant information for WSD into hidden states. As sentential information is helpful in disambiguating ambiguous words, we hypothesize that self1429 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1429–1435, c Hong Kong, China, November 3–7, 2019. 2019 Asso"
D19-1149,W17-4702,1,0.83005,"dden states in higher encoder layers do not perform disambiguation better than those in lower layers and conclude that encoders do not encode enough relevant context for disambiguation. However, their results are based on small data sets, and we wish to revisit this question with larger-scale data sets. Tang et al. (2018b) speculate that encoders have encoded the relevant information for WSD into hidden states before decoding but without any experimental tests. In this paper, we first train a classifier for WSD, on a much larger data set than Marvin and Koehn (2018), extracted from ContraWSD (Rios et al., 2017), for both German→English (DE→EN) and German→French (DE→FR). The classifier is fed a representation of ambiguous nouns and a word sense (represented as the embedding of a translation candidate), and has to predict whether the two match. We can learn the role that encoders play in encoding information relevant for WSD by comparing different representations: word embeddings and encoder hidden states at different layers. We extract encoder hidden states from both RNNbased (RNNS2S) (Luong et al., 2015) and Transformer (Vaswani et al., 2017) models. Belinkov et al. (2017a,b) have shown that the hig"
D19-1149,E17-3017,1,0.827005,"e attention distributions of ambiguous nouns to nouns in gen1430 eral1 in two respects. One is the attention weight over the word itself. The other one is the concentration of attention distributions. We use attention entropy (Ghader and Monz, 2017) to measure the concentration. EAt (xt ) = − |x| X examples are used for training. We train 10 times with different seeds for each classifier and apply average accuracy. Table 1 lists the detailed statistics of the data. More experimental details are provided in the Appendix. At(xi , xt ) log At(xi , xt ) (1) i=1 For NMT models, we use the Sockeye (Hieber et al., 2017) toolkit to train RNNS2Ss and Transformers. DE→EN training data is from the WMT17 shared task (Bojar et al., 2017). DE→FR training data is from Europarl (v7) (Koehn, 2005) and News Commentary (v11) cleaned by Rios et al. (2017).3 In ContraWSD, each ambiguous noun has a small number of translation candidates. The average number of word senses per noun is 2.4 and 2.3 in DE→EN and DE→FR, respectively. We generate instances that are labelled with one candidate and a binary value indicating whether it corresponds to the correct sense. we get 50,792 and 43,268 instances in DE→EN and DE→FR, respectiv"
D19-1149,D13-1176,0,0.086378,"ain a classifier to predict whether a translation is correct given the representation of an ambiguous noun. We find that encoder hidden states outperform word embeddings significantly which indicates that encoders adequately encode relevant information for disambiguation into hidden states. In contrast to encoders, the effect of decoder is different in models with different architectures. Moreover, the attention weights and attention entropy show that self-attention can detect ambiguous nouns and distribute more attention to the context. 1 Introduction Neural machine translation (NMT) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) have access to the whole source sentence for the prediction of each word, which intuitively allows them to perform word sense disambiguation (WSD) better than previous phrase-based methods, and Rios et al. (2018) have confirmed this empirically. However, it is still unclear which component dominates the ability to disambiguate word senses. We explore the ability of NMT encoders and decoders to disambiguate word senses by evaluating hidden states and investigating the self-attention distributions. Marvin and"
D19-1149,W18-6437,1,0.905897,"Missing"
D19-1149,P16-1162,1,0.680945,"Missing"
D19-1149,D18-1458,1,0.891786,"Missing"
D19-1149,W18-6304,1,0.867723,"owever, it is still unclear which component dominates the ability to disambiguate word senses. We explore the ability of NMT encoders and decoders to disambiguate word senses by evaluating hidden states and investigating the self-attention distributions. Marvin and Koehn (2018) find that the hidden states in higher encoder layers do not perform disambiguation better than those in lower layers and conclude that encoders do not encode enough relevant context for disambiguation. However, their results are based on small data sets, and we wish to revisit this question with larger-scale data sets. Tang et al. (2018b) speculate that encoders have encoded the relevant information for WSD into hidden states before decoding but without any experimental tests. In this paper, we first train a classifier for WSD, on a much larger data set than Marvin and Koehn (2018), extracted from ContraWSD (Rios et al., 2017), for both German→English (DE→EN) and German→French (DE→FR). The classifier is fed a representation of ambiguous nouns and a word sense (represented as the embedding of a translation candidate), and has to predict whether the two match. We can learn the role that encoders play in encoding information re"
D19-1149,R19-1136,1,0.828719,"Missing"
D19-1149,P18-1117,1,0.760909,"rent layers. We extract encoder hidden states from both RNNbased (RNNS2S) (Luong et al., 2015) and Transformer (Vaswani et al., 2017) models. Belinkov et al. (2017a,b) have shown that the higher layers are better at learning semantics. We hypothesize that the hidden states in higher layers incorporate more relevant information for WSD than those in lower layers. In addition to encoders, we also probe how much do decoder hidden states contribute to the WSD classification task. Recently, the distributions of attention mechanisms have been used for interpreting NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018b; Voita et al., 2019; Tang et al., 2019). We further investigate the attention weights and attention entropy of self-attention in encoders to explore how self-attention incorporates relevant information for WSD into hidden states. As sentential information is helpful in disambiguating ambiguous words, we hypothesize that self1429 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1429–1435, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computat"
D19-1149,P19-1580,1,0.860242,"tates from both RNNbased (RNNS2S) (Luong et al., 2015) and Transformer (Vaswani et al., 2017) models. Belinkov et al. (2017a,b) have shown that the higher layers are better at learning semantics. We hypothesize that the hidden states in higher layers incorporate more relevant information for WSD than those in lower layers. In addition to encoders, we also probe how much do decoder hidden states contribute to the WSD classification task. Recently, the distributions of attention mechanisms have been used for interpreting NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018b; Voita et al., 2019; Tang et al., 2019). We further investigate the attention weights and attention entropy of self-attention in encoders to explore how self-attention incorporates relevant information for WSD into hidden states. As sentential information is helpful in disambiguating ambiguous words, we hypothesize that self1429 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1429–1435, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Correct or Incorrect a"
D19-1149,W17-4717,0,\N,Missing
D19-1448,P17-1080,0,0.0460681,"gure 11c) parts of a sentence. It can be clearly seen that LM first accumulates information about the left part of CCG, understanding the syntactic structure of the past. Then this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and"
D19-1448,I17-1001,0,0.0439048,"gure 11c) parts of a sentence. It can be clearly seen that LM first accumulates information about the left part of CCG, understanding the syntactic structure of the past. Then this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and"
D19-1448,D18-1313,0,0.0516911,"arly seen that LM first accumulates information about the left part of CCG, understanding the syntactic structure of the past. Then this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coefficients are used to estimate"
D19-1448,P18-2003,0,0.133599,"at the top MLM layers. 1 Introduction Deep (i.e. multi-layered) neural networks have become the standard approach for many natural language processing (NLP) tasks, and their analysis has been an active topic of research. One popular approach for analyzing representations of neural models is to evaluate how informative they are for various linguistic tasks, so-called “probing tasks”. Previous work has made some interesting observations regarding these representations; for example, Zhang and Bowman (2018) show that untrained LSTMs outperform trained ones on a word identity prediction task; and Blevins et al. (2018) show that up to a certain layer performance of representations obtained from a deep LM improves on a constituent labeling task, but then decreases, while with representations obtained from an MT encoder performance continues to improve up to the highest layer. These observations have, however, been somewhat anecdotal and an explanation of the process behind such behavior has been lacking. In this paper, we attempt to explain more generally why such behavior is observed. Rather than measuring the quality of representations obtained from a particular model on some auxiliary task, we characteriz"
D19-1448,I17-1015,0,0.0126937,"tence. It can be clearly seen that LM first accumulates information about the left part of CCG, understanding the syntactic structure of the past. Then this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coeffici"
D19-1448,P14-5010,0,0.00471906,"Missing"
D19-1448,N18-1202,0,0.0802825,"vels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coefficients are used to estimate the contribution of different layers to a particular task. Figure 12: t-SNE of different occurrences of the token “is”, CCG tag is in color (intensity of a color is a token position). On the x-axis are layers. line of research by analyzing how word representations evolve between layers and gives insights into how models trained on different tasks come to represent different information. Canonical correlation analysis has been previously used to investigate learning dynamics of"
D19-1448,W18-5431,0,0.0280036,"this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coefficients are used to estimate the contribution of different layers to a particular task. Figure 12: t-SNE of different occurrences of the token “is”, CCG tag is in c"
D19-1448,N19-1329,0,0.0659016,"On the x-axis are layers. line of research by analyzing how word representations evolve between layers and gives insights into how models trained on different tasks come to represent different information. Canonical correlation analysis has been previously used to investigate learning dynamics of CNNs and RNNs, to measure the intrinsic dimensionality of layers in CNNs and compare representations of networks which memorize and generalize (Raghu et al., 2017; Morcos et al., 2018). Bau et al. (2019) used SVCCA as one of the methods used for identifying important individual neurons in NMT models. Saphra and Lopez (2019) used SVCCA to investigate how representations of linguistic structure are learned over time in LMs. 8 Conclusions In this work, we analyze how the learning objective determines the information flow in the model. We propose to view the evolution of a token representation between layers from the compression/prediction trade-off perspective. We conduct a series of experiments supporting this view and propose a possible explanation for superior performance of MLM over LM for pretraining. We relate our findings to observations previously made in the context of probing tasks. Acknowledgments We wou"
D19-1448,P16-1162,1,0.356275,"Missing"
D19-1448,D16-1159,0,0.0223667,"es information about the left part of CCG, understanding the syntactic structure of the past. Then this information gets dismissed while forming information about future. Figure 12 shows how representations of different occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coefficients are used to estimate the contribution of diffe"
D19-1448,P17-1026,0,0.0240237,"ifferent occurrences of the token “is” get reordered in the space according to CCG tags (colors correspond to tags). 7 Additional related work Previous work analyzed representations of MT and/or LM models by using probing tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016; Tenney et al., 2019b) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018; Tenney et al., 2019b). Our work complements this 5 To derive CCG supertags, we use Yoshikawa et al. (2017) tagger, the latest version with ELMO: https://github. com/masashi-y/depccg. 6 In their experiments, representations are pooled across layers with the scalar mixing technique similar to the one used in the ELMO model (Peters et al., 2018). The probing classifier is trained jointly with the mixing weights, and the learned coefficients are used to estimate the contribution of different layers to a particular task. Figure 12: t-SNE of different occurrences of the token “is”, CCG tag is in color (intensity of a color is a token position). On the x-axis are layers. line of research by analyzing how"
D19-1448,W18-5448,0,0.173809,"g the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers. 1 Introduction Deep (i.e. multi-layered) neural networks have become the standard approach for many natural language processing (NLP) tasks, and their analysis has been an active topic of research. One popular approach for analyzing representations of neural models is to evaluate how informative they are for various linguistic tasks, so-called “probing tasks”. Previous work has made some interesting observations regarding these representations; for example, Zhang and Bowman (2018) show that untrained LSTMs outperform trained ones on a word identity prediction task; and Blevins et al. (2018) show that up to a certain layer performance of representations obtained from a deep LM improves on a constituent labeling task, but then decreases, while with representations obtained from an MT encoder performance continues to improve up to the highest layer. These observations have, however, been somewhat anecdotal and an explanation of the process behind such behavior has been lacking. In this paper, we attempt to explain more generally why such behavior is observed. Rather than"
D19-1448,P19-1452,0,0.430652,"re we primarily want to understand what representation in each layer ‘focuses’ on. We evaluate to what extent a certain property is important for defining a token representation at each layer by (1) selecting a large number of token occurrences and taking their representations; (2) validating if a value of the property is the same for token occurrences corresponding to the closest representations. Though our approach is different from probing tasks, we choose the properties which will enable us to relate to other works reporting similar behaviour (Zhang and Bowman, 2018; Blevins et al., 2018; Tenney et al., 2019a). The properties we consider are token identity, position in a sentence, neighboring tokens and CCG supertags. 6.1 Methodology For our analysis, we take 100 random word types from the top 5,000 in our vocabulary. For each word type, we gather 1,000 different occurrences along with the representations from all three models. For each representation, we take the closest neighbors among representations at each layer and evaluate the percentage of neighbors with the same value of the property. 6.2 Preserving token identity and position In this section, we track the loss of information about token"
D19-1448,P19-1580,1,0.84886,"ers. In contrast, rare tokens are not the most influencing ones at the lower layers of MLM. We hypothesize that the training procedure of MLM, with masking out some tokens or replacing them with random ones, teaches the model not to over-rely on these tokens before their context is well understood. To test our hypothesis, we additionally trained MT and LM models with token dropout on the input side (Figure 6). As we expected, there is no extreme influence of rare tokens when using this regularization, supporting the above interpretation. Interestingly, our earlier study of the MT Transformer (Voita et al., 2019) shows how this influence of rare tokens is implemented by the model. In that work, we observed that, for any considered language pair, there is one dedicated attention head in the first encoder layer which tends to point to the least frequent tokens in every sentence. The above analysis suggest that this phenomenon is likely due to overfitting. We also analyzed the extent of change and influence splitting tokens according to their part of speech; see appendix for details. 6 What does a layer represent? Whereas in the previous section we were interested in quantifying the amount of information"
E12-1055,D11-1033,0,0.470556,"subdomains. 3 We can ignore the fifth feature, the phrase penalty, which is a constant. dividual model probabilities. It is defined as follows: p(x|y; λ) = n X λi pi (x|y) (1) i=1 with λi being the P interpolation weight of each model i, and with ( i λi ) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of B LEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and perform optimization through MERT. Monolingual optimization objecti"
E12-1055,W07-0702,0,0.0116063,"settings. However, researchers who demonstrated this fact did so with arbitrary weights (e.g. (Koehn, 2002)), or by empirically testing different weights (e.g. (Nakov and Ng, 2009)). We do not know of any research on automatically determining weights for this method, or which is not limited to two corpora. 4 For instance if the word pairs (the,der) and (man,Mann) are known, but the phrase pair (the man, der Mann) is not. 5 P Unlike equation 1, equation 3 does not require that ( i λi ) = 1. 2.3 Alternative Paths A third method is using multiple translation models as alternative decoding paths (Birch et al., 2007), an idea which Koehn and Schroeder (2007) first used for domain adaptation. This approach has the attractive theoretical property that adding new models is guaranteed to lead to equal or better performance, given the right weights. At best, a model is beneficial with appropriate weights. At worst, we can set the feature weights so that the decoding paths of one model are never picked for the final translation. In practice, each translation model adds 5 features and thus 5 more dimensions to the weight space, which leads to longer search, search errors, and/or overfitting. The expectation is t"
E12-1055,J93-2003,0,0.0281509,"Missing"
E12-1055,2011.eamt-1.5,0,0.0309033,"Missing"
E12-1055,P11-2031,0,0.0539653,"closely tailored to translation modelling outperform a naive implementation? 4.1 Data and Methods In terms of tools and techniques used, we mostly adhere to the work flow described for the WMT 2011 baseline system9 . The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. We report two translation measures: B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al., 2011) for resampling and significance testing. We compare three baselines and four translation model mixture techniques. The three baselines are a purely in-domain model, a purely outof-domain model, and a model trained on the concatenation of the two, which corresponds to equation 3 with uniform weights. Additionally, we evaluate perplexity optimization with weighted counts and the two implementations of linear interpolation contrasted in section 2.1. The two linear interpolations that are contrasted are a naive one, i.e. a direct, unnormalized interpolation of 9 http://www.statmt.org/wmt11/baseli"
E12-1055,P07-1092,0,0.012923,"¸su et al. (2011) that patent translation systems are highly domain-sensitive and suffer from the inclusion of parallel training data from other patent subdomains. 3 We can ignore the fifth feature, the phrase penalty, which is a constant. dividual model probabilities. It is defined as follows: p(x|y; λ) = n X λi pi (x|y) (1) i=1 with λi being the P interpolation weight of each model i, and with ( i λi ) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of B LEU scores on a development set is promising, but slow and impractical. There"
E12-1055,W11-2107,0,0.0217929,"ntroduced in section 2, we want to investigate the following open research questions. 1. Does an implementation of linear interpolation that is more closely tailored to translation modelling outperform a naive implementation? 4.1 Data and Methods In terms of tools and techniques used, we mostly adhere to the work flow described for the WMT 2011 baseline system9 . The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. We report two translation measures: B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al., 2011) for resampling and significance testing. We compare three baselines and four translation model mixture techniques. The three baselines are a purely in-domain model, a purely outof-domain model, and a model trained on the concatenation of the two, which corresponds to equation 3 with uniform weights. Additionally, we evaluate perplexity optimization with weighted counts and the two implementations of linear interpolation contrasted in section 2.1. The two"
E12-1055,W08-0334,0,0.0677494,"er from the inclusion of parallel training data from other patent subdomains. 3 We can ignore the fifth feature, the phrase penalty, which is a constant. dividual model probabilities. It is defined as follows: p(x|y; λ) = n X λi pi (x|y) (1) i=1 with λi being the P interpolation weight of each model i, and with ( i λi ) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of B LEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and per"
E12-1055,W07-0717,0,0.91509,"model probabilities. It is defined as follows: p(x|y; λ) = n X λi pi (x|y) (1) i=1 with λi being the P interpolation weight of each model i, and with ( i λi ) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of B LEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and perform optimization through MERT. Monolingual optimization objectives such as language model perplexity have the advantage of being well-known and readily availabl"
E12-1055,D10-1044,0,0.707487,"Missing"
E12-1055,W01-0504,0,0.0669454,"determining weights is not adequately resolved. The picture looks better in language modelling, where model interpolation through perplexity minimization has become a widespread method of domain adaptation. We investigate the applicability of this method for translation models, and discuss possible applications. Domain Adaptation for Translation Models To motivate efforts in domain adaptation, let us review why additional training data can improve, but also decrease translation quality. Adding more training data to a translation system is easy to motivate through the data sparseness problem. Koehn and Knight (2001) show that translation quality correlates strongly with how often a word occurs in the training corpus. Rare words or phrases pose a problem in several stages of MT modelling, from word alignment to the computation of translation probabilities through Maximum Likelihood Estimation. Unknown words are typically copied verbatim to the target text, which may be a good strategy for named entities, but is often wrong otherwise. In general, more data allows for a better word alignment, a better estimation of translation probabilities, and for the consideration of more context (in phrase-based or synt"
E12-1055,W07-0733,0,0.747127,"demonstrated this fact did so with arbitrary weights (e.g. (Koehn, 2002)), or by empirically testing different weights (e.g. (Nakov and Ng, 2009)). We do not know of any research on automatically determining weights for this method, or which is not limited to two corpora. 4 For instance if the word pairs (the,der) and (man,Mann) are known, but the phrase pair (the man, der Mann) is not. 5 P Unlike equation 1, equation 3 does not require that ( i λi ) = 1. 2.3 Alternative Paths A third method is using multiple translation models as alternative decoding paths (Birch et al., 2007), an idea which Koehn and Schroeder (2007) first used for domain adaptation. This approach has the attractive theoretical property that adding new models is guaranteed to lead to equal or better performance, given the right weights. At best, a model is beneficial with appropriate weights. At worst, we can set the feature weights so that the decoding paths of one model are never picked for the final translation. In practice, each translation model adds 5 features and thus 5 more dimensions to the weight space, which leads to longer search, search errors, and/or overfitting. The expectation is that, at least with MERT, using alternative"
E12-1055,N03-1017,0,0.0080215,"vector λ to 1. We do this for p(t|s) and lex(t|s), but not for p(s|t) and lex(s|t), the reasoning being the con540 sequences for perplexity minimization (see section 2.4). Namely, we do not want to penalize a small in-domain model for having a high outof-vocabulary rate on the source side, but we do want to penalize models that know the source phrase, but not its correct translation. A second modification pertains to the lexical weights lex(s|t) and lex(t|s), which form no true probability distribution, but are derived from the individual word translation probabilities of a phrase pair (see (Koehn et al., 2003)). We propose to not interpolate the features directly, but the word translation probabilities which are the basis of the lexical weight computation. The reason for this is that word pairs are less sparse than phrase pairs, so that we can even compute lexical weights for phrase pairs which are unknown in a model.4 2.2 Weighted Counts Weighting of different corpora can also be implemented through a modified Maximum Likelihood Estimation. The traditional equation for MLE is: p(x|y) = c(x, y) c(x, y) =P 0 c(y) x0 c(x , y) (2) where c denotes the count of an observation, and p the model probabilit"
E12-1055,J82-2005,0,0.816373,"Missing"
E12-1055,W10-1715,0,0.01413,"t is defined as follows: p(x|y; λ) = n X λi pi (x|y) (1) i=1 with λi being the P interpolation weight of each model i, and with ( i λi ) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of B LEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and perform optimization through MERT. Monolingual optimization objectives such as language model perplexity have the advantage of being well-known and readily available, but their relation"
E12-1055,2005.mtsummit-papers.11,0,0.275341,"y ambiguous, and a strong source of ambiguity is the 1 We borrow this term from early evolutionary biology to emphasize that the question in domain adaptation is not how “good” or “bad” the data is, but how well-adapted it is to the task at hand. 539 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 539–549, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics domain of a text. The German word “Wort” (engl. word) is typically translated as floor in Europarl, a corpus of Parliamentary Proceedings (Koehn, 2005), owing to the high frequency of phrases such as you have the floor, which is translated into German as Sie haben das Wort. This translation is highly idiomatic and unlikely to occur in other contexts. Still, adding Europarl as out-of-domain training data shifts the probability distribution of p(t|“Wort”) in favour of p(“floor”|“Wort”), and may thus lead to improper translations. We will refer to the two problems as the data sparseness problem and the ambiguity problem. Adding out-of-domain data typically mitigates the data sparseness problem, but exacerbates the ambiguity problem. The net gai"
E12-1055,D09-1074,0,0.535907,"is considered more relevant for the task at hand. This has been done for language models using techniques from information retrieval (Zhao et al., 2004), or perplexity (Lin et al., 1997; Moore and Lewis, 2010). Data selection has also been proposed for translation models (Axelrod et al., 2011). Note that for translation models, data selection offers an unattractive trade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine. Our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models being combined. Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by weighting individual phrase pairs. These more fine-grained methods need not be seen as alternatives to coarse-grained ones. Foster et al. (2010) combine the two, applying linear interpolation to combine the instance542 Data set Alpine (in-domain) Europarl JRC Acquis OpenSubtitles v2 Total train Dev Test weighted out-of-domain model with an in-domain model. 4 Evaluation Apart from measuring the performance of the approaches introduced in section 2, we want to investig"
E12-1055,P10-2041,0,0.066268,"translations. This means that LM adaptation may have similar effects as TM adaptation, and that the two are to some extent redundant. Foster and Kuhn (2007) find that “both TM and LM adaptation are effective”, but that “combined LM and TM adaptation is not better than LM adaptation on its own”. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. This has been done for language models using techniques from information retrieval (Zhao et al., 2004), or perplexity (Lin et al., 1997; Moore and Lewis, 2010). Data selection has also been proposed for translation models (Axelrod et al., 2011). Note that for translation models, data selection offers an unattractive trade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine. Our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models being combined. Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by weighting individual phrase pairs. These more fine-graine"
E12-1055,D09-1141,0,0.138655,"ta from other patent subdomains. 3 We can ignore the fifth feature, the phrase penalty, which is a constant. dividual model probabilities. It is defined as follows: p(x|y; λ) = n X λi pi (x|y) (1) i=1 with λi being the P interpolation weight of each model i, and with ( i λi ) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of B LEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and perform optimization through MERT. Monolingu"
E12-1055,J03-1002,0,0.00885087,"p) is defined as:6 6 See (Chen and Goodman, 1998) for a short discussion of the equation. In short, a lower cross-entropy indicates that the model is better able to predict the development set. 541 H(p) = − X p˜(x, y) log2 p(x|y) (4) x,y The phrase pairs (x, y) whose probability we measure, and their empirical probability p˜ need to be extracted from a development set, whereas p is the model probability. To obtain the phrase pairs, we process the development set with the same word alignment and phrase extraction tools that we use for training, i.e. GIZA++ and heuristics for phrase extraction (Och and Ney, 2003). The objective function is the minimization of the cross-entropy, with the weight vector λ as argument: X ˆ = arg min − p˜(x, y) log2 p(x|y; λ) (5) λ λ x,y We can fill in equations 1 or 3 for p(x|y; λ). The optimization itself is convex and can be done with off-the-shelf software.7 We use L-BFGS with numerically approximated gradients (Byrd et al., 1995). Perplexity minimization has the advantage that it is well-defined for both weighted counts and linear interpolation, and can be quickly computed. Other than in language modelling, where p(x|y) is the probability of a word given a n-gram hist"
E12-1055,P02-1040,0,0.105675,"ing the performance of the approaches introduced in section 2, we want to investigate the following open research questions. 1. Does an implementation of linear interpolation that is more closely tailored to translation modelling outperform a naive implementation? 4.1 Data and Methods In terms of tools and techniques used, we mostly adhere to the work flow described for the WMT 2011 baseline system9 . The main tools are Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003), with settings as described in the WMT 2011 guide. We report two translation measures: B LEU (Papineni et al., 2002) and METEOR 1.3 (Denkowski and Lavie, 2011). All results are lowercased and tokenized, measured with five independent runs of MERT (Och and Ney, 2003) and MultEval (Clark et al., 2011) for resampling and significance testing. We compare three baselines and four translation model mixture techniques. The three baselines are a purely in-domain model, a purely outof-domain model, and a model trained on the concatenation of the two, which corresponds to equation 3 with uniform weights. Additionally, we evaluate perplexity optimization with weighted counts and the two implementations of linear inter"
E12-1055,steinberger-etal-2006-jrc,0,0.0981994,"Missing"
E12-1055,volk-etal-2010-challenges,0,0.0270733,"Missing"
E12-1055,I08-2088,0,0.0392032,"parallel training data from other patent subdomains. 3 We can ignore the fifth feature, the phrase penalty, which is a constant. dividual model probabilities. It is defined as follows: p(x|y; λ) = n X λi pi (x|y) (1) i=1 with λi being the P interpolation weight of each model i, and with ( i λi ) = 1. For SMT, linear interpolation of translation models has been used in numerous systems. The approaches diverge in how they set the interpolation weights. Some authors use uniform weights (Cohn and Lapata, 2007), others empirically test different interpolation coefficients (Finch and Sumita, 2008; Yasuda et al., 2008; Nakov and Ng, 2009; Axelrod et al., 2011), others apply monolingual metrics to set the weights for TM interpolation (Foster and Kuhn, 2007; Koehn et al., 2010). There are reasons against all these approaches. Uniform weights are easy to implement, but give little control. Empirically, it has been shown that they often do not perform optimally (Finch and Sumita, 2008; Yasuda et al., 2008). An optimization of B LEU scores on a development set is promising, but slow and impractical. There is no easy way to integrate linear interpolation into loglinear SMT frameworks and perform optimization thr"
E12-1055,C04-1059,0,0.0658585,"the probability distribution in favour of in-domain translations. This means that LM adaptation may have similar effects as TM adaptation, and that the two are to some extent redundant. Foster and Kuhn (2007) find that “both TM and LM adaptation are effective”, but that “combined LM and TM adaptation is not better than LM adaptation on its own”. A second strand of research in domain adaptation is data selection, i.e. choosing a subset of the training data that is considered more relevant for the task at hand. This has been done for language models using techniques from information retrieval (Zhao et al., 2004), or perplexity (Lin et al., 1997; Moore and Lewis, 2010). Data selection has also been proposed for translation models (Axelrod et al., 2011). Note that for translation models, data selection offers an unattractive trade-off between the data sparseness and the ambiguity problem, and that the optimal amount of data to select is hard to determine. Our discussion of mixture-modelling is relatively coarse-grained, with 2-10 models being combined. Matsoukas et al. (2009) propose an approach where each sentence is weighted according to a classifier, and Foster et al. (2010) extend this approach by"
E12-1055,P07-2045,0,\N,Missing
E17-1083,P05-1074,0,0.835722,"with methods varying according to the type of paraphrase being induced (lexical or structural), the type of data used (e.g., monolingual or parallel corpus), the underlying representation (surface form or syntax trees), and the acquisition method itself. For an overview of these issues we refer the interested reader to Madnani and Dorr (2010). We focus on bilingual pivoting methods and aspects of neural machine translation pertaining to our model. We also discuss related work on paraphrastic embeddings. Bilingual Pivoting Paraphrase extraction using bilingual parallel corpora was proposed by Bannard and Callison-Burch (2005). Their method first extracts a bilingual phrase table and then obtains English paraphrases by pivoting through foreign language phrases. Paraphrases for a given phrase are ranked using a paraphrase probability defined in terms of the translation model probabilities P( f |e) and P(e |f ) where f and e are the foreign and English strings, respectively. Motivated by the wish to model sentential paraphrases, follow-up work focused on syntaxdriven techniques again within the bilingual pivoting framework. Extensions include representing paraphrases via rules obtained from a synchronous context free"
E17-1083,P14-1133,0,0.0609363,"wider context compared to phrase-based approaches: target paraphrases are predicted based on the meaning of the source input and all previously generated target words. Introduction Paraphrasing can be broadly described as the task of using an alternative surface form to express the same semantic content (Madnani and Dorr, 2010). Much of the appeal of paraphrasing stems from its potential application to a wider range of NLP problems. Examples include query and pattern expansion (Riezler et al., 2007), summarization (Barzilay, 2003), question answering (Lin and Pantel, 2001), semantic parsing (Berant and Liang, 2014), semantic role labeling (Woodsend and Lapata, 2014), and machine translation (CallisonBurch et al., 2006). Most of the recent literature has focused on the automatic extraction of paraphrases from various different types of corpora consisting of parallel, non-parallel, and comparable texts. One of the most successful proposals uses bilingual parallel corpora to induce paraphrases based on techniques from phrase-based statistical machine translation (SMT, Koehn et al. (2003)). The intuition behind In the remainder of the paper, we introduce our 881 Proceedings of the 15th Conference of the Eur"
E17-1083,D16-1026,0,0.261996,"quence into a list of continuous-space representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2014) is used to generate the region of focus during decoding. We employ NMT as the backbone of our paraphrasing model. In its simplest form our model exploits a one-to-one NMT architecture: the source English sentence is translated into k candidate foreign sentences and then back-translated into English. Inspired by multi-way machine translation which has shown performance gains over single-pair models (Zoph and Knight, 2016; Dong et al., 2015; Firat et al., 2016a), we also explore an alternative pivoting technique which uses multiple languages rather than a single one. Our model inherits advantages from NMT such as a small memory footprint and conceptually easy decoding (implemented as beam search). Beyond paraphrase generation, we experimentally show that the representations learned by our model are useful in semantic relatedness tasks. Related Work The literature on paraphrasing is vast with methods varying according to the type of paraphrase being induced (lexical or structural), the type of data used (e.g., monolingual or parallel corpus), the un"
E17-1083,ganitkevitch-callison-burch-2014-multilingual,0,0.0321136,"f Edinburgh 10 Crichton Street, Edinburgh EH8 9AB J.Mallinson@ed.ac.uk, {rsennric,mlap}@inf.ed.ac.uk Abstract Bannard and Callison-Burch’s (2005) bilingual pivoting method is that two English strings e1 and e2 that translate to the same foreign string f can be assumed to have the same meaning. The method then pivots over f to extract he1 , e2 i as a pair of paraphrases. Drawing inspiration from syntaxbased SMT, several subsequent efforts (CallisonBurch, 2008; Ganitkevitch et al., 2011) extended this technique to syntactic paraphrases leading to the creation of PPDB (Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014), a largescale paraphrase database containing over a billion of paraphrase pairs in 23 different languages. Recognizing and generating paraphrases is an important component in many natural language processing applications. A wellestablished technique for automatically extracting paraphrases leverages bilingual corpora to find meaning-equivalent phrases in a single language by “pivoting” over a shared translation in another language. In this paper we revisit bilingual pivoting in the context of neural machine translation and present a paraphrasing model based purely on neural networks. Our mode"
E17-1083,N06-1003,0,0.0207028,"Missing"
E17-1083,D11-1108,0,0.0423867,"Missing"
E17-1083,D08-1021,0,0.0310719,"hrases for a given phrase are ranked using a paraphrase probability defined in terms of the translation model probabilities P( f |e) and P(e |f ) where f and e are the foreign and English strings, respectively. Motivated by the wish to model sentential paraphrases, follow-up work focused on syntaxdriven techniques again within the bilingual pivoting framework. Extensions include representing paraphrases via rules obtained from a synchronous context free grammar (Ganitkevitch et al., 2011; Madnani et al., 2007) as well as labeling paraphrases with linguistic annotations such as CCG categories (Callison-Burch, 2008) and partof-speech tags (Zhao et al., 2008). In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar. We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table. Paraphrastic Embeddings The successful use of word embeddings in various NLP tasks has provided further impetus to use paraphrases. Wieting et al. (2015) take the paraphrases contained in PPDB and embed them into a low-dimensional space using a recursive neural network similar to"
E17-1083,N13-1092,0,0.44047,"Missing"
E17-1083,P15-1166,0,0.00840479,"reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2014) is used to generate the region of focus during decoding. We employ NMT as the backbone of our paraphrasing model. In its simplest form our model exploits a one-to-one NMT architecture: the source English sentence is translated into k candidate foreign sentences and then back-translated into English. Inspired by multi-way machine translation which has shown performance gains over single-pair models (Zoph and Knight, 2016; Dong et al., 2015; Firat et al., 2016a), we also explore an alternative pivoting technique which uses multiple languages rather than a single one. Our model inherits advantages from NMT such as a small memory footprint and conceptually easy decoding (implemented as beam search). Beyond paraphrase generation, we experimentally show that the representations learned by our model are useful in semantic relatedness tasks. Related Work The literature on paraphrasing is vast with methods varying according to the type of paraphrase being induced (lexical or structural), the type of data used (e.g., monolingual or para"
E17-1083,P11-1105,0,0.0088428,"onal resources or parameter estimation. It also learns phrase and sentence embeddings for free without any model adjustments or recourse to resources like PPDB. 4 4.2 Throughout our experiments we compare PARA N ET against a paraphrase model trained with a commonly used Statistical Machine Translation system (SMT), which we henceforth refer to as PARA S TAT. Specifically, for each language pair used, an equivalent IBM Model 4 phrase-based translation model was trained. Additionally, an Operation Sequence Model (OSM) was included, which has been shown to improve the performance of SMT systems (Durrani et al., 2011). SMT translation models were implemented using both GIZA++ (Och and Ney, 2003) and MOSES (Koehn et al., 2007) and were trained using the same pre-processed bilingual data provided to the NMT systems. The SMT systems used a KenLM 5-gram language model (Heafield, 2011), trained on the mono-lingual data from WMT 2015. For all languages pairs, both KenLM and MOSES were trained using the standard settings. BLEU scores for the SMT systems are given in Table 1. Under the SMT models, paraphrase probabilities were calculated analogously to Equation (7): Experiments We evaluated PARA N ET in several wa"
E17-1083,S12-1086,0,0.0277751,"task: a random baseline, a logistic regression baseline with minimal Model fr de cz de, fr de, cz fr, cz fr, cz, de random WTMF logistic reg ASOBEK MITRE PARA S TAT PARA N ET 0.540 0.569 0.543 0.571 0.547 0.569 0.543 0.569 0.540 0.570 0.546 0.568 0.539 0.568 0.017 0.350 0.511 0.475 0.619 Table 4: Semantic similarity results (Pearson) on the PIT-2015 data set. Boldface indicates the best performing paraphrasing model. n-gram word overlap features; and a model which uses weighted matrix factorization (WTMF) and has access to dictionary definitions provided in WordNet, OntoNotes, and Wiktionary (Guo and Diab, 2012). The last two rows show the highest scoring systems: ASOBEK (Eyecioglu and Keller, 2015) ranked 1st in the identification subtask and MITRE (Zarrella et al., 2015) in the similarity subtask. Whereas ASOBEK uses knowledge-lean features based on word and character n-gram overlap, MITRE is a combination of multiple systems including mixtures of string matching metrics, alignments using tweet-specific word representations, and recurrent neural networks. As can be seen, PARA N ET achieves better similarity and detection score than all baselines and PARA S TAT, for any combinations of lan887 Model"
E17-1083,N13-1073,0,0.0153296,"iple sentences from one language, but also over multiple sentences from multiple languages. Multi-lingual pivoting has been recently shown to improve translation quality (Firat et al., 2016b), especially for low-resource language pairs. Here, we hypothesize that it will also lead to more accurate paraphrases. Multi-lingual pivoting requires a small extension to late-weighted combination. We illustrate with German as a second language. First, 884 Two men sailing in couple sailing in a a tiny vocabulary size as 10000 and 25 uni-gram translations, using a bilingual dictionary based on fastalign (Dyer et al., 2013). In our experiments, we used up to six encoder-decoder NMT models (three pairs); English→French, French→English, English→Czech, Czech→English, English→German, German→English. All systems were trained on the available training data from the WMT15 shared translation task (4.2 million, 15.7 million, and 39 million sentence pairs for EN↔DE, EN↔CS, and EN↔FR, respectively). For EN↔DE and EN→CS, we also had access to back-translated monolingual training data (Sennrich et al., 2016a), which we also used in training. The data was pre-processed using standard pre-processing scripts found in MOSES (Koe"
E17-1083,W11-2123,0,0.0135758,"tistical Machine Translation system (SMT), which we henceforth refer to as PARA S TAT. Specifically, for each language pair used, an equivalent IBM Model 4 phrase-based translation model was trained. Additionally, an Operation Sequence Model (OSM) was included, which has been shown to improve the performance of SMT systems (Durrani et al., 2011). SMT translation models were implemented using both GIZA++ (Och and Ney, 2003) and MOSES (Koehn et al., 2007) and were trained using the same pre-processed bilingual data provided to the NMT systems. The SMT systems used a KenLM 5-gram language model (Heafield, 2011), trained on the mono-lingual data from WMT 2015. For all languages pairs, both KenLM and MOSES were trained using the standard settings. BLEU scores for the SMT systems are given in Table 1. Under the SMT models, paraphrase probabilities were calculated analogously to Equation (7): Experiments We evaluated PARA N ET in several ways: (a) we examined whether the paraphrases learned by our model correlate with human judgments of paraphrase quality; (b) we assessed PARA N ET in paraphrase and similarity detection tasks; and (c) in a sentence-level paraphrase generation task. We first present deta"
E17-1083,S15-2011,0,0.0112022,"cz de, fr de, cz fr, cz fr, cz, de random WTMF logistic reg ASOBEK MITRE PARA S TAT PARA N ET 0.540 0.569 0.543 0.571 0.547 0.569 0.543 0.569 0.540 0.570 0.546 0.568 0.539 0.568 0.017 0.350 0.511 0.475 0.619 Table 4: Semantic similarity results (Pearson) on the PIT-2015 data set. Boldface indicates the best performing paraphrasing model. n-gram word overlap features; and a model which uses weighted matrix factorization (WTMF) and has access to dictionary definitions provided in WordNet, OntoNotes, and Wiktionary (Guo and Diab, 2012). The last two rows show the highest scoring systems: ASOBEK (Eyecioglu and Keller, 2015) ranked 1st in the identification subtask and MITRE (Zarrella et al., 2015) in the similarity subtask. Whereas ASOBEK uses knowledge-lean features based on word and character n-gram overlap, MITRE is a combination of multiple systems including mixtures of string matching metrics, alignments using tweet-specific word representations, and recurrent neural networks. As can be seen, PARA N ET achieves better similarity and detection score than all baselines and PARA S TAT, for any combinations of lan887 Model fr de cz de, fr de, cz fr, cz fr, cz, de Tokencos DLS@CU PARA S TAT PARA N ET 0.657 0.682"
E17-1083,P15-1001,0,0.173292,"ined with conventional phrase-based pivoting approaches. 1 In this paper we revisit the bilingual pivoting approach from the perspective of neural machine translation, a new approach to machine translation based purely on neural networks (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). At its core, NMT uses a deep neural network trained end-to-end to maximize the conditional probability of a correct translation given a source sentence, using a bilingual corpus. NMT models have obtained state-of-the art performance for several language pairs (Jean et al., 2015b; Luong et al., 2015), using only parallel data for training, and minimal linguistic information. In this paper we show how the bilingual pivoting method can be ported to NMT and argue that it offers at least three advantages over conventional methods. Firstly, our neural paraphrasing model learns continuous space representations for phrases and sentences (aka embeddings) that can be usefully incorporated in downstream tasks such as recognizing textual similarity and entailment. Secondly, the proposed model is able to either score a pair of paraphrase candidates (of arbitrary length) and gene"
E17-1083,P13-1158,0,0.0466156,"a) the Multiple-Translation Chinese (MTC) corpus (Huang et al., 2002) contains news stories from three sources of journalistic Mandarin Chinese text translated into English by 4 translation agencies; we sampled 1,000 sentences for training and testing, respectively (each source sentence had an average of 4 paraphrases); (b) the Jules Vernes Twenty Thousand Leagues Under the Sea novel (Leagues) corpus (Pang et al., 2003) contains two English translations of the French novel; we sampled 500 sentences for training/testing (each source sentence had one paraphrase); and (c) the Wikianswers corpus (Fader et al., 2013) which contains questions taken from the website3 wiki answers; we sampled 1,000 questions for training/testing (each question has on average 21 paraphrases). Semantic Textual Similarity In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets. We present results on the Semeval-2015 English subtask which contains sentences from a wide range of domains, including newswire headlines, image descriptions, and answers from Q&A websites. The training/test sets consist of 11,250 and 3,000 sentence pairs, respectively. Sentence pairs are rated on"
E17-1083,W15-3014,0,0.089871,"ined with conventional phrase-based pivoting approaches. 1 In this paper we revisit the bilingual pivoting approach from the perspective of neural machine translation, a new approach to machine translation based purely on neural networks (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). At its core, NMT uses a deep neural network trained end-to-end to maximize the conditional probability of a correct translation given a source sentence, using a bilingual corpus. NMT models have obtained state-of-the art performance for several language pairs (Jean et al., 2015b; Luong et al., 2015), using only parallel data for training, and minimal linguistic information. In this paper we show how the bilingual pivoting method can be ported to NMT and argue that it offers at least three advantages over conventional methods. Firstly, our neural paraphrasing model learns continuous space representations for phrases and sentences (aka embeddings) that can be usefully incorporated in downstream tasks such as recognizing textual similarity and entailment. Secondly, the proposed model is able to either score a pair of paraphrase candidates (of arbitrary length) and gene"
E17-1083,W07-0716,0,0.0285947,"gual phrase table and then obtains English paraphrases by pivoting through foreign language phrases. Paraphrases for a given phrase are ranked using a paraphrase probability defined in terms of the translation model probabilities P( f |e) and P(e |f ) where f and e are the foreign and English strings, respectively. Motivated by the wish to model sentential paraphrases, follow-up work focused on syntaxdriven techniques again within the bilingual pivoting framework. Extensions include representing paraphrases via rules obtained from a synchronous context free grammar (Ganitkevitch et al., 2011; Madnani et al., 2007) as well as labeling paraphrases with linguistic annotations such as CCG categories (Callison-Burch, 2008) and partof-speech tags (Zhao et al., 2008). In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar. We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table. Paraphrastic Embeddings The successful use of word embeddings in various NLP tasks has provided further impetus to use paraphrases. Wieting et al. (2015) take the paraphrases"
E17-1083,D13-1176,0,0.0421331,"present a paraphrasing model based purely on neural networks. Our model represents paraphrases in a continuous space, estimates the degree of semantic relatedness between text segments of arbitrary length, or generates candidate paraphrases for any source input. Experimental results across tasks and datasets show that neural paraphrases outperform those obtained with conventional phrase-based pivoting approaches. 1 In this paper we revisit the bilingual pivoting approach from the perspective of neural machine translation, a new approach to machine translation based purely on neural networks (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). At its core, NMT uses a deep neural network trained end-to-end to maximize the conditional probability of a correct translation given a source sentence, using a bilingual corpus. NMT models have obtained state-of-the art performance for several language pairs (Jean et al., 2015b; Luong et al., 2015), using only parallel data for training, and minimal linguistic information. In this paper we show how the bilingual pivoting method can be ported to NMT and argue that it offers at least three advantages over conventional methods"
E17-1083,J03-1002,0,0.0384719,"ngs for free without any model adjustments or recourse to resources like PPDB. 4 4.2 Throughout our experiments we compare PARA N ET against a paraphrase model trained with a commonly used Statistical Machine Translation system (SMT), which we henceforth refer to as PARA S TAT. Specifically, for each language pair used, an equivalent IBM Model 4 phrase-based translation model was trained. Additionally, an Operation Sequence Model (OSM) was included, which has been shown to improve the performance of SMT systems (Durrani et al., 2011). SMT translation models were implemented using both GIZA++ (Och and Ney, 2003) and MOSES (Koehn et al., 2007) and were trained using the same pre-processed bilingual data provided to the NMT systems. The SMT systems used a KenLM 5-gram language model (Heafield, 2011), trained on the mono-lingual data from WMT 2015. For all languages pairs, both KenLM and MOSES were trained using the standard settings. BLEU scores for the SMT systems are given in Table 1. Under the SMT models, paraphrase probabilities were calculated analogously to Equation (7): Experiments We evaluated PARA N ET in several ways: (a) we examined whether the paraphrases learned by our model correlate with"
E17-1083,N03-1024,0,0.546875,"Missing"
E17-1083,N03-1017,0,0.0605213,"(Riezler et al., 2007), summarization (Barzilay, 2003), question answering (Lin and Pantel, 2001), semantic parsing (Berant and Liang, 2014), semantic role labeling (Woodsend and Lapata, 2014), and machine translation (CallisonBurch et al., 2006). Most of the recent literature has focused on the automatic extraction of paraphrases from various different types of corpora consisting of parallel, non-parallel, and comparable texts. One of the most successful proposals uses bilingual parallel corpora to induce paraphrases based on techniques from phrase-based statistical machine translation (SMT, Koehn et al. (2003)). The intuition behind In the remainder of the paper, we introduce our 881 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 881–893, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics paraphrase model and experimentally compare it to the phrase-based pivoting approach. We evaluate the model’s paraphrasing capability both intrinsically in a paraphrase detection task (i.e., decide the degree of semantic similarity between two sentences) and extrinsically in a generation task."
E17-1083,P15-2070,0,0.00975573,"Missing"
E17-1083,P07-2045,0,0.0111244,"13). In our experiments, we used up to six encoder-decoder NMT models (three pairs); English→French, French→English, English→Czech, Czech→English, English→German, German→English. All systems were trained on the available training data from the WMT15 shared translation task (4.2 million, 15.7 million, and 39 million sentence pairs for EN↔DE, EN↔CS, and EN↔FR, respectively). For EN↔DE and EN→CS, we also had access to back-translated monolingual training data (Sennrich et al., 2016a), which we also used in training. The data was pre-processed using standard pre-processing scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, following Sennrich et al. (2016b). BLEU scores for each NMT system can be seen in Table 1. small boat sail boat Figure 2: Attention between two sentences. Line thickness indicates the strength of the attention. source: TF F j F,E1  α(E2i , E1 , F )=∑ P(E2 |E1 , F)·∑(αEi,m2 ,F·αm, j ) (8) F m An example shown in Figure 2 where attention has successfully identified the semantically equivalent parts of two sentences. Beyond providing interpretable paraphrasing, attention scores can be used as features in both generation and classification tasks. Furth"
E17-1083,2005.mtsummit-papers.11,0,0.0723828,"N ET probabilities (Equation (7)) assigned to paraphrase pairs and human judgments. Figure 3 shows correlation coefficients for all language pairs using a single foreign pivot and 200 pivots. Across all language combinations multiple pivots2 achieve better correlations, with the German, Czech pair performing best with ρ = 0.53. For comparison, Pavlick et al. (2015) report a correlation of ρ = 0.41 using Equation (9) and PPDB (Ganitkevitch et al., 2013). The latter contains over 100 million paraphrases and was constructed over several English-to-foreign parallel corpora including Europarl v7 (Koehn, 2005) which contains bitexts for the 19 European languages. Following Pavlick et al. (2015), we next developed a supervised scoring model. Specifically, we fit a decision tree regressor on the PPDB 2.0 dataset using the implementation provided in scikit-learn (Pedregosa et al., 2011). To improve accuracy and control over-fitting we built an ensemble of regression trees using the ExtraTrees algorithm (Geurts et al., 2006) which fits a number of randomized decision trees (a.k.a. extratrees) on various sub-samples of the dataset. In our experiments 1,000 trees were trained to minimize mean square erro"
E17-1083,P04-1077,0,0.0339287,"his to the noisy nature of these two datasets which contain a wealth of paraphrases, a few of which are ungrammatical, contain typos or abbreviations leading to low scores among humans. sentence and rewards those close to the target: iBLEU(s, rs , c) = αBLEU(c, rs ) − (1 − α)BLEU(c, s) where s, is the source sentence, rs , is the target and c is the candidate paraphrase. (1 − α)BLEU(c, s), measures the originality of the candidate paraphrase, BLEU(c, rs ) measures semantic adequacy, and α is a tuning parameter which balances the two. Sentence level BLEU is calculated using plus one smoothing (Lin and Och, 2004). PARA N ET relies on a relatively simple architecture which is trained end-to-end with the objective of maximizing the likelihood of the training data. Since evaluation metrics cannot be straightforwardly integrated into this training procedure, we reranked the k-best paraphrases obtained from PARA N ET using a simple classifier which favors sentences which are dissimilar to the source. Specifically, we trained a decision tree regression model with iBLEU as the target variable using the same features described in Section 4.4. Examples of paraphrases generated by PARA N ET are shown in the App"
E17-1083,P07-1059,0,0.0328543,"generate target paraphrases for a given source input. Due to the architecture of NMT, generation takes advantage of wider context compared to phrase-based approaches: target paraphrases are predicted based on the meaning of the source input and all previously generated target words. Introduction Paraphrasing can be broadly described as the task of using an alternative surface form to express the same semantic content (Madnani and Dorr, 2010). Much of the appeal of paraphrasing stems from its potential application to a wider range of NLP problems. Examples include query and pattern expansion (Riezler et al., 2007), summarization (Barzilay, 2003), question answering (Lin and Pantel, 2001), semantic parsing (Berant and Liang, 2014), semantic role labeling (Woodsend and Lapata, 2014), and machine translation (CallisonBurch et al., 2006). Most of the recent literature has focused on the automatic extraction of paraphrases from various different types of corpora consisting of parallel, non-parallel, and comparable texts. One of the most successful proposals uses bilingual parallel corpora to induce paraphrases based on techniques from phrase-based statistical machine translation (SMT, Koehn et al. (2003))."
E17-1083,P16-1009,1,0.169343,"ailing in a a tiny vocabulary size as 10000 and 25 uni-gram translations, using a bilingual dictionary based on fastalign (Dyer et al., 2013). In our experiments, we used up to six encoder-decoder NMT models (three pairs); English→French, French→English, English→Czech, Czech→English, English→German, German→English. All systems were trained on the available training data from the WMT15 shared translation task (4.2 million, 15.7 million, and 39 million sentence pairs for EN↔DE, EN↔CS, and EN↔FR, respectively). For EN↔DE and EN→CS, we also had access to back-translated monolingual training data (Sennrich et al., 2016a), which we also used in training. The data was pre-processed using standard pre-processing scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, following Sennrich et al. (2016b). BLEU scores for each NMT system can be seen in Table 1. small boat sail boat Figure 2: Attention between two sentences. Line thickness indicates the strength of the attention. source: TF F j F,E1  α(E2i , E1 , F )=∑ P(E2 |E1 , F)·∑(αEi,m2 ,F·αm, j ) (8) F m An example shown in Figure 2 where attention has successfully identified the semantically equivalent parts of two sentences."
E17-1083,D15-1166,0,0.0652313,"ents paraphrases in a continuous space, estimates the degree of semantic relatedness between text segments of arbitrary length, or generates candidate paraphrases for any source input. Experimental results across tasks and datasets show that neural paraphrases outperform those obtained with conventional phrase-based pivoting approaches. 1 In this paper we revisit the bilingual pivoting approach from the perspective of neural machine translation, a new approach to machine translation based purely on neural networks (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Sutskever et al., 2014; Luong et al., 2015). At its core, NMT uses a deep neural network trained end-to-end to maximize the conditional probability of a correct translation given a source sentence, using a bilingual corpus. NMT models have obtained state-of-the art performance for several language pairs (Jean et al., 2015b; Luong et al., 2015), using only parallel data for training, and minimal linguistic information. In this paper we show how the bilingual pivoting method can be ported to NMT and argue that it offers at least three advantages over conventional methods. Firstly, our neural paraphrasing model learns continuous space rep"
E17-1083,P16-1162,1,0.0395329,"ailing in a a tiny vocabulary size as 10000 and 25 uni-gram translations, using a bilingual dictionary based on fastalign (Dyer et al., 2013). In our experiments, we used up to six encoder-decoder NMT models (three pairs); English→French, French→English, English→Czech, Czech→English, English→German, German→English. All systems were trained on the available training data from the WMT15 shared translation task (4.2 million, 15.7 million, and 39 million sentence pairs for EN↔DE, EN↔CS, and EN↔FR, respectively). For EN↔DE and EN→CS, we also had access to back-translated monolingual training data (Sennrich et al., 2016a), which we also used in training. The data was pre-processed using standard pre-processing scripts found in MOSES (Koehn et al., 2007). Rare words were split into sub-word units, following Sennrich et al. (2016b). BLEU scores for each NMT system can be seen in Table 1. small boat sail boat Figure 2: Attention between two sentences. Line thickness indicates the strength of the attention. source: TF F j F,E1  α(E2i , E1 , F )=∑ P(E2 |E1 , F)·∑(αEi,m2 ,F·αm, j ) (8) F m An example shown in Figure 2 where attention has successfully identified the semantically equivalent parts of two sentences."
E17-1083,J10-3003,0,0.158386,"tream tasks such as recognizing textual similarity and entailment. Secondly, the proposed model is able to either score a pair of paraphrase candidates (of arbitrary length) and generate target paraphrases for a given source input. Due to the architecture of NMT, generation takes advantage of wider context compared to phrase-based approaches: target paraphrases are predicted based on the meaning of the source input and all previously generated target words. Introduction Paraphrasing can be broadly described as the task of using an alternative surface form to express the same semantic content (Madnani and Dorr, 2010). Much of the appeal of paraphrasing stems from its potential application to a wider range of NLP problems. Examples include query and pattern expansion (Riezler et al., 2007), summarization (Barzilay, 2003), question answering (Lin and Pantel, 2001), semantic parsing (Berant and Liang, 2014), semantic role labeling (Woodsend and Lapata, 2014), and machine translation (CallisonBurch et al., 2006). Most of the recent literature has focused on the automatic extraction of paraphrases from various different types of corpora consisting of parallel, non-parallel, and comparable texts. One of the mos"
E17-1083,D13-1170,0,0.00125829,"and partof-speech tags (Zhao et al., 2008). In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar. We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table. Paraphrastic Embeddings The successful use of word embeddings in various NLP tasks has provided further impetus to use paraphrases. Wieting et al. (2015) take the paraphrases contained in PPDB and embed them into a low-dimensional space using a recursive neural network similar to Socher et al. (2013). In follow-up work (Wieting et al., 2016), they learn sentence embeddings based on supervision provided by PPDB. In our approach, embeddings are learned as part of the model and are available for any-length segments making use of no additional machinery beyond NMT itself. 3 Neural Paraphrasing In this section we present PARA N ET, our Paraphrasing model based on Neural Machine Translation. PARA N ET uses neural machine translation to first translate from English to a foreign pivot, which is then back-translated to English, producing a paraphrase. In the following, we briefly overview the basi"
E17-1083,S14-2039,0,0.0144247,"Missing"
E17-1083,N16-1004,0,0.0165551,"networks. The encoder reads the source sequence into a list of continuous-space representations from which the decoder generates the target sequence. An attention mechanism (Bahdanau et al., 2014) is used to generate the region of focus during decoding. We employ NMT as the backbone of our paraphrasing model. In its simplest form our model exploits a one-to-one NMT architecture: the source English sentence is translated into k candidate foreign sentences and then back-translated into English. Inspired by multi-way machine translation which has shown performance gains over single-pair models (Zoph and Knight, 2016; Dong et al., 2015; Firat et al., 2016a), we also explore an alternative pivoting technique which uses multiple languages rather than a single one. Our model inherits advantages from NMT such as a small memory footprint and conceptually easy decoding (implemented as beam search). Beyond paraphrase generation, we experimentally show that the representations learned by our model are useful in semantic relatedness tasks. Related Work The literature on paraphrasing is vast with methods varying according to the type of paraphrase being induced (lexical or structural), the type of data used (e.g.,"
E17-1083,P08-1089,0,0.0599002,"araphrase probability defined in terms of the translation model probabilities P( f |e) and P(e |f ) where f and e are the foreign and English strings, respectively. Motivated by the wish to model sentential paraphrases, follow-up work focused on syntaxdriven techniques again within the bilingual pivoting framework. Extensions include representing paraphrases via rules obtained from a synchronous context free grammar (Ganitkevitch et al., 2011; Madnani et al., 2007) as well as labeling paraphrases with linguistic annotations such as CCG categories (Callison-Burch, 2008) and partof-speech tags (Zhao et al., 2008). In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar. We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table. Paraphrastic Embeddings The successful use of word embeddings in various NLP tasks has provided further impetus to use paraphrases. Wieting et al. (2015) take the paraphrases contained in PPDB and embed them into a low-dimensional space using a recursive neural network similar to Socher et al. (2013). In follow-up work (Wi"
E17-1083,P12-2008,0,0.0669534,"three languages as pivots, and compared PARA N ET and PARA S TAT directly. Our results are summarized in Table 5. The third block in the table presents a In order to select the best paraphrase candidate for a given input sentence, PARA S TAT was optimized on the training set using Minimum Error Training (MERT, Och and Ney (2003)). MERT integrates automatic evaluation metrics such as BLEU into the training process to achieve optimal end-to-end performance. Naively optimizing for BLEU, however, will result in a trivial paraphrasing system heavily biased towards producing identity “paraphrases”. Sun and Zhou (2012) introduce iBLEU which we also adopt. iBLEU penalizes paraphrases which are similar to the source 3 http://wiki.answers.com/ 888 Model fr de cz Gold PARA S TAT PARA N ET 0.280 0.299 0.282 0.295 0.280 0.291 0.599 Model Wikianswers Leagues MTC All PARA S TAT 2.09 2.38 2.23 2.26 PARA N ET 1.86 1.94 1.70 1.83 Humans 2.17 1.81 2.0 2.0 Table 7: Mean Rankings given to paraphrases by human participants (a lower score is better). Table 6: Mean iBLEU across three datasets. datasets. For the sake of brevity, we only show results with one pivot language since combinations performed slightly worse for both"
E17-1083,N07-1061,0,0.0145047,"ces from the same language, F1 and F2 . Each translation path individ(4) Pivoting Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is not a translation path from the source language to the target. Instead, pivoting takes advantage of paths through an intermediate language. The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in traditional phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) 883 s0 s1 s2 s3 s4 Le il temps fait est grande superbe EOL EOL s0 s1 s2 s3 s5 s6 s7 s8 s9 The weather is great EOL s4 s5 s6 s7 s8 Figure 1: Late-weighted combination: two pivot sentences are simultaneously translated to one target sentence. Blue circles indicate the encoders, which individually encode the two source sentences. After the EOL token is seen, decoding starts (red circles). At each time step the two decoders produce a probability distribution over all words, which are then combined (in the yellow square) using Equation (6). From this combined distribution a word is chosen, which i"
E17-1083,Q15-1025,0,0.0133408,"kevitch et al., 2011; Madnani et al., 2007) as well as labeling paraphrases with linguistic annotations such as CCG categories (Callison-Burch, 2008) and partof-speech tags (Zhao et al., 2008). In contrast, our model is syntax-agnostic, paraphrases are represented on the surface level without knowledge of any underlying grammar. We capture paraphrases at varying levels of granularity, words, phrases or sentences without having to explicitly create a phrase table. Paraphrastic Embeddings The successful use of word embeddings in various NLP tasks has provided further impetus to use paraphrases. Wieting et al. (2015) take the paraphrases contained in PPDB and embed them into a low-dimensional space using a recursive neural network similar to Socher et al. (2013). In follow-up work (Wieting et al., 2016), they learn sentence embeddings based on supervision provided by PPDB. In our approach, embeddings are learned as part of the model and are available for any-length segments making use of no additional machinery beyond NMT itself. 3 Neural Paraphrasing In this section we present PARA N ET, our Paraphrasing model based on Neural Machine Translation. PARA N ET uses neural machine translation to first transla"
E17-1083,P07-1108,0,0.0205991,"of two pivot sentences from the same language, F1 and F2 . Each translation path individ(4) Pivoting Pivoting is often used in machine translation to overcome the shortage of parallel data, i,e., when there is not a translation path from the source language to the target. Instead, pivoting takes advantage of paths through an intermediate language. The idea dates back at least to Kay (1997), who observed that ambiguities in translating from one language onto another may be resolved if a translation into some third language is available, and has met with success in traditional phrase-based SMT (Wu and Wang, 2007; Utiyama and Isahara, 2007) 883 s0 s1 s2 s3 s4 Le il temps fait est grande superbe EOL EOL s0 s1 s2 s3 s5 s6 s7 s8 s9 The weather is great EOL s4 s5 s6 s7 s8 Figure 1: Late-weighted combination: two pivot sentences are simultaneously translated to one target sentence. Blue circles indicate the encoders, which individually encode the two source sentences. After the EOL token is seen, decoding starts (red circles). At each time step the two decoders produce a probability distribution over all words, which are then combined (in the yellow square) using Equation (6). From this combined distributi"
E17-1083,Q14-1034,0,0.0181394,"e 3: Paraphrase detection results (F1) on the PIT-2015 data set. Boldface indicates the best performing paraphrasing model. Paraphrase Identification and Similarity The SemEval-2015 shared task on Paraphrase and Semantic Similarity In Twitter (PIT) uses a training and development set of 17,790 sentence pairs and a test set of 972 sentence pairs. By design, the dataset contains colloquial sentences representing informal language usage and sentence pairs which are lexically similar but semantically dissimilar. Sentence pairs were crawled from Twitter’s trending topics and associated tweets (see Xu et al. (2014) for details). The shared task consists of a (binary) paraphrase identification subtask (i.e., determine whether two sentences are paraphrases) and an optional semantic similarity task (i.e., determine the similarity between two sentences on a scale of 1–5, where 5 means completely equivalent and 1 not equivalent). We trained a decision tree regressor on the PIT-2015 similarity dataset using the features described above. Once trained, the decision tree regressor can be readily applied to the semantic similarity subtask. For the paraphrase detection subtask, we use the same model and apply a th"
E17-1083,S15-2002,0,0.031926,"Missing"
E17-1083,N16-1101,0,\N,Missing
E17-2060,L16-1453,0,0.0173533,"ncertain Mr. Ensign’s office German (correct) [...] des amerikanischen Kongresses [...], dass der Plan verabschiedet wird er ruht sich aus das Timing [...] ist unsicher Senator Ensigns Büro German (contrastive) * [...] der amerikanischen Kongresses * [...], dass der Plan verabschiedet werden * er ruht sich an das Timing [..] ist sicher Senator Enisgns Büro Table 1: Example contrastive translations pair for each error category. translation, including agreement over long distances (Koehn and Hoang, 2007; Williams and Koehn, 2011), discontiguous verb-particle constructions (Nießen and Ney, 2000; Loáiciga and Gulordava, 2016), generalization to unseen words (specifically, transliteration of names (Durrani et al., 2014)), and ensuring that polarity is maintained (Wetzel and Bond, 2012; Chen and Zhu, 2014; Fancellu and Webber, 2015). We report results for neural machine translation systems with different choice of subword unit, identifying strengths and weaknesses of recentlyproposed models. 2 4. polarity: arguably, polarity errors are undermeasured the most by string-based MT metrics, since a single word/morpheme can reverse the meaning of a translation. We reverse polarity by deleting/inserting the negation partic"
E17-2060,P16-1100,0,0.016981,"tion systems. By introducing specific translation errors to the contrastive translations, we gain valuable insight into the ability of state-of-the-art neural MT systems to handle several challenging linguistic phenomena. A core finding is that recently proposed characterlevel decoders for neural machine translation outperform subword models at processing unknown names, but perform worse at modelling morphosyntactic agreement, where information needs to be carried over long distances. We encourage the use of LingEval97 to assess alternative architectures, such as hybrid word-character models (Luong and Manning, 2016), or dilated convolutional networks (Kalchbrenner et al., 2016). For the tested systems, the most challenging error type is the deletion of negation markers, and we hope that our test set will facilitate development and evaluation of models that try to improve in that respect. Finally, the evaluation via contrastive translation pairs is a very flexible approach, and can be applied to new language pairs and error types. Acknowledgments This project received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21) and 688139 (SUMMA). 3"
E17-2060,E14-1064,0,0.0255516,"rman (contrastive) * [...] der amerikanischen Kongresses * [...], dass der Plan verabschiedet werden * er ruht sich an das Timing [..] ist sicher Senator Enisgns Büro Table 1: Example contrastive translations pair for each error category. translation, including agreement over long distances (Koehn and Hoang, 2007; Williams and Koehn, 2011), discontiguous verb-particle constructions (Nießen and Ney, 2000; Loáiciga and Gulordava, 2016), generalization to unseen words (specifically, transliteration of names (Durrani et al., 2014)), and ensuring that polarity is maintained (Wetzel and Bond, 2012; Chen and Zhu, 2014; Fancellu and Webber, 2015). We report results for neural machine translation systems with different choice of subword unit, identifying strengths and weaknesses of recentlyproposed models. 2 4. polarity: arguably, polarity errors are undermeasured the most by string-based MT metrics, since a single word/morpheme can reverse the meaning of a translation. We reverse polarity by deleting/inserting the negation particle nicht (’not’), swapping the determiner ein (’a’) and its negative counterpart kein (’no’), or deleting/inserting the negation prefix un-. 5. transliteration: subword-level models"
E17-2060,C00-2162,0,0.14215,"the timing [...] is uncertain Mr. Ensign’s office German (correct) [...] des amerikanischen Kongresses [...], dass der Plan verabschiedet wird er ruht sich aus das Timing [...] ist unsicher Senator Ensigns Büro German (contrastive) * [...] der amerikanischen Kongresses * [...], dass der Plan verabschiedet werden * er ruht sich an das Timing [..] ist sicher Senator Enisgns Büro Table 1: Example contrastive translations pair for each error category. translation, including agreement over long distances (Koehn and Hoang, 2007; Williams and Koehn, 2011), discontiguous verb-particle constructions (Nießen and Ney, 2000; Loáiciga and Gulordava, 2016), generalization to unseen words (specifically, transliteration of names (Durrani et al., 2014)), and ensuring that polarity is maintained (Wetzel and Bond, 2012; Chen and Zhu, 2014; Fancellu and Webber, 2015). We report results for neural machine translation systems with different choice of subword unit, identifying strengths and weaknesses of recentlyproposed models. 2 4. polarity: arguably, polarity errors are undermeasured the most by string-based MT metrics, since a single word/morpheme can reverse the meaning of a translation. We reverse polarity by deletin"
E17-2060,P16-1160,0,0.0882553,"iteration 3490 96.1 98.6 98.3 96.4 99.0 Table 4: Accuracy (in percent) of models on different categories of contrastive errors. Best single model result in bold (multiple bold results indicate that difference to best system is not statistically significant). 1 3.2 accuracy gual training data, an ensemble of models, and bidirectional decoding. Results Firstly, we report case-sensitive B LEU scores for all systems we trained for comparison to previous work.10 Results are shown in Table 3. The results confirm that our systems are comparable to previously reported results (Sennrich et al., 2016a; Chung et al., 2016), and that performance of the three systems is relatively close in terms of B LEU. The metric does not provide any insight into the respective strengths and weaknesses of different text representations. Our main result is the assessment via contrastive translation pairs, shown in Table 4. We find that despite obtaining similar B LEU scores, the models have learned different structures to a different degree. The models with character decoder make fewer transliteration errors than the BPE-to-BPE model. However, they perform more poorly on separable verb particles and agreement, especially as dis"
E17-2060,P02-1040,0,0.122805,"trastive translation pairs based on the WMT English→German translation task, with errors automatically created with simple rules. We report results for a number of systems, and find that recently introduced character-level NMT systems perform better at transliteration than models with byte-pair encoding (BPE) segmentation, but perform more poorly at morphosyntactic agreement, and translating discontiguous units of meaning. 1 Introduction It has historically been difficult to analyse how well a machine translation system can learn specific linguistic phenomena. Automatic metrics such as B LEU (Papineni et al., 2002) provide no linguistic insight, and automatic error analysis 1 Test set and evaluation script are available at https: //github.com/rsennrich/lingeval97 376 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 376–382, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics category NP agreement subject-verb agr. separable verb particle polarity transliteration English [...] of the American Congress [...] that the plan will be approved he is resting the timing [...] is uncertain Mr."
E17-2060,E14-4029,0,0.0156861,"verabschiedet wird er ruht sich aus das Timing [...] ist unsicher Senator Ensigns Büro German (contrastive) * [...] der amerikanischen Kongresses * [...], dass der Plan verabschiedet werden * er ruht sich an das Timing [..] ist sicher Senator Enisgns Büro Table 1: Example contrastive translations pair for each error category. translation, including agreement over long distances (Koehn and Hoang, 2007; Williams and Koehn, 2011), discontiguous verb-particle constructions (Nießen and Ney, 2000; Loáiciga and Gulordava, 2016), generalization to unseen words (specifically, transliteration of names (Durrani et al., 2014)), and ensuring that polarity is maintained (Wetzel and Bond, 2012; Chen and Zhu, 2014; Fancellu and Webber, 2015). We report results for neural machine translation systems with different choice of subword unit, identifying strengths and weaknesses of recentlyproposed models. 2 4. polarity: arguably, polarity errors are undermeasured the most by string-based MT metrics, since a single word/morpheme can reverse the meaning of a translation. We reverse polarity by deleting/inserting the negation particle nicht (’not’), swapping the determiner ein (’a’) and its negative counterpart kein (’no’), o"
E17-2060,W15-1301,0,0.0168585,"[...] der amerikanischen Kongresses * [...], dass der Plan verabschiedet werden * er ruht sich an das Timing [..] ist sicher Senator Enisgns Büro Table 1: Example contrastive translations pair for each error category. translation, including agreement over long distances (Koehn and Hoang, 2007; Williams and Koehn, 2011), discontiguous verb-particle constructions (Nießen and Ney, 2000; Loáiciga and Gulordava, 2016), generalization to unseen words (specifically, transliteration of names (Durrani et al., 2014)), and ensuring that polarity is maintained (Wetzel and Bond, 2012; Chen and Zhu, 2014; Fancellu and Webber, 2015). We report results for neural machine translation systems with different choice of subword unit, identifying strengths and weaknesses of recentlyproposed models. 2 4. polarity: arguably, polarity errors are undermeasured the most by string-based MT metrics, since a single word/morpheme can reverse the meaning of a translation. We reverse polarity by deleting/inserting the negation particle nicht (’not’), swapping the determiner ein (’a’) and its negative counterpart kein (’no’), or deleting/inserting the negation prefix un-. 5. transliteration: subword-level models should be able to copy or t"
E17-2060,schmid-etal-2004-smor,0,0.00918763,"lish to German. Table 1 shows examples for each error type. Most are motivated by frequent translation errors; for EN→DE, source and target script are the same, so technically, we do not perform transliteration. Since transliteration of names and copying them is handled the same way by the encoder-decoder networks that we tested, we consider this error type a useful proxy to test the models’ transliteration capability. All errors are introduced automatically, relying on statistics from the training corpus, a syntactic analysis with ParZu (Sennrich et al., 2013), and a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014) to identify the relevant constructions and introduce errors. For contrastive pairs with agreement errors, we also annotate the distance between the words. For translation errors where we want to assess generalization to rare words (all except negation particles), we also provide the training set frequency of the word involved in the error (in case of multiple words, we report the lower frequency). The automatic processing has limitations, and we opt for a high-precision approach – for instance, we only change the gender of determiners where case and number are unambi"
E17-2060,D13-1176,0,0.0763584,"tences. Chung et al. (2016) argue that the answer is yes, because B LEU on long sentences is similar to a baseline with longer subword units created via byte-pair encoding (BPE) (Sennrich et al., 2016a), but B LEU, being based on precision of short ngrams, is an unsuitable metric to measure the global coherence or grammaticality of a sentence. To allow for a more nuanced analysis of different machine translation systems, we introduce a novel method to assess neural machine translation that can capture specific error categories in an automatic, reproducible fashion. Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) opens up new opportunities for automatic analysis because it can assign scores to arbitrary sentence pairs, in contrast to phrase-based systems, which are often unable to reach the reference translation. We exploit this property for the automatic evaluation of specific aspects of translation by pairing a human reference translation with a contrastive example that is identical except for a specific error. Models are tested as to whether they assign a higher probability to the reference translation than to the contrastive example. A similar method"
E17-2060,D15-1248,1,0.856152,"es for automatic analysis because it can assign scores to arbitrary sentence pairs, in contrast to phrase-based systems, which are often unable to reach the reference translation. We exploit this property for the automatic evaluation of specific aspects of translation by pairing a human reference translation with a contrastive example that is identical except for a specific error. Models are tested as to whether they assign a higher probability to the reference translation than to the contrastive example. A similar method of assessment has previously been used for monolingual language models (Sennrich and Haddow, 2015; Linzen et al., 2016), and we apply it to the task of machine translation. We present a large-scale test set of English→German contrastive translation pairs that allows for the automatic, quantitative analysis of a number of linguistically interesting phenomena that have previously been found to be challenging for machine Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess"
E17-2060,sennrich-kunz-2014-zmorge,1,0.835841,"1 shows examples for each error type. Most are motivated by frequent translation errors; for EN→DE, source and target script are the same, so technically, we do not perform transliteration. Since transliteration of names and copying them is handled the same way by the encoder-decoder networks that we tested, we consider this error type a useful proxy to test the models’ transliteration capability. All errors are introduced automatically, relying on statistics from the training corpus, a syntactic analysis with ParZu (Sennrich et al., 2013), and a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014) to identify the relevant constructions and introduce errors. For contrastive pairs with agreement errors, we also annotate the distance between the words. For translation errors where we want to assess generalization to rare words (all except negation particles), we also provide the training set frequency of the word involved in the error (in case of multiple words, we report the lower frequency). The automatic processing has limitations, and we opt for a high-precision approach – for instance, we only change the gender of determiners where case and number are unambiguous, so that we can prod"
E17-2060,D07-1091,0,0.0778809,"ion English [...] of the American Congress [...] that the plan will be approved he is resting the timing [...] is uncertain Mr. Ensign’s office German (correct) [...] des amerikanischen Kongresses [...], dass der Plan verabschiedet wird er ruht sich aus das Timing [...] ist unsicher Senator Ensigns Büro German (contrastive) * [...] der amerikanischen Kongresses * [...], dass der Plan verabschiedet werden * er ruht sich an das Timing [..] ist sicher Senator Enisgns Büro Table 1: Example contrastive translations pair for each error category. translation, including agreement over long distances (Koehn and Hoang, 2007; Williams and Koehn, 2011), discontiguous verb-particle constructions (Nießen and Ney, 2000; Loáiciga and Gulordava, 2016), generalization to unseen words (specifically, transliteration of names (Durrani et al., 2014)), and ensuring that polarity is maintained (Wetzel and Bond, 2012; Chen and Zhu, 2014; Fancellu and Webber, 2015). We report results for neural machine translation systems with different choice of subword unit, identifying strengths and weaknesses of recentlyproposed models. 2 4. polarity: arguably, polarity errors are undermeasured the most by string-based MT metrics, since a s"
E17-2060,R13-1079,1,0.739067,"o be challenging for the automatic translation from English to German. Table 1 shows examples for each error type. Most are motivated by frequent translation errors; for EN→DE, source and target script are the same, so technically, we do not perform transliteration. Since transliteration of names and copying them is handled the same way by the encoder-decoder networks that we tested, we consider this error type a useful proxy to test the models’ transliteration capability. All errors are introduced automatically, relying on statistics from the training corpus, a syntactic analysis with ParZu (Sennrich et al., 2013), and a finite-state morphology (Schmid et al., 2004; Sennrich and Kunz, 2014) to identify the relevant constructions and introduce errors. For contrastive pairs with agreement errors, we also annotate the distance between the words. For translation errors where we want to assess generalization to rare words (all except negation particles), we also provide the training set frequency of the word involved in the error (in case of multiple words, we report the lower frequency). The automatic processing has limitations, and we opt for a high-precision approach – for instance, we only change the ge"
E17-2060,Q16-1037,0,0.0470437,"ecause it can assign scores to arbitrary sentence pairs, in contrast to phrase-based systems, which are often unable to reach the reference translation. We exploit this property for the automatic evaluation of specific aspects of translation by pairing a human reference translation with a contrastive example that is identical except for a specific error. Models are tested as to whether they assign a higher probability to the reference translation than to the contrastive example. A similar method of assessment has previously been used for monolingual language models (Sennrich and Haddow, 2015; Linzen et al., 2016), and we apply it to the task of machine translation. We present a large-scale test set of English→German contrastive translation pairs that allows for the automatic, quantitative analysis of a number of linguistically interesting phenomena that have previously been found to be challenging for machine Analysing translation quality in regards to specific linguistic phenomena has historically been difficult and time-consuming. Neural machine translation has the attractive property that it can produce scores for arbitrary translations, and we propose a novel method to assess how well NMT systems"
E17-2060,W16-2323,1,0.866064,"MT Quality with Contrastive Translation Pairs Rico Sennrich School of Informatics, University of Edinburgh {rico.sennrich}@ed.ac.uk Abstract (Zeman et al., 2011; Popovic, 2011) is also relatively coarse-grained. A concrete research question that has been unanswered so far is whether character-level decoders for neural machine translation (Chung et al., 2016; Lee et al., 2016) can generate coherent and grammatical sentences. Chung et al. (2016) argue that the answer is yes, because B LEU on long sentences is similar to a baseline with longer subword units created via byte-pair encoding (BPE) (Sennrich et al., 2016a), but B LEU, being based on precision of short ngrams, is an unsuitable metric to measure the global coherence or grammaticality of a sentence. To allow for a more nuanced analysis of different machine translation systems, we introduce a novel method to assess neural machine translation that can capture specific error categories in an automatic, reproducible fashion. Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) opens up new opportunities for automatic analysis because it can assign scores to arbitrary sentence pairs, in contrast t"
E17-2060,P16-1162,1,0.153954,"MT Quality with Contrastive Translation Pairs Rico Sennrich School of Informatics, University of Edinburgh {rico.sennrich}@ed.ac.uk Abstract (Zeman et al., 2011; Popovic, 2011) is also relatively coarse-grained. A concrete research question that has been unanswered so far is whether character-level decoders for neural machine translation (Chung et al., 2016; Lee et al., 2016) can generate coherent and grammatical sentences. Chung et al. (2016) argue that the answer is yes, because B LEU on long sentences is similar to a baseline with longer subword units created via byte-pair encoding (BPE) (Sennrich et al., 2016a), but B LEU, being based on precision of short ngrams, is an unsuitable metric to measure the global coherence or grammaticality of a sentence. To allow for a more nuanced analysis of different machine translation systems, we introduce a novel method to assess neural machine translation that can capture specific error categories in an automatic, reproducible fashion. Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) opens up new opportunities for automatic analysis because it can assign scores to arbitrary sentence pairs, in contrast t"
E17-2060,W12-4203,0,0.0231984,"Senator Ensigns Büro German (contrastive) * [...] der amerikanischen Kongresses * [...], dass der Plan verabschiedet werden * er ruht sich an das Timing [..] ist sicher Senator Enisgns Büro Table 1: Example contrastive translations pair for each error category. translation, including agreement over long distances (Koehn and Hoang, 2007; Williams and Koehn, 2011), discontiguous verb-particle constructions (Nießen and Ney, 2000; Loáiciga and Gulordava, 2016), generalization to unseen words (specifically, transliteration of names (Durrani et al., 2014)), and ensuring that polarity is maintained (Wetzel and Bond, 2012; Chen and Zhu, 2014; Fancellu and Webber, 2015). We report results for neural machine translation systems with different choice of subword unit, identifying strengths and weaknesses of recentlyproposed models. 2 4. polarity: arguably, polarity errors are undermeasured the most by string-based MT metrics, since a single word/morpheme can reverse the meaning of a translation. We reverse polarity by deleting/inserting the negation particle nicht (’not’), swapping the determiner ein (’a’) and its negative counterpart kein (’no’), or deleting/inserting the negation prefix un-. 5. transliteration:"
E17-2060,W11-2126,0,0.0250882,"e American Congress [...] that the plan will be approved he is resting the timing [...] is uncertain Mr. Ensign’s office German (correct) [...] des amerikanischen Kongresses [...], dass der Plan verabschiedet wird er ruht sich aus das Timing [...] ist unsicher Senator Ensigns Büro German (contrastive) * [...] der amerikanischen Kongresses * [...], dass der Plan verabschiedet werden * er ruht sich an das Timing [..] ist sicher Senator Enisgns Büro Table 1: Example contrastive translations pair for each error category. translation, including agreement over long distances (Koehn and Hoang, 2007; Williams and Koehn, 2011), discontiguous verb-particle constructions (Nießen and Ney, 2000; Loáiciga and Gulordava, 2016), generalization to unseen words (specifically, transliteration of names (Durrani et al., 2014)), and ensuring that polarity is maintained (Wetzel and Bond, 2012; Chen and Zhu, 2014; Fancellu and Webber, 2015). We report results for neural machine translation systems with different choice of subword unit, identifying strengths and weaknesses of recentlyproposed models. 2 4. polarity: arguably, polarity errors are undermeasured the most by string-based MT metrics, since a single word/morpheme can rev"
E17-2060,P16-1007,0,0.00632975,"heiten in Hälfte eins hatte Robert Lewandowski. Die besten Gelegenheiten in Hälfte eins hatten Robert Lewandowski. Robert Lewandowski hatte in der ersten Hälfte die besten Möglichkeiten. cost 0.149 0.137 0.090 0.276 0.262 0.129 0.551 0.507 0.046 Table 6: Examples where char-to-char model prefers contrastive translation (subject-verb agreement errors). 1-best translation can make error of same type (example 1), different type (translation of taught is missing in example 2), or no error (example 3). translations other than the 1-best, for instance for applications such as prefix-constrained MT (Wuebker et al., 2016). that aim to overcome this trade-off and perform best in respect to both morphology and syntax. We encourage the use of contrastive translation pairs, and LingEval97, for future analysis, but here discuss some limitations. The first one is by design: being focused on specific translation errors, the evaluation is not suitable as a global quality metric. Also, the evaluation only compares the probability of two translations, a reference translation T and a contrastive translation T 0 , and makes no statement about the most probable translation T ∗ . Even if a model correctly estimates that p(T"
E17-3017,W14-3346,0,0.0173175,"2016). Nematus is available under a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), 644333 (TraMOOC), 644402 (HimL) and 688139 (SUMMA). Usability Features References In addition to the main algorithms to train and decode with an NMT model, Nematus includes fe"
E17-3017,W11-2107,0,0.0197012,"r a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT21), 644333 (TraMOOC), 644402 (HimL) and 688139 (SUMMA). Usability Features References In addition to the main algorithms to train and decode with an NMT model, Nematus includes features aimed towards facilitating ex"
E17-3017,W14-3354,0,0.0551932,"Missing"
E17-3017,E17-2025,0,0.133573,"Missing"
E17-3017,W16-2209,1,0.131218,"axout before the softmax layer. • In both encoder and decoder word embedding layers, we do not use additional biases. • Compared to Look, Generate, Update decoder phases in Bahdanau et al. (2015), we implement Look, Update, Generate which drastically simplifies the decoder implementation (see Table 1). • Optionally, we perform recurrent Bayesian dropout (Gal, 2015). • Instead of a single word embedding at each source position, our input representations allows multiple features (or “factors”) at each time step, with the final embedding being the concatenation of the embeddings of each feature (Sennrich and Haddow, 2016). • We allow tying of embedding matrices (Press and Wolf, 2017; Inan et al., 2016). We will here describe some differences in more detail: available at https://github.com/rsennrich/nematus https://github.com/nyu-dl/dl4mt-tutorial 65 Proceedings of the EACL 2017 Software Demonstrations, Valencia, Spain, April 3-7 2017, pages 65–68 c 2017 Association for Computational Linguistics z0j being the reset and update gate activations. In this formulation, W0 , U0 , Wr0 , U0r , Wz0 , U0z are trained model parameters; σ is the logistic sigmoid activation function. The attention mechanism, ATT, inputs the"
E17-3017,W16-2323,1,0.216734,"Sutskever et al., 2014) has recently established itself as a new state-of-the art in machine translation. We present Nematus1 , a new toolkit for Neural Machine Translation. Nematus has its roots in the dl4mt-tutorial.2 We found the codebase of the tutorial to be compact, simple and easy to extend, while also producing high translation quality. These characteristics make it a good starting point for research in NMT. Nematus has been extended to include new functionality based on recent research, and has been used to build top-performing systems to last year’s shared translation tasks at WMT (Sennrich et al., 2016) and IWSLT (Junczys-Dowmunt and Birch, 2016). Nematus is implemented in Python, and based on the Theano framework (Theano Development Team, 2016). It implements an attentional encoder–decoder architecture similar to Bahdanau et al. (2015). Our neural network architecture differs in some aspect from theirs, and we will discuss differences in more detail. We will also describe additional functionality, aimed to enhance usability and performance, which has been implemented in Nematus. 1 2 Neural Network Architecture • In the decoder, we use a feedforward hidden layer with tanh non-linearity rathe"
E17-3017,P16-1159,0,0.0772511,"ll documented toolkit to support their research. The toolkit is by no means limited to research, and has been used to train MT systems that are currently in production (WIPO, 2016). Nematus is available under a permissive BSD license. By default, the training objective in Nematus is cross-entropy minimization on a parallel training corpus. Training is performed via stochastic gradient descent, or one of its variants with adaptive learning rate (Adadelta (Zeiler, 2012), RmsProp (Tieleman and Hinton, 2012), Adam (Kingma and Ba, 2014)). Additionally, Nematus supports minimum risk training (MRT) (Shen et al., 2016) to optimize towards an arbitrary, sentence-level loss function. Various MT metrics are supported as loss function, including smoothed sentence-level B LEU (Chen and Cherry, 2014), METEOR (Denkowski and Lavie, 2011), BEER (Stanojevic and Sima’an, 2014), and any interpolation of implemented metrics. To stabilize training, Nematus supports early stopping based on cross entropy, or an arbitrary loss function defined by the user. 4 Conclusion Acknowledgments This project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreements 645452 (QT2"
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
I17-2053,P15-1085,0,0.0058698,"age annotations that document the code in sufficient detail. Some existing corpora, such as the the DJANGO dataset and the Project Euler dataset (Oda et al., 2015) have been created by human annotators, who can produce high accuracy examples, but this annotation process is expensive and relatively slow, resulting in small (from a few hundreds to less than 20,000 examples) and homogeneous datasets. Other corpora have been assembled from user-generated descriptions matched to code fragments mined from public websites such as StackOverflow2 (Allamanis et al., 2015b; Iyer et al., 2016) or IFTTT3 (Quirk et al., 2015). These datasets can be large (> 100, 000 examples) but often very noisy. Another approach is to target a very specific domain, namely trading card Automated documentation of programming source code and automated code generation from natural language are challenging tasks of both practical and scientific interest. Progress in these areas has been limited by the low availability of parallel corpora of code and natural language descriptions, which tend to be small and constrained to specific domains. In this work we introduce a large and diverse parallel corpus of a hundred thousands Python func"
I17-2053,E17-3017,1,0.316305,"Missing"
I17-2053,P16-1009,1,0.688317,"nrich et al., 2017). Our objective here is not to compete with syntax-aware techniques such as Yin and Neubig (2017) but to assess a lower bound on the task performance on this dataset without using knowledge of the structure of the programming language. We prepare our datasets considering the function declarations as part of the input for both the documentation and generation tasks. In order to reduce data sparsity, we sub-tokenize with the Moses (Koehn et al., 2007) tokenization script (which splits some source code identifiers that contain punctuation) followed by Byte-Pair Encoding (BPE) (Sennrich et al., 2016b). BPE subtokenization has been shown to be effective for natural language processing, and for code processing it can be considered a data-driven alternative to the heuristic identifier sub-tokenization of Allamanis et al. (2015a). We train our models with the Adam optimizer (Kingma and Ba, 2015) with learning rate 10−4 , batch size 20. We use a vocabulary size of 89500 tokens and we cap training sequence length to 300 tokens for both the source side and the target side. We apply ”Bayesian” recurrent dropout (Gal and Ghahramani, 2016) with drop probability 0.2 and word drop probability 0.1. W"
I17-2053,P16-1195,0,0.249337,"or Automated Code Documentation and Code Generation Antonio Valerio Miceli Barone and Rico Sennrich School of Informatics, The University of Edinburgh amiceli@inf.ed.ac.uk rico.sennrich@ed.ac.uk Abstract 1.1 Major breakthroughs have been recently achieved in machine translation and other hard natural language processing tasks by using neural networks, such as sequence-to-sequence transducers (Bahdanau et al., 2014). In order to properly generalize, neural networks need to be trained on large and diverse datasets. These techniques have also been applied with some success to code documentation (Iyer et al., 2016) and code generation (Ling et al., 2016; Yin and Neubig, 2017), but these works trained and evaluated their models on datasets which are small or limited to restricted domains, in some cases single software projects. Source code can be collected by scraping open source repositories from code hosting services such as GitHub1 (Allamanis and Sutton, 2013; Bhoopchand et al., 2016), but the main difficulty is finding natural language annotations that document the code in sufficient detail. Some existing corpora, such as the the DJANGO dataset and the Project Euler dataset (Oda et al., 2015) have be"
I17-2053,P16-1162,1,0.143699,"nrich et al., 2017). Our objective here is not to compete with syntax-aware techniques such as Yin and Neubig (2017) but to assess a lower bound on the task performance on this dataset without using knowledge of the structure of the programming language. We prepare our datasets considering the function declarations as part of the input for both the documentation and generation tasks. In order to reduce data sparsity, we sub-tokenize with the Moses (Koehn et al., 2007) tokenization script (which splits some source code identifiers that contain punctuation) followed by Byte-Pair Encoding (BPE) (Sennrich et al., 2016b). BPE subtokenization has been shown to be effective for natural language processing, and for code processing it can be considered a data-driven alternative to the heuristic identifier sub-tokenization of Allamanis et al. (2015a). We train our models with the Adam optimizer (Kingma and Ba, 2015) with learning rate 10−4 , batch size 20. We use a vocabulary size of 89500 tokens and we cap training sequence length to 300 tokens for both the source side and the target side. We apply ”Bayesian” recurrent dropout (Gal and Ghahramani, 2016) with drop probability 0.2 and word drop probability 0.1. W"
I17-2053,P17-1041,0,0.347305,"Valerio Miceli Barone and Rico Sennrich School of Informatics, The University of Edinburgh amiceli@inf.ed.ac.uk rico.sennrich@ed.ac.uk Abstract 1.1 Major breakthroughs have been recently achieved in machine translation and other hard natural language processing tasks by using neural networks, such as sequence-to-sequence transducers (Bahdanau et al., 2014). In order to properly generalize, neural networks need to be trained on large and diverse datasets. These techniques have also been applied with some success to code documentation (Iyer et al., 2016) and code generation (Ling et al., 2016; Yin and Neubig, 2017), but these works trained and evaluated their models on datasets which are small or limited to restricted domains, in some cases single software projects. Source code can be collected by scraping open source repositories from code hosting services such as GitHub1 (Allamanis and Sutton, 2013; Bhoopchand et al., 2016), but the main difficulty is finding natural language annotations that document the code in sufficient detail. Some existing corpora, such as the the DJANGO dataset and the Project Euler dataset (Oda et al., 2015) have been created by human annotators, who can produce high accuracy"
I17-2053,P07-2045,0,\N,Missing
I17-2053,P16-1057,0,\N,Missing
L18-1005,P02-1040,0,0.107184,"4b; Anastasiou, 2010). Given the difficulty of idiom translation in MT, it would be helpful to have a method to evaluate idiom translation performance. There is a wide range of methods for evaluating the performance of MT systems, but none of them are satisfactory for the targeted evaluation of idiom translation. The most straightforward method is human evaluation. While human evaluation is highly valuable, it is desirable to develop complementary automatic methods that are low-cost and fast, thus allowing for more rapid and frequent feedback cycles. Popular automatic MT metrics such as BLEU (Papineni et al., 2002) are inexpensive, but are unsuitable for a targeted evaluation. This paper tries to fill this gap by presenting a method to assess the quality of idiom translations. We introduce a new method called “blacklist method” for performance evaluation on idioms, which is based on the intuition that a literal translation of the components of the idiom is likely to be wrong, and easy to spot by defining a blacklist of words that indicate a likely literal translation error. We perform a case study on a special class of Chinese idioms that typically consist of 4 characters, called “cheng2 1. Idiom transl"
L18-1005,W14-1007,0,0.169978,"on. In this research we will only focus on those semantically non-transparent words, which have different literal meanings and idiomatic meanings. We will subsequently refer to them as “Chinese idioms”. We also introduce the CIBB dataset1 for actually executing this evaluation on Chinese→English MT systems. Based on this dataset, we conduct experiments on a state-of-theart NMT system. From the experiments we draw the following conclusions: Idioms are a special figure of speech that are noncompositional and non-literal, though occasionally share surface realizations with literal language uses (Salton et al., 2014b). Idioms are considered highly problematic for a wide variety of NLP tasks (Sag et al., 2002). This belief also holds true for machine translation, because MT systems often make the assumption that meaning is compositional, which is not true for idioms. The compositionality assumption leads to literal translation errors, the word-byword translation of idioms, resulting in a translation that is confusing and not understandable. Therefore, idiom translation is a hard problem in MT and has attracted considerable research interest (Cap et al., 2015; Salton et al., 2014b; Anastasiou, 2010). Given"
L18-1005,W14-0806,0,0.175996,"on. In this research we will only focus on those semantically non-transparent words, which have different literal meanings and idiomatic meanings. We will subsequently refer to them as “Chinese idioms”. We also introduce the CIBB dataset1 for actually executing this evaluation on Chinese→English MT systems. Based on this dataset, we conduct experiments on a state-of-theart NMT system. From the experiments we draw the following conclusions: Idioms are a special figure of speech that are noncompositional and non-literal, though occasionally share surface realizations with literal language uses (Salton et al., 2014b). Idioms are considered highly problematic for a wide variety of NLP tasks (Sag et al., 2002). This belief also holds true for machine translation, because MT systems often make the assumption that meaning is compositional, which is not true for idioms. The compositionality assumption leads to literal translation errors, the word-byword translation of idioms, resulting in a translation that is confusing and not understandable. Therefore, idiom translation is a hard problem in MT and has attracted considerable research interest (Cap et al., 2015; Salton et al., 2014b; Anastasiou, 2010). Given"
L18-1005,P16-1162,1,0.245143,"Missing"
L18-1005,E17-2060,1,0.927475,"biguity, composition, function words, multi-word expressions and so on. (Burlot and Yvon, 2017) introduced a new scheme to evaluate the performance of English→MRL (morphologically rich languages) MT systems on morphological difficulties. The test suite they built consists of three parts, focusing on a system’s morphological adequacy (generating different morphological features in different contexts), fluency (word agreement) and certainty (generating the same morphological features in different contexts), respectively. Evaluation is based on automatic morphological analysis of the MT output. (Sennrich, 2017) proposed a method to construct the test suite automatically for evaluating English→German NMT systems on word agreement, polarity, transliteration, etc. The test suite is made up with minimal translation pairs, where a reference translation is paired with a contrastive translation which introduces a single translation error, allowing to measure the sensitivity of a neural MT (NMT) system towards this type of error. The score on the test suite is also obtained automatically by calculating the precision of the NMT system to assign a higher probability to the correct translation than to the cont"
L18-1005,2006.amta-papers.25,0,0.0701929,"special class of Chinese idioms that typically consist of 4 characters, called “cheng2 1. Idiom translation remains an open problem in Chinese→English NMT 2. Literal translation error is still a prevalent error type 3. The blacklist method is effective at detecting literal translation errors. 2. 2.1. Related Work Global Evaluation Metrics Global evaluation metrics are metrics that evaluate the overall performance of MT systems and allow automatically calculation. There are many well-known global evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), etc. However, these metrics only provide global evaluation and are unable to evaluate MT systems’ performance on specific aspects. Therefore, they are unsatisfactory in evaluating idiom translation performance. 1 This dataset is released https://github.com/sythello/CIBB-dataset 31 at 2.2. Test Suite Methods test suites and efficient evaluation. Test suite methods construct a set of sentences that focus on specific types of difficulties in MT. Typically, we design a set of sentences in the source language for the MT system to translate, and a scoring method to evaluate the translations. The s"
L18-1005,W05-0909,0,0.151617,"or. We perform a case study on a special class of Chinese idioms that typically consist of 4 characters, called “cheng2 1. Idiom translation remains an open problem in Chinese→English NMT 2. Literal translation error is still a prevalent error type 3. The blacklist method is effective at detecting literal translation errors. 2. 2.1. Related Work Global Evaluation Metrics Global evaluation metrics are metrics that evaluate the overall performance of MT systems and allow automatically calculation. There are many well-known global evaluation metrics, such as BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), etc. However, these metrics only provide global evaluation and are unable to evaluate MT systems’ performance on specific aspects. Therefore, they are unsatisfactory in evaluating idiom translation performance. 1 This dataset is released https://github.com/sythello/CIBB-dataset 31 at 2.2. Test Suite Methods test suites and efficient evaluation. Test suite methods construct a set of sentences that focus on specific types of difficulties in MT. Typically, we design a set of sentences in the source language for the MT system to translate, and a scoring method to evalu"
L18-1005,W17-4705,0,0.293701,"mance on specific issues. There are many previous works belonging to this category. (Isabelle et al., 2017) proposed a challenge set approach to evaluate English→French MT systems’ performance on divergence problems. The English sentences in the challenge set are chosen so that their closest French equivalent will be structurally divergent from them in some crucial way. (Burchardt et al., 2017) constructed a test suite for English→German MT systems. This test suite covers a wide variety of linguistic phenomena, such as ambiguity, composition, function words, multi-word expressions and so on. (Burlot and Yvon, 2017) introduced a new scheme to evaluate the performance of English→MRL (morphologically rich languages) MT systems on morphological difficulties. The test suite they built consists of three parts, focusing on a system’s morphological adequacy (generating different morphological features in different contexts), fluency (word agreement) and certainty (generating the same morphological features in different contexts), respectively. Evaluation is based on automatic morphological analysis of the MT output. (Sennrich, 2017) proposed a method to construct the test suite automatically for evaluating Engl"
L18-1005,W15-0903,0,0.0200246,"ace realizations with literal language uses (Salton et al., 2014b). Idioms are considered highly problematic for a wide variety of NLP tasks (Sag et al., 2002). This belief also holds true for machine translation, because MT systems often make the assumption that meaning is compositional, which is not true for idioms. The compositionality assumption leads to literal translation errors, the word-byword translation of idioms, resulting in a translation that is confusing and not understandable. Therefore, idiom translation is a hard problem in MT and has attracted considerable research interest (Cap et al., 2015; Salton et al., 2014b; Anastasiou, 2010). Given the difficulty of idiom translation in MT, it would be helpful to have a method to evaluate idiom translation performance. There is a wide range of methods for evaluating the performance of MT systems, but none of them are satisfactory for the targeted evaluation of idiom translation. The most straightforward method is human evaluation. While human evaluation is highly valuable, it is desirable to develop complementary automatic methods that are low-cost and fast, thus allowing for more rapid and frequent feedback cycles. Popular automatic MT me"
L18-1005,N10-1029,0,0.0181006,"chieves about half the BLEU score of the same system when applied to sentences that do not contain idioms. Among all the translation errors caused by idioms, literal translation errors are believed to be an important error type. (Manojlovic et al., 2017) demonstrated that literal translations predominate in the output of a phrase-based English↔Croatian MT system when translating sentences with idioms. According to our preliminary observations, literal translation errors also occur often in state-of-the-art Chinese→English NMT systems. In order to improve the performance of idiom translation, (Carpuat and Diab, 2010) investigate two strategies: treating idioms and multiword expressions as an atomic unit, and adding a phrase-level feature that identifies multiword expressions. They find that both strategies improve the translation of non-compositional expressions. (Salton et al., 2014b) propose a substitution method that replaces idioms in the source sentence with their literal meaning before translation; after translation, the translation of the literal meaning is replaced with a target language idiom, if possible. 1. Automatic construction, automatic evaluation: large 32 3. Blacklist Method and only need"
L18-1005,D17-1263,0,0.0868153,"t suite methods construct a set of sentences that focus on specific types of difficulties in MT. Typically, we design a set of sentences in the source language for the MT system to translate, and a scoring method to evaluate the translations. The sentence set and the scoring method are designed so that the score assigned to a system indicates the system’s performance on the focused difficulty. This kind of methods makes up for the drawbacks of global evaluation metrics that they cannot assess a system’s performance on specific issues. There are many previous works belonging to this category. (Isabelle et al., 2017) proposed a challenge set approach to evaluate English→French MT systems’ performance on divergence problems. The English sentences in the challenge set are chosen so that their closest French equivalent will be structurally divergent from them in some crucial way. (Burchardt et al., 2017) constructed a test suite for English→German MT systems. This test suite covers a wide variety of linguistic phenomena, such as ambiguity, composition, function words, multi-word expressions and so on. (Burlot and Yvon, 2017) introduced a new scheme to evaluate the performance of English→MRL (morphologically"
L18-1005,L16-1147,0,0.0268605,"on the list. The blacklist consists of the direct translation of the characters mentioned in the last step. 3. Gather source language (Chinese) sentences containing idioms on the list. Note that the method itself does not need reference translations. Nevertheless, if someone is not a speaker of the source language but wishes to get some ideas about the detected literal translation errors, or to check whether the detection is correct, then using translation pairs is more desirable than monolingual sentences. Translation Pairs The translation pairs were extracted from OpenSubtitles2016 dataset (Lison and Tiedemann, 2016), where we searched for Chinese→English translation pairs with idioms on our list. In order to balance the frequency of all the idioms in the translation pairs, preventing the majority being taken up by only a few idioms, we restricted the maximum occurrences of any idiom to be 40. Under such restrictions, we extracted a total of 1194 translation pairs. 4. Feed all the sentences to the MT system to get the translations. 5. Calculate the percentage of translations triggering the blacklist, which is the evaluation score for the system. We draw on an existing idiom list for step 1, and perform st"
L18-1528,abdelali-etal-2014-amara,0,0.150329,"Missing"
L18-1528,D09-1030,0,0.139864,"t properties. Achieving high-quality machine translation in these conditions therefore requires significant amounts of in-domain data for training and testing. Creating such data by hiring professional translators would be expensive, especially considering that it would have to be done for eleven target languages. Therefore we turn our attention to crowdsourcing as a cost-saving alternative. The impact of non-expert input, i.e. translations provided by non-professional translators, on the development and the evaluation of machine translation engines has been investigated in previous research (Callison-Burch, 2009; Zaidan and Callison-Burch, 2011; Ambati, 2012). Crowdsourcing has been used to this end, and the main research concern has been whether input by the general crowd, that has no expertise in linguistics or translation studies, can improve the quality of large-scale machine In this work we use crowdsourcing with carefully designed quality controls to collect translations of MOOC material from English to the eleven project target languages. We combine this data with pre-existing indomain educational data and we use it to build translation systems in a transfer learning approach: we first train n"
L18-1528,2012.eamt-1.60,0,0.16079,"Missing"
L18-1528,2005.mtsummit-papers.11,0,0.12737,"Missing"
L18-1528,W17-3204,0,0.035658,"hat we have access to varying amounts of pre-existing data that we treat as “in-domain”, with varying distance to our actual target domain. Despite all confounding variables, if we consider that the amount of crowdsourced training data is 100-1,000 times smaller than the amount of out-of-domain training data used, and consistently smaller than the amount of preexisting in-domain data, we conclude that the improvements that we observe from adding crowdsourced data cannot just be attributed to having more training data available. Based on the log-linear learning curves reported in related work (Koehn and Knowles, 2017; Miceli Barone et al. 2017), we would expect small or negligible improvements in translation quality if we added the same amount of out-of-domain training data, or pre-existing indomain data, to the systems without crowdsourced data. This confirms the relevance of obtaining in-domain training data that is similar in terms of domain and genre 3346 to the texts that are to be translated, and that the crowdsourced training data is of high value to the MT system. 4. Conclusion We collected crowdsourced translations from English to eleven languages to create a parallel corpus for the educational d"
L18-1528,L16-1003,1,0.854754,"Missing"
L18-1528,D17-1156,1,0.895442,"ystems Our baseline translation systems are GRU attentive sequence-to-sequence neural machine translation models (Bahdanau, 2015). For training, we used the same configuration as the Edinburgh’s submission to the WMT-17 news translation task (Sennrich et al., 2017), which provides a strong baseline. 6 3345 https://translate.yandex.ru/corpus Figure 1: Number of trusted segments collected for each target language for training and testing/tuning. We adapt the baseline systems to the in-domain MOOC data using continued training of the baseline system with MAP-L2 and dropout regularization (Miceli Barone et al. 2017). 3.4 Results Evaluation results are shown in Table 3. We can see that domain adaptation via fine-tuning is effective for all language pairs. Baseline + Preexisting + Crowdsourced en-bg 22.91 23.57 25.89 en-cs 29.86 31.06 32.06 en-de 29.29 32.14 33.69 en-el 35.54 38.01 40.76 en-hr 23.36 23.70 26.43 en-it 32.15 36.19 38.53 en-nl 35.59 38.04 40.07 en-pl 27.16 28.41 30.97 en-pt 39.44 47.68 48.71 en-ru 26.41 29.08 29.78 en-zh 27.93 28.51 29.77 avg 29.97 32.40 34.24 Table 3: Translation quality (BLEU) of baseline system, and systems adapted to domain with/without crowdsourced data. Using only pre-e"
L18-1528,L18-1075,1,0.862976,"Missing"
L18-1528,steinberger-etal-2012-dgt,0,0.0514095,"Missing"
L18-1528,steinberger-etal-2006-jrc,0,0.140295,"Missing"
L18-1528,tiedemann-2012-parallel,0,0.137194,"Missing"
L18-1528,P11-1122,0,0.394868,"Missing"
L18-1528,L16-1561,0,0.0380301,"Missing"
N16-1005,2015.iwslt-papers.5,0,0.0232567,"2) have used a bilingual English–German corpus to automatically annotate the T-V distinction, and train a classifier to predict the address from monolingual English text. Applying a source-side classifier is potential future work, although we note that the baseline encoder– decoder NMT system already has some disambiguating power. Our T-V classification is more comprehensive, including more pronoun forms and imperative verbs. Previous research on neural language models has proposed including various types of extra information, such as topic, genre or document context (Mikolov and Zweig, 2012; Aransa et al., 2015; Ji et al., 2015; Wang and Cho, 2015). Our method is somewhat similar, with the main novel idea being that we can target specific phenomena, such as honorifics, via an automatic annotation of the target side of a parallel corpus. On the modelling side, our method is slightly different in that we pass the extra information to the encoder of an encoder–decoder network, rather than the (decoder) hidden layer or output layer. We found this to be very effective, but trying different architectures is potential future work. In rule-based machine translation, user options to control the level of poli"
N16-1005,D14-1179,0,0.022336,"Missing"
N16-1005,E12-1064,0,0.0189134,"LEU indicates that the T-V distinction is relevant for translation. We expect that the actual relevance for humans depends on the task. For gisting, we expect the T-V distinction to have little effect on comprehensibility. For professional translation that uses MT with postediting, producing the desired honorifics is likely to improve post-editing speed and satisfaction. In an evaluation of MT for subtitle translation, Etchegoyhen et al. (2014) highlight the production of the appropriate T-V form as “a limitation of MT technology” that was “often frustrat[ing]” to post-editors. 6 Related Work Faruqui and Pado (2012) have used a bilingual English–German corpus to automatically annotate the T-V distinction, and train a classifier to predict the address from monolingual English text. Applying a source-side classifier is potential future work, although we note that the baseline encoder– decoder NMT system already has some disambiguating power. Our T-V classification is more comprehensive, including more pronoun forms and imperative verbs. Previous research on neural language models has proposed including various types of extra information, such as topic, genre or document context (Mikolov and Zweig, 2012; Ar"
N16-1005,P15-1001,0,0.153295,"describe rules to automatically annotate the T-V distinction in German text. • we describe how to use target-side T-V annotation in NMT training to control the level of politeness at test time via side constraints. • we perform oracle experiments to demonstrate the impact of controlling politeness in NMT. 2 Background: Neural Machine Translation Attentional neural machine translation (Bahdanau et al., 2015) is the current state of the art for 35 Proceedings of NAACL-HLT 2016, pages 35–40, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics English→German (Jean et al., 2015b; Luong and Manning, 2015). We follow the neural machine translation architecture by Bahdanau et al. (2015), which we will briefly summarize here. However, our approach is not specific to this architecture. The neural machine translation system is implemented as an attentional encoder-decoder network. The encoder is a bidirectional neural network with gated recurrent units (Cho et al., 2014) that reads an input sequence x = (x1 , ..., xm ) and calculates − → −→ a forward sequence of hidden states (h1 , ..., hm ), ← − ←− and a backward sequence (h1 , ..., hm ). The hidden → − ← − states hj and"
N16-1005,2015.iwslt-evaluation.11,0,0.149954,"Missing"
N16-1005,I05-6011,0,0.0321512,"roach is simple and applicable to a wide range of NMT architectures and our experiments suggest that the incorporation of the side constraint as an extra source token is very effective. 4 Automatic Training Set Annotation Our approach relies on annotating politeness in the training set to obtain the politeness feature which we discussed previously. We choose a sentence-level annotation because a target-side honorific may have no word-level correspondence in the source. We will discuss the annotation of German as an example, but our method could be applied to other languages, such as Japanese (Nariyama et al., 2005). German has distinct pronoun forms for informal and polite address, as shown in Table 1. A further difference between informal and polite speech are imperative verbs, and the original imperative forms are considered informal. The polite alternative is to use 3rd person plural forms with subject in position 2: • Ruf mich zurück. (informal) (Call me back.) • Rufen Sie mich zurück. (polite) (Call you me back.) We automatically annotate politeness on a sentence level with rules based on a morphosyntactic annotation by ParZu (Sennrich et al., 2013). Sentences containing imperative verbs are labell"
N16-1005,R13-1079,1,0.389147,"Missing"
N16-1005,tiedemann-2012-parallel,0,0.0240309,"olite forms and (neutral) 3rd person forms by their capitalization. If a sentence matches rules for both classes, we label it as informal – we found that our lowestprecision rule is the annotation of sentence-initial Sie. All sentences without a match are considered neutral. 5 Evaluation Our empirical research questions are as follows: • can we control the production of honorifics in neural machine translation via side constraints? • how important is the T-V distinction for translation quality (as measured by B LEU)? 5.1 Data and Methods We perform English→German experiments on OpenSubtitles (Tiedemann, 2012)1 , a parallel corpus of movie subtitles. Machine translation is commonly used in the professional translation of movie subtitles in a post-editing workflow, and politeness is considered an open problem for subtitle translation (Etchegoyhen et al., 2014). We use OpenSubtitles2012 as training corpus, and random samples from OpenSubtitles2013 for testing. The training corpus consists of of 5.58 million sentence pairs, out of which we label 0.48 million sentence pairs as polite, and 1.09 million as informal. We train an attentional encoder-decoder NMT system using Groundhog2 (Bahdanau et al., 201"
N16-1005,P16-1162,1,\N,Missing
N18-1118,W16-2348,1,0.898526,"Missing"
N18-1118,W09-2404,0,0.253424,"Missing"
N18-1118,L16-1100,0,0.184876,"Missing"
N18-1118,W16-2360,0,0.064602,"Missing"
N18-1118,D17-1263,0,0.069335,"Missing"
N18-1118,E17-3017,1,0.865828,"Missing"
N18-1118,P16-1162,1,0.644258,"Missing"
N18-1118,P17-2031,0,0.068079,"Missing"
N18-1118,L16-1147,0,0.126825,"Missing"
N18-1118,W17-4811,0,0.207132,"Missing"
N18-1118,D17-1301,0,0.230729,"Missing"
N18-1118,N16-1004,0,0.0681389,"Missing"
N18-1118,P02-1040,0,0.10402,"Missing"
N18-1118,W17-4702,1,0.90194,"Missing"
P13-1082,2012.amta-papers.4,0,0.0499181,"ur clustering experiments, the development set is the concatenation of the LEGAL and IT development sets. However, we always use the gold segmentation between LEGAL and IT for MERT and testing. This allows for a detailed analysis of the effect of development data clustering for the purpose of model adaptation. In an unlabelled setting, one would have to run MERT either on the full development set (as we will do for the CZ–EN task) or separately on each cluster, or use an alternative approach to optimize log-linear weights in a multidomain setting, such as feature augmentation as described by (Clark et al., 2012). For both data sets, language models are trained on the target side of the bitexts. In all experiments, we keep the number of component models constant: 12 for EN–DE, 7 for CZ–EN. We vary the number of clusters k from 1, which corresponds to adapting the models to the full development set, to 16. The baseline is the concatenation of all train837 system baseline 1 cluster (no split) 2 clusters 4 clusters 8 clusters 16 clusters gold clusters TM adaptation 34.4 34.5 34.6 34.7* 34.7* 34.7* 35.0* LM adaptation 34.4 33.7 34.0 34.3 34.5 34.7* 35.0* TM+LM adaptation 34.4 34.1 34.4 34.6 34.9* 35.0* 35"
P13-1082,W07-0717,0,0.36588,"e present an architecture that delays the computation of translation model features until decoding, allowing for the application of mixture-modeling techniques at decoding time. We also describe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1 B LEU over unadapted systems and single-domain adaptation. 1 Introduction The effectiveness of domain adaptation approaches such as mixture-modeling (Foster and Kuhn, 2007) has been established, and has led to research on a wide array of adaptation techniques in SMT, for instance (Matsoukas et al., 2009; Shah et al., 2012). In all these approaches, adaptation is performed during model training, with respect to a representative development corpus, and the models are kept unchanged when then system is deployed. Therefore, when working with multiple and/or unlabelled domains, domain adaptation is often impractical for a number of reasons. Firstly, maintaining multiple systems for each language pair, each adapted to a different domain, is costly 2 Related Work (Orti"
P13-1082,N03-1017,0,0.0071177,"on of translation model features to the decoding phase, and allow for multiple knowledge sources (e.g. bitexts or user-provided data) to contribute to their calculation. Our immediate purpose for this paper is domain adaptation in a multi-domain environment, but the delay of the feature computation has other potential applications, e.g. in interactive MT. We are concerned with calculating four features during decoding, henceforth just referred to as the translation model features: p(s|t), lex(s|t), p(t|s) and lex(t|s). s and t denote the source and target phrase. We follow the definitions in (Koehn et al., 2003). Traditionally, the phrase translation probabilities p(s|t) and p(t|s) are estimated through unsmoothed maximum likelihood estimation (MLE). weight the contribution of each component model to the feature calculation. The similarity suggests that our framework could also be used for interactive learning, with the ability to learn a model incrementally from user feedback, and weight it differently than the static models, opening new research opportunities. (Sennrich, 2012b) perform instance weighting of translation models, based on the sufficient statistics. Our framework implements this idea,"
P13-1082,P07-2045,0,0.00955353,"ta set eu fiction navajo news paraweb subtitles techdoc total (train) dev test sentences 1 270 000 830 000 30 000 110 000 370 000 2 840 000 970 000 6 420 000 3500 3500 words (en) 25 600 000 13 700 000 490 000 2 550 000 3 930 000 21 200 000 7 270 000 74 700 000 50 700 49 600 Table 3: Parallel data sets Czech–English. pass decoding, with an unadapted language model in the first phase, and rescoring with a language model adapted online, could perform adequately, and keep the complexity independent of the number of clusters. 5 5.1 Evaluation Data and Methods We conduct all experiments with Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). Log-linear weights are optimized using MERT (Och and Ney, 2003). We keep the word alignment and lexical reordering models constant through the experiments to minimize the number of confounding factors. We report translation quality using B LEU (Papineni et 5 If the development set is labelled, one can also use a gold segmentation of development sets instead of k-means clustering. At decoding time, cluster assignment can be performed by automatically assigning each sentence to the closest centroid, or again through gold labels, if availab"
P13-1082,D09-1074,0,0.52833,"mixture-modeling techniques at decoding time. We also describe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1 B LEU over unadapted systems and single-domain adaptation. 1 Introduction The effectiveness of domain adaptation approaches such as mixture-modeling (Foster and Kuhn, 2007) has been established, and has led to research on a wide array of adaptation techniques in SMT, for instance (Matsoukas et al., 2009; Shah et al., 2012). In all these approaches, adaptation is performed during model training, with respect to a representative development corpus, and the models are kept unchanged when then system is deployed. Therefore, when working with multiple and/or unlabelled domains, domain adaptation is often impractical for a number of reasons. Firstly, maintaining multiple systems for each language pair, each adapted to a different domain, is costly 2 Related Work (Ortiz-Mart´ınez et al., 2010) delay the computation of translation model features for the purpose of interactive machine translation wit"
P13-1082,2010.amta-papers.16,0,0.386013,"Missing"
P13-1082,J03-1002,0,0.00272636,"total (train) dev test sentences 1 270 000 830 000 30 000 110 000 370 000 2 840 000 970 000 6 420 000 3500 3500 words (en) 25 600 000 13 700 000 490 000 2 550 000 3 930 000 21 200 000 7 270 000 74 700 000 50 700 49 600 Table 3: Parallel data sets Czech–English. pass decoding, with an unadapted language model in the first phase, and rescoring with a language model adapted online, could perform adequately, and keep the complexity independent of the number of clusters. 5 5.1 Evaluation Data and Methods We conduct all experiments with Moses (Koehn et al., 2007), SRILM (Stolcke, 2002), and GIZA++ (Och and Ney, 2003). Log-linear weights are optimized using MERT (Och and Ney, 2003). We keep the word alignment and lexical reordering models constant through the experiments to minimize the number of confounding factors. We report translation quality using B LEU (Papineni et 5 If the development set is labelled, one can also use a gold segmentation of development sets instead of k-means clustering. At decoding time, cluster assignment can be performed by automatically assigning each sentence to the closest centroid, or again through gold labels, if available. 836 system baseline 1 cluster (no split) 2 clusters"
P13-1082,N10-1079,0,0.0175553,"Missing"
P13-1082,W12-3102,0,0.0477569,"Missing"
P13-1082,P02-1040,0,0.11158,"Missing"
P13-1082,P11-2031,0,0.0162457,"ers gold clusters TM adaptation IT LEGAL 21.1 49.9 21.3* 49.9 21.6* 49.9 21.7* 49.9 22.1* 49.9 21.1 49.9 21.8* 50.1* LM adaptation IT LEGAL 21.1 49.9 21.8* 49.7 22.2* 50.4* 23.1* 50.2* 23.1* 50.1* 22.6* 50.3* 22.4* 50.1* TM+LM adaptation IT LEGAL 21.1 49.9 21.8* 49.8 22.8* 50.2* 22.6* 50.2* 22.7* 50.3* 21.9* 50.1* 23.2* 49.9 Table 4: Translation experiments EN–DE. B LEU scores reported. Data set kde kdedoc kdegb oo oo3 php tm acquis dgt ecb ep7 nc7 al., 2002). We account for optimizer instability by running 3 independent MERT runs per system, and performing significance testing with MultEval (Clark et al., 2011). Systems significantly better than the baseline with p < 0.01 are marked with (*). We conduct experiments on two data sets. The first is an English–German translation task with two domains, texts related to information technology (IT) and legal documents (LEGAL). We use data sets from both domains, plus out-of-domain corpora, as shown in table 2. 7 data sets come from the domain IT: 6 from OPUS (Tiedemann, 2009) and a translation memory (tm) provided by our industry partner. 3 data sets are from the legal domain: the ECB corpus from OPUS, plus the JRCAcquis (Steinberger et al., 2006) and DGT-"
P13-1082,P12-1099,0,0.0641363,"eight the contribution of each component model to the feature calculation. The similarity suggests that our framework could also be used for interactive learning, with the ability to learn a model incrementally from user feedback, and weight it differently than the static models, opening new research opportunities. (Sennrich, 2012b) perform instance weighting of translation models, based on the sufficient statistics. Our framework implements this idea, with the main difference that the actual combination is delayed until decoding, to support adaptation to multiple domains in a single system. (Razmara et al., 2012) describe an ensemble decoding framework which combines several translation models in the decoding step. Our work is similar to theirs in that the combination is done at runtime, but we also delay the computation of translation model probabilities, and thus have access to richer sufficient statistics. In principle, our architecture can support all mixture operations that (Razmara et al., 2012) describe, plus additional ones such as forms of instance weighting, which are not possible after the translation probabilities have been computed. (Banerjee et al., 2010) focus on the problem of domain i"
P13-1082,2012.eamt-1.43,1,0.814142,"ures: p(s|t), lex(s|t), p(t|s) and lex(t|s). s and t denote the source and target phrase. We follow the definitions in (Koehn et al., 2003). Traditionally, the phrase translation probabilities p(s|t) and p(t|s) are estimated through unsmoothed maximum likelihood estimation (MLE). weight the contribution of each component model to the feature calculation. The similarity suggests that our framework could also be used for interactive learning, with the ability to learn a model incrementally from user feedback, and weight it differently than the static models, opening new research opportunities. (Sennrich, 2012b) perform instance weighting of translation models, based on the sufficient statistics. Our framework implements this idea, with the main difference that the actual combination is delayed until decoding, to support adaptation to multiple domains in a single system. (Razmara et al., 2012) describe an ensemble decoding framework which combines several translation models in the decoding step. Our work is similar to theirs in that the combination is done at runtime, but we also delay the computation of translation model probabilities, and thus have access to richer sufficient statistics. In princ"
P13-1082,E12-1055,1,0.846285,"ures: p(s|t), lex(s|t), p(t|s) and lex(t|s). s and t denote the source and target phrase. We follow the definitions in (Koehn et al., 2003). Traditionally, the phrase translation probabilities p(s|t) and p(t|s) are estimated through unsmoothed maximum likelihood estimation (MLE). weight the contribution of each component model to the feature calculation. The similarity suggests that our framework could also be used for interactive learning, with the ability to learn a model incrementally from user feedback, and weight it differently than the static models, opening new research opportunities. (Sennrich, 2012b) perform instance weighting of translation models, based on the sufficient statistics. Our framework implements this idea, with the main difference that the actual combination is delayed until decoding, to support adaptation to multiple domains in a single system. (Razmara et al., 2012) describe an ensemble decoding framework which combines several translation models in the decoding step. Our work is similar to theirs in that the combination is done at runtime, but we also delay the computation of translation model probabilities, and thus have access to richer sufficient statistics. In princ"
P13-1082,2012.amta-papers.21,1,0.867317,"ues at decoding time. We also describe a method for unsupervised adaptation with development and test data from multiple domains. Experimental results on two language pairs demonstrate the effectiveness of both our translation model architecture and automatic clustering, with gains of up to 1 B LEU over unadapted systems and single-domain adaptation. 1 Introduction The effectiveness of domain adaptation approaches such as mixture-modeling (Foster and Kuhn, 2007) has been established, and has led to research on a wide array of adaptation techniques in SMT, for instance (Matsoukas et al., 2009; Shah et al., 2012). In all these approaches, adaptation is performed during model training, with respect to a representative development corpus, and the models are kept unchanged when then system is deployed. Therefore, when working with multiple and/or unlabelled domains, domain adaptation is often impractical for a number of reasons. Firstly, maintaining multiple systems for each language pair, each adapted to a different domain, is costly 2 Related Work (Ortiz-Mart´ınez et al., 2010) delay the computation of translation model features for the purpose of interactive machine translation with online training. T"
P13-1082,steinberger-etal-2006-jrc,0,0.0294506,"with MultEval (Clark et al., 2011). Systems significantly better than the baseline with p < 0.01 are marked with (*). We conduct experiments on two data sets. The first is an English–German translation task with two domains, texts related to information technology (IT) and legal documents (LEGAL). We use data sets from both domains, plus out-of-domain corpora, as shown in table 2. 7 data sets come from the domain IT: 6 from OPUS (Tiedemann, 2009) and a translation memory (tm) provided by our industry partner. 3 data sets are from the legal domain: the ECB corpus from OPUS, plus the JRCAcquis (Steinberger et al., 2006) and DGT-TM (Steinberger et al., 2012). 2 data sets are out-ofdomain, made available by the 2012 Workshop on Statistical Machine Translation (Callison-Burch et al., 2012). The development sets are random samples from the respective in-domain bitexts (heldout from training). The test sets have been provided by Translated, our industry partner in the M ATE C AT project. λIT 1.0 0.64 1.6 0.76 1.8 0.79 1.3 0.024 0.053 0.071 0.037 0.1 λLEGAL 1.0 12.0 2.3 1.6 4.7 6.3 1.3 3.5 4.5 2.3 0.53 1.1 λcluster 1 1.0 86.0 1.7 0.73 2.4 0.69 1.5 0.018 0.033 0.039 0.024 0.063 λcluster 2 1.0 6.4 2.7 1.7 2.7 3.5 1."
P13-1082,steinberger-etal-2012-dgt,0,0.00728276,"Missing"
P13-1082,D07-1054,0,0.0206694,"n model probabilities, and thus have access to richer sufficient statistics. In principle, our architecture can support all mixture operations that (Razmara et al., 2012) describe, plus additional ones such as forms of instance weighting, which are not possible after the translation probabilities have been computed. (Banerjee et al., 2010) focus on the problem of domain identification in a multi-domain setting. They use separate translation systems for each domain, and a supervised setting, whereas we aim for a system that integrates support for multiple domains, with or without supervision. (Yamamoto and Sumita, 2007) propose unsupervised clustering at both training and decoding time. The training text is divided into a number of clusters, a model is trained on each, and during decoding, each sentence is assigned to the closest cluster-specific model. Our approach bears resemblance to this clustering, but is different in that Yamamoto and Sumita assign each sentence to the closest model, and use this model for decoding, whereas in our approach, each cluster is associated with a mixture of models that is optimized to the cluster, and the number of clusters need not be equal to the number of component models"
P16-1009,W09-0432,0,0.169739,"obtain another improvement of 0.8–1.0 B LEU. Back-translation Quality for Synthetic Data One question that our previous experiments leave open is how the quality of the automatic backtranslation affects training with synthetic data. To investigate this question, we back-translate the same German monolingual corpus with three different German→English systems: • with our baseline system and greedy decoding 4.3 Contrast to Phrase-based SMT The back-translation of monolingual target data into the source language to produce synthetic parallel text has been previously explored for phrasebased SMT (Bertoldi and Federico, 2009; Lambert et al., 2011). While our approach is technically similar, synthetic parallel data fulfills novel roles • with our baseline system and beam search (beam size 12). This is the same system used for the experiments in Table 3. 10 We also experimented with higher ratios of monolingual data, but this led to decreased B LEU scores. 91 name training data baseline (Gülçehre et al., 2015) deep fusion (Gülçehre et al., 2015) baseline parallel parallelsynth parallel/parallelsynth Gigawordmono parallel/Gigawordmono Gigawordsynth parallel/Gigawordsynth instances tst2011 18.4 20.2 18.6 19.9 18.8 21"
P16-1009,P15-1001,0,0.225572,"quences of subword units (Sennrich et al., 2016), and can represent any additional training data with the existing network vocabulary that was learned on the parallel data. In all experiments, the network vocabulary remains fixed. 4.1.1 We evaluate NMT training on parallel text, and with additional monolingual data, on English↔German and Turkish→English, using training and test data from WMT 15 for English↔German, IWSLT 15 for English→German, and IWSLT 14 for Turkish→English. Data and Methods We use Groundhog3 as the implementation of the NMT system for all experiments (Bahdanau et al., 2015; Jean et al., 2015a). We generally follow the settings and training procedure described by Sennrich et al. (2016). For English↔German, we report case-sensitive B LEU on detokenized text with mteval-v13a.pl for comparison to official WMT and IWSLT results. For Turkish→English, we report case-sensitive B LEU on tokenized text with multi-bleu.perl for comparison to results by Gülçehre et al. (2015). Gülçehre et al. (2015) determine the network vocabulary based on the parallel training data, 3 English↔German We use all parallel training data provided by WMT 2015 (Bojar et al., 2015)4 . We use the News Crawl corpora"
P16-1009,J90-2002,0,0.474829,"Missing"
P16-1009,2012.eamt-1.60,0,0.0565125,"n et al., 2012). We also use early stopping, based on B LEU measured every three hours on tst2010, which we treat as development set. For Turkish→English, we use gradient clipping with threshold 5, following Gülçehre et al. (2015), in contrast to the threshold 1 that we use for English↔German, following Jean et al. (2015a). Table 2: Turkish→English training data. cabulary size is 90 000. We also perform experiments on the IWSLT 15 test sets to investigate a cross-domain setting.5 The test sets consist of TED talk transcripts. As indomain training data, IWSLT provides the WIT3 parallel corpus (Cettolo et al., 2012), which also consists of TED talks. 4.1.2 4.2 4.2.1 Results English→German WMT 15 Table 3 shows English→German results with WMT training and test data. We find that mixing parallel training data with monolingual data with a dummy source side in a ratio of 1-1 improves quality by 0.4–0.5 B LEU for the single system, 1 B LEU for the ensemble. We train the system for twice as long as the baseline to provide the training algorithm with a similar amount of parallel training instances. To ensure that the quality improvement is due to the monolingual training instances, and not just increased trainin"
P16-1009,P07-2045,1,0.0662497,"Table 8: Phrase-based SMT results (English→German) on WMT test sets (average of newstest201{4,5}), and IWSLT test sets (average of tst201{3,4,5}), and average B LEU gain from adding synthetic data for both PBSMT and NMT. 6 4 2 0 5 10 15 20 training time (training instances 25 30 ·106 ) Figure 1: Turkish→English training and development set (tst2010) cross-entropy as a function of training time (number of training instances) for different systems. in NMT. To explore the relative effectiveness of backtranslated data for phrase-based SMT and NMT, we train two phrase-based SMT systems with Moses (Koehn et al., 2007), using only WMTparallel , or both WMTparallel and WMTsynth_de for training the translation and reordering model. Both systems contain the same language model, a 5-gram Kneser-Ney model trained on all available WMT data. We use the baseline features described by Haddow et al. (2015). Results are shown in Table 8. In phrase-based SMT, we find that the use of back-translated training data has a moderate positive effect on the WMT test sets (+0.7 B LEU), but not on the IWSLT test sets. This is in line with the expectation that the main effect of back-translated data for phrasebased SMT is domain"
P16-1009,W11-2132,0,0.0362203,"f 0.8–1.0 B LEU. Back-translation Quality for Synthetic Data One question that our previous experiments leave open is how the quality of the automatic backtranslation affects training with synthetic data. To investigate this question, we back-translate the same German monolingual corpus with three different German→English systems: • with our baseline system and greedy decoding 4.3 Contrast to Phrase-based SMT The back-translation of monolingual target data into the source language to produce synthetic parallel text has been previously explored for phrasebased SMT (Bertoldi and Federico, 2009; Lambert et al., 2011). While our approach is technically similar, synthetic parallel data fulfills novel roles • with our baseline system and beam search (beam size 12). This is the same system used for the experiments in Table 3. 10 We also experimented with higher ratios of monolingual data, but this led to decreased B LEU scores. 91 name training data baseline (Gülçehre et al., 2015) deep fusion (Gülçehre et al., 2015) baseline parallel parallelsynth parallel/parallelsynth Gigawordmono parallel/Gigawordmono Gigawordsynth parallel/Gigawordsynth instances tst2011 18.4 20.2 18.6 19.9 18.8 21.2 7.2m 6m/6m 7.6m/7.6m"
P16-1009,D14-1179,0,0.0846514,"Missing"
P16-1009,2015.iwslt-evaluation.11,0,0.370228,"MorphTR 6 89 name training instances syntax-based (Sennrich and Haddow, 2015) Neural MT (Jean et al., 2015b) parallel 37m (parallel) +monolingual 49m (parallel) / 49m (monolingual) +synthetic 44m (parallel) / 36m (synthetic) B LEU newstest2014 newstest2015 single ens-4 single ens-4 22.6 24.4 22.4 19.9 20.4 22.8 23.6 20.4 21.4 23.2 24.6 22.7 23.8 25.7 26.5 Table 3: English→German translation performance (B LEU) on WMT training/test sets. Ens-4: ensemble of 4 models. Number of training instances varies due to differences in training time and speed. name 1 2 3 4 5 fine-tuning data instances NMT (Luong and Manning, 2015) (single model) NMT (Luong and Manning, 2015) (ensemble of 8) parallel +synthetic 2+WITmono_de WMTparallel / WITmono 200k/200k 2+WITsynth_de WITsynth 200k 2+WITparallel WIT 200k tst2013 29.4 31.4 25.2 26.5 26.6 28.2 30.4 B LEU tst2014 27.6 22.6 23.5 23.6 24.4 25.9 tst2015 30.1 24.0 25.5 25.4 26.7 28.4 Table 4: English→German translation performance (B LEU) on IWSLT test sets (TED talks). Single models. test sets, which are news texts. We investigate if monolingual training data is especially valuable if it can be used to adapt a model to a new genre or domain, specifically adapting a system tr"
P16-1009,D15-1166,0,0.659364,"lgorithm with a similar amount of parallel training instances. To ensure that the quality improvement is due to the monolingual training instances, and not just increased training time, we also continued training our baseline system for another week, but saw no improvements in B LEU. Including synthetic data during training is very effective, and yields an improvement over our baseline by 2.8–3.4 B LEU. Our best ensemble system also outperforms a syntax-based baseline (Sennrich and Haddow, 2015) by 1.2–2.1 B LEU. We also substantially outperform NMT results reported by Jean et al. (2015a) and Luong et al. (2015), who previously reported SOTA result.8 We note that the difference is particularly large for single systems, since our ensemble is not as diverse as that of Luong et al. (2015), who used 8 independently trained ensemble components, whereas we sampled 4 ensemble components from the same training run. Turkish→English We use data provided for the IWSLT 14 machine translation track (Cettolo et al., 2014), namely the WIT3 parallel corpus (Cettolo et al., 2012), which consists of TED talks, and the SETimes corpus (Tyers and Alperen, 2010).6 After removal of sentence pairs which contain empty lines"
P16-1009,W15-3013,1,0.372853,"06 ) Figure 1: Turkish→English training and development set (tst2010) cross-entropy as a function of training time (number of training instances) for different systems. in NMT. To explore the relative effectiveness of backtranslated data for phrase-based SMT and NMT, we train two phrase-based SMT systems with Moses (Koehn et al., 2007), using only WMTparallel , or both WMTparallel and WMTsynth_de for training the translation and reordering model. Both systems contain the same language model, a 5-gram Kneser-Ney model trained on all available WMT data. We use the baseline features described by Haddow et al. (2015). Results are shown in Table 8. In phrase-based SMT, we find that the use of back-translated training data has a moderate positive effect on the WMT test sets (+0.7 B LEU), but not on the IWSLT test sets. This is in line with the expectation that the main effect of back-translated data for phrasebased SMT is domain adaptation (Bertoldi and Federico, 2009). Both the WMT test sets and the News Crawl corpora which we used as monolingual data come from the same source, a web crawl of newspaper articles.11 In contrast, News Crawl is out-of-domain for the IWSLT test sets. In contrast to phrase-based"
P16-1009,N06-1020,0,0.0227411,"parameters are tuned on further parallel training data, but the language model parameters are fixed during the finetuning stage. Jean et al. (2015b) also report on experiments with reranking of NMT output with a 5-gram language model, but improvements are small (between 0.1–0.5 B LEU). The production of synthetic parallel texts bears resemblance to data augmentation techniques used in computer vision, where datasets are often augmented with rotated, scaled, or otherwise distorted variants of the (limited) training set (Rowley et al., 1996). Another similar avenue of research is selftraining (McClosky et al., 2006; Schwenk, 2008). The main difference is that self-training typically refers to scenario where the training set is enhanced with training instances with artificially produced output labels, whereas we start with human-produced output (i.e. the translation), and artificially produce an input. We expect that this is more robust towards noise in the automatic translation. Improving NMT with monolingual source data, following similar work on phrasebased SMT (Schwenk, 2008), remains possible future work. Domain adaptation of neural networks via continued training has been shown to be effective for"
P16-1009,2008.iwslt-papers.6,0,0.00923665,"n further parallel training data, but the language model parameters are fixed during the finetuning stage. Jean et al. (2015b) also report on experiments with reranking of NMT output with a 5-gram language model, but improvements are small (between 0.1–0.5 B LEU). The production of synthetic parallel texts bears resemblance to data augmentation techniques used in computer vision, where datasets are often augmented with rotated, scaled, or otherwise distorted variants of the (limited) training set (Rowley et al., 1996). Another similar avenue of research is selftraining (McClosky et al., 2006; Schwenk, 2008). The main difference is that self-training typically refers to scenario where the training set is enhanced with training instances with artificially produced output labels, whereas we start with human-produced output (i.e. the translation), and artificially produce an input. We expect that this is more robust towards noise in the automatic translation. Improving NMT with monolingual source data, following similar work on phrasebased SMT (Schwenk, 2008), remains possible future work. Domain adaptation of neural networks via continued training has been shown to be effective for neural language"
P16-1009,D15-1248,1,0.477278,"r the single system, 1 B LEU for the ensemble. We train the system for twice as long as the baseline to provide the training algorithm with a similar amount of parallel training instances. To ensure that the quality improvement is due to the monolingual training instances, and not just increased training time, we also continued training our baseline system for another week, but saw no improvements in B LEU. Including synthetic data during training is very effective, and yields an improvement over our baseline by 2.8–3.4 B LEU. Our best ensemble system also outperforms a syntax-based baseline (Sennrich and Haddow, 2015) by 1.2–2.1 B LEU. We also substantially outperform NMT results reported by Jean et al. (2015a) and Luong et al. (2015), who previously reported SOTA result.8 We note that the difference is particularly large for single systems, since our ensemble is not as diverse as that of Luong et al. (2015), who used 8 independently trained ensemble components, whereas we sampled 4 ensemble components from the same training run. Turkish→English We use data provided for the IWSLT 14 machine translation track (Cettolo et al., 2014), namely the WIT3 parallel corpus (Cettolo et al., 2012), which consists of T"
P16-1009,P16-1162,1,0.590859,"monolingual data set into English. The German→English system used for this is the baseline system (parallel). Translation took about a week on an NVIDIA Titan Black GPU. For experiments in German→English, we back-translate 4 200 000 monolingual English sentences into German, using the English→German system +synthetic. Note that we always use single models for backtranslation, not ensembles. We leave it to future work to explore how sensitive NMT training with synthetic data is to the quality of the backtranslation. We tokenize and truecase the training data, and represent rare words via BPE (Sennrich et al., 2016). Specifically, we follow Sennrich et al. (2016) in performing BPE on the joint vocabulary with 89 500 merge operations. The network voEvaluation 4.1 sentences 4 200 000 200 000 160 000 000 3 600 000 118 000 000 4 200 000 4 github.com/sebastien-j/LV_groundhog 88 http://www.statmt.org/wmt15/ dataset WIT SETimes Gigawordmono Gigawordsynth sentences 160 000 160 000 177 000 000 3 200 000 We found overfitting to be a bigger problem than with the larger English↔German data set, and follow Gülçehre et al. (2015) in using Gaussian noise (stddev 0.01) (Graves, 2011), and dropout on the output layer (p="
P16-1009,W15-4006,0,0.0304229,"Missing"
P16-1162,D15-1249,0,0.114088,": 2, &apos;n e w e s t &lt;/w&gt;&apos;:6, &apos;w i d e s t &lt;/w&gt;&apos;:3} num_merges = 10 for i in range(num_merges): pairs = get_stats(vocab) best = max(pairs, key=pairs.get) vocab = merge_vocab(best, vocab) print(best) r· lo lo w e r· → → → → r· lo low er· Figure 1: BPE merge operations learned from dictionary {‘low’, ‘lowest’, ‘newer’, ‘wider’}. gorithm 1. In practice, we increase efficiency by indexing all pairs, and updating data structures incrementally. The main difference to other compression algorithms, such as Huffman encoding, which have been proposed to produce a variable-length encoding of words for NMT (Chitnis and DeNero, 2015), is that our symbol sequences are still interpretable as subword units, and that the network can generalize to translate and produce new words (unseen at training time) on the basis of these subword units. Figure 1 shows a toy example of learned BPE operations. At test time, we first split words into sequences of characters, then apply the learned operations to merge the characters into larger, known symbols. This is applicable to any word, and allows for open-vocabulary networks with fixed symbol vocabularies.3 In our example, the OOV ‘lower’ would be segmented into ‘low er·’. 3 The only sym"
P16-1162,D14-1179,0,0.315754,"Missing"
P16-1162,W02-0603,0,0.0880881,"t of the k most frequent word types unsegmented. Only the unigram representation is truly open-vocabulary. However, the unigram representation performed poorly in preliminary experiments, and we report translation results with a bigram representation, which is empirically better, but unable to produce some tokens in the test set with the training set vocabulary. We report statistics for several word segmentation techniques that have proven useful in previous SMT research, including frequency-based compound splitting (Koehn and Knight, 2003), rulebased hyphenation (Liang, 1983), and Morfessor (Creutz and Lagus, 2002). We find that they only moderately reduce vocabulary size, and do not solve the unknown word problem, and we thus find them unsuitable for our goal of open-vocabulary translation without back-off dictionary. BPE meets our goal of being open-vocabulary, and the learned merge operations can be applied to the test set to obtain a segmentation with no unknown symbols.10 Its main difference from the character-level model is that the more compact representation of BPE allows for shorter sequences, and that the attention model operates on variable-length units.11 Table 1 shows BPE with 59 500 merge"
P16-1162,E14-4029,0,0.0798953,"ranslated independently, our NMT models show robustness towards oversplitting. 2 Subword Translation The main motivation behind this paper is that the translation of some words is transparent in 1716 pothesis in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an"
P16-1162,N13-1073,0,0.0814933,"s), and continue training each with a fixed embedding layer (as suggested by (Jean et al., 2015)) for 12 hours. We perform two independent training runs for each models, once with cut-off for gradient clipping (Pascanu et al., 2013) of 5.0, once with a cut-off of 1.0 – the latter produced better single models for most settings. We report results of the system that performed best on our development set (newstest2013), and of an ensemble of all 8 models. We use a beam size of 12 for beam search, with probabilities normalized by sentence length. We use a bilingual dictionary based on fast-align (Dyer et al., 2013). For our baseline, this serves as back-off dictionary for rare words. We also use the dictionary to speed up translation for all experiments, only performing the softmax over a filtered list of candidate translations (like Jean et al. (2015), we use K = 30000; K 0 = 10). 4.1 Subword statistics Apart from translation quality, which we will verify empirically, our main objective is to represent an open vocabulary through a compact fixed-size subword vocabulary, and allow for efficient training and decoding.8 Statistics for different segmentations of the Ger6 Clipped unigram precision is essenti"
P16-1162,W15-3013,1,0.108755,"ased system in terms of B LEU, but not in terms of CHR F3. Regarding other neural systems, Luong et al. (2015a) report a B LEU score of 25.9 on newstest2015, but we note that they use an ensemble of 8 independently trained models, and also report strong improvements from applying dropout, which we did not use. We are confident that our improvements to the translation of rare words are orthogonal to improvements achievable through other improvements in the network archi1720 tecture, training algorithm, or better ensembles. For English→Russian, the state of the art is the phrase-based system by Haddow et al. (2015). It outperforms our WDict baseline by 1.5 B LEU. The subword models are a step towards closing this gap, and BPE-J90k yields an improvement of 1.3 B LEU, and 2.0 CHR F3, over WDict. As a further comment on our translation results, we want to emphasize that performance variability is still an open problem with NMT. On our development set, we observe differences of up to 1 B LEU between different models. For single systems, we report the results of the model that performs best on dev (out of 8), which has a stabilizing effect, but how to control for randomness deserves further attention in futu"
P16-1162,P15-1001,0,0.284282,"z o.o. Samsung R&D Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the German Abwasser|behandlungs|anlange ‘sewage water treatment plant’, for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector. For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up (Jean et al., 2015; Luong et al., 2015b). We note that such techniques make assumptions that often do not hold true in practice. For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example. Also, word-level models are unable to translate or generate unseen words. Copying unknown words into the target text, as done by (Jean et al., 2015; Luong et al., 2015b), is a reasonable strategy for names, but morphological changes and transliteration is often required, e"
P16-1162,D13-1176,0,0.0821851,"ansliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 B LEU, respectively. 1 Introduction Neural machine translation has recently shown impressive results (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). However, the translation of rare words is an open problem. The vocabulary of neural models is typically limited to 30 000–50 000 words, but translation is an open-vocabulary probThe research presented in this publication was conducted in cooperation with Samsung Electronics Polska sp. z o.o. Samsung R&D Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the"
P16-1162,E03-1076,0,0.106965,"ch. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmen"
P16-1162,P07-2045,1,0.138461,"ranslation of rare and unseen words in neural machine translation by representing them via subword units? • Which segmentation into subword units performs best in terms of vocabulary size, text size, and translation quality? We perform experiments on data from the shared translation task of WMT 2015. For English→German, our training set consists of 4.2 million sentence pairs, or approximately 100 million tokens. For English→Russian, the training set consists of 2.6 million sentence pairs, or approximately 50 million tokens. We tokenize and truecase the data with the scripts provided in Moses (Koehn et al., 2007). We use newstest2013 as development set, and report results on newstest2014 and newstest2015. We report results with B LEU (mteval-v13a.pl), and CHR F3 (Popovi´c, 2015), a character n-gram F3 score which was found to correlate well with 4 In practice, we simply concatenate the source and target side of the training set to learn joint BPE. 5 Since the Russian training text also contains words that use the Latin alphabet, we also apply the Latin BPE operations. 1718 human judgments, especially for translations out of English (Stanojevi´c et al., 2015). Since our main claim is concerned with the"
P16-1162,D15-1176,0,0.0929974,"resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multilingual algorithms have been proposed (Snyder and Barzilay, 2008). We find these intriguing, but inapplicable at test time. Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015a; Kim et al., 2015). An effort to apply such techniques to NMT, parallel to ours, has found no significant improvement over word-based approaches (Ling et al., 2015b). One technical difference from our work is that the attention mechanism still operates on the level of words in the model by Ling et al. (2015b), and that the representation of each word is fixed-length. We expect that the attention mechanism benefits from our variable-length representation: the network can learn to place attention on different subword units at each step. Recall our introductory example Abwasserbehandlungsanlang"
P16-1162,W13-3512,0,0.0897464,"Missing"
P16-1162,D15-1166,0,0.660487,"Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the German Abwasser|behandlungs|anlange ‘sewage water treatment plant’, for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector. For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up (Jean et al., 2015; Luong et al., 2015b). We note that such techniques make assumptions that often do not hold true in practice. For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example. Also, word-level models are unable to translate or generate unseen words. Copying unknown words into the target text, as done by (Jean et al., 2015; Luong et al., 2015b), is a reasonable strategy for names, but morphological changes and transliteration is often required, especially if alphabe"
P16-1162,P15-1002,0,0.130638,"Institute Poland. lem, and especially for languages with productive word formation processes such as agglutination and compounding, translation models require mechanisms that go below the word level. As an example, consider compounds such as the German Abwasser|behandlungs|anlange ‘sewage water treatment plant’, for which a segmented, variable-length representation is intuitively more appealing than encoding the word as a fixed-length vector. For word-level NMT models, the translation of out-of-vocabulary words has been addressed through a back-off to a dictionary look-up (Jean et al., 2015; Luong et al., 2015b). We note that such techniques make assumptions that often do not hold true in practice. For instance, there is not always a 1-to-1 correspondence between source and target words because of variance in the degree of morphological synthesis between languages, like in our introductory compounding example. Also, word-level models are unable to translate or generate unseen words. Copying unknown words into the target text, as done by (Jean et al., 2015; Luong et al., 2015b), is a reasonable strategy for names, but morphological changes and transliteration is often required, especially if alphabe"
P16-1162,P12-1018,0,0.0565115,"s in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspe"
P16-1162,C00-2162,0,0.0990133,"ct of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables."
P16-1162,W15-3049,0,0.534349,"Missing"
P16-1162,E12-1015,0,0.0627825,"ignment model αij , which models the probability that yi is aligned to xj . The alignment model is a singlelayer feedforward neural network that is learned jointly with the rest of the network through backpropagation. A detailed description can be found in (Bahdanau et al., 2015). Training is performed on a parallel corpus with stochastic gradient descent. For translation, a beam search with small beam size is employed. 3 • cognates and loanwords. Cognates and loanwords with a common origin can differ in regular ways between languages, so that character-level translation rules are sufficient (Tiedemann, 2012). Example: claustrophobia (English) Klaustrophobie (German) Êëàóñòðîôîáèÿ (Klaustrofobiâ) (Russian) Neural Machine Translation • morphologically complex words. Words containing multiple morphemes, for instance formed via compounding, affixation, or inflection, may be translatable by translating the morphemes separately. Example: solar system (English) Sonnensystem (Sonne + System) (German) Naprendszer (Nap + Rendszer) (Hungarian) In an analysis of 100 rare tokens (not among the 50 000 most frequent types) in our German training data1 , the majority of tokens are potentially translatable from E"
P16-1162,W07-0705,0,0.0774644,"words is transparent in 1716 pothesis in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best"
P16-1162,2007.mtsummit-papers.65,0,0.105199,"f unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multiling"
P16-1162,D15-1248,1,0.126623,"Missing"
P16-1162,P08-1084,0,0.0964265,"ion algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multilingual algorithms have been proposed (Snyder and Barzilay, 2008). We find these intriguing, but inapplicable at test time. Various techniques have been proposed to produce fixed-length continuous word vectors based on characters or morphemes (Luong et al., 2013; Botha and Blunsom, 2014; Ling et al., 2015a; Kim et al., 2015). An effort to apply such techniques to NMT, parallel to ours, has found no significant improvement over word-based approaches (Ling et al., 2015b). One technical difference from our work is that the attention mechanism still operates on the level of words in the model by Ling et al. (2015b), and that the representation of each word is f"
P16-1162,P12-2063,0,0.075538,"es, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subword units may be taskspecific. For speech recognition, phone-level language models have been used (Bazzi and Glass, 2000). Mikolov et al. (2012) investigate subword language models, and propose to use syllables. For multilingual segmentation tasks, multilingual algorithms have been"
P16-1162,W15-3031,0,0.0681316,"Missing"
P16-1162,2009.eamt-1.3,0,0.0688114,"t in 1716 pothesis in Sections 4 and 5. First, we discuss different subword representations. 3.1 Related Work For Statistical Machine Translation (SMT), the translation of unknown words has been the subject of intensive research. A large proportion of unknown words are names, which can just be copied into the target text if both languages share an alphabet. If alphabets differ, transliteration is required (Durrani et al., 2014). Character-based translation has also been investigated with phrase-based models, which proved especially successful for closely related languages (Vilar et al., 2007; Tiedemann, 2009; Neubig et al., 2012). The segmentation of morphologically complex words such as compounds is widely used for SMT, and various algorithms for morpheme segmentation have been investigated (Nießen and Ney, 2000; Koehn and Knight, 2003; Virpioja et al., 2007; Stallard et al., 2012). Segmentation algorithms commonly used for phrase-based SMT tend to be conservative in their splitting decisions, whereas we aim for an aggressive segmentation that allows for open-vocabulary translation with a compact network vocabulary, and without having to resort to back-off dictionaries. The best choice of subwor"
P18-1117,N18-1118,1,0.741682,"reasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using context-aware NMT models, has demonstrated to yield performance improvements, it is still not clear what kinds of discourse phenomena are successfully handled by the NMT systems and, importantly, how they are modeled. Understanding this would inform development of future discourse-aware NMT models, as it will suggest what kind of inductive biases need to be encoded in the architecture or which linguistic features need to be exploited. In our work we aim to enhance our understanding of the modelling of selected discourse phenomena in NMT. To this end, we const"
P18-1117,W09-2404,0,0.13593,"99; Hardmeier, 2012). Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter app"
P18-1117,D12-1026,0,0.029012,"se phenomena is important in translation (Mitkov, 1999; Hardmeier, 2012). Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2"
P18-1117,D11-1084,0,0.33727,"nded context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using context-aware NMT models"
P18-1117,2010.iwslt-papers.10,0,0.64861,"nation of the context and source sentences (+0.6). 1 Introduction It has long been argued that handling discourse phenomena is important in translation (Mitkov, 1999; Hardmeier, 2012). Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in i"
P18-1117,P14-5010,0,0.0941084,"are relatively sparse in a general-purpose test set, and previous work has designed targeted evaluation of pronoun translation (Hardmeier et al., 2015; Miculicich Werlen and Popescu-Belis, 2017; Bawden et al., 2018). However, we note that in Russian, grammatical gender is not only marked on pronouns, but also on adjectives and verbs. Rather than using a pronoun-specific evaluation, we present results with BLEU on test sets where we hypothesize context to be relevant, specifically sentences containing co-referential pronouns. We feed Stanford CoreNLP open-source coreference resolution system (Manning et al., 2014a) with pairs of sentences to find examples where there is a link between one of the pronouns under consideration and the context. We focus on anaphoric instances of “it” (this excludes, among others, pleonastic uses of ”it”), and instances of the pronouns “I”, “you”, and “yours” that are coreferent with an expression in the previous sentence. All these pronouns express ambiguity in the translation into Russian, and the model has learned to attend to context for their translation (Table 2). To combat data sparsity, the test sets are extracted from large amounts of held-out data of OpenSubtitle"
P18-1117,D13-1037,0,0.0258483,"cts anaphora while CoreNLP fails. Nevertheless, there is room for improvement, and improving the attention component is likely to boost translation performance. 6 Related work Our analysis focuses on how our context-aware neural model implicitly captures anaphora. Early work on anaphora phenomena in statistical machine translation has relied on external systems for coreference resolution (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010). Results 1271 were mixed, and the low performance of coreference resolution systems was identified as a problem for this type of system. Later work by Hardmeier et al. (2013) has shown that cross-lingual pronoun prediction systems can implicitly learn to resolve coreference, but this work still relied on external feature extraction to identify anaphora candidates. Our experiments show that a contextaware neural machine translation system can implicitly learn coreference phenomena without any feature engineering. Tiedemann and Scherrer (2017) and Bawden et al. (2018) analyze the attention weights of context-aware NMT models. Tiedemann and Scherrer (2017) find some evidence for aboveaverage attention on contextual history for the translation of pronouns, and our ana"
P18-1117,2012.amta-papers.20,0,0.0268581,"been argued that handling discourse phenomena is important in translation (Mitkov, 1999; Hardmeier, 2012). Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al.,"
P18-1117,E14-1035,0,0.0790868,"Missing"
P18-1117,D17-1195,0,0.0494454,"Missing"
P18-1117,W10-1737,0,0.304292,"Missing"
P18-1117,L18-1275,0,0.2103,"e phenomena are successfully handled by the NMT systems and, importantly, how they are modeled. Understanding this would inform development of future discourse-aware NMT models, as it will suggest what kind of inductive biases need to be encoded in the architecture or which linguistic features need to be exploited. In our work we aim to enhance our understanding of the modelling of selected discourse phenomena in NMT. To this end, we construct a simple discourse-aware model, demonstrate that it achieves improvements over the discourse-agnostic baseline on an English-Russian subtitles dataset (Lison et al., 2018) and study which context information is being captured in the model. Specifically, we start with the Trans1264 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1264–1274 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics former (Vaswani et al., 2017), a state-of-the-art model for context-agnostic NMT, and modify it in such way that it can handle additional context. In our model, a source sentence and a context sentence are first encoded independently, and then a single attention layer, in a comb"
P18-1117,W17-4802,0,0.148428,"short sentences, context does not seem disproportionally more useful for these sentences. 5.3 Analysis of pronoun translation The analysis of the attention model indicates that the model attends heavily to the contextual history for the translation of some pronouns. Here, we investigate whether this context-aware modelling results in empirical improvements in translation Ambiguous pronouns and translation quality Ambiguous pronouns are relatively sparse in a general-purpose test set, and previous work has designed targeted evaluation of pronoun translation (Hardmeier et al., 2015; Miculicich Werlen and Popescu-Belis, 2017; Bawden et al., 2018). However, we note that in Russian, grammatical gender is not only marked on pronouns, but also on adjectives and verbs. Rather than using a pronoun-specific evaluation, we present results with BLEU on test sets where we hypothesize context to be relevant, specifically sentences containing co-referential pronouns. We feed Stanford CoreNLP open-source coreference resolution system (Manning et al., 2014a) with pairs of sentences to find examples where there is a link between one of the pronouns under consideration and the context. We focus on anaphoric instances of “it” (th"
P18-1117,W05-0908,0,0.021397,"Missing"
P18-1117,P16-1162,1,0.553356,"nce) context encoder (next sentence) context encoder (random context) sentences, to let the shared layers know whether it is encoding a source or a context sentence. 4 Experiments 4.1 Data and setting We use the publicly available OpenSubtitles2018 corpus (Lison et al., 2018) for English and Russian.1 As described in the appendix, we apply data cleaning and randomly choose 2 million training instances from the resulting data. For development and testing, we randomly select two subsets of 10000 instances from movies not encountered in training.2 Sentences were encoded using byte-pair encoding (Sennrich et al., 2016), with source and target vocabularies of about 32000 tokens. We generally used the same parameters and optimizer as in the original Transformer (Vaswani et al., 2017). The hyperparameters, preprocessing and training details are provided in the supplementary material. 5 Results and analysis We start by experiments motivating the setting and verifying that the improvements are indeed genuine, i.e. they come from inducing predictive features of the context. In subsequent section 5.2, we analyze the features induced by the context encoder and perform error analysis. 5.1 Overall performance We use"
P18-1117,P12-1048,0,0.0435972,"ntence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using context-aware NMT models, has demonstrated to yield performanc"
P18-1117,W10-2602,0,0.126031,"2012). Using extended context, beyond the single source sentence, should in principle be beneficial in ambiguous cases and also ensure that generated translations are coherent. Nevertheless, machine translation systems typically ignore discourse phenomena and translate sentences in isolation. Earlier research on this topic focused on handling specific phenomena, such as translating pronouns (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Hardmeier et al., 2015), discourse connectives (Meyer et al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using cont"
P18-1117,W17-4811,0,0.50249,"tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using context-aware NMT models, has demonstrated to yield performance improvements, it is still not clear what kinds of discourse phenomena are successfully handled by the NMT systems and, importantly, how they are modeled. Understanding this would inform development of future discourse-aware NMT models, as it will suggest what kind of inductive biases need to be encoded in the architecture or which linguistic features need to be exploited. In our work we aim to enhance our understanding of the modelling of selected discourse phenomena in NMT."
P18-1117,D17-1301,0,0.115635,"t al., 2012), verb tense (Gong et al., 2012), increasing lexical consistency (Carpuat, 2009; Tiedemann, 2010; Gong et al., 2011), or topic adaptation (Su et al., 2012; Hasler et al., 2014), with special-purpose features engineered to model these phenomena. However, with traditional statistical machine translation being largely supplanted with neural machine translation (NMT) models trained in an end-toend fashion, an alternative is to directly provide additional context to an NMT system at training time and hope that it will succeed in inducing relevant predictive features (Jean et al., 2017; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden et al., 2018). While the latter approach, using context-aware NMT models, has demonstrated to yield performance improvements, it is still not clear what kinds of discourse phenomena are successfully handled by the NMT systems and, importantly, how they are modeled. Understanding this would inform development of future discourse-aware NMT models, as it will suggest what kind of inductive biases need to be encoded in the architecture or which linguistic features need to be exploited. In our work we aim to enhance our understanding of the modelling of select"
P18-1117,N16-1114,0,0.0253566,"ubstantial improvements in performance on these subsets. By comparing attention distributions induced by our model against co-reference links, we conclude that the model implicitly captures coreference phenomena, even without having any kind of specialized features which could help it in this subtask. These observations also suggest potential directions for future work. For example, effective co-reference systems go beyond relying simply on embeddings of contexts. One option would be to integrate ‘global’ features summarizing properties of groups of mentions predicted as linked in a document (Wiseman et al., 2016), or to use latent relations to trace en• we introduce a context-aware neural model, which is effective and has a sufficiently simple and interpretable interface between the context and the rest of the translation model; • we analyze the flow of information from the context and identify pronoun translation as the key phenomenon captured by the model; • by comparing to automatically predicted or human-annotated coreference relations, we observe that the model implicitly captures anaphora. Neural Machine Translation Given a source sentence x = (x1 , x2 , . . . , xS ) and a target sentence y = (y"
P19-1021,D18-1399,0,0.0457339,"G¨ulc¸ehre et al., 2015) to the training of parts of the NMT model with additional objectives, including a language modelling objective (G¨ulc¸ehre et al., 2015; Sennrich et al., 2016b; Ramachandran et al., 2017), an autoencoding objective (Luong et al., 2016; Currey et al., 2017), or a round-trip objective, where the model is trained to predict monolingual (target-side) training data that has been back-translated into the source language (Sennrich et al., 2016b; He et al., 2016; Cheng et al., 2016). As an extreme case, models that rely exclusively on monolingual data have been shown to work (Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a; Lample et al., 2018b). Similarly, parallel data from other language pairs can be used to pre-train the network or jointly learn representations (Zoph et al., 2016; Chen et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018; Gu et al., 2018a,b; Kocmi and Bojar, 2018). 3.2 Language Representation Subword representations such as BPE (Sennrich et al., 2016c) have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-r"
P19-1021,D18-1549,0,0.031334,"G¨ulc¸ehre et al., 2015) to the training of parts of the NMT model with additional objectives, including a language modelling objective (G¨ulc¸ehre et al., 2015; Sennrich et al., 2016b; Ramachandran et al., 2017), an autoencoding objective (Luong et al., 2016; Currey et al., 2017), or a round-trip objective, where the model is trained to predict monolingual (target-side) training data that has been back-translated into the source language (Sennrich et al., 2016b; He et al., 2016; Cheng et al., 2016). As an extreme case, models that rely exclusively on monolingual data have been shown to work (Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a; Lample et al., 2018b). Similarly, parallel data from other language pairs can be used to pre-train the network or jointly learn representations (Zoph et al., 2016; Chen et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018; Gu et al., 2018a,b; Kocmi and Bojar, 2018). 3.2 Language Representation Subword representations such as BPE (Sennrich et al., 2016c) have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-r"
P19-1021,P16-1185,0,0.0160684,"s. Methods to improve NMT with monolingual data range from the integration of a separately trained language model (G¨ulc¸ehre et al., 2015) to the training of parts of the NMT model with additional objectives, including a language modelling objective (G¨ulc¸ehre et al., 2015; Sennrich et al., 2016b; Ramachandran et al., 2017), an autoencoding objective (Luong et al., 2016; Currey et al., 2017), or a round-trip objective, where the model is trained to predict monolingual (target-side) training data that has been back-translated into the source language (Sennrich et al., 2016b; He et al., 2016; Cheng et al., 2016). As an extreme case, models that rely exclusively on monolingual data have been shown to work (Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a; Lample et al., 2018b). Similarly, parallel data from other language pairs can be used to pre-train the network or jointly learn representations (Zoph et al., 2016; Chen et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018; Gu et al., 2018a,b; Kocmi and Bojar, 2018). 3.2 Language Representation Subword representations such as BPE (Sennrich et al., 2016c) have become a popular choice to achieve open-vocabulary translation. BPE h"
P19-1021,D18-1398,0,0.144414,"p objective, where the model is trained to predict monolingual (target-side) training data that has been back-translated into the source language (Sennrich et al., 2016b; He et al., 2016; Cheng et al., 2016). As an extreme case, models that rely exclusively on monolingual data have been shown to work (Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a; Lample et al., 2018b). Similarly, parallel data from other language pairs can be used to pre-train the network or jointly learn representations (Zoph et al., 2016; Chen et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018; Gu et al., 2018a,b; Kocmi and Bojar, 2018). 3.2 Language Representation Subword representations such as BPE (Sennrich et al., 2016c) have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-resource settings, the effect of vocabulary size on translation quality is relatively small; Haddow et al. (2018) report mixed results when comparing vocabularies of 30k and 90k subwords. In low-resource settings, large vocabularies result in low-frequency (sub)words being represented as atom"
P19-1021,N12-1047,0,0.0653774,"90 000 parallel sentences of training data, 1000 for development, and 2000 for testing. For both PBSMT and NMT, we apply the same tokenization and truecasing using Moses scripts. For NMT, we also learn BPE subword segmentation with 30 000 merge operations, shared between German and English, and independently for Korean→English. 4.2 PBSMT Baseline We use Moses (Koehn et al., 2007) to train a PBSMT system. We use MGIZA (Gao and Vogel, 2008) to train word alignments, and lmplz (Heafield et al., 2013) for a 5-gram LM. Feature weights are optimized on the dev set to maximize BLEU with batch MIRA (Cherry and Foster, 2012) – we perform multiple runs where indicated. Unlike Koehn and Knowles (2017), we do not use extra data for the LM. Both PBSMT and NMT can benefit from monolingual data, so the availability of monolingual data is no longer an exclusive advantage of PBSMT (see 2.2). 2 Implementation released in Nematus: https://github.com/EdinburghNLP/nematus 3 https://sites.google.com/site/ koreanparalleldata/ 213 BLEU ID system 100k 3.2M 1 phrase-based SMT 15.87 ± 0.19 26.60 ± 0.00 2 NMT baseline 0.00 ± 0.00 25.70 ± 0.33 3 2 + ”mainstream improvements” (dropout, tied embeddings, layer normalization, bideep RNN"
P19-1021,W18-6412,1,0.840637,"ata from other language pairs can be used to pre-train the network or jointly learn representations (Zoph et al., 2016; Chen et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018; Gu et al., 2018a,b; Kocmi and Bojar, 2018). 3.2 Language Representation Subword representations such as BPE (Sennrich et al., 2016c) have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-resource settings, the effect of vocabulary size on translation quality is relatively small; Haddow et al. (2018) report mixed results when comparing vocabularies of 30k and 90k subwords. In low-resource settings, large vocabularies result in low-frequency (sub)words being represented as atomic units at training time, and the ability to learn good high-dimensional representations of these is doubtful. Sennrich et al. (2017a) propose a minimum frequency threshold for subword units, and splitting any less frequent subword into smaller units or characters. We expect that such a threshold reduces the need to carefully tune the vocabulary size to the dataset, leading to more aggressive segmentation on smaller"
P19-1021,D18-1461,0,0.0304434,"ler datasets. Previous work has argued for larger batch sizes in NMT (Morishita et al., 2017; Neishi et al., 2017), but we More broadly, this line of research still accepts the premise that NMT models are data-inefficient and require large amounts of auxiliary data to train. In this work, we want to re-visit this point, and will focus on techniques to make more efficient use of small amounts of parallel training data. Low-resource NMT without auxiliary data has received less attention; work in this direction ¨ includes (Ostling and Tiedemann, 2017; Nguyen and Chiang, 2018). 1 In related work, Cherry et al. (2018) have shown that, given deep encoders and decoders, character-level models can outperform other subword segmentations. In preliminary experiments, a character-level model performed poorly in our low-resource setting. 212 find that using smaller batches is beneficial in lowresource settings. More aggressive dropout, including dropping whole words at random (Gal and Ghahramani, 2016), is also likely to be more important. We report results on a narrow hyperparameter search guided by previous work and our own intuition. 3.4 subword vocabulary Lexical Model Finally, we implement and test the lexica"
P19-1021,W17-5708,0,0.0312774,"sed methods is impaired when languages are morphologically different, or when training domains do not match (Søgaard et al., 2018) 3.3 Hyperparameter Tuning Due to long training times, hyperparameters are hard to optimize by grid search, and are often re-used across experiments. However, best practices differ between high-resource and lowresource settings. While the trend in high-resource settings is towards using larger and deeper models, Nguyen and Chiang (2018) use smaller and fewer layers for smaller datasets. Previous work has argued for larger batch sizes in NMT (Morishita et al., 2017; Neishi et al., 2017), but we More broadly, this line of research still accepts the premise that NMT models are data-inefficient and require large amounts of auxiliary data to train. In this work, we want to re-visit this point, and will focus on techniques to make more efficient use of small amounts of parallel training data. Low-resource NMT without auxiliary data has received less attention; work in this direction ¨ includes (Ostling and Tiedemann, 2017; Nguyen and Chiang, 2018). 1 In related work, Cherry et al. (2018) have shown that, given deep encoders and decoders, character-level models can outperform othe"
P19-1021,D18-1103,0,0.0327773,"2017), or a round-trip objective, where the model is trained to predict monolingual (target-side) training data that has been back-translated into the source language (Sennrich et al., 2016b; He et al., 2016; Cheng et al., 2016). As an extreme case, models that rely exclusively on monolingual data have been shown to work (Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a; Lample et al., 2018b). Similarly, parallel data from other language pairs can be used to pre-train the network or jointly learn representations (Zoph et al., 2016; Chen et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018; Gu et al., 2018a,b; Kocmi and Bojar, 2018). 3.2 Language Representation Subword representations such as BPE (Sennrich et al., 2016c) have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-resource settings, the effect of vocabulary size on translation quality is relatively small; Haddow et al. (2018) report mixed results when comparing vocabularies of 30k and 90k subwords. In low-resource settings, large vocabularies result in low-frequency (sub)words being re"
P19-1021,W18-6325,0,0.0611661,"he model is trained to predict monolingual (target-side) training data that has been back-translated into the source language (Sennrich et al., 2016b; He et al., 2016; Cheng et al., 2016). As an extreme case, models that rely exclusively on monolingual data have been shown to work (Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a; Lample et al., 2018b). Similarly, parallel data from other language pairs can be used to pre-train the network or jointly learn representations (Zoph et al., 2016; Chen et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018; Gu et al., 2018a,b; Kocmi and Bojar, 2018). 3.2 Language Representation Subword representations such as BPE (Sennrich et al., 2016c) have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-resource settings, the effect of vocabulary size on translation quality is relatively small; Haddow et al. (2018) report mixed results when comparing vocabularies of 30k and 90k subwords. In low-resource settings, large vocabularies result in low-frequency (sub)words being represented as atomic units at training time,"
P19-1021,N18-1031,0,0.0886393,"iveness depends on the availability of large amounts of suitable auxiliary data, and other conditions being met. For example, the effectiveness of unsupervised methods is impaired when languages are morphologically different, or when training domains do not match (Søgaard et al., 2018) 3.3 Hyperparameter Tuning Due to long training times, hyperparameters are hard to optimize by grid search, and are often re-used across experiments. However, best practices differ between high-resource and lowresource settings. While the trend in high-resource settings is towards using larger and deeper models, Nguyen and Chiang (2018) use smaller and fewer layers for smaller datasets. Previous work has argued for larger batch sizes in NMT (Morishita et al., 2017; Neishi et al., 2017), but we More broadly, this line of research still accepts the premise that NMT models are data-inefficient and require large amounts of auxiliary data to train. In this work, we want to re-visit this point, and will focus on techniques to make more efficient use of small amounts of parallel training data. Low-resource NMT without auxiliary data has received less attention; work in this direction ¨ includes (Ostling and Tiedemann, 2017; Nguyen"
P19-1021,P07-2045,0,0.0160856,"e data cleanup and train/dev split as Ranzato et al. (2016), resulting in 159 000 parallel sentences of training data, and 7584 for development. As a second language pair, we evaluate our systems on a Korean–English dataset3 with around 90 000 parallel sentences of training data, 1000 for development, and 2000 for testing. For both PBSMT and NMT, we apply the same tokenization and truecasing using Moses scripts. For NMT, we also learn BPE subword segmentation with 30 000 merge operations, shared between German and English, and independently for Korean→English. 4.2 PBSMT Baseline We use Moses (Koehn et al., 2007) to train a PBSMT system. We use MGIZA (Gao and Vogel, 2008) to train word alignments, and lmplz (Heafield et al., 2013) for a 5-gram LM. Feature weights are optimized on the dev set to maximize BLEU with batch MIRA (Cherry and Foster, 2012) – we perform multiple runs where indicated. Unlike Koehn and Knowles (2017), we do not use extra data for the LM. Both PBSMT and NMT can benefit from monolingual data, so the availability of monolingual data is no longer an exclusive advantage of PBSMT (see 2.2). 2 Implementation released in Nematus: https://github.com/EdinburghNLP/nematus 3 https://sites."
P19-1021,I17-2050,0,0.0417057,"l., 2016; Currey et al., 2017), or a round-trip objective, where the model is trained to predict monolingual (target-side) training data that has been back-translated into the source language (Sennrich et al., 2016b; He et al., 2016; Cheng et al., 2016). As an extreme case, models that rely exclusively on monolingual data have been shown to work (Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a; Lample et al., 2018b). Similarly, parallel data from other language pairs can be used to pre-train the network or jointly learn representations (Zoph et al., 2016; Chen et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018; Gu et al., 2018a,b; Kocmi and Bojar, 2018). 3.2 Language Representation Subword representations such as BPE (Sennrich et al., 2016c) have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-resource settings, the effect of vocabulary size on translation quality is relatively small; Haddow et al. (2018) report mixed results when comparing vocabularies of 30k and 90k subwords. In low-resource settings, large vocabularies result in low-frequenc"
P19-1021,W17-3204,0,0.535773,"rms SMT at about 15 million words, and even beats a SMT system with a big 2 billion word in-domain language model unNMT, evaluating their importance with der high-resource conditions. Introduction While neural machine translation (NMT) has achieved impressive performance in high-resource data conditions, becoming dominant in the field (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017), recent research has argued that these models are highly data-inefficient, and underperform phrase-based statistical machine translation (PBSMT) or unsupervised methods in low-data conditions (Koehn and Knowles, 2017; Lample et al., 2018b). In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. Our main contributions are as follows: 7 Spanish was last represented in 2013, are we used data from tems if few parallel resources available. In 8 http://statmt.org/wmt13/translation-task.html https://github.com/rse both papers, NMT systems are trained with hyperparameters that are typical for high-resource set31 • we explore best practices for low-resource 211 Proceedings of the 57th Annual Meeting of the Association for C"
P19-1021,P02-1040,0,0.104777,"E→EN data, and for KO→EN data. hlt = tanh(W ftl ) + ftl 4 sentences To simulate different amounts of training resources, we randomly subsample the IWSLT training corpus 5 times, discarding half of the data at each step. Truecaser and BPE segmentation are learned on the full training corpus; as one of our experiments, we set the frequency threshold for subword units to 10 in each subcorpus (see 3.2). Table 1 shows statistics for each subcorpus, including the subword vocabulary. Translation outputs are detruecased, detokenized, and compared against the reference with cased BLEU using sacreBLEU (Papineni et al., 2002; Post, 2018).4 Like Ranzato et al. (2016), we report BLEU on the concatenated dev sets for IWSLT 2014 (tst2010, tst2011, tst2012, dev2010, dev2012). Experiments Data and Preprocessing We use the TED data from the IWSLT 2014 German→English shared translation task (Cettolo et al., 2014). We use the same data cleanup and train/dev split as Ranzato et al. (2016), resulting in 159 000 parallel sentences of training data, and 7584 for development. As a second language pair, we evaluate our systems on a Korean–English dataset3 with around 90 000 parallel sentences of training data, 1000 for developm"
P19-1021,J82-2005,0,0.669096,"Missing"
P19-1021,W18-6319,0,0.0908715,"N data. hlt = tanh(W ftl ) + ftl 4 sentences To simulate different amounts of training resources, we randomly subsample the IWSLT training corpus 5 times, discarding half of the data at each step. Truecaser and BPE segmentation are learned on the full training corpus; as one of our experiments, we set the frequency threshold for subword units to 10 in each subcorpus (see 3.2). Table 1 shows statistics for each subcorpus, including the subword vocabulary. Translation outputs are detruecased, detokenized, and compared against the reference with cased BLEU using sacreBLEU (Papineni et al., 2002; Post, 2018).4 Like Ranzato et al. (2016), we report BLEU on the concatenated dev sets for IWSLT 2014 (tst2010, tst2011, tst2012, dev2010, dev2012). Experiments Data and Preprocessing We use the TED data from the IWSLT 2014 German→English shared translation task (Cettolo et al., 2014). We use the same data cleanup and train/dev split as Ranzato et al. (2016), resulting in 159 000 parallel sentences of training data, and 7584 for development. As a second language pair, we evaluate our systems on a Korean–English dataset3 with around 90 000 parallel sentences of training data, 1000 for development, and 2000"
P19-1021,E17-2025,0,0.0509998,"e network architectures, to optimize NMT for low-resource conditions. 2.2 3 Methods for Low-Resource Neural Machine Translation 3.1 Mainstream Improvements We consider the hyperparameters used by Koehn and Knowles (2017) to be our baseline. This baseline does not make use of various advances in NMT architectures and training tricks. In contrast to the baseline, we use a BiDeep RNN architecture (Miceli Barone et al., 2017), label smoothing (Szegedy et al., 2016), dropout (Srivastava et al., 2014), word dropout (Sennrich et al., 2016a), layer normalization (Ba et al., 2016) and tied embeddings (Press and Wolf, 2017). Improving Low-Resource Neural Machine Translation The bulk of research on low-resource NMT has focused on exploiting monolingual data, or parallel data involving other language pairs. Methods to improve NMT with monolingual data range from the integration of a separately trained language model (G¨ulc¸ehre et al., 2015) to the training of parts of the NMT model with additional objectives, including a language modelling objective (G¨ulc¸ehre et al., 2015; Sennrich et al., 2016b; Ramachandran et al., 2017), an autoencoding objective (Luong et al., 2016; Currey et al., 2017), or a round-trip obj"
P19-1021,W17-4710,1,0.86271,"Computational Linguistics, pages 211–221 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tings, and the authors did not tune hyperparameters, or change network architectures, to optimize NMT for low-resource conditions. 2.2 3 Methods for Low-Resource Neural Machine Translation 3.1 Mainstream Improvements We consider the hyperparameters used by Koehn and Knowles (2017) to be our baseline. This baseline does not make use of various advances in NMT architectures and training tricks. In contrast to the baseline, we use a BiDeep RNN architecture (Miceli Barone et al., 2017), label smoothing (Szegedy et al., 2016), dropout (Srivastava et al., 2014), word dropout (Sennrich et al., 2016a), layer normalization (Ba et al., 2016) and tied embeddings (Press and Wolf, 2017). Improving Low-Resource Neural Machine Translation The bulk of research on low-resource NMT has focused on exploiting monolingual data, or parallel data involving other language pairs. Methods to improve NMT with monolingual data range from the integration of a separately trained language model (G¨ulc¸ehre et al., 2015) to the training of parts of the NMT model with additional objectives, including a"
P19-1021,D17-1039,0,0.0624816,"Missing"
P19-1021,D16-1137,0,0.0709901,"Missing"
P19-1021,E17-3017,1,0.88896,"Missing"
P19-1021,D16-1163,0,0.0368624,"an autoencoding objective (Luong et al., 2016; Currey et al., 2017), or a round-trip objective, where the model is trained to predict monolingual (target-side) training data that has been back-translated into the source language (Sennrich et al., 2016b; He et al., 2016; Cheng et al., 2016). As an extreme case, models that rely exclusively on monolingual data have been shown to work (Artetxe et al., 2018b; Lample et al., 2018a; Artetxe et al., 2018a; Lample et al., 2018b). Similarly, parallel data from other language pairs can be used to pre-train the network or jointly learn representations (Zoph et al., 2016; Chen et al., 2017; Nguyen and Chiang, 2017; Neubig and Hu, 2018; Gu et al., 2018a,b; Kocmi and Bojar, 2018). 3.2 Language Representation Subword representations such as BPE (Sennrich et al., 2016c) have become a popular choice to achieve open-vocabulary translation. BPE has one hyperparameter, the number of merge operations, which determines the size of the final vocabulary. For high-resource settings, the effect of vocabulary size on translation quality is relatively small; Haddow et al. (2018) report mixed results when comparing vocabularies of 30k and 90k subwords. In low-resource setting"
P19-1021,P16-1009,1,0.901446,"putational Linguistics tings, and the authors did not tune hyperparameters, or change network architectures, to optimize NMT for low-resource conditions. 2.2 3 Methods for Low-Resource Neural Machine Translation 3.1 Mainstream Improvements We consider the hyperparameters used by Koehn and Knowles (2017) to be our baseline. This baseline does not make use of various advances in NMT architectures and training tricks. In contrast to the baseline, we use a BiDeep RNN architecture (Miceli Barone et al., 2017), label smoothing (Szegedy et al., 2016), dropout (Srivastava et al., 2014), word dropout (Sennrich et al., 2016a), layer normalization (Ba et al., 2016) and tied embeddings (Press and Wolf, 2017). Improving Low-Resource Neural Machine Translation The bulk of research on low-resource NMT has focused on exploiting monolingual data, or parallel data involving other language pairs. Methods to improve NMT with monolingual data range from the integration of a separately trained language model (G¨ulc¸ehre et al., 2015) to the training of parts of the NMT model with additional objectives, including a language modelling objective (G¨ulc¸ehre et al., 2015; Sennrich et al., 2016b; Ramachandran et al., 2017), an a"
P19-1021,P16-1162,1,0.847633,"putational Linguistics tings, and the authors did not tune hyperparameters, or change network architectures, to optimize NMT for low-resource conditions. 2.2 3 Methods for Low-Resource Neural Machine Translation 3.1 Mainstream Improvements We consider the hyperparameters used by Koehn and Knowles (2017) to be our baseline. This baseline does not make use of various advances in NMT architectures and training tricks. In contrast to the baseline, we use a BiDeep RNN architecture (Miceli Barone et al., 2017), label smoothing (Szegedy et al., 2016), dropout (Srivastava et al., 2014), word dropout (Sennrich et al., 2016a), layer normalization (Ba et al., 2016) and tied embeddings (Press and Wolf, 2017). Improving Low-Resource Neural Machine Translation The bulk of research on low-resource NMT has focused on exploiting monolingual data, or parallel data involving other language pairs. Methods to improve NMT with monolingual data range from the integration of a separately trained language model (G¨ulc¸ehre et al., 2015) to the training of parts of the NMT model with additional objectives, including a language modelling objective (G¨ulc¸ehre et al., 2015; Sennrich et al., 2016b; Ramachandran et al., 2017), an a"
P19-1021,P18-1072,0,0.0246256,"ent subword into smaller units or characters. We expect that such a threshold reduces the need to carefully tune the vocabulary size to the dataset, leading to more aggressive segmentation on smaller datasets.1 While semi-supervised and unsupervised approaches have been shown to be very effective for some language pairs, their effectiveness depends on the availability of large amounts of suitable auxiliary data, and other conditions being met. For example, the effectiveness of unsupervised methods is impaired when languages are morphologically different, or when training domains do not match (Søgaard et al., 2018) 3.3 Hyperparameter Tuning Due to long training times, hyperparameters are hard to optimize by grid search, and are often re-used across experiments. However, best practices differ between high-resource and lowresource settings. While the trend in high-resource settings is towards using larger and deeper models, Nguyen and Chiang (2018) use smaller and fewer layers for smaller datasets. Previous work has argued for larger batch sizes in NMT (Morishita et al., 2017; Neishi et al., 2017), but we More broadly, this line of research still accepts the premise that NMT models are data-inefficient an"
P19-1021,P17-1176,0,\N,Missing
P19-1021,W17-4715,0,\N,Missing
P19-1021,W18-6425,0,\N,Missing
P19-1021,W17-4739,1,\N,Missing
P19-1021,N18-1033,0,\N,Missing
P19-1021,W08-0509,0,\N,Missing
P19-1021,K18-1010,0,\N,Missing
P19-1116,N18-1118,1,0.72825,"(a) red – V-form, blue – T-form; (b) red – feminine, blue – masculine. pronouns refer to the same person, the pronouns, as well as verbs that agree with them, should be translated using the same form. See Figure 1(a) for an example translation that violates T-V consistency. Figure 1(b) shows an example of inconsistent first person gender (marked on the verb), although the speaker is clearly the same. Anaphora are a form of deixis that received a lot of attention in MT research, both from the perspective of modelling (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018, among others) and targeted evaluation (Hardmeier et al., 2015; Guillou and Hardmeier, 2016; Müller et al., 2018), and we list anaphora errors separately, and will not further focus on them. 2.2.2 Ellipsis Ellipsis is the omission from a clause of one or more words that are nevertheless understood in the context of the remaining elements. In machine translation, elliptical constructions in the source language pose a problem if the target language does not allow the same types of ellipsis (requiring the elided material to be predicted from context), or if the elided materia"
P19-1116,W12-3156,0,0.0411155,"ocument-level context, and train all components jointly on this data, we focus on an asymmetric scenario where we have a large amount of sentence-level data, used to train our first-pass model, and a smaller amount of document-level data, used to train our secondpass decoder, keeping the first-pass model fixed. Automatic evaluation of the discourse phenomena we consider is challenging. For lexical cohesion, Wong and Kit (2012) count the ratio between the number of repeated and lexically similar content words over the total number of content words in a target document. However, Guillou (2013); Carpuat and Simard (2012) find that translations generated by a machine translation system tend to be similarly or more lexically consistent, as measured by a similar metric, than human ones. This even holds for sentence-level systems, where the increased consistency is not due to improved cohesion, but accidental – Ott et al. (2018) show that beam search introduces a bias towards frequent words, which could be one factor explaining this finding. This means that a higher repetition rate does not mean that a translation system is in fact more cohesive, and we find that even our baseline is more repetitive than the huma"
P19-1116,D11-1084,0,0.114119,"ensively in MT research.3 We classified ellipsis examples which lead to errors in sentence-level translations by the type of error they cause. Results are provided in Table 4. It can be seen that the most frequent problems related to ellipsis that we find in our annotated corpus are wrong morphological forms, followed by wrongly predicted verbs in case of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples"
P19-1116,W13-3302,0,0.0331035,"ining data has document-level context, and train all components jointly on this data, we focus on an asymmetric scenario where we have a large amount of sentence-level data, used to train our first-pass model, and a smaller amount of document-level data, used to train our secondpass decoder, keeping the first-pass model fixed. Automatic evaluation of the discourse phenomena we consider is challenging. For lexical cohesion, Wong and Kit (2012) count the ratio between the number of repeated and lexically similar content words over the total number of content words in a target document. However, Guillou (2013); Carpuat and Simard (2012) find that translations generated by a machine translation system tend to be similarly or more lexically consistent, as measured by a similar metric, than human ones. This even holds for sentence-level systems, where the increased consistency is not due to improved cohesion, but accidental – Ott et al. (2018) show that beam search introduces a bias towards frequent words, which could be one factor explaining this finding. This means that a higher repetition rate does not mean that a translation system is in fact more cohesive, and we find that even our baseline is mo"
P19-1116,L16-1100,0,0.0269998,", the pronouns, as well as verbs that agree with them, should be translated using the same form. See Figure 1(a) for an example translation that violates T-V consistency. Figure 1(b) shows an example of inconsistent first person gender (marked on the verb), although the speaker is clearly the same. Anaphora are a form of deixis that received a lot of attention in MT research, both from the perspective of modelling (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018, among others) and targeted evaluation (Hardmeier et al., 2015; Guillou and Hardmeier, 2016; Müller et al., 2018), and we list anaphora errors separately, and will not further focus on them. 2.2.2 Ellipsis Ellipsis is the omission from a clause of one or more words that are nevertheless understood in the context of the remaining elements. In machine translation, elliptical constructions in the source language pose a problem if the target language does not allow the same types of ellipsis (requiring the elided material to be predicted from context), or if the elided material affects the syntax of the sentence; for example, the grammatical function of a noun phrase and thus its inflec"
P19-1116,W04-3250,0,0.33155,"es context on the target side and performed best in a contrastive evaluation of pronoun translation (Müller et al., 2018). 6.2 General results BLEU scores for our model and the baselines are given in Table 6.5 For context-aware models, all sentences in a group were translated, and then only the current sentence is evaluated. We also report BLEU for the context-agnostic baseline trained only on 1.5m dataset to show how the performance is influenced by the amount of data. We observe that our model is no worse in BLEU than the baseline despite the second-pass model 5 We use bootstrap resampling (Koehn, 2004) for significance testing. 1204 model baseline (1.5m) baseline (6m) concat s-hier-to-2.tied CADec BLEU 29.10 32.40 31.56 26.68 32.38 Table 6: BLEU scores. CADec trained with p = 0.5. Scores for CADec are not statistically different from the baseline (6m). being trained only on a fraction of the data. In contrast, the concatenation baseline, trained on a mixture of data with and without context is about 1 BLEU below the context-agnostic baseline and our model when using all 3 context sentences. CADec’s performance remains the same independently from the number of context sentences (1, 2 or 3) a"
P19-1116,P07-2045,0,0.00694205,"as the proportion of times the true translation is preferred over the contrastive ones. Test set statistics are shown in Table 5. 3.1 Deixis From Table 3, we see that the most frequent error category related to deixis in our annotated corpus is the inconsistency of T-V forms when translating second person pronouns. The test set we 1201 total deixis lex. cohesion ellipsis (infl.) ellipsis (VP) 3000 2000 500 500 the same inflection. Verbs which are used to construct such contrastive translations are the top-10 lemmas of translations of the verb “do” which we get from the lexical table of Moses (Koehn et al., 2007) induced from the training data. latest relevant context 1st 2nd 3rd 1000 855 1000 630 1000 515 3.3 Table 5: Size of test sets: total number of test instances and with regard to the latest context sentence with politeness indication or with the named entity under consideration. For ellipsis, we distinguish whether model has to predict correct noun phrase inflection, or correct verb sense (VP ellipsis). construct for this category tests the ability of a machine translation system to produce translations with consistent level of politeness. We semi-automatically identify sets of consecutive sent"
P19-1116,C18-1050,0,0.290311,"ellipsis examples which lead to errors in sentence-level translations by the type of error they cause. Results are provided in Table 4. It can be seen that the most frequent problems related to ellipsis that we find in our annotated corpus are wrong morphological forms, followed by wrongly predicted verbs in case of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) N"
P19-1116,W17-4806,0,0.122204,"introduce a model that is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU.1 1 Introduction With the recent rapid progress of neural machine translation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not suffic"
P19-1116,P18-1118,0,0.231045,"the recent rapid progress of neural machine translation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate between consistent and inconsistent translations (Wong and Kit, 2012).2 For example, if multiple translations of a name are possible, forcing consistency is essentially as likely to make all occur"
P19-1116,D18-1325,0,0.564624,"anslation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate between consistent and inconsistent translations (Wong and Kit, 2012).2 For example, if multiple translations of a name are possible, forcing consistency is essentially as likely to make all occurrences of the name match the reference translat"
P19-1116,J91-1002,0,0.468219,"cause. Results are provided in Table 4. It can be seen that the most frequent problems related to ellipsis that we find in our annotated corpus are wrong morphological forms, followed by wrongly predicted verbs in case of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) Name translation inconsistency. (b) Inconsistent translation. Using either of the highlighted translat"
P19-1116,W18-6307,1,0.907879,"rbs that agree with them, should be translated using the same form. See Figure 1(a) for an example translation that violates T-V consistency. Figure 1(b) shows an example of inconsistent first person gender (marked on the verb), although the speaker is clearly the same. Anaphora are a form of deixis that received a lot of attention in MT research, both from the perspective of modelling (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Jean et al., 2017b; Bawden et al., 2018; Voita et al., 2018, among others) and targeted evaluation (Hardmeier et al., 2015; Guillou and Hardmeier, 2016; Müller et al., 2018), and we list anaphora errors separately, and will not further focus on them. 2.2.2 Ellipsis Ellipsis is the omission from a clause of one or more words that are nevertheless understood in the context of the remaining elements. In machine translation, elliptical constructions in the source language pose a problem if the target language does not allow the same types of ellipsis (requiring the elided material to be predicted from context), or if the elided material affects the syntax of the sentence; for example, the grammatical function of a noun phrase and thus its inflection in Russian may de"
P19-1116,E17-2104,0,0.0159519,"which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) Name translation inconsistency. (b) Inconsistent translation. Using either of the highlighted translations consistently would be good. focus on repetition with two frequent cases in our annotated corpus being reiteration of named entities (Figure 3(a)) and reiteration of more general phrase types for emphasis (Figure 3(b)) or in clarification questions. 3"
P19-1116,E12-3010,0,0.0228523,"ase of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) Name translation inconsistency. (b) Inconsistent translation. Using either of the highlighted translations consistently would be good. focus on repetition with two frequent cases in our annotated corpus being reiteration of named entities (Figure 3(a)) and reiteration of more general phrase types for emphasis (F"
P19-1116,P16-1162,1,0.549441,"Missing"
P19-1116,W10-2602,0,0.0527347,"been studied extensively in MT research.3 We classified ellipsis examples which lead to errors in sentence-level translations by the type of error they cause. Results are provided in Table 4. It can be seen that the most frequent problems related to ellipsis that we find in our annotated corpus are wrong morphological forms, followed by wrongly predicted verbs in case of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem."
P19-1116,W17-4811,0,0.282935,"this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU.1 1 Introduction With the recent rapid progress of neural machine translation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate between consistent and incons"
P19-1116,P18-1117,1,0.867087,"1 Introduction With the recent rapid progress of neural machine translation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate between consistent and inconsistent translations (Wong and Kit, 2012).2 For example, if multiple translations of a name are possible, forcing consistency is essentially as"
P19-1116,D17-1301,0,0.232651,"at is suitable for this scenario and demonstrate major gains over a context-agnostic baseline on our new benchmarks without sacrificing performance as measured with BLEU.1 1 Introduction With the recent rapid progress of neural machine translation (NMT), translation mistakes and inconsistencies due to the lack of extra-sentential context are becoming more and more noticeable among otherwise adequate translations produced by standard context-agnostic NMT systems (Läubli et al., 2018). Though this problem has recently triggered a lot of attention to contextaware translation (Jean et al., 2017a; Wang et al., 2017; Tiedemann and Scherrer, 2017; Bawden 1 We release code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate"
P19-1116,N16-1113,0,0.0317856,"llipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) Name translation inconsistency. (b) Inconsistent translation. Using either of the highlighted translations consistently would be good. focus on repetition with two frequent cases in our annotated corpus being reiteration of named entities (Figure 3(a)) and reiteration of more general phrase types for emphasis (Figure 3(b)) or in c"
P19-1116,D12-1097,0,0.508088,"code and data sets https://github.com/lena-voita/ good-translation-wrong-in-context. at et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Agrawal et al., 2018; Miculicich et al., 2018; Zhang et al., 2018), the progress and widespread adoption of the new paradigm is hampered by several important problems. Firstly, it is highly non-trivial to design metrics which would reliably trace the progress and guide model design. Standard machine translation metrics (e.g., BLEU) do not appear appropriate as they do not sufficiently differentiate between consistent and inconsistent translations (Wong and Kit, 2012).2 For example, if multiple translations of a name are possible, forcing consistency is essentially as likely to make all occurrences of the name match the reference translation as making them all different from the reference. Second, most previous work on context-aware NMT has made the assumption that all the bilingual data is available at the document level. However, isolated parallel sentences are a lot easier to acquire and hence only a fraction of the parallel data will be at the document level in any practical scenario. In other words, a context-aware model trained only on documentlevel"
P19-1116,P98-2233,0,0.0378362,"tated corpus are wrong morphological forms, followed by wrongly predicted verbs in case of verb phrase ellipsis in English, which does not exist in Russian, thus requiring the prediction of the verb in the Russian translation (Figure 2(b)). 2.2.3 Lexical cohesion Lexical cohesion has been studied previously in MT (Tiedemann, 2010; Gong et al., 2011; Wong and Kit, 2012; Kuang et al., 2018; Miculicich et al., 2018, among others). There are various cohesion devices (Morris and Hirst, 1991), and a good translation should exhibit lexical cohesion beyond the sentence level. We 3 Exceptions include (Yamamoto and Sumita, 1998), and work on the related phenomenon of pronoun dropping (Russo et al., 2012; Wang et al., 2016; Rios and Tuggener, 2017). (b) EN But that’s not what I’m talking about. I’m talking about your future. RU Но я говорю не об этом. Речь о твоём будущем. RU No ya govoryu ne ob etom. Rech’ o tvoyom budushchem. Figure 3: Examples of lack of lexical cohesion in MT. (a) Name translation inconsistency. (b) Inconsistent translation. Using either of the highlighted translations consistently would be good. focus on repetition with two frequent cases in our annotated corpus being reiteration of named entitie"
P19-1116,W15-2501,0,\N,Missing
P19-1116,W10-1737,0,\N,Missing
P19-1116,2010.iwslt-papers.10,0,\N,Missing
P19-1116,W05-0908,0,\N,Missing
P19-1116,C98-2228,0,\N,Missing
P19-1116,L18-1275,0,\N,Missing
P19-1149,D15-1075,0,0.0207718,"ing sequence representation. We compare LRN with several cutting-edge recurrent units, including LSTM, GRU, ATR and SRU. For all comparisons, we keep the neural architecture intact and only alter the recurrent unit.3 All RNNs are implemented without specialized cuDNN kernels. Unless otherwise stated, different models on the same task share the same set of hyperparameters. 6.1 Natural Language Inference Settings Natural language inference reasons about the entailment relationship between a premise sentence and a hypothesis sentence. We use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) and treat the task as a three-way classification task. This dataset contains 549,367 premise-hypothesis pairs for training, 9,842 pairs for developing and 9,824 pairs for testing. We employ accuracy for evaluation. We implement a variant of the word-by-word attention model (Rockt¨aschel et al., 2016) using Tensorflow for this task, where we stack two additional bidirectional RNNs upon the final sequence representation and incorporate character embedding for word-level representation. The pretrained GloVe (Pennington et al., 2014) word vectors are used to initialize word embedding. We also int"
P19-1149,P18-1008,0,0.165455,"eight assigned to each token, tightly connecting LRN with self-attention networks. We apply LRN as a drop-in replacement of existing recurrent units in several neural sequential models. Extensive experiments on six NLP tasks show that LRN yields the best running efficiency with little or no loss in model performance.1 1 Introduction Various natural language processing (NLP) tasks can be categorized as sequence modeling tasks, where recurrent networks (RNNs) are widely applied and contribute greatly to state-of-the-art neural systems (Yang et al., 2018; Peters et al., 2018; Zhang et al., 2018; Chen et al., 2018; Kim et al., 2019). To avoid the optimization bottleneck caused by gradient vanishing and/or explosion (Bengio et al., 1994), Hochreiter and Schmidhuber (1997) and Cho et al. (2014) develop gate structures to ease information propagation from distant words to the current position. Nevertheless, integrating these traditional gates inevitably increases computational overhead which is accumulated along token positions due to the sequen1 Source code is available at https://github.com/ bzhangGo/lrn. tial nature of RNNs. As a result, the weak parallelization of RNNs makes the benefits from improved"
P19-1149,D14-1179,0,0.0440816,"Missing"
P19-1149,D17-1215,0,0.0175927,"75.36 85.15 69.91 WT2 +Finetune 61.45 68.68 73.05 73.35 84.97 68.86 +Dynamic 40.68 44.60 49.77 48.65 57.97 46.97 Table 5: Test perplexity on PTB and WT2 language modeling task. “#Params”: the parameter number in PTB task. Finetune: fintuning the model after convergence. Dynamic dynamic evaluation. Lower perplexity indicates better performance. Model LSTM* LSTM GRU ATR SRU LRN #Params 245K 192K 87K 161K 129K NER 90.94 89.61 89.35 88.46 88.89 88.56 EM and 83.83 F1), beating both GRU and LSTM (+0.33EM, +0.71F1). As recent studies show that cases in SQuAD are dominated by local pattern matching (Jia and Liang, 2017), we argue that LRN is good at handling local dependencies. 6.5 Table 6: F1 score on CoNLL-2003 English NER task. “#Params”: the parameter number in NER task. LSTM* denotes the reported result (Lample et al., 2016). given document, which involves complex sentence matching, reasoning and knowledge association. We use the SQuAD corpus (Rajpurkar et al., 2016) for this task and adopt span-based extraction method. This corpus contains over 100K document-question-answer triples. We report exact match (EM) and F1-score (F1) on the development set for evaluation. We employ the public available rnet m"
P19-1149,N16-1030,0,0.304683,"TB task. Finetune: fintuning the model after convergence. Dynamic dynamic evaluation. Lower perplexity indicates better performance. Model LSTM* LSTM GRU ATR SRU LRN #Params 245K 192K 87K 161K 129K NER 90.94 89.61 89.35 88.46 88.89 88.56 EM and 83.83 F1), beating both GRU and LSTM (+0.33EM, +0.71F1). As recent studies show that cases in SQuAD are dominated by local pattern matching (Jia and Liang, 2017), we argue that LRN is good at handling local dependencies. 6.5 Table 6: F1 score on CoNLL-2003 English NER task. “#Params”: the parameter number in NER task. LSTM* denotes the reported result (Lample et al., 2016). given document, which involves complex sentence matching, reasoning and knowledge association. We use the SQuAD corpus (Rajpurkar et al., 2016) for this task and adopt span-based extraction method. This corpus contains over 100K document-question-answer triples. We report exact match (EM) and F1-score (F1) on the development set for evaluation. We employ the public available rnet model (Wang et al., 2017)6 in Tensorflow. We use the default model settings: character embedding size 8, hidden size 75, batch size 64, and Adadelta optimizer (Zeiler, 2012) with initial learning rate of 0.5. Gradie"
P19-1149,D18-1477,0,0.0742202,"hese traditional gates inevitably increases computational overhead which is accumulated along token positions due to the sequen1 Source code is available at https://github.com/ bzhangGo/lrn. tial nature of RNNs. As a result, the weak parallelization of RNNs makes the benefits from improved model capacity expensive in terms of computational efficiency. Recent studies introduce different solutions to this issue. Zhang et al. (2018) introduce the addition-subtraction twin-gated recurrent unit (ATR), which reduces the amount of matrix operations by developing parameter-shared twin-gate mechanism. Lei et al. (2018) introduce the simple recurrent unit (SRU), which improves model parallelization by moving matrix computations outside the recurrence. Nevertheless, both ATR and SRU perform affine transformations of the previous hidden state for gates, though SRU employs a vector parameter rather than a matrix parameter. In addition, SRU heavily relies on its highway component, without which the recurrent component itself suffers from weak capacity and generalization (Lei et al., 2018). In this paper, we propose a lightweight recurrent network (LRN), which combines the strengths of ATR and SRU. The structure"
P19-1149,D18-1152,0,0.0221496,"ing/caching purpose. Appleyard et al. (2016) upgrade NIVIDIA’s cuDNN implementation through exposing parallelism between operations within the recurrence. Kuchaiev and Ginsburg (2017) reduce the number of model parameters by factorizing or partitioning LSTM matrices. In general, all these techniques can be applied to any recurrent units to reduce computational overhead. Our work is closely related with ATR and SRU. Although recent work shows that novel recurrent units derived from weighted finite state automata are effective without the hidden-to-hidden connection (Balduzzi and Ghifary, 2016; Peng et al., 2018), we empirically observe that including previous hidden states for gates is crucial for model capacity which also resonates with the evolution of SRU. Unlike ATR and SRU, however, we demonstrate that the affine transformation on the previous hidden state for gates is unnecessary. In addition, our model has a strong connection with selfattention networks. 3 Lightweight Recurrent Network Given a sequence of input X = [x|1 ; x|2 ; . . . ; x|n ] ∈ Rn×d with length of n, LRN operates as follows2 : 1539 2 Q, K, V = XWq , XWk , XWv (1) it = σ(kt + ht−1 ) (2) ft = σ(qt − ht−1 ) (3) ht = g(it vt + ft h"
P19-1149,D14-1162,0,0.0832648,"ence. We use the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) and treat the task as a three-way classification task. This dataset contains 549,367 premise-hypothesis pairs for training, 9,842 pairs for developing and 9,824 pairs for testing. We employ accuracy for evaluation. We implement a variant of the word-by-word attention model (Rockt¨aschel et al., 2016) using Tensorflow for this task, where we stack two additional bidirectional RNNs upon the final sequence representation and incorporate character embedding for word-level representation. The pretrained GloVe (Pennington et al., 2014) word vectors are used to initialize word embedding. We also integrate the base BERT (Devlin et al., 2018) to improve contextual modeling. 3 Due to possible dimension mismatch, we include an additional affine transformation on the input matrix for the highway component in SRU. In addition, we only report and compare speed statistics when all RNNs are optimally implemented where computations that can be done before the recurrence are moved outside. 1541 Model #Params Zhang et al. (2015) LSTM GRU This ATR Work SRU LRN 227K 176K 74K 194K 151K AmaPolar ERR Time 6.10 4.37 0.947 4.39 0.948 4.78 0.86"
P19-1149,N18-1202,0,0.244672,"recurrence in LRN only manipulates the weight assigned to each token, tightly connecting LRN with self-attention networks. We apply LRN as a drop-in replacement of existing recurrent units in several neural sequential models. Extensive experiments on six NLP tasks show that LRN yields the best running efficiency with little or no loss in model performance.1 1 Introduction Various natural language processing (NLP) tasks can be categorized as sequence modeling tasks, where recurrent networks (RNNs) are widely applied and contribute greatly to state-of-the-art neural systems (Yang et al., 2018; Peters et al., 2018; Zhang et al., 2018; Chen et al., 2018; Kim et al., 2019). To avoid the optimization bottleneck caused by gradient vanishing and/or explosion (Bengio et al., 1994), Hochreiter and Schmidhuber (1997) and Cho et al. (2014) develop gate structures to ease information propagation from distant words to the current position. Nevertheless, integrating these traditional gates inevitably increases computational overhead which is accumulated along token positions due to the sequen1 Source code is available at https://github.com/ bzhangGo/lrn. tial nature of RNNs. As a result, the weak parallelization o"
P19-1149,D16-1264,0,0.0366418,"M* LSTM GRU ATR SRU LRN #Params 245K 192K 87K 161K 129K NER 90.94 89.61 89.35 88.46 88.89 88.56 EM and 83.83 F1), beating both GRU and LSTM (+0.33EM, +0.71F1). As recent studies show that cases in SQuAD are dominated by local pattern matching (Jia and Liang, 2017), we argue that LRN is good at handling local dependencies. 6.5 Table 6: F1 score on CoNLL-2003 English NER task. “#Params”: the parameter number in NER task. LSTM* denotes the reported result (Lample et al., 2016). given document, which involves complex sentence matching, reasoning and knowledge association. We use the SQuAD corpus (Rajpurkar et al., 2016) for this task and adopt span-based extraction method. This corpus contains over 100K document-question-answer triples. We report exact match (EM) and F1-score (F1) on the development set for evaluation. We employ the public available rnet model (Wang et al., 2017)6 in Tensorflow. We use the default model settings: character embedding size 8, hidden size 75, batch size 64, and Adadelta optimizer (Zeiler, 2012) with initial learning rate of 0.5. Gradient norm is cut to 5.0. We also experiment with Elmo (Peters et al., 2018), and feed the Elmo representation in before the encoding layer and afte"
P19-1149,P16-1162,1,0.413823,"ransforming meaning from a source language to a target language. We experiment with the WMT14 English-German translation task (Bojar et al., 2014) which consists of 4.5M training sentence pairs.5 We use newstest2013 as our development set and newstest2014 as our test set. Casesensitive tokenized BLEU score is used for evaluation. We implement a variant of the GNMT system (Wu et al., 2016) using Tensorflow, enhanced with residual connections, layer normalization, label smoothing, a context-aware component (Zhang et al., 2017) and multi-head attention (Vaswani et al., 2017). Byte-pair encoding (Sennrich et al., 2016) is used to reduce the vocabulary size to 32K. We set the hidden size and embedding size to 1024. Models are trained using Adam optimizer with adaptive learning rate sched4 We notice that ATR operates faster than SRU. This is because though in theory SRU can be highly optimized for parallelization, computational framework like Tensorflow can not handle it automatically and the smaller amount of calculation in ATR has more advantage in practice. 5 Preprocessed data is available at (Zhang et al., 2018): https://drive.google.com/open?id= 15WRLfle66CO1zIGKbyz0FsFmUcINyb4X. #Params 2.67M 2.31M 1.59"
P19-1149,W03-0419,0,0.112778,"Missing"
P19-1149,P17-1018,0,0.0790658,"ized for parallelization, computational framework like Tensorflow can not handle it automatically and the smaller amount of calculation in ATR has more advantage in practice. 5 Preprocessed data is available at (Zhang et al., 2018): https://drive.google.com/open?id= 15WRLfle66CO1zIGKbyz0FsFmUcINyb4X. #Params 2.67M 2.31M 1.59M 2.44M 2.14M Base 71.1/79.5 70.46/78.98 70.41/79.15 69.73/78.70 69.27/78.41 70.11/78.83 +Elmo -/75.17/82.79 75.81/83.12 75.06/82.76 74.56/82.50 76.14/83.83 Table 4: Exact match/F1-score on SQuad dataset. “#Params”: the parameter number of Base. rnet*: results published by Wang et al. (2017). ule (Chen et al., 2018). We cut gradient norm to 1.0 and set the token size to 32K. Label smoothing rate is set to 0.1. Model Variant Apart from LRN, we develop an improved variant for machine translation that includes an additional output gate. Formally, we change the Eq. (4) to the following one: ct = it vt + ft ht−1 (11) ot = σ(Wo xt − ct ) (12) ht = o t c t (13) We denote this variant oLRN. Like LRN, the added matrix transformation in oLRN can be shifted out of the recurrence, bringing in little computational overhead. The design of this output gate ot is inspired by the LSTM structure,"
P19-1149,P18-1166,1,0.900703,"ly manipulates the weight assigned to each token, tightly connecting LRN with self-attention networks. We apply LRN as a drop-in replacement of existing recurrent units in several neural sequential models. Extensive experiments on six NLP tasks show that LRN yields the best running efficiency with little or no loss in model performance.1 1 Introduction Various natural language processing (NLP) tasks can be categorized as sequence modeling tasks, where recurrent networks (RNNs) are widely applied and contribute greatly to state-of-the-art neural systems (Yang et al., 2018; Peters et al., 2018; Zhang et al., 2018; Chen et al., 2018; Kim et al., 2019). To avoid the optimization bottleneck caused by gradient vanishing and/or explosion (Bengio et al., 1994), Hochreiter and Schmidhuber (1997) and Cho et al. (2014) develop gate structures to ease information propagation from distant words to the current position. Nevertheless, integrating these traditional gates inevitably increases computational overhead which is accumulated along token positions due to the sequen1 Source code is available at https://github.com/ bzhangGo/lrn. tial nature of RNNs. As a result, the weak parallelization of RNNs makes the ben"
P19-1149,D18-1459,1,0.628347,"ly manipulates the weight assigned to each token, tightly connecting LRN with self-attention networks. We apply LRN as a drop-in replacement of existing recurrent units in several neural sequential models. Extensive experiments on six NLP tasks show that LRN yields the best running efficiency with little or no loss in model performance.1 1 Introduction Various natural language processing (NLP) tasks can be categorized as sequence modeling tasks, where recurrent networks (RNNs) are widely applied and contribute greatly to state-of-the-art neural systems (Yang et al., 2018; Peters et al., 2018; Zhang et al., 2018; Chen et al., 2018; Kim et al., 2019). To avoid the optimization bottleneck caused by gradient vanishing and/or explosion (Bengio et al., 1994), Hochreiter and Schmidhuber (1997) and Cho et al. (2014) develop gate structures to ease information propagation from distant words to the current position. Nevertheless, integrating these traditional gates inevitably increases computational overhead which is accumulated along token positions due to the sequen1 Source code is available at https://github.com/ bzhangGo/lrn. tial nature of RNNs. As a result, the weak parallelization of RNNs makes the ben"
P19-1580,P17-1080,0,0.0432762,"U, WMT) retained more readily in the lower layers, while decoder-encoder attention heads are retained in the higher layers. This suggests that lower layers of the Transformer’s decoder are mostly responsible for language modeling, while higher layers are mostly responsible for conditioning on the source sentence. These observations are similar for both datasets we use. 7 Related work One popular approach to the analysis of NMT representations is to evaluate how informative they are for various linguistic tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018). Bisazza and Tump (2018) showed that the target language determines which information gets encoded. This agrees with our results for different domains on the English-Russian translation task in Section 5.2.2. There we observed that attention heads are more likely to track syntactic relations requiring more complex agreement in the target language (in this case the subject-verb relation). An alternative method to study the ability of l"
P19-1580,I17-1004,0,0.047812,"age (in this case the subject-verb relation). An alternative method to study the ability of language models and machine translation models to capture hierarchical information is to test their sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Sennrich, 2017; Tang et al., 2018). While this line of work has shown that NMT models, including the Transformer, do learn some syntactic structures, our work provides further insight into the role of multi-head attention. There are several works analyzing attention weights of different NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; 5804 Raganato and Tiedemann, 2018). Raganato and Tiedemann (2018) use the self-attention weights of the Transformer’s encoder to induce a tree structure for each sentence and compute the unlabeled attachment score of these trees. However they do not evaluate specific syntactic relations (i.e. labeled attachment scores) or consider how different heads specialize to specific dependency relations. Recently Bau et al. (2019) proposed a method for identifying important individual neurons in NMT models. They show that similar important neurons emerge in diffe"
P19-1580,N18-1108,0,0.0148612,"isazza and Tump (2018) showed that the target language determines which information gets encoded. This agrees with our results for different domains on the English-Russian translation task in Section 5.2.2. There we observed that attention heads are more likely to track syntactic relations requiring more complex agreement in the target language (in this case the subject-verb relation). An alternative method to study the ability of language models and machine translation models to capture hierarchical information is to test their sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Sennrich, 2017; Tang et al., 2018). While this line of work has shown that NMT models, including the Transformer, do learn some syntactic structures, our work provides further insight into the role of multi-head attention. There are several works analyzing attention weights of different NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; 5804 Raganato and Tiedemann, 2018). Raganato and Tiedemann (2018) use the self-attention weights of the Transformer’s encoder to induce a tree structure for each sentence and compute the unlabeled attachment score of"
P19-1580,Q16-1037,0,0.0407968,"d Tiedemann, 2018). Bisazza and Tump (2018) showed that the target language determines which information gets encoded. This agrees with our results for different domains on the English-Russian translation task in Section 5.2.2. There we observed that attention heads are more likely to track syntactic relations requiring more complex agreement in the target language (in this case the subject-verb relation). An alternative method to study the ability of language models and machine translation models to capture hierarchical information is to test their sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Sennrich, 2017; Tang et al., 2018). While this line of work has shown that NMT models, including the Transformer, do learn some syntactic structures, our work provides further insight into the role of multi-head attention. There are several works analyzing attention weights of different NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; 5804 Raganato and Tiedemann, 2018). Raganato and Tiedemann (2018) use the self-attention weights of the Transformer’s encoder to induce a tree structure for each sentence and compute the unlabe"
P19-1580,L18-1275,0,0.0155653,", German and French. For each language pair, we use the same number of sentence pairs from WMT data to control for the amount of training data and train Transformer models with the same numbers of parameters. We use 2.5m sentence pairs, corresponding to the amount of English–Russian parallel training data (excluding UN and Paracrawl). In Section 5.2 we use the same held-out data for all language pairs; these are 50k English sentences taken from the WMT EN-FR data not used in training. For English-Russian, we perform additional experiments using the publicly available OpenSubtitles2018 corpus (Lison et al., 2018) to evaluate the impact of domains on our results. In Section 6 we concentrate on English-Russian and two domains: WMT and OpenSubtitles. Model hyperparameters, preprocessing and training details are provided in appendix B. 4 Identifying Important Heads Previous work analyzing how representations are formed by the Transformer’s multi-head attention mechanism focused on either the average or the maximum attention weights over all heads (Voita et al., 2018; Tang et al., 2018), but neither method explicitly takes into account the varying importance of different heads. Also, this obscures the role"
P19-1580,P14-5010,0,0.00588199,"Missing"
P19-1580,W18-5431,0,0.278397,"ws the encoderdecoder framework using stacked multi-head selfattention and fully connected layers. Multi-head attention was shown to make more efficient use of the model’s capacity: performance of the model with 8 heads is almost 1 BLEU point higher than that of a model of the same size with single-head attention (Vaswani et al., 2017). The Transformer achieved state-of-the-art results in recent shared translation tasks (Bojar et al., 2018; Niehues et al., 2018). Despite the model’s widespread adoption and recent attempts to investigate the kinds of information learned by the model’s encoder (Raganato and Tiedemann, 2018), the analysis of multi-head attention and its importance 1 We release code at https://github.com/ lena-voita/the-story-of-heads. • Can we significantly reduce the number of attention heads while preserving translation quality? We start by identifying the most important heads in each encoder layer using layer-wise relevance propagation (Ding et al., 2017). For heads judged to be important, we then attempt to characterize the roles they perform. We observe the following types of role: positional (heads attending to an adjacent token), syntactic (heads attending to tokens in a specific syntactic"
P19-1580,E17-2060,1,0.860043,"et language determines which information gets encoded. This agrees with our results for different domains on the English-Russian translation task in Section 5.2.2. There we observed that attention heads are more likely to track syntactic relations requiring more complex agreement in the target language (in this case the subject-verb relation). An alternative method to study the ability of language models and machine translation models to capture hierarchical information is to test their sensitivity to specific grammatical errors (Linzen et al., 2016; Gulordava et al., 2018; Tran et al., 2018; Sennrich, 2017; Tang et al., 2018). While this line of work has shown that NMT models, including the Transformer, do learn some syntactic structures, our work provides further insight into the role of multi-head attention. There are several works analyzing attention weights of different NMT models (Ghader and Monz, 2017; Voita et al., 2018; Tang et al., 2018; 5804 Raganato and Tiedemann, 2018). Raganato and Tiedemann (2018) use the self-attention weights of the Transformer’s encoder to induce a tree structure for each sentence and compute the unlabeled attachment score of these trees. However they do not ev"
P19-1580,P16-1162,1,0.561712,"Missing"
P19-1580,D16-1159,0,0.0459911,"ntion heads are retained in the higher layers. This suggests that lower layers of the Transformer’s decoder are mostly responsible for language modeling, while higher layers are mostly responsible for conditioning on the source sentence. These observations are similar for both datasets we use. 7 Related work One popular approach to the analysis of NMT representations is to evaluate how informative they are for various linguistic tasks. Different levels of linguistic analysis have been considered including morphology (Belinkov et al., 2017a; Dalvi et al., 2017; Bisazza and Tump, 2018), syntax (Shi et al., 2016) and semantics (Hill et al., 2017; Belinkov et al., 2017b; Raganato and Tiedemann, 2018). Bisazza and Tump (2018) showed that the target language determines which information gets encoded. This agrees with our results for different domains on the English-Russian translation task in Section 5.2.2. There we observed that attention heads are more likely to track syntactic relations requiring more complex agreement in the target language (in this case the subject-verb relation). An alternative method to study the ability of language models and machine translation models to capture hierarchical inf"
P19-1580,D18-1458,1,0.942299,"Rest Can Be Pruned Elena Voita1,2 David Talbot1 Fedor Moiseev1,5 Rico Sennrich3,4 Ivan Titov3,2 1 Yandex, Russia 2 University of Amsterdam, Netherlands 3 University of Edinburgh, Scotland 4 University of Zurich, Switzerland 5 Moscow Institute of Physics and Technology, Russia {lena-voita, talbot, femoiseev}@yandex-team.ru rico.sennrich@ed.ac.uk ititov@inf.ed.ac.uk Abstract for translation is challenging. Previous analysis of multi-head attention considered the average of attention weights over all heads at a given position or focused only on the maximum attention weights (Voita et al., 2018; Tang et al., 2018), but neither method explicitly takes into account the varying importance of different heads. Also, this obscures the roles played by individual heads which, as we show, influence the generated translations to differing extents. We attempt to answer the following questions: Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most impo"
P19-1580,W18-6304,1,0.947178,"Rest Can Be Pruned Elena Voita1,2 David Talbot1 Fedor Moiseev1,5 Rico Sennrich3,4 Ivan Titov3,2 1 Yandex, Russia 2 University of Amsterdam, Netherlands 3 University of Edinburgh, Scotland 4 University of Zurich, Switzerland 5 Moscow Institute of Physics and Technology, Russia {lena-voita, talbot, femoiseev}@yandex-team.ru rico.sennrich@ed.ac.uk ititov@inf.ed.ac.uk Abstract for translation is challenging. Previous analysis of multi-head attention considered the average of attention weights over all heads at a given position or focused only on the maximum attention weights (Voita et al., 2018; Tang et al., 2018), but neither method explicitly takes into account the varying importance of different heads. Also, this obscures the roles played by individual heads which, as we show, influence the generated translations to differing extents. We attempt to answer the following questions: Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most impo"
P19-1580,D18-1503,0,0.0632653,"Missing"
P19-1580,P18-1117,1,0.937646,"e Heavy Lifting, the Rest Can Be Pruned Elena Voita1,2 David Talbot1 Fedor Moiseev1,5 Rico Sennrich3,4 Ivan Titov3,2 1 Yandex, Russia 2 University of Amsterdam, Netherlands 3 University of Edinburgh, Scotland 4 University of Zurich, Switzerland 5 Moscow Institute of Physics and Technology, Russia {lena-voita, talbot, femoiseev}@yandex-team.ru rico.sennrich@ed.ac.uk ititov@inf.ed.ac.uk Abstract for translation is challenging. Previous analysis of multi-head attention considered the average of attention weights over all heads at a given position or focused only on the maximum attention weights (Voita et al., 2018; Tang et al., 2018), but neither method explicitly takes into account the varying importance of different heads. Also, this obscures the roles played by individual heads which, as we show, influence the generated translations to differing extents. We attempt to answer the following questions: Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We fin"
Q15-1013,W05-0909,0,0.0826357,"n-grams. HWCM is not only suitable for our task because it operates on the same structures as the dependency language models, but also because our string-to-tree SMT architecture produces trees that can be evaluated directly, without requiring a separate parse of the translation output, a task for which few parsers are optimized. For extracting syntactic n-grams from the reference translations of the respective development and test sets, we automatically parse them, using the same preprocessing as for training. We count syntactic n-grams of sizes 1 to 4, mirroring the typical usage of B LEU. Banerjee and Lavie (2005) have demonstrated the importance of recall in MT evaluation, and we compute the harmonic mean of precision and recall, which we denote HWCMf , instead of the original, precision-based metric. 5 Evaluation We perform three evaluations of our dependency language models. Our perplexity evaluation measures model perplexity on the 1-best output of a 6 For efficiency reasons, our experimental systems only perform SCFG parsing for spans of up to 50 words, and use glue rules to concatenate partial derivations in longer sentences. Better decoding algorithms have reduced the need for this limit (Sennri"
Q15-1013,N10-1080,0,0.0217994,"eural networks have been proposed for syntactic parsing (Socher et al., 2010; Le and Zuidema, 2014). The recursive nature of such models allows for the encoding of more context; for an efficient integration into the dynamic programming search of SMT decoding, we deem our model, which makes stronger Markov assumptions, more suitable. While B LEU has been the standard objective function for tuning the log-linear parameters in SMT systems, recent work has investigated alternative objective functions. Some authors concluded that none of the tested alternatives could consistently outperform B LEU (Cer et al., 2010; Callison-Burch et al., 2011). Liu et al. (2011) report that tuning on the TESLA metric gives better results than tuning on B LEU; Lo et al. (2013) do the same for MEANT. There is related work on improving morphological agreement and subcategorisation through postediting (Rosa et al., 2012) or independent models for inflection generation (Toutanova et al., 2008; Weller et al., 2013). The latter models initially produce a stemmed translation, then predict the inflection through feature-rich sequence models. Such a pipeline of prediction steps is less powerful than our joint prediction of stems"
Q15-1013,2003.mtsummit-papers.6,0,0.0848318,"to the dependency language model described in Shen et al. (2010), or the h-gram model proposed by Zhang (2009), both of which have been used for SMT. We make different modelling assumptions, relying less on configurational information, but including the prediction of dependency labels in the model. We argue that our relational modelling assumptions are more suitable for languages with a relatively free word order such as German. To a lesser extent, our work is similar to other parsing models that have been used for language modelling, such as lexicalized PCFGs (Charniak, 2001; Collins, 2003; Charniak et al., 2003), or structured language models (Chelba and Jelinek, 2000); previous efforts to include them in the translation process failed to improve translation performance (Och et al., 2004; Post and Gildea, 2008). Differences in our work that could explain why we see improvements include the use of Neural Networks for training the model on the automatically parsed training text, instead of re-using existing parser models, which could be seen as a form of self-training (McClosky et al., 2006), and the integration of the language model into the decoder instead of n-best reranking. Also, there are major d"
Q15-1013,P01-1017,0,0.107781,"tions. Our work is most similar to the dependency language model described in Shen et al. (2010), or the h-gram model proposed by Zhang (2009), both of which have been used for SMT. We make different modelling assumptions, relying less on configurational information, but including the prediction of dependency labels in the model. We argue that our relational modelling assumptions are more suitable for languages with a relatively free word order such as German. To a lesser extent, our work is similar to other parsing models that have been used for language modelling, such as lexicalized PCFGs (Charniak, 2001; Collins, 2003; Charniak et al., 2003), or structured language models (Chelba and Jelinek, 2000); previous efforts to include them in the translation process failed to improve translation performance (Och et al., 2004; Post and Gildea, 2008). Differences in our work that could explain why we see improvements include the use of Neural Networks for training the model on the automatically parsed training text, instead of re-using existing parser models, which could be seen as a form of self-training (McClosky et al., 2006), and the integration of the language model into the decoder instead of n-"
Q15-1013,N12-1047,0,0.0716509,"4 shared translation task and is stateof-the art. Our biggest deviation from this setup is that we do not enforce the morphological agreement constraints that are provided by a unification grammar (Williams and Koehn, 2011), but use them for analysis instead. For English→Russian, we copy the language-independent settings from the the English→German set-up, and perform dependency parsing with a Russian model for the Maltparser (Nivre et al., 2006; Sharoff and Nivre, 2011), applying projectivization after parsing. We tune our system on a development set of 2000 sentences with k-best batch MIRA (Cherry and Foster, 2012) on B LEU and a linear interpolation of B LEU and HWCMf , and report both scores for evaluation. We also report METEOR (Denkowski and Lavie, 2011) for German and TER (Snover et al., 2006). We control for optimizer instability by running the optimization three times per system and performing significance testing with Multeval (Clark et al., 2011), which we enhanced to also perform significance testing for HWCMf . 5.2 Implementation notes on model by Shen et al. (2010) We reimplement the model by Shen et al. (2010) for our evaluation. The authors did not specify training and smoothing of their m"
Q15-1013,P11-2031,0,0.0517744,"et-up, and perform dependency parsing with a Russian model for the Maltparser (Nivre et al., 2006; Sharoff and Nivre, 2011), applying projectivization after parsing. We tune our system on a development set of 2000 sentences with k-best batch MIRA (Cherry and Foster, 2012) on B LEU and a linear interpolation of B LEU and HWCMf , and report both scores for evaluation. We also report METEOR (Denkowski and Lavie, 2011) for German and TER (Snover et al., 2006). We control for optimizer instability by running the optimization three times per system and performing significance testing with Multeval (Clark et al., 2011), which we enhanced to also perform significance testing for HWCMf . 5.2 Implementation notes on model by Shen et al. (2010) We reimplement the model by Shen et al. (2010) for our evaluation. The authors did not specify training and smoothing of their model, so we only adopt their definition of the context window, and use the same neural network architecture as for our other models. Specifically, we use two neural networks: one for left dependents, and one for right dependents. We use maximum-likelihood estimation for the head of root nodes, ignoring unseen events. To distinguish between paren"
Q15-1013,J03-4003,0,0.0287317,"is most similar to the dependency language model described in Shen et al. (2010), or the h-gram model proposed by Zhang (2009), both of which have been used for SMT. We make different modelling assumptions, relying less on configurational information, but including the prediction of dependency labels in the model. We argue that our relational modelling assumptions are more suitable for languages with a relatively free word order such as German. To a lesser extent, our work is similar to other parsing models that have been used for language modelling, such as lexicalized PCFGs (Charniak, 2001; Collins, 2003; Charniak et al., 2003), or structured language models (Chelba and Jelinek, 2000); previous efforts to include them in the translation process failed to improve translation performance (Och et al., 2004; Post and Gildea, 2008). Differences in our work that could explain why we see improvements include the use of Neural Networks for training the model on the automatically parsed training text, instead of re-using existing parser models, which could be seen as a form of self-training (McClosky et al., 2006), and the integration of the language model into the decoder instead of n-best reranking."
Q15-1013,W11-2107,0,0.0256639,"nstraints that are provided by a unification grammar (Williams and Koehn, 2011), but use them for analysis instead. For English→Russian, we copy the language-independent settings from the the English→German set-up, and perform dependency parsing with a Russian model for the Maltparser (Nivre et al., 2006; Sharoff and Nivre, 2011), applying projectivization after parsing. We tune our system on a development set of 2000 sentences with k-best batch MIRA (Cherry and Foster, 2012) on B LEU and a linear interpolation of B LEU and HWCMf , and report both scores for evaluation. We also report METEOR (Denkowski and Lavie, 2011) for German and TER (Snover et al., 2006). We control for optimizer instability by running the optimization three times per system and performing significance testing with Multeval (Clark et al., 2011), which we enhanced to also perform significance testing for HWCMf . 5.2 Implementation notes on model by Shen et al. (2010) We reimplement the model by Shen et al. (2010) for our evaluation. The authors did not specify training and smoothing of their model, so we only adopt their definition of the context window, and use the same neural network architecture as for our other models. Specifically,"
Q15-1013,P14-1129,0,0.141742,"Missing"
Q15-1013,P02-1043,0,0.0771471,"hy we see improvements include the use of Neural Networks for training the model on the automatically parsed training text, instead of re-using existing parser models, which could be seen as a form of self-training (McClosky et al., 2006), and the integration of the language model into the decoder instead of n-best reranking. Also, there are major differences in the parsing models themselves. For instance, note that the structured LM by Chelba and Jelinek (2000) uses a binary branching structure, and that complex label sets would be required to encode subcategorisation frames in binary trees (Hockenmaier and Steedman, 2002). Our neural network is a standard feed-forward neural network as introduced by Bengio et al. (2003). Recently, recursive neural networks have been proposed for syntactic parsing (Socher et al., 2010; Le and Zuidema, 2014). The recursive nature of such models allows for the encoding of more context; for an efficient integration into the dynamic programming search of SMT decoding, we deem our model, which makes stronger Markov assumptions, more suitable. While B LEU has been the standard objective function for tuning the log-linear parameters in SMT systems, recent work has investigated alterna"
Q15-1013,D14-1081,0,0.0231274,"and the integration of the language model into the decoder instead of n-best reranking. Also, there are major differences in the parsing models themselves. For instance, note that the structured LM by Chelba and Jelinek (2000) uses a binary branching structure, and that complex label sets would be required to encode subcategorisation frames in binary trees (Hockenmaier and Steedman, 2002). Our neural network is a standard feed-forward neural network as introduced by Bengio et al. (2003). Recently, recursive neural networks have been proposed for syntactic parsing (Socher et al., 2010; Le and Zuidema, 2014). The recursive nature of such models allows for the encoding of more context; for an efficient integration into the dynamic programming search of SMT decoding, we deem our model, which makes stronger Markov assumptions, more suitable. While B LEU has been the standard objective function for tuning the log-linear parameters in SMT systems, recent work has investigated alternative objective functions. Some authors concluded that none of the tested alternatives could consistently outperform B LEU (Cer et al., 2010; Callison-Burch et al., 2011). Liu et al. (2011) report that tuning on the TESLA m"
Q15-1013,W05-0904,0,0.377773,"grams N-gram based metrics such as B LEU (Papineni et al., 2002) are still predominantly used to optimize the log-linear parameters of SMT systems, and (to a lesser extent) to evaluate the final translation systems. However, n-gram metrics are not well suited to measure fluency phenomena with string-level gaps, and there is a danger that B LEU underestimates the modelling power of dependency language models, resulting in a suboptimal assignment of loglinear weights. As an alternative metric that operates on the level of syntactic n-grams, we use a variant of the head-word chain metric (HWCM) (Liu and Gildea, 2005). HWCM is a precision metric similar to B LEU, but instead of counting n-gram matches between the translation output and the reference, it compares head-word chains, or syntactic n-grams. HWCM is not only suitable for our task because it operates on the same structures as the dependency language models, but also because our string-to-tree SMT architecture produces trees that can be evaluated directly, without requiring a separate parse of the translation output, a task for which few parsers are optimized. For extracting syntactic n-grams from the reference translations of the respective develo"
Q15-1013,D11-1035,0,0.0478835,"Missing"
Q15-1013,2013.iwslt-evaluation.5,0,0.0189995,"the encoding of more context; for an efficient integration into the dynamic programming search of SMT decoding, we deem our model, which makes stronger Markov assumptions, more suitable. While B LEU has been the standard objective function for tuning the log-linear parameters in SMT systems, recent work has investigated alternative objective functions. Some authors concluded that none of the tested alternatives could consistently outperform B LEU (Cer et al., 2010; Callison-Burch et al., 2011). Liu et al. (2011) report that tuning on the TESLA metric gives better results than tuning on B LEU; Lo et al. (2013) do the same for MEANT. There is related work on improving morphological agreement and subcategorisation through postediting (Rosa et al., 2012) or independent models for inflection generation (Toutanova et al., 2008; Weller et al., 2013). The latter models initially produce a stemmed translation, then predict the inflection through feature-rich sequence models. Such a pipeline of prediction steps is less powerful than our joint prediction of stems and inflection. For instance, in example 2 in Table 4, our model chooses a different stem to match the subcategorisation frame of the translation;"
Q15-1013,N06-1020,0,0.0224734,"ing models that have been used for language modelling, such as lexicalized PCFGs (Charniak, 2001; Collins, 2003; Charniak et al., 2003), or structured language models (Chelba and Jelinek, 2000); previous efforts to include them in the translation process failed to improve translation performance (Och et al., 2004; Post and Gildea, 2008). Differences in our work that could explain why we see improvements include the use of Neural Networks for training the model on the automatically parsed training text, instead of re-using existing parser models, which could be seen as a form of self-training (McClosky et al., 2006), and the integration of the language model into the decoder instead of n-best reranking. Also, there are major differences in the parsing models themselves. For instance, note that the structured LM by Chelba and Jelinek (2000) uses a binary branching structure, and that complex label sets would be required to encode subcategorisation frames in binary trees (Hockenmaier and Steedman, 2002). Our neural network is a standard feed-forward neural network as introduced by Bengio et al. (2003). Recently, recursive neural networks have been proposed for syntactic parsing (Socher et al., 2010; Le and"
Q15-1013,nivre-etal-2006-maltparser,0,0.0260969,"ency parsing of the German texts (Sennrich et al., 2013). It is described in more detail in (Williams et al., 2014). This setup was ranked 1–2 (out of 18) in the WMT 2014 shared translation task and is stateof-the art. Our biggest deviation from this setup is that we do not enforce the morphological agreement constraints that are provided by a unification grammar (Williams and Koehn, 2011), but use them for analysis instead. For English→Russian, we copy the language-independent settings from the the English→German set-up, and perform dependency parsing with a Russian model for the Maltparser (Nivre et al., 2006; Sharoff and Nivre, 2011), applying projectivization after parsing. We tune our system on a development set of 2000 sentences with k-best batch MIRA (Cherry and Foster, 2012) on B LEU and a linear interpolation of B LEU and HWCMf , and report both scores for evaluation. We also report METEOR (Denkowski and Lavie, 2011) for German and TER (Snover et al., 2006). We control for optimizer instability by running the optimization three times per system and performing significance testing with Multeval (Clark et al., 2011), which we enhanced to also perform significance testing for HWCMf . 5.2 Imple"
Q15-1013,N04-1021,0,0.330066,"ontinuous in the surface string, and are thus not modelled well by traditional n-gram language models. Examples include morphological agreement, e.g. subject-verb agreement in languages that do not (exclusively) follow SVO word order, subcategorisation, and collocations involving distant, but syntactically linked words. Syntactic language models try to overcome the limitation to a local n-gram context by using syntactically related words (and non-terminals) as context information. Despite their theoretical attractiveness, it has proven difficult to improve SMT with parsers as language models (Och et al., 2004; Post and Gildea, 2008). This paper describes an effective method to model, train, decode with, and weight a syntactic language model for SMT. While all these aspects are important for successfully applying a syntactic language model, our primary contributions are a novel dependency language model which improves over prior work by making relational modelling assumptions, which we argue are better suited for languages with a (relatively) free word order, and the use of a syntactic evaluation metric for optimizing the loglinear parameters of the SMT model. While language models that operate on"
Q15-1013,P02-1040,0,0.0928655,"rules which concatenate two subtrees. Since the resulting glue structures do not occur in the training data, we do not estimate their probability in our model. When encountering the root of a glue rule in our language model, we recursively evaluate its children, but ignore the glue node itself. This could introduce a bias towards using more glue rules during translation. To counter this, and encourage the production of linguistically plausible trees, we assign a fixed, high cost to glue rules. Glue rules thus play a small 173 4 Optimizing Syntactic N-grams N-gram based metrics such as B LEU (Papineni et al., 2002) are still predominantly used to optimize the log-linear parameters of SMT systems, and (to a lesser extent) to evaluate the final translation systems. However, n-gram metrics are not well suited to measure fluency phenomena with string-level gaps, and there is a danger that B LEU underestimates the modelling power of dependency language models, resulting in a suboptimal assignment of loglinear weights. As an alternative metric that operates on the level of syntactic n-grams, we use a variant of the head-word chain metric (HWCM) (Liu and Gildea, 2005). HWCM is a precision metric similar to B L"
Q15-1013,2008.amta-papers.16,0,0.452542,"urface string, and are thus not modelled well by traditional n-gram language models. Examples include morphological agreement, e.g. subject-verb agreement in languages that do not (exclusively) follow SVO word order, subcategorisation, and collocations involving distant, but syntactically linked words. Syntactic language models try to overcome the limitation to a local n-gram context by using syntactically related words (and non-terminals) as context information. Despite their theoretical attractiveness, it has proven difficult to improve SMT with parsers as language models (Och et al., 2004; Post and Gildea, 2008). This paper describes an effective method to model, train, decode with, and weight a syntactic language model for SMT. While all these aspects are important for successfully applying a syntactic language model, our primary contributions are a novel dependency language model which improves over prior work by making relational modelling assumptions, which we argue are better suited for languages with a (relatively) free word order, and the use of a syntactic evaluation metric for optimizing the loglinear parameters of the SMT model. While language models that operate on words linked through a d"
Q15-1013,P05-1034,0,0.118421,"Missing"
Q15-1013,W12-3146,0,0.0405991,"Missing"
Q15-1013,R13-1079,1,0.661519,"ould require the (costly) marginalization over all sequences of dependency labels D and topologies T , but like the SMT decoder itself, we approximate the search for the best translation by searching for the highest-scoring derivation, meaning that we directly integrate Pw and Pl as two features into the log-linear SMT model. We use selfnormalized neural networks with precomputation of the hidden layer, which makes the integration into decoding reasonably fast. The decoder builds the translation bottom-up, and the full context is not available for all symbols in the hypothesis. Vaswani et al. (2013) propose to use a special null word for unavailable context, their embedding being the weighted average of the input embeddings of all other words. We adopt this strategy, with the difference that we use separate null words for each position in the context window in order to reflect distributional differences between the different positions, e.g. between ancestor labels and sibling labels. Symbols are re-scored as more context becomes available in decoding, but poor approximations could affect pruning and thus lead to search errors. In Table 1, we illustrate the use of null words with a 5-gram"
Q15-1013,W14-4011,1,0.530437,"(2005) have demonstrated the importance of recall in MT evaluation, and we compute the harmonic mean of precision and recall, which we denote HWCMf , instead of the original, precision-based metric. 5 Evaluation We perform three evaluations of our dependency language models. Our perplexity evaluation measures model perplexity on the 1-best output of a 6 For efficiency reasons, our experimental systems only perform SCFG parsing for spans of up to 50 words, and use glue rules to concatenate partial derivations in longer sentences. Better decoding algorithms have reduced the need for this limit (Sennrich, 2014). baseline SMT system and a human reference translation. Our SMT evaluation integrates the model as a feature function in a string-to-tree SMT system and evaluates its impact on translation quality. Finally, we quantify the effect of different language models on grammaticality by measuring the number of agreement errors of our SMT systems. We refer to the unlabelled variant of our model (equation 2) as DLM, and to the labelled variant (equation 4) as RDLM, emphasizing that the latter is a relational dependency LM. 5.1 Data and Methods We perform our experiments on English→German data from the"
Q15-1013,2006.amta-papers.25,0,0.0381478,"ammar (Williams and Koehn, 2011), but use them for analysis instead. For English→Russian, we copy the language-independent settings from the the English→German set-up, and perform dependency parsing with a Russian model for the Maltparser (Nivre et al., 2006; Sharoff and Nivre, 2011), applying projectivization after parsing. We tune our system on a development set of 2000 sentences with k-best batch MIRA (Cherry and Foster, 2012) on B LEU and a linear interpolation of B LEU and HWCMf , and report both scores for evaluation. We also report METEOR (Denkowski and Lavie, 2011) for German and TER (Snover et al., 2006). We control for optimizer instability by running the optimization three times per system and performing significance testing with Multeval (Clark et al., 2011), which we enhanced to also perform significance testing for HWCMf . 5.2 Implementation notes on model by Shen et al. (2010) We reimplement the model by Shen et al. (2010) for our evaluation. The authors did not specify training and smoothing of their model, so we only adopt their definition of the context window, and use the same neural network architecture as for our other models. Specifically, we use two neural networks: one for left"
Q15-1013,P08-1059,0,0.0554165,"e standard objective function for tuning the log-linear parameters in SMT systems, recent work has investigated alternative objective functions. Some authors concluded that none of the tested alternatives could consistently outperform B LEU (Cer et al., 2010; Callison-Burch et al., 2011). Liu et al. (2011) report that tuning on the TESLA metric gives better results than tuning on B LEU; Lo et al. (2013) do the same for MEANT. There is related work on improving morphological agreement and subcategorisation through postediting (Rosa et al., 2012) or independent models for inflection generation (Toutanova et al., 2008; Weller et al., 2013). The latter models initially produce a stemmed translation, then predict the inflection through feature-rich sequence models. Such a pipeline of prediction steps is less powerful than our joint prediction of stems and inflection. For instance, in example 2 in Table 4, our model chooses a different stem to match the subcategorisation frame of the translation; it is not possible to fix the baseline translation with inflection changes alone. 7 Conclusion The main contribution of this paper is the description of a relational dependency language model.10 We show that it is a"
Q15-1013,D09-1088,0,0.0554677,"Missing"
Q15-1013,D13-1140,0,0.371005,", D, T ) ≈ Pl (i), i=1 if wi 6=  otherwise (4) Figure 2 illustrates the prediction of a subtree of the dependency tree in Figure 1. Note that T is encoded implicitly, and can be retrieved from D through a stack to which all nodes (except for preterminal and ST OP nodes) are pushed after prediction, and from which the last node is popped when predicting a ST OP node. 3 Neural Network Training and SMT Decoding We extract all training instances from automatically parsed training text, and perform training with a standard feed-forward neural network (Bengio et al., 2003), using the NPLM toolkit (Vaswani et al., 2013). Back-off smoothing schemes are unsatisfactory because it is unclear which part of the context should be forgotten first, and neural networks elegantly solve this problem. We use two separate networks, one for Pw and one for Pl . Both networks share the same input vocabulary, but are trained and applied independently. The model input is a (2q + 2r)-word context vector (+1 for Pw to encode li ), each word being mapped to a shared embedding layer. We use a single hidden layer with rectifiedlinear activation function, and noise-contrastive estimation (NCE). We integrate our dependency language m"
Q15-1013,P13-1058,0,0.0369029,"ction for tuning the log-linear parameters in SMT systems, recent work has investigated alternative objective functions. Some authors concluded that none of the tested alternatives could consistently outperform B LEU (Cer et al., 2010; Callison-Burch et al., 2011). Liu et al. (2011) report that tuning on the TESLA metric gives better results than tuning on B LEU; Lo et al. (2013) do the same for MEANT. There is related work on improving morphological agreement and subcategorisation through postediting (Rosa et al., 2012) or independent models for inflection generation (Toutanova et al., 2008; Weller et al., 2013). The latter models initially produce a stemmed translation, then predict the inflection through feature-rich sequence models. Such a pipeline of prediction steps is less powerful than our joint prediction of stems and inflection. For instance, in example 2 in Table 4, our model chooses a different stem to match the subcategorisation frame of the translation; it is not possible to fix the baseline translation with inflection changes alone. 7 Conclusion The main contribution of this paper is the description of a relational dependency language model.10 We show that it is a valuable asset to a st"
Q15-1013,W11-2126,0,0.653559,"plexity evaluation, we use the same vocabulary and training data as for the Neural Network models. For the English→German SMT evaluation, our baseline system is a string-to-tree SMT system with Moses (Koehn et al., 2007), with dependency parsing of the German texts (Sennrich et al., 2013). It is described in more detail in (Williams et al., 2014). This setup was ranked 1–2 (out of 18) in the WMT 2014 shared translation task and is stateof-the art. Our biggest deviation from this setup is that we do not enforce the morphological agreement constraints that are provided by a unification grammar (Williams and Koehn, 2011), but use them for analysis instead. For English→Russian, we copy the language-independent settings from the the English→German set-up, and perform dependency parsing with a Russian model for the Maltparser (Nivre et al., 2006; Sharoff and Nivre, 2011), applying projectivization after parsing. We tune our system on a development set of 2000 sentences with k-best batch MIRA (Cherry and Foster, 2012) on B LEU and a linear interpolation of B LEU and HWCMf , and report both scores for evaluation. We also report METEOR (Denkowski and Lavie, 2011) for German and TER (Snover et al., 2006). We control"
Q15-1013,W14-3324,1,0.811809,"ing with SRILM 7 On our test set, a node has an average of 4.6 ancestors (σ = 2.5), and 1.2 left siblings (σ = 1.3). 174 (Stolcke, 2002). The model in the SMT baseline uses the full vocabulary and a linear interpolation of component models for domain adaptation. For the perplexity evaluation, we use the same vocabulary and training data as for the Neural Network models. For the English→German SMT evaluation, our baseline system is a string-to-tree SMT system with Moses (Koehn et al., 2007), with dependency parsing of the German texts (Sennrich et al., 2013). It is described in more detail in (Williams et al., 2014). This setup was ranked 1–2 (out of 18) in the WMT 2014 shared translation task and is stateof-the art. Our biggest deviation from this setup is that we do not enforce the morphological agreement constraints that are provided by a unification grammar (Williams and Koehn, 2011), but use them for analysis instead. For English→Russian, we copy the language-independent settings from the the English→German set-up, and perform dependency parsing with a Russian model for the Maltparser (Nivre et al., 2006; Sharoff and Nivre, 2011), applying projectivization after parsing. We tune our system on a deve"
Q15-1013,C08-1112,0,\N,Missing
Q15-1013,J10-4005,0,\N,Missing
Q15-1013,2008.amta-papers.4,0,\N,Missing
Q15-1013,P07-2045,0,\N,Missing
Q15-1013,E12-1068,0,\N,Missing
Q15-1013,W14-3302,0,\N,Missing
Q15-1013,2011.iwslt-papers.8,0,\N,Missing
R13-1079,schmid-etal-2004-smor,0,0.0167188,"er in the functional disambiguation of noun phrases, i.e. relations such as subject, object, and genitive modifier, while MaltParser finds more coordinations, albeit with lower precision. Some selected f1 val3 Morphology For parsing, morphology tools provide two useful types of information. Lemma information allows for less sparse representation of statistical data, and inflectional analyses can be used to enforce agreement constraints, and for the functional disambiguation of German noun phrases. As alternatives to GERTWOL, we investigate two morphology tools, both based on the SMOR grammar (Schmid et al., 2004), which is open source and licensed under GPL v2. The first is the SMOR grammar with the lexicon of the University of Stuttgart (consequently referred to as SMOR). The lexicon is closed-source, and can be licensed for research purposes. Secondly, we investigate Morphisto (Zielinski and Simon, 2009), which combines the SMOR grammar with an open-source lexicon, provided under the Creative Commons 3.0 BY-SA Non-Commercial license. One problem with the SMOR grammar is that the morphology does not produce conventional lemmas, but derivational analyses as shown in figure 2. Specifically, the word fo"
R13-1079,P05-1022,0,0.0313835,"Missing"
R13-1079,C10-2129,0,0.10406,"Missing"
R13-1079,2005.mtsummit-papers.11,0,0.0249766,"Missing"
R13-1079,telljohann-etal-2004-tuba,0,0.0198654,"sitional preferences, and is trained on the TüBa-D/Z, a hand-annotated treebank of Introduction German NLP tools such as part-of-speech taggers, morphology tools, and syntactic parsers often require licensing and suffer from usage restrictions, which makes the deployment of an NLP pipeline that combines several components cumbersome at best, impossible at worst (if no license can be obtained). Some restrictions are rooted in the copyright and/or licenses of the annotated corpora on which statistical taggers or parsers can be trained for German, such as TIGER (Brants et al., 2002) or Tüba-D/Z (Telljohann et al., 2004). There have been attempts to bypass these restrictions through corpus masking (Rehm et al., 2007), but for statistical models that require lexical information, this is not an option. We discuss ParZu, a German dependency parser that relies on external tools for POS tagging and morphological analysis, and combines a handwritten grammar and a statistical disambiguation module that is trained on a treebank. We describe attempts to move towards components with freer licensing. We also discuss techniques to improve 1 Among the lexicalized rules is a closed list of nouns which can head noun phrases"
R13-1079,P10-1052,0,0.0305138,"Missing"
R13-1079,P09-1040,0,0.0124778,"ensing. We also discuss techniques to improve 1 Among the lexicalized rules is a closed list of nouns which can head noun phrases with temporal function, such as Er schläft jeden Tag (English: ‘he sleeps every day’). 601 Proceedings of Recent Advances in Natural Language Processing, pages 601–609, Hissar, Bulgaria, 7-13 September 2013. adv subj objc obja adv subj pred det Jetzt frage ADV VVFIN ich PPER mich , wofür PRF $, PWAV das ART Ganze gut war . NN ADJD VAFIN $. Figure 1: TüBa-D/Z parse tree in dependency format. (English: ‘Now I ask myself what good it did.’) parsing, we use MaltParser (Nivre, 2009), with settings optimized with MaltOptimizer (Ballesteros and Nivre, 2012). MaltParser is a tool for data-driven dependency parsing which implements various algorithms. For TüBa-D/Z, MaltOptimizer selects the stack projective algorithm (Nivre, 2009) with pseudo-projective pre- and postprocessing. The algorithm generates a parse tree through a sequence of transitions from an initial configuration (a NULL word on the stack, all words of the sentence in the buffer, and an empty set of labelled dependency arcs) to a terminal configuration (a NULL word on the stack, an empty buffer, and a set of la"
R13-1079,ballesteros-nivre-2012-maltoptimizer-system,0,\N,Missing
R19-1136,P17-1080,0,0.0288899,"e encoders. Encoders are responsible for providing source hidden representations to the decoder. Encoderfree models have to use word embeddings to represent source tokens without the help of encoders. Thus, the source-side representations probably lead to the performance gap. PPL 11.7 10.3 9.9 9.5 9.6 BLEU 15.9 17.6 18.4 18.6 18.9 Table 4: The performance of Transformer models that have different layers in the encoder, including the perplexity (PPL) on the development set and the BLEU scores on newstest2015. It has been shown that encoders could extract syntactic and semantic features in NMT (Belinkov et al., 2017a,b; Poliak et al., 2018). In the meantime, contextual information is encoded in hidden representations as well. Hence we conclude that the quality of source representations is the main factor causing the big gap between Transformer and Trans-noEnc. In Table 5, our additional experiments on DE→EN and ZH→EN confirm that models with contextualized representations are much better. Transformer models always outperform TransnoEnc models substantially. Lan. DE→EN ZH→EN Figure 1: The attention entropy of each attention layer and the entire attention mechanism. 4.3 Param. 71.4M 76.9M 87.9M 98.9M 104.4"
R19-1136,I17-1001,0,0.062453,"Missing"
R19-1136,P17-1106,0,0.0293548,"ngs of NMT models. However, Ghader and Monz (2017) and Koehn and Knowles (2017) have shown that the attention mechanism is different from a word alignment. While there are linguistically plausible explanations in some cases – when translating a verb, knowledge about the subject, object etc. may be relevant information – other cases are harder to explain, such as an off-by-one mismatch between attention and word alignment for some models. We suspect that such a pattern can be learned if relevant information is passed to neighboring representations via recurrent or self-attentional connections. Ding et al. (2017) show that only using attention is not sufficient for deep interpretation and propose to use layer-wise relevance propagation to better understand NMT. Wang et al. (2018) replace the attention model with an alignment model and a lexical model to make NMT models more interpretable. The proposed model is not superior but on a par with the attentional model. They clarify the difference between alignment models and attention models by saying that that the alignment model is to identify translation equivalents while the attention model is to predict the next target word. In this paper, we try to un"
R19-1136,I17-1004,0,0.654762,"et al., 2017) represent the source. The decoder is a standard Transformer (Vaswani et al., 2017) or recurrent neural network (RNN) that attends to embeddings via attention mechanisms. As motivation for our architecture simplification, consider the attention mechanism1 (Bahdanau et al., 2015; Luong et al., 2015), which has been introduced to extract features from the hidden representations in encoders dynamically. Attention and alignment were initially used interchangeably, but it was soon discovered that the attention mechanism can behave very differently from traditional word alignment (see Ghader and Monz, 2017; Koehn and Knowles, 2017). One reason for this discrepancy is that the attention mechanism operates on representations that potentially includes information from the whole sentence due to the encoder’s recurrent or self-attentional architecture. Intuitively, bypassing these encoder layers and attending word embeddings directly could lead to a more alignment-like, and thus predictable and interpretable behavior of the attention model. By comparing encoder-free models with conventional models, we can better understand the working mechanism of NMT, figure out which components are more crucial, a"
R19-1136,E17-3017,1,0.836013,"nificantly improves the alignment quality and performs as well as the aligners based on traditional IBM models. 3 Experiments In addition to training Transformer and TransnoEnc models, we also compare Trans-noEnc with NMT models based on RNNs (RNNS2S). We train RNNS2S models without encoders (RNNS2SnoEnc), without attention mechanisms (RNNS2SnoAtt), and without both encoders and attention mechanisms (RNNS2S-noAtt-noEnc) to explore which component is more important for NMT. We also investigate the importance of positional embeddings in Trans-noEnc. 3.1 Experimental Settings We use the Sockeye (Hieber et al., 2017) toolkit, which is based on MXNet (Chen et al., 2015), to train models. Each encoder/decoder has 6 layers. For RNNS2S, we choose long short-term memory (LSTM) RNN units. Transformers have 8 attention heads. The size of embeddings and hidden states is 768. We tie the source, target, and output embeddings. The dropout rate of embeddings and Transformer blocks is set to 0.1. The dropout rate 1187 of RNNs is 0.2. All the models are trained with a single GPU. During training, each mini-batch contains 2,048 tokens. A model checkpoint is saved every 1,000 updates. We use Adam (Kingma and Ba, 2015) as"
R19-1136,D13-1176,0,0.11271,"ce. The decoder is a standard Transformer or recurrent neural network that directly attends to embeddings via attention mechanisms. Experimental results show (1) that the attention mechanism in encoder-free models acts as a strong feature extractor, (2) that the word embeddings in encoder-free models are competitive to those in conventional models, (3) that non-contextualized source representations lead to a big performance drop, and (4) that encoder-free models have different effects on alignment quality for German→English and Chinese→English. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has emerged in the last few years and has achieved new state-ofthe-art performance. However, NMT models are black boxes for humans and are hard to interpret. NMT models employ encoder-decoder architectures where an encoder encodes source-side sentences and an attentional decoder generates targetside sentences based on the outputs of the encoder. In this paper, we attempt to obtain a more interpretable NMT model by simplifying the encoderdecoder architecture. We train encoder-free models where the sums of word embeddings and s"
R19-1136,W17-3204,0,0.402881,"t the source. The decoder is a standard Transformer (Vaswani et al., 2017) or recurrent neural network (RNN) that attends to embeddings via attention mechanisms. As motivation for our architecture simplification, consider the attention mechanism1 (Bahdanau et al., 2015; Luong et al., 2015), which has been introduced to extract features from the hidden representations in encoders dynamically. Attention and alignment were initially used interchangeably, but it was soon discovered that the attention mechanism can behave very differently from traditional word alignment (see Ghader and Monz, 2017; Koehn and Knowles, 2017). One reason for this discrepancy is that the attention mechanism operates on representations that potentially includes information from the whole sentence due to the encoder’s recurrent or self-attentional architecture. Intuitively, bypassing these encoder layers and attending word embeddings directly could lead to a more alignment-like, and thus predictable and interpretable behavior of the attention model. By comparing encoder-free models with conventional models, we can better understand the working mechanism of NMT, figure out which components are more crucial, and learn lessons for impro"
R19-1136,P18-1164,0,0.145965,"the difference between alignment models and attention models by saying that that the alignment model is to identify translation equivalents while the attention model is to predict the next target word. In this paper, we try to understand NMT by simplifying the model. We explore the importance of different NMT components and what causes the performance gap after model simplification. 2.2 Alignments and Source Embeddings Nguyen and Chiang (2018) introduce a lexical model to generate a target word directly based on the source words. With the lexical model, NMT models generate better alignments. Kuang et al. (2018) propose three different methods to bridge source and target word embeddings. The bridging methods can significantly improve the translation quality. Moreover, the word alignments generated by the model are improved as well. Our encoder-free model is a simplification and only attends to the source word embeddings. We aim to interpret NMT models rather than pursuing better performance. Different from previous work, Zenkel et al. (2019) introduce a separate alignment layer directly optimizing the word alignment. The alignment layer is an attention network learning to attend to source tokens give"
R19-1136,D15-1166,0,0.655745,"ctly attends to embeddings via attention mechanisms. Experimental results show (1) that the attention mechanism in encoder-free models acts as a strong feature extractor, (2) that the word embeddings in encoder-free models are competitive to those in conventional models, (3) that non-contextualized source representations lead to a big performance drop, and (4) that encoder-free models have different effects on alignment quality for German→English and Chinese→English. 1 Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has emerged in the last few years and has achieved new state-ofthe-art performance. However, NMT models are black boxes for humans and are hard to interpret. NMT models employ encoder-decoder architectures where an encoder encodes source-side sentences and an attentional decoder generates targetside sentences based on the outputs of the encoder. In this paper, we attempt to obtain a more interpretable NMT model by simplifying the encoderdecoder architecture. We train encoder-free models where the sums of word embeddings and sinusoid embeddings (Vaswani et al., 2017) represent the source. The"
R19-1136,N18-1031,0,0.0160366,"odel with an alignment model and a lexical model to make NMT models more interpretable. The proposed model is not superior but on a par with the attentional model. They clarify the difference between alignment models and attention models by saying that that the alignment model is to identify translation equivalents while the attention model is to predict the next target word. In this paper, we try to understand NMT by simplifying the model. We explore the importance of different NMT components and what causes the performance gap after model simplification. 2.2 Alignments and Source Embeddings Nguyen and Chiang (2018) introduce a lexical model to generate a target word directly based on the source words. With the lexical model, NMT models generate better alignments. Kuang et al. (2018) propose three different methods to bridge source and target word embeddings. The bridging methods can significantly improve the translation quality. Moreover, the word alignments generated by the model are improved as well. Our encoder-free model is a simplification and only attends to the source word embeddings. We aim to interpret NMT models rather than pursuing better performance. Different from previous work, Zenkel et a"
R19-1136,J03-1002,0,0.0344722,"Missing"
R19-1136,P02-1040,0,0.106525,"ls are trained with a single GPU. During training, each mini-batch contains 2,048 tokens. A model checkpoint is saved every 1,000 updates. We use Adam (Kingma and Ba, 2015) as the optimizer. The initial learning rate is set to 0.0001. If the performance on the validation set has not improved for 8 checkpoints, the learning rate is multiplied by 0.7. We set the early stopping patience to 32 checkpoints. The training data is from the WMT15 shared task (Bojar et al., 2015) on Finnish–English (FI– EN). We choose newsdev2015 as the validation set and use newstest2015 as the test set. All the BLEU (Papineni et al., 2002) scores are measured by SacreBLEU (Post, 2018). There are about 2.1M sentence pairs in the training set after preprocessing. We learn a joint BPE model with 32K subword units (Sennrich et al., 2016). We employ the models that have the best perplexity on the validation set for the evaluation. We set the beam size to 8 during inference. To test the universality of our findings, we conduct experiments on DE→EN and ZH→EN as well. For DE→EN, we use the training data from the WMT17 shared task (Bojar et al., 2017). We use newstest2013 as the validation set and newstest2017 as the test set. We learn"
R19-1136,N18-2082,0,0.050386,"Missing"
R19-1136,W18-6319,0,0.0611463,"ini-batch contains 2,048 tokens. A model checkpoint is saved every 1,000 updates. We use Adam (Kingma and Ba, 2015) as the optimizer. The initial learning rate is set to 0.0001. If the performance on the validation set has not improved for 8 checkpoints, the learning rate is multiplied by 0.7. We set the early stopping patience to 32 checkpoints. The training data is from the WMT15 shared task (Bojar et al., 2015) on Finnish–English (FI– EN). We choose newsdev2015 as the validation set and use newstest2015 as the test set. All the BLEU (Papineni et al., 2002) scores are measured by SacreBLEU (Post, 2018). There are about 2.1M sentence pairs in the training set after preprocessing. We learn a joint BPE model with 32K subword units (Sennrich et al., 2016). We employ the models that have the best perplexity on the validation set for the evaluation. We set the beam size to 8 during inference. To test the universality of our findings, we conduct experiments on DE→EN and ZH→EN as well. For DE→EN, we use the training data from the WMT17 shared task (Bojar et al., 2017). We use newstest2013 as the validation set and newstest2017 as the test set. We learn a joint BPE model with 32k subword units. For"
R19-1136,P16-1162,1,0.372616,"tial learning rate is set to 0.0001. If the performance on the validation set has not improved for 8 checkpoints, the learning rate is multiplied by 0.7. We set the early stopping patience to 32 checkpoints. The training data is from the WMT15 shared task (Bojar et al., 2015) on Finnish–English (FI– EN). We choose newsdev2015 as the validation set and use newstest2015 as the test set. All the BLEU (Papineni et al., 2002) scores are measured by SacreBLEU (Post, 2018). There are about 2.1M sentence pairs in the training set after preprocessing. We learn a joint BPE model with 32K subword units (Sennrich et al., 2016). We employ the models that have the best perplexity on the validation set for the evaluation. We set the beam size to 8 during inference. To test the universality of our findings, we conduct experiments on DE→EN and ZH→EN as well. For DE→EN, we use the training data from the WMT17 shared task (Bojar et al., 2017). We use newstest2013 as the validation set and newstest2017 as the test set. We learn a joint BPE model with 32k subword units. For ZH→EN, we choose the CWMT parallel data of the WMT17 shared task for training. We use newsdev2017 as the validation set and newstest2017 as the test set"
R19-1136,D18-1458,1,0.905703,"Missing"
R19-1136,W18-6304,1,0.912854,"inese and English separately. There are about 5.9M and 9M sentence pairs in the training set after preprocessing in DE→EN and ZH→EN, respectively. 3.2 Results Table 1 shows the performance of all the trained models. Encoder-free models (NMT-noEncs) perform rather poorly compared to conventional NMT models.3 It is interesting that Trans-noEnc obtains a BLEU score similar to the RNNS2S model. Even though the attention networks only attend to the non-contextualized word embeddings, Trans-noEnc still performs as well as the RNNS2S by paying attention to the context with multiple attention layers. Tang et al. (2018a) find that the superiority of Transformer models is attributed to the self-attention network which is a powerful semantic feature extractor. Given our results, we conclude that the attention mechanism is also a strong feature extractor in Trans-noEnc without self-attention in the encoder. Model Transformer Trans-noEnc RNNS2S RNNS2S-noEnc RNNS2S-noAtt RNNS2S-noAtt-noEnc Trans-noEnc-noPos Param. 104.4M 71.4M 91.5M 64.3M 90.3M 63.1M 71.4M PPL 9.6 11.7 14.9 25.2 33.3 53.7 26.6 BLEU 18.9 15.9 15.9 12.5 8.2 4.1 7.1 Table 1: The performance of NMT models. PPL is the perplexity on the development se"
R19-1136,P18-2060,0,0.0312967,"e linguistically plausible explanations in some cases – when translating a verb, knowledge about the subject, object etc. may be relevant information – other cases are harder to explain, such as an off-by-one mismatch between attention and word alignment for some models. We suspect that such a pattern can be learned if relevant information is passed to neighboring representations via recurrent or self-attentional connections. Ding et al. (2017) show that only using attention is not sufficient for deep interpretation and propose to use layer-wise relevance propagation to better understand NMT. Wang et al. (2018) replace the attention model with an alignment model and a lexical model to make NMT models more interpretable. The proposed model is not superior but on a par with the attentional model. They clarify the difference between alignment models and attention models by saying that that the alignment model is to identify translation equivalents while the attention model is to predict the next target word. In this paper, we try to understand NMT by simplifying the model. We explore the importance of different NMT components and what causes the performance gap after model simplification. 2.2 Alignment"
sennrich-kunz-2014-zmorge,telljohann-etal-2004-tuba,0,\N,Missing
sennrich-kunz-2014-zmorge,adolphs-2008-acquiring,0,\N,Missing
sennrich-kunz-2014-zmorge,C10-2129,0,\N,Missing
sennrich-kunz-2014-zmorge,W10-1734,0,\N,Missing
sennrich-kunz-2014-zmorge,W11-2126,0,\N,Missing
sennrich-kunz-2014-zmorge,R13-1079,1,\N,Missing
sennrich-kunz-2014-zmorge,schmid-etal-2004-smor,0,\N,Missing
W11-2120,W10-1739,0,0.251142,"ntrastive systems, we trained systems without the rescoring step described in section 2.2; we found no statistically significant difference from the submission system. In this translation task, the statistics from the parallel corpus seem to be ineffective at improving decoding, contrary to our findings in (Sennrich, 2011), where rescoring the phrase table improved BLEU scores by 0.7 points. We will address possible reasons for this discrepancy in the following section. 3.1 Interpretation The main characteristic that sets our approach apart from other system combination software such as MANY (Barrault, 2010) and MEMT (Heafield and Lavie, 2010) is its reliance on word and phrase frequencies in a parallel corpus to guide decoding, whereas MANY and MEMT operate purely on the target side, without requiring/exploiting the source text or parallel data. We integrate the information from a parallel corpus into the decoding process by extracting phrase translations from the translation hypotheses and scoring these phrase translations on the basis of the frequencies from the parallel corpus. The properties of this re-scored phrase table proved attractive for the translation task in (Sennrich, 2011), but le"
W11-2120,E06-1032,0,0.0479415,"Missing"
W11-2120,W09-0401,0,0.0608507,"primary system6 . We only saw few cases in which a system produced a translation against which there was evidence in our primary corpus. One instance is the German word Kindergarten (English: kindergarten; nursery), which is translated as children’s garden by one system. In the combined system, this translation is dispreferred. (Chen et al., 2009) argue that a combination of dissimilar systems might yield better results. Rule-based systems could fulfill this role; they are also an attractive choice given their high quality (as judged by human evaluators) in earlier evaluations (e.g. WMT 2009 (Callison-Burch et al., 2009)). We did not pursue this idea, since we optimized for highest BLEU score, both during MERT and for the selection of the submission system, a scoring method that has been shown to undervalue rule-based systems (CallisonBurch et al., 2006). The failure to outperform the individual best system in this translation task does not invalidate our approach. It merely highlights that different conditions call for different tools. Our approach relies strongly on parallel training data, in contrast to system combination tools such as MANY (Barrault, 2010) and MEMT (Heafield and Lavie, 2010). In this sett"
W11-2120,W10-1703,0,0.0606807,"et al., 2007). A Moses SMT system (Koehn et al., 2007) is used as a backbone, trained on the WMT 2011 training data. Translation hypotheses by other systems are integrated through a second phrase table. In this second phrase table, the phrase translation probabilities and lexical weights are computed based on the word and phrase frequencies in both the translation hypotheses and a parallel training corpus. On the evaluation data in (Sennrich, 2011), this system significantly outperformed MEMT (Heafield and Lavie, 2010), which was among the best-performing system combination tools at WMT 2010 (Callison-Burch et al., 2010). In this paper, we apply the same approach to a different translation scenario, namely the WMT 2011 System Description We participated in the system combination task DE– EN and EN–DE. Since the combination is achieved by integrating translation hypotheses into an existing Moses system, which we will call the primary system, we first describe the methods and data used for training this primary system. Then, we describe how the translation hypotheses are selected out of the individual system submissions and integrated into the Moses system. 2.1 Primary System For the training of the primary sys"
W11-2120,W07-0726,0,0.0720243,"tions DE–EN and EN–DE. The system uses Moses as a backbone, with the outputs of the 2–3 best individual systems being integrated through additional phrase tables. The system compares well to other system combination submissions, with no other submission being significantly better. A BLEU-based comparison to the individual systems, however, indicates that it achieves no significant gains over the best individual system. 1 2 Introduction For our submission to the WMT 2011 shared task, we built a system with the multi-engine MT approach described in (Sennrich, 2011), which builds on the idea by (Chen et al., 2007). A Moses SMT system (Koehn et al., 2007) is used as a backbone, trained on the WMT 2011 training data. Translation hypotheses by other systems are integrated through a second phrase table. In this second phrase table, the phrase translation probabilities and lexical weights are computed based on the word and phrase frequencies in both the translation hypotheses and a parallel training corpus. On the evaluation data in (Sennrich, 2011), this system significantly outperformed MEMT (Heafield and Lavie, 2010), which was among the best-performing system combination tools at WMT 2010 (Callison-Burc"
W11-2120,W09-0405,0,0.074674,"ithout significant loss in performance (see (Sennrich, 2011)). 167 System Primary Best individual Submission Vanilla scoring BLEU 21.11 24.16 24.44 24.42 Table 1: DE–EN results. Case-insensitive BLEU scores. 2.3 Hypothesis Selection For the secondary phrase table, we chose to select the n best individual systems according to their BLEU score on the tuning set. We determined the optimal n empirically by trying different n, measuring each system’s BLEU score on the tuning set and selecting the highest-scoring one. For the DE–EN translation task, n = 2 turned out to be optimal, for EN–DE, n = 3. Chen et al. (2009) propose additional, tunable features in the phrase table to indicate the origin of phrase translations. For better comparability with the results described in (Sennrich, 2011), we did not add such features. This means that there are no a priori weights that bias the phrase selection for or against certain systems, but that decoding is purely driven by the usual Moses features: two phrase tables – the primary one and the re-scored, secondary one – the language model, the primary reordering model, and the corresponding parameters established through MERT. 3 Results In the manual evaluation, the"
W11-2120,P05-1066,0,0.11245,"Missing"
W11-2120,W08-0509,0,0.0135852,"ies for later re-use. We did not submit the primary system outputs to the Machine Translation shared task, since we did not experiment with new techniques. Instead, the primary system forms the backbone of the system combination system. 2.2 Integrating Secondary Phrase Tables To combine the output of several systems, we train a second phrase table on the translation hypotheses of these systems. For this, we create a parallel corpus consisting of n translation hypotheses and n copies of the corresponding source text, both lowercased and detokenized.2 We compute the word alignment with MGIZA++ (Gao and Vogel, 2008), based on the word alignment model from the primary corpus that we have previously saved to disk. After training a phrase table from the wordaligned corpus with Moses, the lexical weights and translation probabilities are rescored, using the sufficient statistics (i.e. the word, phrase and word/phrase pair counts) of both the primary and the secondary corpus. This rescoring step has been shown to markedly improve performance in (Sennrich, 2011). We will discuss its effects in section 3.1. The rescored phrase table is integrated into the primary Moses system as an alternative decoding path, an"
W11-2120,W10-1744,0,0.228075,"m with the multi-engine MT approach described in (Sennrich, 2011), which builds on the idea by (Chen et al., 2007). A Moses SMT system (Koehn et al., 2007) is used as a backbone, trained on the WMT 2011 training data. Translation hypotheses by other systems are integrated through a second phrase table. In this second phrase table, the phrase translation probabilities and lexical weights are computed based on the word and phrase frequencies in both the translation hypotheses and a parallel training corpus. On the evaluation data in (Sennrich, 2011), this system significantly outperformed MEMT (Heafield and Lavie, 2010), which was among the best-performing system combination tools at WMT 2010 (Callison-Burch et al., 2010). In this paper, we apply the same approach to a different translation scenario, namely the WMT 2011 System Description We participated in the system combination task DE– EN and EN–DE. Since the combination is achieved by integrating translation hypotheses into an existing Moses system, which we will call the primary system, we first describe the methods and data used for training this primary system. Then, we describe how the translation hypotheses are selected out of the individual system"
W11-2120,D07-1103,0,0.0174451,"ypotheses are selected out of the individual system submissions and integrated into the Moses system. 2.1 Primary System For the training of the primary systems, we mostly followed the baseline instructions for the translation task1 . We use news-commentary and Europarl as parallel training data. The language models are a linear interpolation of the news-commentary, Europarl and news corpora, optimized for minimal cross-entropy on the newstest2008 data sets in the respective target language. Additionally, we prune the primary phrase table using statistical significance tests, as described by (Johnson et al., 2007). For the translation direction DE–EN, the German source text is reordered based 1 described at http://www.statmt.org/wmt11/ baseline.html 166 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 166–170, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics on syntactic parsing with Pro3GresDE (Sennrich et al., 2009), and reordering rules similar to those described by (Collins et al., 2005). The Moses phrase table consists of five features: phrase translation probabilities in both translation directions (p(t|s) and p(s|t)), lexical we"
W11-2120,N03-1017,0,0.004401,"is reordered based 1 described at http://www.statmt.org/wmt11/ baseline.html 166 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 166–170, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics on syntactic parsing with Pro3GresDE (Sennrich et al., 2009), and reordering rules similar to those described by (Collins et al., 2005). The Moses phrase table consists of five features: phrase translation probabilities in both translation directions (p(t|s) and p(s|t)), lexical weights (lex(t|s) and lex(s|t)), and a constant phrase penalty (Koehn et al., 2003). The computation of the phrase translation probabilities and lexical weights is based on the word, phrase and word/phrase pair frequencies that are extracted from the parallel corpus. We modified the Moses training scripts to collect and store these frequencies for later re-use. We did not submit the primary system outputs to the Machine Translation shared task, since we did not experiment with new techniques. Instead, the primary system forms the backbone of the system combination system. 2.2 Integrating Secondary Phrase Tables To combine the output of several systems, we train a second phra"
W11-2120,P07-2045,0,0.0109489,"oses as a backbone, with the outputs of the 2–3 best individual systems being integrated through additional phrase tables. The system compares well to other system combination submissions, with no other submission being significantly better. A BLEU-based comparison to the individual systems, however, indicates that it achieves no significant gains over the best individual system. 1 2 Introduction For our submission to the WMT 2011 shared task, we built a system with the multi-engine MT approach described in (Sennrich, 2011), which builds on the idea by (Chen et al., 2007). A Moses SMT system (Koehn et al., 2007) is used as a backbone, trained on the WMT 2011 training data. Translation hypotheses by other systems are integrated through a second phrase table. In this second phrase table, the phrase translation probabilities and lexical weights are computed based on the word and phrase frequencies in both the translation hypotheses and a parallel training corpus. On the evaluation data in (Sennrich, 2011), this system significantly outperformed MEMT (Heafield and Lavie, 2010), which was among the best-performing system combination tools at WMT 2010 (Callison-Burch et al., 2010). In this paper, we apply"
W11-2120,P02-1040,0,0.0803909,"Missing"
W11-2120,W05-0908,0,0.0560302,"Missing"
W11-2120,2011.eamt-1.14,1,0.372712,"tem combination task for the translation directions DE–EN and EN–DE. The system uses Moses as a backbone, with the outputs of the 2–3 best individual systems being integrated through additional phrase tables. The system compares well to other system combination submissions, with no other submission being significantly better. A BLEU-based comparison to the individual systems, however, indicates that it achieves no significant gains over the best individual system. 1 2 Introduction For our submission to the WMT 2011 shared task, we built a system with the multi-engine MT approach described in (Sennrich, 2011), which builds on the idea by (Chen et al., 2007). A Moses SMT system (Koehn et al., 2007) is used as a backbone, trained on the WMT 2011 training data. Translation hypotheses by other systems are integrated through a second phrase table. In this second phrase table, the phrase translation probabilities and lexical weights are computed based on the word and phrase frequencies in both the translation hypotheses and a parallel training corpus. On the evaluation data in (Sennrich, 2011), this system significantly outperformed MEMT (Heafield and Lavie, 2010), which was among the best-performing sy"
W11-4624,P91-1022,0,0.667755,"proach can supersede the dependence of the algorithm on existing MT systems. (Sennrich and Volk, 2010) have demonstrated that the sentence alignment quality of their MTbased algorithm depends on the quality of the MT system. If we can produce a superior MT system using Bleualign, it is worthwhile to test if the resulting MT system can in turn be used for an even Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 175–182 Rico Sennrich and Martin Volk better sentence alignment. 2 Related Work The first sentence alignment algorithms by (Brown et al., 1991) and (Gale and Church, 1993) are based on a length-comparison between source and target text and work without language-specific information.1 A second strand of sentence alignment algorithms work with lexical correspondences. This is either done on the basis of correspondence rules (Simard et al., 1993), with external dictionaries (Varga et al., 2005), or using a translation model trained on the parallel text itself (Moore, 2002; Varga et al., 2005). The latter requires a preliminary sentence alignment of the parallel text, usually performed with a lengthbased algorithm. After this first pass,"
W11-4624,P07-2045,0,0.0218326,"lowing steps: 1. Sentence-align the parallel training corpus. • In the first iteration, use an implementation of the Gale & Church algorithm (or any other sentence alignment tool that does not require additional resources). • In all subsequent iterations: – Automatically translate the corpus using the SMT system trained in the last iteration. – Align the texts using Bleualign and this translation. 2. Train an SMT system on the sentence-aligned corpus. The language model needs only be trained once; we use SRILM (Stolcke, 2002). The SMT system is built with GIZA++ (Och and Ney, 2003) and Moses (Koehn et al., 2007). The most timeconsuming part of each iteration is typically the automatic translation of the training set. In the remainder of this section, we will discuss how the alignment algorithm works, and what potential problems the iterative approach brings. 4.1 The Sentence Alignment Algorithm The sentence alignment algorithm, first described in (Sennrich and Volk, 2010), is a two-pass approach. In the first pass, dynamic programming is used to find a set of 1-to-1 beads that maximizes BLEU score in the document without violating the monotonic order of sentence pairs. In the second pass, unaligned s"
W11-4624,W04-3250,0,0.0459109,"es (Koehn et al., 2007), will be evaluated on a test set of 1000 sentences, held-out from training. The training set consists of 3 300 000 German and 3 740 000 French tokens, measured before sentence alignment.7 We test translation performance in the direction DE–FR, and use a language model trained on 9 511 000 tokens of in-domain text. We did not perform Minimum Error Rate Training, which is typically the most time-intensive step of training an SMT system, in order to limit the computational cost of the iterative approach. Statistical significance is tested with paired bootstrap resampling (Koehn, 2004). 5.1 Results We first establish baseline scores achieved by either using the Gale & Church algorithm or Bleualign with an out-of-domain MT system, shown in table 2. For a wider comparison of different sentence alignment algorithms, see (Lambert et al., 2010). On the alignment test set, Gale & Church’s algorithm fails almost entirely; only 1 out of 468 alignment hypotheses is correct. The reason for the bad performance of the Gale & Church algorithm in this evaluation is that errors tend to propagate, since misaligned sentences may cause neighbouring sentences to be misaligned as well. This is"
W11-4624,J03-1002,0,0.0238675,"iteration consists of the following steps: 1. Sentence-align the parallel training corpus. • In the first iteration, use an implementation of the Gale & Church algorithm (or any other sentence alignment tool that does not require additional resources). • In all subsequent iterations: – Automatically translate the corpus using the SMT system trained in the last iteration. – Align the texts using Bleualign and this translation. 2. Train an SMT system on the sentence-aligned corpus. The language model needs only be trained once; we use SRILM (Stolcke, 2002). The SMT system is built with GIZA++ (Och and Ney, 2003) and Moses (Koehn et al., 2007). The most timeconsuming part of each iteration is typically the automatic translation of the training set. In the remainder of this section, we will discuss how the alignment algorithm works, and what potential problems the iterative approach brings. 4.1 The Sentence Alignment Algorithm The sentence alignment algorithm, first described in (Sennrich and Volk, 2010), is a two-pass approach. In the first pass, dynamic programming is used to find a set of 1-to-1 beads that maximizes BLEU score in the document without violating the monotonic order of sentence pairs."
W11-4624,P02-1040,0,0.086343,"Missing"
W11-4624,2010.amta-papers.14,1,0.846756,"r performance, it is not sufficient to only use easily accessible and alignable texts as training material for SMT systems; ideally, SMT systems should be trained on texts that are similar to those one wishes to translate. This warrants continued research on more robust sentence alignment algorithms. Bleualign is a sentence alignment algorithm that, instead of computing an alignment between the source and target text directly, bases its alignment search on an MT translation of the source text. It has been shown that Bleualign can robustly align texts for which other algorithms perform poorly (Sennrich and Volk, 2010). The quality of sentence alignment has an effect on the performance of SMT systems (Lambert et al., 2010), but high quality is also desirable for other purposes, e.g. when building a translation memory from a text corpus. The main disadvantage of an MT-based algorithm is that it requires an existing MT system. For resource-poor language pairs, this requirement makes the algorithm unattractive. We have investigated the bootstrapping of MTbased sentence alignment with an MT system trained on the to-be-aligned corpus. For this first MT system, a length-based sentence alignment algorithm is used"
W11-4624,J93-1004,0,0.946152,"dependence of the algorithm on existing MT systems. (Sennrich and Volk, 2010) have demonstrated that the sentence alignment quality of their MTbased algorithm depends on the quality of the MT system. If we can produce a superior MT system using Bleualign, it is worthwhile to test if the resulting MT system can in turn be used for an even Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 175–182 Rico Sennrich and Martin Volk better sentence alignment. 2 Related Work The first sentence alignment algorithms by (Brown et al., 1991) and (Gale and Church, 1993) are based on a length-comparison between source and target text and work without language-specific information.1 A second strand of sentence alignment algorithms work with lexical correspondences. This is either done on the basis of correspondence rules (Simard et al., 1993), with external dictionaries (Varga et al., 2005), or using a translation model trained on the parallel text itself (Moore, 2002; Varga et al., 2005). The latter requires a preliminary sentence alignment of the parallel text, usually performed with a lengthbased algorithm. After this first pass, a translation model can be"
W11-4624,D07-1103,0,0.401493,"timation that the phrase - AlbertEggler : is translated to 1954 , Helmut Heuberger en g´eographie ; with a probability of 1.5 Hence, the sentence is mistranslated during the next iteration. The problem is that such mistranslations may cause the same alignment errors to be made in subsequent iterations. In order to prevent random misalignments to 5 The term phrase is used to denote arbitrary word sequences in SMT, without syntactic implications. In this case, the whole sentence is treated as a single phrase by the SMT system. be fossilized, we prune the translation model using the approach by (Johnson et al., 2007). The pruning is based on computing whether the cooccurrence frequency of phrase pairs in the translation model is statistically significant, or to be expected by chance. All phrase pairs whose significance value fall below a predefined threshold are discarded. We chose the significance threshold α + , which among others discards all phrase pairs that co-occur only once.6 5 Evaluation For the evaluation of alignment quality, we manually aligned an article consisting of 468 and 554 sentences (German and French, respectively). This manual alignment serves as a gold standard to which the automat"
W11-4624,volk-etal-2010-challenges,1,0.855556,"Missing"
W11-4624,moore-2002-fast,0,\N,Missing
W11-4624,J93-2003,0,\N,Missing
W11-4633,E06-1032,0,0.0633045,"Missing"
W11-4633,P07-1005,0,0.0257915,"researchers have worked on this topic before with varying success. For example, (Carpuat and Wu, 2005) reported that they could not find “significant better translations” when using Chinese WSD in Chinese-English MT. But two years later the same authors (Carpuat and Wu, 2007) come to the conclusion that the incorporation of WSD within a typical SMT system “consistently improves translation quality” for Chinese-English. They claim that a disambiguation of phrasal units rather than words leads to these improvements. They report on gains of up to 0.5 BLEU points. These findings are in line with (Chan et al., 2007) who have also shown WSD to be beneficial for Chinese to English translation. Basic research on WSD for MT is presented in various papers. For example (Specia et al., 2005) work with automatically derived rules for WSD of seven highly ambiguous verbs in EnglishPortuguese MT. (Apidianaki, 2009) questions the sense inventory which is frequently used in WSD and argues for a semantic analysis based on parallel corpora Greek-English in order to better tailor the sense inventory to MT. (Vickrey et al., 2005) investigate WSD for word translation FrenchEnglish. Our work is also similar to other prepro"
W11-4633,2010.iwslt-papers.6,0,0.0220094,"e beneficial for Chinese to English translation. Basic research on WSD for MT is presented in various papers. For example (Specia et al., 2005) work with automatically derived rules for WSD of seven highly ambiguous verbs in EnglishPortuguese MT. (Apidianaki, 2009) questions the sense inventory which is frequently used in WSD and argues for a semantic analysis based on parallel corpora Greek-English in order to better tailor the sense inventory to MT. (Vickrey et al., 2005) investigate WSD for word translation FrenchEnglish. Our work is also similar to other preprocessing suggestions such as (El-Kahlout and Yvon, 2010). They work on the opposite translation direction and prepare the German input text before training and translation to English. When testing various normalization steps, they obtained the biggest improvements on compound splitting. 3 Our MT Systems for TV Subtitles MT systems for subtitles date back to the work by Popowich et al. (2000) on English to Spanish translation. We have built Machine Translation systems for translating film and TV subtitles from English to Swedish and from Swedish to Danish and Norwegian in a commercial setting. Some of this work has been described earlier by Volk and"
W11-4633,W00-0506,0,0.0467053,"a semantic analysis based on parallel corpora Greek-English in order to better tailor the sense inventory to MT. (Vickrey et al., 2005) investigate WSD for word translation FrenchEnglish. Our work is also similar to other preprocessing suggestions such as (El-Kahlout and Yvon, 2010). They work on the opposite translation direction and prepare the German input text before training and translation to English. When testing various normalization steps, they obtained the biggest improvements on compound splitting. 3 Our MT Systems for TV Subtitles MT systems for subtitles date back to the work by Popowich et al. (2000) on English to Spanish translation. We have built Machine Translation systems for translating film and TV subtitles from English to Swedish and from Swedish to Danish and Norwegian in a commercial setting. Some of this work has been described earlier by Volk and Harder (2007) and Volk (2008). Most films are originally in English and receive English or Swedish subtitles in a first manual step. The subtitler uses the English video and audio (sometimes accompanied by an English transcript). The target language translator subsequently has access to the original English video and audio but also to"
W11-4633,H05-1097,0,0.0211466,"when using Chinese WSD in Chinese-English MT. But two years later the same authors (Carpuat and Wu, 2007) come to the conclusion that the incorporation of WSD within a typical SMT system “consistently improves translation quality” for Chinese-English. They claim that a disambiguation of phrasal units rather than words leads to these improvements. They report on gains of up to 0.5 BLEU points. These findings are in line with (Chan et al., 2007) who have also shown WSD to be beneficial for Chinese to English translation. Basic research on WSD for MT is presented in various papers. For example (Specia et al., 2005) work with automatically derived rules for WSD of seven highly ambiguous verbs in EnglishPortuguese MT. (Apidianaki, 2009) questions the sense inventory which is frequently used in WSD and argues for a semantic analysis based on parallel corpora Greek-English in order to better tailor the sense inventory to MT. (Vickrey et al., 2005) investigate WSD for word translation FrenchEnglish. Our work is also similar to other preprocessing suggestions such as (El-Kahlout and Yvon, 2010). They work on the opposite translation direction and prepare the German input text before training and translation t"
W11-4633,2007.mtsummit-papers.66,1,0.816798,"n, 2010). They work on the opposite translation direction and prepare the German input text before training and translation to English. When testing various normalization steps, they obtained the biggest improvements on compound splitting. 3 Our MT Systems for TV Subtitles MT systems for subtitles date back to the work by Popowich et al. (2000) on English to Spanish translation. We have built Machine Translation systems for translating film and TV subtitles from English to Swedish and from Swedish to Danish and Norwegian in a commercial setting. Some of this work has been described earlier by Volk and Harder (2007) and Volk (2008). Most films are originally in English and receive English or Swedish subtitles in a first manual step. The subtitler uses the English video and audio (sometimes accompanied by an English transcript). The target language translator subsequently has access to the original English video and audio but also to the source language subtitles and the time codes. In most cases the translator will reuse the time codes and insert the target language subtitle. She can, on occasion, change the time codes if she deems them inappropriate for the target language text. We have built SMT system"
W11-4633,2010.jec-1.7,1,0.900795,"Missing"
W11-4633,P05-1048,0,0.0351715,"ins also capitalized subtitles like CLEOPATRA’S BEAUTY SALON which we have not counted here. Bolette Sandford Pedersen, Gunta Neˇspore and Inguna Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 238–245 Disambiguation of English Contractions for Machine Translation of TV Subtitles by describing some related disambiguation work and then our MT systems for subtitles. 2 Related Work on Word Sense Disambiguation for MT Our approach can be seen as a special type of word sense disambiguation (WSD) for MT. Many researchers have worked on this topic before with varying success. For example, (Carpuat and Wu, 2005) reported that they could not find “significant better translations” when using Chinese WSD in Chinese-English MT. But two years later the same authors (Carpuat and Wu, 2007) come to the conclusion that the incorporation of WSD within a typical SMT system “consistently improves translation quality” for Chinese-English. They claim that a disambiguation of phrasal units rather than words leads to these improvements. They report on gains of up to 0.5 BLEU points. These findings are in line with (Chan et al., 2007) who have also shown WSD to be beneficial for Chinese to English translation. Basic"
W11-4633,D07-1007,0,0.0319294,"1 Conference Proceedings, pp. 238–245 Disambiguation of English Contractions for Machine Translation of TV Subtitles by describing some related disambiguation work and then our MT systems for subtitles. 2 Related Work on Word Sense Disambiguation for MT Our approach can be seen as a special type of word sense disambiguation (WSD) for MT. Many researchers have worked on this topic before with varying success. For example, (Carpuat and Wu, 2005) reported that they could not find “significant better translations” when using Chinese WSD in Chinese-English MT. But two years later the same authors (Carpuat and Wu, 2007) come to the conclusion that the incorporation of WSD within a typical SMT system “consistently improves translation quality” for Chinese-English. They claim that a disambiguation of phrasal units rather than words leads to these improvements. They report on gains of up to 0.5 BLEU points. These findings are in line with (Chan et al., 2007) who have also shown WSD to be beneficial for Chinese to English translation. Basic research on WSD for MT is presented in various papers. For example (Specia et al., 2005) work with automatically derived rules for WSD of seven highly ambiguous verbs in Engl"
W11-4633,E09-1010,0,\N,Missing
W12-3105,W10-1703,0,0.0513929,"Missing"
W12-3105,W11-2106,0,0.0901512,"Missing"
W12-3105,fishel-etal-2012-terra,1,0.86613,"Missing"
W12-3105,N06-1014,0,0.0173292,"Zeman et al., 2011) and Hjerson (Popovi´c and Ney, 2011) use different methods for automatic error analysis. Addicter explicitly aligns the hypothesis and reference translations and induces error categories based on the alignment coverage while Hjerson compares words encompassed in the WER (word error rate) and PER (position-independent word error rate) scores to the same end. Previous evaluation of Addicter shows that hypothesis-reference alignment coverage (in terms of discovered word pairs) directly influences error analysis quality; to increase alignment coverage we used Berkeley aligner (Liang et al., 2006) and trained it on and applied it to the whole set of reference-hypothesis pairs for every language pair. Both tools use word lemmas for their analysis; we used TreeTagger (Schmid, 1995) for analyzing English, Spanish, German and French and Morˇce (Spoustov´a et al., 2007) to analyze Czech. The same tools are used for PoS-tagging in some experiments. 2.2 HYP -1 Binary Classification Pairwise comparison of sentence pairs is achieved with a binary SVM classifier, trained via sequential minimal optimization (Platt, 1998), implemented in Weka (Hall et al., 2009). The input feature vectors are comp"
W12-3105,P02-1040,0,0.0842937,"Missing"
W12-3105,W11-2111,0,0.0174685,"e scores remain to be checked against the human 69 judgments from WMT’12. The introduced TerrorCat metric has certain dependencies. For one thing, in order to apply it to new languages, a training set of manual rankings is required – although this can be viewed as an advantage, since it enables the user to tune the metric to his/her own preference. Additionally, the metric depends on lemmatization and PoS-tagging. There is a number of directions to explore in the future. For one, both Addicter and Hjerson report MT errors related more to adequacy than fluency, although it was shown last year (Parton et al., 2011) that fluency is an important component in rating translation quality. It is also important to test how well the metric performs if lemmatization and PoStagging are not available. For this year’s competition, training data was taken separately for every language pair; it remains to be tested whether combining human judgements with the same target language and different source languages leads to better or worse performance. To conclude, we have described TerrorCat, one of the submissions to the metrics shared task of WMT’12. TerrorCat is rather demanding to apply on one hand, having more requir"
W12-3105,J11-4002,1,0.872172,"Missing"
W12-3105,W07-1709,0,0.0315439,"Missing"
W14-3310,E14-2008,1,0.50059,"nburgh, Scotland † Karlsruhe Institute of Technology, Karlsruhe, Germany ∗ {freitag,peitz,wuebker,ney}@cs.rwth-aachen.de ‡ {mhuck,ndurrani,pkoehn}@inf.ed.ac.uk ‡ v1rsennr@staffmail.ed.ac.uk ‡ maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk † {teresa.herrmann,eunah.cho,alex.waibel}@kit.edu ‡ Matthias Abstract joint WMT submission of three EU-BRIDGE project partners. RWTH Aachen University (RWTH), the University of Edinburgh (UEDIN) and Karlsruhe Institute of Technology (KIT) all provided several individual systems which were combined by means of the RWTH Aachen system combination approach (Freitag et al., 2014). As distinguished from our EU-BRIDGE joint submission to the IWSLT 2013 evaluation campaign (Freitag et al., 2013), we particularly focused on translation of news text (instead of talks) for WMT. Besides, we put an emphasis on engineering syntaxbased systems in order to combine them with our more established phrase-based engines. We built combined system setups for translation from German to English as well as from English to German. This paper gives some insight into the technology behind the system combination framework and the combined engines which have been used to produce the joint EU-B"
W14-3310,D08-1089,0,0.0336771,"sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been emp"
W14-3310,N04-1035,0,0.0285873,"gmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, an"
W14-3310,P05-1066,1,0.0515024,"employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tune"
W14-3310,P06-1121,0,0.0128251,". The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model"
W14-3310,P13-2071,1,0.0664637,"ction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolate"
W14-3310,2012.iwslt-papers.17,1,0.805256,".1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel"
W14-3310,W13-2212,1,0.898684,"ction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolate"
W14-3310,P12-1031,0,0.00714997,"um Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes"
W14-3310,P13-2121,1,0.0604984,"Missing"
W14-3310,W14-3309,1,0.840972,"btained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translatio"
W14-3310,W11-2123,0,0.00995075,"a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus"
W14-3310,W06-1607,0,0.0222136,"., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering"
W14-3310,W13-0805,1,0.0274022,"GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with s"
W14-3310,2011.iwslt-papers.5,1,0.681344,"rying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase t"
W14-3310,2009.iwslt-papers.4,1,0.346713,"bility, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model scoring during decoding. Model weights are optimized to maximize B LEU. 2000 sentences from the newstest2008-2012 sets have been selected as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser"
W14-3310,D09-1022,1,0.0473567,"ingle generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012. System combination weights are either optimized on newstest2011 or newstest2012. We kept newstest2013 as an unseen test set wh"
W14-3310,P07-1019,0,0.0254758,"he settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language mod"
W14-3310,2011.iwslt-evaluation.9,1,0.882056,"n, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is d"
W14-3310,W13-2258,1,0.0602517,"r IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHK"
W14-3310,P10-2041,0,0.0226125,"for tuning the system combination or any of the individual systems. In total, the English→German system uses the following language models: two 4-gram wordbased language models trained on the parallel data and the filtered Common Crawl data separately, two 5-gram POS-based language models trained on the same data as the word-based language models, and a 4-gram cluster-based language model trained on 1,000 MKCLS word classes. The German→English system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). Again, a 4-gram cluster-based language model trained on 1000 MKCLS word classes is applied. 5 6.1 The automatic scores of all individual systems as well as of our final system combination submission are given in Table 1. KIT, UEDIN and RWTH are each providing one individual phrasebased system output. RWTH (hiero) and UEDIN (GHKM) are providing additional systems based on the hierarchical translation model and a stringto-tree syntax model. The pairwise difference of the single system performances is up to 1.3 points in B LEU and 2.5 points in T ER. For German→English, our system combination p"
W14-3310,W14-3362,1,0.700694,"ned with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT trans"
W14-3310,W09-0435,0,0.00463497,"erated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained"
W14-3310,P03-1054,0,0.00425242,"ion obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the sy"
W14-3310,W08-0303,0,0.0192279,"sing an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word ord"
W14-3310,D07-1091,1,0.0290057,"2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English→"
W14-3310,E03-1076,1,0.31096,"the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och"
W14-3310,W11-2124,1,0.0228284,"trip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012. System combination weights are either optimized on news"
W14-3310,2005.iwslt-1.8,1,0.035532,"2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single"
W14-3310,J03-1002,1,0.0147027,"of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid,"
W14-3310,E99-1010,0,0.0419329,"l., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 div"
W14-3310,P07-2045,1,0.0154876,"th a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004),"
W14-3310,P03-1021,0,0.0102254,"03) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has"
W14-3310,W14-3317,1,0.820032,"ty, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in B LEU and 1.0 points in T ER on the WMT newstest2013 test set compared to the best single systems. 1 Introduction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering"
W14-3310,N04-1022,0,0.063305,"it (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual train"
W14-3310,W10-1738,1,0.159493,"Missing"
W14-3310,W08-1005,0,0.0331415,"nce reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2"
W14-3310,P06-1055,0,0.0182004,"dditional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English→German, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs ar"
W14-3310,W12-3150,1,0.544959,"uster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model a"
W14-3310,W08-1006,0,0.0232292,"e system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koeh"
W14-3310,W14-3324,1,0.0949085,"Missing"
W14-3310,2007.tmi-papers.21,0,0.125992,", 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system tra"
W14-3310,C12-3061,1,0.125144,"013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has"
W14-3310,C08-1098,0,0.00933067,"target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newst"
W14-3310,D13-1138,1,0.0721313,", morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules ("
W14-3310,C04-1024,0,0.0194264,"f the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottman"
W14-3310,R13-1079,1,0.28133,"stem and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster e"
W14-3310,N07-1051,0,\N,Missing
W14-3310,W05-0836,1,\N,Missing
W14-3310,W05-0909,0,\N,Missing
W14-3310,N13-1001,1,\N,Missing
W14-3310,2010.iwslt-evaluation.11,1,\N,Missing
W14-3310,2013.iwslt-evaluation.16,1,\N,Missing
W14-3310,W13-2213,1,\N,Missing
W14-3310,W14-3313,1,\N,Missing
W14-3310,W11-2145,1,\N,Missing
W14-3310,2013.iwslt-evaluation.3,1,\N,Missing
W14-3324,D10-1063,0,0.0712692,"rench and Czech1 . We also experimented with random subsets of size 2,000. For the filtering technique, we make the assumption that finding suitable weights for all the feature functions requires the optimizer to see a range of feature values and to see hypotheses that can partially match the reference translations in order to rank the hypotheses. For example, if a Table 1: Parameter settings for rule composition. Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 2.3 Language Model We used all available monolingual data to train 5-gram language models. Language models for each monolingual corpus were trained using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) and then interpolated using weights tuned to minimize perplexity on the development set. 2.4 Tuning Feature Functions Our feature functions are unchanged from the previous two years. They include the n-gram lan1 For Russian and"
W14-3324,N12-1047,0,0.0432105,"ons: • p (C, β |α, ∼) and p (α |C, β, ∼), the direct and indirect translation probabilities. • plex (β |α) and plex (α |β), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (π), the monolingual PCFG probability of the tree fragment π from which the rule was extracted. • exp(−1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. The main grammar and glue grammars have distinct penalty features. Value 5 20 5 2.5 The feature weights were tuned using the Moses implementation of MERT (Och, 2003) for all systems except English-to-German, for which we used k-best MIRA (Cherry and Foster, 2012) due to the larger number of features. We used tuning sentences drawn from all of the previous years’ test sets (except newstest2013, which was used as the development test set). In order to speed up the tuning process, we used subsets of the full tuning sets with sentence pairs up to length 30 (Max-30) and further applied a filtering technique to reduce the tuning set size to 2,000 sentence pairs for the language pairs involving German, French and Czech1 . We also experimented with random subsets of size 2,000. For the filtering technique, we make the assumption that finding suitable weights"
W14-3324,N03-1017,1,0.0126398,"ngs shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C → hα, β, ∼i where C is a target-side non-terminal label, α is a string of source terminals and non-terminals, β is a string of target terminals and non-terminals, and ∼ is a one-to-one correspondence between source and target non-terminals, we score the rule according to the following functions: • p (C, β |α, ∼) and p (α |C, β, ∼), the direct and indirect translation probabilities. • plex (β |α) and plex (α |β), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (π), the monolingual PCFG probability of the tree fragment π from which the rule was extracted. • exp(−1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. The main grammar and glue grammars have distinct penalty features. Value 5 20 5 2.5 The feature weights were tuned using the Moses implementation of MERT (Och, 2003) for all systems except English-to-German, for which we used k-best MIRA (Cherry and Foster, 2012) due to the larger number of features. We used tuning sentences drawn from all of the previous years’ test sets (except newstest2013, which was used as the deve"
W14-3324,P11-2072,0,0.0390442,"sentence pairs for the language pairs involving German, French and Czech1 . We also experimented with random subsets of size 2,000. For the filtering technique, we make the assumption that finding suitable weights for all the feature functions requires the optimizer to see a range of feature values and to see hypotheses that can partially match the reference translations in order to rank the hypotheses. For example, if a Table 1: Parameter settings for rule composition. Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 2.3 Language Model We used all available monolingual data to train 5-gram language models. Language models for each monolingual corpus were trained using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998) and then interpolated using weights tuned to minimize perplexity on the development set. 2.4 Tuning Feature Functions Our feature functions are unchanged from the"
W14-3324,P07-2045,1,0.0160827,"of the parser. For the English-German system we used the default Moses tokenization scheme, which is similar to that of the German parsers. For the systems that translate into English, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to parse the target-side of the training corpus. As we will describe in Section 3, we tried a variety of parsers for German. We did not perform any corpus filtering other than the standard Moses method, which removes As last year (Nadejde et al., 2013), our systems are based on the string-to-tree pipeline implemented in the Moses toolkit (Koehn et al., 2007). We paid particular attention to the production of grammatical German, trying various parsers and incorporating target-side compound splitting and morphosyntactic constraints; for Hindi and Russian, we employed the new Moses transliteration model to handle out-of-vocabulary words; and for German to English, we experimented with tree binarization, obtaining good results from right binarization. We also present our first syntax-based results for French-English, the scale of which defeated us 207 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207–214, c Baltimore, Ma"
W14-3324,E14-4029,1,0.429224,"o English results on the devtest (newsdev2013) and test (newstest2014) sets. • passive clauses are not allowed to have accusative objects. 210 system baseline devtest 86,341,766 test 88,657,327 (OOV) input words are therefore a comparatively large source of translation error: in the devtest set (newsdev2014) and filtered test set (newstest2014) the average OOV rates are 1.08 and 1.16 unknown words per sentence, respectively. Assuming a significant fraction of OOV words to be named entities and thus amenable to transliteration, we applied the post-processing transliteration method described in Durrani et al. (2014) and implemented in Moses. In brief, this is an unsupervised method that i) uses EM to induce a corpus of transliteration examples from the parallel training data; ii) learns a monotone character-level phrasebased SMT model from the transliteration corpus; and iii) substitutes transliterations for OOVs in the system output by using the monolingual language model and other features to select between transliteration candidates.5 Table 10 shows B LEU scores with and without transliteration on the devtest and filtered test sets. Due to a bug in the submitted system, the language model trained on t"
W14-3324,W13-2221,1,0.776308,", we used the Moses tokenizer’s -penn option, which uses a tokenization scheme that more closely matches that of the parser. For the English-German system we used the default Moses tokenization scheme, which is similar to that of the German parsers. For the systems that translate into English, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to parse the target-side of the training corpus. As we will describe in Section 3, we tried a variety of parsers for German. We did not perform any corpus filtering other than the standard Moses method, which removes As last year (Nadejde et al., 2013), our systems are based on the string-to-tree pipeline implemented in the Moses toolkit (Koehn et al., 2007). We paid particular attention to the production of grammatical German, trying various parsers and incorporating target-side compound splitting and morphosyntactic constraints; for Hindi and Russian, we employed the new Moses transliteration model to handle out-of-vocabulary words; and for German to English, we experimented with tree binarization, obtaining good results from right binarization. We also present our first syntax-based results for French-English, the scale of which defeated"
W14-3324,N13-1073,0,0.0302293,"lative clauses must contain a relative (or interrogative) pronoun in their first constituent. Table 4 shows B LEU scores with systems trained with different parsers, and for our extensions of the baseline system. 4 Czech to English For Czech to English we used the core setup described in Section 2 without modification. Table 5 shows the B LEU scores. system baseline B LEU devtest test 24.8 27.0 Table 5: Czech to English results on the devtest (newstest2013) and test (newstest2014) sets. 5 French to English For French to English, alignment of the parallel corpus was performed using fast_align (Dyer et al., 2013) instead of MGIZA++ due to the large volume of parallel data. Table 6 shows B LEU scores for the system and Table 7 shows the resulting grammar sizes after filtering for the evaluation sets. • correct subcategorization of auxiliary/modal verbs in regards to the inflection of the full verb. system baseline B LEU devtest test 29.4 32.3 Table 6: French to English results on the devtest (newsdev2013) and test (newstest2014) sets. • passive clauses are not allowed to have accusative objects. 210 system baseline devtest 86,341,766 test 88,657,327 (OOV) input words are therefore a comparatively large"
W14-3324,W14-3310,1,0.0748258,"of Informatics, University of Edinburgh 2 Center for Speech and Language Processing, The Johns Hopkins University Abstract last year. This year we were able to train a system using all available training data, a task that was made considerably easier through principled filtering of the tuning set. Although our system was not ready in time for human evaluation, we present B LEU scores in this paper. In addition to the five single-system submissions described here, we also contributed our English-German and German-English systems for use in the collaborative EU-BRIDGE system combination effort (Freitag et al., 2014). This paper is organised as follows. In Section 2 we describe the core setup that is common to all systems. In subsequent sections we describe language-pair specific variations and extensions. For each language pair, we present results for both the development test set (newstest2013 in most cases) and for the filtered test set (newstest2014) that was provided after the system submission deadline. We refer to these as ‘devtest’ and ‘test’, respectively. This paper describes the string-to-tree systems built at the University of Edinburgh for the WMT 2014 shared translation task. We developed sy"
W14-3324,J03-1002,0,0.00637422,"he form sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C → hα, β, ∼i where C is a target-side non-terminal label, α is a string of source terminals and non-terminals, β is a string of target terminals and non-terminals, and ∼ is a one-to-one"
W14-3324,P03-1021,0,0.00672489,"source and target non-terminals, we score the rule according to the following functions: • p (C, β |α, ∼) and p (α |C, β, ∼), the direct and indirect translation probabilities. • plex (β |α) and plex (α |β), the direct and indirect lexical weights (Koehn et al., 2003). • ppcfg (π), the monolingual PCFG probability of the tree fragment π from which the rule was extracted. • exp(−1/count(r)), a rule rareness penalty. • exp(1), a rule penalty. The main grammar and glue grammars have distinct penalty features. Value 5 20 5 2.5 The feature weights were tuned using the Moses implementation of MERT (Och, 2003) for all systems except English-to-German, for which we used k-best MIRA (Cherry and Foster, 2012) due to the larger number of features. We used tuning sentences drawn from all of the previous years’ test sets (except newstest2013, which was used as the development test set). In order to speed up the tuning process, we used subsets of the full tuning sets with sentence pairs up to length 30 (Max-30) and further applied a filtering technique to reduce the tuning set size to 2,000 sentence pairs for the language pairs involving German, French and Czech1 . We also experimented with random subsets"
W14-3324,W10-1734,0,0.0101538,"cht Figure 1: Syntactic representation of split compound Bundesberufungsgericht (Engl: federal appeals court). B LEU devtest test 19.0 18.3 19.3 18.6 19.5 18.6 19.6 19.1 19.8 19.1 19.9 19.2 20.0 19.8 20.2 20.1 Table 4: English to German translation results on devtest (newstest2013) and test (newstest2014) sets. We discriminatively learn non-terminal labels for unknown words using sparse features, rather than estimating a probability distribution of nonterminal labels from singleton statistics in the training corpus. We perform target-side compound splitting, using a hybrid method described by Fritzinger and Fraser (2010) that combines a finite-state morphology and corpus statistics. As finite-state morphology analyzer, we use Zmorge (Sennrich and Kunz, 2014). An original contribution of our experiments is a syntactic representation of split compounds which eliminates typical problems with target-side compound splitting, namely erroneous reorderings and compound merging. We represent split compounds as a syntactic tree with the last segment as head, preceded by a modifier. A modifier consists of an optional modifier, a segment and a (possibly empty) joining element. An example is shown in Figure 1. This hierar"
W14-3324,N04-1035,0,0.0656309,"s word count, and various scores for the synchronous derivation. Each grammar rule has a number of precomputed scores. For a grammar rule r of the form sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C → hα, β, ∼i where C is a target-side non-t"
W14-3324,W08-1005,0,0.0619769,"e 19.2 19.2 19.1 19.4 De-En 26.9 27.0 27.2 27.0 Table 3: B LEU results on devtest and test sets with different tuning sets: Full, Max-30, filtered subsets of Max-30 and average of three random subsets of Max-30 (size of filtered/random subsets: 2,000). 3 English to German We use the projective output of the dependency parser ParZu (Sennrich et al., 2013) for the syntactic annotation of our primary submission. Contrastive systems were built with other parsers: BitPar (Schmid, 2004), the German Stanford Parser (Rafferty and Manning, 2008), and the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). The set of syntactic labels provided by ParZu has been refined to reduce overgeneralization phenomena. Specifically, we disambiguate the labels ROOT (used for the root of a sentence, but also commas, punctuation marks, and sentence fragments), KON and CJ (coordinations of different constituents), and GMOD (pre- or postmodifying genitive modifier). 2 These can be arbitrary tokens that do not match any reference token. 3 For random subsets from the full tuning set the performance was similar but resulted in standard deviations of up to 0.36 across three random sets. 4 Note however that due to"
W14-3324,P06-1121,0,0.026303,"ious scores for the synchronous derivation. Each grammar rule has a number of precomputed scores. For a grammar rule r of the form sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C → hα, β, ∼i where C is a target-side non-terminal label, α is a"
W14-3324,P06-1055,0,0.08415,"German • Czech-English • French-English 2 2.1 • German-English • Hindi-English • Russian-English System Overview Pre-processing The training data was normalized using the WMT normalize-punctuation.perl script then tokenized and truecased. Where the target language was English, we used the Moses tokenizer’s -penn option, which uses a tokenization scheme that more closely matches that of the parser. For the English-German system we used the default Moses tokenization scheme, which is similar to that of the German parsers. For the systems that translate into English, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to parse the target-side of the training corpus. As we will describe in Section 3, we tried a variety of parsers for German. We did not perform any corpus filtering other than the standard Moses method, which removes As last year (Nadejde et al., 2013), our systems are based on the string-to-tree pipeline implemented in the Moses toolkit (Koehn et al., 2007). We paid particular attention to the production of grammatical German, trying various parsers and incorporating target-side compound splitting and morphosyntactic constraints; for Hindi and Russian, we employed th"
W14-3324,W08-0509,0,0.0379218,"ule has a number of precomputed scores. For a grammar rule r of the form sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C → hα, β, ∼i where C is a target-side non-terminal label, α is a string of source terminals and non-terminals, β is a strin"
W14-3324,C04-1024,0,0.606547,"t En-De De-En 19.9 26.7 19.8 26.2 19.8 26.2 19.7 26.4 Tuning set Full Max-30 Filtered Random Cs-En 27.5 27.2 27.5 27.3 test En-De 19.2 19.2 19.1 19.4 De-En 26.9 27.0 27.2 27.0 Table 3: B LEU results on devtest and test sets with different tuning sets: Full, Max-30, filtered subsets of Max-30 and average of three random subsets of Max-30 (size of filtered/random subsets: 2,000). 3 English to German We use the projective output of the dependency parser ParZu (Sennrich et al., 2013) for the syntactic annotation of our primary submission. Contrastive systems were built with other parsers: BitPar (Schmid, 2004), the German Stanford Parser (Rafferty and Manning, 2008), and the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). The set of syntactic labels provided by ParZu has been refined to reduce overgeneralization phenomena. Specifically, we disambiguate the labels ROOT (used for the root of a sentence, but also commas, punctuation marks, and sentence fragments), KON and CJ (coordinations of different constituents), and GMOD (pre- or postmodifying genitive modifier). 2 These can be arbitrary tokens that do not match any reference token. 3 For random subsets from the full tuni"
W14-3324,sennrich-kunz-2014-zmorge,1,0.826405,"18.6 19.5 18.6 19.6 19.1 19.8 19.1 19.9 19.2 20.0 19.8 20.2 20.1 Table 4: English to German translation results on devtest (newstest2013) and test (newstest2014) sets. We discriminatively learn non-terminal labels for unknown words using sparse features, rather than estimating a probability distribution of nonterminal labels from singleton statistics in the training corpus. We perform target-side compound splitting, using a hybrid method described by Fritzinger and Fraser (2010) that combines a finite-state morphology and corpus statistics. As finite-state morphology analyzer, we use Zmorge (Sennrich and Kunz, 2014). An original contribution of our experiments is a syntactic representation of split compounds which eliminates typical problems with target-side compound splitting, namely erroneous reorderings and compound merging. We represent split compounds as a syntactic tree with the last segment as head, preceded by a modifier. A modifier consists of an optional modifier, a segment and a (possibly empty) joining element. An example is shown in Figure 1. This hierarchical representation ensures that compounds can be easily merged in post-processing (by removing the spaces and special characters around j"
W14-3324,R13-1079,1,0.108908,"ble 2: Size of full tuning sets and with sentence length up to 30. Tuning set Full Max-30 Filtered Random Cs-En 25.1 24.7 24.9 24.8 devtest En-De De-En 19.9 26.7 19.8 26.2 19.8 26.2 19.7 26.4 Tuning set Full Max-30 Filtered Random Cs-En 27.5 27.2 27.5 27.3 test En-De 19.2 19.2 19.1 19.4 De-En 26.9 27.0 27.2 27.0 Table 3: B LEU results on devtest and test sets with different tuning sets: Full, Max-30, filtered subsets of Max-30 and average of three random subsets of Max-30 (size of filtered/random subsets: 2,000). 3 English to German We use the projective output of the dependency parser ParZu (Sennrich et al., 2013) for the syntactic annotation of our primary submission. Contrastive systems were built with other parsers: BitPar (Schmid, 2004), the German Stanford Parser (Rafferty and Manning, 2008), and the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). The set of syntactic labels provided by ParZu has been refined to reduce overgeneralization phenomena. Specifically, we disambiguate the labels ROOT (used for the root of a sentence, but also commas, punctuation marks, and sentence fragments), KON and CJ (coordinations of different constituents), and GMOD (pre- or postmodifying g"
W14-3324,D07-1078,0,0.252787,"ystem after filtering for the devtest (newstest2013) and test (newstest2014) sets. 6 German to English German compounds were split using the script provided with Moses. For training the primary system, the target parse trees were restructured before rule extraction by right binarization. Since binarization strategies increase the tree depth and number of nodes by adding virtual non-terminals, we increased the extraction parameters to: Rule Depth = 7, Node Count = 100, Rule Size = 7. A thorough investigation of binarization methods for restructuring Penn Treebank style trees was carried out by Wang et al. (2007). Table 8 shows B LEU scores for the baseline system and two systems employing different binarization strategies. Table 9 shows the resulting grammar sizes after filtering for the evaluation sets. Results on the development set showed no improvement when left binarization was used for restructuring the trees, although the grammar size increased significantly. system baseline + right binarization (primary) + left binarization system baseline + transliteration (submission) + transliteration (fixed) B LEU devtest test 26.2 27.2 26.8 28.2 26.3 - Table 10: Hindi to English results with and without"
W14-3324,W13-2230,0,0.0742053,"ender Voice Definiteness Aspect Case 7 3 3 3 3 7 3 7 3 3 Data sparsity issues for this language pair are exacerbated by the rich inflectional morphology of Russian. Many Russian word forms express grammatical distinctions that are either absent from English translations (like grammatical gender) or are expressed by different means (like grammatical function being expressed through syntactic configuration rather than case). We adopt the widelyused approach of simplifying morphologicallycomplex source forms to remove distinctions that we believe to be redundant. Our method is similar to that of Weller et al. (2013) except that ours is much more conservative (in their experiments, Weller et al. (2013) found morphological reduction to harm translation indicating that useful information was likely to have been discarded). Table 11: Feature values that are retained (3) or deleted (7) during morphological reduction of Russian. We used TreeTagger (Schmid, 1994) to obtain a lemma-tag pair for each Russian word. The tag specifies the word class and various morphosyntactic feature values. For example, the adjective республиканская (‘republican’) gets the lemmatag pair республиканский + Afpfsnf, where the code A"
W14-3324,W11-2126,1,0.895685,"present split compounds as a syntactic tree with the last segment as head, preceded by a modifier. A modifier consists of an optional modifier, a segment and a (possibly empty) joining element. An example is shown in Figure 1. This hierarchical representation ensures that compounds can be easily merged in post-processing (by removing the spaces and special characters around joining elements), and that no segments are placed outside of a compound in the translation. We use unification-based constraints to model morphological agreement within German noun phrases, and between subjects and verbs (Williams and Koehn, 2011). Additionally, we add constraints that operate on the internal tree structure of the translation hypotheses, to enforce several syntactic constraints that were frequently violated in the baseline system: • relative clauses must contain a relative (or interrogative) pronoun in their first constituent. Table 4 shows B LEU scores with systems trained with different parsers, and for our extensions of the baseline system. 4 Czech to English For Czech to English we used the core setup described in Section 2 without modification. Table 5 shows the B LEU scores. system baseline B LEU devtest test 24."
W14-3324,W12-3150,1,0.794398,"probability of the derivation’s target yield, its word count, and various scores for the synchronous derivation. Each grammar rule has a number of precomputed scores. For a grammar rule r of the form sentence pairs with dubious length ratios and sentence pairs where parsing fails for the target-side sentence. 2.2 Translation Model Our translation grammar is a synchronous contextfree grammar (SCFG) with phrase-structure labels on the target side and the generic non-terminal label X on the source side. The grammar was extracted from the wordaligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004; Galley et al., 2006). For word alignment we used MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003). Minimal GHKM rules were composed into larger rules subject to parameterized restrictions on size defined in terms of the resulting target tree fragment. A good choice of parameter settings depends on the annotation style of the target-side parse trees. We used the settings shown in Table 1, which were chosen empirically during the development of last years’ systems: Parameter Rule depth Node count Rule size C"
W14-3324,N07-1051,0,\N,Missing
W14-3324,Q15-1013,1,\N,Missing
W14-3324,W08-1006,0,\N,Missing
W14-3324,D09-1108,0,\N,Missing
W14-3324,E99-1010,0,\N,Missing
W14-3324,N13-3005,0,\N,Missing
W14-3324,N09-2019,0,\N,Missing
W14-3324,P13-2121,1,\N,Missing
W14-3324,P03-2041,0,\N,Missing
W14-3324,W05-0904,0,\N,Missing
W14-3324,herrmann-etal-2014-manual,0,\N,Missing
W14-3324,D14-1082,0,\N,Missing
W14-3324,W15-1004,0,\N,Missing
W14-3324,D15-1248,1,\N,Missing
W14-3324,P14-2024,0,\N,Missing
W14-3324,P14-1129,0,\N,Missing
W14-3324,J07-2003,0,\N,Missing
W14-3324,vilar-etal-2006-error,0,\N,Missing
W14-3324,W14-4018,1,\N,Missing
W14-3324,2014.eamt-1.38,0,\N,Missing
W14-3324,D13-1140,0,\N,Missing
W14-4011,J07-2003,0,0.330109,"Missing"
W14-4011,P11-2072,0,0.146697,"wo nonterminals per rule, and no adjacent non-terminals on the source side. 98 algorithm Scope-3 CYK+ + recursive + compression with at most 3 decision points, or scope 3, as an alternative to binarization that allows parsing in cubic time. In a runtime evaluation, SMT with their pruned, unbinarized grammar offers a better speed-quality trade-off than synchronous binarization because, even though both have the same complexity characteristics, synchronous binarization increases both the overall number of rules, and the number of non-terminals, which increases the grammar constant. In contrast, Chung et al. (2011) compare binarization and Earley-style parsing with scope-pruned grammars, and find Earley-style parsing to be slower. They attribute the comparative slowness of Earley-style parsing to the cost of building and storing the dot chart during decoding, which is exactly the problem that our paper addresses. Williams and Koehn (2012) describe a parsing algorithm motivated by Hopkins and Langmead (2010) in which they store the grammar in a compact trie with source terminal symbols or a generic gap symbol as edge labels. Each path through this trie corresponds to a rule pattern, and is associated wit"
W14-4011,P10-4002,0,0.0197272,"inuation of the rule in the grammar, either a source-side terminal symbol or pair of non-terminal-symbols. If a node represents a completed rule, it is also associated with a collection of left-hand-side symbols and the associated target-side rules and probabilities. A trie data structure allows for an efficient grammar lookup, since all rules with the same preIntroduction SCFG decoding can be performed with monolingual parsing algorithms, and various SMT systems implement the CYK+ algorithm or a close Earley-style variant (Zhang et al., 2006; Koehn et al., 2007; Venugopal and Zollmann, 2009; Dyer et al., 2010; Vilar et al., 2012). The CYK+ algorithm (Chappelier and Rajman, 1998) generalizes the CYK algorithm to n-ary rules by performing a dynamic binarization of the grammar during parsing through a so-called dot chart. The construction of the dot chart is a major cause of space inefficiency in SCFG decoding with CYK+, and memory consumption makes the algorithm impractical for long sentences without artificial limits on the span of chart cells. We demonstrate that, by changing the traversal through the main parse chart, we can eliminate the dot chart from the CYK+ algorithm at no computational cost"
W14-4011,N04-1035,0,0.124543,".64 0.15 0.15 Table 2: Peak memory consumption (in GB) of string-to-tree SMT decoder for sentences of different length n with different parsing algorithms. data from the ACL 2014 Ninth Workshop on Statistical Machine Translation (WMT) shared translation task, consisting of 4.5 million sentence pairs of parallel data and a total of 120 million sentences of monolingual data. We build a stringto-tree translation system English→German, using target-side syntactic parses obtained with the dependency parser ParZu (Sennrich et al., 2013). A synchronous grammar is extracted with GHKM rule extraction (Galley et al., 2004; Galley et al., 2006), and the grammar is pruned to scope 3. The synchronous grammar contains 38 million rule pairs with 23 million distinct source-side rules. We report decoding time for a random sample of 1000 sentences from the newstest2013/4 sets (average sentence length: 21.9 tokens), and peak memory consumption for sentences of 20, 40, and 80 tokens. We do not report the time and space required for loading the SMT models, which is stable for all experiments.9 The parsing algorithm only accounts for part of the cost during decoding, and the relative gains from optimizing the parsing algo"
W14-4011,P06-1121,0,0.473231,"lexity for SCFG decoding. In an evaluation on a string-totree SMT scenario, we empirically demonstrate substantial improvements in memory consumption and translation speed. 1 2 SCFG Decoding To motivate our algorithm, we want to highlight some important differences between (monolingual) CFG parsing and SCFG decoding. Grammars in SMT are typically several orders of magnitude larger than for monolingual parsing, partially because of the large amounts of training data employed to learn SCFGs, partially because SMT systems benefit from using contextually rich rules rather than only minimal rules (Galley et al., 2006). Also, the same right-hand-side rule on the source side can be associated with many translations, and different (source and/or target) lefthand-side symbols. Consequently, a compact representation of the grammar is of paramount importance. We follow the implementation in the Moses SMT toolkit (Koehn et al., 2007) which encodes an SCFG as a trie in which each node represents a (partial or completed) rule, and a node has outgoing edges for each possible continuation of the rule in the grammar, either a source-side terminal symbol or pair of non-terminal-symbols. If a node represents a completed"
W14-4011,P12-2058,0,0.0336614,"Missing"
W14-4011,N13-1116,0,0.0130813,"istinct source-side rules. We report decoding time for a random sample of 1000 sentences from the newstest2013/4 sets (average sentence length: 21.9 tokens), and peak memory consumption for sentences of 20, 40, and 80 tokens. We do not report the time and space required for loading the SMT models, which is stable for all experiments.9 The parsing algorithm only accounts for part of the cost during decoding, and the relative gains from optimizing the parsing algorithm are highest if the rest of the decoder is fast. For best speed, we use cube pruning with language model boundary word grouping (Heafield et al., 2013) in all experiments. We set no limit to the maximal span of SCFG rules, but only keep the best 100 productions per span for cube pruning. The cube pruning limit itself is set to 1000. Empirical Results We empirically compare our algorithm to the CYK+ algorithm, and the Scope-3 algorithm as described by Williams and Koehn (2012), in a string-to-tree SMT task. All parsing algorithms are equivalent in terms of translation output, and our evaluation focuses on memory consumption and speed. 5.1 n = 20 0.02 0.32 0.02 0.02 5.2 Memory consumption Peak memory consumption for different sentence lengths"
W14-4011,W11-2123,0,0.0232639,"lgorithms are equivalent in terms of translation output, and our evaluation focuses on memory consumption and speed. 5.1 n = 20 0.02 0.32 0.02 0.02 5.2 Memory consumption Peak memory consumption for different sentence lengths is shown in Table 2. For sentences of length 80, we observe more than 50 GB in peak memory consumption for CYK+, which makes it impractical for long sentences, especially for multi-threaded decoding. Our recursive variants keep memory consumption small, as does the Data For SMT decoding, we use the Moses toolkit (Koehn et al., 2007) with KenLM for language model queries (Heafield, 2011). We use training 9 The language model consumes 13 GB of memory, and the SCFG 37 GB. We leave the task of compacting the grammar to future research. 99 decoding time (seconds) for sentences of length 80 – are attributable to a reduction in the number of computational steps. The large speed difference between CYK+ and the recursive variant is somewhat more surprising, given the similarity of the two algorithms. Profiling results show that the recursive variant is not only faster because it saves the computational overhead of creating and destroying the dot chart, but that it also has a better l"
W14-4011,D10-1063,0,0.54419,"resented by a single node. Rules are matched to the input in a bottom-upfashion as described in the next section. A single rule or rule prefix can match the input many times, either by matching different spans of the input, or by matching the same span, but with different subspans for its non-terminal symbols. Each production is uniquely identified by a span, a grammar trie node, and back-pointers to its subderivations. The same is true for a partial production (dotted item). A key difference between monolingual parsing and SCFG decoding, whose implications on time complexity are discussed by Hopkins and Langmead (2010), is that SCFG decoders need to consider language model costs when searching for the best derivation of an input sentence. This critically affects the parser’s ability to discard dotted items early. For CFG parsing, we only need to keep one partial production per rule prefix and span, or k for k-best parsing, selecting the one(s) whose subderivations have the lower cost in case of ambiguity. For SCFG decoding, the subderivation with the higher local cost may be the globally better choice after taking language model costs into account. Consequently, SCFG decoders need to consider multiple possi"
W14-4011,P07-2045,0,0.00713759,"g. Grammars in SMT are typically several orders of magnitude larger than for monolingual parsing, partially because of the large amounts of training data employed to learn SCFGs, partially because SMT systems benefit from using contextually rich rules rather than only minimal rules (Galley et al., 2006). Also, the same right-hand-side rule on the source side can be associated with many translations, and different (source and/or target) lefthand-side symbols. Consequently, a compact representation of the grammar is of paramount importance. We follow the implementation in the Moses SMT toolkit (Koehn et al., 2007) which encodes an SCFG as a trie in which each node represents a (partial or completed) rule, and a node has outgoing edges for each possible continuation of the rule in the grammar, either a source-side terminal symbol or pair of non-terminal-symbols. If a node represents a completed rule, it is also associated with a collection of left-hand-side symbols and the associated target-side rules and probabilities. A trie data structure allows for an efficient grammar lookup, since all rules with the same preIntroduction SCFG decoding can be performed with monolingual parsing algorithms, and variou"
W14-4011,R13-1079,1,0.461402,"comparison of the two in the next section. 5 n = 40 0.04 2.63 0.04 0.04 n = 80 0.34 51.64 0.15 0.15 Table 2: Peak memory consumption (in GB) of string-to-tree SMT decoder for sentences of different length n with different parsing algorithms. data from the ACL 2014 Ninth Workshop on Statistical Machine Translation (WMT) shared translation task, consisting of 4.5 million sentence pairs of parallel data and a total of 120 million sentences of monolingual data. We build a stringto-tree translation system English→German, using target-side syntactic parses obtained with the dependency parser ParZu (Sennrich et al., 2013). A synchronous grammar is extracted with GHKM rule extraction (Galley et al., 2004; Galley et al., 2006), and the grammar is pruned to scope 3. The synchronous grammar contains 38 million rule pairs with 23 million distinct source-side rules. We report decoding time for a random sample of 1000 sentences from the newstest2013/4 sets (average sentence length: 21.9 tokens), and peak memory consumption for sentences of 20, 40, and 80 tokens. We do not report the time and space required for loading the SMT models, which is stable for all experiments.9 The parsing algorithm only accounts for part o"
W14-4011,W12-3150,0,0.771479,"speed-quality trade-off than synchronous binarization because, even though both have the same complexity characteristics, synchronous binarization increases both the overall number of rules, and the number of non-terminals, which increases the grammar constant. In contrast, Chung et al. (2011) compare binarization and Earley-style parsing with scope-pruned grammars, and find Earley-style parsing to be slower. They attribute the comparative slowness of Earley-style parsing to the cost of building and storing the dot chart during decoding, which is exactly the problem that our paper addresses. Williams and Koehn (2012) describe a parsing algorithm motivated by Hopkins and Langmead (2010) in which they store the grammar in a compact trie with source terminal symbols or a generic gap symbol as edge labels. Each path through this trie corresponds to a rule pattern, and is associated with the set of grammar rules that share the same rule pattern. Their algorithm initially constructs a secondary trie that records all rule patterns that apply to the input sentence, and stores the position of matching terminal symbols. Then, chart cells are populated by constructing a lattice for each rule pattern identified in th"
W14-4011,J97-3002,0,0.324057,"t incurring any trade-off in time complexity. Dunlop et al. (2010) employ a similar matrix compression strategy for CYK parsing, but their method is different to ours in that they employ matrix compression on the grammar, which they assume to be in Chomsky Normal Form, whereas we represent n-ary grammars as tries, and use matrix compression for the chart. An obvious alternative to n-ary parsing is the use of binary grammars, and early SCFG models for SMT allowed only binary rules, as in the hierarchical models by Chiang (2007)8 , or binarizable ones as in inversion-transduction grammar (ITG) (Wu, 1997). Whether an n-ary rule can be binarized depends on the rule-internal reorderings between non-terminals; Zhang et al. (2006) describe a synchronous binarization algorithm. Hopkins and Langmead (2010) show that the complexity of parsing n-ary rules is determined by the number of choice points, i.e. non-terminals that are initial, consecutive, or final, since terminal symbols in the rule constrain which cells are possible application contexts of a non-terminal symbol. They propose pruning of the SCFG to rules Chart Compression Given a partial production for span (i, j), the number of chart cells"
W15-3024,D14-1082,0,0.137464,"Missing"
W15-3024,P14-1129,0,0.137886,"Missing"
W15-3024,N13-1073,0,0.0469731,"2.1 Introduction 2.2 System Overview Pre-processing The training data was pre-processed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five language pairs: English paired with Czech, Finnish, French, German, and Russian. We built syntax-based systems in both translation directions for all language pairs except English-French. For English → German, we continued to develop our string-to-tree system, which has proven highly competi"
W15-3024,P03-2041,0,0.147009,"Missing"
W15-3024,W15-1004,0,0.0358237,"Missing"
W15-3024,P06-1121,0,0.275709,"Missing"
W15-3024,W13-2221,1,0.922677,"Missing"
W15-3024,W08-0509,0,0.0425569,"e our English-German system; and source-side morphological segmentation of Finnish using Morfessor. 2.1 Introduction 2.2 System Overview Pre-processing The training data was pre-processed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five language pairs: English paired with Czech, Finnish, French, German, and Russian. We built syntax-based systems in both translation directions for all language pairs except English-French. For En"
W15-3024,P14-2024,0,0.0198394,"tion.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five language pairs: English paired with Czech, Finnish, French, German, and Russian. We built syntax-based systems in both translation directions for all language pairs except English-French. For English → German, we continued to develop our string-to-tree system, which has proven highly competitive in previous years. Additions this year included the use of a dependency language model, an alternative tuning metric, and soft source-syntactic constraints. For translation from En"
W15-3024,P05-1013,0,0.0872144,"Missing"
W15-3024,P13-2121,1,0.887764,"Missing"
W15-3024,J03-1002,0,0.0231219,"tation of Finnish using Morfessor. 2.1 Introduction 2.2 System Overview Pre-processing The training data was pre-processed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five language pairs: English paired with Czech, Finnish, French, German, and Russian. We built syntax-based systems in both translation directions for all language pairs except English-French. For English → German, we continued to develop our string-to-tree syst"
W15-3024,herrmann-etal-2014-manual,0,0.0544561,"Missing"
W15-3024,E99-1010,0,0.163731,"Missing"
W15-3024,D10-1063,0,0.128747,"Missing"
W15-3024,P03-1021,0,0.122835,"Missing"
W15-3024,W14-4018,1,0.8972,"Missing"
W15-3024,N03-1017,1,0.0581822,"Missing"
W15-3024,P06-1055,0,0.105477,"ared translation task. We developed systems for all language pairs except French-English. This year we focused on: translation out of English using tree-to-string models; continuing to improve our English-German system; and source-side morphological segmentation of Finnish using Morfessor. 2.1 Introduction 2.2 System Overview Pre-processing The training data was pre-processed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five la"
W15-3024,W05-0904,0,0.182994,"Missing"
W15-3024,D15-1248,1,0.828811,"Missing"
W15-3024,D14-2005,1,0.826874,"Missing"
W15-3024,R13-1079,1,0.817325,"d on: translation out of English using tree-to-string models; continuing to improve our English-German system; and source-side morphological segmentation of Finnish using Morfessor. 2.1 Introduction 2.2 System Overview Pre-processing The training data was pre-processed using scripts from the Moses toolkit. We first normalized the data using the normalize-punctuation.perl script then performed tokenization, parsing, and truecasing. To parse the English data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). To parse the German data, we used the ParZu dependency parser (Sennrich et al., 2013). Word Alignment For word alignment we used either MGIZA++ (Gao and Vogel, 2008), a multi-threaded implementation of GIZA++ (Och and Ney, 2003), or fast_align (Dyer et al., 2013). In preliminary experiments, we found that the tree-to-string systems were particularly sensitive to the choice of word aligner, echoing a previous observation by Neubig and Duh (2014). See the individual treeto-string system descriptions in Section 3. This year’s WMT shared translation task featured five language pairs: English paired with Czech, Finnish, French, German, and Russian. We built syntax-based systems in"
W15-3024,W14-4011,1,0.893791,"Missing"
W15-3024,Q15-1013,1,0.867045,"Missing"
W15-3024,N13-3005,0,0.0310419,"Missing"
W15-3024,D13-1140,0,0.162963,"Missing"
W15-3024,vilar-etal-2006-error,0,0.110074,"Missing"
W15-3024,W12-3150,1,0.927673,"Missing"
W15-3024,W14-3324,1,0.644531,"oft source-syntactic constraints. For translation from English into Czech, Finnish, and Russian, we built STSG-based treeto-string systems. Support for this type of model is a recent addition to the Moses toolkit. In previous years, our systems have all used string-to-tree models and have only translated into English and German. For Finnish → English, we experimented with unsupervised morphological segmentation using Morfessor 2.0 (Virpioja et al., 2013). For the remaining systems (Czech → English, German → English, and Russian → English), our systems were essentially the same as last year’s (Williams et al., 2014) except for the addition of this year’s training data. 2.3 Language Model We used all available monolingual data to train one interpolated 5-gram language model for each system. Using either lmplz (Heafield et al., 2013) or the SRILM toolkit (Stolcke, 2002), we first trained an individual language model for each of the supplied monolingual training corpora. These models all used modified Kneser-Ney smoothing (Chen and Goodman, 1998). We then interpolated the individual models using SRILM, providing the target-side of the system’s tuning set (Section 2.7) for perplexity-based weight optimizatio"
W16-2209,N06-2001,0,0.200365,"tem (Example 6) retains the original word order, which is highly unusual in English, especially for prose in the news domain. A syntactic annotation of the source sentence could support the attentional encoder-decoder in learning which words in the German source to attend (and translate) first. We will investigate the usefulness of linguistic features for the language pair German↔English, considering the following linguistic features: 2.1 Adding Input Features Our main innovation over the standard encoderdecoder architecture is that we represent the encoder input as a combination of features (Alexandrescu and Kirchhoff, 2006). We here show the equation for the forward states of the encoder (for the simple RNN case; consider (Bahdanau et al., 2015) for GRU): • lemmas • subword tags (see Section 3.2) • morphological features • POS tags • dependency labels → − − − → → −→ h j = tanh(W Exj + U h j−1 ) The inclusion of lemmas is motivated by the hope for a better generalization over inflectional variants of the same word form. The other linguistic features are motivated by disambiguation, as discussed in our introductory examples. 2 (1) where E ∈ Rm×Kx is a word embedding ma− → → − trix, W ∈ Rn×m , U ∈ Rn×n are weight m"
W16-2209,D14-1082,0,0.0870718,"onds to the full word. Our generalized model of the previous section supports an arbitrary number of input features. In this paper, we will focus on a number of wellknown linguistic features. Our main empirical question is if providing linguistic features to the encoder improves the translation quality of neural machine translation systems, or if the information emerges from training encoder-decoder models on raw text, making its inclusion via explicit features redundant. All linguistic features are predicted automatically; we use Stanford CoreNLP (Toutanova et al., 2003; Minnen et al., 2001; Chen and Manning, 2014) to annotate the English input for English→German, and ParZu (Sennrich et al., 2013) to annotate the German input for German→English. We here discuss the individual features in more detail. 3.1 3.3 Morphological Features For German→English, the parser annotates the German input with morphological features. Different word types have different sets of features – for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect – and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for unde"
W16-2209,D14-1179,0,0.105075,"Missing"
W16-2209,P16-1078,0,0.0585141,"o used in other tasks for which neural models have recently been employed, such as syntactic parsing (Chen and Manning, 2014). This paper addresses the question whether linguistic features on the source side are beneficial for neural machine translation. On the target side, linguistic features are harder to obtain for a generation task such as machine translation, since this would require incremental parsing of the hypotheses at test time, and this is possible future work. Among others, our model incorporates information from a dependency annotation, but is still a sequence-to-sequence model. Eriguchi et al. (2016) propose a tree-to-sequence model whose encoder computes vector representations for each phrase in the source tree. Their focus is on exploiting the (unlabelled) structure of a syntactic annotation, whereas we are focused on the disambiguation power of the functional dependency labels. Factored translation models are often used in phrase-based SMT (Koehn and Hoang, 2007) as a means to incorporate extra linguistic information. However, neural MT can provide a much more flexible mechanism for adding such information. Because phrase-based models cannot easily generalize to new feature combination"
W16-2209,N13-1090,0,0.0045048,"ance, nouns have case, number and gender, while verbs have person, number, tense and aspect – and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for underspecified features, as a string, and treat each such string as a separate feature value. Lemma Using lemmas as input features guarantees sharing of information between word forms that share the same base form. In principle, neural models can learn that inflectional variants are semantically related, and represent them as similar points in the continuous vector space (Mikolov et al., 2013). However, while this has been demonstrated for high-frequency words, we expect that a lemmatized representation increases data efficiency; low-frequency variants may even be unknown to word-level models. With character- or subwordlevel models, it is unclear to what extent they can learn the similarity between low-frequency word forms that share a lemma, especially if the word forms are superficially dissimilar. Consider the following two German word forms, which share the lemma liegen ‘lie’: 3.4 POS Tags and Dependency Labels In our introductory examples, we motivated POS tags and dependency"
W16-2209,W15-3014,0,0.0288839,"glish source sentence in Example 1 (our translation in Example 2), a neural MT system (our baseline system from Section 4) mistranslates close as a verb, and produces the German verb schließen (Example 3), even though close is an adjective in this sentence, which has the German translation nah. Intuitively, partof-speech annotation of the English input could disambiguate between verb, noun, and adjective meanings of close. As a second example, consider the following German→English example: Introduction Neural machine translation has recently achieved impressive results (Bahdanau et al., 2015; Jean et al., 2015), while learning from raw, sentencealigned parallel text and using little in the way of external linguistic information.3 However, we hypothesize that various levels of linguistic annotation can be valuable for neural machine translation. Lemmatisation can reduce data sparse1 https://github.com/rsennrich/nematus https://github.com/rsennrich/ wmt16-scripts 3 Linguistic tools are most commonly used in preprocessing, e.g. for Turkish segmentation (Gülçehre et al., 2015). 2 4. Gefährlich ist die Route aber dennoch . dangerous is the route but still . 83 Proceedings of the First Conference on Machi"
W16-2209,D07-1091,0,0.0218021,"this would require incremental parsing of the hypotheses at test time, and this is possible future work. Among others, our model incorporates information from a dependency annotation, but is still a sequence-to-sequence model. Eriguchi et al. (2016) propose a tree-to-sequence model whose encoder computes vector representations for each phrase in the source tree. Their focus is on exploiting the (unlabelled) structure of a syntactic annotation, whereas we are focused on the disambiguation power of the functional dependency labels. Factored translation models are often used in phrase-based SMT (Koehn and Hoang, 2007) as a means to incorporate extra linguistic information. However, neural MT can provide a much more flexible mechanism for adding such information. Because phrase-based models cannot easily generalize to new feature combinations, the individual models either treat each feature combination as an atomic unit, resulting in data sparsity, or assume independence between features, for instance by having separate language models for words and POS tags. In contrast, we exploit the strong generalization ability of neural networks, and expect that even new feature combinations, e.g. a word that appears"
W16-2209,P16-1162,1,0.91322,"− − − → n → −→ h j = tanh(W ( Ek xjk ) + U h j−1 ) (2) k=1 where k is the vector concatenation, Ek ∈ are the feature embedding matrices, with P|F | k=1 mk = m, and Kk is the vocabulary size of the kth feature. In other words, we look up separate embedding vectors for each feature, which are then concatenated. The length of the concatenated vector matches the total embedding size, and all other parts of the model remain unchanged. Rmk ×Kk 4 https://github.com/nyu-dl/ dl4mt-tutorial 84 3 Linguistic Input Features a fixed symbol vocabulary, using a segmentation based on byte-pair encoding (BPE) (Sennrich et al., 2016c). We note that in BPE segmentation, some symbols are potentially ambiguous, and can either be a separate word, or a subword segment of a larger word. Also, text is represented as a sequence of subword units with no explicit word boundaries, but word boundaries are potentially helpful to learn which symbols to attend to, and when to forget information in the recurrent layers. We propose an annotation of subword structure similar to popular IOB format for chunking and named entity recognition, marking if a symbol in the text forms the beginning (B), inside (I), or end (E) of a word. A separate"
W16-2209,W15-3031,0,0.0302229,"Missing"
W16-2209,N03-1033,0,0.0679687,"separate tag (O) is used if a symbol corresponds to the full word. Our generalized model of the previous section supports an arbitrary number of input features. In this paper, we will focus on a number of wellknown linguistic features. Our main empirical question is if providing linguistic features to the encoder improves the translation quality of neural machine translation systems, or if the information emerges from training encoder-decoder models on raw text, making its inclusion via explicit features redundant. All linguistic features are predicted automatically; we use Stanford CoreNLP (Toutanova et al., 2003; Minnen et al., 2001; Chen and Manning, 2014) to annotate the English input for English→German, and ParZu (Sennrich et al., 2013) to annotate the German input for German→English. We here discuss the individual features in more detail. 3.1 3.3 Morphological Features For German→English, the parser annotates the German input with morphological features. Different word types have different sets of features – for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect – and features may be underspecified. We treat the concatenation of all morphological featu"
W16-2209,W15-3049,0,0.0318322,"Missing"
W16-2209,W16-2327,1,0.886618,"ifferent features is not fully cumulative; we note that the information encoded in different features overlaps. For instance, both the dependency labels and the morphological features encode the distinction between German subjects and accusative objects, the former through different labels (subj and obja), the latter through grammatical case (nominative and accusative). We also evaluated adding linguistic features to a stronger baseline, which includes synthetic parallel training data. In addition, we compare our neural systems against phrase-based (PBSMT) and syntax-based (SBSMT) systems by (Williams et al., 2016), all of which make use of linguistic annotation on the source and/or target side. Results are shown in Table 4. For German→English, we observe similar improvements in the best development perplexity (45.2 → 44.1), test set B LEU (37.5→38.5) and CHR F3 (62.2 → 62.8). Our test set B LEU is on par to the best submitted system to this year’s WMT 16 shared translation task, which is similar to our baseline MT system, but which also uses a right-to-left decoder for reranking (Sennrich et al., 2016a). We expect that linguistic input features and bidirectional decoding are orthogonal, and that we cou"
W16-2209,W05-0908,0,0.0545676,"sagree, partly because they are highly sensitive to the length of the output. B LEU is precision-based, whereas CHR F3 considers both precision and recall, with a bias for recall. For B LEU, we also report whether differences between systems are statistically significant Table 1: Vocabulary size, and size of embedding layer of linguistic features, in system that includes all features, and contrastive experiments that add a single feature over the baseline. The embedding layer size of the word feature is set to bring the total size to 500. according to a bootstrap resampling significance test (Riezler and Maxwell, 2005). We train models for about a week, and report results for an ensemble of the 4 last saved models (with models saved every 12 hours). The ensemble serves to smooth the variance between single models. Decoding is performed with beam search with a beam size of 12. To ensure that performance improvements are not simply due to an increase in the number of model parameters, we keep the total size of the embedding layer fixed to 500. Table 1 lists the embedding size we use for linguistic features – the embedding layer size of the word-level feature varies, and is set to bring the total embedding lay"
W16-2209,N16-1004,0,0.062145,"Missing"
W16-2209,schmid-etal-2004-smor,0,0.0316331,"s and Dependency Labels In our introductory examples, we motivated POS tags and dependency labels as possible disambiguators. Each word is associated with one POS tag, and one dependency label. The latter is the label of the edge connecting a word to its syntactic head, or ’ROOT’ if the word has no syntactic head. 3.5 • liegt ‘lies’ (3.p.sg. present) • läge ‘lay’ (3.p.sg. subjunctive II) On Using Word-level Features in a Subword Model The lemmatisers we use are based on finite-state methods, which ensures a large coverage, even for infrequent word forms. We use the Zmorge analyzer for German (Schmid et al., 2004; Sennrich and Kunz, 2014), and the lemmatiser in the Stanford CoreNLP toolkit for English (Minnen et al., 2001). We segment rare words into subword units using BPE. The subword tags encode the segmentation of words into subword units, and need no further modification. All other features are originally word-level features. To annotate the segmented source text with features, we copy the word’s feature value to all its subword units. An example is shown in Figure 1. 3.2 4 Subword Tags Evaluation We evaluate our systems on the WMT16 shared translation task English↔German. The parallel In our exp"
W16-2209,sennrich-kunz-2014-zmorge,1,0.809645,"ls In our introductory examples, we motivated POS tags and dependency labels as possible disambiguators. Each word is associated with one POS tag, and one dependency label. The latter is the label of the edge connecting a word to its syntactic head, or ’ROOT’ if the word has no syntactic head. 3.5 • liegt ‘lies’ (3.p.sg. present) • läge ‘lay’ (3.p.sg. subjunctive II) On Using Word-level Features in a Subword Model The lemmatisers we use are based on finite-state methods, which ensures a large coverage, even for infrequent word forms. We use the Zmorge analyzer for German (Schmid et al., 2004; Sennrich and Kunz, 2014), and the lemmatiser in the Stanford CoreNLP toolkit for English (Minnen et al., 2001). We segment rare words into subword units using BPE. The subword tags encode the segmentation of words into subword units, and need no further modification. All other features are originally word-level features. To annotate the segmented source text with features, we copy the word’s feature value to all its subword units. An example is shown in Figure 1. 3.2 4 Subword Tags Evaluation We evaluate our systems on the WMT16 shared translation task English↔German. The parallel In our experiments, we operate on th"
W16-2209,R13-1079,1,0.410147,"trary number of input features. In this paper, we will focus on a number of wellknown linguistic features. Our main empirical question is if providing linguistic features to the encoder improves the translation quality of neural machine translation systems, or if the information emerges from training encoder-decoder models on raw text, making its inclusion via explicit features redundant. All linguistic features are predicted automatically; we use Stanford CoreNLP (Toutanova et al., 2003; Minnen et al., 2001; Chen and Manning, 2014) to annotate the English input for English→German, and ParZu (Sennrich et al., 2013) to annotate the German input for German→English. We here discuss the individual features in more detail. 3.1 3.3 Morphological Features For German→English, the parser annotates the German input with morphological features. Different word types have different sets of features – for instance, nouns have case, number and gender, while verbs have person, number, tense and aspect – and features may be underspecified. We treat the concatenation of all morphological features of a word, using a special symbol for underspecified features, as a string, and treat each such string as a separate feature v"
W16-2209,P16-1009,1,0.927518,"− − − → n → −→ h j = tanh(W ( Ek xjk ) + U h j−1 ) (2) k=1 where k is the vector concatenation, Ek ∈ are the feature embedding matrices, with P|F | k=1 mk = m, and Kk is the vocabulary size of the kth feature. In other words, we look up separate embedding vectors for each feature, which are then concatenated. The length of the concatenated vector matches the total embedding size, and all other parts of the model remain unchanged. Rmk ×Kk 4 https://github.com/nyu-dl/ dl4mt-tutorial 84 3 Linguistic Input Features a fixed symbol vocabulary, using a segmentation based on byte-pair encoding (BPE) (Sennrich et al., 2016c). We note that in BPE segmentation, some symbols are potentially ambiguous, and can either be a separate word, or a subword segment of a larger word. Also, text is represented as a sequence of subword units with no explicit word boundaries, but word boundaries are potentially helpful to learn which symbols to attend to, and when to forget information in the recurrent layers. We propose an annotation of subword structure similar to popular IOB format for chunking and named entity recognition, marking if a symbol in the text forms the beginning (B), inside (I), or end (E) of a word. A separate"
W16-2209,W16-2323,1,\N,Missing
W16-2316,P13-2071,0,0.0139652,"Adding multiple models as separate features becomes thus similar to ensemble translation with pure neural models. In this section we give algorithmic details about integrating GPU-based soft-attention neural translation models into Moses as part of the feature function framework. Our work differs from Alkhouli et al. (2015) in the following aspects: Phrase-Based baseline systems We base our set-up on a Moses system (Koehn et al., 2007) with a number of additional feature functions. Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013). We perform no language-specific adaptations or modifications. The two systems differ only 1. While Alkhouli et al. (2015) integrate RNNbased translation models in phrase-based decoding, this work is to our knowledge the first to integrate soft-attention models. 3 In experiments not described in this paper, we tried BPE encoding for the English-German language pair and found subword units to cope well with German compound nouns when used for phrase-based SMT. 4 This artificial data has not been used for the creation of the phrase-based system, but it might be worthwhile to explore this possib"
W16-2316,E14-4029,0,0.0220874,"th phrase-based decoding. Experiments have been conducted for the English-Russian language pair in both translation directions. For these experiments we re-implemented the inference step of the models described in Bahdanau et al. (2015) (more exactly the DL4MT1 variant also present in Nematus2 ) in efficient 1 https://github.com/nyu-dl/ dl4mt-tutorial 2 https://github.com/rsennrich/nematus 319 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 319–325, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mechanisms (Durrani et al., 2014). Resorting to subword units eliminates the need for these.3 3 with respect to translation direction and the available (monolingual) training data. For domainadaptation, we rely solely on parameter tuning with Batch-Mira (Cherry and Foster, 2012) and on-line log-linear interpolation. Binary domainindicators for each separate parallel corpus are introduced to the phrase-tables (four indicators) and a separate language model per parallel and monolingual resource is trained (en:16 and ru:12). All language models are 5-gram models with Modified Kneser-Ney smoothing and without pruning thresholds ("
W16-2316,P13-2121,0,0.0373563,"Missing"
W16-2316,P07-2045,0,0.00567367,"rectly with our code. One or multiple models can be added to the Moses log-linear model as different instances of the same feature, which during tuning can be separately weighted. Adding multiple models as separate features becomes thus similar to ensemble translation with pure neural models. In this section we give algorithmic details about integrating GPU-based soft-attention neural translation models into Moses as part of the feature function framework. Our work differs from Alkhouli et al. (2015) in the following aspects: Phrase-Based baseline systems We base our set-up on a Moses system (Koehn et al., 2007) with a number of additional feature functions. Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013). We perform no language-specific adaptations or modifications. The two systems differ only 1. While Alkhouli et al. (2015) integrate RNNbased translation models in phrase-based decoding, this work is to our knowledge the first to integrate soft-attention models. 3 In experiments not described in this paper, we tried BPE encoding for the English-German language pair and found subword units to cope well with German c"
W16-2316,J10-4005,0,0.0110406,"Moses to a minimum, we propose two-pass stack decoding where the first pass is a hypothesis and expansions collection step and the second pass is the original expansion and scoring step. Between the two steps we pre-calculate perhypothesis scores with the procedure described above. The data structure introduced in Figure 1 is then reused for probability look-up during the scoring phrase of stack decoding as if individual hypotheses where scored on-the-fly. Figure 3 contains our complete proposal for two-pass stack decoding, a modification of the original stack decoding algorithm described in Koehn (2010). We dissect stack decoding into smaller reusable pieces that can be passed functors to perform different tasks for the same sets of hypotheses. The main reason for this is the small word “applicable” in line 12, which hides a complicated set of target phrase choices based on reordering limits and coverage vectors which should 5.3 Stack rescoring The previous approach cannot be used with lazy decoding algorithms — like cube pruning — 5 Large matrix sizes, however, do slow-down translation speed significantly. 322 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: procedure S TACK R ESCORING Place"
W16-2316,W15-3034,0,0.135613,"ed a C++/CUDA version of the inference step for the neural models trained with DL4MT or Nematus, which can be used directly with our code. One or multiple models can be added to the Moses log-linear model as different instances of the same feature, which during tuning can be separately weighted. Adding multiple models as separate features becomes thus similar to ensemble translation with pure neural models. In this section we give algorithmic details about integrating GPU-based soft-attention neural translation models into Moses as part of the feature function framework. Our work differs from Alkhouli et al. (2015) in the following aspects: Phrase-Based baseline systems We base our set-up on a Moses system (Koehn et al., 2007) with a number of additional feature functions. Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013). We perform no language-specific adaptations or modifications. The two systems differ only 1. While Alkhouli et al. (2015) integrate RNNbased translation models in phrase-based decoding, this work is to our knowledge the first to integrate soft-attention models. 3 In experiments not described in this pa"
W16-2316,N12-1047,0,0.347327,"exactly the DL4MT1 variant also present in Nematus2 ) in efficient 1 https://github.com/nyu-dl/ dl4mt-tutorial 2 https://github.com/rsennrich/nematus 319 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 319–325, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mechanisms (Durrani et al., 2014). Resorting to subword units eliminates the need for these.3 3 with respect to translation direction and the available (monolingual) training data. For domainadaptation, we rely solely on parameter tuning with Batch-Mira (Cherry and Foster, 2012) and on-line log-linear interpolation. Binary domainindicators for each separate parallel corpus are introduced to the phrase-tables (four indicators) and a separate language model per parallel and monolingual resource is trained (en:16 and ru:12). All language models are 5-gram models with Modified Kneser-Ney smoothing and without pruning thresholds (Heafield et al., 2013). We treat different years of the News Crawl data as different domains to take advantage of possible recency-based effects. During parameter tuning on the newstest2014 test set, we can unsurprisingly observe that weights for"
W16-2316,P16-1162,1,\N,Missing
W16-2316,P16-1009,1,\N,Missing
W16-2320,W16-2304,1,0.833347,"factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. T"
W16-2320,W05-0909,0,0.583183,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W16-2320,P13-2071,1,0.873371,"odel trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. The first part was used as development set while t"
W16-2320,D15-1129,1,0.847985,"training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based loc"
W16-2320,N13-1073,0,0.034805,"preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ ques"
W16-2320,J04-2004,0,0.0194677,"en systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probabilit"
W16-2320,2011.mtsummit-papers.30,0,0.020392,"-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 201"
W16-2320,N12-1047,0,0.591808,"rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard p"
W16-2320,E14-2008,1,0.72015,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,P05-1033,0,0.151933,"ative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combin"
W16-2320,J07-2003,0,0.558191,"this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the bou"
W16-2320,W14-3310,1,0.909876,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D14-1179,0,0.0138582,"Missing"
W16-2320,2014.iwslt-evaluation.7,1,0.925781,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D08-1089,0,0.0211464,", built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. T"
W16-2320,N15-1105,0,0.046766,"Missing"
W16-2320,W08-0509,0,0.06624,"probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. We validate the model every 10 000 minibatches via B LEU on a validation set, and perform early stopping on B LEU. Decoding is performed with beam search with a beam size of 12. A more detailed description of the system, and more experimental results, can be found in (Sennrich et al., 2016a). 3.10 3.11 USFD’s phrase-based system is built using the Moses toolkit, with MGIZA (Gao and Vogel, 2008) for word alignment and KenLM (Heafield et al., 2013) for language model training. We use all available parallel data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the featur"
W16-2320,W16-2315,1,0.820309,"vs et al., 2012) that features language-specific data filtering and cleaning modules. Tilde’s system was trained on all available parallel data. Two language models are trained using KenLM (Heafield, 2011): 1) a 5-gram model using the Europarl and SETimes2 corpora, and 2) a 3-gram model using the Common Crawl corpus. We also apply a custom tokenization tool that takes into account specifics of the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural Sy"
W16-2320,D07-1103,0,0.032902,"bbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT ("
W16-2320,W14-3360,0,0.0191919,"NMT system on newsdev2016/2, but lags behind on newstest2016. Removing the by itself weakest system shows a slight degradation on newsdev2016/2 and newstest2016, hinting that it still provides valuable information. Table 2 shows a comparison between all systems by scoring the translation output against each other in T ER and B LEU. We see that the neural networks outputs differ the most from all the other systems. Figure 1: System A: the large building; System B: the large home; System C: a big house; System D: a huge house; Reference: the big house. classes were generated using the method of Green et al. (2014). 4 System Combination System combination produces consensus translations from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alig"
W16-2320,P07-2045,1,0.010375,", 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based"
W16-2320,P13-2121,0,0.0645203,"Missing"
W16-2320,W11-2123,0,0.124165,"nslation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phras"
W16-2320,N04-1022,0,0.037676,"f the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpu"
W16-2320,2015.iwslt-papers.3,1,0.744353,"g. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn e"
W16-2320,J06-4004,0,0.106202,"Missing"
W16-2320,2009.iwslt-papers.4,0,0.0984189,"target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SETimes2) directly into the log-linear combination of the system and let MIRA (Cherry and Foster, 2012) optimize their weights along with all other features in tuning, rather than relying on a single linearly interpolated language model. We add another background language model estimated over a concatenation of all Ro"
W16-2320,P10-2041,0,0.0238386,"ontaining the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using"
W16-2320,P07-1019,0,0.236745,"grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language"
W16-2320,2012.amta-papers.19,1,0.778005,"common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set usin"
W16-2320,W11-2211,1,0.902733,"ty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system provided by the RWTH is an attention-based recurrent neural network similar to (Bahdanau et al., 2015). The implementation is based on Blocks (van Merri¨enboer et al., 2015) and Theano (Bergstra et al., 20"
W16-2320,W13-2264,1,0.852517,"stic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additio"
W16-2320,W13-2258,1,0.869431,"extraction, we impose less strict extraction constraints than the Moses defaults. We extract more hierarchical rules by allowing for a maximum of ten symbols on the source side, a maximum span of twenty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system prov"
W16-2320,E99-1010,0,0.040797,"ion 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and th"
W16-2320,P03-1021,0,0.501814,". To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SE"
W16-2320,D14-1003,1,0.932306,"with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule appli"
W16-2320,P06-1055,0,0.0121776,"anslation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but als"
W16-2320,2007.tmi-papers.21,0,0.0230136,"n 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add t"
W16-2320,P16-1161,1,0.729045,"omanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard phrasebased setup is the addition of a feature-rich discriminative translation model which is conditioned on both source- and target-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-"
W16-2320,tufis-etal-2008-racais,0,0.107217,"Missing"
W16-2320,P16-1009,1,0.78198,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,P12-3008,0,0.0597108,"Missing"
W16-2320,P16-1162,1,0.259658,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,W10-1738,1,0.885055,"rget-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical ph"
W16-2320,P15-4020,1,0.815128,"data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the feature set with the 17 baseline black-box features from sentencelevel Quality Estimation (QE) produced with Quest++4 (Specia et al., 2015). The 1000-best lists are then reranked and the top-best hypothesis extracted using the nbest rescorer available within the Moses toolkit. 3.12 UvA We use a phrase-based machine translation system (Moses) with a distortion limit of 6 and lexicalized reordering. Before translation, the English source side is preordered using the neural preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in"
W16-2320,W16-2327,1,0.84726,"Missing"
W16-2320,D13-1138,1,0.859072,"(Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integra"
W16-2320,2002.tmi-tutorials.2,0,0.0608664,"omanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all"
W16-2323,P16-1009,1,0.795503,"d better ensembles. Decoding is performed with beam search with a beam size of 12. For some language pairs, we used the AmuNMT C++ decoder4 as a more efficient alternative to the theano implementation of the dl4mt tutorial. Introduction We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs: English↔Czech, English↔German, English↔Romanian and English↔Russian. Our systems are based on an attentional encoder-decoder (Bahdanau et al., 2015), using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary (Sennrich et al., 2016b). We experimented with using automatic back-translations of the 2.1 Byte-pair encoding (BPE) To enable open-vocabulary translation, we segment words via byte-pair encoding (BPE)5 (Sen1 We have released the implementation that we used for the experiments as an open source toolkit: https://github. com/rsennrich/nematus 2 We have released scripts, sample configs, synthetic training data and trained models: https://github.com/ rsennrich/wmt16-scripts 3 https://github.com/nyu-dl/ dl4mt-tutorial 4 https://github.com/emjotde/amunmt 5 https://github.com/rsennrich/ subword-nmt 371 Proceedings of the"
W16-2323,P16-1162,1,0.641761,"d better ensembles. Decoding is performed with beam search with a beam size of 12. For some language pairs, we used the AmuNMT C++ decoder4 as a more efficient alternative to the theano implementation of the dl4mt tutorial. Introduction We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs: English↔Czech, English↔German, English↔Romanian and English↔Russian. Our systems are based on an attentional encoder-decoder (Bahdanau et al., 2015), using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary (Sennrich et al., 2016b). We experimented with using automatic back-translations of the 2.1 Byte-pair encoding (BPE) To enable open-vocabulary translation, we segment words via byte-pair encoding (BPE)5 (Sen1 We have released the implementation that we used for the experiments as an open source toolkit: https://github. com/rsennrich/nematus 2 We have released scripts, sample configs, synthetic training data and trained models: https://github.com/ rsennrich/wmt16-scripts 3 https://github.com/nyu-dl/ dl4mt-tutorial 4 https://github.com/emjotde/amunmt 5 https://github.com/rsennrich/ subword-nmt 371 Proceedings of the"
W16-2323,W16-2316,1,\N,Missing
W16-2327,J07-2003,0,0.787531,"dual monolingual corpora, we first used lmplz (Heafield et al., 2013) to train count-based 5-gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). We then used the SRILM toolkit (Stolcke, 2002) to linearly interpolate the models http://ufal.mff.cuni.cz/treex 399 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 399–410, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2.6 using weights tuned to minimize perplexity on the development set. In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based"
W16-2327,P11-2072,0,0.0157943,"ubject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 4.5 Word Alignment Baseline Features Our core set of string-to-tree feature functions is unchanged from previous years. It includes the ngram language model’s log probability for the target string, the target word count, the rule count, and several pre-computed rule-specific scores. The rule-specific scores were: the direct and indirect translation probabilities; the direct and indirect lexical weights (Koeh"
W16-2327,P05-1066,0,0.077238,"s representing the decoding search space) on a concatenation of newssyscomb2009 and newstest2008–2012. BLEU 26.8 26.2 25.6 26.4 26.6 26.5 3.5 Table 2: Effect of each of the language models used in the English→Romanian system. The experiments are not cumulative, so we first try pruning the “all” language model, then go back to the unpruned version and remove each LM in turn, observing the effect. The submitted system used all four LMs, and the scores shown are uncased B LEU scores on newstest2016. 3.4 German→English For phrase-based translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. The operation sequence model for the German→English phrase-based system was unpruned. We integrated three language models: an unpruned LM over all English data except the CommonCrawl monolingual corpus; a pruned LM over CommonCrawl; and a pruned LM over the monolingual News Crawl 2015 corpus. In addition to lexical smoothing with the standard lexicon models, we utilized a source-to-target IBM Model 1 (Brown et al., 1993) for sentence-level lexical scoring in a similar manner as described by Huck et al."
W16-2327,P14-1129,0,0.035129,"on limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English Co"
W16-2327,P13-2071,0,0.020975,"a weighted linear combination of features. The core features of our model are a 5-gram LM score (i.e. log probability), phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with Good-Turing smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and right-to-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 2.5 Decoding Tuning Since our feature set (generally around 500 to 1000 features) was too large for MERT, we used k-best batch MIRA for tuning (Cherry and Foster, 2012). To speed up tuning we applied threshold pruning to th"
W16-2327,J93-2003,0,0.052624,"Missing"
W16-2327,D14-1082,0,0.163978,"ram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), which was ranked (joint) first at WMT 15. Our improvements this year stem from particle verb restructuring (Sennrich and Haddow, 2015), and the use of the new monolingual News Crawl 2015 corpus for In 4 cases, the system with constraints delivered much better translation, and three of those were overall improvement of the sentence structure. In 41 cases, the area was better for various reasons. Most frequently (16 cases), this was ind"
W16-2327,N13-1073,0,0.099487,"Missing"
W16-2327,N12-1047,0,0.356006,"rection. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features (limited to the top K words in each language, typically with K = 50). 2.5 Decoding Tuning Since our feature set (generally around 500 to 1000 features) was too large for MERT, we used k-best batch MIRA for tuning (Cherry and Foster, 2012). To speed up tuning we applied threshold pruning to the phrase table, based on the direct translation model probability. 400 are quite understandable, e.g. source yös Intian on sanottu olevan kiinnostunut puolustusyhteistyösopimuksesta Japanin kanssa. base India is also said to be interested in puolustusyhteistyösopimuksesta with Japan. bpe India is also said to be interested in defence cooperation agreement with Japan. reference India is also reportedly hoping for a deal on defence collaboration between the two nations. However applying BPE to Finnish can also result in some rather odd trans"
W16-2327,W06-1607,0,0.0250361,"ts here, we used 100,000 BPE merges to create the model. Applying BPE to Finnish→English was clearly effective at addressing the unknown word problem, and in many cases the resulting translations Baseline Features We follow the standard approach to SMT of scoring translation hypotheses using a weighted linear combination of features. The core features of our model are a 5-gram LM score (i.e. log probability), phrase translation and lexical translation scores, word and phrase penalties, and a linear distortion score. The phrase translation probabilities are smoothed with Good-Turing smoothing (Foster et al., 2006). We used the hierarchical lexicalized reordering model (Galley and Manning, 2008) with 4 possible orientations (monotone, swap, discontinuous left and discontinuous right) in both left-to-right and right-to-left direction. We also used the operation sequence model (OSM) (Durrani et al., 2013) with 4 count based supportive features. We further employed domain indicator features (marking which training corpus each phrase pair was found in), binary phrase count indicator features, sparse phrase length features, and sparse source word deletion, target word insertion, and word translation features"
W16-2327,D10-1063,0,0.0144921,"fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 4.5 Word Alignment Baseline Features Our core set of string-to-tree feature functions is unchanged from previous years. It includes the ngram language model’s log probability for the target string, the target word count, the rule count, and several pre-computed rule-specific scores. The rule-specific scores were: the direct and indirect translation probabilities; the direct and indirect lexical weights (Koehn et al., 2003); the monolingual PCFG probability of the tree fra"
W16-2327,P07-1019,0,0.0831252,"For individual monolingual corpora, we first used lmplz (Heafield et al., 2013) to train count-based 5-gram language models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). We then used the SRILM toolkit (Stolcke, 2002) to linearly interpolate the models http://ufal.mff.cuni.cz/treex 399 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 399–410, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2.6 using weights tuned to minimize perplexity on the development set. In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based"
W16-2327,P06-1121,0,0.0809512,"Missing"
W16-2327,2015.iwslt-evaluation.4,1,0.850731,"ms are given in Table 3. English→German For the English→German phrase-based system, we exploited several translation factors in addition to word surface forms, in particular: Och clusters (with 50 classes) and part-of-speech tags (Ratnaparkhi, 1996) on the English side, as well as Och clusters (50 classes), morphological tags, and part-of-speech tags on the German side (Schmid, 2000). Recent experiments for our IWSLT 2015 phrase-based system have reconfirmed that English→German translation quality can benefit from these factors when supplementary models over factored representations are used (Huck and Birch, 2015). For WMT16, we utilized the factors in the translation model, in operation sequence models, and in language models (for linearly interpolated 7-gram LMs over Och clusters and morphological tags). Sparse source word deletion, target word insertion, and word translation features were integrated over the top 200 word surface forms and over selected factors (source and target Och clusters, source part-of-speech tags and target morphological tags). An unpruned 5-gram LM over words that was trained on all German data except the CommonCrawl monolingual corpus was supplemented by a separate pruned LM"
W16-2327,N04-1035,0,0.0743322,"pus was made up of three parts: all the English monolingual medical data from WMT14 medical, WMT16 biomedical and EMEA (11M sentences); all the English LDC GigaWord data (180M sentences); and all the English general domain data from WMT16 (240M sentences). We used the monolingual data to build three different language models which were then linearly interpolated. System tuning was with the SCIELO development data provided for the biomedical task. 4 4.4 SCFG rules were extracted from the word-aligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004, 2006). Minimal GHKM rules were composed into larger rules subject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexic"
W16-2327,W14-4018,1,0.849099,"s, improving overall sentence structure on average. Crazy Reordering 3 Table 7: Manual evaluation of translations as proposed by the English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), w"
W16-2327,D08-1089,0,0.179545,"Missing"
W16-2327,W08-0509,0,0.143531,"Missing"
W16-2327,2011.iwslt-papers.1,1,0.855699,"al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. The operation sequence model for the German→English phrase-based system was unpruned. We integrated three language models: an unpruned LM over all English data except the CommonCrawl monolingual corpus; a pruned LM over CommonCrawl; and a pruned LM over the monolingual News Crawl 2015 corpus. In addition to lexical smoothing with the standard lexicon models, we utilized a source-to-target IBM Model 1 (Brown et al., 1993) for sentence-level lexical scoring in a similar manner as described by Huck et al. (2011) for hierarchical systems. We tuned on the concatenation of newssyscomb2009 and newstest2008–2012. Unlike last year’s system (Haddow et al., 2015)—and different from the inverse translation direction (English→German)—we refrained from using any factors and instead set up a system that operates over surface form word representations only. In relation to last year’s system, we were able to maintain high translation quality as measured in B LEU despite the abandonment of factors. However, we suspect that human judgment scores may suffer a bit from the abandonment of a factored model. We decided t"
W16-2327,N10-1129,0,0.0414698,"Missing"
W16-2327,W15-3013,1,0.881832,"Missing"
W16-2327,E03-1076,0,0.0923236,"concatenation of newssyscomb2009 and newstest2008–2012. BLEU 26.8 26.2 25.6 26.4 26.6 26.5 3.5 Table 2: Effect of each of the language models used in the English→Romanian system. The experiments are not cumulative, so we first try pruning the “all” language model, then go back to the unpruned version and remove each LM in turn, observing the effect. The submitted system used all four LMs, and the scores shown are uncased B LEU scores on newstest2016. 3.4 German→English For phrase-based translation from German, we applied syntactic pre-reordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) in a preprocessing step on the source side. The operation sequence model for the German→English phrase-based system was unpruned. We integrated three language models: an unpruned LM over all English data except the CommonCrawl monolingual corpus; a pruned LM over CommonCrawl; and a pruned LM over the monolingual News Crawl 2015 corpus. In addition to lexical smoothing with the standard lexicon models, we utilized a source-to-target IBM Model 1 (Brown et al., 1993) for sentence-level lexical scoring in a similar manner as described by Huck et al. (2011) for hierarchical systems. We tuned on th"
W16-2327,N03-1017,0,0.0439096,"011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binarization. 4.5 Word Alignment Baseline Features Our core set of string-to-tree feature functions is unchanged from previous years. It includes the ngram language model’s log probability for the target string, the target word count, the rule count, and several pre-computed rule-specific scores. The rule-specific scores were: the direct and indirect translation probabilities; the direct and indirect lexical weights (Koehn et al., 2003); the monolingual PCFG probability of the tree fragment from which the rule was extracted; and a rule As in the phrase-based models, we used fast_align for word alignment and the grow-diag-final-and heuristic for symmetrization. 4.3 unbinarized 5 20 5 Table 4: Parameter settings for rule composition. The parameters were relaxed for systems that used binarization to allow for the increase in tree node density. Except for English-Czech, which we describe separately in Section 5.1, preprocessing was similar to the phrase-based systems (Section 2.3). To parse the target-side of the training data,"
W16-2327,N04-1022,0,0.115351,"anguage models with modified Kneser-Ney smoothing (Chen and Goodman, 1998). We then used the SRILM toolkit (Stolcke, 2002) to linearly interpolate the models http://ufal.mff.cuni.cz/treex 399 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 399–410, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2.6 using weights tuned to minimize perplexity on the development set. In decoding we applied cube pruning (Huang and Chiang, 2007) with a stack size of 5000 (reduced to 1000 for tuning), Minimum Bayes Risk decoding (Kumar and Byrne, 2004), a maximum phrase length of 5, a distortion limit of 6, 100best translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams on"
W16-2327,N12-1005,0,0.0147426,"t translation options and the no-reorderingover-punctuation heuristic (Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English CommonCrawl corpus."
W16-2327,W05-0904,0,0.0750355,"e English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), which was ranked (joint) first at WMT 15. Our improvements this year stem from particle verb restructuring (Sennrich and Haddow, 2015)"
W16-2327,H05-1066,0,0.0224782,"s. We used randomly-chosen subsets of the previous years’ test data to speed up decoding. 5 5.1 system baseline + constraints HimL1 23.3 23.6 B LEU HimL2 Khresmoi 18.6 20.4 18.8 20.7 Table 5: Translation results on the development system for English→Czech with unification-based constraints. Cased B LEU scores are shown. They are averaged over three tuning runs (note that baseline weights are reused in the experiments with constraints). Syntax-based Experiments English→Czech For English→Czech, we used Treex to preprocess and parse the Czech-side of the training data. Treex uses the MST parser (McDonald et al., 2005), which produces dependency graphs with non-projective arcs. In order to extract SCFG rules, we first applied the following conversion process: i) the dependency graphs were projectivized using the Malt Parser, which implements the method described in Nivre and Nilsson (2005) (we used the ‘Head’ encoding scheme); ii) the projective dependency graphs were converted to CFG trees. In addition, we reduced the complex positional tags to simple POS tags by discarding the morphological attributes. The CFG trees were not binarized. We also experimented with unificationbased agreement and case governme"
W16-2327,W13-2221,1,0.819391,"e used the monolingual data to build three different language models which were then linearly interpolated. System tuning was with the SCIELO development data provided for the biomedical task. 4 4.4 SCFG rules were extracted from the word-aligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004, 2006). Minimal GHKM rules were composed into larger rules subject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restrictions on rule composition, fully non-lexical unary rules were eliminated using the method described in Chung et al. (2011) and rules with scope greater than 3 (Hopkins and Langmead, 2010) were pruned from the translation grammar. Scope pruning makes parsing tractable without the need for grammar binar"
W16-2327,W14-4011,1,0.900492,"Missing"
W16-2327,P05-1013,0,0.487034,"ased constraints. Cased B LEU scores are shown. They are averaged over three tuning runs (note that baseline weights are reused in the experiments with constraints). Syntax-based Experiments English→Czech For English→Czech, we used Treex to preprocess and parse the Czech-side of the training data. Treex uses the MST parser (McDonald et al., 2005), which produces dependency graphs with non-projective arcs. In order to extract SCFG rules, we first applied the following conversion process: i) the dependency graphs were projectivized using the Malt Parser, which implements the method described in Nivre and Nilsson (2005) (we used the ‘Head’ encoding scheme); ii) the projective dependency graphs were converted to CFG trees. In addition, we reduced the complex positional tags to simple POS tags by discarding the morphological attributes. The CFG trees were not binarized. We also experimented with unificationbased agreement and case government constraints (Williams and Koehn, 2011; Williams, 2014). Specifically, our constraints were designed to enforce: i) case, gender, and number agreement between nouns and pre-nominal adjectival modifiers; ii) number and person agreement between subjects and verbs; iii) case a"
W16-2327,Q15-1013,1,0.838097,"hypothesis better in a surprisingly larger span of words, improving overall sentence structure on average. Crazy Reordering 3 Table 7: Manual evaluation of translations as proposed by the English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We repo"
W16-2327,P03-1021,0,0.0414024,"translation task. We used two test sets from the HimL project and the Khresmoi test set. Results with and without constraints are shown in Table 5. We used hard constraints and reused the baseline weights (re-tuning did not appear to give additional gains). rareness penalty. 4.6 Decoding Decoding for the string-to-tree models is based on Sennrich’s (2014) recursive variant of the CYK+ parsing algorithm combined with LM integration via cube pruning (Chiang, 2007). 4.7 Tuning The feature weights for the English→Czech and Finnish→English systems were tuned using the Moses implementation of MERT (Och, 2003). For the remaining systems we used k-best MIRA (Cherry and Foster, 2012) due to the use of sparse features. We used randomly-chosen subsets of the previous years’ test data to speed up decoding. 5 5.1 system baseline + constraints HimL1 23.3 23.6 B LEU HimL2 Khresmoi 18.6 20.4 18.8 20.7 Table 5: Translation results on the development system for English→Czech with unification-based constraints. Cased B LEU scores are shown. They are averaged over three tuning runs (note that baseline weights are reused in the experiments with constraints). Syntax-based Experiments English→Czech For English→Cze"
W16-2327,D15-1248,1,0.87645,"for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we annotate the source text with the Stanford Neural Network dependency parser (Chen and Manning, 2014), along with heuristic projectivization (Nivre and Nilsson, 2005). Results are shown in Table 8. We report results of last year’s system (Williams et al., 2015), which was ranked (joint) first at WMT 15. Our improvements this year stem from particle verb restructuring (Sennrich and Haddow, 2015), and the use of the new monolingual News Crawl 2015 corpus for In 4 cases, the system with constraints delivered much better translation, and three of thos"
W16-2327,P06-1055,0,0.0370971,"ility of the tree fragment from which the rule was extracted; and a rule As in the phrase-based models, we used fast_align for word alignment and the grow-diag-final-and heuristic for symmetrization. 4.3 unbinarized 5 20 5 Table 4: Parameter settings for rule composition. The parameters were relaxed for systems that used binarization to allow for the increase in tree node density. Except for English-Czech, which we describe separately in Section 5.1, preprocessing was similar to the phrase-based systems (Section 2.3). To parse the target-side of the training data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, and the ParZu dependency parser (Sennrich et al., 2013) for German. Except where stated otherwise, we right-binarized the trees after parsing to increase rule coverage. 4.2 Rule Extraction Language Models As in the phrase-based systems (Section 2.3), we used linearly-interpolated language models as standard, with some systems adding Common403 In preliminary experiments we used a smaller training set, comprising 2 million sentence pairs sampled from OPUS and monolingual data from last year’s WMT translation task. We used two test sets from the HimL project"
W16-2327,P16-1162,1,0.597202,"rm any corpus filtering other than the standard Moses method, which removes sentence pairs with extreme length ratios, and sentences longer than 80 tokens. Introduction Edinburgh’s submissions to the WMT 2016 news translation task fall into two distinct groups: neural translation systems and statistical translation systems. In this paper, we describe the statistical systems, which includes a mix of phrase-based and syntax-based approaches. We also include a brief description of our phrase-based submission to the WMT16 biomedical translation task. Our neural systems are described separately in Sennrich et al. (2016a). In most cases, our statistical systems build on last year’s, incorporating recent modelling refinements and adding this year’s new training data. For Romanian—a new language this year—we paid particular attention to language-specific processing of diacritics. For English→Czech, we experimented with a string-to-tree system, first using Treex1 (formerly TectoMT; Popel and Žabokrtský, 2010) to produce Czech dependency parses, then converting them to constituency representation and extracting GHKM rules. In the next two sections, we describe the phrasebased systems, first describing the core s"
W16-2327,W11-2126,1,0.846339,"dependency graphs with non-projective arcs. In order to extract SCFG rules, we first applied the following conversion process: i) the dependency graphs were projectivized using the Malt Parser, which implements the method described in Nivre and Nilsson (2005) (we used the ‘Head’ encoding scheme); ii) the projective dependency graphs were converted to CFG trees. In addition, we reduced the complex positional tags to simple POS tags by discarding the morphological attributes. The CFG trees were not binarized. We also experimented with unificationbased agreement and case government constraints (Williams and Koehn, 2011; Williams, 2014). Specifically, our constraints were designed to enforce: i) case, gender, and number agreement between nouns and pre-nominal adjectival modifiers; ii) number and person agreement between subjects and verbs; iii) case agreement between prepositions and nouns; iv) use of nominative case for subject nouns. For every Czech word in the training data, we obtained a set of morphological analyses using MorphoDiTa (Straková et al., 2014). From these analyses, we constructed a lexicon of feature structures. For constraint extraction, we used handwritten rules along the lines of those d"
W16-2327,R13-1079,1,0.855359,"ased models, we used fast_align for word alignment and the grow-diag-final-and heuristic for symmetrization. 4.3 unbinarized 5 20 5 Table 4: Parameter settings for rule composition. The parameters were relaxed for systems that used binarization to allow for the increase in tree node density. Except for English-Czech, which we describe separately in Section 5.1, preprocessing was similar to the phrase-based systems (Section 2.3). To parse the target-side of the training data, we used the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) for English, and the ParZu dependency parser (Sennrich et al., 2013) for German. Except where stated otherwise, we right-binarized the trees after parsing to increase rule coverage. 4.2 Rule Extraction Language Models As in the phrase-based systems (Section 2.3), we used linearly-interpolated language models as standard, with some systems adding Common403 In preliminary experiments we used a smaller training set, comprising 2 million sentence pairs sampled from OPUS and monolingual data from last year’s WMT translation task. We used two test sets from the HimL project and the Khresmoi test set. Results with and without constraints are shown in Table 5. We used"
W16-2327,W12-3150,1,0.844228,"M sentences of parallel data. Our monolingual corpus was made up of three parts: all the English monolingual medical data from WMT14 medical, WMT16 biomedical and EMEA (11M sentences); all the English LDC GigaWord data (180M sentences); and all the English general domain data from WMT16 (240M sentences). We used the monolingual data to build three different language models which were then linearly interpolated. System tuning was with the SCIELO development data provided for the biomedical task. 4 4.4 SCFG rules were extracted from the word-aligned parallel data using the Moses implementation (Williams and Koehn, 2012) of the GHKM algorithm (Galley et al., 2004, 2006). Minimal GHKM rules were composed into larger rules subject to restrictions on the size of the resulting tree fragment. We used the settings shown in Table 4, which were chosen empirically during the development of 2013’s systems (Nadejde et al., 2013). parameter rule depth node count rule size Syntax-based System Overview For all syntax-based systems, we used a string-totree model based on a synchronous context-free grammar (SCFG) with linguistically-motivated labels on the target side. 4.1 Preprocessing binarized 7 30 7 Further to the restri"
W16-2327,W14-3324,1,0.800794,"Missing"
W16-2327,W15-3024,1,0.879589,"rder. Since the hard unification constraints effectively only avoid some of the possible translations (i.e. reduce the search space), we conclude that having to obey mere agreement constraints helps to select a hypothesis better in a surprisingly larger span of words, improving overall sentence structure on average. Crazy Reordering 3 Table 7: Manual evaluation of translations as proposed by the English→Czech system with unification constraints vs. the same system without constraints. 5.2 English→German This year’s string-to-tree submission for English→German is similar to last year’s system (Williams et al., 2015). In addition to the baseline feature functions, it contains count-based 5-gram Neural Network language model (NPLM) (Vaswani et al., 2013), a relational dependency language model (RDLM) (Sennrich, 2015), and soft source-syntactic constraints (Huck et al., 2014). The parameters of the model are tuned towards the linear interpolation of B LEU and the syntactic metric HWCM (Liu and Gildea, 2005; Sennrich, 2015). Trees are transformed through binarization and a hierarchical representation of morphologically complex words (Sennrich and Haddow, 2015). For the soft source-syntactic constraints, we a"
W16-2327,P14-5003,0,0.0285107,"Missing"
W16-2327,tiedemann-2012-parallel,0,0.108338,". 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English CommonCrawl corpus. We used newsdev2015 for tuning, and newsdev2015 for testing during system development. One clear problem that we noted with our submission from last year was the large number of OOVs, which were then copied directly into the English output. This is undoubtedly due to the agglutinative nature of Finnish, and probably was the cause of our system being poorly judged by human evaluators,"
W16-2327,D13-1140,0,0.194779,"(Koehn and Haddow, 2009). CommonCrawl LMs Our CommonCrawl language models were trained in the same way as the individual corpus-specific standard models, but were not linearly-interpolated with other LMs. Instead, the log probabilities of CommonCrawl LMs were added as separate features of the systems’ linear models. 3 Neural LMs For some of our phrase-based systems we experimented with feed-forward neural network language models, both trained on target n-grams only, and on “joint” or “bilingual” ngrams (Devlin et al., 2014; Le et al., 2012). For training these models we used the NPLM toolkit (Vaswani et al., 2013), for which we have now implemented gradient clipping to address numerical issues often encountered during training. 2.4 3.1 Phrase-based Experiments Finnish→English Similar to last year (Haddow et al., 2015), we built an unconstrained system for Finnish→English using data extracted from OPUS (Tiedemann, 2012). Our parallel training set was the same as we used previously, but the language model training set was extended with the addition of the news2015 monolingual corpus and the large WMT16 English CommonCrawl corpus. We used newsdev2015 for tuning, and newsdev2015 for testing during system d"
W16-2327,N07-1051,0,\N,Missing
W16-2327,W09-0429,1,\N,Missing
W16-2327,P05-1045,0,\N,Missing
W16-2327,P13-2121,0,\N,Missing
W17-4702,2015.iwslt-evaluation.11,0,0.105275,"Missing"
W17-4702,N06-2001,0,0.0358218,"ses can be found in the appendix. We include the location of the sentence in the original corpus in our metadata to allow future experiments with document-level information.3 4 Sense Embeddings in Neural Machine Translation In addition to the evaluation of a standard NMT model on the word sense disambiguation task detailed in the previous section, we present two experiments on German→English and German→French to improve lexical choice using methods from WSD. In a first approach, we compute sense embeddings and include the resulting sense labels into the NMT model as additional input features (Alexandrescu and Kirchhoff, 2006; Sennrich and Haddow, 2016). For our second experiment, instead of adding the labels directly to the input, we use them to build lexical chains of similar words in the given document. These lexical chains contain information about the topic and/or domain of the document, and we include them as additional features into our NMT model. 2 Sentence pairs have been extracted from the following corpora: • WMT test and development sets 2006-2016 (de-en) and 2006-2013 (de-fr) • Cr´edit Suisse News Corpus https://pub.cl. uzh.ch/projects/b4c/de/ 4.1 • corpora from OPUS ((Tiedemann, 2012)): Sense Embeddi"
W17-4702,W17-4813,1,0.751083,"alculate the cosine similarity between their sense embeddings.6 The closer to 1.0 the resulting value is, the higher their semantic similarity. To distinguish between similar and non-similar senses, we set a threshold of 0.85 that we manually picked by looking at how different values affect the resulting lexical chains: a lower threshold builds lexical chains containing sense words that are not sufficiently related, whereas a higher threshold results in semantically strong, but possibly incomplete lexical chains that do not cover all words belonging to the chain. We use the method proposed by Mascarell (2017) to detect lexical chains in a document. This method is inspired by Morris and Hirst (1991)’s approach, which manually finds lexical chains in a document using a thesaurus to obtain the similarity between words. As detailed in Section 4.1, we use sense embeddings instead of a dictionary to compute the semantic similarity. Given a document as input, our method processes sentences and their content words sequentially. For each sentence, it computes the semantic similarity between the current content word c and each previous content word c’ in the previous five sentences, based on the approach by"
W17-4702,S10-1002,0,0.0510973,"Missing"
W17-4702,D16-1025,0,0.0236082,"experiment, we extract lexical chains based on sense embeddings from the document and integrate this information into the NMT model. While a baseline NMT system disambiguates frequent word senses quite reliably, the annotation with both sense labels and lexical chains improves the neural models’ performance on rare word senses. 11 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 11–19 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics more fluent than phrase-based SMT (Neubig et al., 2015; Bojar et al., 2016; Bentivogli et al., 2016), but results in terms of adequacy are more mixed. Bentivogli et al. (2016) report improvements in lexical choice based on HTER matches on the lemma-level, while Bojar et al. (2016) found no clear improvement in a direct assessment of adequacy. Neubig et al. (2015) perform an error annotation in which the number of lexical choice errors even increases slightly by reranking a syntaxbased statistical machine translation system with an NMT model. We aim to allow for a large-scale, reproducible method of assessing the capability of an NMT model to perform lexical disambiguation. NMT systems can no"
W17-4702,J91-1002,0,0.113241,"e resulting value is, the higher their semantic similarity. To distinguish between similar and non-similar senses, we set a threshold of 0.85 that we manually picked by looking at how different values affect the resulting lexical chains: a lower threshold builds lexical chains containing sense words that are not sufficiently related, whereas a higher threshold results in semantically strong, but possibly incomplete lexical chains that do not cover all words belonging to the chain. We use the method proposed by Mascarell (2017) to detect lexical chains in a document. This method is inspired by Morris and Hirst (1991)’s approach, which manually finds lexical chains in a document using a thesaurus to obtain the similarity between words. As detailed in Section 4.1, we use sense embeddings instead of a dictionary to compute the semantic similarity. Given a document as input, our method processes sentences and their content words sequentially. For each sentence, it computes the semantic similarity between the current content word c and each previous content word c’ in the previous five sentences, based on the approach by Morris and Hirst (1991). If c and c’ are semantically similar, our method proceeds as foll"
W17-4702,W13-0801,0,0.12293,"Missing"
W17-4702,W16-4610,0,0.0356735,"Missing"
W17-4702,W15-5003,0,0.0230918,"achine translation system. For the second experiment, we extract lexical chains based on sense embeddings from the document and integrate this information into the NMT model. While a baseline NMT system disambiguates frequent word senses quite reliably, the annotation with both sense labels and lexical chains improves the neural models’ performance on rare word senses. 11 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 11–19 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics more fluent than phrase-based SMT (Neubig et al., 2015; Bojar et al., 2016; Bentivogli et al., 2016), but results in terms of adequacy are more mixed. Bentivogli et al. (2016) report improvements in lexical choice based on HTER matches on the lemma-level, while Bojar et al. (2016) found no clear improvement in a direct assessment of adequacy. Neubig et al. (2015) perform an error annotation in which the number of lexical choice errors even increases slightly by reranking a syntaxbased statistical machine translation system with an NMT model. We aim to allow for a large-scale, reproducible method of assessing the capability of an NMT model to perf"
W17-4702,P02-1040,0,0.124351,"Missing"
W17-4702,W16-1620,0,0.00791701,"took my U.S. passport and got in the snake for Extranjeros. So I took my U.S. passport and got in the serpent for Extranjeros. source: reference: Er hat zwar schnell den Finger am Abzug, aber er ist eben neu. Il a la gˆachette facile mais c’est parce qu’il d´ebute. contrastive: contrastive: contrastive: contrastive: Il a la soustraction facile mais c’est parce qu’il d´ebute. Il a la d´eduction facile mais c’est parce qu’il d´ebute. Il a la sortie facile mais c’est parce qu’il d´ebute. Il a la r´etraction facile mais c’est parce qu’il d´ebute. Table 1: Contrastive Translations use SenseGram4 (Pelevina et al., 2016), which has been shown to perform as good as stat-of-theart unsupervised WSD systems. The method to learn the sense embeddings using SenseGram consists of four steps that we briefly summarise here. First, the method learns word embeddings using the word2vec toolkit (Mikolov et al., 2013).5 It then uses these word embeddings to build a word similarity graph, where each word is linked to its 200 nearest neighbours. Next, it induces a sense inventory, where each sense is represented by a cluster of words (e.g. the sense of table-furniture is represented with the word cluster desk, bench, dining t"
W17-4702,S13-2029,0,0.37929,"Missing"
W17-4702,E17-2060,1,0.0729243,"of one of its other meanings. We cluster different translations that overlap in meaning, i.e. that are (at least sometimes) used interchangeably. We do not produce any contrastive translations that belong to the same cluster as the reference translation. As an example, we show the sense clusters that we consider for two ambiguous German words: Schlange: serpent, snake line, queue Abzug: withdrawal, departure trigger discount, subtraction r´etraction, sortie gˆachette d´eduction, soustraction Table 1 shows an example of source, reference, and contrastive sentences. Our approach is inspired by Sennrich (2017), who use contrastive translation pairs to evaluate various error types, including morpho-syntactic agreement and polarity errors. Apart from focusing on another error type, namely word sense errors, our approach differs in that we pair a human reference translation not just with one contrastive example, but a set of contrastive examples, i.e. a set of incorrect translations of the semantically ambiguous source word. The model is considered correct if it scores the human reference translation higher than all of the contrastive translations. Note that this evaluation does not directly assess th"
W17-4702,E17-3017,1,0.719905,"Missing"
W17-4702,W16-2209,1,0.831825,"We include the location of the sentence in the original corpus in our metadata to allow future experiments with document-level information.3 4 Sense Embeddings in Neural Machine Translation In addition to the evaluation of a standard NMT model on the word sense disambiguation task detailed in the previous section, we present two experiments on German→English and German→French to improve lexical choice using methods from WSD. In a first approach, we compute sense embeddings and include the resulting sense labels into the NMT model as additional input features (Alexandrescu and Kirchhoff, 2006; Sennrich and Haddow, 2016). For our second experiment, instead of adding the labels directly to the input, we use them to build lexical chains of similar words in the given document. These lexical chains contain information about the topic and/or domain of the document, and we include them as additional features into our NMT model. 2 Sentence pairs have been extracted from the following corpora: • WMT test and development sets 2006-2016 (de-en) and 2006-2013 (de-fr) • Cr´edit Suisse News Corpus https://pub.cl. uzh.ch/projects/b4c/de/ 4.1 • corpora from OPUS ((Tiedemann, 2012)): Sense Embeddings Sense embeddings are vec"
W17-4702,P16-1162,1,0.0866083,"ict it. In all these tasks, a word-level translation (or set of translations) is defined as the gold label. A major problem is that an MT system will be punished for producing a synonym, paraphrase, or inflected variant of the predefined gold label. We thus propose a more constrained task where an MT system has to select one out of a predefined set of translations. Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) has recently emerged as the new state of the art in machine translation, producing top-ranked systems in recent shared tasks (Luong and Manning, 2015; Sennrich et al., 2016a; Neubig, 2016). The strengths and weaknesses of NMT have been the subject of recent research, and previous studies involving human analysis have consistently found NMT to be Word sense disambiguation is necessary in translation because different word senses often have different translations. Neural machine translation models learn different senses of words as part of an endto-end translation task, and their capability to perform word sense disambiguation has so far not been quantified. We exploit the fact that neural translation models can score arbitrary translations to design a novel cross"
W17-4702,tiedemann-2012-parallel,0,0.0071642,"Alexandrescu and Kirchhoff, 2006; Sennrich and Haddow, 2016). For our second experiment, instead of adding the labels directly to the input, we use them to build lexical chains of similar words in the given document. These lexical chains contain information about the topic and/or domain of the document, and we include them as additional features into our NMT model. 2 Sentence pairs have been extracted from the following corpora: • WMT test and development sets 2006-2016 (de-en) and 2006-2013 (de-fr) • Cr´edit Suisse News Corpus https://pub.cl. uzh.ch/projects/b4c/de/ 4.1 • corpora from OPUS ((Tiedemann, 2012)): Sense Embeddings Sense embeddings are vector representations of word senses in a vector space, but unlike word embeddings, where every word form receives a vector representation, with sense embeddings we obtain separate vector representations for each sense of a given word. To compute the sense embeddings we – Global Voices (http://opus.lingfil. uu.se/GlobalVoices.php) – Books (http://opus.lingfil.uu.se/ Books.php) – EU Bookshop Corpus (http://opus. lingfil.uu.se/EUbookshop.php) – OpenSubtitles 2016 (German-French) (http://opus.lingfil.uu.se/ OpenSubtitles2016.php) 3 A snapshot of the corpo"
W17-4702,H05-1097,0,0.0415032,"Missing"
W17-4702,L16-1561,0,0.00833714,"Missing"
W17-4702,W09-2413,0,\N,Missing
W17-4702,W16-2323,1,\N,Missing
W17-4707,W07-0702,1,0.671821,"oposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) made strong independence assumptions between the target word sequence and the CCG categories. In this work we take advantage of the expressive power of recurrent neural networks to learn representations that generate both words and CCG supertags, conditioned on the entire lexical and syntactic target history. ments, and also tense and morphological aspects of the word in a given context. Consider the sentence in Figure 1. This sentence contains two PP attachments and could lead to several disambiguation possibilities (“in” can attach to “Netanyahu” or “receives”, and “of” can attach to “capit"
W17-4707,J07-2003,0,0.0156397,"ining. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. Th"
W17-4707,W14-4012,0,0.028354,"Missing"
W17-4707,P17-2021,0,0.0761778,"ework to show source and target syntax provide complementary information. Applying more tightly coupled linguistic factors on the target for NMT has been previously investigated. Niehues et al. (2016) proposed a factored RNN-based language model for re-scoring an n-best list produced by a phrase-based MT system. In recent work, Mart´ınez et al. (2016) implemented a factored NMT decoder which generated both lemmas and morphological tags. The two factors were then post-processed to generate the word form. Unfortunately no real gain was reported for these experiments. Concurrently with our work, Aharoni and Goldberg (2017) proposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based model"
W17-4707,D14-1179,0,0.00956885,"Missing"
W17-4707,N16-1024,0,0.0146616,"al. (2016) proposed a factored RNN-based language model for re-scoring an n-best list produced by a phrase-based MT system. In recent work, Mart´ınez et al. (2016) implemented a factored NMT decoder which generated both lemmas and morphological tags. The two factors were then post-processed to generate the word form. Unfortunately no real gain was reported for these experiments. Concurrently with our work, Aharoni and Goldberg (2017) proposed serializing the target constituency trees and Eriguchi et al. (2017) model target dependency relations by augmenting the NMT decoder with a RNN grammar (Dyer et al., 2016). In our work, we use CCG supertags which are a more compact representation of global syntax. Furthermore, we do not focus on model architectures, and instead we explore the more general problem of including target syntax in NMT: comparing tightly and loosely coupled syntactic information and showing source and target syntax are complementary. Previous work on integrating CCG supertags in factored phrase-based models (Birch et al., 2007) made strong independence assumptions between the target word sequence and the CCG categories. In this work we take advantage of the expressive power of recurr"
W17-4707,P16-1231,0,0.0172413,"butions over separate target vocabularies for the words and the tags: (7) (8) pword y At training time we pre-process the target sequence to add the syntactic annotation and then split only the words into byte-pair-encoding (BPE) (Sennrich et al., 2016b) sub-units. At = T1 Y word p(yjword |x, y1:j−1 ) T2 Y tag p(yktag |x, y1:k−1 ) j ptag y = k 71 (9) (10) The final loss is the sum of the losses for the two decoders: l = −(log(pword ) + log(ptag y y )) DE-EN RO-EN (11) test 2,994 1,984 sets in Table 1. Dependency labels are annotated with ParZU (Sennrich et al., 2013) for German and SyntaxNet (Andor et al., 2016) for Romanian. All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit (Sennrich et al., 2017).4 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a; Sennrich and Haddow, 2016) with minor modifications: we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014). We select the best single models according to BLEU on the development set and use the four best single models for the ensembles. To show that we report results over strong baselines, table 2 compares the scores obtained by ou"
W17-4707,P16-1078,0,0.0552534,"ntence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. The combinators of CCG allow the supertags to capture global syntactic constraints locally. Though NMT 69 Source-side BPE: Obama IOB: O CCG: NP Target-side receives O ((S[dcl]NP)/PP)/NP Net+ B NP an+ I NP yahu E NP in O PP/NP the O NP/N capital O N of O (NPNP)/NP USA"
W17-4707,D16-1025,0,0.0250595,"Missing"
W17-4707,N04-1035,0,0.0228682,"ht coupling of target words and syntax (by interleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned wi"
W17-4707,W16-2208,0,0.0269734,"Missing"
W17-4707,P02-1040,0,0.120603,"of the training data is annotated with CCG lexical tags2 using EasySRL (Lewis et al., 2015) and the available pre-trained model3 . Some longer sentences cannot be processed by the parser and therefore we eliminate them from our training and test data. We report the sentence counts for the filtered data This work 31.0 27.8 28.0 25.6 Sennrich et. al 28.5 26.8 27.8 23.9 Table 2: Comparison of baseline systems in this work and in Sennrich et al. (2016a). Casesensitive BLEU scores reported over newstest2016 with mteval-13a.perl. 1 Normalized diacritics. During training we validate our models with (Papineni et al., 2002) on development sets: newstest2013 for German↔English and newsdev2016 for Romanian↔English. We evaluate the systems on newstest2016 test sets for both lanBLEU 1 We use the same data and annotations for the interleaving approach. 2 The CCG tags include features such as the verb tense (e.g. [ng] for continuous form) or the sentence type (e.g. [pss] for passive). 3 https://github.com/uwnlp/EasySRL 4 https://github.com/rsennrich/nematus There are different encodings for letters with cedilla (s¸,t¸) used interchangeably throughout the corpus. https://en.wikipedia.org/wiki/Romanian_ alphabet#ISO_885"
W17-4707,D13-1176,0,0.032968,"urther improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word in a sentence. The combinators of CCG allow the supertags to capture global syntactic constraints locally. Though NMT 69 Source-side BPE: O"
W17-4707,W05-0908,0,0.0147936,"lish and newsdev2016 for Romanian↔English. We evaluate the systems on newstest2016 test sets for both lanBLEU 1 We use the same data and annotations for the interleaving approach. 2 The CCG tags include features such as the verb tense (e.g. [ng] for continuous form) or the sentence type (e.g. [pss] for passive). 3 https://github.com/uwnlp/EasySRL 4 https://github.com/rsennrich/nematus There are different encodings for letters with cedilla (s¸,t¸) used interchangeably throughout the corpus. https://en.wikipedia.org/wiki/Romanian_ alphabet#ISO_8859 5 72 guage pairs and use bootstrap resampling (Riezler and Maxwell, 2005) to test statistical significance. We compute BLEU with multi-bleu.perl over tokenized sentences both on the development sets, for early stopping, and on the test sets for evaluating our systems. Words are segmented into sub-units that are learned jointly for source and target using BPE (Sennrich et al., 2016b), resulting in a vocabulary size of 85,000. The vocabulary size for CCG supertags was 500. For the experiments with source-side features we use the BPE sub-units and the IOB tags as baseline features. We keep the total word embedding size fixed to 500 dimensions. We allocate 10 dimension"
W17-4707,Q15-1013,1,0.864408,"n the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntacti"
W17-4707,D15-1169,0,0.0163034,"DE→EN EN→DE RO→EN EN→RO1 Experimental Setup and Evaluation 4.1 dev 2,986 1,984 Table 1: Number of sentences in the training, development and test sets. We use EasySRL to label the English side of the parallel corpus with CCG supertags1 instead of using a corpus with gold annotations as in Luong et al. (2016). 4 train 4,468,314 605,885 Data and methods We train the neural MT systems on all the parallel data available at WMT16 (Bojar et al., 2016) for the German↔English and Romanian↔English language pairs. The English side of the training data is annotated with CCG lexical tags2 using EasySRL (Lewis et al., 2015) and the available pre-trained model3 . Some longer sentences cannot be processed by the parser and therefore we eliminate them from our training and test data. We report the sentence counts for the filtered data This work 31.0 27.8 28.0 25.6 Sennrich et. al 28.5 26.8 27.8 23.9 Table 2: Comparison of baseline systems in this work and in Sennrich et al. (2016a). Casesensitive BLEU scores reported over newstest2016 with mteval-13a.perl. 1 Normalized diacritics. During training we validate our models with (Papineni et al., 2002) on development sets: newstest2013 for German↔English and newsdev2016"
W17-4707,E17-3017,1,0.894752,"Missing"
W17-4707,W16-2209,1,0.896448,") show that NMT significantly improves over phrase-based SMT, in particular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment. Another study by Shi et al. (2016) shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled. Recent work which incorporates additional source-side linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even though neural models have strong learning capabilities, explicit features can still improve translation quality. In this work, we examine the benefit of incorporating global syntactic information on the target-side. We also address the question of how best to incorporate this information. For language pairs where syntactic resources are available on both the source and target-side, we show that approaches to incorporate source syntax and target syntax are complementary. We propose a method for tightly coupling words and syntax by interleaving the target syntactic representation"
W17-4707,W07-0701,0,0.202057,"words and syntax (by interleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories,"
W17-4707,W16-2323,1,0.729608,"ish, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German→English and 1.2 BLEU for Romanian→English. 1 Introduction Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al., 2016a; Junczys-Dowmunt et al., 2016). Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed cap68 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 68–79 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al. (2016) co-train a translation model and a source-side syntactic parser which share the encoder. Our multitask models extend their work to attention-based NMT models and to predicting target-side syntax as the sec"
W17-4707,W16-2204,1,0.813749,"signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indicate context-sensitive morpho-syntactic properties of a word"
W17-4707,P16-1162,1,0.46286,"ish, a lowresource pair and also several syntactic phenomena including prepositional phrase attachment. Furthermore, a tight coupling of words and syntax improves translation quality more than multitask training. By combining target-syntax with adding source-side dependency labels in the embedding layer, we obtain a total improvement of 0.9 BLEU for German→English and 1.2 BLEU for Romanian→English. 1 Introduction Sequence-to-sequence neural machine translation (NMT) models (Sutskever et al., 2014; Cho et al., 2014b; Bahdanau et al., 2015) are state-of-the-art on a multitude of language-pairs (Sennrich et al., 2016a; Junczys-Dowmunt et al., 2016). Part of the appeal of neural models is that they can learn to implicitly model phenomena which underlie high quality output, and some syntax is indeed cap68 Proceedings of the Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 68–79 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics et al. (2016) co-train a translation model and a source-side syntactic parser which share the encoder. Our multitask models extend their work to attention-based NMT models and to predicting target-side syntax as the sec"
W17-4707,R13-1079,1,0.793726,"1 and T2 . This results in two probability distributions over separate target vocabularies for the words and the tags: (7) (8) pword y At training time we pre-process the target sequence to add the syntactic annotation and then split only the words into byte-pair-encoding (BPE) (Sennrich et al., 2016b) sub-units. At = T1 Y word p(yjword |x, y1:j−1 ) T2 Y tag p(yktag |x, y1:k−1 ) j ptag y = k 71 (9) (10) The final loss is the sum of the losses for the two decoders: l = −(log(pword ) + log(ptag y y )) DE-EN RO-EN (11) test 2,994 1,984 sets in Table 1. Dependency labels are annotated with ParZU (Sennrich et al., 2013) for German and SyntaxNet (Andor et al., 2016) for Romanian. All the neural MT systems are attentional encoder-decoder networks (Bahdanau et al., 2015) as implemented in the Nematus toolkit (Sennrich et al., 2017).4 We use similar hyper-parameters to those reported by (Sennrich et al., 2016a; Sennrich and Haddow, 2016) with minor modifications: we used mini-batches of size 60 and Adam optimizer (Kingma and Ba, 2014). We select the best single models according to BLEU on the development set and use the four best single models for the ensembles. To show that we report results over strong baselin"
W17-4707,D16-1159,0,0.0475338,"Alexandra Birch1 1 School of Informatics, University of Edinburgh 2 Adam Mickiewicz University 3 Dep. of Computer Science, Johns Hopkins University {m.nadejde,siva.reddy, rico.sennrich, a.birch}@ed.ac.uk {t.dwojak,junczys}@amu.edu.pl, phi@jhu.edu Abstract tured by these models. In a detailed analysis, Bentivogli et al. (2016) show that NMT significantly improves over phrase-based SMT, in particular with respect to morphology and word order, but that results can still be improved for longer sentences and complex syntactic phenomena such as prepositional phrase (PP) attachment. Another study by Shi et al. (2016) shows that the encoder layer of NMT partially learns syntactic information about the source language, however complex syntactic phenomena such as coordination or PP attachment are poorly modeled. Recent work which incorporates additional source-side linguistic information in NMT models (Luong et al., 2016; Sennrich and Haddow, 2016) show that even though neural models have strong learning capabilities, explicit features can still improve translation quality. In this work, we examine the benefit of incorporating global syntactic information on the target-side. We also address the question of h"
W17-4707,W12-3150,1,0.846453,"erleaving) improves translation quality more than the decoupled signal from multitask training. • We show that incorporating source-side linguistic information is complimentary to our method, further improving the translation quality. • We present a fine-grained analysis of SNMT and show consistent gains for different linguistic phenomena and sentence lengths. 2 Related work Syntax has helped in statistical machine translation (SMT) to capture dependencies between distant words that impact morphological agreement, subcategorisation and word order (Galley et al., 2004; Menezes and Quirk, 2007; Williams and Koehn, 2012; Nadejde et al., 2013; Sennrich, 2015; Nadejde et al., 2016a,b; Chiang, 2007). There has been some work in NMT on modeling source-side syntax implicitly or explicitly. Kalchbrenner and Blunsom (2013); Cho et al. (2014a) capture the hierarchical aspects of language implicitly by using convolutional neural networks, while Eriguchi et al. (2016) use the parse tree of the source sentence to guide the recurrence and attention model in tree-to-sequence NMT. Luong 3 Modeling Syntax in NMT CCG is a lexicalised formalism in which words are assigned with syntactic categories, i.e., supertags, that indi"
W17-4710,W14-4012,0,0.19765,"Missing"
W17-4710,D14-1179,0,0.0569526,"Missing"
W17-4710,Q16-1027,0,0.426398,"schemes (Pascanu et al., 2014) that give rise to different, orthogonal, definitions of depth (Zhang et al., 2016) which can affect the model performance depending on a given task. This is further complicated in sequence-to-sequence models as they contain multiple sub-networks, recurrent or feed-forward, each of which can be deep in different ways, giving rise to a large number of possible configurations. In this work we focus on stacked and deep transition recurrent architectures as defined by Pascanu et al. (2014). Different types of stacked architectures have been successfully used for NMT (Zhou et al., 2016; Wu et al., 2016). However, there is a lack of empirical comparisons of different deep architectures. Deep transition architectures have been successfully used for language modeling (Zilly et al., 2016), but not for NMT so far. We evaluate these architectures, both alone and in combination, varying the connection scheme between the different components and their depth over the different dimensions, measuring the performance of the different configurations on the WMT news translation task.1 Related work includes that of Britz et al. (2017), who have performed an exploration of NMT architecture"
W17-4710,E17-2060,1,0.824839,"o models are large and their results are highlighted separately. tances, since each layer may lose information during forward computation or backpropagation. This may not be a significant issue in the encoder, as the attention mechanism provides short paths from any source word state to the decoder, but the decoder contains no such shortcuts between its states, therefore it might be possible that this negatively affects its ability to model long-distance relationships in the target text, such as subject–verb agreement. Here, we seek to answer this question by testing our models on Lingeval97 (Sennrich, 2017), a test set which provides contrastive translation pairs for different types of errors. For the example of subject-verb agreement, contrastive translations are created from a reference translation by changing the grammatical number of the verb, and we can measure how often the NMT model prefers the correct reference over the contrastive variant. In Figure 5, we show accuracy as a function of the distance between subject and verb. We find that information is successfully passed over long distances by the deep recurrent transition network. Even for decisions that require information to be carri"
W17-4710,E17-3017,1,0.827403,"der in this work are GRU (Cho et al., 2014a) sequence-to-sequence transducers (Sutskever et al., 2014; Cho et al., 2014b) with attention (Bahdanau et al., 2015). In this section we describe the baseline system and the variants that we evaluated. 2.1 Figure 1: Deep transition decoder GRU transition for the current time step is carried over as the ""state"" input of the first GRU transition for the next time step. Applying this architecture to NMT is a novel contribution. Baseline Architecture As our baseline, we use the NMT architecture implemented in Nematus, which is described in more depth by Sennrich et al. (2017b). We augment it with layer normalization (Ba et al., 2016), which we have found to both improve translation quality and make training considerably faster. For our discussion, it is relevant that the baseline architecture already exhibits two types of depth: 2.2.1 Deep Transition Encoder As in a baseline shallow Nematus system, the encoder is a bidirectional recurrent neural network. Let Ls be the encoder recurrence depth, then for the i-th source word in the forward direction the → − → − forward source word state h i ≡ h i,Ls is computed as:  →  → − − h i,1 = GRU1 xi , h i−1,Ls  →  → − −"
W17-4710,W17-4739,1,\N,Missing
W17-4739,P10-2041,0,0.0320685,"Missing"
W17-4739,buck-etal-2014-n,1,0.80914,"Missing"
W17-4739,W17-4705,0,0.0149621,"Missing"
W17-4739,D14-1179,0,0.00518473,"Missing"
W17-4739,P16-1009,1,0.524474,"ablative experiments, reporting on the effectivenes of layer normalization, deep architectures, and different ensembling techniques. 1 Introduction We participated in the WMT17 shared news translation task for 12 translation directions, translating between English and Czech, German, Latvian, Russian, Turkish and Chinese, and in the WMT17 shared biomedical translation task for English to Czech, German, Polish and Romanian.1 We submitted neural machine translation systems trained with Nematus (Sennrich et al., 2017). Our setup is based on techniques described in last year’s system description (Sennrich et al., 2016a), including the use of subword models (Sennrich et al., 1 2 Novelties Here we describe the main differences to last year’s systems. We provide trained models and training commands at http://data.statmt.org/wmt17_systems/ 389 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 389–399 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics 2.1 Subword Segmentation stacked architecture. Implementations of both of these architectures are available in Nematus. For completeness, we here reproduce the description of the"
W17-4739,P16-1162,1,\N,Missing
W17-4739,E17-2025,0,\N,Missing
W18-6304,D14-1179,0,0.101609,"Missing"
W18-6304,P18-1167,0,0.0729323,"chanisms and the traditional word alignment. They find that attention mechanisms not only pay attention to the aligned source tokens but also distribute attention to some unaligned source tokens. In this paper, we perform a more fine-grained investigation of attention mechanisms, focusing on the task of translating ambiguous nouns. We also explore the advanced attention mechanisms in Transformer models (Vaswani et al., 2017). The encoder-decoder attention mechanisms differ in NMT models. Tang et al. (2018b) evaluate different NMT models, but focusing on NMT architectures. Tang et al. (2018a); Domhan (2018) compare different attention mechanisms. However, there is no detailed analysis on attention mechanisms. In this paper, we mainly investigate the encoderdecoder attention mechanisms. More specifically, we explore how attention mechanisms work when translating ambiguous nouns. ambiguous nouns, compared to when translating other words. To test this hypothesis, we compare the attention weight over ambiguous nouns with the attention weight over all words and all nouns. In Section 6, we first compare the two different attention mechanisms. Then, we explore the relation between accuracy and attentio"
W18-6304,N13-1073,0,0.0367842,"are multiple attention functions which compute the attention from the linearly projected vectors in parallel. Then, the context vectors from all the heads are concatenated and fed into the decoder networks. 4 Instead of using NMT models to score the contrastive translations, we use NMT models to translate source sentences and evaluate the translations of the ambiguous nouns directly. We evaluate two popular NMT models with different attention mechanisms. One is RNNS2S with the vanilla attention mechanism, and the other is Transformer with the advanced attention mechanism. We apply fast-align (Dyer et al., 2013) to get the aligned translations of ambiguous nouns. To achieve better alignment, we run fast-align on both training data and test data which includes reference translations and generated translations. However, for some ambiguous nouns, there is no alignment. We call these ambiguous nouns filtered. There are multiple reference translations for 3.2 ContraWSD ContraWSD3 from Rios et al. (2017) consists of contrastive translation sets where the human ref3 Evaluation https://github.com/a-rios/ContraWSD 28 each ambiguous noun in ContraWSD. We additionally add their synonyms4 into the reference tran"
W18-6304,I17-1004,0,0.241437,"find that attentional NMT models perform well in translating ambiguous words with frequent senses,2 while Liu et al. (2018) show that there are plenty of incorrect translations of ambiguous words. In Section 4, we evaluate the translations of ambiguous nouns, using the test set from Rios et al. (2017). In this setting, we expect to get a more accurate picture of the WSD performance of NMT models. In Section 5, we present a fine-grained investigation of attention distributions of different attention mechanisms. We focus on the process of translating the given ambiguous nouns. Previous studies (Ghader and Monz, 2017; Koehn and Knowles, 2017) have shown that attention mechanisms learn to pay attention to some unaligned but useful context tokens for predictions. Thus, we hypothesize that attention mechanisms distribute more attention to context tokens when translating Recent work has shown that the encoderdecoder attention mechanisms in neural machine translation (NMT) are different from the word alignment in statistical machine translation. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that attention"
W18-6304,E17-3017,1,0.888864,"Missing"
W18-6304,D13-1176,0,0.268957,"Missing"
W18-6304,W17-3204,0,0.18504,"MT models perform well in translating ambiguous words with frequent senses,2 while Liu et al. (2018) show that there are plenty of incorrect translations of ambiguous words. In Section 4, we evaluate the translations of ambiguous nouns, using the test set from Rios et al. (2017). In this setting, we expect to get a more accurate picture of the WSD performance of NMT models. In Section 5, we present a fine-grained investigation of attention distributions of different attention mechanisms. We focus on the process of translating the given ambiguous nouns. Previous studies (Ghader and Monz, 2017; Koehn and Knowles, 2017) have shown that attention mechanisms learn to pay attention to some unaligned but useful context tokens for predictions. Thus, we hypothesize that attention mechanisms distribute more attention to context tokens when translating Recent work has shown that the encoderdecoder attention mechanisms in neural machine translation (NMT) are different from the word alignment in statistical machine translation. In this paper, we focus on analyzing encoder-decoder attention mechanisms, in the case of word sense disambiguation (WSD) in NMT models. We hypothesize that attention mechanisms pay more attent"
W18-6304,C18-1112,1,0.837026,"decoder. Koehn and Knowles (2017) and Ghader and Monz (2017) investigate the relation between attention mechanisms and the traditional word alignment. They find that attention mechanisms not only pay attention to the aligned source tokens but also distribute attention to some unaligned source tokens. In this paper, we perform a more fine-grained investigation of attention mechanisms, focusing on the task of translating ambiguous nouns. We also explore the advanced attention mechanisms in Transformer models (Vaswani et al., 2017). The encoder-decoder attention mechanisms differ in NMT models. Tang et al. (2018b) evaluate different NMT models, but focusing on NMT architectures. Tang et al. (2018a); Domhan (2018) compare different attention mechanisms. However, there is no detailed analysis on attention mechanisms. In this paper, we mainly investigate the encoderdecoder attention mechanisms. More specifically, we explore how attention mechanisms work when translating ambiguous nouns. ambiguous nouns, compared to when translating other words. To test this hypothesis, we compare the attention weight over ambiguous nouns with the attention weight over all words and all nouns. In Section 6, we first comp"
W18-6304,N03-1017,0,0.098419,"s, we reveal that the first few layers gradually learn to “align” source and target tokens and the last few layers learn to extract features from the related but unaligned context tokens. 1 Introduction Human languages exhibit many different types of ambiguity. Lexical ambiguity refers to the fact that words can have more than one semantic meaning. Dealing with these lexical ambiguities is a challenge for various NLP tasks. Word sense disambiguation (WSD) is recognizing the correct meaning of an ambiguous word, with the help of contextual information. In statistical machine translation (SMT) (Koehn et al., 2003), a system could explicitly take context tokens into account to improve the translation of ambiguous words (Vickrey et al., 2005). By con1 Denotes the encoder-decoder attention mechanism in this paper, unless otherwise specified. 2 More than 2,000 instances in the training set. 26 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 26–35 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64004 at different encoder layers in WSD, while we focus on exploring the attenti"
W18-6304,D18-1458,1,0.879952,"Missing"
W18-6304,N18-1121,0,0.116653,"he contextual information needed for disambiguation. Moreover, how the attention mechanism1 deals with ambiguous words is also not known yet. In this paper, we focus on the question of how encoder-decoder attention mechanisms deal with ambiguous nouns. We explore two different attention mechanisms. One is the vanilla one-layer attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), and the other one is the Transformer attention mechanism (Vaswani et al., 2017). Rios et al. (2017) find that attentional NMT models perform well in translating ambiguous words with frequent senses,2 while Liu et al. (2018) show that there are plenty of incorrect translations of ambiguous words. In Section 4, we evaluate the translations of ambiguous nouns, using the test set from Rios et al. (2017). In this setting, we expect to get a more accurate picture of the WSD performance of NMT models. In Section 5, we present a fine-grained investigation of attention distributions of different attention mechanisms. We focus on the process of translating the given ambiguous nouns. Previous studies (Ghader and Monz, 2017; Koehn and Knowles, 2017) have shown that attention mechanisms learn to pay attention to some unalign"
W18-6304,D15-1166,0,0.635015,"et al., 2015; Luong et al., 2015), each hidden state incorporates contextual information. Hence, NMT models could potentially perform WSD well. However, there are no empirical results to indicate that the hidden states encode the contextual information needed for disambiguation. Moreover, how the attention mechanism1 deals with ambiguous words is also not known yet. In this paper, we focus on the question of how encoder-decoder attention mechanisms deal with ambiguous nouns. We explore two different attention mechanisms. One is the vanilla one-layer attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), and the other one is the Transformer attention mechanism (Vaswani et al., 2017). Rios et al. (2017) find that attentional NMT models perform well in translating ambiguous words with frequent senses,2 while Liu et al. (2018) show that there are plenty of incorrect translations of ambiguous words. In Section 4, we evaluate the translations of ambiguous nouns, using the test set from Rios et al. (2017). In this setting, we expect to get a more accurate picture of the WSD performance of NMT models. In Section 5, we present a fine-grained investigation of attention distributions of different atte"
W18-6304,H05-1097,0,0.135158,"ract features from the related but unaligned context tokens. 1 Introduction Human languages exhibit many different types of ambiguity. Lexical ambiguity refers to the fact that words can have more than one semantic meaning. Dealing with these lexical ambiguities is a challenge for various NLP tasks. Word sense disambiguation (WSD) is recognizing the correct meaning of an ambiguous word, with the help of contextual information. In statistical machine translation (SMT) (Koehn et al., 2003), a system could explicitly take context tokens into account to improve the translation of ambiguous words (Vickrey et al., 2005). By con1 Denotes the encoder-decoder attention mechanism in this paper, unless otherwise specified. 2 More than 2,000 instances in the training set. 26 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 26–35 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64004 at different encoder layers in WSD, while we focus on exploring the attention mechanisms that connect the encoder and the decoder. Koehn and Knowles (2017) and Ghader and Monz (2017) investigate the relat"
W18-6304,W18-1812,0,0.119796,"onstrained WSD task. They create well-designed test sets to evaluate the performance of NMT models in distinguishing different senses of ambiguous words, rather than evaluating the translations of ambiguous words directly. By contrast, Liu et al. (2018) evaluate the translations of ambiguous words but on a common test set. Scoring the contrastive translations is not evaluating the real output of NMT models. In this paper, we directly evaluate the translations generated by NMT models, using ContraWSD as the test set. In NMT, the encoder may encode contextual information into the hidden states. Marvin and Koehn (2018) explore the ability of hidden states ct = αt h (1) where αt is the attention vector at time step t. αt is 27 Predictions Softmax Predictions Softmax Attention αt h ct Multi-head Attention ct st−1 αt h Decoder Networks Encoder hidden states st−1 Decoder Block st−1 n-layer blocks ct Decoder Block Encoder hidden states (a) Vanilla attention mechanism (b) Advanced attention mechanism Figure 1: Different attention mechanisms between encoders and decoders in NMT. erence translations are paired with one or more contrastive variants. Given an ambiguous word in the source sentence, the correct transla"
W18-6304,W17-4702,1,0.910664,"odels could potentially perform WSD well. However, there are no empirical results to indicate that the hidden states encode the contextual information needed for disambiguation. Moreover, how the attention mechanism1 deals with ambiguous words is also not known yet. In this paper, we focus on the question of how encoder-decoder attention mechanisms deal with ambiguous nouns. We explore two different attention mechanisms. One is the vanilla one-layer attention mechanism (Bahdanau et al., 2015; Luong et al., 2015), and the other one is the Transformer attention mechanism (Vaswani et al., 2017). Rios et al. (2017) find that attentional NMT models perform well in translating ambiguous words with frequent senses,2 while Liu et al. (2018) show that there are plenty of incorrect translations of ambiguous words. In Section 4, we evaluate the translations of ambiguous nouns, using the test set from Rios et al. (2017). In this setting, we expect to get a more accurate picture of the WSD performance of NMT models. In Section 5, we present a fine-grained investigation of attention distributions of different attention mechanisms. We focus on the process of translating the given ambiguous nouns. Previous studies"
W18-6304,P16-1162,1,0.577454,"ion and label smoothing (0.1) in all models. We tie the source and target embeddings. The dropout rate of embeddings and Transformer blocks is set to 0.1. The dropout rate of RNNs is 0.2. The attention mechanism in Transformer has 8 heads. We use the training data from the WMT17 shared task.6 We choose newstest2013 as the validation set, and use newstest2014 and newstest2017 as the test sets. All the BLEU scores are measured by SacreBLEU. There are about 5.9 million sentence pairs in the training set after preprocessing with Moses scripts. We learn a joint BPE model with 32,000 subword units (Sennrich et al., 2016). There are 6,330 sentences left after filtering the sentences with segmented ambiguous nouns. We employ the models that have the best perplexity on the validation set for the evaluation. √ √ √ √ √ Table 1: Different groups of translations. Ref. denotes the reference translations. Incor. represents the incorrect senses. No means that there is neither a√ correct nor an incorrect sense of the ambiguous noun. indicates that the translations belong to the reference translations or incorrect senses or neither. Since the alignment learnt by fast-align is not perfect, we also consider the translation"
W18-6307,N13-1073,0,0.0600439,"ronoun examples in the test suite by Bawden et al. (2018) require the previous target sentence for disambiguation, and thus do not reward models that condition on more than one sentence of context. 63 Alignment it→es it→sie it→er it→ist it→Sie it→, it→das it→dies it→wird it→man it→ihn Frequency Probability 255764 64446 44543 42614 26054 21037 17992 11943 11886 10539 7744 0.334 0.084 0.058 0.055 0.034 0.027 0.023 0.015 0.015 0.013 0.010 To provide a basis for filtering with constraints, we tokenize the whole data set with the Moses tokenizer, generate symmetric word alignments with fast_align (Dyer et al., 2013), parse the English text with CoreNLP (Manning et al., 2014), parse the German text with ParZu (Sennrich et al., 2013) and perform coreference resolution on both sides. The coreference chains are obtained with the neural model of CoreNLP for English, and with CorZu for German (Tuggener, 2016), respectively. Then we opt for high-precision, aggressive filtering, according to the following protocol: for each pair of sentences (e, f ) in English and German, extract iff Table 2: Frequency and probability of alignments of it in the training data of our systems (all data from the WMT 2017 news transl"
W18-6307,L16-1100,0,0.0569093,"between reference-based metrics and human judges, especially because there often exist valid alternative translations that use different pronouns than the reference. Our test set, and our protocol of generating contrastive examples, is focused on selected pronouns to minimize the risk of producing contrastive examples that are actually valid translations. Evaluation of Pronoun Translation Pronouns can serve a variety of functions with complex cross-lingual variation (Guillou, 2016), and hand-picked, manually annotated test suites have been presented for the evaluation of pronoun translation (Guillou and Hardmeier, 2016; Isabelle et al., 2017; Bawden et al., 2018). While suitable for analysis, the small size of the test suites makes it hard to make statistically confident comparisons between systems, and the hand-picked nature of the test suites introduces biases.3 To overcome these problems, we opted for a fully automatic approach to constructing a large-scale test suite. Conceptually, our test set is most similar to the “cross-lingual pronoun prediction” task held at DiscoMT and WMT in recent years (Hardmeier et al., 2015; Guillou et al., 2016; Loáiciga et al., 2017): participants are asked to fill a gap i"
W18-6307,W17-4801,0,0.0272592,"evaluation of pronoun translation (Guillou and Hardmeier, 2016; Isabelle et al., 2017; Bawden et al., 2018). While suitable for analysis, the small size of the test suites makes it hard to make statistically confident comparisons between systems, and the hand-picked nature of the test suites introduces biases.3 To overcome these problems, we opted for a fully automatic approach to constructing a large-scale test suite. Conceptually, our test set is most similar to the “cross-lingual pronoun prediction” task held at DiscoMT and WMT in recent years (Hardmeier et al., 2015; Guillou et al., 2016; Loáiciga et al., 2017): participants are asked to fill a gap in a target sentence, where gaps correspond to pronouns. The first edition of the task focused on English→French, and it was found that local context (such as the verb group) was a strong signal for pronoun prediction. Hence, future editions only provided target-side lemmas instead of fully inflected forms, which makes the task less suitable to evaluate end-to-end neural machine translation systems, although such systems have been trained on the task (Jean et al., 2017). Loáiciga et al. (2017) do not report on the proportion of intra-sentential and inter-"
W18-6307,D18-1513,0,0.0349608,"naphora with a long distance between the pronoun and its antecedent. Our focus on evaluating end-to-end NMT systems also relieves us from having to provide annotated training sets, and reduces pressure to achieve balance and full coverage of phenomena.4 An alternative approach to automatically evaluate pronoun translation are reference-based methods that produce a score based on word alignment between source, translation output, and reference translation, and identification of pronouns in them, such as AutoPRF (Hardmeier and Federico, 2010) and APT (Miculicich Werlen and Popescu-Belis, 2017). Guillou and Hardmeier (2018) perform a human meta-evaluation and show substantial disagreement between reference-based metrics and human judges, especially because there often exist valid alternative translations that use different pronouns than the reference. Our test set, and our protocol of generating contrastive examples, is focused on selected pronouns to minimize the risk of producing contrastive examples that are actually valid translations. Evaluation of Pronoun Translation Pronouns can serve a variety of functions with complex cross-lingual variation (Guillou, 2016), and hand-picked, manually annotated test suit"
W18-6307,W16-2345,0,0.0211813,"een presented for the evaluation of pronoun translation (Guillou and Hardmeier, 2016; Isabelle et al., 2017; Bawden et al., 2018). While suitable for analysis, the small size of the test suites makes it hard to make statistically confident comparisons between systems, and the hand-picked nature of the test suites introduces biases.3 To overcome these problems, we opted for a fully automatic approach to constructing a large-scale test suite. Conceptually, our test set is most similar to the “cross-lingual pronoun prediction” task held at DiscoMT and WMT in recent years (Hardmeier et al., 2015; Guillou et al., 2016; Loáiciga et al., 2017): participants are asked to fill a gap in a target sentence, where gaps correspond to pronouns. The first edition of the task focused on English→French, and it was found that local context (such as the verb group) was a strong signal for pronoun prediction. Hence, future editions only provided target-side lemmas instead of fully inflected forms, which makes the task less suitable to evaluate end-to-end neural machine translation systems, although such systems have been trained on the task (Jean et al., 2017). Loáiciga et al. (2017) do not report on the proportion of int"
W18-6307,P14-5010,0,0.00360193,"require the previous target sentence for disambiguation, and thus do not reward models that condition on more than one sentence of context. 63 Alignment it→es it→sie it→er it→ist it→Sie it→, it→das it→dies it→wird it→man it→ihn Frequency Probability 255764 64446 44543 42614 26054 21037 17992 11943 11886 10539 7744 0.334 0.084 0.058 0.055 0.034 0.027 0.023 0.015 0.015 0.013 0.010 To provide a basis for filtering with constraints, we tokenize the whole data set with the Moses tokenizer, generate symmetric word alignments with fast_align (Dyer et al., 2013), parse the English text with CoreNLP (Manning et al., 2014), parse the German text with ParZu (Sennrich et al., 2013) and perform coreference resolution on both sides. The coreference chains are obtained with the neural model of CoreNLP for English, and with CorZu for German (Tuggener, 2016), respectively. Then we opt for high-precision, aggressive filtering, according to the following protocol: for each pair of sentences (e, f ) in English and German, extract iff Table 2: Frequency and probability of alignments of it in the training data of our systems (all data from the WMT 2017 news translation task). Alignments are produced by a fast_align model."
W18-6307,2010.iwslt-papers.10,0,0.0776322,"nter-sentential anaphora, with meta-data allowing for a focus on inter-sentential anaphora with a long distance between the pronoun and its antecedent. Our focus on evaluating end-to-end NMT systems also relieves us from having to provide annotated training sets, and reduces pressure to achieve balance and full coverage of phenomena.4 An alternative approach to automatically evaluate pronoun translation are reference-based methods that produce a score based on word alignment between source, translation output, and reference translation, and identification of pronouns in them, such as AutoPRF (Hardmeier and Federico, 2010) and APT (Miculicich Werlen and Popescu-Belis, 2017). Guillou and Hardmeier (2018) perform a human meta-evaluation and show substantial disagreement between reference-based metrics and human judges, especially because there often exist valid alternative translations that use different pronouns than the reference. Our test set, and our protocol of generating contrastive examples, is focused on selected pronouns to minimize the risk of producing contrastive examples that are actually valid translations. Evaluation of Pronoun Translation Pronouns can serve a variety of functions with complex cros"
W18-6307,P18-1118,0,0.280184,"Voita et al., 2018). One aspect that differs between these works is the architecture of the encoder and attention. While Jean et al. (2017); Bawden et al. (2018) extend an RNN encoder-decoder with a second encoder that the decoder attends to, Voita et al. (2018) extend the Transformer architecture with an encoder that is attended to by the main encoder. Voita et al. (2018) also introduce parameter sharing between the main encoder and the context encoder, but do not empirically demonstrate its importance. While the number of encoded sentences in the previous work is fixed, Wang et al. (2017); Maruf and Haffari (2018) explore the integration of variable-size context through a hierarchical architecture, where a first-level RNN reads in words to produce sentence vectors, which are then fed into a second-level RNN to produce a document summary. Apart from differences in the architectures, related work varies in whether it considers source context, target context, or both (see Table 1 for an overview of language arcs and context types). Some work considers only source context, but for pronoun translation, target-side context is intuitively important for disambiguation, especially if the antecedent itself is am"
W18-6307,W15-2501,0,0.0501115,"tated test suites have been presented for the evaluation of pronoun translation (Guillou and Hardmeier, 2016; Isabelle et al., 2017; Bawden et al., 2018). While suitable for analysis, the small size of the test suites makes it hard to make statistically confident comparisons between systems, and the hand-picked nature of the test suites introduces biases.3 To overcome these problems, we opted for a fully automatic approach to constructing a large-scale test suite. Conceptually, our test set is most similar to the “cross-lingual pronoun prediction” task held at DiscoMT and WMT in recent years (Hardmeier et al., 2015; Guillou et al., 2016; Loáiciga et al., 2017): participants are asked to fill a gap in a target sentence, where gaps correspond to pronouns. The first edition of the task focused on English→French, and it was found that local context (such as the verb group) was a strong signal for pronoun prediction. Hence, future editions only provided target-side lemmas instead of fully inflected forms, which makes the task less suitable to evaluate end-to-end neural machine translation systems, although such systems have been trained on the task (Jean et al., 2017). Loáiciga et al. (2017) do not report on"
W18-6307,W17-4802,0,0.03596,"for a focus on inter-sentential anaphora with a long distance between the pronoun and its antecedent. Our focus on evaluating end-to-end NMT systems also relieves us from having to provide annotated training sets, and reduces pressure to achieve balance and full coverage of phenomena.4 An alternative approach to automatically evaluate pronoun translation are reference-based methods that produce a score based on word alignment between source, translation output, and reference translation, and identification of pronouns in them, such as AutoPRF (Hardmeier and Federico, 2010) and APT (Miculicich Werlen and Popescu-Belis, 2017). Guillou and Hardmeier (2018) perform a human meta-evaluation and show substantial disagreement between reference-based metrics and human judges, especially because there often exist valid alternative translations that use different pronouns than the reference. Our test set, and our protocol of generating contrastive examples, is focused on selected pronouns to minimize the risk of producing contrastive examples that are actually valid translations. Evaluation of Pronoun Translation Pronouns can serve a variety of functions with complex cross-lingual variation (Guillou, 2016), and hand-picked"
W18-6307,E17-3017,1,0.913607,"Central Bank. When the second sentence is translated from English to German, the translation of the pronoun it is ambiguous. This ambiguity can only be resolved with context awareness: if a translation system has access to the previous English sentence, the previous German translation, or both, it can determine the antecedent the pronoun refers to. In this German sentence, the antecedent Europäische Zentralbank dictates the feminine gender of the pronoun sie. It is unfortunate, then, that current NMT systems generally operate on the sentence level (Vaswani et al., 2017; Gehring et al., 2017; Hieber et al., 2017). Documents are translated sentenceby-sentence for practical reasons, such as linebased processing in a pipeline and reduced computational complexity. Furthermore, improvements of larger-context models over baselines in terms of document-level metrics such as BLEU or RIBES have been moderate, so that their computational overhead does not seem justified, and so that it is hard to develop more effective context-aware architectures and empirically validate them. Introduction Even though machine translation has improved considerably with the advent of neural machine translation (NMT) (Sutskever et"
W18-6307,W18-6319,0,0.0174257,"catenated) or 50 (if input lines are single sentences). For our Transformer-based experiments, we use a custom implementation and follow the hyperparameters from Vaswani et al. (2017); Voita et al. (2018). Systems are trained on lowercased text that was encoded using BPE (32k merge operations). Models consist of 6 encoder and decoder layers with 8 attention heads. The hidden state size is 512, the size of feedforward layers is 2048. Model performance is evaluated in terms of BLEU, on newstest2017, newstest2018 and all sentence pairs from our pronoun test set. We compute scores with SacreBLEU (Post, 2018).9 Evaluation with BLEU is done mainly to control for overall translation quality. To evaluate pronoun translation, we perform contrastive evaluation and report the accuracy of models on our contrastive test set. 6 with antecedent distance 1 benefits most from the tested context-aware models (up to +20 percentage points accuracy). However, we note two surprising patterns: • For inter-sentential anaphora, the performance of all systems, including the baseline, improves with increasing antecedent distance. • Context-aware systems that consider one preceding sentence also improve on intrasententi"
W18-6307,D17-1263,0,0.071976,"Missing"
W18-6307,W17-4806,0,0.25993,"that precedes it, for the source side of the corpus or both sides. All of their models are standard sequence-to-sequence models built with recurrent neural networks (RNNs), since the method does not require any architectural change. Agrawal et al. (2018) use the same concatenation technique with a Transformer architecture (Vaswani et al., 2017), and experiment with wider context. A number of works do propose changes to the NMT architecture. A common technique is to extend a standard encoder-decoder model by additional encoders for the context sentence(s), with a modified attention mechanism (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). One aspect that differs between these works is the architecture of the encoder and attention. While Jean et al. (2017); Bawden et al. (2018) extend an RNN encoder-decoder with a second encoder that the decoder attends to, Voita et al. (2018) extend the Transformer architecture with an encoder that is attended to by the main encoder. Voita et al. (2018) also introduce parameter sharing between the main encoder and the context encoder, but do not empirically demonstrate its importance. While the number of encoded sentences in the previous work is fixed"
W18-6307,P16-1162,1,0.490891,"on. 5 Experiments We train all models on the data from the WMT 2017 English→German news translation shared task (∼ 5.8 million sentence pairs). These corpora do not have document boundaries, therefore a small fraction of sentences will be paired with wrong context, but we expect the model to be robust against occasional random context (see also Voita et al. 2018). Experimental setups for the RNN and Transformer models are different, and we describe them separately. 66 All RNN-based models are trained with Nematus (Sennrich et al., 2017). We learn a joint BPE model with 89.5k merge operations (Sennrich et al., 2016). We train shallow models with an embedding size of 512, a hidden layer size of 1024 and layer normalization. Models are trained with Adam (Kingma and Ba, 2015), with an initial learning rate of 0.0001. We apply early stopping based on validation perplexity. The batch size for training is 80, and the maximum length of training sequences is 100 (if input sentences are concatenated) or 50 (if input lines are single sentences). For our Transformer-based experiments, we use a custom implementation and follow the hyperparameters from Vaswani et al. (2017); Voita et al. (2018). Systems are trained o"
W18-6307,R13-1079,1,0.751252,"and thus do not reward models that condition on more than one sentence of context. 63 Alignment it→es it→sie it→er it→ist it→Sie it→, it→das it→dies it→wird it→man it→ihn Frequency Probability 255764 64446 44543 42614 26054 21037 17992 11943 11886 10539 7744 0.334 0.084 0.058 0.055 0.034 0.027 0.023 0.015 0.015 0.013 0.010 To provide a basis for filtering with constraints, we tokenize the whole data set with the Moses tokenizer, generate symmetric word alignments with fast_align (Dyer et al., 2013), parse the English text with CoreNLP (Manning et al., 2014), parse the German text with ParZu (Sennrich et al., 2013) and perform coreference resolution on both sides. The coreference chains are obtained with the neural model of CoreNLP for English, and with CorZu for German (Tuggener, 2016), respectively. Then we opt for high-precision, aggressive filtering, according to the following protocol: for each pair of sentences (e, f ) in English and German, extract iff Table 2: Frequency and probability of alignments of it in the training data of our systems (all data from the WMT 2017 news translation task). Alignments are produced by a fast_align model. • e contains the English pronoun it, and f contains a Germ"
W18-6307,L16-1147,0,0.0715632,"Missing"
W18-6307,W17-4811,0,0.34516,"capability to correctly translate pronouns. The test suite consists of pairs of source and target sentences, in combination with contrastive translation variants (for evaluation by model scoring) and additional linguistic and contextual information (for further analysis). The resource is freely available.1 Additionally, we evaluate several context-aware models that have recently been proposed in the literature on this test set, and extend existing models with parameter tying. The main contributions of our paper are: The simplest possible extension is to translate units larger than sentences. Tiedemann and Scherrer (2017) concatenate each sentence with the sentence that precedes it, for the source side of the corpus or both sides. All of their models are standard sequence-to-sequence models built with recurrent neural networks (RNNs), since the method does not require any architectural change. Agrawal et al. (2018) use the same concatenation technique with a Transformer architecture (Vaswani et al., 2017), and experiment with wider context. A number of works do propose changes to the NMT architecture. A common technique is to extend a standard encoder-decoder model by additional encoders for the context senten"
W18-6307,P18-1117,1,0.423931,"f the corpus or both sides. All of their models are standard sequence-to-sequence models built with recurrent neural networks (RNNs), since the method does not require any architectural change. Agrawal et al. (2018) use the same concatenation technique with a Transformer architecture (Vaswani et al., 2017), and experiment with wider context. A number of works do propose changes to the NMT architecture. A common technique is to extend a standard encoder-decoder model by additional encoders for the context sentence(s), with a modified attention mechanism (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). One aspect that differs between these works is the architecture of the encoder and attention. While Jean et al. (2017); Bawden et al. (2018) extend an RNN encoder-decoder with a second encoder that the decoder attends to, Voita et al. (2018) extend the Transformer architecture with an encoder that is attended to by the main encoder. Voita et al. (2018) also introduce parameter sharing between the main encoder and the context encoder, but do not empirically demonstrate its importance. While the number of encoded sentences in the previous work is fixed, Wang et al. (2017); Maruf and Haffari (2"
W18-6307,D17-1301,0,0.212868,"Bawden et al., 2018; Voita et al., 2018). One aspect that differs between these works is the architecture of the encoder and attention. While Jean et al. (2017); Bawden et al. (2018) extend an RNN encoder-decoder with a second encoder that the decoder attends to, Voita et al. (2018) extend the Transformer architecture with an encoder that is attended to by the main encoder. Voita et al. (2018) also introduce parameter sharing between the main encoder and the context encoder, but do not empirically demonstrate its importance. While the number of encoded sentences in the previous work is fixed, Wang et al. (2017); Maruf and Haffari (2018) explore the integration of variable-size context through a hierarchical architecture, where a first-level RNN reads in words to produce sentence vectors, which are then fed into a second-level RNN to produce a document summary. Apart from differences in the architectures, related work varies in whether it considers source context, target context, or both (see Table 1 for an overview of language arcs and context types). Some work considers only source context, but for pronoun translation, target-side context is intuitively important for disambiguation, especially if t"
W18-6412,W18-6401,1,0.759353,"airs (both directions). For these experiments, we use the same training sets and data preparation as in our system submissions, but train the deep RNNs with a working memory of 10GB, validating every 1,000 steps, and testing for convergence with a patience of 10. We use exponential smoothing and show the results on a single smoothed model. From the results in Table 7 we see that the multihead/hop extension has a small positive effect on B LEU in most language pairs. 27.6 28.2 27.7 28.1 26.9 Table 5: Results for EN↔TR systems on official WMT test sets. human evaluation from the findings paper (Bojar et al., 2018). In terms of the clustering provided by the organisers, we were in the top constrained cluster (i.e. no significant difference was observed between ours and the best constrained system) for EN→CS, DE→EN, ET→EN, FI→EN, TR→EN and EN→TR, i.e. 6/14 language pairs. Nevertheless, Table 6 still shows that our systems generally lag behind the best submitted systems. This is contrast to the 2017 shared task, where we achieved the highest scores in most of the language pairs where we submitted systems. We hypothesise that other groups have taken fuller advantage of the transformer architecture, and als"
W18-6412,P18-1008,0,0.0363778,"with the raw data. 2.3 Model Architecture For this submission we considered two types of sequence-to-sequence architectures: a transformer (Vaswani et al., 2017) and a deep RNN, specifically the BiDeep GRU encoder-decoder (Miceli Bar400 one et al., 2017). Both architectures4 are implemented in the Marian open source neural machine translation framework (Junczys-Dowmunt et al., 2018). For the transformer architecture we used the “wmt2017-transformer” setup from the Marian example collection5 . We extended the RNN with with multi-head and multi-hop attention. Multi-head attention is similar to Chen et al. (2018), with an MLP attention mechanism using a single tanh hidden layer followed by one soft-max layer for each attention heads. We further include an optional projection layer on the attended context with layer normalisation in order to avoid increasing the total size of the attended context. Let C ∈ RNs ×de be the input sentence representation produced by the encoder, where Ns is the source sentence length and de is the top-level bidirectional encoder state dimension. Let s ∈ Rdd be an internal decoder state at some step. Then for source sentence position i we compute a vector of M attention weig"
W18-6412,W17-4715,1,0.879995,"experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data selection and weighting For some language pairs, we experimented with different data selection schemes, motivated by the introduction of the noisy ParaCrawl corpora to the task (Section 2.1). We also applied weighting of different corpora to most language pairs, particularly DE↔EN (Section 3.5). Extensions to Back-translation For TR↔EN (Section 3.7) we used copied monolingual data (Currey et al., 2017a) and iterative back-translation. System Details 2.1 Data and Selection All our systems were constrained in the sense that they only used the supplied parallel data (including ParaCrawl) for training the systems. We also used the monolingual news crawls to create extra synthetic parallel data by back-translation, for all language pairs, and by copying monolingual data for TR↔EN. During training we generally used newsdev2016 or newstest2016 for validation, and newstest2017 for development testing (i.e. model selection), except for ZH↔EN, and ET↔EN, where we used the recent newsdev sets instead"
W18-6412,N13-1073,0,0.0722204,"Missing"
W18-6412,W16-2323,1,0.938501,"ntroduce new RNN-variant, mixed RNN/Transformer ensembles, data selection and weighting, and extensions to backtranslation. 1 2 In this section we describe the general properties of our systems, as well as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data"
W18-6412,P07-2045,0,0.00708596,"e of the ParaCrawl corpus from about 36 million sentence pairs to ca. 18 million sentence pairs. 10 5 0 0.0 0.2 0.4 0.6 Proportion of ParaCrawl 0.8 1.0 Figure 1: Result of translation perplexity filtering of ParaCrawl on 2 language pairs we monitored the performance on a validation set (newstest2016) and observed the point where translation quality started to deteriorate. We used the translation plausibility score at this point as the threshold for selecting data for training the final systems. 2.2 Preprocessing For most language pairs, our preprocessing setup consisted of the Moses pipeline (Koehn et al., 2007) of normalisation, tokenisation and truecasing, followed by byte-pair encoding (BPE) (Sennrich et al., 2016c). We generally applied joint BPE, with the number of merge operations set on a per-pair basis, detailed in Section 3. Different pipelines were used for processing the two languages written in non-Latin scripts (i.e. Chinese and Russian), also explained in Section 3. For some language pairs (those including Czech, Estonian, Finnish and German) we used the preprocessed data provided by the organisers (which is preprocessed up to truecasing), whilst for the others we started with the raw d"
W18-6412,P12-3005,0,0.0282872,"id identified a significant proportion of the data as these other two Slavic languages, but on inspecting a sample, they were found nearly always to be Czech. The issue with langid is that we just give it the text, without providing any prior knowledge, when in actual fact there is a strong prior that CzEng sentences are really Czech and English, by construction 25 fi-en et-en 20 15 Bleu Language Identifier Filtering This was applied to the CS↔EN and DE↔EN corpora, based on observations that CzEng, and ParaCrawl both contain sentence pairs in the “wrong” language. For CS↔EN we applied langid (Lui and Baldwin, 2012) to both sids of the data, removing any sentences whose English side is not labelled as English, or whose Czech is not labelled as Czech, Slovak or Slovenian3 . For DE↔EN, we just applied langid to ParaCrawl and retained only those pairs where each side was identified as the ‘correct’ language by langid. This reduced the size of the ParaCrawl corpus from about 36 million sentence pairs to ca. 18 million sentence pairs. 10 5 0 0.0 0.2 0.4 0.6 Proportion of ParaCrawl 0.8 1.0 Figure 1: Result of translation perplexity filtering of ParaCrawl on 2 language pairs we monitored the performance on a va"
W18-6412,W17-4710,1,0.858373,"ll as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data selection and weighting For some language pairs, we experimented with different data selection schemes, motivated by the introduction of the noisy ParaCrawl corpora to the task (Section 2.1). We also"
W18-6412,P10-2041,0,0.0719435,"Missing"
W18-6412,W17-4739,1,0.864199,"3 times the tokens on the other side, following Hassan et al. (2018). After preprocessing the corpus size was 23.6M sentences. We then applied BPE using 18,000 merge operations and we used the top 18,000 BPE segments as vocabulary. We augmented our data with backtranslated 4 The BiDeep GRU is obtainable using the -best-deep option. 5 https://github.com/marian-nmt/ marian-examples 401 6 The implementation of the multi-head and multi-hop attention architectures is available at: https://github. com/EdinburghNLP/marian-dev 7 https://marian-nmt.github.io 8 https://github.com/fxsjy/jieba ZH↔EN from Sennrich et al. (2017), which consists of 8.6M sentences for EN→ZH and 19.7M for ZH→EN. We trained using the BiDeep architecture with multi-head attention with 1 hop and 3 heads. We decoded using an ensemble of 5 L2R systems and a beam of 12 for EN→ZH and 6 L2R systems and a beam of 12 for ZH→EN. Due to time constraints, we were not able to train any of the systems to convergence. 3.2 Czech ↔ English After preprocessing, language filtering (see Sections 2.1 and 2.2), and removing any parallel sentences where neither side contains an ASCII letter, we were left with around 50M sentence pairs. We then learned a joint"
W18-6412,P16-1162,1,0.881886,"ntroduce new RNN-variant, mixed RNN/Transformer ensembles, data selection and weighting, and extensions to backtranslation. 1 2 In this section we describe the general properties of our systems, as well as some novel approaches that we tried this year such as data selection and a variant on the GRU-based RNN architecture. The specifics of our submissions for each language pair are described in Section 3. Introduction For the WMT18 news translation task, we were the only team to make submissions to all 14 language pairs. Our submissions built on our strong results of the WMT16 and WMT17 tasks (Sennrich et al., 2016a, 2017), in that we used neural machine translation (NMT) with byte-pair encoding (BPE) (Sennrich et al., 2016c), back-translation (Sennrich et al., 2016b) and deep RNNs (Miceli Barone et al., 2017). For this year’s submissions we experimented with new architectures, and new ways of data handling. In brief, the innovations that we introduced this year are: Architecture This year we experimented with the Transformer architecture (Vaswani et al., 2017), as implemented by Marian (Junczys-Dowmunt et al., 2018), as well as introducing a new variant on the deep RNN architectire (Section 2.3). Data"
W18-6437,D16-1025,0,0.0599861,"Missing"
W18-6437,P02-1040,0,0.108976,"Missing"
W18-6437,W17-4702,1,0.920865,"ample, in the fine-grained human evaluation by Klubiˇcka et al. (2017), mistranslations were the most frequent error category for the NMT system they evaluated, whereas fluency errors dominated in phrase-based machine translation.1 Our aim is to quantify one aspect of adequacy, word sense disambiguation (WSD), in a reproducible and semi-automatic way, to track progress over time and compare different types of systems in this respect. We present a German→English test set to semiautomatically assess an MT systems performance on word sense disambiguation. The test set is based on ContraWSD (Rios Gonzales et al., 2017), but has been further filtered to reduce noise, and we use a different evaluation protocol. Instead of scoring a set of translations and measuring whether the reference translation is scored highest, we base the evaluation on the 1-best translation output to make the evaluation applicable to black-box systems. We report results on all German→English submissions to the WMT 2018 shared translation task (Bojar et al., 2018), plus a number of baseline systems from previous years. We present a task to measure an MT system’s capability to translate ambiguous words with their correct sense according"
W18-6437,W16-2323,1,0.839545,"al., 2017). Interestingly, we observe major improvements in lexical choice since the 2016 systems, with a jump of 5 percentage points in 2017, and another 8 percentage points by the best system in 2018. While these experiments were not under controlled data conditions10 , we believe that this imEvaluation We present results for all submissions to the WMT’18 shared translation task for German→English. In addition, we include several baseline systems in our evaluation to track performance over time. We report results for Edinburgh’s WMT’16 and WMT’17 submitted neural systems for German→English (Sennrich et al., 2016, 2017), which were ranked first in 2016, and tied first in 2017.9 We also include Edinburgh’s WMT’16 syntax-based system (Williams et al., 2016), ranked tied second in 2016, to compare the now dominant neural systems to a more traditional SMT system. We report the WSD accuracy for each system, in two variants: automatic and full. For automatic accuracy only case 1 is considered correct, and cases 2–4 are considered wrong. Full accuracy considers some cases 3 and 4 (where both a correct and an incorrect translation, or none of the listed translations, are found) correct, if they were found to"
W18-6437,D18-1458,1,0.899208,"Missing"
W18-6437,tiedemann-2012-parallel,0,0.0392549,"Missing"
W18-6437,L18-1215,0,0.0311105,"x-based statistical models, but results are more mixed when comparing ade2 Test Suite Rather than measuring word sense disambiguation against a manually defined sense inventory such as those in Wordnet (Miller, 1995), we perform a task-based evaluation, focusing on homonyms whose different senses have distinct translations.2 1 Note that, while mistranslations were the most frequent error category in NMT, their absolute number was still lower in the NMT system output than in the phrase-based one. 2 Other task-based evaluation sets for word sense disambiguation include (Lefever and Hoste, 2013; Gorman et al., 2018). 588 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 588–596 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64064 The collection of test cases consists of 3249 German–English sentence pairs where the German source contains one of 20 ambiguous words that have more than one possible translation in English.3 We have associated the 20 ambiguous words with a total of 45 word senses, and extracted up to 100 examples for each sense. The set of ambiguous words and"
W18-6437,P16-1008,0,0.033847,"Missing"
W18-6437,S13-2029,0,0.144776,"minant phrase-based/syntax-based statistical models, but results are more mixed when comparing ade2 Test Suite Rather than measuring word sense disambiguation against a manually defined sense inventory such as those in Wordnet (Miller, 1995), we perform a task-based evaluation, focusing on homonyms whose different senses have distinct translations.2 1 Note that, while mistranslations were the most frequent error category in NMT, their absolute number was still lower in the NMT system output than in the phrase-based one. 2 Other task-based evaluation sets for word sense disambiguation include (Lefever and Hoste, 2013; Gorman et al., 2018). 588 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 588–596 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64064 The collection of test cases consists of 3249 German–English sentence pairs where the German source contains one of 20 ambiguous words that have more than one possible translation in English.3 We have associated the 20 ambiguous words with a total of 45 word senses, and extracted up to 100 examples for each sense. The set"
W18-6437,W16-2327,1,0.845832,"nd another 8 percentage points by the best system in 2018. While these experiments were not under controlled data conditions10 , we believe that this imEvaluation We present results for all submissions to the WMT’18 shared translation task for German→English. In addition, we include several baseline systems in our evaluation to track performance over time. We report results for Edinburgh’s WMT’16 and WMT’17 submitted neural systems for German→English (Sennrich et al., 2016, 2017), which were ranked first in 2016, and tied first in 2017.9 We also include Edinburgh’s WMT’16 syntax-based system (Williams et al., 2016), ranked tied second in 2016, to compare the now dominant neural systems to a more traditional SMT system. We report the WSD accuracy for each system, in two variants: automatic and full. For automatic accuracy only case 1 is considered correct, and cases 2–4 are considered wrong. Full accuracy considers some cases 3 and 4 (where both a correct and an incorrect translation, or none of the listed translations, are found) correct, if they were found to be correct upon manual inspection. We also report BLEU scores on newstest2018, and on the WSD test suite, for comparison. 4 Results Results on th"
W18-6437,W15-3024,1,0.847633,"Missing"
W18-6437,L16-1561,0,0.0409419,"Missing"
W19-5211,W18-6412,1,0.90039,"Missing"
W19-5211,D18-1338,0,0.0449376,"Missing"
W19-5211,P17-1080,0,0.101257,"ine Translation with Lexical Shortcuts Denis Emelin1 , Ivan Titov1, 2 , and Rico Sennrich1, 3 1 University of Edinburgh, Scotland University of Amsterdam, Netherlands 3 University of Zurich, Switzerland D.Emelin@sms.ed.ac.uk ititov@inf.ed.ac.uk rico.sennrich@ed.ac.uk 2 Abstract networks (Chen et al., 2018), superior ability to perform lexical disambiguation, and capacity for capturing long-distance dependencies on par with existing alternatives (Tang et al., 2018). Recently, several studies have investigated the nature of features encoded within individual layers of neural translation models (Belinkov et al., 2017, 2018). One central finding reported in this body of work is that, in recurrent architectures, different layers prioritize different information types. As such, lower layers appear to predominantly perform morphological and syntactic processing, whereas semantic features reach their highest concentration towards the top of the layer stack. One necessary consequence of this distributed learning is that different types of information encoded within input representations received by the translation model have to be transported to the layers specialized in exploiting them. Within the transformer"
W19-5211,D18-1458,1,0.887688,"Missing"
W19-5211,W18-6319,0,0.0134713,"et al., 2017), the addition of skip connections between individual layers of a deep neural network results in an implicit ‘deep supervision’ effect (Lee et al., 2015), which aids the training process. In case of our modified transformer, this corresponds to the embedding layer receiving its learning signal from the model’s overall optimization objective as well as from each layer it is connected to, making the model easier to train. 3 3.1 3.3 The results of our translation experiments are summarized in Tables 1-2. To ensure their comparability, we evaluate translation quality using sacreBLEU (Post, 2018). As such, our baseline performance diverges from that reported in (Vaswani et al., 2017). We address this by evaluating our EN→DE models using the scoring script from the tensor2tensor toolkit3 (Vaswani et al., 2018) on the tokenized model output, and list the corresponding BLEU scores in the first column of Table 1. Our evaluation shows that the introduction of lexical shortcuts consistently improves translation quality of the transformer model across different test-sets and language pairs, outperforming transformer-BASE by 0.5 BLEU on average. With feature-fusion, we see even stronger impro"
W19-5211,D16-1079,0,0.0606861,"Missing"
W19-5211,W17-4702,1,0.799236,"Missing"
W19-5211,P18-2051,0,0.0440579,"Missing"
W19-5211,D16-1159,0,0.125732,"Missing"
W19-5211,W18-6401,0,\N,Missing
W19-5211,I17-1001,0,\N,Missing
W19-5211,P16-1162,1,\N,Missing
W19-5211,P18-1164,0,\N,Missing
