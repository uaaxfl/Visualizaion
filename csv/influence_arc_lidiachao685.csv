2013.mtsummit-posters.3,2001.mtsummit-papers.3,0,0.108074,"), position independent word error rate PER (Tillmann et al., 1997) (variant of WER that disregards word ordering), BLEU (Papineni et al., 2002) (the geometric mean of n-gram precision by the system output with respect to reference translations), NIST (Doddington, 2002) (adding the information weight) and GTM (Turian et al., 2003). Recently, many other methods were proposed to revise or improve the previous works. One of the categories is the lexical similarity based metric. The metrics of this kind include the edit distance based method, such as the TER (Snover et al., 2006) and the work of (Akiba et al., 2001) in addition to WER and PER, the precision based method such as SIA (Liu and Gildea, 2006) in addition to BLEU and NIST, recall based method such as ROUGE (Lin and Hovy, 2003), the word order information utilized by (Wong and Kit, 2008), (Isozaki et al., 2010) and (Talbot et al., 2011), and the combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between"
2013.mtsummit-posters.3,W11-2104,0,0.0177607,"of each factor, for instance, the word position could be free in some languages but strictly constrained in other languages. In practice, these employed features by hLEP ORE are also the vital ones when people facilitate language translation. This is the philology behind the formulation and the study of this work, and we believe 219 human’s translation ideology is the exact direction that MT systems should try to approach. Furthermore, this work specifies that different external resources or linguistic information could be integrated into this model easily. As suggested by other works, e.g. (Avramidis et al., 2011), the POS information is considered in the experiments and shows some improvements on certain language pairs. There are several main contributions of this paper compared with our previous work (Han et al., 2013). This work combines the utilizing of surface words and linguistic features together (instead of relying on the consilience of the POS sequence only). This paper measures the systemlevel hLEP OR score by the arithmetical mean of each sentence-level score (instead of the Harmonic mean of system-level internal factors). This paper shows the performances of enhanced method hLEP ORE on all"
2013.mtsummit-posters.3,P08-1007,0,0.0968294,"version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this kind include the syntactic similarity such as the Part-of-Speech information used by ROSE (Song and Cohn, 2011) and MPF (Popovic, 2011), and phrase information employed by (Echizen-ya and Araki, 2010) and (Han et al., 2013b); the semantic similarity such as Textual entailment used by (Mirkin et al., 2009), Synonyms by (Chan and Ng, 2008), paraphrase by (Snover et al., 2009). The evaluation methods proposed previously suffer from several main weaknesses more or less: perform well in certain language pairs but weak on others, which we call the language-bias problem; consider no linguistic information (not reasonable from the aspect of linguistic analysis) or too many linguistic features (making it difficult in replicability), which we call the extremism problem; present incomprehensive factors (e.g. BLEU focus on precision only). To address these problems, a novel automatic evaluation metric is proposed in this paper with enhan"
2013.mtsummit-posters.3,P12-1098,0,0.188008,"Gildea, 2006) in addition to BLEU and NIST, recall based method such as ROUGE (Lin and Hovy, 2003), the word order information utilized by (Wong and Kit, 2008), (Isozaki et al., 2010) and (Talbot et al., 2011), and the combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this kind include the syntactic similarity such as the Part-of-Speech information used by ROSE (Song and Cohn, 2011) and MPF (Popovic, 2011), and phrase information employed by (Echizen-ya and Araki, 2010) and (Han et al., 2013b); the semantic similarity such as Textual entailment used by (Mirkin et al., 2009), Synonyms by (Chan and Ng, 2008), paraphrase by (Snover et al., 2009). The evaluation methods proposed previously suffer from several main weaknesses more or less: perform well in certain language pairs but weak o"
2013.mtsummit-posters.3,W02-1001,0,0.0209873,"ation SIGMT (WMT workshop) which contain eight corpora including English-to-other (Spanish, Czech, French and German) and other-to-English. There are indeed a lot of linguistic POS tagger tools for different languages available. We conduct an evaluation with different POS taggers, and find that the employing of POS information can make an increase of the correlation score with human judgment for some language pairs but little or no effect on others. The employed POS tagging tools include Berkeley POS tagger for French, English and German (Petrov et al., 2006), COMPOST Czech morphology tagger (Collins, 2002) and TreeTagger Spanish tagger (Schmid, 1994). To avoid the overfitting problem, the WMT 20081 data are used in the development stage for the tuning of the parameters and the WMT 2011 corpora are used in testing. The tuned parameter values for different language pairs are shown in Table 1. The abbreviations EN, CZ, DE, ES and FR mean English, Czech, German, Spanish and French respectively. In the ngram word (POS) alignment, bigram is selected in all the language pairs. To make the model concise using as fewer of external resources as possible, the value of “N/A” means the POS information of th"
2013.mtsummit-posters.3,W11-2107,0,0.0217796,"posed to revise or improve the previous works. One of the categories is the lexical similarity based metric. The metrics of this kind include the edit distance based method, such as the TER (Snover et al., 2006) and the work of (Akiba et al., 2001) in addition to WER and PER, the precision based method such as SIA (Liu and Gildea, 2006) in addition to BLEU and NIST, recall based method such as ROUGE (Lin and Hovy, 2003), the word order information utilized by (Wong and Kit, 2008), (Isozaki et al., 2010) and (Talbot et al., 2011), and the combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this kind include the syntactic similarity such as the Part-of-Speech information used by ROSE (Song and Cohn, 2011) and MPF (Popovic, 2011), and phrase information employed by (Echizen-ya and Araki, 2010) and"
2013.mtsummit-posters.3,P10-1012,0,0.218458,"1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this kind include the syntactic similarity such as the Part-of-Speech information used by ROSE (Song and Cohn, 2011) and MPF (Popovic, 2011), and phrase information employed by (Echizen-ya and Araki, 2010) and (Han et al., 2013b); the semantic similarity such as Textual entailment used by (Mirkin et al., 2009), Synonyms by (Chan and Ng, 2008), paraphrase by (Snover et al., 2009). The evaluation methods proposed previously suffer from several main weaknesses more or less: perform well in certain language pairs but weak on others, which we call the language-bias problem; consider no linguistic information (not reasonable from the aspect of linguistic analysis) or too many linguistic features (making it difficult in replicability), which we call the extremism problem; present incomprehensive facto"
2013.mtsummit-posters.3,C12-2044,1,0.646962,"method such as SIA (Liu and Gildea, 2006) in addition to BLEU and NIST, recall based method such as ROUGE (Lin and Hovy, 2003), the word order information utilized by (Wong and Kit, 2008), (Isozaki et al., 2010) and (Talbot et al., 2011), and the combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this kind include the syntactic similarity such as the Part-of-Speech information used by ROSE (Song and Cohn, 2011) and MPF (Popovic, 2011), and phrase information employed by (Echizen-ya and Araki, 2010) and (Han et al., 2013b); the semantic similarity such as Textual entailment used by (Mirkin et al., 2009), Synonyms by (Chan and Ng, 2008), paraphrase by (Snover et al., 2009). The evaluation methods proposed previously suffer from several main weaknesses more or less: perform well in cert"
2013.mtsummit-posters.3,D10-1092,0,0.0133498,"ngton, 2002) (adding the information weight) and GTM (Turian et al., 2003). Recently, many other methods were proposed to revise or improve the previous works. One of the categories is the lexical similarity based metric. The metrics of this kind include the edit distance based method, such as the TER (Snover et al., 2006) and the work of (Akiba et al., 2001) in addition to WER and PER, the precision based method such as SIA (Liu and Gildea, 2006) in addition to BLEU and NIST, recall based method such as ROUGE (Lin and Hovy, 2003), the word order information utilized by (Wong and Kit, 2008), (Isozaki et al., 2010) and (Talbot et al., 2011), and the combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this kind include the syntactic similarity such as the Part-of-Speech information used by R"
2013.mtsummit-posters.3,J10-4005,0,0.0130971,"1 Introduction The machine translation (MT) began as early as in the 1950s (Weaver, 1955) and gained a big progress science the 1990s due to the development of computers (storage capacity and computational power) and the enlarged bilingual corpora (Marino et al., 2006), e.g. (Och, 2003) presented MERT (Minimum Error Rate Training) for loglinear statistical machine translation (SMT) models to achieve better translation quality, (Su et al., 2009) used the Thematic Role Templates model to improve the translation and (Xiong et al., 2011) employed the maximum-entropy model etc. The statistical MT (Koehn, 2010) became mainly approaches in MT literature. Due to the wide-spread development of MT systems, the MT evaluation becomes more and more important to tell us how well the MT systems perform and whether they make some progress. However, the MT evaluation is difficult because some reasons, e.g. language variability results in no single correct translation, the natural languages are highly ambiguous and different languages do not always express the same content in the same way (Arnold, 2003). How to evaluate each MT system’s quality and what should be the criteria have become the new challenges in f"
2013.mtsummit-posters.3,2006.amta-papers.25,0,0.0548797,"and the closest reference translation), position independent word error rate PER (Tillmann et al., 1997) (variant of WER that disregards word ordering), BLEU (Papineni et al., 2002) (the geometric mean of n-gram precision by the system output with respect to reference translations), NIST (Doddington, 2002) (adding the information weight) and GTM (Turian et al., 2003). Recently, many other methods were proposed to revise or improve the previous works. One of the categories is the lexical similarity based metric. The metrics of this kind include the edit distance based method, such as the TER (Snover et al., 2006) and the work of (Akiba et al., 2001) in addition to WER and PER, the precision based method such as SIA (Liu and Gildea, 2006) in addition to BLEU and NIST, recall based method such as ROUGE (Lin and Hovy, 2003), the word order information utilized by (Wong and Kit, 2008), (Isozaki et al., 2010) and (Talbot et al., 2011), and the combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuatio"
2013.mtsummit-posters.3,W11-2113,0,0.100525,"(Talbot et al., 2011), and the combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this kind include the syntactic similarity such as the Part-of-Speech information used by ROSE (Song and Cohn, 2011) and MPF (Popovic, 2011), and phrase information employed by (Echizen-ya and Araki, 2010) and (Han et al., 2013b); the semantic similarity such as Textual entailment used by (Mirkin et al., 2009), Synonyms by (Chan and Ng, 2008), paraphrase by (Snover et al., 2009). The evaluation methods proposed previously suffer from several main weaknesses more or less: perform well in certain language pairs but weak on others, which we call the language-bias problem; consider no linguistic information (not reasonable from the aspect of linguistic analysis) or too many linguistic features (making it diffic"
2013.mtsummit-posters.3,J06-4004,0,0.0780015,"Missing"
2013.mtsummit-posters.3,N03-1020,0,0.232062,"precision by the system output with respect to reference translations), NIST (Doddington, 2002) (adding the information weight) and GTM (Turian et al., 2003). Recently, many other methods were proposed to revise or improve the previous works. One of the categories is the lexical similarity based metric. The metrics of this kind include the edit distance based method, such as the TER (Snover et al., 2006) and the work of (Akiba et al., 2001) in addition to WER and PER, the precision based method such as SIA (Liu and Gildea, 2006) in addition to BLEU and NIST, recall based method such as ROUGE (Lin and Hovy, 2003), the word order information utilized by (Wong and Kit, 2008), (Isozaki et al., 2010) and (Talbot et al., 2011), and the combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this k"
2013.mtsummit-posters.3,C92-2067,0,0.740146,"rehension (improved intelligibility) by Defense Advanced Research Projects Agency (DARPA) of US (White et al., 1994). The manual evaluations suffer the main disadvantage that it is time-consuming and thus too expensive to do frequently. Sima’an, K., Forcada, M.L., Grasmick, D., Depraetere, H., Way, A. (eds.) Proceedings of the XIV Machine Translation Summit (Nice, September 2–6, 2013), p. 215–222. c 2013 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. The early automatic evaluation metrics include the word error rate WER (Su et al., 1992) (edit distance between the system output and the closest reference translation), position independent word error rate PER (Tillmann et al., 1997) (variant of WER that disregards word ordering), BLEU (Papineni et al., 2002) (the geometric mean of n-gram precision by the system output with respect to reference translations), NIST (Doddington, 2002) (adding the information weight) and GTM (Turian et al., 2003). Recently, many other methods were proposed to revise or improve the previous works. One of the categories is the lexical similarity based metric. The metrics of this kind include the edit"
2013.mtsummit-posters.3,H05-1093,0,0.648371,"nd PER, the precision based method such as SIA (Liu and Gildea, 2006) in addition to BLEU and NIST, recall based method such as ROUGE (Lin and Hovy, 2003), the word order information utilized by (Wong and Kit, 2008), (Isozaki et al., 2010) and (Talbot et al., 2011), and the combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this kind include the syntactic similarity such as the Part-of-Speech information used by ROSE (Song and Cohn, 2011) and MPF (Popovic, 2011), and phrase information employed by (Echizen-ya and Araki, 2010) and (Han et al., 2013b); the semantic similarity such as Textual entailment used by (Mirkin et al., 2009), Synonyms by (Chan and Ng, 2008), paraphrase by (Snover et al., 2009). The evaluation methods proposed previously suffer from several main weaknesses more or"
2013.mtsummit-posters.3,W11-2102,0,0.012421,"formation weight) and GTM (Turian et al., 2003). Recently, many other methods were proposed to revise or improve the previous works. One of the categories is the lexical similarity based metric. The metrics of this kind include the edit distance based method, such as the TER (Snover et al., 2006) and the work of (Akiba et al., 2001) in addition to WER and PER, the precision based method such as SIA (Liu and Gildea, 2006) in addition to BLEU and NIST, recall based method such as ROUGE (Lin and Hovy, 2003), the word order information utilized by (Wong and Kit, 2008), (Isozaki et al., 2010) and (Talbot et al., 2011), and the combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this kind include the syntactic similarity such as the Part-of-Speech information used by ROSE (Song and Cohn, 2011)"
2013.mtsummit-posters.3,P06-2070,0,0.503439,"disregards word ordering), BLEU (Papineni et al., 2002) (the geometric mean of n-gram precision by the system output with respect to reference translations), NIST (Doddington, 2002) (adding the information weight) and GTM (Turian et al., 2003). Recently, many other methods were proposed to revise or improve the previous works. One of the categories is the lexical similarity based metric. The metrics of this kind include the edit distance based method, such as the TER (Snover et al., 2006) and the work of (Akiba et al., 2001) in addition to WER and PER, the precision based method such as SIA (Liu and Gildea, 2006) in addition to BLEU and NIST, recall based method such as ROUGE (Lin and Hovy, 2003), the word order information utilized by (Wong and Kit, 2008), (Isozaki et al., 2010) and (Talbot et al., 2011), and the combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2"
2013.mtsummit-posters.3,P09-1089,0,0.0526004,"ercome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this kind include the syntactic similarity such as the Part-of-Speech information used by ROSE (Song and Cohn, 2011) and MPF (Popovic, 2011), and phrase information employed by (Echizen-ya and Araki, 2010) and (Han et al., 2013b); the semantic similarity such as Textual entailment used by (Mirkin et al., 2009), Synonyms by (Chan and Ng, 2008), paraphrase by (Snover et al., 2009). The evaluation methods proposed previously suffer from several main weaknesses more or less: perform well in certain language pairs but weak on others, which we call the language-bias problem; consider no linguistic information (not reasonable from the aspect of linguistic analysis) or too many linguistic features (making it difficult in replicability), which we call the extremism problem; present incomprehensive factors (e.g. BLEU focus on precision only). To address these problems, a novel automatic evaluation metric is"
2013.mtsummit-posters.3,P03-1021,0,0.0209479,"e assigned parameter weights are tunable according to the special characteristics of focused language pairs. Experiments show that this novel evaluation metric yields better performances compared with several classic evaluation metrics (including BLEU, TER and METEOR) and two state-of-the-art ones including ROSE and MPF. 1 Introduction The machine translation (MT) began as early as in the 1950s (Weaver, 1955) and gained a big progress science the 1990s due to the development of computers (storage capacity and computational power) and the enlarged bilingual corpora (Marino et al., 2006), e.g. (Och, 2003) presented MERT (Minimum Error Rate Training) for loglinear statistical machine translation (SMT) models to achieve better translation quality, (Su et al., 2009) used the Thematic Role Templates model to improve the translation and (Xiong et al., 2011) employed the maximum-entropy model etc. The statistical MT (Koehn, 2010) became mainly approaches in MT literature. Due to the wide-spread development of MT systems, the MT evaluation becomes more and more important to tell us how well the MT systems perform and whether they make some progress. However, the MT evaluation is difficult because som"
2013.mtsummit-posters.3,P02-1040,0,0.0870016,"Missing"
2013.mtsummit-posters.3,P06-1055,0,0.0115172,"from the ACL’s special interest group of machine translation SIGMT (WMT workshop) which contain eight corpora including English-to-other (Spanish, Czech, French and German) and other-to-English. There are indeed a lot of linguistic POS tagger tools for different languages available. We conduct an evaluation with different POS taggers, and find that the employing of POS information can make an increase of the correlation score with human judgment for some language pairs but little or no effect on others. The employed POS tagging tools include Berkeley POS tagger for French, English and German (Petrov et al., 2006), COMPOST Czech morphology tagger (Collins, 2002) and TreeTagger Spanish tagger (Schmid, 1994). To avoid the overfitting problem, the WMT 20081 data are used in the development stage for the tuning of the parameters and the WMT 2011 corpora are used in testing. The tuned parameter values for different language pairs are shown in Table 1. The abbreviations EN, CZ, DE, ES and FR mean English, Czech, German, Spanish and French respectively. In the ngram word (POS) alignment, bigram is selected in all the language pairs. To make the model concise using as fewer of external resources as possible, t"
2013.mtsummit-posters.3,W11-2110,0,0.109142,"combination of precision and recall such as Meteor-1.3 (Denkowski and Lavie, 2011) (an modified version of Meteor, includes ranking and adequacy versions and has overcome some weaknesses of previous version such as noise in the paraphrase matching, lack of punctuation handling and discrimination between word types), BLANC (Lita et al., 2005), LEPOR (Han et al., 2012) and PORT (Chen et al., 2012). Another category is the employing of linguistic features. The metrics of this kind include the syntactic similarity such as the Part-of-Speech information used by ROSE (Song and Cohn, 2011) and MPF (Popovic, 2011), and phrase information employed by (Echizen-ya and Araki, 2010) and (Han et al., 2013b); the semantic similarity such as Textual entailment used by (Mirkin et al., 2009), Synonyms by (Chan and Ng, 2008), paraphrase by (Snover et al., 2009). The evaluation methods proposed previously suffer from several main weaknesses more or less: perform well in certain language pairs but weak on others, which we call the language-bias problem; consider no linguistic information (not reasonable from the aspect of linguistic analysis) or too many linguistic features (making it difficult in replicability), w"
2013.mtsummit-posters.3,2003.mtsummit-papers.51,0,0.265063,"The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC-BY-ND. The early automatic evaluation metrics include the word error rate WER (Su et al., 1992) (edit distance between the system output and the closest reference translation), position independent word error rate PER (Tillmann et al., 1997) (variant of WER that disregards word ordering), BLEU (Papineni et al., 2002) (the geometric mean of n-gram precision by the system output with respect to reference translations), NIST (Doddington, 2002) (adding the information weight) and GTM (Turian et al., 2003). Recently, many other methods were proposed to revise or improve the previous works. One of the categories is the lexical similarity based metric. The metrics of this kind include the edit distance based method, such as the TER (Snover et al., 2006) and the work of (Akiba et al., 2001) in addition to WER and PER, the precision based method such as SIA (Liu and Gildea, 2006) in addition to BLEU and NIST, recall based method such as ROUGE (Lin and Hovy, 2003), the word order information utilized by (Wong and Kit, 2008), (Isozaki et al., 2010) and (Talbot et al., 2011), and the combination of pr"
2013.mtsummit-posters.3,J82-2005,0,0.609319,"nguistic information (part-of-speech, n-grammar) but not very much. To make the metric perform well on different language pairs, extensive factors are designed to reflect the translation quality and the assigned parameter weights are tunable according to the special characteristics of focused language pairs. Experiments show that this novel evaluation metric yields better performances compared with several classic evaluation metrics (including BLEU, TER and METEOR) and two state-of-the-art ones including ROSE and MPF. 1 Introduction The machine translation (MT) began as early as in the 1950s (Weaver, 1955) and gained a big progress science the 1990s due to the development of computers (storage capacity and computational power) and the enlarged bilingual corpora (Marino et al., 2006), e.g. (Och, 2003) presented MERT (Minimum Error Rate Training) for loglinear statistical machine translation (SMT) models to achieve better translation quality, (Su et al., 2009) used the Thematic Role Templates model to improve the translation and (Xiong et al., 2011) employed the maximum-entropy model etc. The statistical MT (Koehn, 2010) became mainly approaches in MT literature. Due to the wide-spread developmen"
2013.mtsummit-posters.3,1994.amta-1.25,0,0.0293897,"Missing"
2020.acl-main.41,D18-1338,0,0.0223727,"nslate natural languages (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Since NMT benefits from a massive amount of training data and works in a cross-lingual setting, it becomes much hungrier for training time than other natural language processing (NLP) tasks. ∗ † Equal Contribution Corresponding author Based on self-attention networks (Parikh et al., 2016; Lin et al., 2017), Transformer (Vaswani et al., 2017) has become the most widely used architecture for NMT. Recent studies on improving Transformer, e.g. deep models equipped with up to 30layer encoders (Bapna et al., 2018; Wu et al., 2019; Wang et al., 2019; Zhang et al., 2019a), and scaling NMTs which use a huge batch size to train with 128 GPUs (Ott et al., 2018; Edunov et al., 2018), face a challenge to the efficiency of their training. Curriculum learning (CL), which aims to train machine learning models better and faster (Bengio et al., 2009), is gaining an intuitive appeal to both academic and industrial NMT systems. The basic idea of CL is to train a model using examples ranging from “easy” to “difficult” in different learning stages, and thus the criterion of difficulty is vital to the selection of exa"
2020.acl-main.41,Q17-1010,0,0.0294707,"were trained on each language separately with 32K merge operations. All of the compared and implemented systems are the base Transformer (Vaswani et al., 2017) using the open-source toolkit Marian (JunczysDowmunt et al., 2018).4 We tie the target input embedding and target output embedding (Press and Wolf, 2017). The Adam (Kingma and Ba, 2015) optimizer has been used to update the model parameters with hyperparameters β1 = 0.9, β2 = 0.98, ε = 10−9 . We use the variable learning rate proposed by Vaswani et al. (2017) with 16K warm up steps and a peak learning rate 0.0003. We employed FastText (Bojanowski et al., 2017)5 with its default settings to train the word embedding model for calculating the norm-based sentence difficulty; an example is given in Figure 1. The hyperparameters λm and λw controlling the norm-based model competence and norm-based sentence weight were tuned on the development set of En-De, with the value of 2.5 and 0.5, respectively. To test the adaptability of these two hyperparameters, we use them directly for the Zh-En translation task without any tuning. We compare the proposed methods with the re-implemented 2 https://github.com/fxsjy/jieba http://www.statmt.org/moses/ 4 https://mari"
2020.acl-main.41,D18-1045,0,0.0239956,"nd works in a cross-lingual setting, it becomes much hungrier for training time than other natural language processing (NLP) tasks. ∗ † Equal Contribution Corresponding author Based on self-attention networks (Parikh et al., 2016; Lin et al., 2017), Transformer (Vaswani et al., 2017) has become the most widely used architecture for NMT. Recent studies on improving Transformer, e.g. deep models equipped with up to 30layer encoders (Bapna et al., 2018; Wu et al., 2019; Wang et al., 2019; Zhang et al., 2019a), and scaling NMTs which use a huge batch size to train with 128 GPUs (Ott et al., 2018; Edunov et al., 2018), face a challenge to the efficiency of their training. Curriculum learning (CL), which aims to train machine learning models better and faster (Bengio et al., 2009), is gaining an intuitive appeal to both academic and industrial NMT systems. The basic idea of CL is to train a model using examples ranging from “easy” to “difficult” in different learning stages, and thus the criterion of difficulty is vital to the selection of examples. Zhang et al. (2018) summarize two kinds of difficulty criteria in CL for NMT: 1) linguistically motivated sentence difficulty, e.g. sentence length, word freque"
2020.acl-main.41,D19-1633,0,0.0414129,"Missing"
2020.acl-main.41,D13-1176,0,0.0150008,"model competence makes NMT learn the curriculum in a fully automated way, while the norm-based sentence weight further enhances the learning of the vector representation of the NMT. Experimental results for the WMT’14 English– German and WMT’17 Chinese–English translation tasks demonstrate that the proposed method outperforms strong baselines in terms of BLEU score (+1.17/+1.56) and training speedup (2.22x/3.33x). 1 Introduction The past several years have witnessed the rapid development of neural machine translation (NMT) based on an encoder–decoder framework to translate natural languages (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Since NMT benefits from a massive amount of training data and works in a cross-lingual setting, it becomes much hungrier for training time than other natural language processing (NLP) tasks. ∗ † Equal Contribution Corresponding author Based on self-attention networks (Parikh et al., 2016; Lin et al., 2017), Transformer (Vaswani et al., 2017) has become the most widely used architecture for NMT. Recent studies on improving Transformer, e.g. deep models equipped with up to 30layer encoders (Bapna et al., 2018; Wu et al., 2019; Wang et al., 2019;"
2020.acl-main.41,N19-1208,0,0.259839,"ce is defined which is a function that takes the training step t as input and outputs a competence value from 0 to 1:1 s 1 − c20 c(t) ∈ (0, 1] = min(1, t + c20 ) (7) λt where c0 = 0.01 is the initial competence at the beginning of training and λt is a hyperparameter determining the length of the curriculum. For the sentence difficulty, they use cumulative density function (CDF) to transfer the distribution of sentence difficulties into (0, 1]: 429 ˆ n ) ∈ (0, 1] = CDF({d(xn )}N )n d(x n=1 (8) 1 We introduce the square root competence model since it has the best performance in Platanios et al. (2019). 20K 20 15K 15 10 BLEU Score Norm of Source Embedding 25 10K BLEU Norm 5K 0 20K 40K 60K Training Step 80K 5 0 100K Figure 2: Norm of NMT source embedding and BLEU score of a vanilla Transformer on the WMT’14 English–German translation task. The BLEU scores are calculated on the development set. Both the norm and BLEU score grow rapidly until the 30K training step. The score of difficult sentences tends to be 1, while that of easy sentences tends to be 0. The model uniformly samples curricula whose difficulty is lower than the model competence at each training step, thus making the model learn"
2020.acl-main.41,P16-1162,0,0.142708,"hinese–English (Zh-En) translation tasks. For En-De, the training set consists of 4.5M sentence pairs with 107M English words and 113M German words. The development is newstest13 and the test set is newstest14. For the Zh-En, the training set contains roughly 20M sentence pairs. The development is newsdev2017 and the test set is newstest2017. The Chinese data were segmented by jieba,2 while the others were tokenized by the tokenize.perl script from Moses.3 We filtered the sentence pairs with a source or target length over 200 tokens. Rare words in each data set were split into sub-word units (Sennrich et al., 2016). The BPE models were trained on each language separately with 32K merge operations. All of the compared and implemented systems are the base Transformer (Vaswani et al., 2017) using the open-source toolkit Marian (JunczysDowmunt et al., 2018).4 We tie the target input embedding and target output embedding (Press and Wolf, 2017). The Adam (Kingma and Ba, 2015) optimizer has been used to update the model parameters with hyperparameters β1 = 0.9, β2 = 0.98, ε = 10−9 . We use the variable learning rate proposed by Vaswani et al. (2017) with 16K warm up steps and a peak learning rate 0.0003. We em"
2020.acl-main.41,P10-1040,0,0.0402979,"be: 1) sentences with lengths from short to long; 2) sentences with words whose frequency goes from high to low (i.e. word rarity); and 3) uncertainty of sentences (from low to high uncertainties) measured by models trained in previous epochs or pre-trained language models. Table 1 shows the sentences of the training curricula provided by vanilla Transformer and the proposed method. 3 3.1 θˆ = L(D; θ0 ) θ0 N X log P (yn |xn ; θ0 ) Norm-based Sentence Difficulty Most NLP systems have been taking advantage of distributed word embeddings to capture the syntactic and semantic features of a word (Turian et al., 2010; Mikolov et al., 2013). A word embedding (vector) can be divided into two parts: the norm and the direction: NMT uses a single large neural network to construct a translation model that translates a source sentence x into a target sentence y. During training, given a parallel corpus D = {hxn , yn i}N n=1 , NMT aims to maximize its log-likelihood: = arg max Norm-based Curriculum Learning (1) n=1 428 w w = ||w ||· |{z} ||w|| |{z } norm (5) direction In practice, the word embedding, represented by w, is the key component of a neural model (Liu et al., w 2019a,b), and the direction ||w|| can also"
2020.acl-main.41,P19-1176,1,0.84531,"r and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). Since NMT benefits from a massive amount of training data and works in a cross-lingual setting, it becomes much hungrier for training time than other natural language processing (NLP) tasks. ∗ † Equal Contribution Corresponding author Based on self-attention networks (Parikh et al., 2016; Lin et al., 2017), Transformer (Vaswani et al., 2017) has become the most widely used architecture for NMT. Recent studies on improving Transformer, e.g. deep models equipped with up to 30layer encoders (Bapna et al., 2018; Wu et al., 2019; Wang et al., 2019; Zhang et al., 2019a), and scaling NMTs which use a huge batch size to train with 128 GPUs (Ott et al., 2018; Edunov et al., 2018), face a challenge to the efficiency of their training. Curriculum learning (CL), which aims to train machine learning models better and faster (Bengio et al., 2009), is gaining an intuitive appeal to both academic and industrial NMT systems. The basic idea of CL is to train a model using examples ranging from “easy” to “difficult” in different learning stages, and thus the criterion of difficulty is vital to the selection of examples. Zhang et al. (2018) summarize"
2020.acl-main.41,P19-1352,1,0.889841,"Missing"
2020.acl-main.41,W18-6301,0,0.0442477,"of training data and works in a cross-lingual setting, it becomes much hungrier for training time than other natural language processing (NLP) tasks. ∗ † Equal Contribution Corresponding author Based on self-attention networks (Parikh et al., 2016; Lin et al., 2017), Transformer (Vaswani et al., 2017) has become the most widely used architecture for NMT. Recent studies on improving Transformer, e.g. deep models equipped with up to 30layer encoders (Bapna et al., 2018; Wu et al., 2019; Wang et al., 2019; Zhang et al., 2019a), and scaling NMTs which use a huge batch size to train with 128 GPUs (Ott et al., 2018; Edunov et al., 2018), face a challenge to the efficiency of their training. Curriculum learning (CL), which aims to train machine learning models better and faster (Bengio et al., 2009), is gaining an intuitive appeal to both academic and industrial NMT systems. The basic idea of CL is to train a model using examples ranging from “easy” to “difficult” in different learning stages, and thus the criterion of difficulty is vital to the selection of examples. Zhang et al. (2018) summarize two kinds of difficulty criteria in CL for NMT: 1) linguistically motivated sentence difficulty, e.g. senten"
2020.acl-main.41,P02-1040,0,0.106865,"nd λw of the norm-based sentence weight function. competence-based CL (Platanios et al., 2019).6 During training, the mini-batch contains nearly 32K source tokens and 32K target tokens. We evaluated the models every 2.5K steps, and chose the best performing model for decoding. The maximum training step was set to 100K for En-De and 150K for Zh-En. During testing, we tuned the beam size and length penalty (Wu et al., 2016) on the development data, using a beam size of 6 and a length penalty of 0.6 for En-De, and a beam size of 12 and a length penalty of 1.0 for ZhEn. We report the 4-gram BLEU (Papineni et al., 2002) score given by the multi-bleu.perl script. The codes and scripts of the proposed norm-based CL and our re-implemented competence-based CL are freely available at https://github.com/NLP2CT/ norm-nmt. 6 We use its best settings, i.e. the rarity-based sentence difficulty and the square root competence function. Main Results Table 2 shows the results of the En-De translation task in terms of BLEU scores and training speedup. Models (1) to (4) are the existing baselines of this translation benchmark. Model (5) is our implemented base Transformer with 100K training steps, obtaining 27.64 BLEU score"
2020.acl-main.41,D16-1244,0,0.102986,"Missing"
2020.acl-main.41,N19-1119,0,0.112261,"Missing"
2020.acl-main.41,E17-2025,0,0.0194392,"newstest2017. The Chinese data were segmented by jieba,2 while the others were tokenized by the tokenize.perl script from Moses.3 We filtered the sentence pairs with a source or target length over 200 tokens. Rare words in each data set were split into sub-word units (Sennrich et al., 2016). The BPE models were trained on each language separately with 32K merge operations. All of the compared and implemented systems are the base Transformer (Vaswani et al., 2017) using the open-source toolkit Marian (JunczysDowmunt et al., 2018).4 We tie the target input embedding and target output embedding (Press and Wolf, 2017). The Adam (Kingma and Ba, 2015) optimizer has been used to update the model parameters with hyperparameters β1 = 0.9, β2 = 0.98, ε = 10−9 . We use the variable learning rate proposed by Vaswani et al. (2017) with 16K warm up steps and a peak learning rate 0.0003. We employed FastText (Bojanowski et al., 2017)5 with its default settings to train the word embedding model for calculating the norm-based sentence difficulty; an example is given in Figure 1. The hyperparameters λm and λw controlling the norm-based model competence and norm-based sentence weight were tuned on the development set of"
2020.acl-main.41,P19-1558,0,0.0686694,"ce is defined which is a function that takes the training step t as input and outputs a competence value from 0 to 1:1 s 1 − c20 c(t) ∈ (0, 1] = min(1, t + c20 ) (7) λt where c0 = 0.01 is the initial competence at the beginning of training and λt is a hyperparameter determining the length of the curriculum. For the sentence difficulty, they use cumulative density function (CDF) to transfer the distribution of sentence difficulties into (0, 1]: 429 ˆ n ) ∈ (0, 1] = CDF({d(xn )}N )n d(x n=1 (8) 1 We introduce the square root competence model since it has the best performance in Platanios et al. (2019). 20K 20 15K 15 10 BLEU Score Norm of Source Embedding 25 10K BLEU Norm 5K 0 20K 40K 60K Training Step 80K 5 0 100K Figure 2: Norm of NMT source embedding and BLEU score of a vanilla Transformer on the WMT’14 English–German translation task. The BLEU scores are calculated on the development set. Both the norm and BLEU score grow rapidly until the 30K training step. The score of difficult sentences tends to be 1, while that of easy sentences tends to be 0. The model uniformly samples curricula whose difficulty is lower than the model competence at each training step, thus making the model learn"
2020.acl-main.41,D19-1083,0,0.139415,"; Sutskever et al., 2014; Bahdanau et al., 2015). Since NMT benefits from a massive amount of training data and works in a cross-lingual setting, it becomes much hungrier for training time than other natural language processing (NLP) tasks. ∗ † Equal Contribution Corresponding author Based on self-attention networks (Parikh et al., 2016; Lin et al., 2017), Transformer (Vaswani et al., 2017) has become the most widely used architecture for NMT. Recent studies on improving Transformer, e.g. deep models equipped with up to 30layer encoders (Bapna et al., 2018; Wu et al., 2019; Wang et al., 2019; Zhang et al., 2019a), and scaling NMTs which use a huge batch size to train with 128 GPUs (Ott et al., 2018; Edunov et al., 2018), face a challenge to the efficiency of their training. Curriculum learning (CL), which aims to train machine learning models better and faster (Bengio et al., 2009), is gaining an intuitive appeal to both academic and industrial NMT systems. The basic idea of CL is to train a model using examples ranging from “easy” to “difficult” in different learning stages, and thus the criterion of difficulty is vital to the selection of examples. Zhang et al. (2018) summarize two kinds of diffic"
2020.acl-main.41,I17-2046,0,0.0678737,"ifferent learning stages, and thus the criterion of difficulty is vital to the selection of examples. Zhang et al. (2018) summarize two kinds of difficulty criteria in CL for NMT: 1) linguistically motivated sentence difficulty, e.g. sentence length, word frequency, and the number of coordinating conjunctions, which is easier to obtain (Kocmi and Bojar, 2017; Platanios et al., 2019); 2) model-based sentence difficulty, e.g. sentence uncertainties derived from independent language models or the models trained in previous time steps or epochs, which tends to be intuitively effective but costly (Zhang et al., 2017; Kumar et al., 2019; Zhang et al., 2019b; Zhou et al., 2020). In this paper, we propose a novel norm-based criterion for the difficulty of a sentence, which takes advantage of both model-based and linguistically motivated difficulty features. We observe that the norms of the word vectors trained on simple neural networks are expressive enough to model the two features, which are easy to obtain while possessing learning-dependent features. For example, most of the frequent words and context-insensitive rare words will have vectors with small norms. 427 Proceedings of the 58th Annual Meeting of"
2020.acl-main.41,N19-1189,0,0.285749,"ce is defined which is a function that takes the training step t as input and outputs a competence value from 0 to 1:1 s 1 − c20 c(t) ∈ (0, 1] = min(1, t + c20 ) (7) λt where c0 = 0.01 is the initial competence at the beginning of training and λt is a hyperparameter determining the length of the curriculum. For the sentence difficulty, they use cumulative density function (CDF) to transfer the distribution of sentence difficulties into (0, 1]: 429 ˆ n ) ∈ (0, 1] = CDF({d(xn )}N )n d(x n=1 (8) 1 We introduce the square root competence model since it has the best performance in Platanios et al. (2019). 20K 20 15K 15 10 BLEU Score Norm of Source Embedding 25 10K BLEU Norm 5K 0 20K 40K 60K Training Step 80K 5 0 100K Figure 2: Norm of NMT source embedding and BLEU score of a vanilla Transformer on the WMT’14 English–German translation task. The BLEU scores are calculated on the development set. Both the norm and BLEU score grow rapidly until the 30K training step. The score of difficult sentences tends to be 1, while that of easy sentences tends to be 0. The model uniformly samples curricula whose difficulty is lower than the model competence at each training step, thus making the model learn"
2020.acl-main.41,2020.acl-main.620,1,0.767007,"y is vital to the selection of examples. Zhang et al. (2018) summarize two kinds of difficulty criteria in CL for NMT: 1) linguistically motivated sentence difficulty, e.g. sentence length, word frequency, and the number of coordinating conjunctions, which is easier to obtain (Kocmi and Bojar, 2017; Platanios et al., 2019); 2) model-based sentence difficulty, e.g. sentence uncertainties derived from independent language models or the models trained in previous time steps or epochs, which tends to be intuitively effective but costly (Zhang et al., 2017; Kumar et al., 2019; Zhang et al., 2019b; Zhou et al., 2020). In this paper, we propose a novel norm-based criterion for the difficulty of a sentence, which takes advantage of both model-based and linguistically motivated difficulty features. We observe that the norms of the word vectors trained on simple neural networks are expressive enough to model the two features, which are easy to obtain while possessing learning-dependent features. For example, most of the frequent words and context-insensitive rare words will have vectors with small norms. 427 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 427–436"
2020.acl-main.620,J90-2002,0,0.784561,"Missing"
2020.acl-main.620,P18-1008,0,0.0283384,"reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule. 1 Figure 1: The change of confidence in an area during the learning. Humans (red) experience the process of overconfidence⇒despair⇒enlightenment (DunningKruger Curve), while prior work that exploits CL in NMT assumes a monotonically increased curve (green, Platanios et al., 2019). Interestingly, our model automatically draws a similar tendency as humans (blue). Introduction Neural machine translation (NMT) has advanced the state-of-the-art on various translation tasks (Hassan et al., 2018; Chen et al., 2018). A wellperformed NMT is trained using an end-to-end framework (Sutskever et al., 2014) that profits from large-scale training corpus and various optimization tricks (Ott et al., 2018; Xu et al., 2019; Li et al., 2020). These techniques boost the translation quality, in the meanwhile, leading to massive hyper-parameters to be tuned and expensive development costs (Popel and Bojar, 2018). Recent studies (Zhang et al., 2018, 2019; Platanios et al., 2019; Liu et al., 2020) have proven that feeding training examples in a meaningful order rather than considering them randomly can accelerate the mod"
2020.acl-main.620,N19-1423,0,0.522454,"-to-English translation tasks. The experimental results demonstrate that the proposed model consistently improves translation performance over the strong T RANS FORMER (Vaswani et al., 2017) baseline and related methods that exploit CL into NMT. Extensive analyses confirm that: 1) our approach significantly speeds up the model convergence; 2) using data uncertainty to present the translation difficulty surpasses its sentence length and word rarity counterparts, and this superiority can be further expanded by exploiting a language model that is trained on large-scale external data, i.e. B ERT (Devlin et al., 2019); 3) the model uncertainty performs a selfadaptive manner to assess the model competence regardless the pre-defined patterns. 2 Preliminary NMT uses a single, large neural network to build translation model, aiming to maximize the conditional distribution of sentence pairs using parallel corpus (Sutskever et al., 2014; Bahdanau et al., 2015; Yang et al., 2019; Wan et al., 2020). Formally, the learning objective is to minimize the following loss function over the training corpus D = {(xn , y n )}N n=1 , with the size being N : L = E(xn ,yn )∼D [− log P(y n |xn ; θ)] (1) where xn and y n indicat"
2020.acl-main.620,W11-2123,0,0.165674,"e and target uncertainty: udata (xn , y n ) = udata (xn ) + udata (y n ) (4) To our best knowledge, due to the lack of interpretability on scoring the joint difficulty in a sentence pair, all the existing methods that exploit CL into NMT merely measure data difficulty on either source or target. Our method provides an alternative way to tackle this problem with the concept of joint probability distribution. We expect the joint uncertainty to further improve the performance. 6936 In this paper, we examine three widely used LMs to appraise the data uncertainty: a statistical ngram LM – K EN LM (Heafield, 2011), a neural LM – R NN LM (Mikolov et al., 2010), and a multilingual neural LM that trained on billions of external sentences – B ERT (Devlin et al., 2019). Note that, the modeling of data uncertainty is not limited to our approach. It can be also quantified by other manners, e.g. estimating the data likelihood with Monte Carlo approximation (Der Kiureghian and Ditlevsen, 2009) or validating the translation distribution using a well-trained NMT model (Zhang et al., 2018). In contrast to these time-consuming techniques, LM marginally increases the computational cost and easy to be applied, confor"
2020.acl-main.620,W04-3250,0,0.607556,"Missing"
2020.acl-main.620,2020.acl-main.41,1,0.680739,"uction Neural machine translation (NMT) has advanced the state-of-the-art on various translation tasks (Hassan et al., 2018; Chen et al., 2018). A wellperformed NMT is trained using an end-to-end framework (Sutskever et al., 2014) that profits from large-scale training corpus and various optimization tricks (Ott et al., 2018; Xu et al., 2019; Li et al., 2020). These techniques boost the translation quality, in the meanwhile, leading to massive hyper-parameters to be tuned and expensive development costs (Popel and Bojar, 2018). Recent studies (Zhang et al., 2018, 2019; Platanios et al., 2019; Liu et al., 2020) have proven that feeding training examples in a meaningful order rather than considering them randomly can accelerate the model ∗ Corresponding author convergence thus reducing the computational cost. Such methods refer to curriculum learning (CL, Bengio et al., 2009), in which a model is taught as a human from simple concepts to complex ones. There exists two open problems in the integration of CL with NMT, i.e. the assessment of data difficulty and the programme of learning schedule. Considering the former, prior studies (Kocmi and Bojar, 2017; Platanios et al., 2019) intuitively treat huma"
2020.acl-main.620,W18-6301,0,0.0433435,") experience the process of overconfidence⇒despair⇒enlightenment (DunningKruger Curve), while prior work that exploits CL in NMT assumes a monotonically increased curve (green, Platanios et al., 2019). Interestingly, our model automatically draws a similar tendency as humans (blue). Introduction Neural machine translation (NMT) has advanced the state-of-the-art on various translation tasks (Hassan et al., 2018; Chen et al., 2018). A wellperformed NMT is trained using an end-to-end framework (Sutskever et al., 2014) that profits from large-scale training corpus and various optimization tricks (Ott et al., 2018; Xu et al., 2019; Li et al., 2020). These techniques boost the translation quality, in the meanwhile, leading to massive hyper-parameters to be tuned and expensive development costs (Popel and Bojar, 2018). Recent studies (Zhang et al., 2018, 2019; Platanios et al., 2019; Liu et al., 2020) have proven that feeding training examples in a meaningful order rather than considering them randomly can accelerate the model ∗ Corresponding author convergence thus reducing the computational cost. Such methods refer to curriculum learning (CL, Bengio et al., 2009), in which a model is taught as a human"
2020.acl-main.620,P02-1040,0,0.107251,"e also conduct experiments on the large-scale training corpus, i.e. WMT17 Zh⇒En, in which, 20M examples are extracted as the training set. We use the standard validation and test sets provided in each translation task. The Chinese sentences are segmented by the word segmentation toolkit Jieba,4 while the sentences in other languages are tokenized using the scripts provided in Moses.5 All the data are 3 processed by byte-pair encoding to alleviate the Out-of-Vocabulary problem (Sennrich et al., 2016b) with 32K merge operations for both language pairs. The case-sensitive 4-gram NIST BLEU score (Papineni et al., 2002) is used as the evaluation metric. Our code is available at https://github.com/ NLP2CT/ua-cl-nmt 4 https://github.com/fxshy/jieba 5 https://github.com/mosesdecoder 4.2 Ablation Study In this section, we evaluate the effectiveness of different components in CL on the En⇒De task. In the first two series of experiments, we investigate the effects of different measures of data difficulty and model competence. Then, we check how the baby steps applied in our training strategy affect the performance. The results are concluded in Table 1. 6938 6 https://github.com/kpu/kenlm DATA -U Model T RANSFORMER"
2020.acl-main.620,N19-1119,0,0.108545,"ork to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule. 1 Figure 1: The change of confidence in an area during the learning. Humans (red) experience the process of overconfidence⇒despair⇒enlightenment (DunningKruger Curve), while prior work that exploits CL in NMT assumes a monotonically increased curve (green, Platanios et al., 2019). Interestingly, our model automatically draws a similar tendency as humans (blue). Introduction Neural machine translation (NMT) has advanced the state-of-the-art on various translation tasks (Hassan et al., 2018; Chen et al., 2018). A wellperformed NMT is trained using an end-to-end framework (Sutskever et al., 2014) that profits from large-scale training corpus and various optimization tricks (Ott et al., 2018; Xu et al., 2019; Li et al., 2020). These techniques boost the translation quality, in the meanwhile, leading to massive hyper-parameters to be tuned and expensive development costs ("
2020.acl-main.620,P16-1162,0,0.631496,"parallel corpus (Sutskever et al., 2014; Bahdanau et al., 2015; Yang et al., 2019; Wan et al., 2020). Formally, the learning objective is to minimize the following loss function over the training corpus D = {(xn , y n )}N n=1 , with the size being N : L = E(xn ,yn )∼D [− log P(y n |xn ; θ)] (1) where xn and y n indicate the source and target sides of the n-th example in training data. θ denotes the trainable parameters of NMT model. During the training, the examples randomly feed to vanilla model, regardless of their order, making the development of a well-performed NMT system time-consuming (Sennrich et al., 2016a; Popel and Bojar, 2018; Yang et al., 2020). An alternative way to speed up the training process and boost the performance of a neural network is to exploit CL (Elman, 1993; Krueger and Dayan, 2009; Bengio et al., 2009). Related Work on Exploring CL Several studies have shown the effectiveness of CL in the field of computer vision (Sarafianos et al., 2017; Wang et al., 2019c; Guo et al., 2018), as well as a range of NLP tasks, including math word problem (Zaremba and Sutskever, 2014), sentiment analysis (Cirik et al., 2016), and natural answer generation (Liu et al., 2018). They point out tha"
2020.acl-main.620,D19-1073,0,0.0598411,"Missing"
2020.acl-main.620,P19-1123,0,0.429485,"data. θ denotes the trainable parameters of NMT model. During the training, the examples randomly feed to vanilla model, regardless of their order, making the development of a well-performed NMT system time-consuming (Sennrich et al., 2016a; Popel and Bojar, 2018; Yang et al., 2020). An alternative way to speed up the training process and boost the performance of a neural network is to exploit CL (Elman, 1993; Krueger and Dayan, 2009; Bengio et al., 2009). Related Work on Exploring CL Several studies have shown the effectiveness of CL in the field of computer vision (Sarafianos et al., 2017; Wang et al., 2019c; Guo et al., 2018), as well as a range of NLP tasks, including math word problem (Zaremba and Sutskever, 2014), sentiment analysis (Cirik et al., 2016), and natural answer generation (Liu et al., 2018). They point out that CL can solve the problem in some tasks that is hard to train through presenting training data in an easy-to-hard order. Kocmi and Bojar (2017) first apply CL into NMT and suggest two sticking points, i.e. data difficulty and learning schedule. Partially inspired by their findings, Thompson et al. (2018), Zhang et al. (2019), Wang et al. (2019b) and Kumar et al. succeed on"
2020.acl-main.620,P19-1295,1,0.856418,"rocess of overconfidence⇒despair⇒enlightenment (DunningKruger Curve), while prior work that exploits CL in NMT assumes a monotonically increased curve (green, Platanios et al., 2019). Interestingly, our model automatically draws a similar tendency as humans (blue). Introduction Neural machine translation (NMT) has advanced the state-of-the-art on various translation tasks (Hassan et al., 2018; Chen et al., 2018). A wellperformed NMT is trained using an end-to-end framework (Sutskever et al., 2014) that profits from large-scale training corpus and various optimization tricks (Ott et al., 2018; Xu et al., 2019; Li et al., 2020). These techniques boost the translation quality, in the meanwhile, leading to massive hyper-parameters to be tuned and expensive development costs (Popel and Bojar, 2018). Recent studies (Zhang et al., 2018, 2019; Platanios et al., 2019; Liu et al., 2020) have proven that feeding training examples in a meaningful order rather than considering them randomly can accelerate the model ∗ Corresponding author convergence thus reducing the computational cost. Such methods refer to curriculum learning (CL, Bengio et al., 2009), in which a model is taught as a human from simple conce"
2020.acl-main.620,P19-1354,1,0.84346,"nty to present the translation difficulty surpasses its sentence length and word rarity counterparts, and this superiority can be further expanded by exploiting a language model that is trained on large-scale external data, i.e. B ERT (Devlin et al., 2019); 3) the model uncertainty performs a selfadaptive manner to assess the model competence regardless the pre-defined patterns. 2 Preliminary NMT uses a single, large neural network to build translation model, aiming to maximize the conditional distribution of sentence pairs using parallel corpus (Sutskever et al., 2014; Bahdanau et al., 2015; Yang et al., 2019; Wan et al., 2020). Formally, the learning objective is to minimize the following loss function over the training corpus D = {(xn , y n )}N n=1 , with the size being N : L = E(xn ,yn )∼D [− log P(y n |xn ; θ)] (1) where xn and y n indicate the source and target sides of the n-th example in training data. θ denotes the trainable parameters of NMT model. During the training, the examples randomly feed to vanilla model, regardless of their order, making the development of a well-performed NMT system time-consuming (Sennrich et al., 2016a; Popel and Bojar, 2018; Yang et al., 2020). An alternative"
2020.emnlp-main.80,P18-1008,0,0.0343184,"g self-paced learning, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.1 1 Introduction Neural machine translation (NMT) has achieved promising results with the use of various optimization tricks (Hassan et al., 2018; Chen et al., 2018; Xu et al., 2019; Li et al., 2020; Yang et al., 2020). In spite of that, these techniques lead to increased training time and massive hyper-parameters, making the development of a well-performed system expensive (Popel and Bojar, 2018; Ott et al., 2018). As an alternative mitigation, curriculum learning (CL, Elman, 1993; Bengio et al., 2009) has shown its effectiveness on speeding up the convergence and stabilizing the NMT model training (Zhang et al., 2018; Platanios et al., 2019). CL teaches NMT model from easy examples to complex ones rather than equally considering all samples, where the"
2020.emnlp-main.80,P18-1069,0,0.0830225,"h place distributions over the weights of network. For efficiency, we adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016) to approximate Bayesian inference. Given current NMT model parameterized by θ and a mini-batch consisting of N sentence pairs {(x1 , y1 ), · · · , (xN , yN )}, we first perform M passes through the network, where the m-th pass θˆm randomly deactivates part of neurons. Thus, each example yields M sets of conditional probabilities. The lower variance of translation probabilities reflects higher confidence that the model has with respect to the instance (Dong et al., 2018; Wang et al., 2019). We propose multi-granularity strategies for confidence estimation: n ,θ ˆm ) denotes the variwhere Var{P (yjn |xn , y<j ance of the translation probability with respect to yjn . Similar to sentence-level strategy, the confidence scores of tokens are normalized as: exp(βˆjn ) βjn = PJ , ˆn t=1 exp(βt ) (4) where J indicates the length of target sentence yn . 2.2 Training Strategy A larger confidence score indicates that the current model is confident on the corresponding example. Therefore, the model should learn more from the predicted loss. In order to govern the learnin"
2020.emnlp-main.80,N19-1208,0,0.0303738,"ds on the number of training step. 10k BLEU score At the early stage of the study, the model learns more from confident samples, thus accelerating the training. The hesitant samples are not completely ignorant, but relatively few can be learned. As training proceeds, the loss of high-confidence samples gradually reduce, and the model will pay more attention on “complex” samples with low prediction accuracy, thus raising their confidence. In this way, the loss of different samples are dynamically revised, eventually balancing the learning. Contrast to related studies (Zhang et al., 2018, 2019; Kumar et al., 2019; Platanios et al., 2019) which adopt CL into NMT with predefined patterns, the superiority of our model lies in its flexibility on both learning emphasis and strategy. Several researchers may concern about the processing speed when integrating Monte Carlo Dropout sampling. Contrary to prior studies which estimate confidence during inference (Dong et al., 2018; Wang et al., 2019), we only perform forward propagation M = 5 times in training time, which avoids the auto-regressive decoding for efficiency. 20k 40k 80k End 27 25 23 21 −3 −2 −1 0 k 1 2 3 Figure 2: Affects of k on best performance af"
2020.emnlp-main.80,D18-1317,1,0.891192,"Missing"
2020.emnlp-main.80,2020.acl-main.41,1,0.837622,"idering all samples, where the keys lie in the definition of “diffi∗ Baosong Yang and Derek F. Wong are co-corresponding authors. Work was done when Yu Wan was interning at DAMO Academy, Alibaba Group. 1 Our codes: https://github.com/NLP2CT/SPL for NMT. culty” and the strategy of curricula design (Krueger and Dayan, 2009; Kocmi and Bojar, 2017). Existing studies artificially determine data difficulty according to prior linguistic knowledge such as sentence length (SL) and word rarity (WR) (Platanios et al., 2019; Zhang et al., 2019; Zhou et al., 2020), and manually tune the learning schedule (Liu et al., 2020; Fomicheva et al., 2020). However, neither there exists a clear distinction between easy and hard examples (Kumar et al., 2010), nor these human intuitions exactly conform to effective model training (Zhang et al., 2018). Instead, we resolve this problem by introducing self-paced learning (Kumar et al., 2010), where the emphasis of learning can be dynamically determined by model itself rather than human intuitions. Specifically, our model measures the level of confidence on each training example (Gal and Ghahramani, 2016; Xiao and Wang, 2019), where an easy sample is actually the one of high"
2020.emnlp-main.80,W18-6301,0,0.15898,"ion tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.1 1 Introduction Neural machine translation (NMT) has achieved promising results with the use of various optimization tricks (Hassan et al., 2018; Chen et al., 2018; Xu et al., 2019; Li et al., 2020; Yang et al., 2020). In spite of that, these techniques lead to increased training time and massive hyper-parameters, making the development of a well-performed system expensive (Popel and Bojar, 2018; Ott et al., 2018). As an alternative mitigation, curriculum learning (CL, Elman, 1993; Bengio et al., 2009) has shown its effectiveness on speeding up the convergence and stabilizing the NMT model training (Zhang et al., 2018; Platanios et al., 2019). CL teaches NMT model from easy examples to complex ones rather than equally considering all samples, where the keys lie in the definition of “diffi∗ Baosong Yang and Derek F. Wong are co-corresponding authors. Work was done when Yu Wan was interning at DAMO Academy, Alibaba Group. 1 Our codes: https://github.com/NLP2CT/SPL for NMT. culty” and the strategy of curr"
2020.emnlp-main.80,N19-1119,0,0.356016,"ine translation (NMT) has achieved promising results with the use of various optimization tricks (Hassan et al., 2018; Chen et al., 2018; Xu et al., 2019; Li et al., 2020; Yang et al., 2020). In spite of that, these techniques lead to increased training time and massive hyper-parameters, making the development of a well-performed system expensive (Popel and Bojar, 2018; Ott et al., 2018). As an alternative mitigation, curriculum learning (CL, Elman, 1993; Bengio et al., 2009) has shown its effectiveness on speeding up the convergence and stabilizing the NMT model training (Zhang et al., 2018; Platanios et al., 2019). CL teaches NMT model from easy examples to complex ones rather than equally considering all samples, where the keys lie in the definition of “diffi∗ Baosong Yang and Derek F. Wong are co-corresponding authors. Work was done when Yu Wan was interning at DAMO Academy, Alibaba Group. 1 Our codes: https://github.com/NLP2CT/SPL for NMT. culty” and the strategy of curricula design (Krueger and Dayan, 2009; Kocmi and Bojar, 2017). Existing studies artificially determine data difficulty according to prior linguistic knowledge such as sentence length (SL) and word rarity (WR) (Platanios et al., 2019;"
2020.emnlp-main.80,P10-1063,0,0.0353053,"ning through regulating the training loss, as illustrated in Fig. 1. 2.1 αn = PN 24 . 22 t α t=1 exp(ˆ ) (2) 20 Token-Level Confidence (TLC) 5000Intuitively, 50000 95000 confidence scores can be evaluated at more Training Step fine-grained level. We extend our model into token-level so as to estimate the confidence on translating each element in target sentence yn . The confidence βˆjn of the j-th token yjn is: n ˆm M βˆjn = (1 − Var{P (yjn |xn , y<j , θ )}m=1 )k , (3) Confidence Estimation We propose to determine the learning emphasis according to the model confidence (Ueffing and Ney, 2005; Soricut and Echihabi, 2010), which quantifies whether the current model is confident or hesitant on translating the training samples. The model confidence can be quantified by Bayesian neural networks (Buntine and Weigend, 1991; Neal, 1996), which place distributions over the weights of network. For efficiency, we adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016) to approximate Bayesian inference. Given current NMT model parameterized by θ and a mini-batch consisting of N sentence pairs {(x1 , y1 ), · · · , (xN , yN )}, we first perform M passes through the network, where the m-th pass θˆm random"
2020.emnlp-main.80,H05-1096,0,0.0843226,"ntrol the focus of learning through regulating the training loss, as illustrated in Fig. 1. 2.1 αn = PN 24 . 22 t α t=1 exp(ˆ ) (2) 20 Token-Level Confidence (TLC) 5000Intuitively, 50000 95000 confidence scores can be evaluated at more Training Step fine-grained level. We extend our model into token-level so as to estimate the confidence on translating each element in target sentence yn . The confidence βˆjn of the j-th token yjn is: n ˆm M βˆjn = (1 − Var{P (yjn |xn , y<j , θ )}m=1 )k , (3) Confidence Estimation We propose to determine the learning emphasis according to the model confidence (Ueffing and Ney, 2005; Soricut and Echihabi, 2010), which quantifies whether the current model is confident or hesitant on translating the training samples. The model confidence can be quantified by Bayesian neural networks (Buntine and Weigend, 1991; Neal, 1996), which place distributions over the weights of network. For efficiency, we adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016) to approximate Bayesian inference. Given current NMT model parameterized by θ and a mini-batch consisting of N sentence pairs {(x1 , y1 ), · · · , (xN , yN )}, we first perform M passes through the network, w"
2020.emnlp-main.80,D19-1073,0,0.13871,"ns over the weights of network. For efficiency, we adopt widely used Monte Carlo dropout sampling (Gal and Ghahramani, 2016) to approximate Bayesian inference. Given current NMT model parameterized by θ and a mini-batch consisting of N sentence pairs {(x1 , y1 ), · · · , (xN , yN )}, we first perform M passes through the network, where the m-th pass θˆm randomly deactivates part of neurons. Thus, each example yields M sets of conditional probabilities. The lower variance of translation probabilities reflects higher confidence that the model has with respect to the instance (Dong et al., 2018; Wang et al., 2019). We propose multi-granularity strategies for confidence estimation: n ,θ ˆm ) denotes the variwhere Var{P (yjn |xn , y<j ance of the translation probability with respect to yjn . Similar to sentence-level strategy, the confidence scores of tokens are normalized as: exp(βˆjn ) βjn = PJ , ˆn t=1 exp(βt ) (4) where J indicates the length of target sentence yn . 2.2 Training Strategy A larger confidence score indicates that the current model is confident on the corresponding example. Therefore, the model should learn more from the predicted loss. In order to govern the learning schedule automatic"
2020.emnlp-main.80,P19-1295,1,0.827329,"ng, where NMT model is allowed to 1) automatically quantify the learning confidence over training examples; and 2) flexibly govern its learning via regulating the loss in each iteration step. Experimental results over multiple translation tasks demonstrate that the proposed model yields better performance than strong baselines and those models trained with human-designed curricula on both translation quality and convergence speed.1 1 Introduction Neural machine translation (NMT) has achieved promising results with the use of various optimization tricks (Hassan et al., 2018; Chen et al., 2018; Xu et al., 2019; Li et al., 2020; Yang et al., 2020). In spite of that, these techniques lead to increased training time and massive hyper-parameters, making the development of a well-performed system expensive (Popel and Bojar, 2018; Ott et al., 2018). As an alternative mitigation, curriculum learning (CL, Elman, 1993; Bengio et al., 2009) has shown its effectiveness on speeding up the convergence and stabilizing the NMT model training (Zhang et al., 2018; Platanios et al., 2019). CL teaches NMT model from easy examples to complex ones rather than equally considering all samples, where the keys lie in the d"
2020.emnlp-main.80,2020.acl-main.620,1,0.767745,"l from easy examples to complex ones rather than equally considering all samples, where the keys lie in the definition of “diffi∗ Baosong Yang and Derek F. Wong are co-corresponding authors. Work was done when Yu Wan was interning at DAMO Academy, Alibaba Group. 1 Our codes: https://github.com/NLP2CT/SPL for NMT. culty” and the strategy of curricula design (Krueger and Dayan, 2009; Kocmi and Bojar, 2017). Existing studies artificially determine data difficulty according to prior linguistic knowledge such as sentence length (SL) and word rarity (WR) (Platanios et al., 2019; Zhang et al., 2019; Zhou et al., 2020), and manually tune the learning schedule (Liu et al., 2020; Fomicheva et al., 2020). However, neither there exists a clear distinction between easy and hard examples (Kumar et al., 2010), nor these human intuitions exactly conform to effective model training (Zhang et al., 2018). Instead, we resolve this problem by introducing self-paced learning (Kumar et al., 2010), where the emphasis of learning can be dynamically determined by model itself rather than human intuitions. Specifically, our model measures the level of confidence on each training example (Gal and Ghahramani, 2016; Xiao and Wan"
2021.acl-short.5,E06-1031,0,0.0840458,"0.024 0.374 0.165 -0.063 0.627 -0.043 -0.013 cute Apply to Score Function System 2 T he 0.619 -0.213 0.164 -0.035 cat -0.015 -0.109 0.101 Difficulty Calculation the monkey T he 0.890 0.128 0.246 -0.009 -0.151 cat 0.176 0.589 0.004 -0.239 -0.074 0.302 0.155 cute -0.035 0.085 is 0.218 -0.002 0.822 0.312 0.204 0.868 T he is so cute puppy System 1 System 2 Candidates Figure 1: Illustration of combining difficulty weight with BERTScore. RBERT denotes the vanilla recall-based BERTScore while DA-RBERT denotes the score augmented with translation difficulty. edit-distance based (Snover et al., 2006; Leusch et al., 2006), alignment-based (Banerjee and Lavie, 2005), embedding-based (Zhang et al., 2020; Chow et al., 2019; Lo, 2019) and end-to-end based (Sellam et al., 2020). BLEU (Papineni et al., 2002) is widely used as a vital criterion in the comparison of MT system performance but its reliability has been doubted on entering neural machine translation age (Shterionov et al., 2018; Mathur et al., 2020). Due to the fact that BLEU and its variants only assess surface linguistic features, some metrics leveraging contextual embedding and end-toend training bring semantic information into the evaluation, which fu"
2021.acl-short.5,W05-0909,0,0.783749,"-0.013 cute Apply to Score Function System 2 T he 0.619 -0.213 0.164 -0.035 cat -0.015 -0.109 0.101 Difficulty Calculation the monkey T he 0.890 0.128 0.246 -0.009 -0.151 cat 0.176 0.589 0.004 -0.239 -0.074 0.302 0.155 cute -0.035 0.085 is 0.218 -0.002 0.822 0.312 0.204 0.868 T he is so cute puppy System 1 System 2 Candidates Figure 1: Illustration of combining difficulty weight with BERTScore. RBERT denotes the vanilla recall-based BERTScore while DA-RBERT denotes the score augmented with translation difficulty. edit-distance based (Snover et al., 2006; Leusch et al., 2006), alignment-based (Banerjee and Lavie, 2005), embedding-based (Zhang et al., 2020; Chow et al., 2019; Lo, 2019) and end-to-end based (Sellam et al., 2020). BLEU (Papineni et al., 2002) is widely used as a vital criterion in the comparison of MT system performance but its reliability has been doubted on entering neural machine translation age (Shterionov et al., 2018; Mathur et al., 2020). Due to the fact that BLEU and its variants only assess surface linguistic features, some metrics leveraging contextual embedding and end-toend training bring semantic information into the evaluation, which further improves the correlation with human ju"
2021.acl-short.5,P04-1077,0,0.1918,"nuously being introduced to correlate with human judgements. Unfortunately, cutting-edge MT systems are too close in performance and generation style for such metrics to rank systems. Even for a metric whose correlation is reliable in most cases, empirical research has shown that it poorly correlates with human ratings when evaluating competitive systems (Ma et al., 2019; Mathur et al., 2020), ∗ † 2 Related Work The existing MT evaluation metrics can be categorized into the following types according to their underlying matching sub-units: n-gram based (Papineni et al., 2002; Doddington, 2002; Lin and Och, 2004; Han et al., 2012; Popovi´c, 2015), Equal contribution Corresponding author 26 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 26–32 August 1–6, 2021. ©2021 Association for Computational Linguistics + Diffculty-Aware Machine Translation Evaluation Query from Pairwise Similarity System 1 Reference is 0.299 cute 0.022 So 0.024 0.374 0.165 -0.063 0.627 -0.043 -0.013 cute Apply to Score Function System 2 T he 0.619 -0.213 0.164 -0.035 cat -0.015 -0.109 0.101 Dif"
2021.acl-short.5,2020.acl-main.41,1,0.752831,"lated as PK d(t) = 1 − k=1 maxh∈hk K sim(t, h) DA-RBERT = 1 X d(t) max sim(t, h) h∈h |t |t∈t DA-PBERT = 1 X d(h) max sim(t, h) (4) t∈t |h| (3) h∈h DA-FBERT = 2 · DA-RBERT · DA-PBERT (5) DA-RBERT + DA-PBERT For any h ∈ / t, we simply let d(h) = 1, i.e., retaining the original calculation. The motivation is that the human assessor keeps their initial matching judgement if the test taker produces a unique but reasonable alternative answer. We regard DA-FBERT as the DA-BERTScore in the following part. There are many variants of our proposed method: 1) designing more elaborate difficulty function (Liu et al., 2020; Zhan et al., 2021); 2) applying a smoothing function to the difficulty distribution; and 3) using other kinds of F -score, e.g., F0.5 -score. The aim of this paper is not to explore this whole space but simply to show that a straightforward implementation works well for MT evaluation. (2) An example is shown in Figure 1: the entity “cat” is improperly translated to “monkey” and “puppy”, resulting in a lower pairwise similarity of the token “cat”, which indicates higher translation difficulty. Therefore, by incorporating the translation difficulty into the evaluation process, the token “cat”"
2021.acl-short.5,W19-5358,0,0.0114256,"015 -0.109 0.101 Difficulty Calculation the monkey T he 0.890 0.128 0.246 -0.009 -0.151 cat 0.176 0.589 0.004 -0.239 -0.074 0.302 0.155 cute -0.035 0.085 is 0.218 -0.002 0.822 0.312 0.204 0.868 T he is so cute puppy System 1 System 2 Candidates Figure 1: Illustration of combining difficulty weight with BERTScore. RBERT denotes the vanilla recall-based BERTScore while DA-RBERT denotes the score augmented with translation difficulty. edit-distance based (Snover et al., 2006; Leusch et al., 2006), alignment-based (Banerjee and Lavie, 2005), embedding-based (Zhang et al., 2020; Chow et al., 2019; Lo, 2019) and end-to-end based (Sellam et al., 2020). BLEU (Papineni et al., 2002) is widely used as a vital criterion in the comparison of MT system performance but its reliability has been doubted on entering neural machine translation age (Shterionov et al., 2018; Mathur et al., 2020). Due to the fact that BLEU and its variants only assess surface linguistic features, some metrics leveraging contextual embedding and end-toend training bring semantic information into the evaluation, which further improves the correlation with human judgement. Among them, BERTScore (Zhang et al., 2020) has achieved a"
2021.acl-short.5,W19-5356,0,0.0159203,".164 -0.035 cat -0.015 -0.109 0.101 Difficulty Calculation the monkey T he 0.890 0.128 0.246 -0.009 -0.151 cat 0.176 0.589 0.004 -0.239 -0.074 0.302 0.155 cute -0.035 0.085 is 0.218 -0.002 0.822 0.312 0.204 0.868 T he is so cute puppy System 1 System 2 Candidates Figure 1: Illustration of combining difficulty weight with BERTScore. RBERT denotes the vanilla recall-based BERTScore while DA-RBERT denotes the score augmented with translation difficulty. edit-distance based (Snover et al., 2006; Leusch et al., 2006), alignment-based (Banerjee and Lavie, 2005), embedding-based (Zhang et al., 2020; Chow et al., 2019; Lo, 2019) and end-to-end based (Sellam et al., 2020). BLEU (Papineni et al., 2002) is widely used as a vital criterion in the comparison of MT system performance but its reliability has been doubted on entering neural machine translation age (Shterionov et al., 2018; Mathur et al., 2020). Due to the fact that BLEU and its variants only assess surface linguistic features, some metrics leveraging contextual embedding and end-toend training bring semantic information into the evaluation, which further improves the correlation with human judgement. Among them, BERTScore (Zhang et al., 2020) has"
2021.acl-short.5,W19-5302,0,0.0116951,"eely available at https://github.com/NLP2CT /Difficulty-Aware-MT-Evaluation. 1 Introduction The human labor needed to evaluate machine translation (MT) evaluation is expensive. To alleviate this, various automatic evaluation metrics are continuously being introduced to correlate with human judgements. Unfortunately, cutting-edge MT systems are too close in performance and generation style for such metrics to rank systems. Even for a metric whose correlation is reliable in most cases, empirical research has shown that it poorly correlates with human ratings when evaluating competitive systems (Ma et al., 2019; Mathur et al., 2020), ∗ † 2 Related Work The existing MT evaluation metrics can be categorized into the following types according to their underlying matching sub-units: n-gram based (Papineni et al., 2002; Doddington, 2002; Lin and Och, 2004; Han et al., 2012; Popovi´c, 2015), Equal contribution Corresponding author 26 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 26–32 August 1–6, 2021. ©2021 Association for Computational Linguistics + Diffculty-Aware M"
2021.acl-short.5,W14-3348,0,0.0175457,"ation when K is lower than 6, meaning that the current metrics are ineffective when facing competitive systems. With the help of difficulty weights, the degradation in the correlation is alleviated, e.g., improving τ score from 0.07 to 0.73 for BERTScore (K = 6). These results indicate the effectiveness of our approach, establishing the necessity for adding difficulty. Comparing Metrics In order to compare with the metrics that have different underlying evaluation mechanism, four representative metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), BERTScore (Zhang et al., 2020), which are correspondingly driven by n-gram, edit distance, word alignment and embedding similarity, are involved in the comparison experiments without losing popularity. For ensuring reproducibility, the original12 and widely used implementation3 was used in the experiments. Case Study of Ranking Table 2 presents a case study on the En→De task. Existing metrics consistently select MSRA’s system as the best system, which shows a large divergence from human judgement. DA-BERTScore ranks it the same as human (4th) because most of its translations have low difficu"
2021.acl-short.5,2020.acl-main.448,0,0.160097,"https://github.com/NLP2CT /Difficulty-Aware-MT-Evaluation. 1 Introduction The human labor needed to evaluate machine translation (MT) evaluation is expensive. To alleviate this, various automatic evaluation metrics are continuously being introduced to correlate with human judgements. Unfortunately, cutting-edge MT systems are too close in performance and generation style for such metrics to rank systems. Even for a metric whose correlation is reliable in most cases, empirical research has shown that it poorly correlates with human ratings when evaluating competitive systems (Ma et al., 2019; Mathur et al., 2020), ∗ † 2 Related Work The existing MT evaluation metrics can be categorized into the following types according to their underlying matching sub-units: n-gram based (Papineni et al., 2002; Doddington, 2002; Lin and Och, 2004; Han et al., 2012; Popovi´c, 2015), Equal contribution Corresponding author 26 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 26–32 August 1–6, 2021. ©2021 Association for Computational Linguistics + Diffculty-Aware Machine Translation Eva"
2021.acl-short.5,P02-1040,0,0.121089,"us automatic evaluation metrics are continuously being introduced to correlate with human judgements. Unfortunately, cutting-edge MT systems are too close in performance and generation style for such metrics to rank systems. Even for a metric whose correlation is reliable in most cases, empirical research has shown that it poorly correlates with human ratings when evaluating competitive systems (Ma et al., 2019; Mathur et al., 2020), ∗ † 2 Related Work The existing MT evaluation metrics can be categorized into the following types according to their underlying matching sub-units: n-gram based (Papineni et al., 2002; Doddington, 2002; Lin and Och, 2004; Han et al., 2012; Popovi´c, 2015), Equal contribution Corresponding author 26 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 26–32 August 1–6, 2021. ©2021 Association for Computational Linguistics + Diffculty-Aware Machine Translation Evaluation Query from Pairwise Similarity System 1 Reference is 0.299 cute 0.022 So 0.024 0.374 0.165 -0.063 0.627 -0.043 -0.013 cute Apply to Score Function System 2 T he 0.619 -0.213 0.1"
2021.acl-short.5,2020.emnlp-main.5,0,0.0151027,"ity “cat” is improperly translated to “monkey” and “puppy”, resulting in a lower pairwise similarity of the token “cat”, which indicates higher translation difficulty. Therefore, by incorporating the translation difficulty into the evaluation process, the token “cat” is more contributive while the other words like “cute” are less important in the overall score. 4 Experiments Data The WMT19 English↔German (En↔De) evaluation tasks are challenging due to the large discrepancy between human and automated assessments in terms of reporting the best system (Bojar et al., 2018; Barrault et al., 2019; Freitag et al., 2020). To sufficiently validate the effectiveness of Score Function Due to the fact that the translation generated by a current NMT model is fluent enough but not adequate yet, F -score which takes into account the Precision and Recall, is more appropriate to aggregate the matching scores, instead 28 SYSTEM BLEU ↑ TER ↓ METEOR ↑ BERTScore ↑ DA-BERTScore ↑ HUMAN ↑ Facebook.6862 Microsoft.sd.6974 Microsoft.dl.6808 MSRA.6926 UCAM.6731 NEU.6763 0.4364 (⇓5) 0.4477 (⇓1) 0.4483 (⇑1) 0.4603 (⇑3) 0.4413 (X0) 0.4460 (⇑2) 0.4692 (⇓5) 0.4583 (⇓1) 0.4591 (⇓1) 0.4504 (⇑3) 0.4636 (X0) 0.4563 (⇑4) 0.6077 (⇓3) 0.60"
2021.acl-short.5,W15-3049,0,0.0505166,"Missing"
2021.acl-short.5,2020.acl-main.704,0,0.0219251,"ulation the monkey T he 0.890 0.128 0.246 -0.009 -0.151 cat 0.176 0.589 0.004 -0.239 -0.074 0.302 0.155 cute -0.035 0.085 is 0.218 -0.002 0.822 0.312 0.204 0.868 T he is so cute puppy System 1 System 2 Candidates Figure 1: Illustration of combining difficulty weight with BERTScore. RBERT denotes the vanilla recall-based BERTScore while DA-RBERT denotes the score augmented with translation difficulty. edit-distance based (Snover et al., 2006; Leusch et al., 2006), alignment-based (Banerjee and Lavie, 2005), embedding-based (Zhang et al., 2020; Chow et al., 2019; Lo, 2019) and end-to-end based (Sellam et al., 2020). BLEU (Papineni et al., 2002) is widely used as a vital criterion in the comparison of MT system performance but its reliability has been doubted on entering neural machine translation age (Shterionov et al., 2018; Mathur et al., 2020). Due to the fact that BLEU and its variants only assess surface linguistic features, some metrics leveraging contextual embedding and end-toend training bring semantic information into the evaluation, which further improves the correlation with human judgement. Among them, BERTScore (Zhang et al., 2020) has achieved a remarkable performance across MT evaluation"
2021.acl-short.5,C12-2044,1,0.783342,"duced to correlate with human judgements. Unfortunately, cutting-edge MT systems are too close in performance and generation style for such metrics to rank systems. Even for a metric whose correlation is reliable in most cases, empirical research has shown that it poorly correlates with human ratings when evaluating competitive systems (Ma et al., 2019; Mathur et al., 2020), ∗ † 2 Related Work The existing MT evaluation metrics can be categorized into the following types according to their underlying matching sub-units: n-gram based (Papineni et al., 2002; Doddington, 2002; Lin and Och, 2004; Han et al., 2012; Popovi´c, 2015), Equal contribution Corresponding author 26 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 26–32 August 1–6, 2021. ©2021 Association for Computational Linguistics + Diffculty-Aware Machine Translation Evaluation Query from Pairwise Similarity System 1 Reference is 0.299 cute 0.022 So 0.024 0.374 0.165 -0.063 0.627 -0.043 -0.013 cute Apply to Score Function System 2 T he 0.619 -0.213 0.164 -0.035 cat -0.015 -0.109 0.101 Difficulty Calculatio"
2021.acl-short.5,2006.amta-papers.25,0,0.561195,"s 0.299 cute 0.022 So 0.024 0.374 0.165 -0.063 0.627 -0.043 -0.013 cute Apply to Score Function System 2 T he 0.619 -0.213 0.164 -0.035 cat -0.015 -0.109 0.101 Difficulty Calculation the monkey T he 0.890 0.128 0.246 -0.009 -0.151 cat 0.176 0.589 0.004 -0.239 -0.074 0.302 0.155 cute -0.035 0.085 is 0.218 -0.002 0.822 0.312 0.204 0.868 T he is so cute puppy System 1 System 2 Candidates Figure 1: Illustration of combining difficulty weight with BERTScore. RBERT denotes the vanilla recall-based BERTScore while DA-RBERT denotes the score augmented with translation difficulty. edit-distance based (Snover et al., 2006; Leusch et al., 2006), alignment-based (Banerjee and Lavie, 2005), embedding-based (Zhang et al., 2020; Chow et al., 2019; Lo, 2019) and end-to-end based (Sellam et al., 2020). BLEU (Papineni et al., 2002) is widely used as a vital criterion in the comparison of MT system performance but its reliability has been doubted on entering neural machine translation age (Shterionov et al., 2018; Mathur et al., 2020). Due to the fact that BLEU and its variants only assess surface linguistic features, some metrics leveraging contextual embedding and end-toend training bring semantic information into th"
2021.emnlp-main.663,2020.emnlp-main.175,0,0.0446963,"Missing"
2021.emnlp-main.663,D17-1159,0,0.033541,"tained without resorting to external resources and has been proven beneficial to sentence modeling (Yang et al., 2018; Xu et al., 2019). For each word xm i , we add m } and (xm , xm }. This two edges (xm , x i i+1 i i−1 means we add links from the current word to its adjacent words. • Dependency directly models syntactic and semantic relations between two words in a sentence. Dependency relations not only provide linguistic meanings but also allow connections between words with a longer distance. Previous practices have shown that dependency relations enhance representation learning of words (Marcheggiani and Titov, 2017; Strubell et al., 2018; Lin et al., 2019). Given a dependency tree of the sentence and m m a word xm i , we add a graph edge (xi , xj ) if m xm i is a headword of xj . Graph Rep. Hg0, . . . , Hgm, . . . , Hgm Sentence Emb. E 0, . . . , E m, . . . , E M S0 Sentence Rep. GCN S 0 … … Sm S m 其实 他的 名字 SM SM 我 我 ⽶格尔 L× … … 看着 说 ⽶格尔 ⽶格尔 … … Input Figure 2: Illustration of the proposed document graph encoder. L in this paper is set to 2. It helps understand the logic and structure of the document and resolve the ambiguities. In n this paper we add a graph edge (xm i , xj ) if m n xi is a referent of"
2021.emnlp-main.663,P18-1118,0,0.0729723,"ang et al., 2018; Yang et al., 2019). Different from representation-based approaches, Tu et al. (2018) and Kuang et al. (2018) propose using a cache to memorize context information, which can be either history hidden states or lexicons. To keep tracking of most recent contexts, the cache is updated when new translations are generated. Therefore, long-distance contexts would likely be erased. How to use long-distance contexts is drawing attention in recent years. Approaches, like treating the whole document as a long sentence (JunczysDowmunt, 2019) and using memory and hierarchical structures (Maruf and Haffari, 2018; Maruf et al., 2019; Tan et al., 2019), are proposed to take global contexts into consideration. However, Kim et al. (2019) point out that not all the words in a document are beneficial to context integration, suggesting that it is essential for each word to focus on its own relevant context. 1 Dependency and coreference relations are from Stanford 8435 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8435–8448 c November 7–11, 2021. 2021 Association for Computational Linguistics To address this problem, we suppose to build a document graph for a d"
2021.emnlp-main.663,N19-1313,0,0.278191,"al., 2019). Different from representation-based approaches, Tu et al. (2018) and Kuang et al. (2018) propose using a cache to memorize context information, which can be either history hidden states or lexicons. To keep tracking of most recent contexts, the cache is updated when new translations are generated. Therefore, long-distance contexts would likely be erased. How to use long-distance contexts is drawing attention in recent years. Approaches, like treating the whole document as a long sentence (JunczysDowmunt, 2019) and using memory and hierarchical structures (Maruf and Haffari, 2018; Maruf et al., 2019; Tan et al., 2019), are proposed to take global contexts into consideration. However, Kim et al. (2019) point out that not all the words in a document are beneficial to context integration, suggesting that it is essential for each word to focus on its own relevant context. 1 Dependency and coreference relations are from Stanford 8435 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8435–8448 c November 7–11, 2021. 2021 Association for Computational Linguistics To address this problem, we suppose to build a document graph for a document, where each"
2021.emnlp-main.663,D18-1325,0,0.320702,"… … Figure 1: The structure of graph. Solid lines in blue depict adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al."
2021.emnlp-main.663,N19-4009,0,0.0419119,"Missing"
2021.emnlp-main.663,P02-1040,0,0.111876,"ntences. (3) We adopted the WMT19 document-level corpus published by Scherrer et al. (2019) for the En-De translation task. This data contains 2.9M parallel sentences with document boundaries and 10.3M back-translated sentence pairs. All data are tokenized and segmented into subword units using the byte-pair encoding (Sennrich et al., 2016). We apply 32k merge steps for each language on En-Fr, En-Ru, En-De tasks, and 30k for Zh-En task. As a node in a document graph represents a word rather than its subwords, we average embeddings of the subwords as the embedding of the node. The 4-gram BLEU (Papineni et al., 2002) is used as the evaluation metric. only train additional parameters introduced by our methods. We set the layers of the document graph encoder to 2 and share their parameters3 . To compare our graph-based method with prior works, we reimplement several document-level baselines on the T RANSFORMER architecture and replace their context modules with ours (Please refer to Supplementary on details): • C TX (Zhang et al., 2018) employs an additional encoder to learn context representations, which are then integrated by cross-attention mechanisms. • H AN (Miculicich et al., 2018) uses a hierarchical"
2021.emnlp-main.663,D19-6506,0,0.0176526,"marks with different corpus size: (1) IWSLT En–Fr and Zh–En translation tasks (Cettolo et al., 2012) with around 200K sentence pairs for training. Following convention (Wang et al., 2017; Miculicich et al., 2018; Zhang et al., 2018), both language pairs take dev2010 as the development set. tst2010 is used for testing on En–Fr and tst2010∼tst2013 on Zh–En. (2) Opensubtitle2018 En–Ru translation corpus released by Voita et al. (2018), which contains 6M sentence pairs for training, among which 1.5M sentence pairs have context sentences. (3) We adopted the WMT19 document-level corpus published by Scherrer et al. (2019) for the En-De translation task. This data contains 2.9M parallel sentences with document boundaries and 10.3M back-translated sentence pairs. All data are tokenized and segmented into subword units using the byte-pair encoding (Sennrich et al., 2016). We apply 32k merge steps for each language on En-Fr, En-Ru, En-De tasks, and 30k for Zh-En task. As a node in a document graph represents a word rather than its subwords, we average embeddings of the subwords as the embedding of the node. The 4-gram BLEU (Papineni et al., 2002) is used as the evaluation metric. only train additional parameters i"
2021.emnlp-main.663,W04-3250,0,0.149933,"+0.61 19.56↑ +0.27 41.54 +1.07 20.02⇑ +0.62 41.89↑ Pre-integration − 19.01 − 41.35 +0.99 20.00⇑ +0.52 41.87 +1.45 20.46⇑ +0.98 42.33⇑ 4 − 4 − En-Ru Test 31.98 Para. 4 Speed - 24.9k 31.27 +0.92 +0.93 31.95 32.87⇑ 32.88⇑ 22.06M 21.01M 21.01M 16.3k 17.7K 17.0K − − − −0.07 +0.01 32.07 32.36 32.54 32.47 32.55 7.30 7.36 8.39 6.27 6.27 M M M M M 19.9k 14.4k 7.7 k 19.7K 18.9K − +0.45 +0.47 32.44 32.89↑ 32.91↑ 0.01 M 5.27 M 6.27 M 19.6K 19.7K 18.5K Table 1: Main results (BLEU) on IWSLT Zh–En and EN–FR, WMT19 En–De, and Opensubtitle2018 En–Ru translation tasks. “↑ / ⇑” denotes significant improvement (Koehn, 2004) over the best baseline model with context on each task at p < 0.05/0.01, respectively. The models in bold are selected to merge with our document graph methods. “Para.” and “Speed” indicate the model size (M = million) and training speed (tokens/second), respectively. ∗ denotes that the model considers the target context. Ablation Relations Comp. Model BASE +A DJACENCY +D EPENDENCY +L EXICAL +C OREFERENCE +I NTRA +I NTER +A LL Dev 29.75 30.50 30.75 30.68 30.49 30.95 30.89 31.79 Test 36.93 37.69 37.81 37.78 37.54 38.04 37.97 38.94 Table 2: Ablation study of source graph variants on IWSLT En-Fr"
2021.emnlp-main.663,P16-1162,0,0.0583485,"rs take dev2010 as the development set. tst2010 is used for testing on En–Fr and tst2010∼tst2013 on Zh–En. (2) Opensubtitle2018 En–Ru translation corpus released by Voita et al. (2018), which contains 6M sentence pairs for training, among which 1.5M sentence pairs have context sentences. (3) We adopted the WMT19 document-level corpus published by Scherrer et al. (2019) for the En-De translation task. This data contains 2.9M parallel sentences with document boundaries and 10.3M back-translated sentence pairs. All data are tokenized and segmented into subword units using the byte-pair encoding (Sennrich et al., 2016). We apply 32k merge steps for each language on En-Fr, En-Ru, En-De tasks, and 30k for Zh-En task. As a node in a document graph represents a word rather than its subwords, we average embeddings of the subwords as the embedding of the node. The 4-gram BLEU (Papineni et al., 2002) is used as the evaluation metric. only train additional parameters introduced by our methods. We set the layers of the document graph encoder to 2 and share their parameters3 . To compare our graph-based method with prior works, we reimplement several document-level baselines on the T RANSFORMER architecture and repla"
2021.emnlp-main.663,N19-1238,0,0.0553695,"Missing"
2021.emnlp-main.663,D18-1548,0,0.0200062,"ternal resources and has been proven beneficial to sentence modeling (Yang et al., 2018; Xu et al., 2019). For each word xm i , we add m } and (xm , xm }. This two edges (xm , x i i+1 i i−1 means we add links from the current word to its adjacent words. • Dependency directly models syntactic and semantic relations between two words in a sentence. Dependency relations not only provide linguistic meanings but also allow connections between words with a longer distance. Previous practices have shown that dependency relations enhance representation learning of words (Marcheggiani and Titov, 2017; Strubell et al., 2018; Lin et al., 2019). Given a dependency tree of the sentence and m m a word xm i , we add a graph edge (xi , xj ) if m xm i is a headword of xj . Graph Rep. Hg0, . . . , Hgm, . . . , Hgm Sentence Emb. E 0, . . . , E m, . . . , E M S0 Sentence Rep. GCN S 0 … … Sm S m 其实 他的 名字 SM SM 我 我 ⽶格尔 L× … … 看着 说 ⽶格尔 ⽶格尔 … … Input Figure 2: Illustration of the proposed document graph encoder. L in this paper is set to 2. It helps understand the logic and structure of the document and resolve the ambiguities. In n this paper we add a graph edge (xm i , xj ) if m n xi is a referent of xj given by coreference"
2021.emnlp-main.663,C18-1050,0,0.0465172,"re of graph. Solid lines in blue depict adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra con"
2021.emnlp-main.663,D18-1512,0,0.0276142,"adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean et al., 2015; Zhang e"
2021.emnlp-main.663,D19-1168,0,0.269716,"al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean et al., 2015; Zhang et al., 2018; Yang et al., 2019). Different from representation-based approaches, Tu et al. (2018) and Kuang et al. (2018) propose using a cache to memorize context information, which can be either history hidden states or lexicons. To keep tracking of most recent contexts, the cache is updated when new translations are generated. Therefore, long-distance contexts would likely be erased. How to use long-distance contexts is drawing attention in recent years. Approaches, like treating the whole document as a long sentence (JunczysDowmunt, 201"
2021.emnlp-main.663,W17-4811,0,0.147345,"look at ⽶格尔 Miguel S 我 I 说 say ⽶格尔 Miguel 名字 name … … Figure 1: The structure of graph. Solid lines in blue depict adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicic"
2021.emnlp-main.663,2020.acl-main.322,0,0.0188567,"iterative phrases. 3) Ell. tests whether models correctly predict ellipsis verb phrases or the morphology of words. The Discourse test set consists of two probing tasks on En–Fr: 1) Coref. aims to test whether the gender of an anaphoric pronoun (it or they) is coherent with the previous sentence. 2) Cohe. is a set of ambiguous examples whose correct translations rely on the context. Result on Discourse Phenomena As shown in Table 5, all the context-aware models comprehensively improve the performance on discourse phenomena over the context-agnostic BASE model. Results on the the N OISE model (Li et al., 2020) indicate that the improvement is not merely because of robust training. Compared to prior contextaware models, our model achieves the best accuracy on all tasks. Especially on the Lex., Coref. and Cohe. tasks, our model outperforms others over two points. Note that on the ellipsis task graph edges are usually missing for elided verb phrases. For example, given the following source sentence and its context (Voita et al., 2019b), the verbs “told” and “did” are not directly connected in our graph but indirectly connected via the coreference relation of their neighbors “Nick” and “he”. Hence, our"
2021.emnlp-main.663,Q18-1029,0,0.123816,"Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean et al., 2015; Zhang et al., 2018; Yang"
2021.emnlp-main.663,2020.acl-main.321,0,0.0865574,"enote both the attention and gated residual mechanisms. In this paper, the Context-Attn sublayer is used in three different ways, as shown in Figure 3: • Hyb-integration: integrates the contextual information with an additional Context-Attn layer inside each encoder layer (Zhang et al., 2018). • Post-integration: aggregates the contextual information by adding a Context-Attn layer after the encoder (Tan et al., 2019; Miculicich et al., 2018; Maruf et al., 2019). • Pre-integration: interpolates the context representation before the encoder, which can be considered as the hierarchical embedded (Ma et al., 2020). 3 Experiments Data We evaluate our approach on translation benchmarks with different corpus size: (1) IWSLT En–Fr and Zh–En translation tasks (Cettolo et al., 2012) with around 200K sentence pairs for training. Following convention (Wang et al., 2017; Miculicich et al., 2018; Zhang et al., 2018), both language pairs take dev2010 as the development set. tst2010 is used for testing on En–Fr and tst2010∼tst2013 on Zh–En. (2) Opensubtitle2018 En–Ru translation corpus released by Voita et al. (2018), which contains 6M sentence pairs for training, among which 1.5M sentence pairs have context sente"
2021.emnlp-main.663,D19-1081,0,0.119533,"en denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean et al., 2015; Zhang et al., 2018; Yang et al., 2019). Diff"
2021.emnlp-main.663,P19-1116,0,0.123822,"en denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean et al., 2015; Zhang et al., 2018; Yang et al., 2019). Diff"
2021.emnlp-main.663,P18-1117,0,0.0800035,"ines in blue depict adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hierarchical networks (Miculicich et al., 2018; Wang et al., 2017; Tan et al., 2019) or extra context encoders (Jean"
2021.emnlp-main.663,D17-1301,1,0.892888,"ly 他的 his S 我 I 看着 look at ⽶格尔 Miguel S 我 I 说 say ⽶格尔 Miguel 名字 name … … Figure 1: The structure of graph. Solid lines in blue depict adjacency relations. Dash lines in green denote dependency relations. Lexical consistency is represented as dashed lines in red. The brown line means a coreference relation. S denotes Sentence node. We just show aspects of sentences for convenience.1 Introduction Although neural machine translation (NMT) has achieved great success on sentence-level translation tasks, many studies pointed out that translation mistakes become more noticeable at the documentlevel (Wang et al., 2017; Tiedemann and Scherrer, 2017; Zhang et al., 2018; Miculicich et al., 2018; Kuang et al., 2018; Voita et al., 2018; Läubli et al., 2018; Tu et al., 2018; Voita et al., 2019b; Kim et al., 2019; Yang et al., 2019). They proved that these mistakes can be alleviated by feeding the contexts into context-agnostic NMT models. Previous works have explored various methods to integrate context information into NMT models. ∗ Work was done when Mingzhou Xu was interning at Noah’s Ark Lab. They usually take a limited number of previous sentences as contexts and learn context-aware representations using hi"
2021.findings-acl.373,2020.acl-main.705,0,0.0141674,"copying penalty can play a greater role and bring a significant performance boost. This also verifies the effectiveness of the copying penalty. 4 4.1 Related Work tures which are then fed into NMT models; and 2) parameter initialization, where part/all of the parameters of an NMT model are initialized by a pre-trained model and then training the model on downstream datasets (i.e., parallel corpus). About knowledge extraction, Yang et al. (2020a) and Zhu et al. (2020) explore enhancing encoder and decoder representations by leveraging pretrained BERT models (Devlin et al., 2019). In addition, Chen et al. (2020) distill the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample,"
2021.findings-acl.373,N19-1423,0,0.355934,"ussein Tantawi war anwesend. Table 1: Training objective gap between Seq2Seq LM pre-training and NMT training. LM learns to reconstruct a few source tokens and copy most of them, while NMT learns more translation rather than copying. Underlines denote artificial noises, and highlights indicate expected copying tokens. 2020). As a range of surface, syntactic and semantic information has been encoded in the initialized parameters (Jawahar et al., 2019; Goldberg, 2019), they are expected to bring benefits to NMT models and hence the translation quality. Introduction Self-supervised pre-training (Devlin et al., 2019; Song et al., 2019), which acquires general knowledge from a large amount of unlabeled data to help better and faster learning downstream tasks, has an intuitive appeal for neural machine translation (NMT; Bahdanau et al., 2015; Vaswani et al., 2017). One direct way to utilize pre-trained knowledge is initializing the NMT model with a pre-trained language model (LM) before training it on parallel data (Conneau and Lample, 2019; Liu et al., ∗ Work was done when Xuebo Liu and Liang Ding were interning at Tencent AI Lab. However, there is a discrepancy between the training objective of sequence-"
2021.findings-acl.373,2021.acl-long.266,1,0.684327,"of different copying penalties in P RETRAINED. Penalizing copying (i.e., α &lt; 1 ) brings benefits to the translations of various sources. Translating source original sentences is more sensitive to copying behaviors, leading to a larger score degradation when encouraging copying (i.e., α &gt; 1 ). greater than 1, which verifies our claim. 3.3 Out-of-domain Robustness Improving out-of-domain (OOD) robustness is one of the benefits of pre-training for NLP tasks (Hendrycks et al., 2020; Tu et al., 2020), but the OOD sentences usually contain some lowfrequency proper nouns which are hard to translate (Ding et al., 2021). In this part, we take the first step towards understanding how pre-training affects the OOD robustness of NMT models. Setup We followed M¨uller et al. (2020) to preprocess all the used data sets.8 We served the medical domain as the training domain (i.e., using the data from the medical domain for model training and validation), which consists of 1.1M training examples and 2,000 validation examples. The test set of the medical domain contains 1,691 examples, while the test sets of the IT, Koran, law, and subtitle domains are with 2,000 examples respectively. For training R ANDOM, we used the"
2021.findings-acl.373,2020.emnlp-main.6,0,0.476957,"Missing"
2021.findings-acl.373,P16-1154,0,0.027074,", 2019; Liu et al., 2019; Yang et al., 2019). Compared with training from scratch, fine-tuning a pre-trained model on downstream datasets usually pushes state-of-the-art performances, while reducing computational and labeling costs. Previous studies mainly investigate the effect of pre-training on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even t"
2021.findings-acl.373,2020.acl-main.244,0,0.0313561,"Missing"
2021.findings-acl.373,P19-1356,0,0.0328339,"Missing"
2021.findings-acl.373,W18-2709,0,0.0142241,"wles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding of copying behaviors in NMT models. We observe that the translation of proper nouns in the source original text contains more copying tokens, which sheds light upon future works. 5 Conclusion and Future Work We find that NMT models with pre-training are prone to generate more copying tokens. We introduce a copying ratio and a copying error rate to quantitatively analyze copying behaviors in NMT evaluation. In addition, a simple and effective copying penalty is proposed to enhance the copying behaviors during model inference. Experimental results prove th"
2021.findings-acl.373,D18-1339,0,0.0316042,"is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding of copying behaviors in NMT models. We observe that the translation of proper nouns in the source original text c"
2021.findings-acl.373,W17-3204,0,0.0160039,"raining on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah"
2021.findings-acl.373,D11-1034,0,0.0335393,"Missing"
2021.findings-acl.373,2020.acl-main.703,0,0.0349923,"re-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answerin"
2021.findings-acl.373,2020.emnlp-main.210,0,0.43631,"NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answering and natural lang"
2021.findings-acl.373,2020.tacl-1.47,0,0.0560822,"Missing"
2021.findings-acl.373,2021.ccl-1.108,0,0.0281972,"Missing"
2021.findings-acl.373,2020.amta-research.14,0,0.0673361,"Missing"
2021.findings-acl.373,W19-5333,0,0.0331562,"Missing"
2021.findings-acl.373,N19-4009,0,0.0143054,"of pre-training on NMT in the perspective of copying behaviors. We expect to provide more evidence for controlling the copying behaviors of NMT models. 2.1 Experimental Setup Data We conducted experiments on the widelyused WMT14 English-German benchmark. We used the processed data provided by Vaswani et al. (2017), which consists of 4.5M sentence pairs.1 We used all the training data for model training. The validation set is newstest2013 of 3,000 examples and the test set is newstest2014 of 3,003 examples. Models and Settings We implemented all the models by the open-sourced toolkit fairseq (Ott et al., 2019).2 We used 8 V100 GPUs for the experiments. We mainly compared two models: 1) R ANDOM, which is a vanilla NMT model whose weights are randomly initialized without pre-training; and 2) P RETRAINED, an NMT model using the weights of pre-trained mBART.cc253 for parameter initialization, which has shown its usability and reliability for translation tasks (Tran et al., 2020; Tang et al., 2020). For the training of R ANDOM, we used the Transformer big setting of Ott et al. (2018b) with a huge training batch size of 460K tokens.4 For P RETRAINED, we fine-tuned on the pre-trained mBART.cc25 with a tra"
2021.findings-acl.373,W18-6301,0,0.122055,"is newstest2014 of 3,003 examples. Models and Settings We implemented all the models by the open-sourced toolkit fairseq (Ott et al., 2019).2 We used 8 V100 GPUs for the experiments. We mainly compared two models: 1) R ANDOM, which is a vanilla NMT model whose weights are randomly initialized without pre-training; and 2) P RETRAINED, an NMT model using the weights of pre-trained mBART.cc253 for parameter initialization, which has shown its usability and reliability for translation tasks (Tran et al., 2020; Tang et al., 2020). For the training of R ANDOM, we used the Transformer big setting of Ott et al. (2018b) with a huge training batch size of 460K tokens.4 For P RETRAINED, we fine-tuned on the pre-trained mBART.cc25 with a training batch size of 131K tokens. The hyperparameters keep the same with R ANDOM except the 0.2 label smoothing, 2500 warm-up steps, and 1e-4 maximum learning rate. Evaluation For each model, we selected the checkpoint with the lowest perplexity on the validation set for testing. The beam size is 5 and the length penalty is 0.6. In addition to report1 https://drive.google.com/uc?id=0B_ bZck-ksdkpM25jRUN2X2UxMm8 2 https://github.com/pytorch/fairseq 3 https://github.com/pytor"
2021.findings-acl.373,P02-1040,0,0.11011,"rseq/ blob/master/examples/scaling_nmt/README. md#3-train-a-model 4266 Source Target R ANDOM P RETRAINED Military ruler Field Marshal Hussein Tantawi was in attendance. Der Milit¨arf¨uhrer Feldmarschall Hussein Tantawi war anwesend. Performance O RACLE R ANDOM P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of"
2021.findings-acl.373,N18-1202,0,0.00927479,"020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of further identifying the side-effect from pre-training. Pre-Training for NMT Recently, pre-training has been shown useful for transferring general knowledge to specific downstream tasks, including text classification, question answering and natural language inference (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019). Compared with training from scratch, fine-tuning a pre-trained model on downstream datasets usually pushes state-of-the-art performances, while reducing computational and labeling costs. Previous studies mainly investigate the effect of pre-training on NMT from two perspectives: 1) knowledge extraction, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monoli"
2021.findings-acl.373,W18-6319,0,0.0179225,"P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of “copying token” by comparing each input and output sentence pair. The denominator is the total number of tokens in output sentences. In general, higher Ratio values indicate more copying behaviors produced by the NMT model, and vice versa. Copying E"
2021.findings-acl.373,2020.tacl-1.40,0,0.0209978,"ying errors and thus the BLEU scores get a sharp degradation when setting the copying penalty Figure 3: BLEU scores of different copying penalties in P RETRAINED. Penalizing copying (i.e., α &lt; 1 ) brings benefits to the translations of various sources. Translating source original sentences is more sensitive to copying behaviors, leading to a larger score degradation when encouraging copying (i.e., α &gt; 1 ). greater than 1, which verifies our claim. 3.3 Out-of-domain Robustness Improving out-of-domain (OOD) robustness is one of the benefits of pre-training for NLP tasks (Hendrycks et al., 2020; Tu et al., 2020), but the OOD sentences usually contain some lowfrequency proper nouns which are hard to translate (Ding et al., 2021). In this part, we take the first step towards understanding how pre-training affects the OOD robustness of NMT models. Setup We followed M¨uller et al. (2020) to preprocess all the used data sets.8 We served the medical domain as the training domain (i.e., using the data from the medical domain for model training and validation), which consists of 1.1M training examples and 2,000 validation examples. The test set of the medical domain contains 1,691 examples, while the test se"
2021.findings-acl.373,2020.tacl-1.18,0,0.136916,"ll the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Transformer encoder (e.g., BERT) or decoder (e.g., GPT (Radford et al., 2018)), the parameters of encoder and decoder can be independently initialized (Conneau and Lample, 2019; Rothe et al., 2020). For the pre-trained model building upon the encoder-decoder architecture (Sutskever et al., 2014), all the model parameters can be directly inherited by NMT, which is easy to use and effective (Song et al., 2019; Lewis et al., 2020; Lin et al., 2020; Yang et al., 2020b). In general, most previous works focus on designing novel pre-training methods and architectures to boost the model performance of NMT, but the understanding of pre-training for NMT is still limited. This paper improves pre-training for NMT by first understanding its weakness in copying behavior, revealing the importance of f"
2021.findings-acl.373,W16-2323,0,0.0591289,"tion, where a fixed pre-trained model is used to encode input sequences into fea4.2 Copying Behaviors of NMT It is a common behavior in Seq2Seq models to copy source tokens to the target sentences, especially in monolingual generation tasks. For example, Gu et al. (2016) propose a copying mechanism to explicitly help model learn copying predictions, showing its effectiveness in the tasks of dialogue and summarization. The copying behaviors also exist in NMT, particularly in languages that share some alphabets (e.g., English and German). Koehn and Knowles (2017) observe that subword-based NMT (Sennrich et al., 2016) outperforms statistical machine translation when translating/copying unknown words. 4272 Knowles and Koehn (2018) find that NMT is able to translate source words in specific contexts via copying (e.g., personal names followed by “Mrs.”), and even these are unknown words. However, too many copying signals (i.e., source and target sentences are identical) in training data may lead to one potential threat: NMT models prefer copying source tokens instead of translating them, resulting in performance degradation (Ott et al., 2018a; Khayrallah and Koehn, 2018). This paper broadens the understanding"
2021.findings-acl.373,2006.amta-papers.25,0,0.166465,"6 Source Target R ANDOM P RETRAINED Military ruler Field Marshal Hussein Tantawi was in attendance. Der Milit¨arf¨uhrer Feldmarschall Hussein Tantawi war anwesend. Performance O RACLE R ANDOM P RETRAINED Anwesend war der Milit¨armachthaber Feldmarschall Hussein Tantawi. Milit¨arischer Feldherr Marshal Hussein Tantawi war anwesend. Table 2: Translation from English to German. The words in color denote the copying tokens of which blue denotes right copies and red denotes copying errors. ing the commonly-used 4-gram BLEU score (Papineni et al., 2002), we also report Translation Error Rate (TER) (Snover et al., 2006) to better capture the translation performance of unigrams, which more directly reflects the copying behaviors of NMT models. Both the scores are calculated by sacrebleu (Post, 2018) with de-tokenized text and unmodified references.5,6 2.2 Copying Ratio Ratio To measure the extent of the copying behaviors in NMT models, we calculate the ratio of copying tokens in translation outputs: PI count(copying token) Ratio = i=1 (1) PI i=1 count(token) where I denotes the total number of sentences in the test set. We count the number of “copying token” by comparing each input and output sentence pair. T"
2021.findings-acl.373,N03-1033,0,0.0595807,"Missing"
2021.findings-acl.373,2020.emnlp-main.208,0,0.123844,"setting CP to 1.2). One possible reason is that the IT domain needs to copy more tokens from the source sentence than translating sentences from other domains, thus the copying penalty can play a greater role and bring a significant performance boost. This also verifies the effectiveness of the copying penalty. 4 4.1 Related Work tures which are then fed into NMT models; and 2) parameter initialization, where part/all of the parameters of an NMT model are initialized by a pre-trained model and then training the model on downstream datasets (i.e., parallel corpus). About knowledge extraction, Yang et al. (2020a) and Zhu et al. (2020) explore enhancing encoder and decoder representations by leveraging pretrained BERT models (Devlin et al., 2019). In addition, Chen et al. (2020) distill the soft labels from BERT to improve predictions for NMT. These methods are effective but costly because the novel NMT architecture needed to be carefully designed and the computation graph has to store the parameters of both the pre-trained model and NMT model. About parameter initialization, pre-trained models in different architectures have been studied. For the pre-trained model whose architecture is similar to Tr"
2021.findings-acl.373,W19-5208,0,0.238508,"Missing"
2021.findings-emnlp.247,W19-5206,0,0.177765,"T and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and EnglishRussian benchmarks show that PT can nicely cowork with BT, leading to state-of-the-art model performances. Extensive analyses show that the tagging mechanism is helpful for enhancing the complementarity between PT and BT by improving the translation of source-original sentences and low-frequency words. Our main contributions are as follows: • We design two probing tasks to investigate the impact of PT and BT on NMT models. • We empirically demonstrate the complementarity between PT and BT. • We show that Tagged BT further improve"
2021.findings-emnlp.247,2021.emnlp-main.263,1,0.833334,"Missing"
2021.findings-emnlp.247,D18-1045,0,0.452877,"Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the"
2021.findings-emnlp.247,2020.acl-main.253,0,0.0509777,"Missing"
2021.findings-emnlp.247,D18-1040,0,0.0187186,"l parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences between PT and BT sity and complexity in synthetic data, showing that adding symbols (e.g., noises and tags) to the back- on improving model performance. We design two probing tasks to study the research question: Which translated source can help NMT distinguish the data from various sources and learn better represen- module of NMT do PT and BT respectively play a tations (Fadaee and Monz, 2018; Wang et al., 2019; greater role in enhancing translation quality? Edunov et al., 2018; Caswell et al., 2019; Marie 3.1 Effects of PT on NMT et al., 2020). The claims and understandings from these works are chiefly at the data-level rather than Given a pre-trained model, it is common to use its the model-level. part or all parameters to initialize the downstream There also exists some works that combine tasks. We design four NMT models, which differ PT and BT to further boost the model perfor- from the NMT components (Encoder vs. Decoder) mance (Conneau et al., 2020; Song et al., 2019; with p"
2021.findings-emnlp.247,2020.emnlp-main.6,0,0.0112886,"ed source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and BT improves the model performance, while adding tags to BT data further improves Liu et al., 2021a; Wang et al., 2021).4 Generally speaking, the translation of Src-Ori is more important than that of Tgt-Ori for practical NMT systems (Graham et al., 2020), thus its performance should be taken seriously. As shown in Table 3, PT performs better on Src-Ori than BT (33.8 vs. 31.9 BLEU) while BT achieves higher scores on TgtOri than PT (45.6 vs. 42.0 BLEU). Besides, simply combining PT and BT can improve the translation quality on both Src-Ori and Tgt-Ori sentences, but the improvement of Src-Ori is lower than only using PT. By introducing tagged BT, the model can achieve better performance than the simple one, especially on source-original sentences. Takeaway: 1) PT and BT complementary in terms of originality of sentences; 2) Tagged BT can allevi"
2021.findings-emnlp.247,2021.acl-long.221,1,0.810105,"Missing"
2021.findings-emnlp.247,2020.emnlp-main.210,0,0.605565,"laborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conducted experiments on the WMT16 English-Romanian (En-Ro) and English-Russia (En-Ru) translation tasks, which are widely-used benchmarks of data augmentation methods for NMT. The training/validation/test sets of the En-Ro include 612K/2K/2K sentence pairs, while those of En-Ru include 2M/3K/3K pairs. Towards better reproducibility, we directly used the BT data provided by Sennrich et al. (2016a)1 , consisting of 2.3M synt"
2021.findings-emnlp.247,2020.acl-main.41,1,0.673903,"o widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and En"
2021.findings-emnlp.247,2021.findings-acl.373,1,0.886854,"o learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conducted experiments on the WMT16 English-Romanian (En-Ro) and English-Russia (En-Ru) translation tasks, which are widely-used benchmarks of data augmentation methods for NMT. The training/validation/test sets of the En-Ro include 612K/2K/"
2021.findings-emnlp.247,2020.tacl-1.47,0,0.265899,"o widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lample, 2019; Liu et al., 2020b; Ding et al., 2021c), there is a pressing need to broaden the understandings of them. To this end, we introduce two probing tasks to investigate the effects of PT and BT on the encoder and decoder modules, respectively. We find that PT mainly contributes to the encoder module while BT brings more benefits to the decoder module. This provides a good explanation for the performance improvement of simply combining PT and BT. Motivated by this finding, we explore a better combination method by leveraging Tagged BT (Caswell et al., 2019). Experiments conducted on the WMT16 English-Romanian and En"
2021.findings-emnlp.247,2020.acl-main.532,0,0.0613034,"Missing"
2021.findings-emnlp.247,N19-4007,0,0.0122229,"riginal while “All” means the whole testset. Table 2: Translation quality on the En-Ro and En-Ru benchmarks. “+” means incorporating PT and (Tagged) BT into NMT models. 4.2 All Src-Ori denotes the testing data originating in the source language, while Tgt-Ori denotes the data translating from the target language. 2903 Effects of Word Frequency Data augmentation is an effective way to improve the translation quality of low-frequency words (Sennrich et al., 2016b). Thus, we compare the performance of the models on translating different frequencies of words. Specifically, we employed compare-mt (Neubig et al., 2019) to calculate the f-measure of translating low- and high-frequency words (&lt;50 vs. ≥50). As shown in Table 4, PT improves more on translating low-frequency words (58.2 vs. 57.5 scores) while BT performs better on high-frequency words (67.3 vs. 66.7 scores). Furthermore, the combination of PT and tagged BT achieves the best performance on both low- and high-frequency words, leading to an overall improvement on the whole words. Similar phenomenons can be observed by combining self-training and BT (Ding et al., 2021b). Takeaway: 1) PT and BT complementary in terms of frequency of words; 2) Tagged"
2021.findings-emnlp.247,P02-1040,0,0.109581,"ir comparison, all the model architectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand th"
2021.findings-emnlp.247,W18-6319,0,0.0200641,"., 2020b). Setting To make a fair comparison, all the model architectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In th"
2021.findings-emnlp.247,2020.tacl-1.18,0,0.187012,"tiveness in improving the model performance of NMT, especially for those language pairs with smaller parallel corpora (Conneau and Lample, 2019). The first research line treats pre-trained models as external knowledge to guidance NMT to learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation directions. In general, previous studies focus on designing novel architectures (Song et al., 2019) and artificial noises for source sentences (Lin et al., 2020; Yang et al., 2020b) but are still unclear why pre-training can boost the model performance of NMT, which is this paper aims to investigate. 2.2 Experimental Setup Data We conduct"
2021.findings-emnlp.247,W16-2323,0,0.284038,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,P16-1009,0,0.470578,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,P16-1162,0,0.668047,"urce code is freely available at https://github.com/ SunbowLiu/PTvsBT. 1 Introduction Neural machine translation (NMT; Bahdanau et al., 2015; Gehring et al., 2017; Vaswani et al., 2017) models are data-hungry and their performances are highly dependent upon the quantity and quality of labeled data, which are expensive and scarce resources (Leong et al., 2021). This motivates the research line of exploiting unlabeled monolingual data for boosting the model performance of NMT. Due to simplicity and effectiveness, pre-training (PT; Devlin et al., 2019; Song et al., 2019) and backtranslation (BT; Sennrich et al., 2016b) are two widely-used techniques for NMT, by leveraging a large amount of monolingual data. While empirically successful, the understandings of PT and BT are still limited at best. Several attempts have been made to better understand them at the data level, e.g. exploring different kinds of noises for the source data (Edunov et al., 2018; Lewis et al., 2020). However, there are few understandings at the model level that how PT and BT affect the internal module (e.g. encoder and decoder) of NMT models. As recent studies start to combine PT and BT for better model performance (Conneau and Lampl"
2021.findings-emnlp.247,2006.amta-papers.25,0,0.0314185,"hitectures and parameters are the same as the pre-trained mBART.cc25.2 The NMT model augmented with PT directly uses the mBART weights for parameter initialization, while the other models randomly initialize their parameters. The training follows Liu et al. (2020b) except that we tuned the learning rate within [3e-5,1e-3] and the dropout within [0.3,0.5] for the vanilla model and BT models. We used the single model with the best validation perplexity for testing. The length penalty is 1.0 and the beam size is 5. We used sacreBLEU (Post, 2018) to calculate BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores with the specific tokenization (Liu et al., 2020b) for Romanian and the default tokenization for Russian. Back-Translation for NMT BT is an alternative to leverage monolingual data for NMT (Sennrich et al., 2016b). It first trains a reversed NMT model for translating target-side monolingual data into synthetic parallel data, and then complements them with the original parallel data to train the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences"
2021.findings-emnlp.247,D19-1149,0,0.0464751,"Missing"
2021.findings-emnlp.247,D19-1073,0,0.0181896,"n the 3 Understanding PT and BT desired NMT model. To improve BT, previous works put attention to the importance of diver- In this section, we aim to better understand the similarities and differences between PT and BT sity and complexity in synthetic data, showing that adding symbols (e.g., noises and tags) to the back- on improving model performance. We design two probing tasks to study the research question: Which translated source can help NMT distinguish the data from various sources and learn better represen- module of NMT do PT and BT respectively play a tations (Fadaee and Monz, 2018; Wang et al., 2019; greater role in enhancing translation quality? Edunov et al., 2018; Caswell et al., 2019; Marie 3.1 Effects of PT on NMT et al., 2020). The claims and understandings from these works are chiefly at the data-level rather than Given a pre-trained model, it is common to use its the model-level. part or all parameters to initialize the downstream There also exists some works that combine tasks. We design four NMT models, which differ PT and BT to further boost the model perfor- from the NMT components (Encoder vs. Decoder) mance (Conneau et al., 2020; Song et al., 2019; with parameter initializa"
2021.findings-emnlp.247,2021.findings-acl.422,1,0.73879,"29.4 33.8 31.5 33.3 31.9 34.8 38.3 42.0 45.4 48.6 45.6 48.7 Tagged BT is to add a special token at the beginning of each back-translated source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and BT improves the model performance, while adding tags to BT data further improves Liu et al., 2021a; Wang et al., 2021).4 Generally speaking, the translation of Src-Ori is more important than that of Tgt-Ori for practical NMT systems (Graham et al., 2020), thus its performance should be taken seriously. As shown in Table 3, PT performs better on Src-Ori than BT (33.8 vs. 31.9 BLEU) while BT achieves higher scores on TgtOri than PT (45.6 vs. 42.0 BLEU). Besides, simply combining PT and BT can improve the translation quality on both Src-Ori and Tgt-Ori sentences, but the improvement of Src-Ori is lower than only using PT. By introducing tagged BT, the model can achieve better performance than the simple one, esp"
2021.findings-emnlp.247,2020.emnlp-main.208,0,0.143525,"al., 2019), which can ac2900 ∗ Work was done when Xuebo Liu and Liang Ding were interning at Tencent AI Lab. Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2900–2907 November 7–11, 2021. ©2021 Association for Computational Linguistics quire knowledge from unlabeled monolingual data, has shown its effectiveness in improving the model performance of NMT, especially for those language pairs with smaller parallel corpora (Conneau and Lample, 2019). The first research line treats pre-trained models as external knowledge to guidance NMT to learn better representations (Yang et al., 2020a; Zhu et al., 2020) and predictions (Chen et al., 2020). These methods are effective but costly since the NMT architecture needed to be elaborately designed. Another research line is directly taking the weights of pre-trained models to initialize NMT models, which is easy to use and advancing the state-ofthe-art (Rothe et al., 2020; Lewis et al., 2020). In this paper, we treat pre-trained mBART (Liu et al., 2020b) as our testbed for parameter initialization, whose benefits have been sufficiently validated (Tran et al., 2020; Tang et al., 2020; Liu et al., 2021a) by multiple translation direct"
2021.findings-emnlp.247,D16-1160,0,0.0238903,"Missing"
2021.findings-emnlp.247,W19-5208,0,0.0274486,"e-art performances on the two benchmarks. Similar tendencies are observed in terms of the TER scores. The above results illustrate the better complementarity between PT and Tagged BT on improving translation quality for NMT models. Analysis We conducted extensive analyses to better understand the improvement of our approach. All results are reported on the En-Ro benchmark. Effects of Sentence Type Recent studies have shown that the evaluation of BT is sensitive to the sentences types, thus we report BLEU scores on the subsets of source-original (Src-Ori) and targetoriginal (Tgt-Ori) datasets (Zhang and Toral, 2019; Model Vanilla + PT + BT + BT + PT + Tagged BT + Tagged BT + PT Tgt 33.7 37.7 38.4 41.2 38.6 41.6 29.4 33.8 31.5 33.3 31.9 34.8 38.3 42.0 45.4 48.6 45.6 48.7 Tagged BT is to add a special token at the beginning of each back-translated source sentence. All Low High 62.8 65.8 65.9 67.8 66.1 68.3 48.5 58.2 57.5 60.8 57.5 61.8 64.6 66.7 67.1 68.8 67.3 69.1 Table 4: F-measure of word translation according to frequency on the En-Ro benchmark. “Low” and “High” respectively denote the buckets of low- and high-frequency words while “All” means the whole words in the test set. Simply combining PT and B"
C12-2044,W10-1703,0,0.0400783,"Missing"
C12-2044,W08-0309,0,0.0239856,"Missing"
C12-2044,W09-0401,0,0.0252022,"avie, 2011), and MP4IBM1 (Popovic et al., 2011) are also used in the literature. AMBER (Chen and Kuhn, 2011) declares a modified version of BLEU and attaches more kinds of penalty coefficients, combining the n-gram precision and recall with the arithmetic average of F-measure. In order to distinguish the reliability of different MT evaluation metrics, people used to apply the Spearman correlation coefficient for evaluation tasks in the workshop of statistical machine translation (WMT) for Association of Computational Linguistics (ACL) (Callison-Burch et al., 2011; Callison-Burch et al., 2010; Callison-Burch et al., 2009, 2008). 2 Related work Some MT evaluation metrics are designed with the part-of-speech (POS) consideration using the linguistic tools, parser or POS tagger, during the words matching period between system-output and reference sentences. Machacek and Bojar (2011) proposed SemPOS metric, which is based on the Czech-targeted work by Kos and Bojar (2009). SemPOS conducts a deep-syntactic analysis of the target language with a modified version of similarity measure from the general overlapping method (Gimenez and Marquez, 2007). However, SemPOS only focuses on the English and Czech words and achie"
C12-2044,W11-2105,0,0.0296052,"e penalty for translation which is shorter than that of references. NIST (Doddington, 2002) added the information weight into evaluation factors. Meteor (Banerjee and Lavie, 2005) proposed an alternative way of calculating matched chunks to describe the n-gram matching degree between machine translations and reference translations. Wong and Kit (2008) introduced position difference in the evaluation metric. Other evaluation metrics, such as TER (Snover et al., 2006), the modified Meteor-1.3 (Denkowski and Lavie, 2011), and MP4IBM1 (Popovic et al., 2011) are also used in the literature. AMBER (Chen and Kuhn, 2011) declares a modified version of BLEU and attaches more kinds of penalty coefficients, combining the n-gram precision and recall with the arithmetic average of F-measure. In order to distinguish the reliability of different MT evaluation metrics, people used to apply the Spearman correlation coefficient for evaluation tasks in the workshop of statistical machine translation (WMT) for Association of Computational Linguistics (ACL) (Callison-Burch et al., 2011; Callison-Burch et al., 2010; Callison-Burch et al., 2009, 2008). 2 Related work Some MT evaluation metrics are designed with the part-of-"
C12-2044,W11-2107,0,0.104017,"y other methods have been proposed to revise or improve it. BLEU considered the n-gram precision and the penalty for translation which is shorter than that of references. NIST (Doddington, 2002) added the information weight into evaluation factors. Meteor (Banerjee and Lavie, 2005) proposed an alternative way of calculating matched chunks to describe the n-gram matching degree between machine translations and reference translations. Wong and Kit (2008) introduced position difference in the evaluation metric. Other evaluation metrics, such as TER (Snover et al., 2006), the modified Meteor-1.3 (Denkowski and Lavie, 2011), and MP4IBM1 (Popovic et al., 2011) are also used in the literature. AMBER (Chen and Kuhn, 2011) declares a modified version of BLEU and attaches more kinds of penalty coefficients, combining the n-gram precision and recall with the arithmetic average of F-measure. In order to distinguish the reliability of different MT evaluation metrics, people used to apply the Spearman correlation coefficient for evaluation tasks in the workshop of statistical machine translation (WMT) for Association of Computational Linguistics (ACL) (Callison-Burch et al., 2011; Callison-Burch et al., 2010; Callison-Bu"
C12-2044,W07-0738,0,0.0130691,"tics (ACL) (Callison-Burch et al., 2011; Callison-Burch et al., 2010; Callison-Burch et al., 2009, 2008). 2 Related work Some MT evaluation metrics are designed with the part-of-speech (POS) consideration using the linguistic tools, parser or POS tagger, during the words matching period between system-output and reference sentences. Machacek and Bojar (2011) proposed SemPOS metric, which is based on the Czech-targeted work by Kos and Bojar (2009). SemPOS conducts a deep-syntactic analysis of the target language with a modified version of similarity measure from the general overlapping method (Gimenez and Marquez, 2007). However, SemPOS only focuses on the English and Czech words and achieves no contribution for other language pairs. To reduce the human tasks during the evaluation, the methodologies that do not need reference translations are growing up. MP4IBM1 (Popovic et al., 2011) used IBM1 model to calculate scores based on morphemes, POS (4-grams) and lexicon probabilities. MP4IBM1 is not a simple model although it is reference independent. For instance, it needs large parallel bilingual corpus, POS taggers (requesting the details about verb tenses, cases, number, gender, etc.) and other tools for spli"
C12-2044,D10-1092,0,0.0175974,", number, gender, etc.) and other tools for splitting words into morphemes. It performed well on the corpus with English as source language following the metric TESLA (Dahlmeier et al., 2011) but got very poor correlation when English is the target language. For example, it gained the system-level correlation score 0.12 and 0.08 respectively on the Spanish-to-English and French-to-English MT evaluation tasks (Callison-Burch et al., 2011) and these two scores mean nearly no correlation with human judgments. Reordering errors play an important role in the translation for distant language pairs (Isozaki et al., 2010). But BLEU and many other metrics are both insensitive to reordering phenomena and relatively time-consuming to compute (Talbot et al., 2011). Snover et al. (2006) introduced Translation Edit Rate (TER) and the possible edits include the insertion, deletion, and substitution of words as well as sequences allowing phrase movements without large penalties. Isozaki et al. (2010) paid attention to word order on the evaluation between Japanese and English. Wong and Kit (2008) designed position difference factor during the alignment of words between 442 reference translations and candidate outputs,"
C12-2044,D11-1035,0,0.031397,"Missing"
C12-2044,N03-2021,0,0.0211548,"Missing"
C12-2044,P02-1040,0,0.12019,"ional resource or tool. Experiments show that this novel metric yields the state-of-the-art correlation with human judgments compared with classic metrics BLEU, TER, Meteor-1.3 and two latest metrics (AMBER and MP4IBM1), which proves it a robust one by employing a feature-rich and model-independent approach. KEYWORDS : Machine translation, Evaluation metric, Context-dependent n-gram alignment, Modified length penalty, Precision, Recall. Proceedings of COLING 2012: Posters, pages 441–450, COLING 2012, Mumbai, December 2012. 441 1 Introduction Since IBM proposed and realized the system of BLEU (Papineni et al., 2002) as the automatic metric for Machine Translation (MT) evaluation, many other methods have been proposed to revise or improve it. BLEU considered the n-gram precision and the penalty for translation which is shorter than that of references. NIST (Doddington, 2002) added the information weight into evaluation factors. Meteor (Banerjee and Lavie, 2005) proposed an alternative way of calculating matched chunks to describe the n-gram matching degree between machine translations and reference translations. Wong and Kit (2008) introduced position difference in the evaluation metric. Other evaluation"
C12-2044,W11-2109,0,0.183371,"ise or improve it. BLEU considered the n-gram precision and the penalty for translation which is shorter than that of references. NIST (Doddington, 2002) added the information weight into evaluation factors. Meteor (Banerjee and Lavie, 2005) proposed an alternative way of calculating matched chunks to describe the n-gram matching degree between machine translations and reference translations. Wong and Kit (2008) introduced position difference in the evaluation metric. Other evaluation metrics, such as TER (Snover et al., 2006), the modified Meteor-1.3 (Denkowski and Lavie, 2011), and MP4IBM1 (Popovic et al., 2011) are also used in the literature. AMBER (Chen and Kuhn, 2011) declares a modified version of BLEU and attaches more kinds of penalty coefficients, combining the n-gram precision and recall with the arithmetic average of F-measure. In order to distinguish the reliability of different MT evaluation metrics, people used to apply the Spearman correlation coefficient for evaluation tasks in the workshop of statistical machine translation (WMT) for Association of Computational Linguistics (ACL) (Callison-Burch et al., 2011; Callison-Burch et al., 2010; Callison-Burch et al., 2009, 2008). 2 Related w"
C12-2044,2006.amta-papers.25,0,0.67423,"ic for Machine Translation (MT) evaluation, many other methods have been proposed to revise or improve it. BLEU considered the n-gram precision and the penalty for translation which is shorter than that of references. NIST (Doddington, 2002) added the information weight into evaluation factors. Meteor (Banerjee and Lavie, 2005) proposed an alternative way of calculating matched chunks to describe the n-gram matching degree between machine translations and reference translations. Wong and Kit (2008) introduced position difference in the evaluation metric. Other evaluation metrics, such as TER (Snover et al., 2006), the modified Meteor-1.3 (Denkowski and Lavie, 2011), and MP4IBM1 (Popovic et al., 2011) are also used in the literature. AMBER (Chen and Kuhn, 2011) declares a modified version of BLEU and attaches more kinds of penalty coefficients, combining the n-gram precision and recall with the arithmetic average of F-measure. In order to distinguish the reliability of different MT evaluation metrics, people used to apply the Spearman correlation coefficient for evaluation tasks in the workshop of statistical machine translation (WMT) for Association of Computational Linguistics (ACL) (Callison-Burch e"
C12-2044,W11-2102,0,0.0284885,"llowing the metric TESLA (Dahlmeier et al., 2011) but got very poor correlation when English is the target language. For example, it gained the system-level correlation score 0.12 and 0.08 respectively on the Spanish-to-English and French-to-English MT evaluation tasks (Callison-Burch et al., 2011) and these two scores mean nearly no correlation with human judgments. Reordering errors play an important role in the translation for distant language pairs (Isozaki et al., 2010). But BLEU and many other metrics are both insensitive to reordering phenomena and relatively time-consuming to compute (Talbot et al., 2011). Snover et al. (2006) introduced Translation Edit Rate (TER) and the possible edits include the insertion, deletion, and substitution of words as well as sequences allowing phrase movements without large penalties. Isozaki et al. (2010) paid attention to word order on the evaluation between Japanese and English. Wong and Kit (2008) designed position difference factor during the alignment of words between 442 reference translations and candidate outputs, but it only selects the candidate word that has the nearest position in principle. Different words or phrases can express the same meanings,"
C12-2044,W07-0734,0,\N,Missing
C12-2044,W11-2106,0,\N,Missing
D17-1150,P17-1175,0,0.0355135,"Missing"
D17-1150,W14-4012,0,0.241662,"Missing"
D17-1150,D14-1179,0,0.0632807,"Missing"
D17-1150,P16-1078,0,0.067992,"ence of tokens. The most fundamental approaches transform the source sentence sequentially into a fixed-length context vector, and the annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b). Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Erig"
D17-1150,D13-1176,0,0.458623,"lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks. 1 PRP1 VP3 VBP5 I Neural machine translation (NMT) automatically learns the abstract features of and semantic relationship between the source and target sentences, and has recently given state-of-the-art results for various translation tasks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). The most widely used model is the encoder-decoder framework (Sutskever et al., 2014), in which the source sentence is encoded into a dense representation, followed by a decoding process which generates the target translation. By exploiting the attention mechanism (Bahdanau et al., 2015), the generation of target words is conditional on the source hidden states, rather than on the context vector alone. From a model architecture perspective, prior studies of the attentive Corresponding author take NP4 PRT6 up NP7 PP8 DT9 NN10 IN11 a position in N"
D17-1150,E17-2093,0,0.113445,"Missing"
D17-1150,D15-1166,0,0.384069,"GRU. The i-th leaf node vector is calculated as: l hli = fGRU (xi , hli−1 ), (1) where xi is the i-th source word embedding and hli−1 denotes the previous hidden state. The parent hidden state h↑i,j summarizes its left child h↑i,k and a non-linear function fsof tmax : p(yj |y1 , ..., yj−1 , x, tr; θ) = fsof tmax (cj ), where cj is the composite hidden state, which consists of a target hidden state sj and a context vector dj : cj = ftanh ([sj , dj ]). Given the previous target word yj−1 , the concatenation of the previous hidden state sj−1 and the previous context vector cj−1 (input-feeding) (Luong et al., 2015), sj , is calculated using a standard sequential GRU network: dec sj = fgru (yj−1 , [sj−1 , cj−1 ]). The context vector dj is computed using an attention model which is used to softly summarize the attended part of the source-side representations. Eriguchi et al. (2016) adopted a tree-based attention mechanism to consider both the word and phrase vectors: right child h↑k+1,j (i &lt; k &lt; j) by applying the tree-GRU (Zhou et al., 2016) as follows: ↑ zi,j = ↑ = ri,k ↑ = rk+1,j e h↑i,j = h↑i,j = R ↑ U(z) hk+1,j i=1 αj (t) = et = + N −1 X k=1 αj (k)hpk , (2) PN exp(et ) PN −1 l i=1 exp(ei ) + (Va )T t"
D17-1150,P02-1040,0,0.0997821,"ectively set as 620 and 1,000. Due to the concatenation in the bidirectional leaf-node encoding, the dimensions of the forward and backward vectors, which are half of those of the other hidden states, are set to 500. In order to prevent over-fitting, the training data is shuffled following each epoch. Moreover, the model parameters are optimized using AdaDelta (Zeiler, 2012), due to its capability for dynamically adapting the learning rate. We set the mini-batch size to 16 and the beam search size to 5. The accuracy of the translation relative to a reference is assessed using the BLEU metric (Papineni et al., 2002). In order to give an equitable comparison, all the NMT models used for comparison are implemented or re-implemented using GRU in our code, based on dl4mt4 . 4.3 Enhanced Hierarchical Representations Firstly, the effectiveness of the enhanced hierarchical representations is evaluated through a set of experiments, the results of which are summarized in Table 3. Compared with the original tree-based encoder (Eriguchi et al., 2016), the model with bidirectional leaf-node encoding (described in Section 3.1) shows better performance. This also reveals that the future context at leaf level can contr"
D17-1150,P16-2049,0,0.0377221,"annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b). Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotati"
D17-1150,P15-1150,0,0.247232,"Missing"
D17-1150,P12-3004,1,0.83203,"ed 1.4M sentence pairs, in which the maximum length of the sentence was 40, from the LDC parallel corpus3 as our training data. The models were developed using NIST mt08 data and were examined using NIST mt04, mt05, and mt06 data. The number of sentences in each dataset is shown in Table 1. On the English side, we used the constituent parser (Zeng et al., 2014, 2015) to produce a binary syntactic tree for each sentence, in constrast to the use of the HPSG parser by Eriguchi et al. (2016). On the Chinese side, the sentences are segmented using the Chinese word segmentation toolkit of NiuTrans (Xiao et al., 2012). To avoid data sparsity, words referring to time, date and number, which are low in frequency, are generalized as ‘$time’, ‘$date’ and ‘$number’. In addition, as described in Section 3.3, the vocabularies are further compressed by segmenting the rare words into sub-word units using BPE. 4.2 Experimental Settings As shown in Table 2, which gives the statistics of the token types, we limit the source and target vo3 Our training data was selected from LDC2000T46, LDC2000T50, LDC2003E14, LDC2004T08, LDC2004T08 and LDC2005T10. Training Set |V |in En |V |in Zh Original 159k 198k Generalization 120k"
D17-1150,C16-1274,0,0.0951232,". By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases. As a result, the learned representations are limited to local information, while failing to capture the global meaning of a sentence. As illustrated in Figure 1, the phrases “take up”1 and “a position”2 have different meanings in different contexts. However, in composing the representations hVP3 and hNP7 for phrases VP3 and NP7 , the current approaches do not account for the differences in meaning which arise as a result of ig"
D17-1150,W16-2209,0,0.0354317,"length context vector, and the annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b). Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up f"
D17-1150,P16-1162,0,0.147293,"formation between the word and phrase vectors. To alleviate the out-ofvocabulary (OOV) problem, we further extend the proposed tree-based model to the sub-word level 1 Take up has the meanings of start doing something new, use space/time, accept an offer, etc. 2 Position has the meanings of location, job offer, rank/status, etc. hp1,8 hp2,8 hp2,3 hp4,8 hp6,8 hp4,5 hp7,8 hl1 hl2 hl3 hl4 hl5 I e tak up a i pos hl6 n tio in hl7 hl8 hl9 m i the roo heos Figure 2: The tree-based model of Eriguchi et al. (2016) comprising a structured and sequential encoder. by integrating byte-pair encoding (BPE) (Sennrich et al., 2016) into the tree-based model (as described in Section 3.3). Experimental results for the NIST English-to-Chinese translation task reveal that the proposed model significantly outperforms the vanilla tree-based (Eriguchi et al., 2016) and sequential NMT models (Bahdanau et al., 2015) (Section 4.1). 2 Tree-Based Neural Machine Translation A neural machine translation system (NMT) aims to use a single neural network to build a translation model, which is trained to maximize the conditional distribution of sentence pairs using a parallel training corpus (Kalchbrenner and Blunsom, 2013; Sutskever et"
I13-1116,P02-1040,0,0.0863375,"ets are considered, including: news-test (NT) data (2009, 2010, 2011) for Fr-En and De-En, which are extracted from the international workshop of SMT (WMT) held annually by the ACL’s special interest group for MT; test data for Zh-En and Zh-Pt are extracted from online web pages. We limited the length of the sentences to be less than fifty, and all of them should be valid aligned parse trees for all the training and testing data. For Chinese, a segmentation model (Zhang et al., 2003) is used for detecting word boundaries. Table 2 shows the translation quality measured in terms of BLEU metric (Papineni et al., 2002) with the original (Ori.) and universal (Uni.) tagset. When Chinese is considered as the source, results are lower than the ones targeted for European languages, probably affected by the corpus selection, size of the corpus, parsing success rate, non-standard linguistic phenomena (Wong et al., 2012), etc. In particular, we observed that the parsing accuracy (either on the original or universal tag-set) for Chinese language is lower compared (1) As an example, in rule (1), the top node of the source tree is [IP], the top node of the target tree is [S], and both trees have five children. Alignme"
I13-1116,J07-2003,0,0.362562,"ly over constrained. This paper presents a unified tagset for all languages at Part-of-Speech and Phrasal Category level in tree-to-tree models. Different experiments are conducted to study for its feasibility, efficiency, and translation quality. 1 Introduction The study of Statistical Machine Translation (SMT) (Lopez, 2008) relying on syntactic information has received wide attention in recent years. In particular, syntactic information is being integrated either on the source or target or both side(s) in training translation models for handling the translation task. In hierarchical models (Chiang, 2007) that consider syntactic information (Zollmann and Venugopal, 2006), the input sentence is analyzed and translated by synchronous context free grammars (SCFG) hierarchically with extra linguistic information. In string-to-tree SMT models (Galley et al., 2004; Zhang et al., 2011), the output of the translation always follows a grammatical syntax of the target language. In treeto-string SMT models (Liu et al., 2006; Wu et al., 2010), source side syntax is used to generate the 907 International Joint Conference on Natural Language Processing, pages 907–911, Nagoya, Japan, 14-18 October 2013. Tag"
I13-1116,P05-1067,0,0.0216401,"tags (VP, VCD, VCP, VNV, VPT, VRD, VSB) are all grouped into CVP, more coverage in the selection of rules is expected. In particular, suppose that in the original tag-set, “想 一 想” (think) is tagged as VCD (verb compounds), while in universal tag-set, it is tagged as CVP. In this case, it 909 NT 2009 NT 2010 NT 2011 Test Data Fr-En Ori. Uni. 11.57 11.59* 10.81 10.84* 12.12 12.15 Zh-En Ori. Uni. 4.79 4.85 De-En Ori. Uni. 9.64 9.66 10.48 10.55* 9.43 9.44 Zh-Pt Ori. Uni. 3.87 3.88 structures based on synchronous tree mapping grammars (Eisner, 2003), and synchronous dependency insertion grammars (Ding and Palmer, 2005). However, their work is targeted on dependency grammars, which is simpler than CFG equivalent formalisms (Fox, 2002). Other studies reported the use of syntactic information from conventional bilingual parsed trees. Zhang et al. (2008) proposed a tree sequence alignment model for bilingual trees. Liu et al. (2009) considered packed forests instead of 1-best trees for the whole translation process. Although both methods tend to increase rule coverage and to relax the overconstrained problem, they require tailored and sophisticated decoders. Zhai et al. (2011) considered the addition of bilingu"
I13-1116,P06-1055,0,0.428987,"versal Tag-set in Tree-to-Tree Translation Models Francisco Oliveira, Derek F. Wong, Lidia S. Chao, Liang Tian, Liangye He Department of Computer and Information Science, University of Macau, Macao SAR, China {olifran, derekfw, lidiasc}@umac.mo, {tianliang0123, wutianshui0515}@gmail.com Abstract translation output. Finally, by considering the syntax of both the source and target languages, treeto-tree SMT models (Zhang et al., 2008; Liu et al., 2009) tend to be the best among the previous models. Basically, all of these models require two extra components: (1) syntax parsers (He et al., 2012; Petrov et al., 2006) in obtaining annotated syntax trees for training the models, and (2) monolingual treebanks (a detailed list can be found in Petrov et al. (2012)) for training the parsers. Currently, many of them are publicly available through Internet, institutions and data consortiums. Independently from the method used, although there are many treebanks available, they typically have their own tag-set defined for different languages, ranging from tens to hundreds of tags, which is hard to conduct the research in a multilingual environment. As a consequence, Petrov et al. (2012) developed a universal Part-o"
I13-1116,P03-2041,0,0.0551776,"del. As an example, in Chinese tag-set, as verb phrase related tags (VP, VCD, VCP, VNV, VPT, VRD, VSB) are all grouped into CVP, more coverage in the selection of rules is expected. In particular, suppose that in the original tag-set, “想 一 想” (think) is tagged as VCD (verb compounds), while in universal tag-set, it is tagged as CVP. In this case, it 909 NT 2009 NT 2010 NT 2011 Test Data Fr-En Ori. Uni. 11.57 11.59* 10.81 10.84* 12.12 12.15 Zh-En Ori. Uni. 4.79 4.85 De-En Ori. Uni. 9.64 9.66 10.48 10.55* 9.43 9.44 Zh-Pt Ori. Uni. 3.87 3.88 structures based on synchronous tree mapping grammars (Eisner, 2003), and synchronous dependency insertion grammars (Ding and Palmer, 2005). However, their work is targeted on dependency grammars, which is simpler than CFG equivalent formalisms (Fox, 2002). Other studies reported the use of syntactic information from conventional bilingual parsed trees. Zhang et al. (2008) proposed a tree sequence alignment model for bilingual trees. Liu et al. (2009) considered packed forests instead of 1-best trees for the whole translation process. Although both methods tend to increase rule coverage and to relax the overconstrained problem, they require tailored and sophis"
I13-1116,petrov-etal-2012-universal,0,0.0966685,"Missing"
I13-1116,W02-1039,0,0.042356,"ular, suppose that in the original tag-set, “想 一 想” (think) is tagged as VCD (verb compounds), while in universal tag-set, it is tagged as CVP. In this case, it 909 NT 2009 NT 2010 NT 2011 Test Data Fr-En Ori. Uni. 11.57 11.59* 10.81 10.84* 12.12 12.15 Zh-En Ori. Uni. 4.79 4.85 De-En Ori. Uni. 9.64 9.66 10.48 10.55* 9.43 9.44 Zh-Pt Ori. Uni. 3.87 3.88 structures based on synchronous tree mapping grammars (Eisner, 2003), and synchronous dependency insertion grammars (Ding and Palmer, 2005). However, their work is targeted on dependency grammars, which is simpler than CFG equivalent formalisms (Fox, 2002). Other studies reported the use of syntactic information from conventional bilingual parsed trees. Zhang et al. (2008) proposed a tree sequence alignment model for bilingual trees. Liu et al. (2009) considered packed forests instead of 1-best trees for the whole translation process. Although both methods tend to increase rule coverage and to relax the overconstrained problem, they require tailored and sophisticated decoders. Zhai et al. (2011) considered the addition of bilingual phrases and binarization of parse trees to deal with the problems. In this work, we proposed the substitution of o"
I13-1116,N04-1035,0,0.0485592,"troduction The study of Statistical Machine Translation (SMT) (Lopez, 2008) relying on syntactic information has received wide attention in recent years. In particular, syntactic information is being integrated either on the source or target or both side(s) in training translation models for handling the translation task. In hierarchical models (Chiang, 2007) that consider syntactic information (Zollmann and Venugopal, 2006), the input sentence is analyzed and translated by synchronous context free grammars (SCFG) hierarchically with extra linguistic information. In string-to-tree SMT models (Galley et al., 2004; Zhang et al., 2011), the output of the translation always follows a grammatical syntax of the target language. In treeto-string SMT models (Liu et al., 2006; Wu et al., 2010), source side syntax is used to generate the 907 International Joint Conference on Natural Language Processing, pages 907–911, Nagoya, Japan, 14-18 October 2013. Tag CNP Chinese English CLP, NP, QP, UCP NP, NAC, WHNP, QP French German Portuguese NP CNP, MPN, NM, NP np CVP VP, VCD, VCP, VNV, VPT, VRD, VSB VP VN, VP, VPpart, VPinf CVP, VP, VZ x, vp CAJP ADJP ADJP, WHADJP AP AA, AP, MTA CAVP ADVP, DNP, DP, LCP ADVP, WHADVP,"
I13-1116,W12-6337,1,0.850232,"asal Category Universal Tag-set in Tree-to-Tree Translation Models Francisco Oliveira, Derek F. Wong, Lidia S. Chao, Liang Tian, Liangye He Department of Computer and Information Science, University of Macau, Macao SAR, China {olifran, derekfw, lidiasc}@umac.mo, {tianliang0123, wutianshui0515}@gmail.com Abstract translation output. Finally, by considering the syntax of both the source and target languages, treeto-tree SMT models (Zhang et al., 2008; Liu et al., 2009) tend to be the best among the previous models. Basically, all of these models require two extra components: (1) syntax parsers (He et al., 2012; Petrov et al., 2006) in obtaining annotated syntax trees for training the models, and (2) monolingual treebanks (a detailed list can be found in Petrov et al. (2012)) for training the parsers. Currently, many of them are publicly available through Internet, institutions and data consortiums. Independently from the method used, although there are many treebanks available, they typically have their own tag-set defined for different languages, ranging from tens to hundreds of tags, which is hard to conduct the research in a multilingual environment. As a consequence, Petrov et al. (2012) develo"
I13-1116,2011.mtsummit-papers.29,0,0.019533,"dependency insertion grammars (Ding and Palmer, 2005). However, their work is targeted on dependency grammars, which is simpler than CFG equivalent formalisms (Fox, 2002). Other studies reported the use of syntactic information from conventional bilingual parsed trees. Zhang et al. (2008) proposed a tree sequence alignment model for bilingual trees. Liu et al. (2009) considered packed forests instead of 1-best trees for the whole translation process. Although both methods tend to increase rule coverage and to relax the overconstrained problem, they require tailored and sophisticated decoders. Zhai et al. (2011) considered the addition of bilingual phrases and binarization of parse trees to deal with the problems. In this work, we proposed the substitution of original tags into universal ones, which has a higher level of abstraction in partially increasing the rule coverage while reducing the size of the rule table. Moreover, our approach does not require big changes in tree-to-tree models for accomplishing the translation task. Table 2: Translation quality comparison Language Pair Fr-En De-En Zh-En Zh-Pt System Ori. Uni. Ori. Uni. Ori. Uni. Ori. Uni. VmPeak (KB) 1,002,040 982,208 761,108 745,724 826"
I13-1116,W03-1730,0,0.00897069,"and Ney, 2003). We used a 5-gram language model for all the languages based on the SRILM toolkit (Stolcke, 2002). Different test sets are considered, including: news-test (NT) data (2009, 2010, 2011) for Fr-En and De-En, which are extracted from the international workshop of SMT (WMT) held annually by the ACL’s special interest group for MT; test data for Zh-En and Zh-Pt are extracted from online web pages. We limited the length of the sentences to be less than fifty, and all of them should be valid aligned parse trees for all the training and testing data. For Chinese, a segmentation model (Zhang et al., 2003) is used for detecting word boundaries. Table 2 shows the translation quality measured in terms of BLEU metric (Papineni et al., 2002) with the original (Ori.) and universal (Uni.) tagset. When Chinese is considered as the source, results are lower than the ones targeted for European languages, probably affected by the corpus selection, size of the corpus, parsing success rate, non-standard linguistic phenomena (Wong et al., 2012), etc. In particular, we observed that the parsing accuracy (either on the original or universal tag-set) for Chinese language is lower compared (1) As an example, in"
I13-1116,W04-3250,0,0.0551106,"Missing"
I13-1116,P08-1064,0,0.0208527,"sal tag-set, it is tagged as CVP. In this case, it 909 NT 2009 NT 2010 NT 2011 Test Data Fr-En Ori. Uni. 11.57 11.59* 10.81 10.84* 12.12 12.15 Zh-En Ori. Uni. 4.79 4.85 De-En Ori. Uni. 9.64 9.66 10.48 10.55* 9.43 9.44 Zh-Pt Ori. Uni. 3.87 3.88 structures based on synchronous tree mapping grammars (Eisner, 2003), and synchronous dependency insertion grammars (Ding and Palmer, 2005). However, their work is targeted on dependency grammars, which is simpler than CFG equivalent formalisms (Fox, 2002). Other studies reported the use of syntactic information from conventional bilingual parsed trees. Zhang et al. (2008) proposed a tree sequence alignment model for bilingual trees. Liu et al. (2009) considered packed forests instead of 1-best trees for the whole translation process. Although both methods tend to increase rule coverage and to relax the overconstrained problem, they require tailored and sophisticated decoders. Zhai et al. (2011) considered the addition of bilingual phrases and binarization of parse trees to deal with the problems. In this work, we proposed the substitution of original tags into universal ones, which has a higher level of abstraction in partially increasing the rule coverage whi"
I13-1116,P06-1077,0,0.0242841,"icular, syntactic information is being integrated either on the source or target or both side(s) in training translation models for handling the translation task. In hierarchical models (Chiang, 2007) that consider syntactic information (Zollmann and Venugopal, 2006), the input sentence is analyzed and translated by synchronous context free grammars (SCFG) hierarchically with extra linguistic information. In string-to-tree SMT models (Galley et al., 2004; Zhang et al., 2011), the output of the translation always follows a grammatical syntax of the target language. In treeto-string SMT models (Liu et al., 2006; Wu et al., 2010), source side syntax is used to generate the 907 International Joint Conference on Natural Language Processing, pages 907–911, Nagoya, Japan, 14-18 October 2013. Tag CNP Chinese English CLP, NP, QP, UCP NP, NAC, WHNP, QP French German Portuguese NP CNP, MPN, NM, NP np CVP VP, VCD, VCP, VNV, VPT, VRD, VSB VP VN, VP, VPpart, VPinf CVP, VP, VZ x, vp CAJP ADJP ADJP, WHADJP AP AA, AP, MTA CAVP ADVP, DNP, DP, LCP ADVP, WHADVP, PRT AdP AVP, CAVP advp CPP CS PP PP, WHPP PP CAC, CPP, PP pp FRAG, IP S, SBAR, SBARQ, SINV, SQ, PRN, FRAG, RRC ROOT, SENT, Ssub, Sint, Srel CS, PSEUDO, S fcl"
I13-1116,D11-1019,0,0.0123454,"of Statistical Machine Translation (SMT) (Lopez, 2008) relying on syntactic information has received wide attention in recent years. In particular, syntactic information is being integrated either on the source or target or both side(s) in training translation models for handling the translation task. In hierarchical models (Chiang, 2007) that consider syntactic information (Zollmann and Venugopal, 2006), the input sentence is analyzed and translated by synchronous context free grammars (SCFG) hierarchically with extra linguistic information. In string-to-tree SMT models (Galley et al., 2004; Zhang et al., 2011), the output of the translation always follows a grammatical syntax of the target language. In treeto-string SMT models (Liu et al., 2006; Wu et al., 2010), source side syntax is used to generate the 907 International Joint Conference on Natural Language Processing, pages 907–911, Nagoya, Japan, 14-18 October 2013. Tag CNP Chinese English CLP, NP, QP, UCP NP, NAC, WHNP, QP French German Portuguese NP CNP, MPN, NM, NP np CVP VP, VCD, VCP, VNV, VPT, VRD, VSB VP VN, VP, VPpart, VPinf CVP, VP, VZ x, vp CAJP ADJP ADJP, WHADJP AP AA, AP, MTA CAVP ADVP, DNP, DP, LCP ADVP, WHADVP, PRT AdP AVP, CAVP ad"
I13-1116,P09-1063,0,0.0318856,"Missing"
I13-1116,W06-3119,0,0.0405038,"tagset for all languages at Part-of-Speech and Phrasal Category level in tree-to-tree models. Different experiments are conducted to study for its feasibility, efficiency, and translation quality. 1 Introduction The study of Statistical Machine Translation (SMT) (Lopez, 2008) relying on syntactic information has received wide attention in recent years. In particular, syntactic information is being integrated either on the source or target or both side(s) in training translation models for handling the translation task. In hierarchical models (Chiang, 2007) that consider syntactic information (Zollmann and Venugopal, 2006), the input sentence is analyzed and translated by synchronous context free grammars (SCFG) hierarchically with extra linguistic information. In string-to-tree SMT models (Galley et al., 2004; Zhang et al., 2011), the output of the translation always follows a grammatical syntax of the target language. In treeto-string SMT models (Liu et al., 2006; Wu et al., 2010), source side syntax is used to generate the 907 International Joint Conference on Natural Language Processing, pages 907–911, Nagoya, Japan, 14-18 October 2013. Tag CNP Chinese English CLP, NP, QP, UCP NP, NAC, WHNP, QP French Germa"
I13-1116,J03-1002,0,0.00324519,"ocessor at 2.9GHz, with 192G physical memory. All the experiments are carried out in Moses toolkit (Koehn et al., 2007). Different language pairs are considered in the experiments, including Fr-En, De-En, Zh-En, and Zh-Pt. The bilingual data we used for Fr-En and De-En are extracted from Europarl Parliament (version 7), while Zh-En and Zh-Pt parallel information are extracted from online web-sites. All sentences are parsed by Berkeley parser (Petrov et al., 2006) and word-aligned by using GIZA++ based on five iterations of IBM model 1, three for IBM models 3 and 4, and five for HMM alignment (Och and Ney, 2003). We used a 5-gram language model for all the languages based on the SRILM toolkit (Stolcke, 2002). Different test sets are considered, including: news-test (NT) data (2009, 2010, 2011) for Fr-En and De-En, which are extracted from the international workshop of SMT (WMT) held annually by the ACL’s special interest group for MT; test data for Zh-En and Zh-Pt are extracted from online web pages. We limited the length of the sentences to be less than fifty, and all of them should be valid aligned parse trees for all the training and testing data. For Chinese, a segmentation model (Zhang et al., 2"
I13-1116,P07-2045,0,\N,Missing
N19-1407,D18-1457,1,0.873171,"ng efficiency. Multi-Head Attention Multi-head attention mechanism (Vaswani et al., 2017) employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018). Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and Strubell et al. (2018) employed different attention heads to capture different linguistic features. Li et al. (2018) introduced disagreement regularizations to encourage the diversity among attention heads. Inspired by recent successes on fusing information across layers (Dou et al., 2018, 2019), Li et al. (2019) proposed to aggregate information captured by different attention heads. Based on these findings, we model interactions among attention heads to exploit the richness of local properties distributed in different heads. 5 Experiments We conducted experiments with the Transformer model (Vaswani et al., 2017) on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks. 4042 For the En⇒De and Zh⇒En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectiv"
N19-1407,D16-1044,0,0.0672508,"N2 ] (8) V [V b h, V b h are elements in the h-th subspace, where K which are calculated by Equations S 4 and 5 respectively. The union operation means combining the keys and values in different subspaces. The corresponding output is calculated as: e h )V eh ohi = ATT(qhi , K (9) The 2D convolution allows SANs to build relevance between elements across adjacent heads, thus flexibly extract local features from different subspaces rather than merely from an unique head. The vanilla SAN models linearly aggregate features from different heads, and this procedure limits the extent of abstraction (Fukui et al., 2016; Li et al., 2019). Multiple sets of representations presented at feature learning time can further improve the expressivity of the learned features (Ngiam et al., 2011; Wu and He, 2018). 4 Related Work Self-Attention Networks Recent studies have shown that S ANs can be further improved by capturing complementary information. For example, Hao et al. (2019) complemented S ANs with recurrence modeling, while Yang et al. (2019) modeled contextual information for S ANs. Concerning modeling locality for S ANs, Yu et al. (2018) injected several CNN layers (Kim, 2014) to fuse local information, the o"
N19-1407,W18-5431,0,0.423676,"aces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns (Yang et al., 2018; Wu et al., 2018; Guo et al., 2019). Second, multi-head attention extracts distinct linguistic properties from each subspace in a parallel fashion (Raganato and Tiedemann, 2018), which fails to exploit useful interactions across different heads. Recent work shows that better features can be learned if different sets of representations are present at feature learning time (Ngiam et al., 2011; Lin et al., 2014). To this end, we propose novel convolutional self-attention networks (C SANs), which model locality for self-attention model and interactions between features learned by different attention heads in an unified framework. Specifically, in order to pay more attention to a local part of the input sequence, we restrict the attention scope to a window of neighboring"
N19-1407,P16-1162,0,0.152427,"eriments We conducted experiments with the Transformer model (Vaswani et al., 2017) on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks. 4042 For the En⇒De and Zh⇒En tasks, the models were trained on widely-used WMT14 and WMT17 corpora, consisting of around 4.5 and 20.62 million sentence pairs, respectively. Concerning Ja⇒En, we used the first two sections of WAT17 corpus as the training data, which consists of 2M sentence pairs. To reduce the vocabulary size, all the data were tokenized and segmented into subword symbols using byte-pair encoding (Sennrich et al., 2016) with 32K merge operations. Following Shaw et al. (2018), we incorporated the proposed model into the encoder, which is a stack of 6 SAN layers. Prior studies revealed that modeling locality in lower layers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; Yang et al., 2018), we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as Vaswani et al. (2017), and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens. 5.1 Effects of Window/Area Size We first investigated the"
N19-1407,N18-2074,0,0.478472,"l allows each head to interact local features with its adjacent subspaces at attention time. We expect that the interaction across different subspaces can further improve the performance of SANs. We evaluate the effectiveness of the proposed model on three widely-used translation tasks: WMT14 English-to-German, WMT17 Chineseto-English, and WAT17 Japanese-to-English. Experimental results demonstrate that our approach consistently improves performance over the strong T RANSFORMER model (Vaswani et al., 2017) across language pairs. Comparing with previous work on modeling locality for SANs (e.g. Shaw et al., 2018; Yang et al., 2018; Sperber et al., 2018), our model boosts performance on both translation quality and training efficiency. 4040 Proceedings of NAACL-HLT 2019, pages 4040–4045 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Bush held Bush aheld talka with talkSharon with Sharon Bush held Bush aheld talka with talkSharon with Sharon Bush held Bush aheld talka with talkSharon with Sharon Bush held Bush aheld talka with talkSharon with Sharon Bush held a talk with SharonBush (a) Vanilla SANs held Bush aheld talka with talkSharon with Sharon (b) 1D"
N19-1407,D14-1181,0,0.00321668,"e extent of abstraction (Fukui et al., 2016; Li et al., 2019). Multiple sets of representations presented at feature learning time can further improve the expressivity of the learned features (Ngiam et al., 2011; Wu and He, 2018). 4 Related Work Self-Attention Networks Recent studies have shown that S ANs can be further improved by capturing complementary information. For example, Hao et al. (2019) complemented S ANs with recurrence modeling, while Yang et al. (2019) modeled contextual information for S ANs. Concerning modeling locality for S ANs, Yu et al. (2018) injected several CNN layers (Kim, 2014) to fuse local information, the output of which is fed to the subsequent SAN layer. Several researches proposed to revise the attention distribution with a parametric localness bias, and succeed on machine translation (Yang et al., 2018) and natural language inference (Guo et al., 2019). While both models introduce additional parameters, our approach is a more lightweight solution without introducing any new parameters. Closely related to this work, Shen et al. (2018a) applied a positional mask to encode temporal order, which only allows SANs to attend to the previous or following tokens in th"
N19-1407,D18-1548,0,0.0713839,"we improve locality modeling from revising attention scope. To make a fair comparison, we re-implemented the above approaches under a same framework. Empirical results on machine translation tasks show the superiority of our approach in both translation quality and training efficiency. Multi-Head Attention Multi-head attention mechanism (Vaswani et al., 2017) employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018). Along this direction, Shen et al. (2018a) explicitly used multiple attention heads to model different dependencies of the same word pair, and Strubell et al. (2018) employed different attention heads to capture different linguistic features. Li et al. (2018) introduced disagreement regularizations to encourage the diversity among attention heads. Inspired by recent successes on fusing information across layers (Dou et al., 2018, 2019), Li et al. (2019) proposed to aggregate information captured by different attention heads. Based on these findings, we model interactions among attention heads to exploit the richness of local properties distributed in different heads. 5 Experiments We conducted experiments with the Transformer model (Vaswani et al., 2017)"
N19-1407,W04-3250,0,0.0278748,"nce improvements over the Transformer baseline. Model T RANSFORMER -BASE + CS ANs T RANSFORMER -B IG + CS ANs WMT14 En⇒De Speed BLEU 1.28 27.31 1.22 28.18⇑ 0.61 28.58 0.50 28.74 WMT17 Zh⇒En Speed BLEU 1.21 24.13 1.16 24.80⇑ 0.58 24.56 0.48 25.01↑ WAT17 Ja⇒En Speed BLEU 1.33 28.10 1.28 28.50↑ 0.65 28.41 0.55 28.73↑ Table 2: Experimental results on WMT14 En⇒De, WMT17 Zh⇒En and WAT17 Ja⇒En test sets. “Speed” denotes the training speed (steps/second). “↑ / ⇑” indicates statistically significant difference from the vanilla self-attention counterpart (p < 0.05/0.01), tested by bootstrap resampling (Koehn, 2004). C NNs, revealing that extracting local features with dynamic weights is superior to assigning fixed parameters. Moreover, while most of the existing approaches (except for Shen et al. (2018a)) introduce new parameters, our methods are parameter-free and thus only marginally affect training efficiency. 5.4 Universality of The Proposed Model To validate the universality of our approach on MT tasks, we evaluated the proposed approach on different language pairs and model settings. Table 2 lists the results on En⇒De, Zh⇒En and Ja⇒En translation tasks. As seen, our model consistently improves tra"
N19-1407,D18-1317,1,0.920185,"ann, 2018). Therefore, we merely apply locality modeling to the lower layers, which same to the configuration in Yu et al. (2018) and Yang et al. (2018). In 4041 this way, the representations are learned in a hierarchical fashion (Yang et al., 2017). That is, the distance-aware and local information extracted by the lower SAN layers, is expected to complement distance-agnostic and global information captured by the higher SAN layers. 3.2 Attention Interaction via 2D Convolution Mutli-head mechanism allows different heads to capture distinct linguistic properties (Raganato and Tiedemann, 2018; Li et al., 2018), especially in diverse local contexts (Yang et al., 2018). We hypothesis that exploiting local properties across heads can further improve the performance of SANs. To this end, we expand the 1-dimensional window to a 2-dimensional area with the new dimension being the index of attention head. Suppose that the area size is (N + 1) × (M + 1) (N ≤ H), the keys and values in the area are: [ eh = b h− N2 , . . . , K b h, . . . , K b h+ N2 ] (7) K [K [ eh = b h− N2 , . . . , V b h, . . . , V b h+ N2 ] (8) V [V b h, V b h are elements in the h-th subspace, where K which are calculated by Equations S"
N19-1407,N19-1359,1,0.91234,"V b h are elements in the h-th subspace, where K which are calculated by Equations S 4 and 5 respectively. The union operation means combining the keys and values in different subspaces. The corresponding output is calculated as: e h )V eh ohi = ATT(qhi , K (9) The 2D convolution allows SANs to build relevance between elements across adjacent heads, thus flexibly extract local features from different subspaces rather than merely from an unique head. The vanilla SAN models linearly aggregate features from different heads, and this procedure limits the extent of abstraction (Fukui et al., 2016; Li et al., 2019). Multiple sets of representations presented at feature learning time can further improve the expressivity of the learned features (Ngiam et al., 2011; Wu and He, 2018). 4 Related Work Self-Attention Networks Recent studies have shown that S ANs can be further improved by capturing complementary information. For example, Hao et al. (2019) complemented S ANs with recurrence modeling, while Yang et al. (2019) modeled contextual information for S ANs. Concerning modeling locality for S ANs, Yu et al. (2018) injected several CNN layers (Kim, 2014) to fuse local information, the output of which is"
N19-1407,D18-1408,0,0.159869,"tion, the performance of SANs can be improved by multi-head attention (Vaswani et al., 2017), which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns (Yang et al., 2018; Wu et al., 2018; Guo et al., 2019). Second, multi-head attention extracts distinct linguistic properties from each subspace in a parallel fashion (Raganato and Tiedemann, 2018), which fails to exploit useful interactions across different heads. Recent work shows that better features can be learned if different sets of representations are present at feature learning time (Ngiam et al., 2011; Lin et al., 2014). To this end, we propose novel convolutional self-attention networks (C SANs), which model locality for self-attention model and interactions between features learned by different attention heads in an u"
N19-1407,D15-1166,0,0.0950172,"ers can achieve better performance (Shen et al., 2018b; Yu et al., 2018; Yang et al., 2018), we applied our approach to the lowest three layers of the encoder. About configurations of NMT models, we used the Base and Big settings same as Vaswani et al. (2017), and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens. 5.1 Effects of Window/Area Size We first investigated the effects of window size (1D-CS ANs) and area size (2D-CS ANs) on En⇒De validation set, as plotted in Figure 2. For 1D-CS ANs, the local size with 11 is superior to other settings. This is consistent with Luong et al. (2015) who found that 10 is the best window size in their local attention experiments. Then, we fixed the number of neighboring tokens being 11 and varied the number of heads. As seen, by considering the features across heads (i.e. > 1), 2D-CS ANs further improve the translation quality. However, when the number of heads in attention goes up, the translation quality inversely drops. One possible reason is that the model still has the flexibility of learning a different distribution for each head with few interactions, while a large amount of interactions assumes more heads make “similar contribution"
N19-1407,D16-1244,0,0.171283,"Missing"
N19-1407,N18-1202,0,0.0838108,"the model to attend to a local region via convolution operations (1D-CS ANs, Figure 1(b)). Accordingly, it provides distance-aware 2 Accordingly, the calculation of corresponding output in Equation (2) is modified as: b h )V bh ohi = ATT(qhi , K (3) Approach 2 (6) As seen, SANs are only allowed to attend to the b h, V b h ), instead of all neighboring tokens (e.g., K the tokens in the sequence (e.g., Kh , Vh ). The SAN-based models are generally implemented as multiple layers, in which higher layers tend to learn semantic information while lower layers capture surface and lexical information (Peters et al., 2018; Raganato and Tiedemann, 2018). Therefore, we merely apply locality modeling to the lower layers, which same to the configuration in Yu et al. (2018) and Yang et al. (2018). In 4041 this way, the representations are learned in a hierarchical fashion (Yang et al., 2017). That is, the distance-aware and local information extracted by the lower SAN layers, is expected to complement distance-agnostic and global information captured by the higher SAN layers. 3.2 Attention Interaction via 2D Convolution Mutli-head mechanism allows different heads to capture distinct linguistic properties (Raganato"
N19-1407,D18-1475,1,0.240925,"e elements. In addition, the performance of SANs can be improved by multi-head attention (Vaswani et al., 2017), which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace. Despite their success, SANs have two major limitations. First, the model fully take into ac∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. count all the elements, which disperses the attention distribution and thus overlooks the relation of neighboring elements and phrasal patterns (Yang et al., 2018; Wu et al., 2018; Guo et al., 2019). Second, multi-head attention extracts distinct linguistic properties from each subspace in a parallel fashion (Raganato and Tiedemann, 2018), which fails to exploit useful interactions across different heads. Recent work shows that better features can be learned if different sets of representations are present at feature learning time (Ngiam et al., 2011; Lin et al., 2014). To this end, we propose novel convolutional self-attention networks (C SANs), which model locality for self-attention model and interactions between features learned by different attent"
N19-1407,D17-1150,1,0.868524,"n, SANs are only allowed to attend to the b h, V b h ), instead of all neighboring tokens (e.g., K the tokens in the sequence (e.g., Kh , Vh ). The SAN-based models are generally implemented as multiple layers, in which higher layers tend to learn semantic information while lower layers capture surface and lexical information (Peters et al., 2018; Raganato and Tiedemann, 2018). Therefore, we merely apply locality modeling to the lower layers, which same to the configuration in Yu et al. (2018) and Yang et al. (2018). In 4041 this way, the representations are learned in a hierarchical fashion (Yang et al., 2017). That is, the distance-aware and local information extracted by the lower SAN layers, is expected to complement distance-agnostic and global information captured by the higher SAN layers. 3.2 Attention Interaction via 2D Convolution Mutli-head mechanism allows different heads to capture distinct linguistic properties (Raganato and Tiedemann, 2018; Li et al., 2018), especially in diverse local contexts (Yang et al., 2018). We hypothesis that exploiting local properties across heads can further improve the performance of SANs. To this end, we expand the 1-dimensional window to a 2-dimensional a"
O12-1015,1998.amta-tutorials.5,0,0.229455,"Document Translation-Based. 144 Updated: June 9, 2012 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) 1. Introduction With the flourishing development of the Internet, the amount of information from a variety of domains is rising dramatically. Although the researchers have done a lot to develop high performance and effective monolingual Information Retrieval (IR), the diversity of information source and the explosive growth of information in different languages drove a great need for IR systems that could cross language boundaries [1]. Cross-Language Information Retrieval (CLIR) has become more important for people to access the information resources written in various languages. Besides, it is of a great significance to alignment documents in multiple languages for Statistical Machine Translation (SMT) systems, of which quality is heavily dependent upon the amount of parallel sentences used in constructing the system. In this paper, we focus on the problems of translation ambiguity, query generation and searching score which are keys to the retrieval performance. First of all, in order to increase the probability that the"
O12-1015,P02-1038,0,0.258696,"CLIR 3.1. Translation Model Currently, the good performing statistical machine translation systems are based on phrase-based models which translate small word sequences at a time. Generally speaking, translation model is common for contiguous sequences of words to translate as a whole. Phrasal translation is certainly significant for CLIR [10], as stated in Section 1. It can do a good job in dealing with term disambiguation. In this work, documents are translated using the translation model provided by Moses, where the log-linear model is considered for training the phrase-based system models [11], and is represented as: M p(e1I |f1 J ) exp( ¦ Om hm (e1I , f1 J )) m 1 M ¦ exp(¦ O h (e' m e '1I m I 1 (2) J , f1 )) m 1 where hm indicates a set of different models, λm means the scaling factors, and the denominator can be ignored during the maximization process. The most important models in Eq. (2) normally are phrase-based models which are carried out in source to target and target to source directions. The source document will maximize the equation to generate the translation including the words most likely to occur in the target document set. 147 Updated: June 9, 2012 Proceedings of the"
O12-1015,2005.mtsummit-papers.11,0,0.10154,"he BLEU (Bilingual Evaluation Understudy) is a classical automatic evaluation method for the translation quality of an MT system [18]. In this evaluation, the translation model is created using the parallel corpus, as described in Section 4. We use another 5,000 sentences from the TestSet1 for 3 Available at http://lucene.apache.org. 151 Updated: June 9, 2012 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) evaluation4. The BLEU value, we obtained, is 32.08. The result is higher than that of the results reported by Koehn in his work [14], of which the BLEU score is 30.1 for the same language pair we used in Europarl corpora. Although we did not use exactly the same data for constructing the translation model, the value of 30.1 was presented as a baseline of the English-Spanish translation quality in Europarl corpora. The BLEU score shows that our translation model performs very well, due to the large number of the training data we used and the pre-processing tasks we designed for cleaning the data. On the other hand, it reveals that the translation quality of our model is good. 5.3. Evaluation of CLIR Model In this section, t"
O12-1015,J03-1002,0,0.0236206,"al Data of Corpus Dataset Training Set TestSet1 TestSet2 Documents 2,900 1,022 23,342 Sentences 1,902,050 80,000 80,000 Size of corpus Words 23,411,545 5,735,464 7,217,827 Ave. words in document 50 6,612 309 4.2. Experimental Setup In order to evaluate our proposed model, the following tools have been used. 1 Available online at http://www.statmt.org/europarl/. 149 Updated: June 9, 2012 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) The probabilistic LMs are constructed on monolingual corpora by using the SRILM [15]. We use GIZA++ [16] to train the word alignment models for different pairs of languages of the Europarl corpus, and the phrase pairs that are consistent with the word alignment are extracted. For constructing the phrase-based statistical machine translation model, we use the open source Moses [17] toolkit, and the translation model is trained based on the log-linear model, as given in Eq. (2). The workflow of constructing the translation model is illustrated in Fig. 2 and it consists of the following main steps2: (1) Preparation of aligned parallel corpus. (2) Preprocessing of training data: tokenization, case c"
O12-1015,P07-2045,0,0.0171069,"llowing tools have been used. 1 Available online at http://www.statmt.org/europarl/. 149 Updated: June 9, 2012 Proceedings of the Twenty-Fourth Conference on Computational Linguistics and Speech Processing (ROCLING 2012) The probabilistic LMs are constructed on monolingual corpora by using the SRILM [15]. We use GIZA++ [16] to train the word alignment models for different pairs of languages of the Europarl corpus, and the phrase pairs that are consistent with the word alignment are extracted. For constructing the phrase-based statistical machine translation model, we use the open source Moses [17] toolkit, and the translation model is trained based on the log-linear model, as given in Eq. (2). The workflow of constructing the translation model is illustrated in Fig. 2 and it consists of the following main steps2: (1) Preparation of aligned parallel corpus. (2) Preprocessing of training data: tokenization, case conversion, and sentences filtering where sentences with length greater than fifty words are removed from the corpus in order to comply with the requirement of Moses. (3) A 5-gram LM is trained on Spanish data with the SRILM toolkits. (4) The phrased-based STM model is therefore"
O12-1015,P02-1040,0,0.0824762,"Missing"
O12-1015,J93-2003,0,\N,Missing
O12-5002,P91-1022,0,0.476947,"Missing"
O12-5002,J93-2003,0,0.0808495,"Missing"
O12-5002,2005.mtsummit-papers.11,0,0.0202666,"Missing"
O12-5002,P07-2045,0,0.0102561,"Missing"
O12-5002,1998.amta-tutorials.5,0,0.224329,"Missing"
O12-5002,P02-1038,0,0.130416,"Missing"
O12-5002,J03-1002,0,0.0112601,"Missing"
O12-5002,P02-1040,0,0.0816796,"Missing"
O12-5002,H91-1026,0,\N,Missing
P13-1076,P11-1144,0,0.05362,"ive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011). As far as we know, however, these methods have not yet been applied to resolve the problem of joint Chinese word segmentation (CWS) and POS tagging. Motivated by the works in (Subramanya et al., 2010; Das and Smith, 2011), for structured problems, graph-based label propagation can be employed to infer valuable syntactic information (ngram-level label distributions) from labeled data to unlabeled data. This study extends this intuition to construct a similarity graph for propagating trigram-level label distributions. The derived label distributions are regarded as prior knowledge to regulariz"
P13-1076,P11-1061,0,0.0350223,"ription Trigram + Context Trigram Left Context Right Context Center Word Trigram - Center Word Left Word + Right Context Right Word + Left Context Type of Trigram: number, punctuation, alphabetic letter and other In most graph-based label propagation tasks, the final effect depends heavily on the quality of the graph. Graph construction thus plays a central role in graph-based label propagation (Zhu et al., 2003). For character-based joint S&T, unlike the unstructured learning problem whose vertices are formed directly by labeled and unlabeled instances, the graph construction is non-trivial. Das and Petrov (2011) mentioned that taking individual characters as the vertices would result in various ambiguities, whereas the similarity measurement is still challenging if vertices corresponding to entire sentences. This study follows the intuitions of graph construction from Subramanya et al. (2010) in which vertices are represented by character trigrams occurring in labeled and unlabeled sentences. Formally, given a set of labeled sentences Dl , and unlabeled ones Du , where D , {Dl , Du }, the goal is to form an undirected weighted graph G = (V, E), where V is defined as the set of vertices which covers a"
P13-1076,N12-1086,0,0.657363,"o be a probability distribution, and fk (yij−1 , yij , xi , j). In this study, 771 the baseline feature templates of joint S&T are the ones used in (Ng and Low, 2004; Jiang et al., 2008), as shown in Table 1. Λ = {λ1 λ2 ...λK } ∈ RK are the weight parameters to be learned. In supervised training, the aim is to estimate the Λ that maximizes the conditional likelihood of the training data while regularizing model parameters: L(Λ) = l X i=1 log p(yi |xi ; Λ) − R(Λ) properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse Inducing Penalties (Das and Smith, 2012). (2) 4 The emphasis of this work is on building a joint S&T model based on two different kinds of data sources, labeled and unlabeled data. In essence, this learning problem can be treated as incorporating certain gainful information, e.g., prior knowledge or label constraints, of unlabeled data into the supervised model. The proposed approach employs a transductive graph-based label propagation"
P13-1076,P08-1102,0,0.01861,"n features derived from unlabeled data. Different from their concern, our emphasis is to learn the semi-supervised model by injecting the label information from a similarity graph constructed from labeled and unlabeled data. The induction method of the proposed approach p(yi |xi ; Λ) = N K XX 1 exp{ λk fk (yij−1 , yij , xi , j)} Z(xi ; Λ) j=1 k=1 (1) where Z(xi ; Λ) is the partition function that normalizes the exponential form to be a probability distribution, and fk (yij−1 , yij , xi , j). In this study, 771 the baseline feature templates of joint S&T are the ones used in (Ng and Low, 2004; Jiang et al., 2008), as shown in Table 1. Λ = {λ1 λ2 ...λK } ∈ RK are the weight parameters to be learned. In supervised training, the aim is to estimate the Λ that maximizes the conditional likelihood of the training data while regularizing model parameters: L(Λ) = l X i=1 log p(yi |xi ; Λ) − R(Λ) properties derived from the graph, e.g., smoothness (Zhu et al., 2003; Subramanya et al., 2010; Talukdar et al., 2008), or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdar and Crammer, 2009) and Sparse Induc"
P13-1076,P09-1059,0,0.0833383,"h Tagging Xiaodong Zeng† Derek F. Wong† Lidia S. Chao† Isabel Trancoso‡ Department of Computer and Information Science, University of Macau ‡ INESC-ID / Instituto Superior T´ecnico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo, isabel.trancoso@inesc-id.pt † Abstract and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. G"
P13-1076,P06-1027,0,0.0785357,"g interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011)."
P13-1076,P09-1058,0,0.158693,"Missing"
P13-1076,P10-1149,0,0.129322,"Missing"
P13-1076,D09-1134,0,0.290005,"04). The joint approaches of word segmentation and POS tagging (joint S&T) are proposed to resolve these two tasks simultaneously. They effectively alleviate the error propagation, because segmentation 770 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 770–779, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics labeled and unlabeled data to achieve the semisupervised learning. The approach performs the incorporation of the derived labeled distributions by manipulating a “virtual evidence” function as described in (Li, 2009). Experiments on the data from the Chinese tree bank (CTB-7) and Microsoft Research (MSR) show that the proposed model results in significant improvement over other comparative candidates in terms of F-score and out-of-vocabulary (OOV) recall. This paper is structured as follows: Section 2 points out the main differences with the related work of this study. Section 3 reviews the background, including supervised character-based joint S&T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The c"
P13-1076,I11-1035,0,0.0980357,"in, development and test set were assigned with the composite tags as described in Section 3.1. 3 775 5.3 based label propagation toolkit that provides several state-of-the-art algorithms. Data Train Develop Test #Sent 17,968 1,659 2,037 #Word 374,697 46,637 65,219 #Char 596,360 79,283 104,502 #OOV 0.074 0.089 Table 3: Training, development and testing data. 5.2 Baseline and Proposed Models In the experiment, the baseline supervised pipeline and joint S&T models are built only on the train data. The proposed model will also be compared with the semi-supervised pipeline S&T model described in (Wang et al., 2011). In addition, two state-of-the-art semi-supervised CRFs algorithms, Jiao’s CRFs (Jiao et al., 2006) and Subramanya’s CRFs (Subramanya et al., 2010), are also used to build joint S&T models. The corresponding settings of the above candidates are listed below: Main Results This experiment yielded a similarity graph that consists of 462,962 trigrams from labeled and unlabeled data. The majority (317,677 trigrams) occurred only in unlabeled data. Based on the development data, the hyperparameters of our model were tuned among the following settings: for the graph propagation, µ ∈ {0.2, 0.5, 0.8}"
P13-1076,N07-2028,0,0.361293,"ults in significant improvement over other comparative candidates in terms of F-score and out-of-vocabulary (OOV) recall. This paper is structured as follows: Section 2 points out the main differences with the related work of this study. Section 3 reviews the background, including supervised character-based joint S&T model based on CRFs and graph-based label propagation. Section 4 presents the details of the proposed approach. Section 5 reports the experiment results. The conclusion is drawn in Section 6. also differs from other semi-supervised CRFs algorithms. Jiao et al. (2006), extended by Mann and McCallum (2007), reported a semi-supervised CRFs model which aims to guide the learning by minimizing the conditional entropy of unlabeled data. The proposed approach regularizes the CRFs by the graph information. Subramanya et al. (2010) proposed a graph-based self-train style semi-supervised CRFs algorithm. In the proposed approach, an analogous way of graph construction intuition is applied. But overall, our approach differs in three important aspects: first, novel feature templates are defined for measuring the similarity between vertices. Second, the critical property, i.e., sparsity, is considered amon"
P13-1076,I08-4029,0,0.0551474,"Missing"
P13-1076,P07-2055,0,0.0139759,"Missing"
P13-1076,C08-1128,0,0.0497034,"Missing"
P13-1076,W04-3236,0,0.787263,"inese Word Segmentation and Part-of-Speech Tagging Xiaodong Zeng† Derek F. Wong† Lidia S. Chao† Isabel Trancoso‡ Department of Computer and Information Science, University of Macau ‡ INESC-ID / Instituto Superior T´ecnico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo, isabel.trancoso@inesc-id.pt † Abstract and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to"
P13-1076,P08-1101,0,0.0453526,"ation and Part-of-Speech Tagging Xiaodong Zeng† Derek F. Wong† Lidia S. Chao† Isabel Trancoso‡ Department of Computer and Information Science, University of Macau ‡ INESC-ID / Instituto Superior T´ecnico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo, isabel.trancoso@inesc-id.pt † Abstract and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervise"
P13-1076,D12-1046,0,0.291926,"Missing"
P13-1076,D10-1082,0,0.0618309,"eng† Derek F. Wong† Lidia S. Chao† Isabel Trancoso‡ Department of Computer and Information Science, University of Macau ‡ INESC-ID / Instituto Superior T´ecnico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo, isabel.trancoso@inesc-id.pt † Abstract and tagging have strong interaction, given that most segmentation ambiguities cannot be resolved without considering the surrounding grammatical constructions encoded in a POS sequence (Qian and Liu, 2012). In the past years, several proposed supervised joint models (Ng and Low, 2004; Zhang and Clark, 2008; Jiang et al., 2009; Zhang and Clark, 2010) achieved reasonably accurate results, but the outstanding problem among these models is that they rely heavily on a large amount of labeled data, i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. Graph-based label propaga"
P13-1076,Y06-1012,0,0.0152817,"Missing"
P13-1076,D10-1017,0,0.360523,", i.e., segmented texts with POS tags. However, the production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011). As far as we know, however, these methods have not yet been applied to resolve the problem of joint Chinese word segmentation (CWS) and POS tagging. Motivated by the works in (Subramanya et al., 2010; Das and Smith, 2011), for structured problems, graph-based label propagation can be employed to infer valuable syntactic information (ngram-level label distributions) from labeled data to unlabeled data. This study extends this intuition to construct a similarity graph for propag"
P13-1076,P11-1139,0,0.0830342,"Missing"
P13-1076,D11-1090,0,0.152421,"Missing"
P13-1076,D08-1061,0,0.239557,"he production of such labeled data is extremely timeconsuming and expensive (Jiao et al., 2006; Jiang et al., 2009). Therefore, semi-supervised joint S&T appears to be a natural solution for easily incorporating accessible unlabeled data to improve the joint S&T model. This study focuses on using a graph-based label propagation method to build a semi-supervised joint S&T model. Graph-based label propagation methods have recently shown they can outperform the state-of-the-art in several natural language processing (NLP) tasks, e.g., POS tagging (Subramanya et al., 2010), knowledge acquisition (Talukdar et al., 2008), shallow semantic parsing for unknown predicate (Das and Smith, 2011). As far as we know, however, these methods have not yet been applied to resolve the problem of joint Chinese word segmentation (CWS) and POS tagging. Motivated by the works in (Subramanya et al., 2010; Das and Smith, 2011), for structured problems, graph-based label propagation can be employed to infer valuable syntactic information (ngram-level label distributions) from labeled data to unlabeled data. This study extends this intuition to construct a similarity graph for propagating trigram-level label distributions. The de"
P13-2031,W02-1001,0,0.0100895,"representational power and consequently better recognition performance for in-ofvocabulary (IV) words. where Z(x; θc ) is a partition function that normalizes the exponential form to be a probability distribution, and f (x, y) are arbitrary feature functions. The aim of CRFs is to estimate the weight parameters θc that maximizes the conditional likelihood of the training data: θˆc = argmax φ(x, wi ) · θw where it maps the segmented sentence w to a global feature vector φ and denotes θw as its corresponding weight parameters. The parameters θw can be estimated by using the Perceptrons method (Collins, 2002) or other online learning algorithms, e.g., Passive Aggressive (Crammer et al., 2006). For the decoding, a beam search decoding method (Zhang and Clark, 2007) is used. Character-based CRFs Model pθc (y|x) = |w| X Semi-supervised Learning via Co-regularizing Both Models As mentioned earlier, the primary challenge of semi-supervised CWS concentrates on the unlabeled data. Obviously, the learning on unlabeled data does not come for “free”. Very often, it is necessary to discover certain gainful information, e.g., label constraints of unlabeled data, that is incorporated to guide the learner towar"
P13-2031,P12-1110,0,0.0343364,"Missing"
P13-2031,P08-1102,0,0.0227771,"e α value is tuned using the development data. 4 #Sentdev 350 2,079 10,136 Table 1: Statistics of CTB-5, CTB-6 and CTB-7 data. The Joint Score Function for Decoding Score(w) = α · log(pθc (y|x)) +(1 − α) · log(φ(x, w) · θw ) #Senttrain 18,089 23,420 31,131 Experiment Setting The experimental data is taken from the Chinese tree bank (CTB). In order to make a fair comparison with the state-of-the-art results, the versions of CTB-5, CTB-6, and CTB-7 are used for the evaluation. The training, development and testing sets are defined according to the previous works. For CTB-5, the data split from (Jiang et al., 2008) is employed. For CTB-6, the same data split as recommended in the CTB-6 official document is used. For CTB-7, the datasets are formed according to the way in (Wang et al., 2011). The corresponding statistic information on these data splits is reported in Table 1. The unlabeled data in 1 The “baseline” uses a different training configuration so that the α values in the decoding are also need to be tuned on the development sets. The tuned α values are {0.6, 0.6, 0.5} for CTB-5, CTB-6 and CTB-7. 174 bold scores indicate that our model does achieve significant gains over these two semi-supervised"
P13-2031,P09-1059,0,0.144133,"Missing"
P13-2031,P11-1139,0,0.0489767,"to better combine the strengths of the two models, the proposed approach uses a joint scoring function in a log-linear combination form for the decoding in the segmentation phase. Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled da171 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171–176, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Segmentation Models enumerates a set of segmentation candidates as GEN:w = GEN(x) for x. The objective is to maximize the following problem for all sentences: There are two classes of CWS models: characterbased and word-based. This section briefly reviews t"
P13-2031,D11-1090,0,0.45216,"duction of segmented Chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is very hard (Jiao et al., 2006). So, one cannot rely only on the manually segmented data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few semi-supervised CWS models have been proposed. Xu et al. (2008) described a Bayesian semisupervised model by considering the segmentation as the hidden variable in machine translation. Sun and Xu (2011) enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a CRFs model. Another similar trial via “feature engineering” was conducted by Wang et al. (2011). This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the “segmentation agreements” between the two different types of view are used to overcome the scarcity of the label information on unlabeled data. The proposed approach trains a character-based and word-based model"
P13-2031,Q13-1001,0,0.0846245,"Missing"
P13-2031,C10-1132,0,0.0370132,"Missing"
P13-2031,I11-1035,0,0.312676,"data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few semi-supervised CWS models have been proposed. Xu et al. (2008) described a Bayesian semisupervised model by considering the segmentation as the hidden variable in machine translation. Sun and Xu (2011) enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a CRFs model. Another similar trial via “feature engineering” was conducted by Wang et al. (2011). This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the “segmentation agreements” between the two different types of view are used to overcome the scarcity of the label information on unlabeled data. The proposed approach trains a character-based and word-based model on labeled data, respectively, as the initial models. Then, the two models are constantly updated using unlabeled examples, where the learning objective is maximizing their segmentation agreements. The agreemen"
P13-2031,C08-1128,0,0.110523,"´ecnico, Lisboa, Portugal nlp2ct.samuel@gmail.com, {derekfw, lidiasc}@umac.mo, isabel.trancoso@inesc-id.pt † Abstract ta. However, the production of segmented Chinese texts is time-consuming and expensive, since hand-labeling individual words and word boundaries is very hard (Jiao et al., 2006). So, one cannot rely only on the manually segmented data to build an everlasting model. This naturally provides motivation for using easily accessible raw texts to enhance supervised CWS models, in semisupervised approaches. In the past years, however, few semi-supervised CWS models have been proposed. Xu et al. (2008) described a Bayesian semisupervised model by considering the segmentation as the hidden variable in machine translation. Sun and Xu (2011) enhanced the segmentation results by interpolating the statistics-based features derived from unlabeled data to a CRFs model. Another similar trial via “feature engineering” was conducted by Wang et al. (2011). This paper presents a semi-supervised Chinese word segmentation (CWS) approach that co-regularizes character-based and word-based models. Similarly to multi-view learning, the “segmentation agreements” between the two different types of view are use"
P13-2031,O03-4002,0,0.532995,"texts without label information. Moreover, in order to better combine the strengths of the two models, the proposed approach uses a joint scoring function in a log-linear combination form for the decoding in the segmentation phase. Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled da171 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171–176, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Segmentation Models enumerates a set of segmentation candidates as GEN:w = GEN(x) for x. The objective is to maximize the following problem for all sentences: There are two classes of CWS models: charact"
P13-2031,P07-1106,0,0.406907,"on. Moreover, in order to better combine the strengths of the two models, the proposed approach uses a joint scoring function in a log-linear combination form for the decoding in the segmentation phase. Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled da171 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171–176, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Segmentation Models enumerates a set of segmentation candidates as GEN:w = GEN(x) for x. The objective is to maximize the following problem for all sentences: There are two classes of CWS models: characterbased and word-based. This section brief"
P13-2031,D10-1082,0,0.0377545,"Missing"
P13-2031,J11-1005,0,0.0141516,"each parameter CRFs with Constraints For the character-based model, this paper follows (T¨ackstr¨om et al., 2013) to incorporate the segmentation agreements into CRFs. The main 173 our experiments is from the XIN CMN portion of Chinese Gigaword 2.0. The articles published in 1991-1993 and 1999-2004 are used as unlabeled data, with 204 million words. The feature templates in (Zhao et al., 2006) and (Zhang and Clark, 2007) are used in train-ing the CRFs model and Perceptrons model, respectively. The experimental platform is implemented based on two popular toolkits: CRF++ (Kudo, 2005) and Zpar (Zhang and Clark, 2011). update iteration k, each raw sentence xu is decoded with the current model into a segmentation zu . If the words in output zu do not match the agreements A(xu ) of the current sentence xu , the parameters are updated by adding the global feature vector of the current training example with the agreements and subtracting the global feature vector of the decoder output, as described in lines 3 and 4 of Algorithm 2. Algorithm 2 Parameter update in word-based model 1: for k = 1...K, u = 1...m do P|w| k−1 2: calculate zu = argmax i=1 φ(xu , wi ) · θw Data CTB-5 CTB-6 CTB-7 w=GEN(x) 3: if zu 6= A(x"
P13-2031,Y06-1012,0,0.201841,"out label information. Moreover, in order to better combine the strengths of the two models, the proposed approach uses a joint scoring function in a log-linear combination form for the decoding in the segmentation phase. Introduction Chinese word segmentation (CWS) is a critical and a necessary initial procedure with respect to the majority of high-level Chinese language processing tasks such as syntax parsing, information extraction and machine translation, since Chinese scripts are written in continuous characters without explicit word boundaries. Although supervised CWS models (Xue, 2003; Zhao et al., 2006; Zhang and Clark, 2007; Sun, 2011) proposed in the past years showed some reasonably accurate results, the outstanding problem is that they rely heavily on a large amount of labeled da171 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 171–176, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Segmentation Models enumerates a set of segmentation candidates as GEN:w = GEN(x) for x. The objective is to maximize the following problem for all sentences: There are two classes of CWS models: characterbased and word-ba"
P13-2031,C10-2139,0,\N,Missing
P13-2031,P06-1027,0,\N,Missing
P14-1128,W05-0909,0,0.0182822,"This EM-style approach monotonically increases J (θ, q) and thus is guaranteed to converge to a local optimum. E-step: q (t+1) = arg minRU (θ(t) , q (t) ) q M-step: θ(t+1) = arg maxL(θ) θ +δ u X X i=1 y∈Y 4 q (t+1) (y|xi ) log pθ (y|xi ) (7) Experiments 4.1 Data and Setup The experiments in this study evaluated the performances of various CWS models in a Chineseto-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU (Papineni et al., 2002), NIST (Doddington et al., 2000) and METEOR (Banerjee and Lavie, 2005), to evaluate the translation quality. The monolingual segmented data, trainTB , is extracted from the Penn Chinese Treebank (CTB7) (Xue et al., 2005), containing 51,447 sentences. The bilingual training data, trainMT , is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7 . This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over"
P14-1128,W08-0336,0,0.546288,"accomplished by using posterior regularization algorithm. The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality. 1 Introduction Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) (Xu et al., 2005; Chang et al., 2008; Zhao et al., 2013). In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” (Ma and Way, 2009). The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008). But one outstanding"
P14-1128,D09-1075,0,0.140948,"nstraints over labelings to bias the model learning. The type-level word boundary extraction is formally described as follows. Given the ith sentence pair hxci , xfi , Ac→f i of the aligned bilini c↔f gual corpus D , the Chinese sentence xci consisting of m characters {xci,1 , xci,2 , ..., xci,m }, and the foreign language sentence xfi , consisting of The gainful supervisions toward a better segmentation solution for SMT are naturally extracted from MT training resources, i.e., bilingual parallel data. This study employs an approximated method introduced in (Xu et al., 2004; Ma and Way, 2009; Chung and Gildea, 2009) to learn bilingual seg1 The distribution is on four word boundary labels indicating the character positions in a word, i.e., B (begin), M (middle), E (end) and S (single character). 2 A word boundary distribution corresponds to the center character of a type. In fact, it aims at reducing label ambiguities to collect boundary information of character trigrams, rather than individual characters (Altun et al., 2006). 1362 n words {xfi,1 , xfi,2 , ..., xfi,n }, Ac→f represents a i hCj , xfi,j i set of alignment pairs aj = that defines connections between a few Chinese characters Cj = {xci,j1 , xc"
P14-1128,P11-1061,0,0.149239,"nments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. (2010), Das and Petrov (2011), Zeng et al. (2013; 2014) and Zhu et al. (2014), proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bias CRFs model’s learning"
P14-1128,N12-1086,0,0.0179428,"onstraint in Section 3.3 that reflects the interactions between the graph and the CRFs model. In other words, GP is integrated with estimation of parametric structural model. This is greatly different from the prior pipelined approaches (Subramanya et al., 2010; Das and Petrov, 2011; Zeng et al., 2013), where GP is run first and its propagated outcomes are then used to bias the structural model. This work seeks to capture the GP benefits during the modeling of sequential correlations. In what follows, the graph setting and propagation expression are introduced. As in conventional GP examples (Das and Smith, 2012), a similarity graph G = (V, E) is constructed over N types extracted from Chinese training data, including treebank Dlc and bitexts Duc . Each vertex Vi has a |T |-dimensional estimated measure vi = {vi,t ; t ∈ T } representing a probability distribution on word boundary tags. The induced typelevel word boundary distributions ri = {ri,t ; t ∈ T } are empirical measures for the corresponding M graph vertices. The edges E ∈ Vi × Vj connect all the vertices. Scores between pairs of graph vertices (types), wij , refer to the similarities of their syntactic environment, which are computed followin"
P14-1128,W13-3505,0,0.0555978,"xi ) Where Zθ (xi ) is a partition function that normalizes the exponential form to be a probability distribution, and f (yik−1 , yik , xi ) are arbitrary feature functions. In our setting, the CRFs model is required to learn from unlabeled data. This work employs the posterior regularization (PR) framework3 (Ganchev et al., 2010) to bias the CRFs model’s learning on unlabeled data, under a constraint encoded by the graph propagation expression. It is expected that similar types in the graph should have approximated expected taggings under the CRFs model. We follow the approach introduced by (He et al., 2013) to set up a penaltybased PR objective with GP: the CRFs likelihood is modified by adding a regularization term, as shown in Equation 4, representing the constraints: RU (θ, q) = KL(q||pθ ) + λP(v) (4) Rather than regularize CRFs model’s posteriors pθ (Y|xi ) directly, our model uses an auxiliary distribution q(Y|xi ) over the possible labelings 3 The readers are refered to the original paper of Ganchev et al. (2010). m X T X X 1(y b = t, y b−1 = c)q(y|xa ) c=1 y∈Y b=1; M(a,b)=Vi u X m X 1(M(a, b) = Vi ) a=1 b=1 (5) The final learning objective combines the CRFs likelihood with the PR regulari"
P14-1128,P07-2045,0,0.0083261,"o be tuned by using the development data (devMT ) among the following settings: for the graph propagation, µ ∈ {0.2, 0.5, 0.8} and ρ ∈ {0.1, 0.3, 0.5, 0.8}; for the PR learning, λ ∈ {0 ≤ λi ≤ 1} and σ ∈ {0 ≤ σi ≤ 1} where the step is 0.1. The best performed joint settings, µ = 0.5, ρ = 0.5, λ = 0.9 and σ = 0.8, were used to measure the final performance. The MT experiment was conducted based on a standard log-linear phrase-based SMT model. The GIZA++ aligner was also adopted to obtain word alignments (Och and Ney, 2003) over the segmented bitexts. The heuristic strategy of growdiag-final-and (Koehn et al., 2007) was used to combine the bidirectional alignments for extracting phrase translations and reordering tables. A 5-gram language model with Kneser-Ney smoothing was trained with SRILM (Stolcke, 2002) on monolingual English data. Moses (Koehn et al., 2007) was used as decoder. The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. 4.2 Various Segmentation Models To provide a thorough analysis, the MT experiments in this study evaluated three baseline segmentation models and two off-the-shelf models, in addition to four variant models that al"
P14-1128,E09-1063,0,0.697137,"ffects to translation quality. 1 Introduction Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) (Xu et al., 2005; Chang et al., 2008; Zhao et al., 2013). In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” (Ma and Way, 2009). The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually lin"
P14-1128,2005.iwslt-1.18,0,0.0267695,"odel induction is accomplished by using posterior regularization algorithm. The experiments on a Chinese-to-English machine translation task reveal that the proposed model can bring positive segmentation effects to translation quality. 1 Introduction Word segmentation is regarded as a critical procedure for high-level Chinese language processing tasks, since Chinese scripts are written in continuous characters without explicit word boundaries (e.g., space in English). The empirical works show that word segmentation can be beneficial to Chinese-to-English statistical machine translation (SMT) (Xu et al., 2005; Chang et al., 2008; Zhao et al., 2013). In fact most current SMT models assume that parallel bilingual sentences should be segmented into sequences of tokens that are meant to be “words” (Ma and Way, 2009). The practice in state-of-the-art MT systems is that Chinese sentences are tokenized by a monolingual supervised word segmentation model trained on the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008)."
P14-1128,P08-1099,0,0.0270901,"wledge. This is accomplished by the posterior regularization (PR) framework (Ganchev et 1361 al., 2010). PR performs regularization on posteriors, so that the learned model itself remains simple and tractable, while during learning it is driven to obey the constraints through setting appropriate parameters. The closest prior study is constrained learning, or learning with prior knowledge. Chang et al. (2008) described constraint driven learning (CODL) that augments model learning on unlabeled data by adding a cost for violating expectations of constraint features designed by domain knowledge. Mann and McCallum (2008) and McCallum et al. (2007) proposed to employ generalized expectation criteria (GE) to specify preferences about model expectations in the form of linear constraints on some feature expectations. 3 Methodology This work aims at building a CWS model adapted to the SMT task. The model induction is shown in Algorithm 1. The input data requires two types of training resources, segmented Chinese sentences from treebank Dlc and parallel unsegmented sentences of Chinese and foreign language Duc and Duf . The first step is to conduct characterbased alignment over bitexts Duc and Duf , where every Chi"
P14-1128,J03-1002,0,0.0487003,"upervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. They leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004), or form labeled data for training a sequence labeling model (Paul et al., 2011). The prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment (Och and Ney, 2003). However, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation. This paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not only to maintain the advantages of a monolingual supervised model, having hand-annotated linguistic knowledge, but also to assimilate the relevant bilingual segmenta1360 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1360–1369, c Baltimore"
P14-1128,P03-1021,0,0.0722543,"the final performance. The MT experiment was conducted based on a standard log-linear phrase-based SMT model. The GIZA++ aligner was also adopted to obtain word alignments (Och and Ney, 2003) over the segmented bitexts. The heuristic strategy of growdiag-final-and (Koehn et al., 2007) was used to combine the bidirectional alignments for extracting phrase translations and reordering tables. A 5-gram language model with Kneser-Ney smoothing was trained with SRILM (Stolcke, 2002) on monolingual English data. Moses (Koehn et al., 2007) was used as decoder. The Minimum Error Rate Training (MERT) (Och, 2003) was used to tune the feature parameters on development data. 4.2 Various Segmentation Models To provide a thorough analysis, the MT experiments in this study evaluated three baseline segmentation models and two off-the-shelf models, in addition to four variant models that also employ the bilingual constraints. We start from three baseline models: • Character Segmenter (CS): this model simply divides Chinese sentences into sequences of characters. • Supervised Monolingual Segmenter (SMS): this model is trained by CRFs on treebank training data (trainTB ). The same feature templates (Zhao et al"
P14-1128,P02-1040,0,0.0913939,"ameter estimation, where the gradient ascent approach still works. This EM-style approach monotonically increases J (θ, q) and thus is guaranteed to converge to a local optimum. E-step: q (t+1) = arg minRU (θ(t) , q (t) ) q M-step: θ(t+1) = arg maxL(θ) θ +δ u X X i=1 y∈Y 4 q (t+1) (y|xi ) log pθ (y|xi ) (7) Experiments 4.1 Data and Setup The experiments in this study evaluated the performances of various CWS models in a Chineseto-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU (Papineni et al., 2002), NIST (Doddington et al., 2000) and METEOR (Banerjee and Lavie, 2005), to evaluate the translation quality. The monolingual segmented data, trainTB , is extracted from the Penn Chinese Treebank (CTB7) (Xue et al., 2005), containing 51,447 sentences. The bilingual training data, trainMT , is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7 . This in-house bilingual corpus is the MT t"
P14-1128,D10-1017,0,0.364816,"erging “char-to-word” alignments to words as supervisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. (2010), Das and Petrov (2011), Zeng et al. (2013; 2014) and Zhu et al. (2014), proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bia"
P14-1128,tian-etal-2014-um,1,0.820676,"ated the performances of various CWS models in a Chineseto-English translation task. The influence of the word segmentation on the final translation is our main investigation. We adopted three state-of-the-art metrics, BLEU (Papineni et al., 2002), NIST (Doddington et al., 2000) and METEOR (Banerjee and Lavie, 2005), to evaluate the translation quality. The monolingual segmented data, trainTB , is extracted from the Penn Chinese Treebank (CTB7) (Xue et al., 2005), containing 51,447 sentences. The bilingual training data, trainMT , is formed by a large in-house Chinese-English parallel corpus (Tian et al., 2014). There are in total 2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7 . This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over 35 million monolingual English sentences, trainLM , crawled from online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, devMT , and testing data, testMT , respectively. For the settings of our model, we adopted the standard feature temp"
P14-1128,P12-2056,0,0.15256,"the handannotated treebank data, e.g., Chinese treebank (CTB) (Xue et al., 2005). These models are conducive to MT to some extent, since they commonly have relatively good aggregate performance and segmentation consistency (Chang et al., 2008). But one outstanding problem is that these models may leave out some crucial segmentation features for SMT, since the output words conform to the treebank segmentation standard designed for monolingually linguistic intuition, rather than specific to the SMT task. In recent years, a number of works (Xu et al., 2005; Chang et al., 2008; Ma and Way, 2009; Xi et al., 2012) attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data. They proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. They leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004), or form labeled da"
P14-1128,P13-1076,1,0.900623,"rvisions, this study extracts word boundary information of characters from the alignments as soft constraints to regularize a CRFs model’s learning. The graph propagation (GP) technique provides a natural way to represent data in a variety of target domains (Belkin et al., 2006). In this technique, the constructed graph has vertices consisting of labeled and unlabeled examples. Pairs of vertices are connected by weighted edges encoding the degree to which they are expected to have the same label (Zhu et al., 2003). Many recent works, such as by Subramanya et al. (2010), Das and Petrov (2011), Zeng et al. (2013; 2014) and Zhu et al. (2014), proposed GP for inferring the label information of unlabeled data, and then leverage these GP outcomes to learn a semi-supervised scalable model (e.g., CRFs). These approaches are referred to as pipelined learning with GP. This study also works with a similarity graph, encoding the learned bilingual knowledge. But, unlike the prior pipelined approaches, this study performs a joint learning behavior in which GP is used as a learning constraint to interact with the CRFs model estimation. One of our main objectives is to bias CRFs model’s learning on unlabeled data,"
P14-1128,W03-1730,0,0.0468148,"• Unsupervised Bilingual Segmenter (UBS): this model is trained on the bitexts (trainMT) following the approach introduced in (Ma and Way, 2009). The optimal set of the model parameter values was found on devMT to be k = 3, tAC = 0.0 and tCOOC = 15. The comparison candidates also involve two popular off-the-shelf segmentation models: • Stanford Segmenter: this model, trained by Chang et al. (2008), treats CWS as a binary word boundary decision task. It covers several features specific to the MT task, e.g., external lexicons and proper noun features. • ICTCLAS Segmenter: this model, trained by Zhang et al. (2003), is a hierarchical HMM segmenter that incorporates parts-ofspeech (POS) information into the probability models and generates multiple HMM models for solving segmentation ambiguities. This work also evaluated four variant models9 that perform alternative ways to incorporate the bilingual constraints based on two state-of-the-art graph-based SSL approaches. • Self-training Segmenters (STS): two variant models were defined by the approach reported in (Subramanya et al., 2010) that uses the supervised CRFs model’s decodings, incorporating empirical and constraint information, for unlabeled examp"
P14-1128,W08-0335,0,0.178558,"results of the proposed model for a Chinese-to-English MT task. The conclusion is drawn in Section 5. 2 Related Work In the literature, many approaches have been proposed to learn CWS models for SMT. They can be put into two categories, monolingual-motivated and bilingual-motivated. The former primarily optimizes monolingual supervised models according to some predefined segmentation properties that are manually summarized from empirical MT evaluations. Chang et al. (2008) enhanced a CRFs segmentation model in MT tasks by tuning the word granularity and improving the segmentation consistence. Zhang et al. (2008) produced a better segmentation model for SMT by concatenating various corpora regardless of their different specifications. Distinct from their behaviors, this work uses automatically learned constraints instead of manually defined ones. Most importantly, the constraints have a better learning guidance since they originate from the bilingual texts. On the other hand, the bilingual-motivated CWS models typically rely on character-based alignments to generate segmentation supervisions. Xu et al. (2004) proposed to employ “chars-to-word” alignments to generate a word dictionary for maximum match"
P14-1128,W06-0127,0,0.249394,"2,244,319 Chinese-English sentence pairs crawled from online resources, concentrated in 5 different domains including laws, novels, spoken, news and miscellaneous7 . This in-house bilingual corpus is the MT training data as well. The target-side language model is built on over 35 million monolingual English sentences, trainLM , crawled from online resources. The NIST evaluation campaign data, MT-03 and MT-05, are selected to comprise the MT development data, devMT , and testing data, testMT , respectively. For the settings of our model, we adopted the standard feature templates introduced by Zhao et al. (2006) for CRFs. The character-based alignment for achieving the “chars-to-word” mappings is accomplished by GIZA++ aligner (Och and Ney, 2003). For the GP, a 10-NNs similarity graph 7 The in-house corpus has been manually validated, in a long process that exceeded 500 hours. was constructed8 . Following (Subramanya et al., 2010; Zeng et al., 2013), the features used to compute similarities between vertices were (Suppose given a type “ w2 w3 w4 ” surrounding contexts “w1 w2 w3 w4 w5 ”): unigram (w3 ), bigram (w1 w2 , w4 w5 , w2 w4 ), trigram (w2 w3 w4 , w2 w4 w5 , w1 w2 w4 ), trigram+context (w1 w2"
P14-1128,W04-1118,0,0.674584,"; Ma and Way, 2009; Xi et al., 2012) attempted to build segmentation models for SMT based on bilingual unsegmented data, instead of monolingual segmented data. They proposed to learn gainful bilingual knowledge as golden-standard segmentation supervisions for training a bilingual unsupervised model. Frequently, the bilingual knowledge refers to the mappings of an individual English word to one or more consecutive Chinese characters, generated via statistical character-based alignment. They leverage such mappings to either constitute a Chinese word dictionary for maximum-matching segmentation (Xu et al., 2004), or form labeled data for training a sequence labeling model (Paul et al., 2011). The prior works showed that these models help to find some segmentations tailored for SMT, since the bilingual word occurrence feature can be captured by the character-based alignment (Och and Ney, 2003). However, these models tend to miss out other linguistic segmentation patterns as monolingual supervised models, and suffer from the negative effects of erroneously alignments to word segmentation. This paper proposes an alternative Chinese Word Segmentation (CWS) model adapted to the SMT task, which seeks not o"
P19-1176,P17-4012,0,0.233608,"ntion model. In this work, we continue the line of research and go towards a much deeper encoder for Transformer. We choose encoders to study because they have a greater impact on performance than decoders and require less computational cost (Domhan, 2018). Our contributions are threefold: • We show that the proper use of layer normalization is the key to learning deep encoders. The deep network of the encoder can be optimized smoothly by relocating the layer normalization unit. While the location of layer normalization has been discussed in recent systems (Vaswani et al., 2018; Domhan, 2018; Klein et al., 2017), as far as we know, its impact has not been studied in deep Trans2 For example, a standard Transformer encoder has 6 layers. Each of them consists of two sub-layers. More sub-layers are involved on the decoder side. 1810 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810–1822 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics xl F L yl LN xl+1 (a) post-norm residual unit xl LN F yl L xl+1 (b) pre-norm residual unit Figure 1: Examples of pre-norm residual unit and postnorm residual unit. F = sub-layer, an"
P19-1176,D15-1166,0,0.0342435,"Transformer-Big/Base baseline (6-layer encoder) by 0.4∼2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big1 . 1 Introduction Neural machine translation (NMT) models have advanced the previous state-of-the-art by learning mappings between sequences via neural networks and attention mechanisms (Sutskever et al., 2014; Bahdanau et al., 2015). The earliest of these read and generate word sequences using a series of recurrent neural network (RNN) units, and the improvement continues when 4-8 layers are stacked for a deeper model (Luong et al., 2015; Wu et al., 2016). More recently, the system based on multi-layer self-attention (call it Transformer) has shown strong results on several large∗ Corresponding author. The source code is available at https://github. com/wangqiangneu/dlcl 1 scale tasks (Vaswani et al., 2017). In particular, approaches of this kind benefit greatly from a wide network with more hidden states (a.k.a. Transformer-Big), whereas simply deepening the network has not been found to outperform the “shallow” counterpart (Bapna et al., 2018). Do deep models help Transformer? It is still an open question for the discipline"
P19-1176,W18-6301,0,0.170192,"Big is updated for 100k/300k steps on the En-De task as Vaswani et al. (2017), 50k/100k steps on the Zh-En-Small task, and 200k/500k steps on the Zh-En-Large task. In our model, we use the dynamic linear combination of layers for both encoder and decoder. For efficient computation, we only combine the output of a complete layer rather than a sub-layer. It should be noted that for deep models (e.g. L ≥ 20), it is hard to handle a full batch in a single GPU due to memory size limitation. We solve this issue by accumulating gradients from two small batches (e.g. batch = 2048) before each update (Ott et al., 2018). In our primitive experiments, we observed that training with larger batches and learning rates worked well for deep models. Therefore all the results of deep models are reported with batch = 8192, lr = 2×10−3 and warmup = 16,000 unless otherwise stated. For fairness, we only use half of the updates of baseline (e.g. update = 50k) to ensure the same amount of data that we actually 12 https://github.com/tensorflow/ tensor2tensor Results Results on the En-De Task In Table 1, we first report results on WMT En-De where we compare to the existing systems based on self-attention. Obviously, while a"
P19-1176,P18-1167,0,0.157394,"canu et al., 2013; Bapna et al., 2018). We note that, despite the significant development effort, simply stacking more layers cannot benefit the system and leads to a disaster of training in some of our experiments. A promising attempt to address this issue is Bapna et al. (2018)’s work. They trained a 16layer Transformer encoder by using an enhanced attention model. In this work, we continue the line of research and go towards a much deeper encoder for Transformer. We choose encoders to study because they have a greater impact on performance than decoders and require less computational cost (Domhan, 2018). Our contributions are threefold: • We show that the proper use of layer normalization is the key to learning deep encoders. The deep network of the encoder can be optimized smoothly by relocating the layer normalization unit. While the location of layer normalization has been discussed in recent systems (Vaswani et al., 2018; Domhan, 2018; Klein et al., 2017), as far as we know, its impact has not been studied in deep Trans2 For example, a standard Transformer encoder has 6 layers. Each of them consists of two sub-layers. More sub-layers are involved on the decoder side. 1810 Proceedings of"
P19-1176,D18-1457,0,0.24854,"value. For pre-norm Transformer, we define G(·) 5 Some of the other single-step methods, e.g. the RungeKutta method, can obtain a higher order by taking several intermediate steps (Butcher, 2003). Higher order generally means more accurate. 1812 y0 y1 y2 y3 x1 x2 x3 x4 y0 y1 y2 y3 x1 x2 x3 x4 1 0 1 0 0 1 0 0 0 1 y0 y1 y2 y3 x1 x2 x3 x4 1 (a) 1 1 1 1 1 1 1 1 1 (b) 1 0 1 0 0 1 .1 .3 .2 .4 y0 y1 y2 y3 x1 x2 x3 x4 1.8 (c) .4 1.2 .3 .2 .8 .1 .3 .5 .7 (d) Figure 2: Connection weights for 3-layer encoder: (a) residual connection (He et al., 2016a), (b) dense residual connection (Britz et al., 2017; Dou et al., 2018), (c) multi-layer representation fusion (Wang et al., 2018b)/transparent attention (Bapna et al., 2018) and (d) our approach. y0 denotes the input embedding. Red denotes the weights are learned by model. et al., 2017; Dou et al., 2018). Multi-layer representation fusion (Wang et al., 2018b) and transparent attention (call it TA) (Bapna et al., 2018) methods can learn a weighted model to fuse layers but they are applied to the topmost layer only. The DLCL model can cover all these methods. It provides ways of weighting and connecting layers in the entire stack. We emphasize that although the id"
P19-1176,N18-1202,0,0.0478456,"encoder by resorting to auxiliary losses in intermediate layers. This method is orthogonal to our DLCL method, though it is used for language modeling, which is not a very heavy task. Densely Residual Connections. Densely residual connections are not new in NMT. They have been studied for different architectures, e.g., RNN (Britz et al., 2017) and Transformer (Dou et al., 2018). Some of the previous studies fix the weight of each layer to a constant, while others learn a weight distribution by using either the self-attention model (Wang et al., 2018b) or a softmax-normalized learnable vector (Peters et al., 2018). They focus more on learning connections from lower-level layers to the topmost layer. Instead, we introduce additional connectivity into the network and learn more densely connections for each layer in an end-to-end fashion. 8 Conclusion We have studied deep encoders in Transformer. We have shown that the deep Transformer models can be easily optimized by proper use of layer normalization, and have explained the reason behind it. Moreover, we proposed an approach based on a dynamic linear combination of layers and successfully trained a 30-layer Transformer system. It is the deepest encoder"
P19-1176,P16-1009,0,0.0361058,"example, the standard residual network is a special case of DLCL, where Wll+1 = 1, and Wkl+1 = 0 for k < l. Figure (2) compares different methods of connecting a 3-layer network. We see that the densely residual network is a fully-connected network with a uniform weighting schema (Britz 4 Experimental Setup We first evaluated our approach on WMT’16 English-German (En-De) and NIST’12 ChineseEnglish (Zh-En-Small) benchmarks respectively. To make the results more convincing, we also experimented on a larger WMT’18 Chinese-English dataset (Zh-En-Large) with data augmentation by back-translation (Sennrich et al., 2016a). 4.1 Datasets and Evaluation For the En-De task, to compare with Vaswani et al. (2017)’s work, we use the same 4.5M preprocessed data 7 , which has been tokenized and 6 Let the encoder depth be M and the decoder depth be N (M &gt; N for a deep encoder model). Then TA newly adds O(M × N ) connections, which are fewer than ours of O(M 2 ) 7 https://drive.google.com/uc?export= download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8 1813 Model Param. Vaswani et al. (2017) (Base) 65M 137M 213M 379M † 210M † 210M 356M 210M 62M 211M 106M 62M 121M 62M 211M 106M 62M 137M Bapna et al. (2018)-deep (Base, 16L) Vaswani e"
P19-1176,P16-1162,0,0.138954,"example, the standard residual network is a special case of DLCL, where Wll+1 = 1, and Wkl+1 = 0 for k < l. Figure (2) compares different methods of connecting a 3-layer network. We see that the densely residual network is a fully-connected network with a uniform weighting schema (Britz 4 Experimental Setup We first evaluated our approach on WMT’16 English-German (En-De) and NIST’12 ChineseEnglish (Zh-En-Small) benchmarks respectively. To make the results more convincing, we also experimented on a larger WMT’18 Chinese-English dataset (Zh-En-Large) with data augmentation by back-translation (Sennrich et al., 2016a). 4.1 Datasets and Evaluation For the En-De task, to compare with Vaswani et al. (2017)’s work, we use the same 4.5M preprocessed data 7 , which has been tokenized and 6 Let the encoder depth be M and the decoder depth be N (M &gt; N for a deep encoder model). Then TA newly adds O(M × N ) connections, which are fewer than ours of O(M 2 ) 7 https://drive.google.com/uc?export= download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8 1813 Model Param. Vaswani et al. (2017) (Base) 65M 137M 213M 379M † 210M † 210M 356M 210M 62M 211M 106M 62M 121M 62M 211M 106M 62M 137M Bapna et al. (2018)-deep (Base, 16L) Vaswani e"
P19-1176,N18-2074,0,0.109651,"Missing"
P19-1176,1983.tc-1.13,0,0.401031,"Missing"
P19-1176,P12-3004,1,0.877973,"Updates, which can be used to approximately measure the required training time. † denotes an estimate value. Note that “-deep” represents the best-achieved result as depth changes. jointly byte pair encoded (BPE) (Sennrich et al., 2016b) with 32k merge operations using a shared vocabulary 8 . We use newstest2013 for validation and newstest2014 for test. For the Zh-En-Small task, we use parts of the bitext provided within NIST’12 OpenMT9 . We choose NIST MT06 as the validation set, and MT04, MT05, MT08 as the test sets. All the sentences are word segmented by the tool provided within NiuTrans (Xiao et al., 2012). We remove the sentences longer than 100 and end up with about 1.9M sentence pairs. Then BPE with 32k operations is used for both sides independently, resulting in a 44k Chinese vocabulary and a 33k English vocabulary respectively. For the Zh-En-Large task, we use exactly the same 16.5M dataset as Wang et al. (2018a), composing of 7.2M-sentence CWMT corpus, 4.2M-sentence UN and News-Commentary combined corpus, and back-translation of 5M-sentence monolingual data from NewsCraw2017. We refer the reader to Wang et al. (2018a) for the details. 8 The tokens with frequencies less than 5 are filtere"
P19-1176,Q16-1027,0,0.0280018,"Missing"
P19-1176,W18-1819,0,0.243782,"er encoder by using an enhanced attention model. In this work, we continue the line of research and go towards a much deeper encoder for Transformer. We choose encoders to study because they have a greater impact on performance than decoders and require less computational cost (Domhan, 2018). Our contributions are threefold: • We show that the proper use of layer normalization is the key to learning deep encoders. The deep network of the encoder can be optimized smoothly by relocating the layer normalization unit. While the location of layer normalization has been discussed in recent systems (Vaswani et al., 2018; Domhan, 2018; Klein et al., 2017), as far as we know, its impact has not been studied in deep Trans2 For example, a standard Transformer encoder has 6 layers. Each of them consists of two sub-layers. More sub-layers are involved on the decoder side. 1810 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810–1822 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics xl F L yl LN xl+1 (a) post-norm residual unit xl LN F yl L xl+1 (b) pre-norm residual unit Figure 1: Examples of pre-norm residual unit and postno"
P19-1176,P17-1013,0,0.0334172,"ntext of neural machine translation since the emergence of RNN-based models. To ease optimization, researchers tried to reduce the number of non-linear transitions (Zhou et al., x1 4 8 x1 x6 6 x2 x3 4 x4 2 x11 2 x5 x6 0 x7 y0 x16 0 y1 y2 4.1 3.3 3.2 1.7 2.3 0.2 0.5 y5 y10 y15 y20 y25 y6 −2 1.1 0.0 0.0 0.1 0.8 0.2 0.0 0.5 x22 ∼ x31 −4 y0 y5 x11 ∼ x21 −2 x31 y4 (b) 6-layer decoder of DLCL x21 x26 y3 0.0 0.5 0.2 0.0 0.0 0.1 y30 (a) 30-layer encoder of DLCL (c) Weight distribution of y10 in the encoder Figure 5: A visualization example of learned weights in our 30-layer pre-norm DLCL model. 2016; Wang et al., 2017). But these attempts are limited to the RNN architecture and may not be straightforwardly applicable to the current Transformer model. Perhaps, the most relevant work to what is doing here is Bapna et al. (2018)’s work. They pointed out that vanilla Transformer was hard to train if the depth of the encoder was beyond 12. They successfully trained a 16-layer Transformer encoder by attending the combination of all encoder layers to the decoder. In their approach, the encoder layers are combined just after the encoding is completed, but not during the encoding process. In contrast, our approach a"
P19-1176,W18-6430,1,0.928098,"proach to learning deep networks, and plays an important role in Transformer. In principle, residual networks can be seen as instances of the ordinary differential equation (ODE), behaving like the forward Euler discretization with an initial value (Chang et al., 2018; Chen et al., 2018b). Euler’s method is probably the most popular firstorder solution to ODE. But it is not yet accurate enough. A possible reason is that only one previous step is used to predict the current value 5 (Butcher, 2003). In MT, the single-step property of the residual network makes the model “forget” distant layers (Wang et al., 2018b). As a result, there is no easy access to features extracted from lower-level layers if the model is very deep. Here, we describe a model which makes direct links with all previous layers and offers efficient access to lower-level representations in a deep stack. We call it dynamic linear combination of layers (DLCL). The design is inspired by the linear multi-step method (LMM) in numerical ODE (Ascher and Petzold, 1998). Unlike Euler’s method, LMM can effectively reuse the information in the previous steps by linear combination to achieve a higher order. Let {y0 , ..., yl } be the output of"
P19-1176,C18-1255,1,0.936275,"proach to learning deep networks, and plays an important role in Transformer. In principle, residual networks can be seen as instances of the ordinary differential equation (ODE), behaving like the forward Euler discretization with an initial value (Chang et al., 2018; Chen et al., 2018b). Euler’s method is probably the most popular firstorder solution to ODE. But it is not yet accurate enough. A possible reason is that only one previous step is used to predict the current value 5 (Butcher, 2003). In MT, the single-step property of the residual network makes the model “forget” distant layers (Wang et al., 2018b). As a result, there is no easy access to features extracted from lower-level layers if the model is very deep. Here, we describe a model which makes direct links with all previous layers and offers efficient access to lower-level representations in a deep stack. We call it dynamic linear combination of layers (DLCL). The design is inspired by the linear multi-step method (LMM) in numerical ODE (Ascher and Petzold, 1998). Unlike Euler’s method, LMM can effectively reuse the information in the previous steps by linear combination to achieve a higher order. Let {y0 , ..., yl } be the output of"
P19-1176,D18-1338,0,\N,Missing
P19-1295,D15-1112,0,0.0438657,"Missing"
P19-1295,P18-1249,0,0.0331647,"nvenient for quantifying their contributions. Experiments on various neural machine translation tasks demonstrate the effectiveness of the proposed method. The extensive analyses verify that the two types of contexts are complementary to each other, and our method gives highly effective improvements in their integration. 1 Introduction Self-attention networks (SANs) (Parikh et al., 2016; Lin et al., 2017) have shown promising results for a range of NLP tasks, including machine translation (Vaswani et al., 2017), contextualized word embedding learning (Devlin et al., 2019), dependency parsing (Kitaev and Klein, 2018) and semantic role labeling (Tan et al., 2018). They learn hidden representations of a sequence by letting each word attend to all words in the sentence regardless of their distances. Such a fully connected structure endows SANs with the appealing strength of collecting the global information (Yu et al., 2018; Shen et al., 2018; Chen et al., 2018; Zhang et al., 2017a; Yang et al., 2019a). However, some recent researches observe that a fully connected SANs may overlook the important ∗ Corresponding author neighboring information (Luong et al., 2015; Sperber et al., 2018; Yang et al., 2019a). Th"
P19-1295,W04-3250,0,0.326169,"R (Sperber et al., 2018) and L OCAL H (Luong et al., 2015) apply Gaussian biases to regularize the conventional attention distribution with a learnable window size and a predicable central position, respectively. L O CAL S (Yang et al., 2018) is the combination of these two approaches. “Param.” denotes the model size. Model T RANSFORMER + L OCAL PATTERN + H YBRID (Gate) En-De 27.67 28.13 28.31⇑ Ja-En 28.10 28.23 28.66⇑ Table 2: Experimental results on WMT17 En⇒De and WAT17 Ja⇒En test sets. “⇑”: significant over the vanilla self-attention counterpart (p < 0.05), tested by bootstrap resampling (Koehn, 2004). Comparison to Existing Approaches We reimplement and compare several existing methods (Sperber et al., 2018; Luong et al., 2015; Yang et al., 2018, 2019b) upon T RANSFORMER. Table 1 reports the results on the En-De test set. Clearly, all the models improve translation quality, reconfirming the necessity of modeling locality for SANs. By leveraging the local and global properties, our models outperform all the related works with fewer additional parameters. Performance across Languages We further conduct experiments on WAT17 Ja-En task, which is a distant language pair (Isozaki et al., 2010)."
P19-1295,N19-1359,1,0.821084,"ating the linguistic properties. We category 10 probing tasks into three groups (“Surf.”: surface, “Sync.”: syntax and “Semc.”: semantics) following the setting in Conneau et al. (2018). For simplistic, we merely reported the average score on each group. and 2) how different representations learn the locality and globality. 5.1 Linguistic Properties Although the proposed model improves the translation performance dramatically, we still lack of understanding on which linguistic perspectives are exactly improved by the two sources of information. To this end, we follow Conneau et al. (2018) and Li et al. (2019) to conduct 10 classification tasks to study what linguistic properties are enhanced by our model. Experiment Setting These tasks are divided into three categories (Conneau et al., 2018): tasks in “Surf.” focus on the surface properties learned in the sentence embedding; “Sync.” are the tasks which designed to evaluate the capabilities of the encoder on capturing the syntactic information; and “Semc.” tasks assess the ability of a model to understanding the denotation of a sentence. For the model setting, we replace the decoder of our translation model to a MLP classifier and keep the encoder"
P19-1295,D15-1166,0,0.850603,"Devlin et al., 2019), dependency parsing (Kitaev and Klein, 2018) and semantic role labeling (Tan et al., 2018). They learn hidden representations of a sequence by letting each word attend to all words in the sentence regardless of their distances. Such a fully connected structure endows SANs with the appealing strength of collecting the global information (Yu et al., 2018; Shen et al., 2018; Chen et al., 2018; Zhang et al., 2017a; Yang et al., 2019a). However, some recent researches observe that a fully connected SANs may overlook the important ∗ Corresponding author neighboring information (Luong et al., 2015; Sperber et al., 2018; Yang et al., 2019a). They find that SANs can be empirically enhanced by restricting the attention scope to a local area. One interesting question arises: how the local and global patterns quantitatively affect the SANs. To this end, we make empirical investigations with a hybrid attention mechanism, which integrates a local and a global attentive representation via a gating scalar. Empirical results on English-to-German and Japanese-to-English tasks demonstrate the effectiveness of using both the local and global information, which are shown complementary with each othe"
P19-1295,P11-2093,0,0.026638,") where σ(.) denotes the logistic sigmoid function. As seen, gating scalar offers the model a possibility to explicitly quantify the contribution of the local and global representations. 4 Experiments We evaluate the effectiveness of the proposed approach on widely used WMT 14 English-toGerman (En-De) and WAT17 Japanese-to-English (Ja-En) translation tasks. For the WAT17 benchmark, we follow (Morishita et al., 2017) to use the 3070 first two sections of WAT17 dataset as the training data, which contains 2M sentences. The Japanese sentences are segmented by the word segmentation toolkit KeTea (Neubig et al., 2011). To alleviate the problem of Out-of-Vocabulary, all the data are segmented into subword units using bytepair encoding (Sennrich et al., 2016) with 32K merge operations. We incorporate the proposed model 1 into the widely used SAN-based framework – T RANSFORMER (Vaswani et al., 2017) and following their network configuration. We refer readers to Appendix A.1 for the details of our data and experimental settings. Prior studies reveal that modeling locality in lower layers can achieve better performance (Shen et al., 2018; Yu et al., 2018; Yang et al., 2018). Therefore, we merely apply the local"
P19-1295,P02-1040,0,0.104604,"he proposed model 1 into the widely used SAN-based framework – T RANSFORMER (Vaswani et al., 2017) and following their network configuration. We refer readers to Appendix A.1 for the details of our data and experimental settings. Prior studies reveal that modeling locality in lower layers can achieve better performance (Shen et al., 2018; Yu et al., 2018; Yang et al., 2018). Therefore, we merely apply the locality model at the lowest two layers of the encoder. According to our empirical results (Section 5.2), we set the window size to 3 (i.e. m = 1). The 4-gram case-sensitive NIST BLEU score (Papineni et al., 2002) is used as the evaluation metric. 4.1 Results In this section, we give the ablation study of the proposed model and compare several existing works upon the same architecture. Effectiveness of Hybrid Attention Mechanism To make the evaluation convincing, we reproduced the reported results in Vaswani et al. (2017) on the same data as the baseline. We first investigate the effect of the local pattern without the global information. As shown in Table 1, restricting the attention scope to a local part is able to improve the performance of translation task, showing the effectiveness of localness mo"
P19-1295,D16-1244,0,0.0962473,"Missing"
P19-1295,D18-1179,0,0.0183767,"and semantic roles in the sentence. Both these results show that different words indeed have distinct requirements of the local and global information. Therefore, modeling locality and globality in a flexible fashion is necessary for SANs on sentence modeling. Gating Scalar across Layers As visualized in Figure 3, the requirements of the local information are reduced with the stacking of layers. This is consistent with the prior findings that the lower layers tend to learn more word- and phrase-level properties than the higher layers, while the top layers of SANs seek more global information (Peters et al., 2018; Yang et al., 2018; Devlin et al., 2019). Moreover, the local information is less than the global information even in the first layer, verifying our hypothesis that both the local and global patterns are necessary for SANs. 6 Gating Scalar across POS We further explore how different types of words learn the local information. In response to this problem, we categorize different words in validation set using the Universal Part-of-Speech tagset.2 Figure 4 shows the averaged factors learned for different types of words at the first layer. As seen, contrary to the content words (e.g, “NOUN”,“VERB"
P19-1295,P16-1162,0,0.0933449,"ution of the local and global representations. 4 Experiments We evaluate the effectiveness of the proposed approach on widely used WMT 14 English-toGerman (En-De) and WAT17 Japanese-to-English (Ja-En) translation tasks. For the WAT17 benchmark, we follow (Morishita et al., 2017) to use the 3070 first two sections of WAT17 dataset as the training data, which contains 2M sentences. The Japanese sentences are segmented by the word segmentation toolkit KeTea (Neubig et al., 2011). To alleviate the problem of Out-of-Vocabulary, all the data are segmented into subword units using bytepair encoding (Sennrich et al., 2016) with 32K merge operations. We incorporate the proposed model 1 into the widely used SAN-based framework – T RANSFORMER (Vaswani et al., 2017) and following their network configuration. We refer readers to Appendix A.1 for the details of our data and experimental settings. Prior studies reveal that modeling locality in lower layers can achieve better performance (Shen et al., 2018; Yu et al., 2018; Yang et al., 2018). Therefore, we merely apply the locality model at the lowest two layers of the encoder. According to our empirical results (Section 5.2), we set the window size to 3 (i.e. m = 1)."
P19-1295,D18-1475,1,0.563184,"is beneficial to the extraction of syntactic features, integrating with the global information further improves the performance on semantic probing tasks. The quantification analysis of gating scalar also indicates that different types of words have different requirements for the local and global information. 2 Related Works Previous work has shown that modeling locality benefits SANs for certain tasks. Luong et al. (2015) proposed a Gaussian-based local attention with a predictable position; Sperber et al. (2018) differently applied a local method with variable window size for acoustic task; Yang et al. (2018) investigated the affect of the dynamical local Gaussian bias by combining these two approaches for the translation task. Different from these methods using a learnable local scope, Yang et al. (2019b) and Wu et al. (2019) restricted the attention area with fixed size by borrowing the concept of convolution into SANs. Although both these methods yield considerable improvements, 3069 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3069–3075 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics they to some exte"
P19-1295,N19-1407,1,0.906131,"et al., 2017) have shown promising results for a range of NLP tasks, including machine translation (Vaswani et al., 2017), contextualized word embedding learning (Devlin et al., 2019), dependency parsing (Kitaev and Klein, 2018) and semantic role labeling (Tan et al., 2018). They learn hidden representations of a sequence by letting each word attend to all words in the sentence regardless of their distances. Such a fully connected structure endows SANs with the appealing strength of collecting the global information (Yu et al., 2018; Shen et al., 2018; Chen et al., 2018; Zhang et al., 2017a; Yang et al., 2019a). However, some recent researches observe that a fully connected SANs may overlook the important ∗ Corresponding author neighboring information (Luong et al., 2015; Sperber et al., 2018; Yang et al., 2019a). They find that SANs can be empirically enhanced by restricting the attention scope to a local area. One interesting question arises: how the local and global patterns quantitatively affect the SANs. To this end, we make empirical investigations with a hybrid attention mechanism, which integrates a local and a global attentive representation via a gating scalar. Empirical results on Engli"
P19-1352,P18-2049,0,0.0200019,"This is similar to the Chinese word “sangsheng” (paired with “killed”) and the English words “died” and “killed”. Figure 6(c) shows that the representations of the Chinese and English words which relate to “president” are very close. 4 Related Work Many previous works focus on improving the word representations of NMT by capturing the fine-grained (character) or coarse-grained (sub-word) monolingual characteristics, such as character-based NMT (Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Cho et al., 2014; Chen et al., 2016), sub-word NMT (Sennrich et al., 2016b; Johnson et al., 2017; Ataman and Federico, 2018), and hybrid NMT (Luong and Manning, 2016). They effectively consider and utilize the morphological information to enhance the word representations. Our work aims to enhance word representations through the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the sou"
P19-1352,P18-1008,0,0.0298591,"n vectors. While in (b), the two word embeddings are made up of two parts, indicating the shared (lined nodes) and the private (unlined nodes) features. This enables the two words to make use of common representation units, leading to a closer relationship between them. With the introduction of ever more powerful architectures, neural machine translation (NMT) has become the most promising machine translation method (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). For word representation, different architectures— including, but not limited to, recurrence-based (Chen et al., 2018), convolution-based (Gehring et al., 2017) and transformation-based (Vaswani et al., 2017) NMT models—have been taking advantage of the distributed word embeddings to capture the syntactic and semantic properties of words (Turian et al., 2010). Corresponding author Rd (a) Standard Introduction ∗ Long NMT usually utilizes three matrices to represent source embeddings, target input embeddings, and target output embeddings (also known as pre-softmax weight), respectively. These embeddings occupy most of the model parameters, which constrains the improvements of NMT because the recent methods beco"
P19-1352,P16-1186,0,0.0318903,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,P16-2058,0,0.0190662,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,P17-1106,1,0.801916,"ical meaning. Based on these observations, we find that the alignment quality is not a key factor affecting the model performance. In contrast, pairing as many as similar words possible helps the model to better learn the bilingual vector space, which improves the translation performance. The following qualitative analyses support these observations either. 3.5 Analysis of the Translation Results Table 6 shows two translation examples of the NIST Chinese-English translation task. To better understand the translations produced by these two models, we use layer-wise relevance propagation (LRP) (Ding et al., 2017) to produce the attention maps of the selected translations, as shown in Figure 4 and 5. In the first example, the Chinese word “sangsheng” is a low-frequency word and its ground truth is “killed”. It is observed the inadequate representation of “sangsheng” leads to a decline in the translation quality of the vanilla, direct bridging, and decoder WT methods. In our proposed 3619 president chief 0 zongtong also zhuxi sangsheng killed −0.1 died zhuxi president zongtong chief also 0.3 yebing 0.35 juzhang president chairman zongtong weiyuanzhang chief premier 0.3 0.2 −0.2 bing −0.3 ye −0.4 −0.2 0"
P19-1352,N13-1073,0,0.201607,"as parallel words that are the translation of each other. According to the word frequency, each source word x is paired with a target aligned word yˆ that has the highest alignment probability among the candidates, and is computed as follows: yˆ = arg max logA(y|x) (1) y∈a(x) where a(·) denotes the set of aligned candidates. It is worth noting the target words that have been paired with the source words cannot be used as candidates. A(·|·) denotes the alignment probability. These can be obtained by either the intrinsic attention mechanism (Bahdanau et al., 2015) or unsupervised word aligner (Dyer et al., 2013). 2.1.2 Words with Same Word Form As shown in Figure 2(b), the sub-word “Ju@@” simultaneously exists in English and German sentences. This kind of word tends to share a medium number of features of the word embeddings. Most of the time, the source and target words with the same word form also share similar lexical meaning. This category of words generally includes Arabic numbers, punctuations, named entities, cognates and loanwords. However, there are some bilingual homographs where the words in the source and target languages look the same but have completely different meanings. For example,"
P19-1352,D17-1146,0,0.228186,"Missing"
P19-1352,N18-1032,0,0.0214543,"pturing the fine-grained (character) or coarse-grained (sub-word) monolingual characteristics, such as character-based NMT (Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Cho et al., 2014; Chen et al., 2016), sub-word NMT (Sennrich et al., 2016b; Johnson et al., 2017; Ataman and Federico, 2018), and hybrid NMT (Luong and Manning, 2016). They effectively consider and utilize the morphological information to enhance the word representations. Our work aims to enhance word representations through the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate"
P19-1352,P02-1040,0,0.104132,"Missing"
P19-1352,D13-1176,0,0.102581,"and (b) shared-private word embeddings. In (a), the English word “Long” and the German word “Lange”, which have similar lexical meanings, are represented by two private d-dimension vectors. While in (b), the two word embeddings are made up of two parts, indicating the shared (lined nodes) and the private (unlined nodes) features. This enables the two words to make use of common representation units, leading to a closer relationship between them. With the introduction of ever more powerful architectures, neural machine translation (NMT) has become the most promising machine translation method (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). For word representation, different architectures— including, but not limited to, recurrence-based (Chen et al., 2018), convolution-based (Gehring et al., 2017) and transformation-based (Vaswani et al., 2017) NMT models—have been taking advantage of the distributed word embeddings to capture the syntactic and semantic properties of words (Turian et al., 2010). Corresponding author Rd (a) Standard Introduction ∗ Long NMT usually utilizes three matrices to represent source embeddings, target input embeddings, and target output embeddings (also kno"
P19-1352,W04-3250,0,0.118362,"Missing"
P19-1352,P18-1164,0,0.205747,"is method can also be adapted to sub-word NMT with a shared source-target sub-word vocabulary and it performs well in language pairs with many of the same characters, such as English-German and English-French (Vaswani et al., 2017). Unfortunately, this method is not applicable to languages that are written in different alphabets, such as Chinese-English (Hassan et al., 2018). Another challenge facing the source and target word embeddings of NMT is the lack of interactions. This degrades the attention performance, leading to some unaligned translations that hurt the translation quality. Hence, Kuang et al. (2018) propose to bridge the source and target embeddings, which brings better attention to the related source and target words. Their method is applicable to any language pairs, providing a tight interaction between the source and target word pairs. However, their method requires additional components and model parameters. In this work, we aim to enhance the word representations and the interactions between the source and target words, while using even fewer parameters. To this end, we present a languageindependent method, which is called sharedprivate bilingual word embeddings, to share a part of"
P19-1352,E17-2025,0,0.382093,"ce and target words with the same word form also share similar lexical meaning. This category of words generally includes Arabic numbers, punctuations, named entities, cognates and loanwords. However, there are some bilingual homographs where the words in the source and target languages look the same but have completely different meanings. For example, the German word “Gift” means “Poison” in English. That is the reason we propose to first pair the words with similar lexical meaning instead of those words with same word forms. This might be the potential limitation of the three-way WT method (Press and Wolf, 2017), where words with the same word form indiscriminately share the same word embedding. 2.1.3 Unrelated Words We regard source and target words that cannot be paired with each other as unrelated words. Figure 2(c) shows an example of a pair of unrelated words. This category is mainly composed of lowfrequency words, such as misspelled words, special characters, and foreign words. In standard NMT, the embeddings of low-frequency words are usually inadequately trained, resulting in a poor word representation. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Sl"
P19-1352,W17-4739,0,0.0393269,"Missing"
P19-1352,P16-1162,0,0.847068,"nt the word embeddings Experiments We carry out our experiments on the small-scale IWSLT’17 {Arabic (Ar), Japanese (Ja), Korean (Ko), Chinese (Zh)}-to-English (En) translation tasks, medium-scale NIST Chinese-English (ZhEn) translation task, and large-scale WMT’14 English-German (En-De) translation task. For the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks, there are respectively 236K, 234K, 227K, and 235K sentence pairs in each training set.4 The validation set is IWSLT17.TED.tst2014 and the test set is IWSLT17.TED.tst2015. For each language, we learn a BPE model with 16K merge operations (Sennrich et al., 2016b). For the NIST Zh-En translation task, the training corpus consists of 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words. We use the NIST MT06 dataset as the validation set and the test sets are the NIST MT02, MT03, MT04, MT05, MT08 datasets. To compare with the recent works, the vocabulary size is limited to 4 https://wit3.fbk.eu/mt.php?release= 2017-01-trnted 3616 Architecture SMT* RNNsearch* Transformer Zh⇒En Vanilla Source bridging Target bridging Direct bridging Vanilla Direct bridging Decoder WT Shared-private Params 74.8M 78.5M 76.6M 78.9M 90.2M 90.5M 74.9M 62.8M E"
P19-1352,P10-1040,0,0.341652,"Missing"
P19-1352,D18-1100,0,0.0187321,"the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embedding matrices. As shown in Figure 3, the source embedding Ex ∈ R|V |×d is computed as follows:: Ex = Exlm ⊕ Exwf ⊕ Exur (2) where ⊕ is the row concatenation operator. Ex(·) ∈ R|V(·) |×d represents the word embeddings of the source words belong to different categories, e.g. lm represents the words with similar lexical meaning. |V(·) |denotes the vocabulary size of the corresponding category. The process of feature sharing is also implemented by matrix co"
P19-1352,D17-1154,0,0.0174627,"w that our model with fewer parameters yields consistent improvements over the strong Transformer baselines. 2 Approach In monolingual vector space, similar words tend to have commonalities in the same dimensions of their word vectors (Mikolov et al., 2013). These commonalities include: (1) a similar degree (value) of the same dimension and (2) a similar positive or negative correlation of the same dimension. Many previous works have noticed this phenomenon and have proposed to use shared vectors to represent similar words in monolingual vector space toward model compression (Li et al., 2016; Zhang et al., 2017b; Li et al., 2018). Motivated by these works, in NMT, we assume that the source and target words that have similar characteristics should also have similar vectors. Hence, we propose to perform this sharing technique in bilingual vector space. More precisely, we share the features (dimensions) between the paired source and target embeddings (vectors). However, in contrast to the previous studies, we also model the private features of the word embedding to preserve the private characteristics of words for source and target languages. The private 3614 features allow the words to better learn th"
P19-1352,C16-1291,0,0.0216196,"tures that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate translation results with the assistance of prior guidance. We provide an alternative that integrates the prior alignments through the sharing of features, which can also leads to a reduction of model parameters. Kuang et al. (2018) propose to shorten the path length between the related source and target embeddings to enhance the embedding layer. We believe that the shared features can be seem as the zero distance between the paired word embeddings. Our proposed method also uses several ideas from the three-way WT m"
P19-1352,P16-1100,0,0.041235,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,D16-1249,0,0.0233985,"the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate translation results with the assistance of prior guidance. We provide an alternative that integrates the prior alignments through the sharing of features, which can also leads to a reduction of model parameters. Kuang et al. (2018) propose to shorten the path length between the related source and target embeddings to enhance the embedding layer. We believe that the shared features can be seem as the zero distance between the paired word embeddings. Our proposed method also uses several ideas from"
P19-1352,D14-1179,0,\N,Missing
P19-1352,Q17-1024,0,\N,Missing
P19-1354,P18-1008,0,0.33076,"@gmail.com, {derekfw,lidiasc}@umac.mo ‡ Tencent AI Lab {vinnylywang,zptu}@tencent.com Abstract its high parallelization in computation, and flexibility in modeling dependencies regardless of distance by explicitly attending to all the signals. Position embedding (Gehring et al., 2017) is generally deployed to capture sequential information for SAN (Vaswani et al., 2017; Shaw et al., 2018). Recent studies claimed that SAN with position embedding is still weak at learning word order information, due to the lack of recurrence structure that is essential for sequence modeling (Shen et al., 2018a; Chen et al., 2018; Hao et al., 2019). However, such claims are mainly based on a theoretical argument, which have not been empirically validated. In addition, this can not explain well why SAN-based models outperform their RNN counterpart in machine translation – a benchmark sequence modeling task (Vaswani et al., 2017). Our goal in this work is to empirically assess the ability of SAN to learn word order. We focus on asking the following research questions: Self-attention networks (SAN) have attracted a lot of interests due to their high parallelization and strong performance on a variety of NLP tasks, e.g. m"
P19-1354,P18-1163,1,0.851012,"whether NMT model has the ability to tackle the wrong order noises. As a results, we make erroneous word order noises on English-German development set by moving one word to another position, and evaluate the drop of the translation quality of each model. As listed in Figure 6, SAN and DiSAN yield less drops on translation quality than their RNN counterpart, demonstrating the effectiveness of self-attention on ablating wrong order noises. We attribute this to the fact that models (e.g. RNN-based models) will not learn to be robust to errors since they are never observed (Sperber et al., 2017; Cheng et al., 2018). On the contrary, since SAN-based NMT encoder is good at recognizing and reserving anomalous word order information under NMT context, it may raise the ability of decoder on handling noises occurred in the training set, thus to be more robust in translating sentences with anomalous word order. Related Work Exploring Properties of SAN SAN has yielded strong empirical performance in a variety of NLP tasks (Vaswani et al., 2017; Tan et al., 2018; Li et al., 2018; Devlin et al., 2019). In response to these impressive results, several studies have emerged with the goal of understanding SAN on many"
P19-1354,P18-1198,0,0.188484,"es across language pairs and model variants. This is consistent with prior observation on NMT systems that both RNN and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recov"
P19-1354,N19-1423,0,0.651089,"rder, and does the conclusion hold in different scenarios (e.g., translation)? Q2: Is the model architecture the critical factor for learning word order in the downstream tasks such as machine translation? Q3: Is position embedding powerful enough to capture word order information for SAN? Introduction Self-attention networks (SAN, Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in a variety of natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), semantic role labelling (Strubell et al., 2018), and language representations (Devlin et al., 2019). The popularity of SAN lies in ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. We approach these questions with a novel probing task – word reordering detection (WRD), which aims to detect the positions of randomly reordered words in the input sentence. We compare SAN with RNN, as well as directional SAN (DiSAN, Shen et al., 2018a) that augments SAN with recurrence modeling. In this study, we focus on the encoders implemented with different architectures, so as to investigate their abilities to learn 3635 Proce"
P19-1354,P17-1080,0,0.0251829,"tence embedding. However, analysis on sentence encodings may introduce confounds, making it difficult to infer whether the relevant information is encoded within the specific position of interest or rather inferred from diffuse information elsewhere in the sentence (Tenney et al., 2019). In this study, we directly probe the token representations for word- and phrase-level properties, which has been widely used for probing token-level representations learned in neural machine translation systems, e.g. part-of-speech, semantic tags, morphology as well as constituent structure (Shi et al., 2016; Belinkov et al., 2017; Blevins et al., 2018). 6 Conclusion In this paper, we introduce a novel word reordering detection task which can probe the ability of a model to extract word order information. With the help of the proposed task, we evaluate RNN, 3642 SAN and DiSAN upon Transformer framework to empirically test the theoretical claims that SAN lacks the ability to learn word order. The results reveal that RNN and DiSAN exactly perform better than SAN on extracting word order information in the case they are trained individually for our task. However, there is no evidence that SAN learns less word order inform"
P19-1354,P18-2003,0,0.0207368,"r, analysis on sentence encodings may introduce confounds, making it difficult to infer whether the relevant information is encoded within the specific position of interest or rather inferred from diffuse information elsewhere in the sentence (Tenney et al., 2019). In this study, we directly probe the token representations for word- and phrase-level properties, which has been widely used for probing token-level representations learned in neural machine translation systems, e.g. part-of-speech, semantic tags, morphology as well as constituent structure (Shi et al., 2016; Belinkov et al., 2017; Blevins et al., 2018). 6 Conclusion In this paper, we introduce a novel word reordering detection task which can probe the ability of a model to extract word order information. With the help of the proposed task, we evaluate RNN, 3642 SAN and DiSAN upon Transformer framework to empirically test the theoretical claims that SAN lacks the ability to learn word order. The results reveal that RNN and DiSAN exactly perform better than SAN on extracting word order information in the case they are trained individually for our task. However, there is no evidence that SAN learns less word order information under the machine"
P19-1354,D17-1219,0,0.0270644,"models have to learn to recognize both the normal and abnormal word order in a sentence. Position Detector Figure 1 (a) depicts the architecture of the position detector. Let the sequential representations H = {h1 , ..., hN } be the output of each encoder noted in Section 3, which are fed to the output layer (Figure 1 (b)). Since only one pair of “I” and “O” labels should be generated in the output sequence, we cast the task as a pointer detection problem (Vinyals et al., 2015). To this end, we turn to an output layer that commonly used in the reading comprehension task (Wang and Jiang, 2017; Du and Cardie, 2017), which aims to identify the start and end positions of the answer in the given text.2 The output layer consists of two sub-layers, which progressively predicts the prob2 Contrary to reading comprehension in which the start and end positions are ordered, “I” and “O” do not have to be ordered in our tasks, that is, the popped word can be inserted to either left or right position. 3636 • Q1: We compare SAN with two recurrence architectures – RNN and DiSAN on the WRD task, thus to quantify their abilities on learning word order (Section 3.1). abilities of each position being labelled as “I” and “"
P19-1354,D16-1011,0,0.0320337,"rmation between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results verified that the learning objective indeed affects more on learning word order information than model architecture in our case. Accuracy According to Layer Several researchers may doubt that the parallel structure of SAN may lead to failure on capturing word order information at higher layers, si"
P19-1354,D18-1317,1,0.861132,"fact that models (e.g. RNN-based models) will not learn to be robust to errors since they are never observed (Sperber et al., 2017; Cheng et al., 2018). On the contrary, since SAN-based NMT encoder is good at recognizing and reserving anomalous word order information under NMT context, it may raise the ability of decoder on handling noises occurred in the training set, thus to be more robust in translating sentences with anomalous word order. Related Work Exploring Properties of SAN SAN has yielded strong empirical performance in a variety of NLP tasks (Vaswani et al., 2017; Tan et al., 2018; Li et al., 2018; Devlin et al., 2019). In response to these impressive results, several studies have emerged with the goal of understanding SAN on many properties. For example, Tran et al. (2018) compared SAN and RNN on language inference tasks, and pointed out that SAN is weak at learning hierarchical structure than its RNN counterpart. Moreover, Tang et al. (2018) conducted experiments on subject-verb agreement and word sense disambiguation tasks. They found that SAN is good at extracting semantic properties, while underperforms RNN on capturing long-distance dependencies. This is in contrast to our intuit"
P19-1354,N19-1359,1,0.842245,"for the encoding of deeper 3641 RNN SAN DiSAN 30 40 50 60 ations (×10K) RNN DiSAN 5 SAN BLEU Drop -3 -4 -5 -6 En-De En-Ja Figure 6: The differences of translation performance when the pre-trained NMT models are fed with the original (“Golden”) and reordered (“Reorder”) source sentences. As seen, SAN and DiSAN perform better on handling noises in terms of erroneous word order. linguistic properties required by machine translation. Recent studies on multi-layer learning shown that different layers tend to learn distinct linguistic information (Peters et al., 2018; Raganato and Tiedemann, 2018; Li et al., 2019). The better accuracy achieved by SAN across layers indicates that SAN indeed tries to preserve more word order information during the learning of other linguistic properties for translation purpose. Effect of Wrong Word Order Noises For humans, a small number of erroneous word orders in a sentence usually does not affect the comprehension. For example, we can understand the meaning of English sentence “Dropped the boy the ball.”, despite its erroneous word order. It is intriguing whether NMT model has the ability to tackle the wrong order noises. As a results, we make erroneous word order noi"
P19-1354,P15-1150,0,0.147118,"Missing"
P19-1354,D15-1166,0,0.07096,"ystems that both RNN and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results ve"
P19-1354,D18-1458,0,0.146942,"Missing"
P19-1354,P11-2093,0,0.0246417,"of WMT14 En⇒De data with maximum length to 80. For each sentence in different sets (i.e. training, validation, and test sets), we construct an instance by randomly moving a word to another position. Finally we construct 7M, 10K and 10K samples for training, validating and testing, respectively. Note that a sentence can be sampled multiple times, thus each dataset in the WRD data contains more instances than that in the machine translation data. All the English and German data are tokenized using the scripts in Moses. The Japanese sentences are segmented by the word segmentation toolkit KeTea (Neubig et al., 2011). To reduce the vocabulary size, all the sentences are processed by byte-pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the data. 4 Experimental Results We return to the central questions originally posed, that is, whether SAN is indeed weak at learning positional information. Using the above experimental design, we give the following answers: A1: SAN-based encoder trained on the WRD data is indeed harder to learn positional information than the recurrence architectures (Section 4.1), while there is no evidence that 3638 Models RNN SAN DiSAN Insert 78.4 73.2 79.6"
P19-1354,D16-1244,0,0.0955198,"Missing"
P19-1354,N18-1202,0,0.0553809,"reserving more word order information, thus to help for the encoding of deeper 3641 RNN SAN DiSAN 30 40 50 60 ations (×10K) RNN DiSAN 5 SAN BLEU Drop -3 -4 -5 -6 En-De En-Ja Figure 6: The differences of translation performance when the pre-trained NMT models are fed with the original (“Golden”) and reordered (“Reorder”) source sentences. As seen, SAN and DiSAN perform better on handling noises in terms of erroneous word order. linguistic properties required by machine translation. Recent studies on multi-layer learning shown that different layers tend to learn distinct linguistic information (Peters et al., 2018; Raganato and Tiedemann, 2018; Li et al., 2019). The better accuracy achieved by SAN across layers indicates that SAN indeed tries to preserve more word order information during the learning of other linguistic properties for translation purpose. Effect of Wrong Word Order Noises For humans, a small number of erroneous word orders in a sentence usually does not affect the comprehension. For example, we can understand the meaning of English sentence “Dropped the boy the ball.”, despite its erroneous word order. It is intriguing whether NMT model has the ability to tackle the wrong order noises"
P19-1354,W18-5431,0,0.0453698,"rder information, thus to help for the encoding of deeper 3641 RNN SAN DiSAN 30 40 50 60 ations (×10K) RNN DiSAN 5 SAN BLEU Drop -3 -4 -5 -6 En-De En-Ja Figure 6: The differences of translation performance when the pre-trained NMT models are fed with the original (“Golden”) and reordered (“Reorder”) source sentences. As seen, SAN and DiSAN perform better on handling noises in terms of erroneous word order. linguistic properties required by machine translation. Recent studies on multi-layer learning shown that different layers tend to learn distinct linguistic information (Peters et al., 2018; Raganato and Tiedemann, 2018; Li et al., 2019). The better accuracy achieved by SAN across layers indicates that SAN indeed tries to preserve more word order information during the learning of other linguistic properties for translation purpose. Effect of Wrong Word Order Noises For humans, a small number of erroneous word orders in a sentence usually does not affect the comprehension. For example, we can understand the meaning of English sentence “Dropped the boy the ball.”, despite its erroneous word order. It is intriguing whether NMT model has the ability to tackle the wrong order noises. As a results, we make errone"
P19-1354,P16-1162,0,0.0652226,"test sets), we construct an instance by randomly moving a word to another position. Finally we construct 7M, 10K and 10K samples for training, validating and testing, respectively. Note that a sentence can be sampled multiple times, thus each dataset in the WRD data contains more instances than that in the machine translation data. All the English and German data are tokenized using the scripts in Moses. The Japanese sentences are segmented by the word segmentation toolkit KeTea (Neubig et al., 2011). To reduce the vocabulary size, all the sentences are processed by byte-pair encoding (BPE) (Sennrich et al., 2016) with 32K merge operations for all the data. 4 Experimental Results We return to the central questions originally posed, that is, whether SAN is indeed weak at learning positional information. Using the above experimental design, we give the following answers: A1: SAN-based encoder trained on the WRD data is indeed harder to learn positional information than the recurrence architectures (Section 4.1), while there is no evidence that 3638 Models RNN SAN DiSAN Insert 78.4 73.2 79.6 Original 73.4 66.0 70.1 Both 68.2 60.1 68.0 ability of SAN to learn word order. The consistency between prior studi"
P19-1354,N18-2074,0,0.225183,"he Ability of Self-Attention Networks to Learn Word Order Baosong Yang† Longyue Wang‡ Derek F. Wong† Lidia S. Chao† Zhaopeng Tu‡∗ † NLP2 CT Lab, Department of Computer and Information Science, University of Macau nlp2ct.baosong@gmail.com, {derekfw,lidiasc}@umac.mo ‡ Tencent AI Lab {vinnylywang,zptu}@tencent.com Abstract its high parallelization in computation, and flexibility in modeling dependencies regardless of distance by explicitly attending to all the signals. Position embedding (Gehring et al., 2017) is generally deployed to capture sequential information for SAN (Vaswani et al., 2017; Shaw et al., 2018). Recent studies claimed that SAN with position embedding is still weak at learning word order information, due to the lack of recurrence structure that is essential for sequence modeling (Shen et al., 2018a; Chen et al., 2018; Hao et al., 2019). However, such claims are mainly based on a theoretical argument, which have not been empirically validated. In addition, this can not explain well why SAN-based models outperform their RNN counterpart in machine translation – a benchmark sequence modeling task (Vaswani et al., 2017). Our goal in this work is to empirically assess the ability of SAN to"
P19-1354,D18-1548,0,0.0539845,": Is recurrence structure obligate for learning word order, and does the conclusion hold in different scenarios (e.g., translation)? Q2: Is the model architecture the critical factor for learning word order in the downstream tasks such as machine translation? Q3: Is position embedding powerful enough to capture word order information for SAN? Introduction Self-attention networks (SAN, Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in a variety of natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017), semantic role labelling (Strubell et al., 2018), and language representations (Devlin et al., 2019). The popularity of SAN lies in ∗ Zhaopeng Tu is the corresponding author of the paper. This work was conducted when Baosong Yang was interning at Tencent AI Lab. We approach these questions with a novel probing task – word reordering detection (WRD), which aims to detect the positions of randomly reordered words in the input sentence. We compare SAN with RNN, as well as directional SAN (DiSAN, Shen et al., 2018a) that augments SAN with recurrence modeling. In this study, we focus on the encoders implemented with different architectures, so a"
P19-1354,D18-1503,0,0.316521,"ordering detection objective. WRD Encoders We first directly train the encoders on the WRD data, to evaluate the abilities of model architectures. The WRD encoders are randomly initialized and co-trained with the output layer. Accordingly, the detection accuracy can be treated as the learning objective of this group of encoders. Meanwhile, we can investigate the reliability of the proposed WRD task by checking whether the performances of different architectures (i.e. RNN, SAN, and DiSAN) are consistent with previous findings on other benchmark NLP tasks (Shen et al., 2018a; Tang et al., 2018; Tran et al., 2018; Devlin et al., 2019). NMT Encoders To quantify how well different architectures learn word order information with the learning objective of machine translation, we first train the NMT models (both encoder and decoder) on bilingual corpus using the same configuration reported by Vaswani et al. (2017). Then, we fix the parameters of the encoder, and only train the parameter associated with the output layer on the WRD data. In this way, we can probe the representations learned by NMT models, on their abilities to learn word order of input sentences. To cope with WRD task, all the models were tr"
P19-1354,D17-1065,0,0.0520865,"Missing"
P19-1354,D18-1475,1,0.836265,"and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results verified that the lea"
P19-1354,N19-1407,1,0.841914,"Missing"
P19-1354,D17-1150,1,0.862188,"ducted on the test set. Clearly, the accuracy of SAN gradually increased with the stacking of layers and consistently outperform that of other models across layers. ever, this kind of stability is destroyed when we pre-train each encoder with a learning objective of machine translation. As seen in Figure 4 (b) and (c), the performance of pre-trained NMT encoders obviously became worse on long-distance cases across language pairs and model variants. This is consistent with prior observation on NMT systems that both RNN and SAN fail to fully capture long-distance dependencies (Tai et al., 2015; Yang et al., 2017; Tang et al., 2018). Regarding to information bottleneck principle (Tishby and Zaslavsky, 2015; Alemi et al., 2016), our NMT models are trained to maximally maintain the relevant information between source and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivia"
P19-1354,P17-1172,0,0.0274143,"urce and target, while abandon irrelevant features in the source sentence, e.g. portion of word order information. Different NLP tasks have distinct requirements on linguistic information (Conneau et al., 2018). For machine translation, the local patterns (e.g. phrases) matter more (Luong et al., 2015; Yang et al., 2018, 2019), while long-distance word order information plays a relatively trivial role in understanding the meaning of a source sentence. Recent studies also pointed out that abandoning irrelevant features in source sentence benefits to some downstream NLP tasks (Lei et al., 2016; Yu et al., 2017; Shen et al., 2018b). An immediate consequence of such kind of data process inequality (Schumacher and Nielsen, 1996) is that information about word order that is lost in encoder cannot be recovered in the detector, and consequently drops the performance on our WRD task. The results verified that the learning objective indeed affects more on learning word order information than model architecture in our case. Accuracy According to Layer Several researchers may doubt that the parallel structure of SAN may lead to failure on capturing word order information at higher layers, since the position"
P19-1354,D10-1092,0,\N,Missing
P19-1354,D16-1159,0,\N,Missing
P19-1354,W17-5706,0,\N,Missing
R13-1094,D11-1033,0,0.274559,"al. 2005), we allow duplicated sentences during the selection which is similar with. All retrieved sentences with their corresponding target translations are ranked according to their similarity scores. 2.2 Perplexity-Based Model The perplexity of a string s with empirical ngram distribution p given a language model q is:  p ( x )log q ( x ) 2 x  2H ( p ,q ) (3) in which H(p, q) is the cross-entropy between p and q. Selecting segments based on a perplexity threshold is equivalent to selecting based on a cross-entropy threshold, which is more often used for this task (Moore and Lewis, 2010; Axelrod et al., 2011). Supposed that HI(s) and HO(s) are the cross-entropy of a string s according to an in-domain language model LMI and non-indomain LMG respectively trained on in-domain data set I and a partition of general-domain data set G. Considering both source (src) and target (tar) side of parallel training data, there are three variants. The first is basic cross-entropy given by: H I src (s) 2 728 Available at http://lucene.apache.org. (4) and the second is cross-entropy difference (Moore and Lewis, 2010): H I src (s)  HGsrc (s) tional speech in a travel setting. All of them were segmented 4 (Zhang,"
R13-1094,J93-2003,0,0.0499157,"Missing"
R13-1094,W07-0722,0,0.0228688,"tion A well-known problem of statistical machine translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a sentence or a sentences pair in the general corp"
R13-1094,P11-2071,0,0.0320572,"Missing"
R13-1094,P12-2023,0,0.0140684,"ne translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a sentence or a sentences pair in the general corpus G. The score of each sG is given by Score(sG"
R13-1094,W07-0717,0,0.025379,"em of statistical machine translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a sentence or a sentences pair in the general corpus G. The score of each"
R13-1094,2005.eamt-1.19,0,0.85304,"ct model of target domain and sG is a sentence or a sentences pair in the general corpus G. The score of each sG is given by Score(sG )  Sim(sG , R) (1) which means if we could find a better function to measure the similarity between sG and R, G could be replaced by a new sub-corpus Gsub for training a domain-specific SMT system. We focus on two data selection criteria that have been explored for domain adaptation. One comes from the realm of information retrieval (IR), which is defined as the cosine of the angle between two vectors based on term frequencyinverse document frequency (TF-IDF). Hildebrand et al. (2005) showed that it is possible to apply this standard IR technique for both TM adaptation and LM adaptation. It is also similar to the offline data optimization approach proposed by Lüet al. (2007), who re-sample and re727 Proceedings of Recent Advances in Natural Language Processing, pages 727–732, Hissar, Bulgaria, 7-13 September 2013. weight sentences in general corpus, achieving an improvement of about 1 BLEU point over the baseline system. This simple co-occurrence based matching only considers keywords overlap, which may result in weakness in filtering irrelevant data. Thus, it needs a larg"
R13-1094,C12-1096,0,0.0304151,"Missing"
R13-1094,2005.mtsummit-papers.11,0,0.0269798,"Missing"
R13-1094,P07-2045,0,0.00757585,"her penalize it according to space and punctuations edit differences. 3 3.1 3.2 Experimental Setup Corpora Two corpora are needed for the domain adaptation task. Our general corpus includes 5 million English-Chinese parallel sentences comprising various genres such as movie subtitle, law literature, news and novel. The in-domain corpus and test set are randomly selected from the IWSLT2010 (International Workshop on Spoken Language Translation) Chinese-English Dialog task 3 , consisting of transcriptions of conversaThe experiments presented in this paper are carried out with the Moses toolkit (Koehn et al., 2007), a state-of-the-art open-source phrasebased SMT system. The translation and the reordering model relied on “grow-diag-final” symmetrized word-to-word alignments built using GIZA++ (Och and Ney, 2003) and the training script of Moses. A 5-gram language model was trained on the target side of the training parallel corpus using the IRSTLM toolkit (Federico et al., 2008), exploiting improved Modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. 3.3 http://iwslt2010.fbk.eu/node/33. Baseline System The baseline system was trained on the general corpus with toolkits"
R13-1094,2010.jec-1.4,0,0.0192472,"onal in-domain corpus is employed to build the LM for perplexity-based retrieval (Moore and Lewis, 2010; Axelrod et al., 2011). Edit-Distance-Based Model Given a sentence sG from a general corpus and a sentence sR from the test set or in-domain corpus, the edit distance for these two sequences is defined as the minimum number of edits, i.e. symbol insertions, deletions and substitutions, for transforming sG into sR. There are several different implementations of the edit-distance-based retrieval model. We used the normalized Levenshtein similarity score (fuzzy matching score, FMS) proposed by Koehn and Senellart (2010): FMS  1  LEDword (sG , sR ) Max( sG , sR ) (7) in which LEDword is a distance function and |s |is the number of tokens of sentence s. In this study, we employed a word-based Levenshtein edit distance function instead of additionally using a letter-based one. If the score of a sentence exceeds a threshold, we will further penalize it according to space and punctuations edit differences. 3 3.1 3.2 Experimental Setup Corpora Two corpora are needed for the domain adaptation task. Our general corpus includes 5 million English-Chinese parallel sentences comprising various genres such as movie sub"
R13-1094,D07-1036,0,0.0841494,"that are more similar to the target domain but different to others in general corpus. The third one is to sum the cross-entropy difference over both source and target side of the corpus:  H I src (s)  HGsrc (s)   H I tar (s)  H G tar (s) Data Set Sentences Tokens Ave. Len. Test Set 3,500 34,382 9.60 In-domain 17,975 151,797 9.45 Training Set 5,211,281 53,650,998 12.93 (6) Table 1: Corpora statistics. The third variant has been is proven to achieve the best result among the three cross-entropy variants (Axelrod et al., 2011). 2.3 In practice, we followed the experiments conducted by Lü et al. (2007) and Hildebrand et al. (2005), where the test set was used to select indomain data from general corpus. The only difference is that an additional in-domain corpus is employed to build the LM for perplexity-based retrieval (Moore and Lewis, 2010; Axelrod et al., 2011). Edit-Distance-Based Model Given a sentence sG from a general corpus and a sentence sR from the test set or in-domain corpus, the edit distance for these two sequences is defined as the minimum number of edits, i.e. symbol insertions, deletions and substitutions, for transforming sG into sR. There are several different implementat"
R13-1094,2012.iwslt-papers.7,0,0.0135626,"computationally-limited environment. 1 Introduction A well-known problem of statistical machine translation (SMT) (Brown et al., 1993) is that the data-driven system is not guaranteed to perform optimally if the data for training and testing are not identically distributed. Domain adaptation for SMT has been explored at different component 1 It could be modeled by an in-domain corpus or text to be translated. levels: word level, phrase level, sentence level and model level. For example mining unknown words from comparable corpora (Daume III and Jagarlamudi, 2011), weighted phrase extraction (Mansour and Ney, 2012), mixing multiple models (Civera and Juan, 2007; Foster and Kuhn, 2007; Eidelman et al., 2012), etc. Recently, data selection as a simple and effective way for this special task has attracted attention. Under the assumption that there exists a large general-domain corpus (general corpus) including sufficient domains, the task of data selection is to translate a domain-specific text using the optimized translation model (TM) or language model (LM) trained by less but more suitable data retrieved from the general corpus. To state it formally, R is an abstract model of target domain and sG is a s"
R13-1094,P10-2041,0,0.277716,". As in (Hildebrand et al. 2005), we allow duplicated sentences during the selection which is similar with. All retrieved sentences with their corresponding target translations are ranked according to their similarity scores. 2.2 Perplexity-Based Model The perplexity of a string s with empirical ngram distribution p given a language model q is:  p ( x )log q ( x ) 2 x  2H ( p ,q ) (3) in which H(p, q) is the cross-entropy between p and q. Selecting segments based on a perplexity threshold is equivalent to selecting based on a cross-entropy threshold, which is more often used for this task (Moore and Lewis, 2010; Axelrod et al., 2011). Supposed that HI(s) and HO(s) are the cross-entropy of a string s according to an in-domain language model LMI and non-indomain LMG respectively trained on in-domain data set I and a partition of general-domain data set G. Considering both source (src) and target (tar) side of parallel training data, there are three variants. The first is basic cross-entropy given by: H I src (s) 2 728 Available at http://lucene.apache.org. (4) and the second is cross-entropy difference (Moore and Lewis, 2010): H I src (s)  HGsrc (s) tional speech in a travel setting. All of them w"
R13-1094,J03-1002,0,0.00747202,"nglish-Chinese parallel sentences comprising various genres such as movie subtitle, law literature, news and novel. The in-domain corpus and test set are randomly selected from the IWSLT2010 (International Workshop on Spoken Language Translation) Chinese-English Dialog task 3 , consisting of transcriptions of conversaThe experiments presented in this paper are carried out with the Moses toolkit (Koehn et al., 2007), a state-of-the-art open-source phrasebased SMT system. The translation and the reordering model relied on “grow-diag-final” symmetrized word-to-word alignments built using GIZA++ (Och and Ney, 2003) and the training script of Moses. A 5-gram language model was trained on the target side of the training parallel corpus using the IRSTLM toolkit (Federico et al., 2008), exploiting improved Modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. 3.3 http://iwslt2010.fbk.eu/node/33. Baseline System The baseline system was trained on the general corpus with toolkits and settings as described above. The baseline BLEU is 29.34 points. This low value is occurred by the fact that he general corpus does not consist of enough sentences on the travel domain and has a lo"
R13-1094,P02-1040,0,0.0855658,"Missing"
R13-1094,W03-1730,0,0.0144502,"Missing"
tian-etal-2014-um,eisele-chen-2010-multiun,0,\N,Missing
tian-etal-2014-um,D07-1103,0,\N,Missing
tian-etal-2014-um,tiedemann-2010-lingua,0,\N,Missing
tian-etal-2014-um,moore-2002-fast,0,\N,Missing
tian-etal-2014-um,C96-2141,0,\N,Missing
tian-etal-2014-um,J03-3002,0,\N,Missing
tian-etal-2014-um,J90-2002,0,\N,Missing
tian-etal-2014-um,P02-1040,0,\N,Missing
tian-etal-2014-um,P07-2045,0,\N,Missing
tian-etal-2014-um,N09-2061,0,\N,Missing
tian-etal-2014-um,J03-1002,0,\N,Missing
tian-etal-2014-um,2005.mtsummit-papers.11,0,\N,Missing
tian-etal-2014-um,ma-cieri-2006-corpus,0,\N,Missing
tian-etal-2014-um,tiedemann-2012-parallel,0,\N,Missing
tian-etal-2014-um,W03-1730,0,\N,Missing
W12-6310,I08-4017,0,0.0426201,"Missing"
W12-6310,I05-3025,0,0.0893904,"Missing"
W12-6310,W10-4137,0,0.0304063,"Missing"
W12-6310,W96-0213,0,0.57737,"Missing"
W12-6310,I05-3027,0,0.110099,"Missing"
W12-6310,O03-4002,0,0.119515,"Missing"
W12-6310,W06-0127,0,0.0482669,"Missing"
W12-6310,W15-3100,0,\N,Missing
W12-6317,C96-1035,0,0.0753091,"artment of Computer and Information Science, University of Macau, Macau SAR, China {MB15463, DerekFw, Lidiac}@umac.mo 2 Micro-blog has more uncertainties than normal Chinese text. For instance, the micro-blog texts contain a large number of network words like “打酱油” and “楼主” which are easily to be mis-segmented due to the arbitrary nature of language. The dialect words and wrong written words are also easily to be mis-segmented according to a limited knowledge of these. To accomplish this task, many approaches had been proposed. The first adapted and efficient approach is the Maximum Matching (Wong & Chan, 1996). Its segmentation accuracy is depending on the quality of system dictionary. System dictionary is a manual defined lexicon that it contains the majority of standardized words. However, with the development of language, new words are springing up. The system dictionary cannot track of newly born vocabularies. Several years later machine learning approaches had been applied. The Maximum Entropy (Shi, 2005) achieved the highest accuracy among most of the tasks in SIGHAN-2005 Bake-off 1 Segmentation contest. During the same contest Conditional Random Fields (CRFs) (Zhou et al, 2005) has the best"
W12-6317,I05-3034,0,0.0315168,"mis-segmented according to a limited knowledge of these. To accomplish this task, many approaches had been proposed. The first adapted and efficient approach is the Maximum Matching (Wong & Chan, 1996). Its segmentation accuracy is depending on the quality of system dictionary. System dictionary is a manual defined lexicon that it contains the majority of standardized words. However, with the development of language, new words are springing up. The system dictionary cannot track of newly born vocabularies. Several years later machine learning approaches had been applied. The Maximum Entropy (Shi, 2005) achieved the highest accuracy among most of the tasks in SIGHAN-2005 Bake-off 1 Segmentation contest. During the same contest Conditional Random Fields (CRFs) (Zhou et al, 2005) has the best performance in solving the out-of-vocabulary (OOV) problem. Hidden Markov Model (HMM) (Zhang et al, 2003) is another efficient approach in Chinese segmentation. It has an efficient approach in handling the word ambiguity 2 issue. Furthermore it achieved the best result in the first Chinese segmentation competition3. The most two common issues in Chinese segmentation are OOV and ambiguity. In this word we"
W12-6317,W03-1709,0,0.0246388,"ctionary is a manual defined lexicon that it contains the majority of standardized words. However, with the development of language, new words are springing up. The system dictionary cannot track of newly born vocabularies. Several years later machine learning approaches had been applied. The Maximum Entropy (Shi, 2005) achieved the highest accuracy among most of the tasks in SIGHAN-2005 Bake-off 1 Segmentation contest. During the same contest Conditional Random Fields (CRFs) (Zhou et al, 2005) has the best performance in solving the out-of-vocabulary (OOV) problem. Hidden Markov Model (HMM) (Zhang et al, 2003) is another efficient approach in Chinese segmentation. It has an efficient approach in handling the word ambiguity 2 issue. Furthermore it achieved the best result in the first Chinese segmentation competition3. The most two common issues in Chinese segmentation are OOV and ambiguity. In this word we assume that the some existing segmentation Abstract This paper proposed a Hidden Markov Model (HMM) based tokenizer for Chinese micro-blog texts. Comparing with normal Chinese texts, micro-blog texts contain more uncertainties. These uncertainties are generally aroused by the irregular use of blo"
W12-6327,W06-0803,0,0.0605728,"Missing"
W12-6327,W06-0129,0,0.0518553,"Missing"
W12-6327,M98-1002,0,0.0937787,"Missing"
W12-6327,W03-0419,0,0.238336,"Missing"
W12-6327,H05-1054,0,0.060689,"Missing"
W12-6327,P02-1060,0,0.124527,"Missing"
W12-6327,D07-1074,0,0.255907,"Missing"
W12-6327,N04-4028,0,0.0805405,"Missing"
W12-6327,doddington-etal-2004-automatic,0,0.172204,"Missing"
W12-6327,W03-0405,0,0.0981105,"Missing"
W12-6327,W15-3100,0,\N,Missing
W12-6327,N04-4000,0,\N,Missing
W12-6333,P05-1022,0,0.253772,"Missing"
W12-6333,P08-1067,0,0.0777914,"Missing"
W12-6333,P03-1054,0,0.264142,"parser trained on an English Treebank for Chinese sentences, we cannot get a good parsing. However, the 188 Proceedings of the Second CIPS-SIGHAN Joint Conference on Chinese Language Processing, pages 188–193, Tianjin, China, 20-21 DEC. 2012 Horizontal Markov Order Vertical Order Table 1: Markovization and corresponding statistical model methodology of parsing can be highly applicable. Even for those corpora with different annotation format, there still has a well-performed parser to fit the specific structure for the data. In this work, we adopt an existing powerful parser, Stanford parser (Klein and Manning, 2003), which has shown its effectiveness in English. We make the necessary modifications for parsing Chinese and apply it to the shared task. In this evaluation, we use TCT Treebank as the developing and experimental data. The Treebank uses an annotation scheme with doubletagging (Zhou, 2004). Under this scheme, every sentence is annotated with a complete parse tree, where each non-terminal constituent is assigned with two tags, the syntactic constituent tag and the grammatical relation tag, which also is a new annotation scheme that differs from with head constituents in previous TCT version. In o"
W12-6333,P04-1061,0,0.0334033,"pe: In this rule, is the left-hand-side, is the head word in the right-hand-side, stands for the modifiers. indicates parent nodes and indicates grandparent nodes (Klein and Manning, 2003). Table 1 gives the unlexicalized parsing models corresponding to different horizontal and vertical orders. The dependency models is a pair of a head and argument, which are words in a sentence. A dependency structure D over a sentence is a set of dependencies (arrows) which form a planar, acyclic graph rooted at the special symbol ROOT, and in which each word in sentence appears as an argument exactly once (Klein and Manning, 2004). The arrow connects a head with a dependent, and the head of a constituent is generated by the head propagation table. The CKY algorithm is used in dependency parsing. Actually, the factored model reaches to the efficient by factoring the two sub-models and simplified both. There is a brief top-level procedure described in (Klein and Manning, 2002 ). 1. Extract the PCFG sub-model and set up the PCFG parser. 2. Use the PCFG parser to find outside scores for each edge. 3. Extract the dependency sub-model and set up the dependency parser. 4. Use the dependency parser to find outside scores for e"
W12-6333,levy-andrew-2006-tregex,0,0.070986,"Missing"
W12-6333,P95-1037,0,0.174307,"he head child in the internal rules with the head propagation table. Besides, the Stanford factored model also is the combination of unlexicalized PCFG models and lexicalized models, it has to encode the lexicalized information in each non-terminal node. Likewise, the lexicalized parser uses the head propagation table as well. However, the newest TCT corpus does not contain the head word information. To this end, we define a specific head propagation table using the TregEx tool after classifying the grammar rules and counting the frequency of some related tags. Which differs from the work of (Magerman, 1995) and (Collins, 1999) that the rules of head finding are defined based on linguistic knowledge. There are three steps to generate the head propagation table. Firstly, we extract all the grammar rules from the TCT corpus, and then classify the rules according to their parent nodes. Secondly, we calculate the frequency of each sort of child node that have the same parent node, then select the higher frequency child nodes as the candidate head word. For example, under the ap (adjective phrase) node, we get some relatively high frequency child nodes by counting showed in the table 2. Thirdly, we se"
W12-6333,P06-1055,0,0.165196,"Missing"
W12-6333,J03-4003,0,\N,Missing
W12-6337,H91-1060,0,0.0573496,"Missing"
W12-6337,P05-1022,0,0.16126,"Missing"
W12-6337,J03-4003,0,0.32321,"Missing"
W12-6337,N09-2064,0,0.0284625,"Missing"
W12-6337,D07-1117,0,0.44967,"Missing"
W12-6337,P03-1054,0,0.177119,", the traditional tree view of sentence 嘉珍和我住在同一條巷子 is shown in Figure 1: 3 Multilingual Parsing Models In our experiments we will employ two multilingual statistical parsers – the Stanford Parser and the Berkeley Parser. We will describe the Stanford package and our modification in order to make this package adapt to the Sinica Treebank in Subsection 3.1. The Berkeley parser will be referred to in Section 3.2. In that Section we will also propose a new Chinese unknown word model. A Factored Model A factored parser, which combine a high optimized unlexicalized parsing model (syntactic model) (Klein and Manning, 2003) and a dependency parser (semantic model) can be trained by the Stanford parser. The unlexicalized model produces a high optimized probabilistic contextfree grammar, which adds some linguistically motivated annotation to both phrasal and Part-ofSpeech tags to do disambiguation. In the lexical dependencies part, the information of direction, distance and valence between a constituent and its modifiers will be encoded into the dependency model. The probability of a tree is then calculated through the product of the probabilities that the syntactic model and the semantic model assign to that tree"
W12-6337,P03-1056,0,0.1478,"IGHAN shared task of Traditional Chinese Parsing. We have adopted two multilingual parsing models – a factored model (Stanford Parser) and an unlexicalized model (Berkeley Parser) for parsing the Sinica Treebank. This paper also proposes a new Chinese unknown word model and integrates it into the Berkeley Parser. Our experiment gives the first result of adapting existing multilingual parsing models to the Sinica Treebank and shows that the parsing accuracy can be improved by our suggested approach. lish ones. For example, the Chinese syntactic tree is constructed flatter than the English one (Levy and Manning, 2003). In this paper, we present our solution for the 2012 CIPS-SIGHAN shared task of Traditional Chinese parsing. We exploit two existing powerful parsing models – the factored model (Stanford Parser) and the unlexicalized model (Berkeley Parser), which have already shown their effectiveness in English, and adapt it to our task with necessary modification. First, in order to make use of Stanford Parser, we try to build a head propagation table of Traditional Chinese for the adaptation of the specific Traditional Chinese Corpus – Sinica Treebank (Chen et al., 2000). Second, we propose a new Chinese"
W12-6337,P06-1055,0,0.0642147,"[+ASP], VHC[+SPV], VI, VJ, VJ[+DE], VJ[+NEG], VK, VK[+ASP], VK[+DE], VK[+NEG], VL, V_11, V_12, V_2, V, S, NP, Na, Nc, Ng, P, DM, D, Di, Dfa, Caa, Caa[P1], Caa[P2], Cab, Cbb, NP, N, Na, Nb, Nc, Ncd, Nd, Nda, Ndb, Ndc, Nde, Ndf, Nep, Neqa, Neqb, Neu, Nf, Nh, N‧的, Nv, PP, P, GP, DE, DM, Caa, Caa[P1], Caa[P2], Cab VE, Ncd, Nes, Ng,P, GP, Caa, Caa[P1], Caa[P2] Neu, Nf, DM Table 2: The Head rules used for Sinica Treebank in the Stanford Parser Table 2 gives our version of Traditional Chinese head propagation table. 4 3.2 3.2.1 The Berkeley Parser An Improved Unlexicalized Model The Berkeley parser (Petrov et al. 2006; Petrov and Klein, 2007) enhanced the unlexicalized model which is adopted in the Stanford parser. In the grammar training phase, Berkeley parser use an automatic approach to realize the tree annotation which is analyzed and testified manually in Stanford’s unlexicalized model; that is, iteratively rectify a raw X-bar grammar by repeatedly splitting and merging non-terminal symbols, with a reasonable smoothing. At first, the baseline Xbar grammar is obtained directly from the raw datasets by a binarization procedure. In each iteration, for splitting, the symbol could be split into subsymbols."
W12-6337,C02-1145,0,0.287768,"Missing"
W12-6337,N07-1051,0,\N,Missing
W12-6337,W00-1201,0,\N,Missing
W12-6337,W00-1205,0,\N,Missing
W13-2245,2001.mtsummit-papers.3,0,0.0208464,"lded promising results especially the statistical models of CRF and NB. The official results show that our CRF model achieved the highest F-score 0.8297 in binary classification of Task 2. 1 Introduction Due to the fast development of Machine translation, different automatic evaluation methods for the translation quality have been proposed in recent years. One of the categories is the lexical similarity based metric. This kind of metrics includes the edit distance based method, such as WER (Su et al., 1992), Multi-reference WER (Nießen et al., 2000), PER (Tillmann et al., 1997), the works of (Akiba, et al., 2001), (Leusch et al., 2006) and (Wang and Manning, 2012); the precision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information us"
W13-2245,P07-1038,0,0.0184591,", VLfin, VCLIinf, VLger, VCLIfin, VLinf, VEadj, VMadj, VEfin, VMfin, VEger, VMger, VEinf, VMinf, VHadj, VSadj, VHfin, VSfin, VHger, VSger, VHinf, VSinf VLadj, X ACRNM, ALFP, ALFS, FO, ITJN, ORD, PAL, PDEL, PE, PNC, SYM . BACKSLASH, CM, COLON, DASH, DOTS, FS, LP, QT, RP, SEMICOLON, SLASH Table 1: Developed POS mapping for Spanish and universal tagset 2 Related Works Gamon et al. (2005) perform a research about reference-free SMT evaluation method on sentence level. This work uses both linear and nonlinear combinations of language model and SVM classifier to find the badly translated sentences. Albrecht and Hwa (2007) conduct the sentencelevel MT evaluation utilizing the regression learning and based on a set of weaker indicators of fluency and adequacy as pseudo references. Specia and Gimenez (2010) use the Confidence Estimation features and a learning mechanism trained on human annotations. They show that the developed models are highly biased by difficulty level of the input segment, therefore they are not appropriate for comparing multiple systems that translate the same input segments. Specia et al. (2010) discussed the issues between the traditional machine translation evaluation and the quality esti"
W13-2245,C12-1008,0,0.0130894,"prediction task and the features are usually extracted from the source sentences and target (translated) sentences. They also show that the developed methods correlate better with human judgments at segment level as compared to traditional metrics. Popović et al. (2011) perform the MT evaluation using the IBM model one with the information of morphemes, 4-gram POS and lexicon probabilities. Mehdad et al. (2012) use the cross-lingual textual entailment to push semantics into the MT evaluation without using reference translations. This evaluation work mainly focuses on the adequacy estimation. Avramidis (2012) performs an automatic sentence-level ranking of multiple machine translations using the features of verbs, nouns, sentences, subordinate clauses and punctuation occurrences to derive the adequacy information. Other descriptions of the MT Quality Estimation tasks can be gained in the works of (Callison-Burch et al., 2012) and (Felice and Specia, 2012). 3 Tasks Information This section introduces the different sub-tasks we participated in the Quality Estimation task of WMT 13 and the methods we used. 3.1 Task 1-1 Sentence-level QE Task 1.1 is to score and rank the post-editing effort of the aut"
W13-2245,W09-0401,0,0.0303723,"mize this margin. { ( ( )) } ‖ ‖ (7) where is normal to the hyper plane, |is the Euclidean norm of , and is the perpendicular distance from the hyper plane to the origin. For details of SVM, see the works of (Cortes and Vapnik, 1995) and (Burges, 1998). EN-ES NB-LPR MAE RMSE Time MAE .315 .399 .40s .304 DE-EN NB-LPR MAE RMSE Time MAE .318 .401 .79s .312 SVM-LPR RMSE Time .551 60.67s SVM-LPR RMSE Time .559 111.7s Table 5: NB-LPR and SVM-LPR training In the training stage, we used all the officially released data of WMT 09, 10, 11 and 12 for the EN-ES and DE-EN language pairs. We used the WEKA (Hall et al., 2009) data mining software to implement the NB and SVM algorithms. The training scores are shown in Table 5. The NBLPR performs lower scores than the SVM-LPR but faster than SVM-LPR. Methods EBLEU-I EBLEU-A NB-LPR Baseline DE-EN Tau(ties |Tau|(ties penalized) ignored) -0.38 -0.03 N/A N/A -0.49 0.01 -0.12 0.08 EN-ES Tau(ties |Tau|(ties penalized) ignored) -0.35 0.02 -0.27 N/A N/A 0.07 -0.23 0.03 Table 6: QE Task 1.2 testing scores The official testing scores are shown in Table 6. Each task is allowed to submit up to two systems and we submitted the results using the methods of EBLEU and NB-LPR. The"
W13-2245,C12-2044,1,0.883308,"Missing"
W13-2245,2013.mtsummit-posters.3,1,0.69041,"ision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA (Dahlmeier et al., 2011), (Liu et al., 2010) and (Han et al., 2013), phrase information used by (Povlsen, et al., 1998) and (Echizen-ya and Araki, 2010), sentence structure used by (Owczarzak et al., 2007); the semantic similarity, such as textual entailment used by (Mirkin et al., 2009) and (Castillo and Estrella, 2012), Synonyms used by METEOR (Lavie and Agarwal, 2007), (Wong and Kit, 2012), (Chan and Ng, 2008); paraphrase used by (Snover et al., 2009). The traditional evaluation metrics tend to evaluate the hypothesis translation as compared to the reference translations that are usually offered by human efforts. However, in the practice, there is usually"
W13-2245,P12-1098,0,0.0126221,"d of metrics includes the edit distance based method, such as WER (Su et al., 1992), Multi-reference WER (Nießen et al., 2000), PER (Tillmann et al., 1997), the works of (Akiba, et al., 2001), (Leusch et al., 2006) and (Wang and Manning, 2012); the precision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA (Dahlmeier et al., 2011), (Liu et al., 2010) and (Han et al., 2013), phrase information used by (Povlsen, et al., 1998) and (Echizen-ya and Araki, 2010), sentence structure used by (Owczarzak et al., 2007); the semantic similarity, such as textual entailment used by (Mirkin et al., 2009) and (Castillo and Estrella, 2012), Synonyms used by METEOR (Lavie and Agarwal, 2007), (Wong and Kit, 2012), (Chan and Ng, 200"
W13-2245,W07-0734,0,0.0404153,"in recent years. One of the categories is the lexical similarity based metric. This kind of metrics includes the edit distance based method, such as WER (Su et al., 1992), Multi-reference WER (Nießen et al., 2000), PER (Tillmann et al., 1997), the works of (Akiba, et al., 2001), (Leusch et al., 2006) and (Wang and Manning, 2012); the precision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA (Dahlmeier et al., 2011), (Liu et al., 2010) and (Han et al., 2013), phrase information used by (Povlsen, et al., 1998) and (Echizen-ya and Araki, 2010), sentence structure used by (Owczarzak et al., 2007); the semantic similarity, such as textual entailment used by (Mirkin et al., 2009) and (Castillo and Estrella, 2012), Synonyms"
W13-2245,E06-1031,0,0.0175746,"especially the statistical models of CRF and NB. The official results show that our CRF model achieved the highest F-score 0.8297 in binary classification of Task 2. 1 Introduction Due to the fast development of Machine translation, different automatic evaluation methods for the translation quality have been proposed in recent years. One of the categories is the lexical similarity based metric. This kind of metrics includes the edit distance based method, such as WER (Su et al., 1992), Multi-reference WER (Nießen et al., 2000), PER (Tillmann et al., 1997), the works of (Akiba, et al., 2001), (Leusch et al., 2006) and (Wang and Manning, 2012); the precision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA (Dahlmeier"
W13-2245,N03-1020,0,0.0954779,"f Machine translation, different automatic evaluation methods for the translation quality have been proposed in recent years. One of the categories is the lexical similarity based metric. This kind of metrics includes the edit distance based method, such as WER (Su et al., 1992), Multi-reference WER (Nießen et al., 2000), PER (Tillmann et al., 1997), the works of (Akiba, et al., 2001), (Leusch et al., 2006) and (Wang and Manning, 2012); the precision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA (Dahlmeier et al., 2011), (Liu et al., 2010) and (Han et al., 2013), phrase information used by (Povlsen, et al., 1998) and (Echizen-ya and Araki, 2010), sentence structure used by (Owczarzak et al., 2007); the semantic"
W13-2245,P10-1012,0,0.0444098,"Missing"
W13-2245,H05-1093,0,0.0164056,"gories is the lexical similarity based metric. This kind of metrics includes the edit distance based method, such as WER (Su et al., 1992), Multi-reference WER (Nießen et al., 2000), PER (Tillmann et al., 1997), the works of (Akiba, et al., 2001), (Leusch et al., 2006) and (Wang and Manning, 2012); the precision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA (Dahlmeier et al., 2011), (Liu et al., 2010) and (Han et al., 2013), phrase information used by (Povlsen, et al., 1998) and (Echizen-ya and Araki, 2010), sentence structure used by (Owczarzak et al., 2007); the semantic similarity, such as textual entailment used by (Mirkin et al., 2009) and (Castillo and Estrella, 2012), Synonyms used by METEOR (Lavie and A"
W13-2245,P06-2070,0,0.0239297,"ion of Task 2. 1 Introduction Due to the fast development of Machine translation, different automatic evaluation methods for the translation quality have been proposed in recent years. One of the categories is the lexical similarity based metric. This kind of metrics includes the edit distance based method, such as WER (Su et al., 1992), Multi-reference WER (Nießen et al., 2000), PER (Tillmann et al., 1997), the works of (Akiba, et al., 2001), (Leusch et al., 2006) and (Wang and Manning, 2012); the precision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA (Dahlmeier et al., 2011), (Liu et al., 2010) and (Han et al., 2013), phrase information used by (Povlsen, et al., 1998) and (Echizen-ya and Araki, 2010), sentence"
W13-2245,W12-3110,0,0.114788,"Missing"
W13-2245,W12-3122,0,0.0147383,"ference translations in order to measure a score reflecting some aspects of its quality, e.g. the BLEU and NIST. The quality estimation addresses this problem by evaluating the quality of translations as a prediction task and the features are usually extracted from the source sentences and target (translated) sentences. They also show that the developed methods correlate better with human judgments at segment level as compared to traditional metrics. Popović et al. (2011) perform the MT evaluation using the IBM model one with the information of morphemes, 4-gram POS and lexicon probabilities. Mehdad et al. (2012) use the cross-lingual textual entailment to push semantics into the MT evaluation without using reference translations. This evaluation work mainly focuses on the adequacy estimation. Avramidis (2012) performs an automatic sentence-level ranking of multiple machine translations using the features of verbs, nouns, sentences, subordinate clauses and punctuation occurrences to derive the adequacy information. Other descriptions of the MT Quality Estimation tasks can be gained in the works of (Callison-Burch et al., 2012) and (Felice and Specia, 2012). 3 Tasks Information This section introduces"
W13-2245,P09-1089,0,0.0140549,"ch as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA (Dahlmeier et al., 2011), (Liu et al., 2010) and (Han et al., 2013), phrase information used by (Povlsen, et al., 1998) and (Echizen-ya and Araki, 2010), sentence structure used by (Owczarzak et al., 2007); the semantic similarity, such as textual entailment used by (Mirkin et al., 2009) and (Castillo and Estrella, 2012), Synonyms used by METEOR (Lavie and Agarwal, 2007), (Wong and Kit, 2012), (Chan and Ng, 2008); paraphrase used by (Snover et al., 2009). The traditional evaluation metrics tend to evaluate the hypothesis translation as compared to the reference translations that are usually offered by human efforts. However, in the practice, there is usually no golden reference for the translated documents, especially on the internet works. How to evaluate the quality of automatically translated documents or sentences without using the reference translations becomes a new cha"
W13-2245,2010.amta-papers.3,0,0.0274666,"ITJN, ORD, PAL, PDEL, PE, PNC, SYM . BACKSLASH, CM, COLON, DASH, DOTS, FS, LP, QT, RP, SEMICOLON, SLASH Table 1: Developed POS mapping for Spanish and universal tagset 2 Related Works Gamon et al. (2005) perform a research about reference-free SMT evaluation method on sentence level. This work uses both linear and nonlinear combinations of language model and SVM classifier to find the badly translated sentences. Albrecht and Hwa (2007) conduct the sentencelevel MT evaluation utilizing the regression learning and based on a set of weaker indicators of fluency and adequacy as pseudo references. Specia and Gimenez (2010) use the Confidence Estimation features and a learning mechanism trained on human annotations. They show that the developed models are highly biased by difficulty level of the input segment, therefore they are not appropriate for comparing multiple systems that translate the same input segments. Specia et al. (2010) discussed the issues between the traditional machine translation evaluation and the quality estimation tasks recently proposed. The traditional MT evaluation metrics require reference translations in order to measure a score reflecting some aspects of its quality, e.g. the BLEU and"
W13-2245,niessen-etal-2000-evaluation,0,0.0794176,"st WMT corpora showed that the designed methods of this paper yielded promising results especially the statistical models of CRF and NB. The official results show that our CRF model achieved the highest F-score 0.8297 in binary classification of Task 2. 1 Introduction Due to the fast development of Machine translation, different automatic evaluation methods for the translation quality have been proposed in recent years. One of the categories is the lexical similarity based metric. This kind of metrics includes the edit distance based method, such as WER (Su et al., 1992), Multi-reference WER (Nießen et al., 2000), PER (Tillmann et al., 1997), the works of (Akiba, et al., 2001), (Leusch et al., 2006) and (Wang and Manning, 2012); the precision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics"
W13-2245,P02-1040,0,0.0895411,"achieved the highest F-score 0.8297 in binary classification of Task 2. 1 Introduction Due to the fast development of Machine translation, different automatic evaluation methods for the translation quality have been proposed in recent years. One of the categories is the lexical similarity based metric. This kind of metrics includes the edit distance based method, such as WER (Su et al., 1992), Multi-reference WER (Nießen et al., 2000), PER (Tillmann et al., 1997), the works of (Akiba, et al., 2001), (Leusch et al., 2006) and (Wang and Manning, 2012); the precision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA (Dahlmeier et al., 2011), (Liu et al., 2010) and (Han et al., 2013), phrase information used by (Povlsen,"
W13-2245,P06-1055,0,0.0366982,"he different sub-tasks we participated in the Quality Estimation task of WMT 13 and the methods we used. 3.1 Task 1-1 Sentence-level QE Task 1.1 is to score and rank the post-editing effort of the automatically translated EnglishSpanish sentences without offering the reference translation. Firstly, we develop the English and Spanish POS tagset mapping as shown in Table 1. The 75 Spanish POS tags yielded by the Treetagger (Schmid, 1994) are mapped to the 12 universal tags developed in (Petrov et al., 2012). The English POS tags are extracted from the parsed sentences using the Berkeley parser (Petrov et al., 2006). Secondly, the enhanced version of BLEU (EBLEU) formula is designed with the factors of modified length penalty ( ), precision, and recall, the and representing the lengths of hypothesis (target) sentence and source sentence respectively. We use the harmonic mean of pre). We assign cision and recall, i.e. ( the weight values and , i.e. higher weight value is assigned to precision, which is different with METEOR (the inverse values). (∑ ( ( { ))) (1) (2) (3) (4) 366 Lastly, the scoring for the post-editing effort of the automatically translated sentences is performed on the extracted POS seque"
W13-2245,C92-2067,0,0.0293436,"thm. The training experiments on the past WMT corpora showed that the designed methods of this paper yielded promising results especially the statistical models of CRF and NB. The official results show that our CRF model achieved the highest F-score 0.8297 in binary classification of Task 2. 1 Introduction Due to the fast development of Machine translation, different automatic evaluation methods for the translation quality have been proposed in recent years. One of the categories is the lexical similarity based metric. This kind of metrics includes the edit distance based method, such as WER (Su et al., 1992), Multi-reference WER (Nießen et al., 2000), PER (Tillmann et al., 1997), the works of (Akiba, et al., 2001), (Leusch et al., 2006) and (Wang and Manning, 2012); the precision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of"
W13-2245,2003.mtsummit-papers.51,0,0.0626443,"ion quality have been proposed in recent years. One of the categories is the lexical similarity based metric. This kind of metrics includes the edit distance based method, such as WER (Su et al., 1992), Multi-reference WER (Nießen et al., 2000), PER (Tillmann et al., 1997), the works of (Akiba, et al., 2001), (Leusch et al., 2006) and (Wang and Manning, 2012); the precision based method, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and SIA (Liu and Gildea, 2006); recall based method, such as ROUGE (Lin and Hovy 2003); and the combination of precision and recall, such as GTM (Turian et al., 2003), METEOR (Lavie and Agarwal, 2007), BLANC (Lita et al., 2005), AMBER (Chen and Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA (Dahlmeier et al., 2011), (Liu et al., 2010) and (Han et al., 2013), phrase information used by (Povlsen, et al., 1998) and (Echizen-ya and Araki, 2010), sentence structure used by (Owczarzak et al., 2007); the semantic similarity, such as textual entailment used by (Mirkin et al., 2009) and (Casti"
W13-2245,D12-1097,0,0.0161902,"d Kuhn, 2011), PORT (Chen et al., 2012b), and LEPOR (Han et al., 2012). Another category is the using of linguistic features. This kind of metrics includes the syntactic similarity, such as the POS information used by TESLA (Dahlmeier et al., 2011), (Liu et al., 2010) and (Han et al., 2013), phrase information used by (Povlsen, et al., 1998) and (Echizen-ya and Araki, 2010), sentence structure used by (Owczarzak et al., 2007); the semantic similarity, such as textual entailment used by (Mirkin et al., 2009) and (Castillo and Estrella, 2012), Synonyms used by METEOR (Lavie and Agarwal, 2007), (Wong and Kit, 2012), (Chan and Ng, 2008); paraphrase used by (Snover et al., 2009). The traditional evaluation metrics tend to evaluate the hypothesis translation as compared to the reference translations that are usually offered by human efforts. However, in the practice, there is usually no golden reference for the translated documents, especially on the internet works. How to evaluate the quality of automatically translated documents or sentences without using the reference translations becomes a new challenge in front of the NLP researchers. 365 Proceedings of the Eighth Workshop on Statistical Machine Trans"
W13-2245,W12-3103,0,\N,Missing
W13-2245,W07-0714,0,\N,Missing
W13-2245,W12-3102,0,\N,Missing
W13-2245,2005.eamt-1.15,0,\N,Missing
W13-2245,W11-2105,0,\N,Missing
W13-2245,P08-1007,0,\N,Missing
W13-2245,W11-2109,0,\N,Missing
W13-2245,W10-1754,0,\N,Missing
W13-2245,W11-2106,0,\N,Missing
W13-2253,W05-0909,0,0.0790852,"02) is one of the commonly used evaluation metrics that is designed to calculate the document level precisions. NIST (Doddington, 2002) metric is proposed based on BLEU but with the information weights added to the n-gram approaches. TER (Snover et al., 2006) is another well-known MT evaluation metric that is designed to calculate the amount of work needed to correct the hypothesis translation according to the reference translations. TER includes the edit categories such as insertion, deletion, substitution of single words and the shifts of word chunks. Other related works include the METEOR (Banerjee and Lavie, 2005) that uses semantic matching (word stem, synonym, and paraphrase), and (Wong and Kit, 2008), (Popovic, 2012), and (Chen et al., 2012) that introduces the word order factors, etc. The traditional evaluation metrics tend to perform well on the language pairs with English as the target language. This paper will introduce the evaluation models that can also perform well on the language pairs that with English as source language. 3 3.1 culated on sentence-level instead of corpus-level that is used in BLEU ( ). Then we define the weighted n-gram harmonic mean of precision and recall (WNHPR). (∑ ( (4"
W13-2253,C12-2044,1,0.865023,"Missing"
W13-2253,W06-3114,0,0.147235,"sed around 1960s (Carroll, 1966), and the adequacy (similar as fidelity), fluency and comprehension (improved intelligibility) (White et al., 1994). Because of the expensive cost of manual evaluations, the automatic evaluation metrics and systems appear recently. The early automatic evaluation metrics include the word error rate WER (Su et al., 1992) and position independent word error rate PER (Tillmann et al., 1997) that are based on the Levenshtein distance. Several promotions for the MT and MT evaluation literatures include the ACL’s annual workshop on statistical machine translation WMT (Koehn and Monz, 2006; Callison-Burch et al., 2012), NIST open machine translation (OpenMT) Evaluation series (Li, 2005) and the international workshop of spoken language translation IWSLT, which is also organized annually from 2004 (Eck and Hori, 2005; 414 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 414–421, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics Paul, 2008, 2009; Paul, et al., 2010; Federico et al., 2011). BLEU (Papineni et al., 2002) is one of the commonly used evaluation metrics that is designed to calculate the document level precisi"
W13-2253,W02-1001,0,0.012518,"experiment results of WMT Metrics Task, we used the part of speech information of the words in question. In grammar, a part of speech, which is also called a word class, a lexical class, or a lexical category, is a linguistic category of lexical items. It is generally defined by the syntactic or morphological behavior of the lexical item in question. The POS information utilized in our metric LEPOR_v3.1, an enhanced version of LEPOR (Han et al., 2012), is extracted using the Berkeley parser (Petrov et al., 2006) for English, German, and French languages, using COMPOST Czech morphology tagger (Collins, 2002) for Czech language, and using TreeTagger (Schmid, 1994) for Spanish and Russian languages respectively. 415 Ratio HPR:LP:NPP(word) HPR:LP:NPP(POS) ( ( CZ-EN 7:2:1 NA 1:9 NA NA other-to-English DE-EN ES-EN 3:2:1 7:2:1 3:2:1 NA 9:1 1:9 9:1 NA 1:9 NA FR-EN 3:2:1 3:2:1 9:1 9:1 9:1 EN-CZ 7:2:1 7:2:1 9:1 9:1 1:9 English-to-other EN-DE EN-ES 1:3:7 3:2:1 7:2:1 NA 9:1 9:1 9:1 NA 1:9 NA EN-FR 3:2:1 3:2:1 9:1 9:1 9:1 Table 1. The tuned weight values in LEPOR_v3.1 system Correlation Score with Human Judgment other-to-English English-to-other System LEPOR_v3.1 nLEPOR_baseline METEOR BLEU TER CZ-EN DE-EN E"
W13-2253,2010.iwslt-evaluation.1,0,0.067653,"Levenshtein distance. Several promotions for the MT and MT evaluation literatures include the ACL’s annual workshop on statistical machine translation WMT (Koehn and Monz, 2006; Callison-Burch et al., 2012), NIST open machine translation (OpenMT) Evaluation series (Li, 2005) and the international workshop of spoken language translation IWSLT, which is also organized annually from 2004 (Eck and Hori, 2005; 414 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 414–421, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics Paul, 2008, 2009; Paul, et al., 2010; Federico et al., 2011). BLEU (Papineni et al., 2002) is one of the commonly used evaluation metrics that is designed to calculate the document level precisions. NIST (Doddington, 2002) metric is proposed based on BLEU but with the information weights added to the n-gram approaches. TER (Snover et al., 2006) is another well-known MT evaluation metric that is designed to calculate the amount of work needed to correct the hypothesis translation according to the reference translations. TER includes the edit categories such as insertion, deletion, substitution of single words and the shifts of wo"
W13-2253,P06-1055,0,0.00760668,"Features The linguistic features could be easily employed into our evaluation models. In the submitted experiment results of WMT Metrics Task, we used the part of speech information of the words in question. In grammar, a part of speech, which is also called a word class, a lexical class, or a lexical category, is a linguistic category of lexical items. It is generally defined by the syntactic or morphological behavior of the lexical item in question. The POS information utilized in our metric LEPOR_v3.1, an enhanced version of LEPOR (Han et al., 2012), is extracted using the Berkeley parser (Petrov et al., 2006) for English, German, and French languages, using COMPOST Czech morphology tagger (Collins, 2002) for Czech language, and using TreeTagger (Schmid, 1994) for Spanish and Russian languages respectively. 415 Ratio HPR:LP:NPP(word) HPR:LP:NPP(POS) ( ( CZ-EN 7:2:1 NA 1:9 NA NA other-to-English DE-EN ES-EN 3:2:1 7:2:1 3:2:1 NA 9:1 1:9 9:1 NA 1:9 NA FR-EN 3:2:1 3:2:1 9:1 9:1 9:1 EN-CZ 7:2:1 7:2:1 9:1 9:1 1:9 English-to-other EN-DE EN-ES 1:3:7 3:2:1 7:2:1 NA 9:1 9:1 9:1 NA 1:9 NA EN-FR 3:2:1 3:2:1 9:1 9:1 9:1 Table 1. The tuned weight values in LEPOR_v3.1 system Correlation Score with Human Judgment"
W13-2253,W12-3106,0,0.0278151,"oddington, 2002) metric is proposed based on BLEU but with the information weights added to the n-gram approaches. TER (Snover et al., 2006) is another well-known MT evaluation metric that is designed to calculate the amount of work needed to correct the hypothesis translation according to the reference translations. TER includes the edit categories such as insertion, deletion, substitution of single words and the shifts of word chunks. Other related works include the METEOR (Banerjee and Lavie, 2005) that uses semantic matching (word stem, synonym, and paraphrase), and (Wong and Kit, 2008), (Popovic, 2012), and (Chen et al., 2012) that introduces the word order factors, etc. The traditional evaluation metrics tend to perform well on the language pairs with English as the target language. This paper will introduce the evaluation models that can also perform well on the language pairs that with English as source language. 3 3.1 culated on sentence-level instead of corpus-level that is used in BLEU ( ). Then we define the weighted n-gram harmonic mean of precision and recall (WNHPR). (∑ ( (4) Thirdly, it is the n-gram based position difference penalty (NPosPenal). This factor is designed to achiev"
W13-2253,2006.amta-papers.25,0,0.0765174,"f spoken language translation IWSLT, which is also organized annually from 2004 (Eck and Hori, 2005; 414 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 414–421, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics Paul, 2008, 2009; Paul, et al., 2010; Federico et al., 2011). BLEU (Papineni et al., 2002) is one of the commonly used evaluation metrics that is designed to calculate the document level precisions. NIST (Doddington, 2002) metric is proposed based on BLEU but with the information weights added to the n-gram approaches. TER (Snover et al., 2006) is another well-known MT evaluation metric that is designed to calculate the amount of work needed to correct the hypothesis translation according to the reference translations. TER includes the edit categories such as insertion, deletion, substitution of single words and the shifts of word chunks. Other related works include the METEOR (Banerjee and Lavie, 2005) that uses semantic matching (word stem, synonym, and paraphrase), and (Wong and Kit, 2008), (Popovic, 2012), and (Chen et al., 2012) that introduces the word order factors, etc. The traditional evaluation metrics tend to perform well"
W13-2253,C92-2067,0,0.163837,"k due to the fact of the diversity of the languages, especially for the evaluation between distant languages (English, Russia, Japanese, etc.). 2 Related works The earliest human assessment methods for machine translation include the intelligibility and fidelity used around 1960s (Carroll, 1966), and the adequacy (similar as fidelity), fluency and comprehension (improved intelligibility) (White et al., 1994). Because of the expensive cost of manual evaluations, the automatic evaluation metrics and systems appear recently. The early automatic evaluation metrics include the word error rate WER (Su et al., 1992) and position independent word error rate PER (Tillmann et al., 1997) that are based on the Levenshtein distance. Several promotions for the MT and MT evaluation literatures include the ACL’s annual workshop on statistical machine translation WMT (Koehn and Monz, 2006; Callison-Burch et al., 2012), NIST open machine translation (OpenMT) Evaluation series (Li, 2005) and the international workshop of spoken language translation IWSLT, which is also organized annually from 2004 (Eck and Hori, 2005; 414 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 414–421, c Sofia,"
W13-2253,1994.amta-1.25,0,0.242009,"Missing"
W13-2253,W06-3124,0,\N,Missing
W13-2253,W12-3102,0,\N,Missing
W13-2253,P02-1040,0,\N,Missing
W13-2253,2005.iwslt-1.1,0,\N,Missing
W13-2253,P12-1098,0,\N,Missing
W13-2253,federico-etal-2012-iwslt,0,\N,Missing
W13-2253,2008.iwslt-evaluation.1,0,\N,Missing
W13-2253,2011.iwslt-evaluation.1,0,\N,Missing
W13-2253,2009.iwslt-evaluation.1,0,\N,Missing
W13-2253,D08-1076,0,\N,Missing
W13-2812,P05-1066,0,0.0540664,"tymne et al. (2008) described that using POS information to split the compounds in a morphologically rich language (German nouns and adjectives) gave an effect for translation output. Holmqvist et al. (2009) also reported that using POS-based and morphologybased sequence model would give an improvement to the translation quality between English and German in WMT09 shared task. In accordance with adding richer information to the training model, reordering the source language text to make it more similar to the target side is confirmed to be another kind of method to improve the word alignment. Collins et al. (2005) employed the forms of syntactic analysis and hand–written rules on the corpus, Xia and McCord (2004) extracted the rules from a parallel text automatically. A statistical machine prereordering method which addressed the reordering problems as a translation from the source sentence to a monotonized source sentence was proposed by Costa-jussà and Fonollosa (2006). Visweswariah et al. (2011) proposed a method which learns a model that can directly reorder source side text from a small parallel corpus with high quality word alignment, but this is hard for people to get such a high-quality aligned"
W13-2812,W06-1609,0,0.0294448,"rman in WMT09 shared task. In accordance with adding richer information to the training model, reordering the source language text to make it more similar to the target side is confirmed to be another kind of method to improve the word alignment. Collins et al. (2005) employed the forms of syntactic analysis and hand–written rules on the corpus, Xia and McCord (2004) extracted the rules from a parallel text automatically. A statistical machine prereordering method which addressed the reordering problems as a translation from the source sentence to a monotonized source sentence was proposed by Costa-jussà and Fonollosa (2006). Visweswariah et al. (2011) proposed a method which learns a model that can directly reorder source side text from a small parallel corpus with high quality word alignment, but this is hard for people to get such a high-quality aligned parallel corpus. Ma et al. (2007) packed some words together with the help of the existing statistical word aligner, which simplify the task of automatic word alignment by packing consecutive words together. These approaches are integrated with morphological information in the translation and decoding model. Our approach is inspired by the approach proposed by"
W13-2812,W09-0421,0,0.0155184,"to reduce the morpheme-level translation ambiguity in an English-Chinese SMT system, Wu et al. (2008) grouped the morphemes into morpheme phrase and used the domain information for translation candidate selection. A contraction separation for Spanish in a Spanish-English SMT system was proposed in (Gispert and Mariño, 2008). Habash et al. (2009) proposed methods to tackle the Arabic enclitics. The experiment in Stymne et al. (2008) described that using POS information to split the compounds in a morphologically rich language (German nouns and adjectives) gave an effect for translation output. Holmqvist et al. (2009) also reported that using POS-based and morphologybased sequence model would give an improvement to the translation quality between English and German in WMT09 shared task. In accordance with adding richer information to the training model, reordering the source language text to make it more similar to the target side is confirmed to be another kind of method to improve the word alignment. Collins et al. (2005) employed the forms of syntactic analysis and hand–written rules on the corpus, Xia and McCord (2004) extracted the rules from a parallel text automatically. A statistical machine prereo"
W13-2812,holmqvist-etal-2012-alignment,0,0.166521,"ormation; reordered the word sequence in the source corpus; deleted case particle and final ending words in Korean; appended the external dictionary in the training step between Korean and English. In the experiment reported by Li et al. (2012), these pre-processing methods on the Korean to Chinese translation system took advantage of POS in their additional factored translation model. In these studies, POS information was reported that it would improve the translation quality, but their taxonomy of POS tag is sole and less. On the word alignment side, we try to implement the idea proposed by Holmqvist et al. (2012), which was reported as a simple, language-independent reordering method to improve the quality of word alignment. But their method did not consider the problem that the probability and the amount would be changed when updated with an improved word alignment. The accuracy of alignment would be improved but the size of phrase-table would be less than the original one because there are more sure alignments generated. The probabilities of word and phrase also have the same problem. Our works are based on the integration of these two methods. We utilized POS information and applied a richer taxono"
W13-2812,P02-1040,0,0.0862268,"wler http://ictclas.nlpir.org/ 3 http://semanticweb.kaist.ac.kr/home/index.php/HanNanum 2 85 is shown that the POS tag set with a richer taxonomy gives a higher translation result. Moreover, two runs of automatic alignment information got better results on the morphologically richer side. All of these methods can be combined together and improve the final translation. Finally, using two tables instead of one modified table in the decoding part will guarantee the translation quality if the reordering model harms the translation result. proved the translation result from 14.98 to 15.50 in BLEU (Papineni et al., 2002). The combination of POS and reordering methods based on the multiple phrase tables and reordering tables got the best performance with BLEU score 17.35. Corpus Baseline POS-based (9 tags) POS-based (22 tags) Alignment-based POS (9 tags) + Alignment POS (22 tags) + Alignment POS (22 tags) + Alignment + two tables KORCHN BLEU 14.98 16.61 16.92 15.50 16.71 17.03 17.35 Acknowledgments This work is partially supported by the Research Committee of University of Macau, and Science and Technology Development Fund of Macau under the grants RG060/09-10S/CS/FST, and 057/2009/A2. Table 2. The translation"
W13-2812,W08-0317,0,0.016837,"al. (2010) transformed the syntactic relations of Chinese SVO patterns and inserted the corresponding transferred relations as pseudo words to solve the problem of word order. In order to reduce the morpheme-level translation ambiguity in an English-Chinese SMT system, Wu et al. (2008) grouped the morphemes into morpheme phrase and used the domain information for translation candidate selection. A contraction separation for Spanish in a Spanish-English SMT system was proposed in (Gispert and Mariño, 2008). Habash et al. (2009) proposed methods to tackle the Arabic enclitics. The experiment in Stymne et al. (2008) described that using POS information to split the compounds in a morphologically rich language (German nouns and adjectives) gave an effect for translation output. Holmqvist et al. (2009) also reported that using POS-based and morphologybased sequence model would give an improvement to the translation quality between English and German in WMT09 shared task. In accordance with adding richer information to the training model, reordering the source language text to make it more similar to the target side is confirmed to be another kind of method to improve the word alignment. Collins et al. (200"
W13-2812,D11-1045,0,0.0499693,"Missing"
W13-2812,W07-0733,0,0.0171544,"lexical translation table and the extraction of phrase table are changed. We assume that after applying our method, the size of the extracted words would increase because more alignments are generated at the end of the second run of GIZA++. Another assumption is that the size of the phrase table would decrease if two languages share such a different word order, because additional alignments would result in some cross alignments but the phrase extraction algorithm could not extract them. Based on two assumptions, we utilize multiple models in the decoding stage. This approach was proposed in (Koehn and Schroeder, 2007; Axelrod et al., 2011) which passes phrase and reordering tables in parallel. We used our modified tables (small size) as the main tables, and the baseline tables (big size) as the additional table when decoding. This can guarantee that if a phrase in testing sentence does not occur in the modified tables, the decoder would find the phrase in the original table. This method is effective in avoiding translation mistakes if our method harms the result. 4 Corpus and system information Token CHN KOR KOR (9) KOR (22) 664,290 539,903 969,445 1,010,117 Avg. Length 7.36 5.98 10.74 11.19 Sentence 90,2"
W13-2812,2008.amta-papers.19,0,0.0382063,"ed restructuring and alignment-based reordering together in the experiment. languages to tackle some problems in SMT: Li et al. (2009) proposed an approach focused on using pre-processing and post-processing methods, such as reordering the source sentences in a Chinese-Korean phrase-based SMT using syntactic information. Lee et al. (2010) transformed the syntactic relations of Chinese SVO patterns and inserted the corresponding transferred relations as pseudo words to solve the problem of word order. In order to reduce the morpheme-level translation ambiguity in an English-Chinese SMT system, Wu et al. (2008) grouped the morphemes into morpheme phrase and used the domain information for translation candidate selection. A contraction separation for Spanish in a Spanish-English SMT system was proposed in (Gispert and Mariño, 2008). Habash et al. (2009) proposed methods to tackle the Arabic enclitics. The experiment in Stymne et al. (2008) described that using POS information to split the compounds in a morphologically rich language (German nouns and adjectives) gave an effect for translation output. Holmqvist et al. (2009) also reported that using POS-based and morphologybased sequence model would g"
W13-2812,W09-0433,0,0.0132708,"improved word alignment. The accuracy of alignment would be improved but the size of phrase-table would be less than the original one because there are more sure alignments generated. The probabilities of word and phrase also have the same problem. Our works are based on the integration of these two methods. We utilized POS information and applied a richer taxonomy of POS tags in the restructuring of Korean, applied reordering method on Korean-Chinese, and combined the POS-based restructuring and alignment-based reordering together in the experiment. languages to tackle some problems in SMT: Li et al. (2009) proposed an approach focused on using pre-processing and post-processing methods, such as reordering the source sentences in a Chinese-Korean phrase-based SMT using syntactic information. Lee et al. (2010) transformed the syntactic relations of Chinese SVO patterns and inserted the corresponding transferred relations as pseudo words to solve the problem of word order. In order to reduce the morpheme-level translation ambiguity in an English-Chinese SMT system, Wu et al. (2008) grouped the morphemes into morpheme phrase and used the domain information for translation candidate selection. A con"
W13-2812,C10-2071,0,0.0182515,"word and phrase also have the same problem. Our works are based on the integration of these two methods. We utilized POS information and applied a richer taxonomy of POS tags in the restructuring of Korean, applied reordering method on Korean-Chinese, and combined the POS-based restructuring and alignment-based reordering together in the experiment. languages to tackle some problems in SMT: Li et al. (2009) proposed an approach focused on using pre-processing and post-processing methods, such as reordering the source sentences in a Chinese-Korean phrase-based SMT using syntactic information. Lee et al. (2010) transformed the syntactic relations of Chinese SVO patterns and inserted the corresponding transferred relations as pseudo words to solve the problem of word order. In order to reduce the morpheme-level translation ambiguity in an English-Chinese SMT system, Wu et al. (2008) grouped the morphemes into morpheme phrase and used the domain information for translation candidate selection. A contraction separation for Spanish in a Spanish-English SMT system was proposed in (Gispert and Mariño, 2008). Habash et al. (2009) proposed methods to tackle the Arabic enclitics. The experiment in Stymne et"
W13-2812,P07-1039,0,0.0240048,"s of syntactic analysis and hand–written rules on the corpus, Xia and McCord (2004) extracted the rules from a parallel text automatically. A statistical machine prereordering method which addressed the reordering problems as a translation from the source sentence to a monotonized source sentence was proposed by Costa-jussà and Fonollosa (2006). Visweswariah et al. (2011) proposed a method which learns a model that can directly reorder source side text from a small parallel corpus with high quality word alignment, but this is hard for people to get such a high-quality aligned parallel corpus. Ma et al. (2007) packed some words together with the help of the existing statistical word aligner, which simplify the task of automatic word alignment by packing consecutive words together. These approaches are integrated with morphological information in the translation and decoding model. Our approach is inspired by the approach proposed by Lee et al (2006) which added 3 POS-based restructuring and alignment-based reordering The POS information is helpful when dealing with morphologically rich languages. In the morphological analysis, the Korean POS tagger involves the analytical task to identify the stem"
W13-2812,D11-1033,0,\N,Missing
W13-2812,C04-1073,0,\N,Missing
W13-2812,P07-2045,0,\N,Missing
W13-2812,J03-1002,0,\N,Missing
W13-2812,2005.mtsummit-papers.11,0,\N,Missing
W13-3605,N12-1086,0,0.0257056,"Missing"
W13-3605,han-etal-2010-using,0,0.0479875,"rror detection and correction are detailed in Section 6, followed by an evaluation, discussion and a conclusion to end the paper. 2 Related Work The issues of grammatical error correction have been discussed from different perspectives for several decades. In this section, we briefly review some related methods. The use of machine learning methods to tackle this problem has shown a promising performance. These methods are normally created based on a large corpus of well-formed native English texts (Tetreault and Chodorow 2008; Tetreault et al., 2010) or annotated non-native data (Gamon, 2010; Han et al., 2010). Although the manually error-tagged text is much more expensive, it has shown improvements over the models trained solely on well-formed native text (Kochmar et al., 2012). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy was generally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or correct the errors in speech or texts (Lynch et al., 2012). However, only using classif"
W13-3605,W12-2025,0,0.47925,"stem for English Grammatical Error Correction Junwen Xing, Longyue Wang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory, Department of Computer and Information Science, University of Macau, Macau S.A.R., China nlp2ct.{vincent, anson}@gmail.com, {derekfw, lidiasc}@umac.mo, nlp2ct.samuel@gmail.com techniques to improve the writing ability for second language learners. Grammatical error correction is the task of automatically detecting and correction erroneous word usage and ill-formed grammatical constructions in text (Dahlmeier et al., 2012). In recent decades, this special task has gained more attention by some organizations such as the Helping Our Own (HOO) challenge (Dale and Kilgarriff, 2010; Dale et al., 2012). Although the performance of grammatical error correction systems has been improved, it is still mostly limited to dealing with the determiner and preposition error types with a very low recall and precision. This year, the CoNLL-2013 shared task extends to include a more comprehensive list of error types, as shown in Table 1. To take on this challenge, this paper proposes pipe-line architecture in combination with sev"
W13-3605,W12-2030,0,0.016206,"non-native data (Gamon, 2010; Han et al., 2010). Although the manually error-tagged text is much more expensive, it has shown improvements over the models trained solely on well-formed native text (Kochmar et al., 2012). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy was generally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or correct the errors in speech or texts (Lynch et al., 2012). However, only using classifiers always cannot give a satisfied performance. Thus, grammar rules and probabilistic language model can be used as a simple but effective assistant for correction of spelling (Kantrowitz et al, 2003) and grammatical errors (Dahlmeier et al., 2012; Lynch et al., 2012; Quan et al., 2012; Rozovskaya et al., 2012). 3 semi-supervised learning method that makes use of a large amount of unlabeled data which is easy to collect. In practice, semi-supervised learning requires less human effort and gives higher accuracy in creating a model. 4 As mentioned before, the corpus"
W13-3605,W13-1703,0,0.181308,"Missing"
W13-3605,W12-2031,0,0.019524,"lem has shown a promising performance. These methods are normally created based on a large corpus of well-formed native English texts (Tetreault and Chodorow 2008; Tetreault et al., 2010) or annotated non-native data (Gamon, 2010; Han et al., 2010). Although the manually error-tagged text is much more expensive, it has shown improvements over the models trained solely on well-formed native text (Kochmar et al., 2012). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy was generally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or correct the errors in speech or texts (Lynch et al., 2012). However, only using classifiers always cannot give a satisfied performance. Thus, grammar rules and probabilistic language model can be used as a simple but effective assistant for correction of spelling (Kantrowitz et al, 2003) and grammatical errors (Dahlmeier et al., 2012; Lynch et al., 2012; Quan et al., 2012; Rozovskaya et al., 2012). 3 semi-supervised learning method that makes use of a l"
W13-3605,N10-1018,0,0.106112,"Missing"
W13-3605,W12-2032,0,0.191472,"rally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or correct the errors in speech or texts (Lynch et al., 2012). However, only using classifiers always cannot give a satisfied performance. Thus, grammar rules and probabilistic language model can be used as a simple but effective assistant for correction of spelling (Kantrowitz et al, 2003) and grammatical errors (Dahlmeier et al., 2012; Lynch et al., 2012; Quan et al., 2012; Rozovskaya et al., 2012). 3 semi-supervised learning method that makes use of a large amount of unlabeled data which is easy to collect. In practice, semi-supervised learning requires less human effort and gives higher accuracy in creating a model. 4 As mentioned before, the corpus contains a low amount of error examples, which results in a high sparsity in the label distribution. In reality, the balance between the error and correct data is crucial for training a robust grammar detection models. Our experiment results demonstrate that too many correct data lead to unfavorable error detection rate. In order to resolv"
W13-3605,P10-2065,0,0.0621966,"l hybrid system is given in Section 5. The methods of grammatical error detection and correction are detailed in Section 6, followed by an evaluation, discussion and a conclusion to end the paper. 2 Related Work The issues of grammatical error correction have been discussed from different perspectives for several decades. In this section, we briefly review some related methods. The use of machine learning methods to tackle this problem has shown a promising performance. These methods are normally created based on a large corpus of well-formed native English texts (Tetreault and Chodorow 2008; Tetreault et al., 2010) or annotated non-native data (Gamon, 2010; Han et al., 2010). Although the manually error-tagged text is much more expensive, it has shown improvements over the models trained solely on well-formed native text (Kochmar et al., 2012). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy was generally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or correct the errors in spee"
W13-3605,C08-1109,0,0.0616285,"ised learning, and the overall hybrid system is given in Section 5. The methods of grammatical error detection and correction are detailed in Section 6, followed by an evaluation, discussion and a conclusion to end the paper. 2 Related Work The issues of grammatical error correction have been discussed from different perspectives for several decades. In this section, we briefly review some related methods. The use of machine learning methods to tackle this problem has shown a promising performance. These methods are normally created based on a large corpus of well-formed native English texts (Tetreault and Chodorow 2008; Tetreault et al., 2010) or annotated non-native data (Gamon, 2010; Han et al., 2010). Although the manually error-tagged text is much more expensive, it has shown improvements over the models trained solely on well-formed native text (Kochmar et al., 2012). Additionally, both generative and discriminative classifiers were widely used. Among them, Maximum Entropy was generally used (Rozovskaya and Roth, 2011; Sakaguchi et al., 2012; Quan et al., 2012) and obtained a good result for preposition and article correction using a large feature set. Naive Bayes 35 were also applied to recognize or c"
W13-3605,N03-5008,0,\N,Missing
W13-3605,N12-1067,0,\N,Missing
W13-3605,W12-2033,0,\N,Missing
W13-3605,W12-2006,0,\N,Missing
W13-3605,W13-3601,0,\N,Missing
W13-3605,W11-2838,0,\N,Missing
W13-3605,W06-3808,0,\N,Missing
W14-1711,J93-2004,0,0.0460517,"ons ing ted Stem constantli combin idea will result in better solut be formul Specific POS RB VBG NNS MD VB IN JJR NNS VBG VBN Figure 1: The factorized sentence. PoS: Part-of-Speech tags denote the morphosyntactic category of a word. The use of PoS sequences enables us to some extent to recover missing determiners, articles, prepositions, as well as the modal verb in a sentence. Empirical studies (Yuan and Felice, 2013) have demonstrated that the use of this information can greatly improve the accuracy of the grammatical error correction. To obtain the PoS, we adopt the Penn Treebank tag set (Marcus et al., 1993), which contains 45 PoS tags. The Stanford parser (Klein and Manning, 2002) is used to extract the PoS information. Inspired by Yuan and Felice (2013), who used preposition-specific tags to fix the problem of being unable to distinguish between prepositions and obtained good performance, we create specific tags both for determiners (i.e., a, an, the) and prepositions. Table 1 provides an example of this modification, where prepositions, TO and IN, and determiner, After decoding, we will de-truecase all these words. 85 DT, are revised to TO_to, IN_of and DT_the, respectively. 2.3 with the word"
W14-1711,I11-1017,0,0.171203,"Missing"
W14-1711,W14-1701,0,0.0182639,"ng & Portuguese-Chinese Machine Translation Laboratory, Department of Computer and Information Science, University of Macau, Macau S.A.R., China {wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo correction of grammatical errors is an active research topic, aiming at improving the writing process with the help of artificial intelligent techniques. Second language learning is a user group of particular interest. Recently, Helping Our Own (HOO) and CoNLL held a number of shared tasks on this topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 2014). Previous studies based on rules (Sidorov et al., 2013), data-driven methods (Berend et al., 2013, Yi et al., 2013) and hybrid methods (Putra and Szabó, 2013, Xing et al., 2013) have shown substantial gains for some frequent error types over baseline methods. Most proposed methods share the commonality that a sub-model is built for a specific type of error, on top of which a strategy is applied to combine a number of these individual models. Also, detection and correction are often split into two steps. For example, Xing et al. (2013) presented the UM-Checker for five error types in the CoNLL"
W14-1711,W13-3601,0,0.0450902,"Language Processing & Portuguese-Chinese Machine Translation Laboratory, Department of Computer and Information Science, University of Macau, Macau S.A.R., China {wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo correction of grammatical errors is an active research topic, aiming at improving the writing process with the help of artificial intelligent techniques. Second language learning is a user group of particular interest. Recently, Helping Our Own (HOO) and CoNLL held a number of shared tasks on this topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 2014). Previous studies based on rules (Sidorov et al., 2013), data-driven methods (Berend et al., 2013, Yi et al., 2013) and hybrid methods (Putra and Szabó, 2013, Xing et al., 2013) have shown substantial gains for some frequent error types over baseline methods. Most proposed methods share the commonality that a sub-model is built for a specific type of error, on top of which a strategy is applied to combine a number of these individual models. Also, detection and correction are often split into two steps. For example, Xing et al. (2013) presented the UM-Checker for five error"
W14-1711,J03-1002,0,0.00694738,"a number of sentence pairs from the parallel corpus as a development set and a test set, disjoint from the training data. Table 2 summarizes the statistics of all the datasets. Corpus Parallel Corpus Additional Monolingual Dev. Set Test Set Sentences 55,503 Tokens 1,124,521 / 1,114,040 85,254,788 2,033,096,800 500 900 10,532 / 10,438 18,032 / 17,906 Table 2: Statistics of used corpora. The experiments were carried out with MOSES 1.0 4 (Philipp Koehn et al., 2007). The translation and the re-ordering model utilizes the “grow-diag-final” symmetrized word-to-word alignments created with GIZA++5 (Och and Ney, 2003) and the training scripts of MOSES. A 5gram LM was trained using the SRILM toolkit6 (Stolcke et al., 2002), exploiting the improved modified Kneser-Ney smoothing (Kneser and Ney, 1995), and quantizing both probabilities and back-off weights. For the log-linear model training, we take minimum-error-rate training (MERT) method as described in (Och, 2003). The result is evaluated by M2 Scorer (Dahlmeier and Ng, 2012) computing precision, recall and F0.5. 3 http://www.statmt.org/wmt14/translation-task.html. http://www.statmt.org/moses/. 5 http://code.google.com/p/giza-pp/. 6 http://www.speech.sri."
W14-1711,W13-3613,0,0.0162211,"ory, Department of Computer and Information Science, University of Macau, Macau S.A.R., China {wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo correction of grammatical errors is an active research topic, aiming at improving the writing process with the help of artificial intelligent techniques. Second language learning is a user group of particular interest. Recently, Helping Our Own (HOO) and CoNLL held a number of shared tasks on this topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 2014). Previous studies based on rules (Sidorov et al., 2013), data-driven methods (Berend et al., 2013, Yi et al., 2013) and hybrid methods (Putra and Szabó, 2013, Xing et al., 2013) have shown substantial gains for some frequent error types over baseline methods. Most proposed methods share the commonality that a sub-model is built for a specific type of error, on top of which a strategy is applied to combine a number of these individual models. Also, detection and correction are often split into two steps. For example, Xing et al. (2013) presented the UM-Checker for five error types in the CoNLL 2013 shared task. The system implements a cascade of fi"
W14-1711,W13-3605,1,0.895589,"Missing"
W14-1711,P06-1032,0,0.430644,"Missing"
W14-1711,W13-1703,0,0.0471889,"e do not de-duplicate sentences. In all cases, the testing process is carried out as follows. The test set is translated by the first translation model, . The output from the first model is then fed into the second translation model, . The output of the second model is used as the final corrections. The second combination approach is to make use of multiple factors for model construction. The question is whether multiple factors when used together may improve the correction results. In this setting we combine two factors together 3 3.1 Experiment Setup Dataset We pre-process the NUCLE corpus (Dahlmeier et al., 2013) as described in Section 2 for training different translation models. We use both the official golden sentences and additional WMT2014 English monolingual data3 to train an in-domain and a general-domain language model (LM), respectively. These language models are linearly interpolated in the decoding phase. We also randomly select a number of sentence pairs from the parallel corpus as a development set and a test set, disjoint from the training data. Table 2 summarizes the statistics of all the datasets. Corpus Parallel Corpus Additional Monolingual Dev. Set Test Set Sentences 55,503 Tokens 1"
W14-1711,W12-2006,0,0.0399988,"eng, Yi Lu Natural Language Processing & Portuguese-Chinese Machine Translation Laboratory, Department of Computer and Information Science, University of Macau, Macau S.A.R., China {wang2008499,vincentwang0229}@gmail.com,derekfw@umac.mo, lidiasc@umac.mo,nlp2ct.samuel@gmail.com,mb25435@umac.mo correction of grammatical errors is an active research topic, aiming at improving the writing process with the help of artificial intelligent techniques. Second language learning is a user group of particular interest. Recently, Helping Our Own (HOO) and CoNLL held a number of shared tasks on this topic (Dale et al., 2012, Ng et al., 2013, Ng et al., 2014). Previous studies based on rules (Sidorov et al., 2013), data-driven methods (Berend et al., 2013, Yi et al., 2013) and hybrid methods (Putra and Szabó, 2013, Xing et al., 2013) have shown substantial gains for some frequent error types over baseline methods. Most proposed methods share the commonality that a sub-model is built for a specific type of error, on top of which a strategy is applied to combine a number of these individual models. Also, detection and correction are often split into two steps. For example, Xing et al. (2013) presented the UM-Checke"
W14-1711,D07-1091,0,0.124464,"Missing"
W14-1711,P07-2045,0,0.0137822,"and a general-domain language model (LM), respectively. These language models are linearly interpolated in the decoding phase. We also randomly select a number of sentence pairs from the parallel corpus as a development set and a test set, disjoint from the training data. Table 2 summarizes the statistics of all the datasets. Corpus Parallel Corpus Additional Monolingual Dev. Set Test Set Sentences 55,503 Tokens 1,124,521 / 1,114,040 85,254,788 2,033,096,800 500 900 10,532 / 10,438 18,032 / 17,906 Table 2: Statistics of used corpora. The experiments were carried out with MOSES 1.0 4 (Philipp Koehn et al., 2007). The translation and the re-ordering model utilizes the “grow-diag-final” symmetrized word-to-word alignments created with GIZA++5 (Och and Ney, 2003) and the training scripts of MOSES. A 5gram LM was trained using the SRILM toolkit6 (Stolcke et al., 2002), exploiting the improved modified Kneser-Ney smoothing (Kneser and Ney, 1995), and quantizing both probabilities and back-off weights. For the log-linear model training, we take minimum-error-rate training (MERT) method as described in (Och, 2003). The result is evaluated by M2 Scorer (Dahlmeier and Ng, 2012) computing precision, recall and"
W14-1711,W13-3607,0,0.17258,"of errors present in an essay are to be detected and corrected (i.e., there is no restriction on the five error types of the 2013 shared task); 2) the official evaluation metric of this year adopts F0.5, weighting precision twice as much as recall. This requires us to explore an alternative universal joint model that can tackle various kinds of grammatical errors as well as join the detection and correction processes together. Regarding grammatical error correction as a process of translation has been shown to be effective (Ehsan and Faili, 2013, Mizumoto et al., 2011, Yoshimoto et al., 2013, Yuan and Felice, 2013). We treat the problematic sentences and golden sentences as pairs of source and target sentences. In SMT, a translation model is trained on a parallel corpus that consists of the source sentences (i.e. sentences that may contain grammatical errors) and the targeted translations (i.e. the grammatically well-formed sentences). The challenge is that we need a large amount of these parallel sentences for constructing such a data-driven SMT system. Some researches (Brockett et al., 2006, Yuan and Felice, 2013) explore generating artificial errors to resolve this sparsity problem. Other studies (Eh"
W14-1711,W13-1700,0,\N,Missing
W14-1711,W13-3608,0,\N,Missing
W14-1711,W13-3604,0,\N,Missing
W14-1711,P03-1021,0,\N,Missing
W14-3328,D11-1033,0,0.0229872,"s: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top N percentage of ranked sentences are considered as additional relevant in-domain data. ( ) 4 We built our baseline system on an optimized level. It is trained on all official in-domain training corpora and a portion of general-domain data. We apply the Moore-Lewis method (Moore and Lewis, 2010) and modified Moore-Lewis method (Axelrod et al., 2011) for selecting in-domain data from the general-domain monolingual and parallel corpora, respectively. The top M percentages of ranked sentences are selected as a pseudo in-domain data to train an additional LM and TM. For LM, we linearly interpolate the additional LM with in-domain LM. For TM, the additional model is log-linearly interpolated with the in-domain model using the multi-decoding method described in (Koehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of baseline system. ( ) (2) 5 where is a input sentence or"
W14-3328,P13-2119,0,0.0169687,"ecific language and translation models. We explore and propose two filtering approaches for this task. The first one is to filter the documents based on their relative score, Eq. (1). We rank all the documents according to their relative scores and select top K percentage of entire collection for further processing. Second, we use a combination method, which takes both the perplexity and relative score into account for the selection. Perplexity-based data selection has shown to be a powerful mean on SMT domain adaptation (Wang et al., 2013; Wang et al., 2014; Toral, 2013; Rubino et al., 2013; Duh et al., 2013). The combination method is carried out as follows: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top N percentage of ranked sentences are considered as additional relevant in-domain data. ( ) 4 We built our baseline system on an optimized level. It is trained on all official in-domain training corpora and a portion of general-domain data. We apply the Moore-Lewis method (Moore a"
W14-3328,2012.eamt-1.38,0,0.0368196,"Missing"
W14-3328,W11-2123,0,0.0131162,"In our experiments, we not only use all available training data provided by the WMT2014 standard translation task 1 (generaldomain data) and medical translation task 2 (indomain data), but also acquire addition indomain bilingual translations (i.e. dictionary) and monolingual data from online sources. First of all, we collect the medical terminologies from the web. This tiny but significant parallel data are helpful to reduce the out-ofvocabulary words (OOVs) in translation models. In addition, the use of larger language models during decoding is aided by more efficient storage and inference (Heafield, 2011). Thus, we crawl more in-domain monolingual data from the Internet based on domain focused web-crawling approach. In order to detect and remove outdomain data from the crawled data, we not only explore text-to-topic classifier, but also propose an alternative filtering approach combined the existing one (text-to-topic classifier) with perplexity. After carefully pre-processing all the available training data, we apply language model adaptation and translation model adaptation using various kinds of training corpora. Experimental results show that the presented approaches are helpful to further"
W14-3328,P02-1040,0,0.0883673,"Missing"
W14-3328,P07-2045,0,0.0104477,"l text. In addition to these general data filtering steps, we introduce some extra steps to pre-process the training data. The first step is to remove the duplicate sentences. In data-driven methods, the more frequent a term occurs, the higher probabil8 Baseline System http://www.statmt.org/moses/?n=Moses.Baseline. Experiments and Results The official medical summary development sets (dev) are used for tuning and evaluating the comparative systems. The official medical summary test sets (test) are only used in our final submitted systems. The experiments were carried out with the Moses 1.010 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++11 (Och and Ney, 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was trained using the SRILM toolkit12 (Stolcke et al., 2002), exploiting improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. For the log-linear model training, we take the minimum-error-rate training (MERT) method as described in (Och, 2003). 9 http://www.nactem.ac.uk/y-matsu/geniass/. http://www.statmt.org/moses/. 11 http://www.ky"
W14-3328,W07-0733,0,0.0310125,"official in-domain training corpora and a portion of general-domain data. We apply the Moore-Lewis method (Moore and Lewis, 2010) and modified Moore-Lewis method (Axelrod et al., 2011) for selecting in-domain data from the general-domain monolingual and parallel corpora, respectively. The top M percentages of ranked sentences are selected as a pseudo in-domain data to train an additional LM and TM. For LM, we linearly interpolate the additional LM with in-domain LM. For TM, the additional model is log-linearly interpolated with the in-domain model using the multi-decoding method described in (Koehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of baseline system. ( ) (2) 5 where is a input sentence or document, ( ) is the probability of -gram segments estimated from the training set. is the number of tokens of an input string. 3 Pre-processing Both official training data and web-crawled resources are processed using the Moses scripts8, this includes the text tokenization, truecasing and length cleaning. For trusecasing, we use both the target side of parallel corpora and monolingual data to train the trucase models. We consider the tar"
W14-3328,P10-2041,0,0.021345,", 2013). The combination method is carried out as follows: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top N percentage of ranked sentences are considered as additional relevant in-domain data. ( ) 4 We built our baseline system on an optimized level. It is trained on all official in-domain training corpora and a portion of general-domain data. We apply the Moore-Lewis method (Moore and Lewis, 2010) and modified Moore-Lewis method (Axelrod et al., 2011) for selecting in-domain data from the general-domain monolingual and parallel corpora, respectively. The top M percentages of ranked sentences are selected as a pseudo in-domain data to train an additional LM and TM. For LM, we linearly interpolate the additional LM with in-domain LM. For TM, the additional model is log-linearly interpolated with the in-domain model using the multi-decoding method described in (Koehn and Schroeder, 2007). Finally, LM adaptation and TM adaptation are combined to further improve the translation quality of b"
W14-3328,P03-1021,0,0.0125664,"ur final submitted systems. The experiments were carried out with the Moses 1.010 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++11 (Och and Ney, 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was trained using the SRILM toolkit12 (Stolcke et al., 2002), exploiting improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. For the log-linear model training, we take the minimum-error-rate training (MERT) method as described in (Och, 2003). 9 http://www.nactem.ac.uk/y-matsu/geniass/. http://www.statmt.org/moses/. 11 http://www.kyloo.net/software/doku.php/mgiza:overview. 12 http://www.speech.sri.com/projects/srilm/. 10 235 Data Set In-domain Parallel Data Lang. Sent. cs/en 1,770,421 de/en 3,894,099 fr/en 4,579,533 cs/en 12,426,374 Generaldomain Parallel Data de/en 4,421,961 fr/en 36,342,530 In-domain Mono. Data Generaldomain Mono. Data Web-crawled In-domain Mono. Data cs fr de en cs fr de en en cs de fr 106,548 1,424,539 2,222,502 7,802,610 33,408,340 30,850,165 84,633,641 85,254,788 8,448,566 44,198 473,171 852,036 Words 9,373,"
W14-3328,J03-1002,0,0.00467779,"e duplicate sentences. In data-driven methods, the more frequent a term occurs, the higher probabil8 Baseline System http://www.statmt.org/moses/?n=Moses.Baseline. Experiments and Results The official medical summary development sets (dev) are used for tuning and evaluating the comparative systems. The official medical summary test sets (test) are only used in our final submitted systems. The experiments were carried out with the Moses 1.010 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++11 (Och and Ney, 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was trained using the SRILM toolkit12 (Stolcke et al., 2002), exploiting improved modified Kneser-Ney smoothing, and quantizing both probabilities and back-off weights. For the log-linear model training, we take the minimum-error-rate training (MERT) method as described in (Och, 2003). 9 http://www.nactem.ac.uk/y-matsu/geniass/. http://www.statmt.org/moses/. 11 http://www.kyloo.net/software/doku.php/mgiza:overview. 12 http://www.speech.sri.com/projects/srilm/. 10 235 Data Set In-domain Parallel Data Lang. Sent. cs/en 1,770"
W14-3328,W13-2803,0,0.0114563,"ta, which would harm the domain-specific language and translation models. We explore and propose two filtering approaches for this task. The first one is to filter the documents based on their relative score, Eq. (1). We rank all the documents according to their relative scores and select top K percentage of entire collection for further processing. Second, we use a combination method, which takes both the perplexity and relative score into account for the selection. Perplexity-based data selection has shown to be a powerful mean on SMT domain adaptation (Wang et al., 2013; Wang et al., 2014; Toral, 2013; Rubino et al., 2013; Duh et al., 2013). The combination method is carried out as follows: we first retrieve the documents based on their relative scores. The documents are then split into sentences, and ranked according to their perplexity using Eq. (2) (Stolcke et al., 2002). The used language model is trained on the official in-domain data. Finally, top N percentage of ranked sentences are considered as additional relevant in-domain data. ( ) 4 We built our baseline system on an optimized level. It is trained on all official in-domain training corpora and a portion of general-domain data."
W14-3328,J03-3002,0,\N,Missing
W14-3328,2011.eamt-1.40,0,\N,Missing
W14-3328,W13-2227,0,\N,Missing
W14-3328,W08-0509,0,\N,Missing
W14-3331,D11-1033,0,0.0286066,"ile 5. Translation Model Adaptation the LM adapted systems are listed with values relative to the Baseline2. The results indicate that As shown in Table 2, general-domain parallel LM adaptation can gain a reasonable improvecorpora are around 1 to 7 times larger than the ment if the LMs are trained on more relevant in-domain ones. We suspect if general-domain data for each pair, instead of using the whole corpus is broad enough to cover some in-domain training data. For different systems, their BLEU sentences. To observe the domain-specificity of general-domain corpus, we firstly evaluate sys8 Axelrod et al. (2011) names the selected data as pseudo intems trained on general-domain corpora. In Tadomain data. We adopt both terminologies in this paper. 256 ble 4, we show the BLEU scores of generaldomain systems9 on translating the medical sentences. The BLEU scores of the compared systems are relative to the Baseline2 and the size of the used general-domain corpus is relative to the corresponding in-domain one. For en-cs, cs-en, en-fr and fr-en pairs, the general-domain parallel corpora we used are 6 times larger than the original ones and we obtain the improved BLEU scores by 1.72 up to 3.96 points. While"
W14-3331,P13-2119,0,0.0215483,"49 1,709,594 25.56 7,802,610 cs 33,408,340 567,174,266 3,431,946 16.98 fr 30,850,165 780,965,861 2,142,470 25.31 General-domain Mono. Data de 84,633,641 1,548,187,668 10,726,992 18.29 en 85,254,788 2,033,096,800 4,488,816 23.85 Table 2: Statistics summary of corpora after pre-processing. only use the in-domain training data, but also the scores peak at different values of N. It gives the selected pseudo in-domain data 8 from generalbest results for cs-en, en-fr and de-en pairs when domain corpus to enhance the LMs (Toral, 2013; N=25, en-cs and en-de pairs when N=50, and frRubino et al., 2013; Duh et al., 2013). Firstly, en pair when N=75. Among them, en-cs and eneach sentence s in general-domain monolingual fr achieve the highest BLEU scores. The reason corpus is scored using the cross-entropy differis that their original monolingual (in-domain) ence method in (Moore and Lewis, 2010), which data for training the LMs are not sufficient. is calculated as follows: When introducing the extra pseudo in-domain data, the systems improve the translation quality score(s)  H I ( s)  HG ( s) (1) by around 2 BLEU points. While for cs-en, fr-en and de-en pairs, the gains are small. However, it where H(s) is t"
W14-3331,W08-0509,0,0.0456129,"Missing"
W14-3331,W11-2123,0,0.011601,"t comparisons with the proposed systems described in Section 4, 5, 6 and 7. 5 7 6 http://www.speech.sri.com/projects/srilm/. http://www.nactem.ac.uk/y-matsu/geniass/. Lang. Pair Baseline1 Baseline2 Diff. en-cs 12.92 +4.65 17.57 cs-en 20.85 +10.44 31.29 en-fr 38.31 +0.05 38.36 fr-en 44.27 +0.09 44.36 en-de 17.81 +0.20 18.01 de-en 32.34 +0.16 32.50 Table 1: BLEU scores of two baseline systems trained on original and processed corpora for different language pairs. 4. Language Model Adaptation The use of LMs (trained on large data) during decoding is aided by more efficient storage and inference (Heafield, 2011). Therefore, we not Data are processed according to Moses baseline tutorial: http://www.statmt.org/moses/?n=Moses.Baseline. 255 Data Set Lang. Sent. Words Vocab. Ave. Len. 9,373,482/ 134,998/ 5.29/ cs/en 1,770,421 10,605,222 156,402 5.99 In-domain 52,211,730/ 1,146,262/ 13.41/ de/en 3,894,099 Parallel Data 58,544,608 487,850 15.03 77,866,237/ 495,856/ 17.00/ fr/en 4,579,533 68,429,649 556,587 14.94 180,349,215/ 1,614,023/ 14.51/ cs/en 12,426,374 183,841,805 1,661,830 14.79 General-domain 106,001,775/ 1,912,953/ 23.97/ de/en 4,421,961 Parallel Data 112,294,414 919,046 25.39 1,131,027,766/ 3,149"
W14-3331,W13-2803,0,0.0112352,"4 37.79 In-domain Mono. Data de 2,222,502 53,840,304 1,415,202 24.23 en 199430649 1,709,594 25.56 7,802,610 cs 33,408,340 567,174,266 3,431,946 16.98 fr 30,850,165 780,965,861 2,142,470 25.31 General-domain Mono. Data de 84,633,641 1,548,187,668 10,726,992 18.29 en 85,254,788 2,033,096,800 4,488,816 23.85 Table 2: Statistics summary of corpora after pre-processing. only use the in-domain training data, but also the scores peak at different values of N. It gives the selected pseudo in-domain data 8 from generalbest results for cs-en, en-fr and de-en pairs when domain corpus to enhance the LMs (Toral, 2013; N=25, en-cs and en-de pairs when N=50, and frRubino et al., 2013; Duh et al., 2013). Firstly, en pair when N=75. Among them, en-cs and eneach sentence s in general-domain monolingual fr achieve the highest BLEU scores. The reason corpus is scored using the cross-entropy differis that their original monolingual (in-domain) ence method in (Moore and Lewis, 2010), which data for training the LMs are not sufficient. is calculated as follows: When introducing the extra pseudo in-domain data, the systems improve the translation quality score(s)  H I ( s)  HG ( s) (1) by around 2 BLEU points. Whi"
W14-3331,J03-1002,0,0.0205709,"Missing"
W14-3331,P03-1021,0,0.0280254,"Missing"
W14-3331,W07-0733,0,0.075072,"Missing"
W14-3331,P07-2045,0,0.0125756,"detailed in Section 2 & 3), the results of each method show reasonable gains. We combine individual approach to further improve the performance of our Experimental Setup All available training data from both WMT2014 standard translation task 1 (general-domain data) and medical translation task 2 (in-domain data) are used in this study. The official medical summary development sets (dev) are used for tuning and evaluating all the comparative systems. The official medical summary test sets (test) are only used in our final submitted systems. The experiments were carried out with the Moses 1.03 (Koehn et al., 2007). The translation and the re-ordering model utilizes the “growdiag-final” symmetrized word-to-word alignments created with MGIZA++ 4 (Och and Ney, 1 http://www.statmt.org/wmt14/translation-task.html. http://www.statmt.org/wmt14/medical-task/. 3 http://www.statmt.org/moses/. 4 http://www.kyloo.net/software/doku.php/mgiza:overview. 2 254 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 254–259, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics 2003; Gao and Vogel, 2008) and the training scripts from Moses. A 5-gram LM was train"
W14-3331,P10-2041,0,0.0194551,"re-processing. only use the in-domain training data, but also the scores peak at different values of N. It gives the selected pseudo in-domain data 8 from generalbest results for cs-en, en-fr and de-en pairs when domain corpus to enhance the LMs (Toral, 2013; N=25, en-cs and en-de pairs when N=50, and frRubino et al., 2013; Duh et al., 2013). Firstly, en pair when N=75. Among them, en-cs and eneach sentence s in general-domain monolingual fr achieve the highest BLEU scores. The reason corpus is scored using the cross-entropy differis that their original monolingual (in-domain) ence method in (Moore and Lewis, 2010), which data for training the LMs are not sufficient. is calculated as follows: When introducing the extra pseudo in-domain data, the systems improve the translation quality score(s)  H I ( s)  HG ( s) (1) by around 2 BLEU points. While for cs-en, fr-en and de-en pairs, the gains are small. However, it where H(s) is the length-normalized crosscan still achieve a significant improvement of entropy. I and G are the in-domain and general0.60 up to 1.12 BLEU points. domain corpora, respectively. G is a random subset (same size as the I) of the general-domain Lang. N=0 N=25 N=50 N=75 N=100 corpus"
W14-3331,W13-2227,0,\N,Missing
W15-3103,N13-1006,0,0.0727381,"Missing"
W15-3103,P10-1149,0,0.169849,"g to an unknown joint distribution ??(??, ??), and the unlabeled samples are independently and identically distributed from distribution ??(??). Many semi-supervised learning models are designed through some assumptions relating ??(??) to the conditional distribution, which cover EM method, Bayesian network, etc. (Zhu, 2008). The graph-based semi-supervised learning (GBSSL) methods have been successfully employed by many researchers. For instance, Goldberg and Zhu (2006) design the GBSSL model for sentiment categorization; Celikyilmaz et al. (2009) propose a GBSSL model for questionanswering; Talukdar and Pereira (2010) use the GBSSL methods for class-Instance acquisition; Subramanya et al. (2010) utilize the GBSSL model for structured tagging models; Zeng et al., (2013) use the GBSSL method for the joint Chinese word segmentation and part of speech (POS) tagging and result in higher performances as compared with previous works. However, as far as we know, the GBSSL method has not been employed into the CNER task. To testify the effectiveness of the GBSSL model in the traditional CNER task, this paper utilizes some unlabeled data to enhance the CRF learning through GBSSL method. 3.1 Graph Construction & Labe"
W15-3103,W06-0115,0,0.203403,"vised Learning Model Aaron Li-Feng Han* Xiaodong Zeng+ Derek F. Wong+ Lidia S. Chao+ * Institute for Logic, Language and Computation, University of Amsterdam Science Park 107, 1098 XG Amsterdam, The Netherlands + NLP2CT Laboratory/Department of Computer and Information Science University of Macau, Macau S.A.R., China l.han@uva.nl nlp2ct.samuel@gmail.com derekfw@umac.mo lidiasc@umac.mo Sang and De Meulder, 2003) NER shared tasks consist of persons, locations, organizations and names of miscellaneous entities, and the languages span from Spanish, Dutch, English, to German. The SIGHAN bakeoff-3 (Levow, 2006) and bakeoff-4 (Jin and Chen, 2008) tasks offer standard Chinese NER (CNER) corpora for training and testing, which contain the three commonly used entities, i.e., personal names, location names, and organization names. The CNER task is generally more difficult than the western languages due to the lack of word boundary information in Chinese expression. Traditional methods used for the entity recognition tend to employ external annotated corpora to enhance the machine learning stage, and improve the testing scores using the enhanced models (Zhang et al., 2006; Mao et al., 2008; Yu et al., 200"
W15-3103,W02-2024,0,0.0581058,"ction Named entity recognition (NER) can be regarded as a sub-task of the information extraction, and plays an important role in the natural language processing literature. The NER challenge has attracted a lot of researchers from NLP, and some successful NER tasks have been held in the past years. The annotations in MUC-7 1 Named Entity tasks (Marsh and Perzanowski, 1998) consist of entities (organization, person, and location), times and quantities such as monetary values and percentages, etc. among the languages of English, Chinese and Japanese. The entity categories in CONLL-02 (Tjong Kim Sang, 2002) and CONLL-03 (Tjong Kim 1 http://wwwnlpir.nist.gov/related_projects/muc/proceedings/ ne_task.html 15 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 15–20, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing the output labels should also be similar (Zhu, 2005). There are mainly three stages in the designed models, i.e., graph construction, label propagation and CRF learning. Graph construction is performed on both labeled and unlabeled data, and the unlabeled data is a"
W15-3103,W09-2208,0,0.020706,"are unseen in the training corpora. In the future work, we plan to give a detailed analysis of the GBSSL model performance on the OOV words for CNER tasks. 5. Related Work This work was supported by the Research Committee of the University of Macau (Grant No. MYRG2015-00175-FST and MYRG201500188-FST) and the Science and Technology Development Fund of Macau (Grant No. 057/2014/A). The first author was supported by NWO VICI under Grant No. 277-89-002. Nadeau (2007) employs the semi-supervised learning method to recognize 100 entity types on English documents with little supervision. Similarly, Liao and Veeramachaneni (2009) propose a simple semi-supervised algorithm for English entity recognition. Liu et al. (2011) design an interesting application of the semi-supervised learning model for online tweets document for English NER. Pham et al. (2012) use semi-supervised learning method of CRFs into the Vietnamese NER task with generalized expectation criteria. Similarly, Vo and Ock (2012) utilize a hybrid approach semi-supervised learning approach into the NER task for Vietnamese document. Wang et al. (2013) and Che et al. (2013) recently propose the usage of bilingual constraints to enhance the NER accuracy. Some"
W15-3103,P11-1037,0,0.0839281,"Missing"
W15-3103,I08-4010,0,0.152645,"i-Feng Han* Xiaodong Zeng+ Derek F. Wong+ Lidia S. Chao+ * Institute for Logic, Language and Computation, University of Amsterdam Science Park 107, 1098 XG Amsterdam, The Netherlands + NLP2CT Laboratory/Department of Computer and Information Science University of Macau, Macau S.A.R., China l.han@uva.nl nlp2ct.samuel@gmail.com derekfw@umac.mo lidiasc@umac.mo Sang and De Meulder, 2003) NER shared tasks consist of persons, locations, organizations and names of miscellaneous entities, and the languages span from Spanish, Dutch, English, to German. The SIGHAN bakeoff-3 (Levow, 2006) and bakeoff-4 (Jin and Chen, 2008) tasks offer standard Chinese NER (CNER) corpora for training and testing, which contain the three commonly used entities, i.e., personal names, location names, and organization names. The CNER task is generally more difficult than the western languages due to the lack of word boundary information in Chinese expression. Traditional methods used for the entity recognition tend to employ external annotated corpora to enhance the machine learning stage, and improve the testing scores using the enhanced models (Zhang et al., 2006; Mao et al., 2008; Yu et al., 2008). The conditional random filed (C"
W15-3103,M98-1002,0,0.0992725,"shows that the unlabeled corpus can enhance the state-of-theart conditional random field (CRF) learning model and has potential to improve the tagging accuracy even though the margin is a little weak and not satisfying in current experiments. 1. Introduction Named entity recognition (NER) can be regarded as a sub-task of the information extraction, and plays an important role in the natural language processing literature. The NER challenge has attracted a lot of researchers from NLP, and some successful NER tasks have been held in the past years. The annotations in MUC-7 1 Named Entity tasks (Marsh and Perzanowski, 1998) consist of entities (organization, person, and location), times and quantities such as monetary values and percentages, etc. among the languages of English, Chinese and Japanese. The entity categories in CONLL-02 (Tjong Kim Sang, 2002) and CONLL-03 (Tjong Kim 1 http://wwwnlpir.nist.gov/related_projects/muc/proceedings/ ne_task.html 15 Proceedings of the Eighth SIGHAN Workshop on Chinese Language Processing (SIGHAN-8), pages 15–20, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics and Asian Federation of Natural Language Processing the output labels should also"
W15-3103,P13-1076,1,0.780568,"sed learning models are designed through some assumptions relating ??(??) to the conditional distribution, which cover EM method, Bayesian network, etc. (Zhu, 2008). The graph-based semi-supervised learning (GBSSL) methods have been successfully employed by many researchers. For instance, Goldberg and Zhu (2006) design the GBSSL model for sentiment categorization; Celikyilmaz et al. (2009) propose a GBSSL model for questionanswering; Talukdar and Pereira (2010) use the GBSSL methods for class-Instance acquisition; Subramanya et al. (2010) utilize the GBSSL model for structured tagging models; Zeng et al., (2013) use the GBSSL method for the joint Chinese word segmentation and part of speech (POS) tagging and result in higher performances as compared with previous works. However, as far as we know, the GBSSL method has not been employed into the CNER task. To testify the effectiveness of the GBSSL model in the traditional CNER task, this paper utilizes some unlabeled data to enhance the CRF learning through GBSSL method. 3.1 Graph Construction & Label Propagation We follow the research of Subramanya et al. (2010) to represent the vertices using character trigrams in labeled and unlabeled sentences for"
W15-3103,W06-0126,0,0.0304881,"glish, to German. The SIGHAN bakeoff-3 (Levow, 2006) and bakeoff-4 (Jin and Chen, 2008) tasks offer standard Chinese NER (CNER) corpora for training and testing, which contain the three commonly used entities, i.e., personal names, location names, and organization names. The CNER task is generally more difficult than the western languages due to the lack of word boundary information in Chinese expression. Traditional methods used for the entity recognition tend to employ external annotated corpora to enhance the machine learning stage, and improve the testing scores using the enhanced models (Zhang et al., 2006; Mao et al., 2008; Yu et al., 2008). The conditional random filed (CRF) models have shown advantages and good performances in CNER tasks as compared with other machine learning algorithms (Zhou et al., 2006; Zhao and Kit, 2008), such as ME, HMM, etc. However, the annotated corpora are generally very expensive and time consuming. On the other hand, there are a lot of freely available unlabeled data in the internet that can be used for our researches. Due to this reason, some researchers begin to explore the usage of the unlabeled data and the semi-supervised learning methods based on labeled t"
W15-3103,D10-1017,0,0.0292594,"endently and identically distributed from distribution ??(??). Many semi-supervised learning models are designed through some assumptions relating ??(??) to the conditional distribution, which cover EM method, Bayesian network, etc. (Zhu, 2008). The graph-based semi-supervised learning (GBSSL) methods have been successfully employed by many researchers. For instance, Goldberg and Zhu (2006) design the GBSSL model for sentiment categorization; Celikyilmaz et al. (2009) propose a GBSSL model for questionanswering; Talukdar and Pereira (2010) use the GBSSL methods for class-Instance acquisition; Subramanya et al. (2010) utilize the GBSSL model for structured tagging models; Zeng et al., (2013) use the GBSSL method for the joint Chinese word segmentation and part of speech (POS) tagging and result in higher performances as compared with previous works. However, as far as we know, the GBSSL method has not been employed into the CNER task. To testify the effectiveness of the GBSSL model in the traditional CNER task, this paper utilizes some unlabeled data to enhance the CRF learning through GBSSL method. 3.1 Graph Construction & Label Propagation We follow the research of Subramanya et al. (2010) to represent t"
W15-3103,I08-4017,0,0.0262352,"mes, location names, and organization names. The CNER task is generally more difficult than the western languages due to the lack of word boundary information in Chinese expression. Traditional methods used for the entity recognition tend to employ external annotated corpora to enhance the machine learning stage, and improve the testing scores using the enhanced models (Zhang et al., 2006; Mao et al., 2008; Yu et al., 2008). The conditional random filed (CRF) models have shown advantages and good performances in CNER tasks as compared with other machine learning algorithms (Zhou et al., 2006; Zhao and Kit, 2008), such as ME, HMM, etc. However, the annotated corpora are generally very expensive and time consuming. On the other hand, there are a lot of freely available unlabeled data in the internet that can be used for our researches. Due to this reason, some researchers begin to explore the usage of the unlabeled data and the semi-supervised learning methods based on labeled training data and unlabeled external data have shown their advantages (Blum and Chawla, 2001; Shin et al., 2006; Zha et al., 2008; Zhang et al., 2013). Abstract Named entity recognition (NER) plays an important role in the NLP li"
W15-3103,W06-0140,0,\N,Missing
W15-3103,N12-1086,0,\N,Missing
W15-3103,P09-1081,0,\N,Missing
W15-3103,W03-0419,0,\N,Missing
W15-3103,I08-4013,0,\N,Missing
W15-3103,I08-4016,0,\N,Missing
W15-3103,I05-3017,0,\N,Missing
W15-3103,W06-3808,0,\N,Missing
Y13-1050,W08-0336,0,0.0699714,"Missing"
Y13-1050,P05-1022,0,0.156174,"Missing"
Y13-1050,J03-4003,0,0.0895741,"d vertex. distribution Integrating the similarity between every two vertices, we can project the most probable POS (selection from the ) tag to the unlabeled words. Through the construction of similarity graph and propagation of labels in this stage, each unlabeled word will get a POS tag. where , denote the estimation generated by our proposed OOV model and the Berkeley model respectively. 3.2 3.4 Generating Latent Tag and Emission Probability to Unlabeled Words s.t. Comparison with Recognition Models Other OOV The proposed approach in this paper differs from previous OOV recognition models. Collins (2003) assigned the UNKNOWN token to unknown ⁄ words, and any pairs not seen in training data would give a zero of estimation. While in (Klein and Manning, 2003), the unknown words were split into one of several word-class categories, based on capitalization, suffix, digit, and other character features. For each of these categories, they took the maximum-likelihood estimation of and add a parameter k to smooth and accommodate unknown words. In (Petrov et al., 2006), they mainly utilized the estimation of rare words to reflect the appearance likelihood of OOV words and the details of the method have"
Y13-1050,P11-1061,0,0.168568,"ure 1-3 in Algorithm 1), the common practice is to construct a similarity graph for the labeled data and unlabeled data, and aim at assigning a POS tag to unlabeled data in a vertex constructing and label propagation tradition. The effect of the label propagation depends heavily on the the quality of the graph. Thus graph construction plays a central role in graph-based label propagation (Zhu et al., 2003). In this stage, we represent vertices by all of the word trigrams with occurrences in labeled and unlabeled sentences to construct the first graph. The graph construction is non-trivial. As Das and Petrov (2011) mentioned that taking individual words as the vertices would result in various ambiguities and the similarity measurement is still challenging. Therefore, in this paper, we follow the same intuitions of graph construction from (Subramanya et al., 2010) by using trigram and the objective focuses on the center word in each vertex. Formally, we are given a set of labeled texts , and a set of unlabeled texts . The goal is to form an undirected weighted graph , in which as the set of vertices, which covers all trigrams extracted from and . Here , where refers to trigrams that occurs at least once"
Y13-1050,N12-1086,0,0.0864566,"leads to a number of graph construction algorithms in the past years. Popular graph construction methods include k-nearest neighbors (k-NN), eneighborhood, and local reconstruction. In this paper , the k-NN method is used to construct the graph. Besides, label propagation operates on the constructed graph. Its primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g. smoothness (Zhu et al., 2003; Subramanya and Bilmes, 2008; Talukdar and Crammer, 2009) or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdarand and Crammer, 2009) and Sparse Inducing Penalties (Das and Smith, 2012). The Sparse Inducing Penalties algorithm is used in this study. 476 3 The Proposed Approach The emphasis of this paper is on presenting a method to recognize Chinese unknown words by using two different kinds of data sources, e.g. labeled texts and unlabeled texts, to construct a specific similarity graph. In essence, th is problem can be treated as incorporating gainful information, e.g. pri"
Y13-1050,W10-1408,0,0.0356391,"Missing"
Y13-1050,D10-1017,0,0.166269,"l propagation depends heavily on the the quality of the graph. Thus graph construction plays a central role in graph-based label propagation (Zhu et al., 2003). In this stage, we represent vertices by all of the word trigrams with occurrences in labeled and unlabeled sentences to construct the first graph. The graph construction is non-trivial. As Das and Petrov (2011) mentioned that taking individual words as the vertices would result in various ambiguities and the similarity measurement is still challenging. Therefore, in this paper, we follow the same intuitions of graph construction from (Subramanya et al., 2010) by using trigram and the objective focuses on the center word in each vertex. Formally, we are given a set of labeled texts , and a set of unlabeled texts . The goal is to form an undirected weighted graph , in which as the set of vertices, which covers all trigrams extracted from and . Here , where refers to trigrams that occurs at least once in labeled data and refers to trigrams that occurs only in the unlabeled data. The edge . In our case, we make use of the knearest neighbors (k-NN) (k=5) method to construct the graph and the edge weights are measured by a symmetric similarity function"
Y13-1050,W12-6333,1,0.886827,"Missing"
Y13-1050,D09-1087,0,0.164782,"l, OOV words will be checked if they belong to temporal noun (NT)3, cardinal number (CD)4, ordinal number (OD)5 or proper noun (NR)6 preferentially. The Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) is an efficient and effective parser that introduces latent annotations to learn high accurate context-free grammars (CFG) directly from a Treebank. Nevertheless, the lexical model of grammar is not well designed to effectively handle the out-of-vocabulary (OOV) words (aka unknown words) universally and the OOV model of Berkeley parser has proved to be more suitable for English in (Huang and Harper, 2009; Attia et al., 2010). The built-in treatment to unseen words of Berkeley parser can be concluded as: utilizing the estimation of rare words 2 to reflect the appearance likelihood of OOV words. 2.2 Graph-based Label Propagation Graph-based label propagation, a critical subclass of semi-supervised learning (SSL), has 3 By checking if the word contains characters like “年” (year), “月” (month), or “日”“号”(day). 4 By checking if the word contains character of number. 5 By checking if the word contains character, such as “第”. 6 By checking if the word contains character, such as “·” 2 In the newest v"
Y13-1050,I11-1025,0,0.0263223,"Missing"
Y13-1050,C02-1145,0,0.0326873,"Missing"
Y13-1050,P13-1076,1,0.826043,"nknown word and use the geometric average to estimate the emission probability of it. However, in this paper, we focus on using a graph-based label 474 Copyright 2013 by Qiuping Huang, Derek F. Wong, Lidia S. Chao, Xiaodong Zeng, and Liangye He 27th Pacific Asia Conference on Language, Information, and Computation pages 474－482 PACLIC-27 propagation method to deal with unknown words. Graph-based label propagation methods have made a remarkable improvement in several natural language processing tasks, e.g. knowledge acquisition (Talukar et al., 2008), Chinese word segmentation and POS tagging (Zeng et al., 2013) and etc. As far as we known, this study is the first attempt at applying graphbased label propagation to resolve the problem of unknown word, which is mainly used to propagate POS tag and derive the emission probabilities to the large amount of unlabeled data by utilizing the limited resource (e.g. POS information from the labeled data, i.e. Penn Chinese Treebank and lexical emission probability learned by the PCFG-LA model). Then the derived unlabeled information generated by graph-based knowledge will be incorporated into the parser. In fact, this method explores a new way to exploit the us"
Y13-1050,D07-1117,0,0.129271,"the unknown words were split into one of several word-class categories, based on capitalization, suffix, digit, and other character features. For each of these categories, they took the maximum-likelihood estimation of and add a parameter k to smooth and accommodate unknown words. In (Petrov et al., 2006), they mainly utilized the estimation of rare words to reflect the appearance likelihood of OOV words and the details of the method have been mentioned in section 2.1. In fact, Chinese words are quite different from English, and the word formation processing for Chinese can be quite complex. Huang et al., (2007) reflected the fact that the characters in any position (prefix, infix, or suffix) can be predictive of the POS type for Chinese words. Inspired by their work, Huang and Harper (2009) improved Chinese unknown word parsing performance by using the geometric average of emission probabilities of all of the characters in the word. Differing from their concerns, we make use of a new perspective to employ unlabeled data to augment the supervised model and to handle the OOV word by graph-based semi-supervised learning. Our emphasis is to learn the semi-supervised model by smoothing the label distribu"
Y13-1050,P03-1054,0,0.0151736,"he unlabeled words. Through the construction of similarity graph and propagation of labels in this stage, each unlabeled word will get a POS tag. where , denote the estimation generated by our proposed OOV model and the Berkeley model respectively. 3.2 3.4 Generating Latent Tag and Emission Probability to Unlabeled Words s.t. Comparison with Recognition Models Other OOV The proposed approach in this paper differs from previous OOV recognition models. Collins (2003) assigned the UNKNOWN token to unknown ⁄ words, and any pairs not seen in training data would give a zero of estimation. While in (Klein and Manning, 2003), the unknown words were split into one of several word-class categories, based on capitalization, suffix, digit, and other character features. For each of these categories, they took the maximum-likelihood estimation of and add a parameter k to smooth and accommodate unknown words. In (Petrov et al., 2006), they mainly utilized the estimation of rare words to reflect the appearance likelihood of OOV words and the details of the method have been mentioned in section 2.1. In fact, Chinese words are quite different from English, and the word formation processing for Chinese can be quite complex."
Y13-1050,P03-1056,0,0.104354,"Missing"
Y13-1050,P06-1055,0,0.693064,"ploit the use of unlabeled data to strengthen the supervised model in parsing. This paper is structured as follows. Section 2 reviews the background, including the lexical model in the Berkeley PCFG-LA model and the graph-based label propagation methods. Section 3 presents the details of our proposed model based on graph-based semi-supervised learning approach and compares with other unknown word recognition models. Experiments setup and result analysis are reported in section 4. The last section draws the conclusion and future work. 2 2.1 In order to get the more refine and accurate grammar, Petrov et al., (2006) developed a simple split-merge-smooth training procedure. In order to counteract over-fitting problem, they introduced a linear smoothing method to smooth the lexical emission probabilities: ∑ (1) (2) where and denotes the number of latent tags from means a set of latent subcategories . In Equation (1), is the model parameters which can be optimized by EM-algorithm. In Equation (2), is a smoothing parameter. Since the lexical model can only generate words observed in the training data, a separate module is needed to handle the OOV words that appear in the test sentences. There are two ways to"
Y13-1050,D08-1114,0,0.0319282,"(Zhu et al., 2003). The great importance of graph construction methods leads to a number of graph construction algorithms in the past years. Popular graph construction methods include k-nearest neighbors (k-NN), eneighborhood, and local reconstruction. In this paper , the k-NN method is used to construct the graph. Besides, label propagation operates on the constructed graph. Its primary objective is to propagate labels from a few labeled vertices to the entire graph by optimizing a loss function based on the constraints or properties derived from the graph, e.g. smoothness (Zhu et al., 2003; Subramanya and Bilmes, 2008; Talukdar and Crammer, 2009) or sparsity (Das and Smith, 2012). State-of-the-art label propagation algorithms include LP-ZGL (Zhu et al., 2003), Adsorption (Baluja et al., 2008), MAD (Talukdarand and Crammer, 2009) and Sparse Inducing Penalties (Das and Smith, 2012). The Sparse Inducing Penalties algorithm is used in this study. 476 3 The Proposed Approach The emphasis of this paper is on presenting a method to recognize Chinese unknown words by using two different kinds of data sources, e.g. labeled texts and unlabeled texts, to construct a specific similarity graph. In essence, th is proble"
Y13-1050,N07-1051,0,\N,Missing
Y13-1050,A00-2018,0,\N,Missing
Y13-1050,W12-6337,1,\N,Missing
