1994.amta-1.20,P89-1018,0,0.0655957,"his ordering information is encoded in the grammar network by virtue of the relative ordering of integer id&apos;s associated with network links. Other types of parameters encoded in the grammar network are those pertaining to basic categories, pre-terminal categories (e.g., determiner), potential specifiers, and adjuncts for each basic category. 3.2. Trace Theory In general, NP and CP nodes are considered to be barriers to movement. However, Korean allows the head noun of a relative clause to be construed with the empty category across more than one intervening CP node, as shown in the following: (2) [CP [CP t1 t2 kyengyengha-ten] hoysa2-ka manghayperi-n] Bill1-un yocum uykisochimhay issta managed-Rel company-Norn is bankrupt-Rel -Top these days is depressed &apos;Bill is such a person that the company which was managed by him has been bankrupt, and he is depressed these days&apos; The subject NP &apos;Bill&apos; is coindexed with the trace in the more deeply embedded relative clause. If we assume, following Chomsky (1986a), that relative clause formation involves movement from an inner clause into an outer subject position, then the grammaticality of the above example suggests that the Trace theory must be"
1994.amta-1.20,P90-1017,1,0.888883,"Missing"
1994.amta-1.20,1993.tmi-1.14,0,0.0957369,"Missing"
1994.amta-1.20,P90-1015,0,0.0644115,"Missing"
1994.amta-1.20,P93-1016,1,0.872256,"Missing"
A00-2011,W99-0905,0,0.0281851,"Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-paralM bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in fiduciary duty include respons"
A00-2011,P98-2127,1,0.537109,"ibility, obligation, role, ... This list is then used to select a translation for duty. In the next section, we describe the resources required by our algorithm. In Section 3, we present an algorithm for constructing the contextually similar words of a word in a context. Section 4 presents the word-for-word glossing algorithm and Section 5 describes the group similarity metric used in our algorithm. In Section 6, we present some experimental results and finally, in Section 7, we conclude with a discussion of future work. 2. Resources The input to our algorithm includes a collocation database (Lin, 1998b) and a corpus-based thesaurus (Lin, 1998a), which are both available on the Interne0. In addition, we require a bilingual thesaurus. Below, we briefly describe these resources. 2.1. Collocation database Given a word w in a dependency relationship (such as subject or object), the collocation database can be used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies2. Figure 1 shows excerpts of the entries in the collocation database for the words corporate, duty, and fiduciary. The database contains a total of 11 million unique depend"
A00-2011,P99-1068,0,0.0119051,"1. Introduction Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-paralM bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in fiduciary duty i"
A00-2011,J90-2002,0,0.167527,"Missing"
A00-2011,P91-1022,0,0.110094,"Missing"
A00-2011,P91-1023,0,0.031315,"a bilingual thesaurus. 1. Introduction Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-paralM bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in f"
A00-2011,J93-1004,0,\N,Missing
A00-2011,C98-2122,1,\N,Missing
A97-2010,P96-1006,0,0.0463232,"Missing"
A97-2010,C92-2070,0,0.0841248,"Missing"
A97-2010,P95-1026,0,0.135162,"Missing"
A97-2010,P94-1020,0,\N,Missing
C02-1144,J98-1006,0,0.026788,"Missing"
C02-1144,P99-1005,0,0.0242603,"hemes have been proposed. They generally fall under two categories: • • comparing cluster outputs with manually generated answer keys (hereon referred to as classes); or embedding the clusters in an application and using its evaluation measure. An example of the first approach considers the average entropy of the clusters, which measures the purity of the clusters (Steinbach, Karypis, and Kumar 2000). However, maximum purity is trivially achieved when each element forms its own cluster. An example of the second approach evaluates the clusters by using them to smooth probability distributions (Lee and Pereira 1999). Like the entropy scheme, we assume that there is an answer key that defines how the elements are supposed to be clustered. Let C be a set of clusters and A be the answer key. We define the editing distance, dist(C, A), as the number of operations required to make C consistent with A. We say that C is consistent with A if there is a one to one mapping between clusters in C and the classes in A such that for each cluster c in C, all elements of c belong to the same class in A. We allow two editing operations: • • merge two clusters; and move an element from one cluster to another. A) C) a b e"
C02-1144,C94-1079,1,0.341631,"we first estimate the probability of a random word belonging to a subhierarchy (a synset and its hyponyms). We use the frequency counts of synsets in the SemCor corpus (Landes, Leacock, Tengi 1998) to estimate the probability of a subhierarchy. Since SemCor is a fairly small corpus, the frequency counts of the synsets in the lower part of the WordNet hierarchy are very sparse. We smooth the probabilities by assuming that all siblings are equally likely given the parent. A class is then defined as the maximal subhierarchy with probability less than a threshold (we used e-2). We used Minipar 1 (Lin 1994), a broadcoverage English parser, to parse about 1GB (144M words) of newspaper text from the TREC collection (1988 AP Newswire, 1989-90 LA Times, and 1991 San Jose Mercury) at a speed of about 500 words/second on a PIII-750 with 512MB memory. We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information values from Section 3. The test set is constructed by intersecting the words in WordNet with the nouns in the corpus whose total mutual information with all of its contexts exceeds a threshold m. Since W"
C02-1144,P98-2127,1,0.0689826,"y of partitional algorithms. Buckshot (Cutting, Karger, Pedersen, Tukey 1992) addresses the problem of randomly selecting initial centroids in K-means by combining it with average-link clustering. Buckshot first applies average-link to a random sample of n elements to generate K clusters. It then uses the centroids of the clusters as the initial K centroids of K-means clustering. The sample size counterbalances the quadratic running time of average-link to make Buckshot efficient: O(K×T×n + nlogn). The parameters K and T are usually considered to be small numbers. 3 Word Similarity Following (Lin 1998), we represent each word by a feature vector. Each feature corresponds to a context in which the word occurs. For example, “threaten with __” is a context. If the word handgun occurred in this context, the context is a feature of handgun. The value of the feature is the pointwise mutual information (Manning and Schütze 1999 p.178) between the feature and the word. Let c be a context and Fc(w) be the frequency count of a word w occurring in context c. The pointwise mutual information between c and w is defined as: miw,c = ∑ Fc ( w ) N Fi ( w ) i N × ∑ Fc ( j ) j N where N = ∑∑ F ( j ) is i i th"
C02-1144,C98-2122,1,\N,Missing
C04-1090,J93-2003,0,0.0033502,"racts a set of transfer rules and their probabilities from the training corpus. A rule translates a path in the source language dependency tree into a fragment in the target dependency tree. The problem of finding the most probable translation becomes a graph-theoretic problem of finding the minimum path covering of the source language dependency tree. 1 Introduction Given a source language sentence S, a statistical machine translation (SMT) model translates it by finding the target language sentence T such that the probability P(T|S) is maximized. In word-based models, such as IBM Model 1-5 (Brown et al 1993), the probability P(T|S) is decomposed into statistical parameters involving words. There have been many recent proposals to improve translation quality by decomposing P(T|S) into probabilities involving phrases. Phrase-based SMT approaches can be classified into two categories. One type of approach works with parse trees. In (Yamada&Knight 2001), for example, the translation model applies three operations (re-order, insert, and translate) to an English parse tree to produce its Chinese translation. A parallel corpus of English parse trees and Chinese sentences are used to obtain the probabili"
C04-1090,P03-1012,1,0.660658,"Missing"
C04-1090,W02-1039,0,0.106637,"ings are not shown in order not to clutter the diagrams. Connect both power cables to the controller Branchez les deux câbles d' alimentation sur le contrôleur 1 2 3 4 5 6 7 8 9 (c) (d) (e) both cables power cables Connect cables Connect to controller deux câbles câbles d' alimentation Branchez les câbles Branchez sur contrôleur (f) Connect cables to controller Branchez les câbles sur contrôleur (g) Connect to the controller Branchez sur le contrôleur Figure 2. Examples of transfer rules extracted from a word-aligned corpus Spans The rule extraction algorithm makes use of the notion of spans (Fox 2002, Lin&Cherry 2003). Given a word alignment and a node n in the source dependency tree, the spans of n induced by the word alignment are consecutive sequences of words in the target sentence. We define two types of spans: Head span: the word sequence aligned with the node n. Phrase span: the word sequence from the lower bound of the head spans of all nodes in the subtree rooted at n to the upper bound of the same set of spans. For example, the spans of the nodes in Fig. 2(a) are listed in Table 1. We used the word-alignment algorithm in (Lin&Cherry 2003a), which enforces a cohesion constraint t"
C04-1090,habash-dorr-2002-handling,0,0.0593311,"Missing"
C04-1090,P02-1050,0,0.039735,"Missing"
C04-1090,W02-1610,0,0.0542886,"e dependency tree is what a target language linguist would construct. For example, derived dependency tree for “X cruzar Y nadando” is shown in Fig. 6(b). Even though it is not a correct dependency tree for Spanish, it does generate the correct word order. Related Work and Discussions 6.1 Transfer-based MT Both our system and transfer-based MT systems take a parse tree in the source language and translate it into a parse tree in the target language with transfer rules. There have been many recent proposals to acquire transfer rules automatically from word-aligned corpus (Carbonell et al 2002, Lavoie et al 2002, Richardson et al 2001). There are two main differences between our system and previous transfer-based approach: the unit of transfer and the generation module. The units of transfer in previous transfer based approach are usually subtrees in the source 1 2 http://www.isi.edu/~koehn/europarl/ http://www.cs.ualberta.ca/~lindek/minipar.htm X swim across Y X cruzar Y nadando X cruzar Y nadando X cross Y swimming (b) (a) Figure 6. Translational Divergence 7 Conclusion and Future Work We proposed a path-based transfer model for machine translation, where the transfer rules are automatically acquir"
C04-1090,N03-2017,1,0.86976,"Missing"
C04-1090,W03-0302,1,0.836141,"Missing"
C04-1090,W02-1018,0,0.0266534,"Missing"
C04-1090,W99-0604,0,0.0253783,"ng phrases. Phrase-based SMT approaches can be classified into two categories. One type of approach works with parse trees. In (Yamada&Knight 2001), for example, the translation model applies three operations (re-order, insert, and translate) to an English parse tree to produce its Chinese translation. A parallel corpus of English parse trees and Chinese sentences are used to obtain the probabilities of the operations. In the second type of phrase-based SMT models, phrases are defined as a block in a word aligned corpus such that words within the block are aligned with words inside the block (Och et al 1999, Marcu&Wong 2002). This definition will treat as phrases many word sequences that are not constituents in parse trees. This may look linguistically counter-intuitive. However, (Koehn et al 2003) found that it is actually harmful to restrict phrases to constituents in parse trees, because the restriction would cause the system to miss many reliable translations, such as the correspondence between “there is” in English and “es gibt” (“it gives”) in German. In this paper, we present a path-based transfer model for machine translation. The model is trained with a word-aligned parallel corpus wher"
C04-1090,P02-1040,0,0.111274,"Missing"
C04-1090,2001.mtsummit-papers.53,0,0.0101393,"s what a target language linguist would construct. For example, derived dependency tree for “X cruzar Y nadando” is shown in Fig. 6(b). Even though it is not a correct dependency tree for Spanish, it does generate the correct word order. Related Work and Discussions 6.1 Transfer-based MT Both our system and transfer-based MT systems take a parse tree in the source language and translate it into a parse tree in the target language with transfer rules. There have been many recent proposals to acquire transfer rules automatically from word-aligned corpus (Carbonell et al 2002, Lavoie et al 2002, Richardson et al 2001). There are two main differences between our system and previous transfer-based approach: the unit of transfer and the generation module. The units of transfer in previous transfer based approach are usually subtrees in the source 1 2 http://www.isi.edu/~koehn/europarl/ http://www.cs.ualberta.ca/~lindek/minipar.htm X swim across Y X cruzar Y nadando X cruzar Y nadando X cross Y swimming (b) (a) Figure 6. Translational Divergence 7 Conclusion and Future Work We proposed a path-based transfer model for machine translation, where the transfer rules are automatically acquired from a word-aligned p"
C04-1090,P01-1067,0,0.264795,"Missing"
C04-1090,N03-1017,0,\N,Missing
C10-1100,J07-4002,0,0.0647416,"Missing"
C10-1100,P01-1005,0,0.017726,"nearly with the log of the number of types in the auxiliary data set. Google V1 is the one data source for which the relationship between accuracy and number of N-grams is not monotonic. After about 100 million unique N-grams, performance starts decreasing. This drop shows the need for Google V2. Since Google V1 contains duplicated web pages and sentences, mistakes that should be rare can appear to be quite frequent. Google V2, which comes from the same snapshot of the web as Google V1, but has only unique sentences, does not show this drop. We regard the results in Figure 2 as a companion to Banko and Brill (2001)’s work on exponentially increasing the amount of labeled training data. Here we see that varying the amount of 892 data. Performance improves log-linearly with the number of parameters (unique N-grams). One can increase performance with larger models, e.g., increasing the size of the unlabeled corpora, or by decreasing the frequency threshold. Alternatively, one can decrease storage costs with smaller models, e.g., decreasing the size of the unlabeled corpora, or by increasing the frequency threshold. Either way, the log-linear relationship between accuracy and model size makes it easy to est"
C10-1100,D08-1107,0,0.0524726,"Missing"
C10-1100,D07-1086,1,0.37949,"bracketing decision for “and television producers” should be different in each. 887 early mistakes can cascade and lead to a chain of incorrect bracketings. Our approach differs from previous work in NP parsing; rather than greedily inserting brackets as in Barker’s algorithm, we use dynamic programming to find the global maximum-scoring parse. In addition, unlike previous approaches that have used local features to make local decisions, we use the full NP to score each potential bracketing. A related line of research aims to segment longer phrases that are queried on Internet search engines (Bergsma and Wang, 2007; Guo et al., 2008; Tan and Peng, 2008). Bergsma and Wang (2007) focus on NP queries of length four or greater. They use supervised learning to make segmentation decisions, with features derived from the noun compound bracketing literature. Evaluating the benefits of parsing NP queries, rather than simply segmenting them, is a natural application of our system. 3 Annotated Data Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). The original PTB left a flat structure for base noun phrases. For example, “retired science teacher,” would be represented a"
C10-1100,D07-1090,0,0.00709778,"Google V2 NEWS 94 1e4 1e5 1e6 1e7 1e8 1e9 Number of Unique N-grams Acknowledgments Figure 2: There is no data like more data. Accuracy improves with the number of parameters (unique N-grams). This trend holds across three different sources of N-grams. unlabeled data can cause an equally predictable improvement in classification performance, without the cost of labeling data. Suzuki and Isozaki (2008) also found a loglinear relationship between unlabeled data (up to a billion words) and performance on three NLP tasks. We have shown that this trend continues well beyond Gigaword-sized corpora. Brants et al. (2007) also found that more unlabeled data (in the form of input to a language model) leads to improvements in BLEU scores for machine translation. Adding noun phrase parsing to the list of problems for which there is a “bigger is better” relationship between performance and unlabeled data shows the wide applicability of this principle. As both the amount of text on the web and the power of computer architecture continue to grow exponentially, collecting and exploiting web-scale auxiliary data in the form of N-gram corpora should allow us to achieve gains in performance linear in time, without any h"
C10-1100,P05-1022,0,0.0544015,"Missing"
C10-1100,J90-1003,1,0.0530333,"ut weighted differently, in each proposed bracketing’s feature set. 6.1 N-gram Features All of the features described in this section require estimates of the probability of specific words or sequences of words. All probabilities are computed using Google V2 (Section 4). 6.1.1 PMI Recall that the adjacency model for the threeword task uses the associations of the two pairs of adjacent words, while the dependency model uses the associations of the two pairs of attachment sites for the initial noun. We generalize the adjacency and dependency models by including the pointwise mutual information (Church and Hanks, 1990) between all pairs of words in the NP: PMI(x, y) = log p(“x y”) p(“x”)p(“y”) (1) For NPs of length n, for each proposed bracketing, we include separate features for the PMI be tween all n2 pairs of words in the NP. For NPs including conjunctions, we include additional PMI features (Section 6.1.2). Since these features are also tied to the proposed bracketing positions (as explained above), this allows us to learn relationships between various associations within the NP and each potential bracketing. For example, consider a proposed bracketing from word 4 to word 5. We learn that a high associ"
C10-1100,J82-3004,1,0.322975,"Missing"
C10-1100,A88-1019,1,0.64327,"Missing"
C10-1100,J05-1003,0,0.0448013,"Missing"
C10-1100,P08-1109,0,0.0084756,"Missing"
C10-1100,P08-1076,0,0.0795093,"Missing"
C10-1100,W04-3201,0,0.0673423,"Missing"
C10-1100,P07-1031,0,0.599259,"are therefore a major opportunity to improve overall base NP parsing. Since in the general case, NP parsing can no longer be thought of as a single binary classification problem, different strategies are required. Barker (1998) reduces the task of parsing longer NPs to making sequential three-word decisions, moving a sliding window along the NP. The window is first moved from right-to-left, inserting right bracketings, and then again from leftto-right, finalizing left bracketings. While Barker (1998) assumes that these three-word decisions can be made in isolation, this is not always valid.1 Vadas and Curran (2007b) employ Barker’s algorithm, but use a supervised classifier to make the sequential bracketing decisions. Because these approaches rely on a sequence of binary decisions, 1 E.g., although the right-most three words are identical in 1) “soap opera stars and television producers,” and 2) “movie and television producers,” the initial right-bracketing decision for “and television producers” should be different in each. 887 early mistakes can cascade and lead to a chain of incorrect bracketings. Our approach differs from previous work in NP parsing; rather than greedily inserting brackets as in Ba"
C10-1100,N01-1025,0,0.169096,"Missing"
C10-1100,A97-1046,0,0.0947829,"Missing"
C10-1100,P95-1007,0,0.510089,"duce a score for every possible NPinternal bracketing and creates a chart of bracketing scores. This chart can be used as features in a full sentence parser or parsed directly with a chart parser. Our parses are highly accurate, creating a strong new standard for this task. Finally, we present experiments that investigate the effects of N-gram frequency cutoffs and various sources of N-gram data. We show an interesting relationship between accuracy and the number of unique N-gram types in the data. 2 Related Work 2.1 Three-Word Noun Compounds The most commonly used data for NP parsing is from Lauer (1995), who extracted 244 three-word noun compounds from the Grolier encyclopedia. When there are only three words, this task reduces to a binary decision: • Left Branching: * [retired science] teacher • Right Branching: retired [science teacher] In Lauer (1995)’s set of noun compounds, twothirds are left branching. The main approach to these three-word noun compounds has been to compute association statistics between pairs of words and then choose the bracketing that corresponds to the more highly associated pair. The two main models are the adjacency model (Marcus, 1980; Liberman and Sproat, 1992;"
C10-1100,lin-etal-2010-new,1,0.133502,"our N-gram features described in Section 6.1 rely on probabilities derived from unlabeled data. To use the largest amount of data possible, we exploit web-scale N-gram corpora. N-gram counts are an efficient way to compress large amounts of data (such as all the text on the web) into a manageable size. An N-gram corpus records how often each unique sequence of words occurs. Co-occurrence probabilities can be calculated directly from the N-gram counts. To keep the size manageable, N-grams that occur with a frequency below a particular threshold can be filtered. The corpus we use is Google V2 (Lin et al., 2010): a new N-gram corpus with N-grams of length 1-5 that we created from the same 1 trillion word snapshot of the web as Google N-grams Version 1 (Brants and Franz, 2006), but with several enhancements. Duplicate sentences are removed, as well as “sentences” which are probably noise (indicated by having a large proportion of non-alphanumeric characters, being very long, or being very short). Removing duplicate sentences is especially important because automaticallygenerated websites, boilerplate text, and legal disclaimers skew the source web data, with sentences that may have only been authored"
C10-1100,W05-0603,0,0.396873,"Missing"
C10-1100,J93-2005,0,0.031406,"who extracted 244 three-word noun compounds from the Grolier encyclopedia. When there are only three words, this task reduces to a binary decision: • Left Branching: * [retired science] teacher • Right Branching: retired [science teacher] In Lauer (1995)’s set of noun compounds, twothirds are left branching. The main approach to these three-word noun compounds has been to compute association statistics between pairs of words and then choose the bracketing that corresponds to the more highly associated pair. The two main models are the adjacency model (Marcus, 1980; Liberman and Sproat, 1992; Pustejovsky et al., 1993; Resnik, 1993) and the dependency model (Lauer, 1995). Under the adjacency model, the bracketing decision is made by comparing the associations between words one and two versus words two and three (i.e. comparing retired science versus science teacher). In contrast, the dependency model compares the associations between one and two versus one and three (retired science versus retired teacher). Lauer (1995) compares the two models and finds the dependency model to be more accurate. Nakov and Hearst (2005) compute the association scores using frequencies, conditional probabilities, χ2 , and mut"
C10-1100,W95-0107,0,0.0291305,"Missing"
C10-1100,J93-2004,0,\N,Missing
C94-1079,P89-1018,0,0.0323165,"Missing"
C94-1079,P92-1024,0,\N,Missing
C94-1079,P93-1016,1,\N,Missing
C98-2122,P90-1034,0,0.391847,"07 ____position 0.25 0.10 task 0.10 0.I0 [ chore 0.ii 0.07 operation 0.10 0.10 I function 0.i0 0.08 I mission 0.12 0.07 I [~atrol 0.07 0.07 I staff 0.i0 0.07 __.__penalty 0.09 0.09 I fee 0.17 0.08 [ _ _ t a r i f f 0.13 0.08 ] tax 0.19 0.07 reservist 0.07 0.07 Figure 3: Similarity tree for ""duty"" Inspection of sample outputs shows that this algorithm works well. However, formal evaluation of its accuracy remains to be future work. 5 Related Work and Conclusion There have been many approaches to automatic detection of similar words from text corpora. Ours is 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collocations and th"
C98-2122,P93-1016,1,0.164555,"g the similarity between their entries and entries in manually created thesauri. Section 4 briefly discuss future work in clustering similar words. Finally, Section 5 reviews related work and summarize our contributions. 768 2 Ilcell, obj-of, containll=4 Ilcell, obj-of, decoratell=2 Word Similarity Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects. We use a broad-coverage parser (Lin, 1993; Lin, 1994) to extract dependency triples from the text corpus. A dependency triple consists of two words and the grammatical relationship between them in the input sentence. For example, the triples extracted from the sentence ""I have a brown dog"" are: (2) (have subj I), (I subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of dog), (dog det a), (a det-of dog) We use the notation ][w, r, w'l[ to denote the frequency count of the dependency triple (w, r, w ~) in the parsed corpus. When w, r, or w ~ is the wild card (,), the frequency counts of all the dependency triples tha"
C98-2122,C94-1079,1,0.0807083,"arity between their entries and entries in manually created thesauri. Section 4 briefly discuss future work in clustering similar words. Finally, Section 5 reviews related work and summarize our contributions. 768 2 Ilcell, obj-of, containll=4 Ilcell, obj-of, decoratell=2 Word Similarity Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects. We use a broad-coverage parser (Lin, 1993; Lin, 1994) to extract dependency triples from the text corpus. A dependency triple consists of two words and the grammatical relationship between them in the input sentence. For example, the triples extracted from the sentence ""I have a brown dog"" are: (2) (have subj I), (I subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of dog), (dog det a), (a det-of dog) We use the notation ][w, r, w'l[ to denote the frequency count of the dependency triple (w, r, w ~) in the parsed corpus. When w, r, or w ~ is the wild card (,), the frequency counts of all the dependency triples that matches th"
C98-2122,P97-1009,1,0.450902,"organized as follows. The next section is concerned with similarities between words based on their distributional patterns. The similarity measure can then be used to create a thesaurus. In Section 3, we evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri. Section 4 briefly discuss future work in clustering similar words. Finally, Section 5 reviews related work and summarize our contributions. 768 2 Ilcell, obj-of, containll=4 Ilcell, obj-of, decoratell=2 Word Similarity Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects. We use a broad-coverage parser (Lin, 1993; Lin, 1994) to extract dependency triples from the text corpus. A dependency triple consists of two words and the grammatical relationship between them in the input sentence. For example, the triples extracted from the sentence ""I have a brown dog"" are: (2) (have subj I), (I subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of d"
C98-2122,P93-1024,0,0.146208,"f its accuracy remains to be future work. 5 Related Work and Conclusion There have been many approaches to automatic detection of similar words from text corpora. Ours is 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collocations and their associated scores were evaluated indirectly by their use in parse tree selection. The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs. The main contribution of this paper is a new evaluation methodology for automatically constructed thesaurus. While previous methods rely on indirect tasks or subje"
C98-2122,J93-1007,0,0.137158,"tree for ""duty"" Inspection of sample outputs shows that this algorithm works well. However, formal evaluation of its accuracy remains to be future work. 5 Related Work and Conclusion There have been many approaches to automatic detection of similar words from text corpora. Ours is 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., 1993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collocations and their associated scores were evaluated indirectly by their use in parse tree selection. The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs. The main contribution of this paper is a ne"
C98-2122,J94-4005,0,\N,Missing
C98-2122,P94-1038,0,\N,Missing
C98-2122,P93-1022,0,\N,Missing
C98-2122,P97-1008,0,\N,Missing
D08-1007,E03-1034,0,0.543489,"ation for Computational Linguistics occurrence model on two tasks: identifying objects of verbs in an unseen corpus and finding pronominal antecedents in coreference data. 2 Related Work Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996). For example, we might have a class Mexican Food and learn that the entire class is suitable for eating. Usually, the classes are from WordNet (Miller et al., 1990), although they can also be inferred from clustering (Rooth et al., 1999). Brockmann and Lapata (2003) compare a number of WordNet-based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class-based approaches do not always outperform simple frequency-based models. Another line of research generalizes using similar words. Suppose we are calculating the probability of a particular noun, n, occurring as the object argument of a given verbal predicate, v. Let Pr(n|v) be the empirical maximum-likelihood estimate from observed text. Dagan et al. (1999) define the similarity-weighted probability, Pr SIM , to be: X PrSIM (n|v) = S"
D08-1007,J90-1003,0,0.111238,"ce word order) to create a neighborhood of implicit negative evidence. We create negatives by substitution rather than perturbation, and use corpuswide statistics to choose our negative instances. 3 Methodology 3.1 Creating Examples To learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text. To create the positives, we automatically parse a large corpus, and then extract the predicate-argument pairs that have a statistical association in this data. We measure this association using pointwise Mutual Information (MI) (Church and Hanks, 1990). The MI between a verb predicate, v, and its object argument, n, is: MI(v, n) = log Pr(n|v) Pr(v, n) = log Pr(v)Pr(n) Pr(n) (2) If MI>0, the probability v and n occur together is greater than if they were independently distributed. We create sets of positive and negative examples separately for each predicate, v. First, we extract all pairs where MI(v, n)>τ as positives. For each positive, we create pseudo-negative examples, (v, n′ ), by pairing v with a new argument, n′ , that either has MI below the threshold or did not occur with v in the corpus. We require each negative n′ to have a simil"
D08-1007,J02-2003,0,0.365322,"nding pronominal antecedents in coreference data. 2 Related Work Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996). For example, we might have a class Mexican Food and learn that the entire class is suitable for eating. Usually, the classes are from WordNet (Miller et al., 1990), although they can also be inferred from clustering (Rooth et al., 1999). Brockmann and Lapata (2003) compare a number of WordNet-based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class-based approaches do not always outperform simple frequency-based models. Another line of research generalizes using similar words. Suppose we are calculating the probability of a particular noun, n, occurring as the object argument of a given verbal predicate, v. Let Pr(n|v) be the empirical maximum-likelihood estimate from observed text. Dagan et al. (1999) define the similarity-weighted probability, Pr SIM , to be: X PrSIM (n|v) = Sim(v ′ , v)Pr(n|v ′ ) (1) v ′ ∈S IMS(v) where Sim(v ′ , v) returns a real-valued similarity between two verbs v ′ an"
D08-1007,C90-3063,0,0.0307244,"Missing"
D08-1007,P07-1028,0,0.233649,"always outperform simple frequency-based models. Another line of research generalizes using similar words. Suppose we are calculating the probability of a particular noun, n, occurring as the object argument of a given verbal predicate, v. Let Pr(n|v) be the empirical maximum-likelihood estimate from observed text. Dagan et al. (1999) define the similarity-weighted probability, Pr SIM , to be: X PrSIM (n|v) = Sim(v ′ , v)Pr(n|v ′ ) (1) v ′ ∈S IMS(v) where Sim(v ′ , v) returns a real-valued similarity between two verbs v ′ and v (normalized over all pair similarities in the sum). In contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross-product of similar pairs. One key issue is how to define the set of similar words, S IMS(w). Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)’s information-theoretic metric work best. Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning"
D08-1007,A00-2017,0,0.0197265,"ew domains, and require no manually-compiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules (Pantel et al., 2007; Roberto et al., 2007). Inferences such as “[X wins Y] ⇒ [X plays Y]” are only valid for certain argu60 ments X and Y. We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. Discriminative techniques are widely used in NLP and have been applied to the related tasks of word prediction and language modeling. Even-Zohar and Roth (2000) use a classifier to predict the most likely word to fill a position in a sentence (in their experiments: a verb) from a set of candidates (sets of verbs), by inspecting the context of the target token (e.g., the presence or absence of a particular nearby word in the sentence). This approach can therefore learn which specific arguments occur with a particular predicate. In comparison, our features are second-order: we learn what kinds of arguments occur with a predicate by encoding features of the arguments. Recent distributed and latentvariable models also represent words with feature vectors"
D08-1007,P03-1001,0,0.0157825,"n, 2002) on a 10 GB corpus, giving 3620 clusters. If a noun belongs in a cluster, a corresponding feature fires. If a noun is in none of the clusters, a no-class feature fires. As an example, CBC cluster 1891 contains: sidewalk, driveway, roadway, footpath, bridge, highway, road, runway, street, alley, path, Interstate, . . . In our training data, we have examples like widen highway, widen road and widen motorway. If we see that we can widen a highway, we learn that we can also widen a sidewalk, bridge, runway, etc. We also made use of the person-name/instance pairs automatically extracted by Fleischman et al. (2003).2 This data provides counts for pairs such as “Edwin Moses, hurdler” and “William Farley, industrialist.” We have features for all concepts and therefore learn their association with each verb. 4 Experiments and Results tioned in Section 3.3. Feature values are normalized within each feature type. We train our (linear kernel) discriminative models using SVMlight (Joachims, 1999) on each partition, but set meta-parameters C (regularization) and j (cost of positive vs. negative misclassifications: max at j=2) on the macroaveraged score across all development partitions. Note that we can not use"
D08-1007,N04-1037,0,0.0142543,"Missing"
D08-1007,J03-3005,0,0.449396,"y rarely contain digits, hyphens, or begin with a human first name like Bob. D SP encodes these interdependent properties as features in a linear classifier. This classifier can score any noun as a plausible argument of eat if indicative features are present; MI can only 61 assign high plausibility to observed (eat,n) pairs. Similarity-smoothed models can make use of the regularities across similar verbs, but not the finergrained string- and token-based features. Our training examples are similar to the data created for pseudodisambiguation, the usual evaluation task for SP models (Erk, 2007; Keller and Lapata, 2003; Rooth et al., 1999). This data consists of triples (v, n, n′ ) where v, n is a predicateargument pair observed in the corpus and v, n′ has not been observed. The models score correctly if they rank observed (and thus plausible) arguments above corresponding unobserved (and thus likely implausible) ones. We refer to this as Pairwise Disambiguation. Unlike this task, we classify each predicate-argument pair independently as plausible/implausible. We also use MI rather than frequency to define the positive pairs, ensuring that the positive pairs truly have a statistical association, and are not"
D08-1007,J98-2002,0,0.364358,"an unseen corpus and finding pronominal antecedents in coreference data. 2 Related Work Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996). For example, we might have a class Mexican Food and learn that the entire class is suitable for eating. Usually, the classes are from WordNet (Miller et al., 1990), although they can also be inferred from clustering (Rooth et al., 1999). Brockmann and Lapata (2003) compare a number of WordNet-based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class-based approaches do not always outperform simple frequency-based models. Another line of research generalizes using similar words. Suppose we are calculating the probability of a particular noun, n, occurring as the object argument of a given verbal predicate, v. Let Pr(n|v) be the empirical maximum-likelihood estimate from observed text. Dagan et al. (1999) define the similarity-weighted probability, Pr SIM , to be: X PrSIM (n|v) = Sim(v ′ , v)Pr(n|v ′ ) (1) v ′ ∈S IMS(v) where Sim(v ′ , v) returns a real-valued similari"
D08-1007,P98-2127,1,0.419826,"text. Dagan et al. (1999) define the similarity-weighted probability, Pr SIM , to be: X PrSIM (n|v) = Sim(v ′ , v)Pr(n|v ′ ) (1) v ′ ∈S IMS(v) where Sim(v ′ , v) returns a real-valued similarity between two verbs v ′ and v (normalized over all pair similarities in the sum). In contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross-product of similar pairs. One key issue is how to define the set of similar words, S IMS(w). Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)’s information-theoretic metric work best. Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules (Pantel et al., 2007; Roberto et al., 2007). Inferences such as “[X wins Y] ⇒ [X plays Y]” are only valid for certain argu60 ments X and Y. We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. Discriminativ"
D08-1007,P07-1010,0,0.0183033,"what kinds of arguments occur with a predicate by encoding features of the arguments. Recent distributed and latentvariable models also represent words with feature vectors (Bengio et al., 2003; Blitzer et al., 2005). Many of these approaches learn both the feature weights and the feature representation. Vectors must be kept low-dimensional for tractability, while learning and inference on larger scales is impractical. By partitioning our examples by predicate, we can efficiently use high-dimensional, sparse vectors. Our technique of generating negative examples is similar to the approach of Okanohara and Tsujii (2007). They learn a classifier to disambiguate actual sentences from pseudo-negative examples sampled from an N-gram language model. Smith and Eisner (2005) also automatically generate negative examples. They perturb their input sequence (e.g. the sentence word order) to create a neighborhood of implicit negative evidence. We create negatives by substitution rather than perturbation, and use corpuswide statistics to choose our negative instances. 3 Methodology 3.1 Creating Examples To learn a discriminative model of selectional preference, we create positive and negative training examples automatic"
D08-1007,N07-1071,0,0.28034,"arguments, while Wang et al. (2005) use the cross-product of similar pairs. One key issue is how to define the set of similar words, S IMS(w). Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)’s information-theoretic metric work best. Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules (Pantel et al., 2007; Roberto et al., 2007). Inferences such as “[X wins Y] ⇒ [X plays Y]” are only valid for certain argu60 ments X and Y. We follow Pantel et al. (2007) in using automatically-extracted semantic classes to help characterize plausible arguments. Discriminative techniques are widely used in NLP and have been applied to the related tasks of word prediction and language modeling. Even-Zohar and Roth (2000) use a classifier to predict the most likely word to fill a position in a sentence (in their experiments: a verb) from a set of candidates (sets of verbs), by inspecting the context of the target t"
D08-1007,P99-1014,0,0.638299,"ber 2008. 2008 Association for Computational Linguistics occurrence model on two tasks: identifying objects of verbs in an unseen corpus and finding pronominal antecedents in coreference data. 2 Related Work Most approaches to SPs generalize from observed predicate-argument pairs to semantically similar ones by modeling the semantic class of the argument, following Resnik (1996). For example, we might have a class Mexican Food and learn that the entire class is suitable for eating. Usually, the classes are from WordNet (Miller et al., 1990), although they can also be inferred from clustering (Rooth et al., 1999). Brockmann and Lapata (2003) compare a number of WordNet-based approaches, including Resnik (1996), Li and Abe (1998), and Clark and Weir (2002), and found that the more sophisticated class-based approaches do not always outperform simple frequency-based models. Another line of research generalizes using similar words. Suppose we are calculating the probability of a particular noun, n, occurring as the object argument of a given verbal predicate, v. Let Pr(n|v) be the empirical maximum-likelihood estimate from observed text. Dagan et al. (1999) define the similarity-weighted probability, Pr S"
D08-1007,P05-1044,0,0.0283286,"th feature vectors (Bengio et al., 2003; Blitzer et al., 2005). Many of these approaches learn both the feature weights and the feature representation. Vectors must be kept low-dimensional for tractability, while learning and inference on larger scales is impractical. By partitioning our examples by predicate, we can efficiently use high-dimensional, sparse vectors. Our technique of generating negative examples is similar to the approach of Okanohara and Tsujii (2007). They learn a classifier to disambiguate actual sentences from pseudo-negative examples sampled from an N-gram language model. Smith and Eisner (2005) also automatically generate negative examples. They perturb their input sequence (e.g. the sentence word order) to create a neighborhood of implicit negative evidence. We create negatives by substitution rather than perturbation, and use corpuswide statistics to choose our negative instances. 3 Methodology 3.1 Creating Examples To learn a discriminative model of selectional preference, we create positive and negative training examples automatically from raw text. To create the positives, we automatically parse a large corpus, and then extract the predicate-argument pairs that have a statistic"
D08-1007,W05-1516,1,0.864942,"f research generalizes using similar words. Suppose we are calculating the probability of a particular noun, n, occurring as the object argument of a given verbal predicate, v. Let Pr(n|v) be the empirical maximum-likelihood estimate from observed text. Dagan et al. (1999) define the similarity-weighted probability, Pr SIM , to be: X PrSIM (n|v) = Sim(v ′ , v)Pr(n|v ′ ) (1) v ′ ∈S IMS(v) where Sim(v ′ , v) returns a real-valued similarity between two verbs v ′ and v (normalized over all pair similarities in the sum). In contrast, Erk (2007) generalizes by substituting similar arguments, while Wang et al. (2005) use the cross-product of similar pairs. One key issue is how to define the set of similar words, S IMS(w). Erk (2007) compared a number of techniques for creating similar-word sets and found that both the Jaccard coefficient and Lin (1998a)’s information-theoretic metric work best. Similarity-smoothed models are simple to compute, potentially adaptable to new domains, and require no manually-compiled resources such as WordNet. Selectional Preferences have also been a recent focus of researchers investigating the learning of paraphrases and inference rules (Pantel et al., 2007; Roberto et al.,"
D08-1007,J05-4002,0,0.0180877,"obj (n|v ′ ) features for every verb that occurs more than 10 times in our corpus. λvv′ may be positive or negative, depending on the relation between v ′ and v. We also include features for the probability of the noun occurring as the subject of other verbs, Prsubj (n|v ′ ). For example, nouns that can be the object of eat will also occur as the subject of taste and contain. Other contexts, such as adjectival and nominal predicates, could also aid the prediction, but have not yet been investigated. The advantage of tuning similarity to the application of interest has been shown previously by Weeds and Weir (2005). They optimize a few metaparameters separately for the tasks of thesaurus generation and pseudodisambiguation. Our approach, on the other hand, discriminatively sets millions of individual similarity values. Like Weeds and Weir (2005), our similarity values are asymmetric. 3.3.2 String-based We include several simple character-based features of the noun string: the number of tokens, the case, and whether it contains digits, hyphens, an apostrophe, or other punctuation. We also include a feature for the first and last token, and fire indicator features if any token in the noun occurs on in-hou"
D08-1007,J04-4004,0,\N,Missing
D08-1007,C98-2122,1,\N,Missing
D18-1235,P17-1171,0,0.0472406,"Missing"
D18-1235,P17-1055,0,0.0340931,"our work. Great effort has been put into the development of sophisticated neural models for machine reading comprehension. The attention mechanism was first introduced by Hermann et al. (2015) into reading comprehension and soon became the dom2110 inating model. Wang and Jiang (2017) proposed to solve machine comprehension using MatchLSTM and answer pointer. Seo et al. (2017) and Xiong et al. (2017) applied different ways to match the question and the context with bidirectional attention. Hu et al. (2017) used iterative aligner to match the question and the passage with feature-rich encoder. Cui et al. (2017) employed one more layer of attention over the bi-directional attention mechanism. Wang et al. (2017) applied a self-matching mechanism to aggregate evidence from the context. Tan et al. (2018) proposed to generate answer from extracted answer span. Yu et al. (2018) proposed to use convolution with self-attention instead of recurrent models in reading comprehension. Recently there are some emerging works starting to touch the reading comprehension task from the answer side. Wang et al. (2018a) proposed to use evidence aggregation to re-rank answer candidates extracted from different passages,"
D18-1235,P17-1147,0,0.0217511,"re the source of answers is a short passage with a few hundred words, the DuReader dataset provides up to 5 full documents, which may contain up to 100K words. This incurs exorbitant demand on memory and training time. To deal with this issue, previous approaches select a single representative paragraph for each document, on which the answer extraction is performed. The original paper of DuReader (He et al., 2017) employed a simple heuristic strategy, and Wang et al. (2018b) trained a paragraph ranking model, while Clark and Gardner (2017) applied TF-IDF based method for the TriviaQA dataset (Joshi et al., 2017) which was in a similar situation. However, answers could come from more than one paragraph. We apply a simple yet effective method to extract contents from multiple paragraphs of the document, aiming to include as much information for the answer extraction as possible. We concatenate the title and the whole document as the passage if it is shorter than a predefined maximum length. If not, we employ passage extraction in the following way: • The title of the document is extracted. Whether a document is relevant to the question could be easily seen from the title. • We compute BLEU-4 score of e"
D18-1235,D16-1264,0,0.259883,"rence problem of a single answer. Combined with a simple heuristic passage extraction strategy for overlong documents, our model increases the ROUGE-L score on the DuReader dataset from 44.18, the previous state-of-the-art, to 51.09. 1 Introduction Machine reading comprehension (MRC) or question answering (QA) has been a long-standing goal in Natural Language Processing. There is a surge of interest in this area due to new end-to-end modeling techniques and the release of several largescale, open-domain datasets. In earlier datasets (Hermann et al., 2015; Hill et al., 2016; Yang et al., 2015; Rajpurkar et al., 2016), the questions did not arise from actual end users. Instead, they were constructed in cloze style or created by crowdworkers given a short passage from well-edited sources such as Wikipedia and CNN/Daily Mail. As a consequence, the questions ∗ Corresponding author: D. Lin (lindek@naturali.io). are usually well-formed and about simple facts, and the answers are guaranteed to exist as short spans in the given candidate passages. In MS-MARCO (Nguyen et al., 2016), the questions were sampled from actual search queries, which may have typos and may not be phrased as questions.1 Multiple short pass"
D18-1235,P16-1159,1,0.808107,"xtracted answer span. Yu et al. (2018) proposed to use convolution with self-attention instead of recurrent models in reading comprehension. Recently there are some emerging works starting to touch the reading comprehension task from the answer side. Wang et al. (2018a) proposed to use evidence aggregation to re-rank answer candidates extracted from different passages, and Wang et al. (2018b) proposed Cross-Passage Answer Verification model for the same purpose. Neither of them involved multiple answers as in this work. Minimum Risk Training (MRT) has been widely used in various tasks in NLP. Shen et al. (2016) introduced MRT into Neural Machine Translation, and Ayana et al. (2016) applied it in Text Summarization. 3 Our Approach In this section we describe in details the architecture of our model which is depicted in Figure 2. 3.1 Passage Extraction Unlike most other datasets where the source of answers is a short passage with a few hundred words, the DuReader dataset provides up to 5 full documents, which may contain up to 100K words. This incurs exorbitant demand on memory and training time. To deal with this issue, previous approaches select a single representative paragraph for each document, o"
D18-1235,P17-1018,0,0.276629,"reading comprehension. The attention mechanism was first introduced by Hermann et al. (2015) into reading comprehension and soon became the dom2110 inating model. Wang and Jiang (2017) proposed to solve machine comprehension using MatchLSTM and answer pointer. Seo et al. (2017) and Xiong et al. (2017) applied different ways to match the question and the context with bidirectional attention. Hu et al. (2017) used iterative aligner to match the question and the passage with feature-rich encoder. Cui et al. (2017) employed one more layer of attention over the bi-directional attention mechanism. Wang et al. (2017) applied a self-matching mechanism to aggregate evidence from the context. Tan et al. (2018) proposed to generate answer from extracted answer span. Yu et al. (2018) proposed to use convolution with self-attention instead of recurrent models in reading comprehension. Recently there are some emerging works starting to touch the reading comprehension task from the answer side. Wang et al. (2018a) proposed to use evidence aggregation to re-rank answer candidates extracted from different passages, and Wang et al. (2018b) proposed Cross-Passage Answer Verification model for the same purpose. Neithe"
D18-1235,P18-1178,0,0.0482647,"Missing"
D18-1235,K17-1028,0,0.0123283,"ted passages. 3.2 Representation of Word m Given a word sequence of question Q = {wtq }t=1 , and a word sequence of extracted passage P = n {wtp }t=1 , we combine different useful information to form the representation of each question word wtq and passage word wtp : 2111 • Word-level embedding: each word w in the question and passage is mapped to its corresponding n-dimensional embedding we. • POS tag embedding: we use a POS tagger to tag each word in the question and passage. Each POS tag is mapped to a m-dimensional embedding pe. • Word-in-question feature: following Chen et al. (2017) and Weissenborn et al. (2017), we use one additional binary feature wiq for each passage word, indicating whether this word occurs in the question. Figure 2: Model Architecture Each question word is represented as the concatenation of the word embedding we, and the POS tag embedding pe, denoted as xq = [we; pe]. Each passage word is additionally concatenated with the word-in-question feature wiq: xp = [we; pe; wiq]. It should be noted that, character-level embedding is an important part of word representation in English MRC models (Seo et al., 2017; Weissenborn et al., 2017; Wang et al., 2017; Tan et al., 2018). Character"
D18-1235,D15-1237,0,0.0634382,"Missing"
dorr-etal-2000-building,W98-1426,0,\N,Missing
dorr-etal-2000-building,dorr-etal-2000-chinese,1,\N,Missing
dorr-etal-2000-building,dorr-katsova-1998-lexical,1,\N,Missing
dorr-etal-2000-building,C94-1038,0,\N,Missing
dorr-etal-2000-building,olsen-etal-1998-enhancing,1,\N,Missing
dorr-etal-2000-building,A97-1021,1,\N,Missing
dorr-etal-2000-building,P98-1046,0,\N,Missing
dorr-etal-2000-building,C98-1046,0,\N,Missing
dorr-etal-2000-building,P98-1116,0,\N,Missing
dorr-etal-2000-building,C98-1112,0,\N,Missing
dorr-etal-2000-building,dorr-etal-1998-thematic,1,\N,Missing
dorr-etal-2000-chinese,C94-1038,0,\N,Missing
dorr-etal-2000-chinese,olsen-etal-1998-enhancing,1,\N,Missing
dorr-etal-2000-chinese,A97-1021,1,\N,Missing
dorr-etal-2000-chinese,P98-1046,0,\N,Missing
dorr-etal-2000-chinese,C98-1046,0,\N,Missing
E06-1019,J93-2003,0,0.048577,"only difference between (Yamada and Knight, 2001) and ITGs; the probability models are also very different. By using a fixed dependency tree inside an ITG, we can revisit the question of whether using a fixed tree is harmful, but in a controlled environment. 2 Alignment Spaces Let an alignment be the entire structure that connects a sentence pair, and let a link be the individual word-to-word connections that make up an alignment. An alignment space determines the set of all possible alignments that can ex145 ist for a given sentence pair. Alignment spaces can emerge from generative stories (Brown et al., 1993), from syntactic notions (Wu, 1997), or they can be imposed to create competition between links (Melamed, 2000). They can generally be described in terms of how links interact. For the sake of describing the size of alignment spaces, we will assume that both sentences have n tokens. The largest alignment space for a sentence 2 pair has 2n possible alignments. This describes the case where each of the n2 potential links can be either on or off with no restrictions. 2.1 Permutation Space A straight-forward way to limit the space of possible alignments is to enforce a one-to-one constraint (Melam"
E06-1019,W02-1039,0,0.0749289,"ted onto the bottom sentence. Zens and Ney (2003) explore the re-orderings allowed by ITGs, and provide a formulation for the number of structures that can be built for a sentence pair of size n. ITGs explore almost all of permutation space when n is small, but their coverage of permutation space falls off quickly for n > 5 (Wu, 1997). 2.3 Dependency Space Dependency space defines the set of all alignments that maintain phrasal cohesion with respect to a dependency tree provided for the English sentence. The space is constrained so that the phrases in the dependency tree always move together. Fox (2002) introduced the notion of headmodifier and modifier-modifier crossings. These occur when a phrase’s image in the Foreign sentence overlaps with the image of its head, or one of its siblings. An alignment with no crossings maintains phrasal cohesion. Figure 2 shows a headmodifier crossing: the image c of a head 2 overlaps with the image (b, d) of 2’s modifier, (3, 4). Lin  A → [AA] |hAAi |e/f  (1)   Figure 2: A phrasal cohesion violation. 1 This is a simplification that ignores null links. The actual number of possible alignments lies between n! and (n + 1)n . and Cherry (2003) used the not"
E06-1019,H91-1026,0,0.0487314,"the ITG and dependency spaces. HD-ITG: The D-ITG method with an added head constraint, as described in Section 3.3. 4.3 Learned objective function The link score flink is usually imperfect, because it is learned from data. Appropriately defined alignment spaces may rule out bad links even if they are assigned high flink values, based on other links in the alignment. We define the following simple link score to test the guidance provided by different alignment spaces: flink (a, E, F ) = φ2 (ei , fj ) − C|i − j| (6) Here, a = (i, j) is a link and φ2 (ei , fj ) returns the φ2 correlation metric (Gale and Church, 1991) 150 Table 1: Results with the learned link score. Method Prec Rec F AER Greedy 78.1 81.4 79.5 20.47 Beam 79.1 82.7 80.7 19.32 Match 79.3 82.7 80.8 19.24 ITG 81.8 83.7 82.6 17.36 Dep 88.8 84.0 86.6 13.40 D-ITG 88.8 84.2 86.7 13.32 HD-ITG 89.2 84.0 86.9 13.15 the only method we have available to search dependency space is also a beam search. The error rates for the three dependency-based methods are similar; no one method provides much more guidance than the other. Enforcing head constraints produces only a small improvement over the D-ITG. Assuming our beam search is approximating a complete s"
E06-1019,N03-2017,1,0.893893,"st advantage of syntactic alignment. We Dekang Lin Google Inc. 1600 Amphitheatre Parkway Mountain View, CA, USA, 94043 lindek@google.com fix an alignment scoring model that works equally well on flat strings as on parse trees, but we vary the space of alignments evaluated with that model. These spaces become smaller as more linguistic guidance is added. We measure the benefits and detriments of these constrained searches. Several of the spaces we investigate draw guidance from a dependency tree for one of the sentences. We will refer to the parsed language as English and the other as Foreign. Lin and Cherry (2003) have shown that adding a dependency-based cohesion constraint to an alignment search can improve alignment quality. Unfortunately, the usefulness of their beam search solution is limited: potential alignments are constructed explicitly, which prevents a perfect search of alignment space and the use of algorithms like EM. However, the cohesion constraint is based on a tree, which should make it amenable to dynamic programming solutions. To enable such techniques, we bring the cohesion constraint inside the ITG framework (Wu, 1997). Zhang and Gildea (2004) compared Yamada and Knight’s (2001) tr"
E06-1019,C94-1079,1,0.699647,"uidance provided by a space, or its capacity to stop an aligner from selecting bad alignments. We also test expressiveness, or how often a space allows an aligner to select the best alignment. In all cases, we report our results in terms of alignment quality, using the standard word alignment error metrics: precision, recall, F-measure and alignment error rate (Och and Ney, 2003). Our test set is the 500 manually aligned sentence pairs created by Franz Och and Hermann Ney (2003). These English-French pairs are drawn from the Canadian Hansards. English dependency trees are supplied by Minipar (Lin, 1994). 4.1 Objective Function In our experiments, we hold all variables constant except for the alignment space being searched, and in the case of imperfect searches, the search method. In particular, all of the methods we test will use the same objective function to select the “best” alignment from their space. Let A be an alignment for an English, Foreign sentence pair, (E, F ). A is represented as a set of links, where each link is a pair of English and Foreign positions, (i, j), that are connected by the alignment. The score of a proposed alignment is: falign (A, E, F ) = X flink (a, E, F ) (5)"
E06-1019,J00-2004,0,0.197774,"g a fixed dependency tree inside an ITG, we can revisit the question of whether using a fixed tree is harmful, but in a controlled environment. 2 Alignment Spaces Let an alignment be the entire structure that connects a sentence pair, and let a link be the individual word-to-word connections that make up an alignment. An alignment space determines the set of all possible alignments that can ex145 ist for a given sentence pair. Alignment spaces can emerge from generative stories (Brown et al., 1993), from syntactic notions (Wu, 1997), or they can be imposed to create competition between links (Melamed, 2000). They can generally be described in terms of how links interact. For the sake of describing the size of alignment spaces, we will assume that both sentences have n tokens. The largest alignment space for a sentence 2 pair has 2n possible alignments. This describes the case where each of the n2 potential links can be either on or off with no restrictions. 2.1 Permutation Space A straight-forward way to limit the space of possible alignments is to enforce a one-to-one constraint (Melamed, 2000). Under such a constraint, each token in the sentence pair can participate in at most one link. Each t"
E06-1019,N03-1021,0,0.0467551,"6 fewer correct links than the D-ITG, each corresponding to a single missed link in a different sentence pair. These misses occur in cases where two modifiers switch position with respect to their head during translation. Surprisingly, there are regularly occurring, systematic constructs that violate the head constraints. An example of such a construct is when an English noun has both adjective and noun modifiers. Cases like “Canadian Wheat Board” are translated as, “Board Canadian of Wheat”, switching the modifiers’ relative positions. These switches correspond to discontinuous constituents (Melamed, 2003) in general bitext parsing. The D-ITG can handle discontinuities by freely grouping constituents to create continuity, but the HD-ITG, with its fixed head and modifiers, cannot. Given that the HD-ITG provides only slightly more guidance than the DITG, we recommend that this type of head information be included only as a soft constraint. 5 Conclusion We have presented two new alignment spaces based on a dependency tree provided for one of the sentences in a sentence pair. We have given grammars to conduct a perfect search of these spaces using an ITG parser. The grammars derive exactly one stru"
E06-1019,J03-1002,0,0.0215508,"Figure 6: Structures allowed by the head constraint. outer modifier Mo between H and the inner modifier Mi . 4 Experiments and Results We compare the alignment spaces described in this paper under two criteria. First we test the guidance provided by a space, or its capacity to stop an aligner from selecting bad alignments. We also test expressiveness, or how often a space allows an aligner to select the best alignment. In all cases, we report our results in terms of alignment quality, using the standard word alignment error metrics: precision, recall, F-measure and alignment error rate (Och and Ney, 2003). Our test set is the 500 manually aligned sentence pairs created by Franz Och and Hermann Ney (2003). These English-French pairs are drawn from the Canadian Hansards. English dependency trees are supplied by Minipar (Lin, 1994). 4.1 Objective Function In our experiments, we hold all variables constant except for the alignment space being searched, and in the case of imperfect searches, the search method. In particular, all of the methods we test will use the same objective function to select the “best” alignment from their space. Let A be an alignment for an English, Foreign sentence pair, (E"
E06-1019,H05-1010,0,0.254959,"(Melamed, 2000). Under such a constraint, each token in the sentence pair can participate in at most one link. Each token in the English sentence picks a token from the Foreign sentence to link to, which is then removed from competition. This allows for n! possible alignments1 , a substan2 tial reduction from 2n . Note that n! is also the number of possible permutations of the n tokens in either one of the two sentences. Permutation space enforces the one-to-one constraint, but allows any reordering of tokens as they are translated. Permutation space methods include weighted maximum matching (Taskar et al., 2005), and approximations to maximum matching like competitive linking (Melamed, 2000). The IBM models (Brown et al., 1993) search a version of permutation space with a one-to-many constraint. 2.2 ITG Space Inversion Transduction Grammars, or ITGs (Wu, 1997) provide an efficient formalism to synchronously parse bitext. This produces a parse tree that decomposes both sentences and also implies a word alignment. ITGs are transduction grammars because their terminal symbols can produce tokens in both the English and Foreign sentences. Inversions occur when the order of constituents is reversed in one"
E06-1019,J97-3002,0,0.895076,"arsed language as English and the other as Foreign. Lin and Cherry (2003) have shown that adding a dependency-based cohesion constraint to an alignment search can improve alignment quality. Unfortunately, the usefulness of their beam search solution is limited: potential alignments are constructed explicitly, which prevents a perfect search of alignment space and the use of algorithms like EM. However, the cohesion constraint is based on a tree, which should make it amenable to dynamic programming solutions. To enable such techniques, we bring the cohesion constraint inside the ITG framework (Wu, 1997). Zhang and Gildea (2004) compared Yamada and Knight’s (2001) tree-to-string alignment model to ITGs. They concluded that methods like ITGs, which create a tree during alignment, perform better than methods with a fixed tree established before alignment begins. However, the use of a fixed tree is not the only difference between (Yamada and Knight, 2001) and ITGs; the probability models are also very different. By using a fixed dependency tree inside an ITG, we can revisit the question of whether using a fixed tree is harmful, but in a controlled environment. 2 Alignment Spaces Let an alignment"
E06-1019,P01-1067,0,0.675498,"s word-level correspondences between parallel sentences. The task originally emerged as an intermediate result of training the IBM translation models (Brown et al., 1993). These models use minimal linguistic intuitions; they essentially treat sentences as flat strings. They remain the dominant method for word alignment (Och and Ney, 2003). There have been several proposals to introduce syntax into word alignment. Some work within the framework of synchronous grammars (Wu, 1997; Melamed, 2003), while others create a generative story that includes a parse tree provided for one of the sentences (Yamada and Knight, 2001). There are three primary reasons to add syntax to word alignment. First, one can incorporate syntactic features, such as grammar productions, into the models that guide the alignment search. Second, movement can be modeled more naturally; when a three-word noun phrase moves during translation, it can be modeled as one movement operation instead of three. Finally, one can restrict the type of movement that is considered, shrinking the number of alignments that are attempted. We investigate this last advantage of syntactic alignment. We Dekang Lin Google Inc. 1600 Amphitheatre Parkway Mountain"
E06-1019,P03-1019,0,0.0391902,"ars in the Foreign sentence. Used as a word aligner, an ITG parser searches a subspace of permutation space: the ITG requires that any movement that occurs during translation be explained by a binary tree with inversions. Alignments that allow no phrases to be formed in bitext are not attempted. This results in two forbidden alignment structures, shown in Figure 1, called “inside-out” transpositions in (Wu, 1997). Note that no pair of contiguous tokens in the top              Figure 1: Forbidden alignments in ITG sentence remain contiguous when projected onto the bottom sentence. Zens and Ney (2003) explore the re-orderings allowed by ITGs, and provide a formulation for the number of structures that can be built for a sentence pair of size n. ITGs explore almost all of permutation space when n is small, but their coverage of permutation space falls off quickly for n > 5 (Wu, 1997). 2.3 Dependency Space Dependency space defines the set of all alignments that maintain phrasal cohesion with respect to a dependency tree provided for the English sentence. The space is constrained so that the phrases in the dependency tree always move together. Fox (2002) introduced the notion of headmodifier"
E06-1019,C04-1060,0,0.788156,"age as English and the other as Foreign. Lin and Cherry (2003) have shown that adding a dependency-based cohesion constraint to an alignment search can improve alignment quality. Unfortunately, the usefulness of their beam search solution is limited: potential alignments are constructed explicitly, which prevents a perfect search of alignment space and the use of algorithms like EM. However, the cohesion constraint is based on a tree, which should make it amenable to dynamic programming solutions. To enable such techniques, we bring the cohesion constraint inside the ITG framework (Wu, 1997). Zhang and Gildea (2004) compared Yamada and Knight’s (2001) tree-to-string alignment model to ITGs. They concluded that methods like ITGs, which create a tree during alignment, perform better than methods with a fixed tree established before alignment begins. However, the use of a fixed tree is not the only difference between (Yamada and Knight, 2001) and ITGs; the probability models are also very different. By using a fixed dependency tree inside an ITG, we can revisit the question of whether using a fixed tree is harmful, but in a controlled environment. 2 Alignment Spaces Let an alignment be the entire structure"
E06-1050,P89-1010,0,0.0651465,"f the path to be nouns. We parsed the AQUAINT corpus (3GB) with Minipar (Lin, 2001) and collected the frequency counts of words appearing in various contexts. Parsing and database construction is performed off-line as the database is identical for all questions. We extracted 527,768 contexts that appeared at least 25 times in the corpus. An example context and its fillers are shown in Figure 1. The context in which a word appears often imposes constraints on the semantic type of the word. This basic idea has been exploited by many proposals for distributional similarity and clustering, e.g., (Church and Hanks, 1989; Lin, 1998; Pereira et al., 1993). Similar to Lin and Pantel (2001), we define the contexts of a word to be the undirected paths in dependency trees involving that word at either the beginning or the end. The following diagram shows an example dependency tree: subj X obj host Africa 2 AP 1 Argentina 1 Athens 16 Atlanta 3 Bangkok 1 ... decades 1 facility 1 government 1 Olympics grant 1 he 2 homeland 3 IOC 1 Iran 2 Jakarta 1 ... president 2 Pusan 1 race 1 readiness 2 Rio de Janeiro 1 Rome 1 Salt Lake City 2 school 1 S. Africa 1 ... Zakopane 4 Figure 1: An example context and its fillers obj 3.2"
E06-1050,W01-1203,0,0.0793952,"Missing"
E06-1050,N01-1005,0,0.0702543,"makes it much easier to select the answer from a sentence that contains the query words. Consider the question “What is the capital of Norway?” We would expect the answer to be a city and could filter out most of the words in the following sentence: The landed aristocracy was virtually crushed by Hakon V, who reigned from 1299 to 1319, and Oslo became the capital of Norway, replacing Bergen as the principal city of the kingdom. The goal of answer typing is to determine whether a word’s semantic type is appropriate as an answer for a question. Many previous approaches to answer typing, e.g., (Ittycheriah et al., 2001; Li and Roth, 2002; Krishnan et al., 2005), employ a predefined set of answer types and use supervised learning or manually constructed rules Dekang Lin Google, Inc. 1600 Amphitheatre Parkway Mountain View, CA lindek@google.com to classify a question according to expected answer type. A disadvantage of this approach is that there will always be questions whose answers do not belong to any of the predefined types. Consider the question: “What are tourist attractions in Reims?” The answer may be many things: a church, a historic residence, a park, a famous intersection, a statue, etc. A common"
E06-1050,H05-1040,0,0.0456985,"om a sentence that contains the query words. Consider the question “What is the capital of Norway?” We would expect the answer to be a city and could filter out most of the words in the following sentence: The landed aristocracy was virtually crushed by Hakon V, who reigned from 1299 to 1319, and Oslo became the capital of Norway, replacing Bergen as the principal city of the kingdom. The goal of answer typing is to determine whether a word’s semantic type is appropriate as an answer for a question. Many previous approaches to answer typing, e.g., (Ittycheriah et al., 2001; Li and Roth, 2002; Krishnan et al., 2005), employ a predefined set of answer types and use supervised learning or manually constructed rules Dekang Lin Google, Inc. 1600 Amphitheatre Parkway Mountain View, CA lindek@google.com to classify a question according to expected answer type. A disadvantage of this approach is that there will always be questions whose answers do not belong to any of the predefined types. Consider the question: “What are tourist attractions in Reims?” The answer may be many things: a church, a historic residence, a park, a famous intersection, a statue, etc. A common method to deal with this problem is to defi"
E06-1050,C02-1150,0,0.266186,"elect the answer from a sentence that contains the query words. Consider the question “What is the capital of Norway?” We would expect the answer to be a city and could filter out most of the words in the following sentence: The landed aristocracy was virtually crushed by Hakon V, who reigned from 1299 to 1319, and Oslo became the capital of Norway, replacing Bergen as the principal city of the kingdom. The goal of answer typing is to determine whether a word’s semantic type is appropriate as an answer for a question. Many previous approaches to answer typing, e.g., (Ittycheriah et al., 2001; Li and Roth, 2002; Krishnan et al., 2005), employ a predefined set of answer types and use supervised learning or manually constructed rules Dekang Lin Google, Inc. 1600 Amphitheatre Parkway Mountain View, CA lindek@google.com to classify a question according to expected answer type. A disadvantage of this approach is that there will always be questions whose answers do not belong to any of the predefined types. Consider the question: “What are tourist attractions in Reims?” The answer may be many things: a church, a historic residence, a park, a famous intersection, a statue, etc. A common method to deal with"
E06-1050,P98-2127,1,0.223074,"We parsed the AQUAINT corpus (3GB) with Minipar (Lin, 2001) and collected the frequency counts of words appearing in various contexts. Parsing and database construction is performed off-line as the database is identical for all questions. We extracted 527,768 contexts that appeared at least 25 times in the corpus. An example context and its fillers are shown in Figure 1. The context in which a word appears often imposes constraints on the semantic type of the word. This basic idea has been exploited by many proposals for distributional similarity and clustering, e.g., (Church and Hanks, 1989; Lin, 1998; Pereira et al., 1993). Similar to Lin and Pantel (2001), we define the contexts of a word to be the undirected paths in dependency trees involving that word at either the beginning or the end. The following diagram shows an example dependency tree: subj X obj host Africa 2 AP 1 Argentina 1 Athens 16 Atlanta 3 Bangkok 1 ... decades 1 facility 1 government 1 Olympics grant 1 he 2 homeland 3 IOC 1 Iran 2 Jakarta 1 ... president 2 Pusan 1 race 1 readiness 2 Rio de Janeiro 1 Rome 1 Salt Lake City 2 school 1 S. Africa 1 ... Zakopane 4 Figure 1: An example context and its fillers obj 3.2.1 det Ques"
E06-1050,H01-1046,1,0.731602,"N host city city hosted X In these paths, words are reduced to their root forms and proper names are reduced to their entity tags (we used MUC7 named entity tags). Paths allow us to balance the specificity of contexts and the sparseness of data. Longer paths typically impose stricter constraints on the slot fillers. However, they tend to have fewer occurrences, making them more prone to errors arising from data sparseness. We have restricted the path length to two (involving at most three words) and require the two ends of the path to be nouns. We parsed the AQUAINT corpus (3GB) with Minipar (Lin, 2001) and collected the frequency counts of words appearing in various contexts. Parsing and database construction is performed off-line as the database is identical for all questions. We extracted 527,768 contexts that appeared at least 25 times in the corpus. An example context and its fillers are shown in Figure 1. The context in which a word appears often imposes constraints on the semantic type of the word. This basic idea has been exploited by many proposals for distributional similarity and clustering, e.g., (Church and Hanks, 1989; Lin, 1998; Pereira et al., 1993). Similar to Lin and Pantel"
E06-1050,W03-2806,0,0.0368044,"Missing"
E06-1050,P93-1024,0,0.696161,"sier to integrate the answer type scores with scores computed by other components in a question answering system in a principled fashion. 3 Resources Before introducing our model, we first describe the resources used in the model. 3.1 Word Clusters Natural language data is extremely sparse. Word clusters are a way of coping with data sparseness by abstracting a given word to a class of related words. Clusters, as used by our probabilistic answer typing system, play a role similar to that of named entity types. Many methods exist for clustering, e.g., (Brown et al., 1990; Cutting et al., 1992; Pereira et al., 1993; Karypis et al., 1999). We used the Clustering By Committee (CBC) 394 Table 1: Words and their clusters Word Clusters suite if it replaces X. For example, the contexts for the word “Olympics” in the above sentence include the following paths: software, network, wireless, ... Context of “Olympics” rooms, bathrooms, restrooms, ... meeting room, conference room, ... ghost X rabbit, squirrel, duck, elephant, frog, ... X X huge, larger, vast, significant, ... 3.2 Contexts 1988 X host X In the clustering generated by CBC, a word may belong to multiple clusters. The clusters to which a word belongs"
E06-1050,J90-1003,0,\N,Missing
E06-1050,J92-4003,0,\N,Missing
E06-1050,C98-2122,1,\N,Missing
E09-1076,P08-1082,0,0.0304011,"rned components alone, although future versions of the system use non-learned components in addition to learned components (Prager et al., 2003). The JAVELIN I system (Nyberg et al., 2005) uses a SVM during the answer/information extraction phase. Although learning is applied in many QA tasks, very few QA systems rely solely on learning. Compositional approaches, in which multiple distinct QA techniques are combined, also show promise for improving QA performance. Echihabi et al. (2003) use three separate answer extraction agents and combine the output scores with a maximum entropy re-ranker. Surdeanu et al. (2008) explore preference ranking for advice or “how to” questions in which a unique correct answer is preferred over all other candidates. Their focus is on complex-answer questions in addition to the use of a collection of user-generated answers rather than answer typing. However, their use of preference ranking mirrors the techniques we describe here in which the relative difference between two candidates at different ranks is more important than the individual candidates. Acknowledgments We would like to thank Debra Shiau for her assistance annotating training and test data and the anonymous rev"
E09-1076,C02-1150,0,0.0403144,"ecision Mean Reciprocal Rank (MRR) 0.9 0.1 First Correct Answer First Appropriate Candidate 0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Recall 100 training examples and so therefore additional questions simply repeat the same information. Requiring only a relatively small number of training examples means that an effective model can be learned with relatively little input in the form of question-answer pairs or annotated candidate lists. 6 the lowest error rate in their QA system, it is not clear how much benefit is obtained from using a relatively coarse-grained set of classes. The approach of Li and Roth (2002) is similar in that it uses learning for answer type detection. They make use of multi-class learning with a Sparse Network of Winnows (SNoW) and a two-layer class hierarchy comprising a total of fifty possible answer types. These finer-grained classes are of more use when computing a notion of appropriateness, although one major drawback is that no entity tagger is discussed that can identify these types in text. Li and Roth (2002) also rely on a rigid set of classes and so run the risk of encountering a new question of an unseen type. Pinchak and Lin (2006) present an alternative in which th"
E09-1076,U04-1002,0,0.0682517,"Missing"
E09-1076,E06-1050,1,0.0873161,"ordering information may still be present to correctly identify appropriate candidates. To apply preference ranking to answer typing, we learn a model over a set of questions q1 , ..., qn . Each question qi has a list of appropriate candidate answers a(i,1) , ..., a(i,u) and a list of inappropriate candidate answers b(i,1) , ..., b(i,v) . The partial ordering r is simply the set ∀i, j, k : {a(i,j) &lt;r b(i,k) } Table 1: Feature templates Pattern E(t, c) C(t, c) C(t0 , c) P C(t, c0 ) t0 c0 S(t) These features are mostly based on question contexts, and are briefly summarized in Table 1. Following Pinchak and Lin (2006), all of our features are derived from a limited corpus (AQUAINT); large-scale text resources are not required for our model to perform well. By restricting ourselves to relatively small corpora, we believe that our approach will easily transfer to other domains or languages (provided parsing resources are available). To address the sparseness of question contexts, we remove lexical elements from question context paths. This removal is performed after feature values are obtained for the fully lexicalized path; the removal of lexical elements simply allows many similar paths to share a single l"
H01-1046,P93-1016,1,0.634692,"wly acquired semantic and statistical knowledge, which allows the parser to better resolve systematic syntactic ambiguities, removing unlikely parts of speech. Our hypothesis is that this will result in higher quality parse trees, which in turn allows extraction of higher quality semantic and statistical knowledge in the second and later cycles. LaTaT is a Language and Text Analysis Toolset that demonstrates this iterative learning process. The main components in the toolset consist of the following: 2. Minipar Minipar is a principle-based English parser (Berwick et al, 1991). Like Principar (Lin, 1993), Minipar represents its grammar as a network where nodes represent grammatical categories and links represent types of syntactic (dependency) relationships. The grammar network consists of 35 nodes and 59 links. Additional nodes and links are created dynamically to represent subcategories of verbs. Minipar employs a message passing algorithm that essentially implements distributed chart parsing. Instead of maintaining a single chart, each node in the grammar network maintains a chart containing partially built structures belonging to the grammatical category represented by the node. The gramm"
H01-1046,P97-1008,0,\N,Missing
J95-2005,J94-4004,1,0.852938,"quently apply filters in order to eliminate those structures that violate GB principles. (See, for example, Abney 1989; Correa 1991; Dorr 1993; Fong 1991.) The current approach provides an alternative to filter-based designs that avoids these difficulties by applying principles to descriptions of structures without actually building the structures themselves. Our approach is similar to that of Lin (1993) in that structure-building is deferred until the descriptions satisfy all principles; however, the current approach differs in that it provides a parameterization mechanism along the lines of Dorr (1994) that allows the system to be ported to languages other than English. We focus particularly on the problem of processing head-final languages such as Korean. We are currently incorporating the parser into a machine translation (MT) system called PRINCITRAN. l In general, parsers of existing principle-based interlingual MT systems are exceedingly inefficient, since they tend to adopt the filter-based paradigm. We combine the benefits of the message-passing paradigm with the benefits of the parameterized approach to build a more efficient, but easily extensible system, that will ultimately be us"
J95-2005,P93-1016,1,0.922789,"-based parsing approaches is that they generally adopt a filter-based paradigm. These approaches typically generate all possible candidate structures of the sentence that satisfy X theory, and then subsequently apply filters in order to eliminate those structures that violate GB principles. (See, for example, Abney 1989; Correa 1991; Dorr 1993; Fong 1991.) The current approach provides an alternative to filter-based designs that avoids these difficulties by applying principles to descriptions of structures without actually building the structures themselves. Our approach is similar to that of Lin (1993) in that structure-building is deferred until the descriptions satisfy all principles; however, the current approach differs in that it provides a parameterization mechanism along the lines of Dorr (1994) that allows the system to be ported to languages other than English. We focus particularly on the problem of processing head-final languages such as Korean. We are currently incorporating the parser into a machine translation (MT) system called PRINCITRAN. l In general, parsers of existing principle-based interlingual MT systems are exceedingly inefficient, since they tend to adopt the filter"
J95-2005,J92-1004,0,0.0191807,"(1993) for more details. 3.1 X Theory The central idea behind X theory is that a phrasal constituent has a layered structure. Every phrasal constituent is considered to have a head (X° = X), which determines the 2 For the purpose of readability, we have omitted integer id's in the graphical representation of the grammar network. Linear ordering is indicated by the starting points of links. For example, C precedes IP in the English network of Figure 1. 3 The idea of constraint application through feature passing among nodes is analogous to techniques applied in the TINA spoken language system (Seneff 1992) except that, in our design, the grammar network is a static data structure; it is not dynamically modified during the parsing process. Thus, we achieve a reduction space requirements. Moreover, our design achieves a reduction in time requirements because we do not retrieve a structure until the resulting parse descriptions satisfy all the network constraints. 257 Computational Linguistics Volume 21, Number 2 properties of the phrase containing it. A phrase potentially contains a complement, resulting in a one-bar level (X = Xbar) projection; it may also contain a specifier (or modifier), resu"
J95-2005,W89-0208,0,\N,Missing
J99-2008,J91-1002,0,0.14624,"Missing"
lin-etal-2010-new,N04-1043,0,\N,Missing
lin-etal-2010-new,sekine-dalwani-2010-ngram,1,\N,Missing
lin-etal-2010-new,C08-3010,1,\N,Missing
lin-etal-2010-new,1999.tc-1.8,0,\N,Missing
lin-etal-2010-new,J93-2004,0,\N,Missing
lin-etal-2010-new,S07-1044,0,\N,Missing
lin-etal-2010-new,N07-2005,1,\N,Missing
lin-etal-2010-new,J92-4003,0,\N,Missing
lin-etal-2010-new,J03-3005,0,\N,Missing
lin-etal-2010-new,P08-1068,0,\N,Missing
lin-etal-2010-new,P03-1059,0,\N,Missing
lin-etal-2010-new,A00-1031,0,\N,Missing
lin-etal-2010-new,P01-1005,0,\N,Missing
lin-etal-2010-new,W05-0603,0,\N,Missing
lin-etal-2010-new,P09-1116,1,\N,Missing
lin-etal-2010-new,Y09-1024,1,\N,Missing
lin-etal-2010-new,U08-1008,0,\N,Missing
lin-etal-2010-new,I05-2018,0,\N,Missing
lin-etal-2010-new,W04-3205,0,\N,Missing
lin-etal-2010-new,J03-3001,0,\N,Missing
lin-etal-2010-new,U07-1008,0,\N,Missing
M93-1022,M93-1009,0,0.0139222,"swapping due to some known memory leaks . I The same 25-MHz CPU as SPARCstation SLC 26 3 1 Lexica l Analyzer Input text b Grammar ---♦► PrincipleParser Network O dynamic data static data processing module non-critica l data flow Parse forest Semantic Analyzer Semantic Network Templates ` . I critical data flow not currentl y include d 1 ecagnizcr Plan Scenarios Template Filler Filled tinplate s Figure 1 : The System Architectur e SYSTEM ARCHITECTUR E Figure 1 shows the NUBA system architecture . NUBA has the following innovative aspects, compared with the generic information extraction system [2] . • The lexical analyzer creates a set, instead of a sequence, of lexical items . This means that the surfac e strings of the lexical items may overlap . • The semantic analyzer takes as inputs, the set of lexical items and a shared packed parse forest, rather than a parse tree or fragments of a parse tree of the input sentence. Further more, the parse forest is optional, i .e ., the semantic analyzer is able to proceed without syntactic analysis . In fact, our official MUC-5 system does not include a parser . • Unlike many other systems, semantic interpretation in NUBA is neither rule-based"
M93-1022,P88-1012,0,0.0265925,"is, to understan d is to seek the best explanation of how the inputs are coherently related . From this abductive perspective , the goal of the parser is to explain how the words in a sentence are structured according to syntactic relationships ; the goal of the semantic interpreter is to find the semantic relationships among the content word s in a sentence ; the goal discourse analyzer is to show how the events mentioned in the sentences fit togethe r to form a coherent plan . Although the abductive formulation of natural language understanding tasks results in significant simpli fications [4], the computational complexity of abductive inference presents a serious problem . Our solution to this problem is obvious abduction [6], a model of abductive inference that covers the kinds of abductiv e inferences people perform without apparent effort, such as parsing, plan recognition, and diagnosis . Obvious abduction uses a network to represent the domain knowledge . Observations correspond to nodes in th e network annotated with a set of attribute-value pairs . An explanation of the observations is a generalize d subtree of the network that connects all the observations . This connectio"
M93-1022,P93-1016,1,0.680071,"e sentence . When a negation word (e .g ., not, no, except) is encountered, all the lexical items following the word are removed . City name recognition : The lexicon contains all the country names and the provinces for several countrie s that are most frequently mentioned in the training corpus . When a country or province name is preceded by a sequence of capitalized unknown words, the sequence is assumed to be a city name . Determination of location of entities : Locations of entities are determined if they appear either befor e or after the entity in the text . PRINCIPLE-BASED PARSIN G In [7], the author presented an efficient, principle-based parser . The parser encodes the Government an d Binding (GB) theory in a network . The nodes in the network represent grammatical categories, such as NP, VP, etc . The links in the network represent dominance relationships between the categories . The G B principles are represented as constraints attached to nodes and links in the network . The lexical items ar e mapped into nodes in the network, annotated with attribute values . The algorithm for obvious abduction i s used to find connections between the words in the network, which are cons"
M93-1022,P92-1024,0,0.0189345,"y keywords . The problem with this approach is that there is no principled method for controlling the interaction o f multiple frames that may be triggered by the same word or the same set of words . Further more, this approach often results in complex frame definitions that are difficult to port to another domain . Semantic interpretation in NUBA is syntax-constrained in the sense that the semantic structure mus t be consistent with syntactic structure. The notion of structural-consistency between semantic and syntacti c structures is similar to the structural-consistency between parse trees [9] . Definition 6 .1 (Span) . An integer interval [i, j] is said to be a span of a sentence if there exists a parse tree and a node n in the parse tree such that the i 'th to j'th word in a sentence is dominated exactly by a consecutive subtrees of n . The difference between the notion of span here and [9] is that the latter requires that a span consists of al l the words that are dominated by a single non-terminal symbol . Figure 3 shows the spans in a parse tree . The spans in a parse forest is the unions of the spans in the trees in the forest . Semantic interpretation in NUBA is based on : D"
M95-1010,J95-2005,1,\N,Missing
M95-1010,C94-1079,1,\N,Missing
M95-1010,J94-2006,0,\N,Missing
M95-1010,J95-2003,0,\N,Missing
M95-1010,M93-1022,1,\N,Missing
M95-1010,P93-1016,1,\N,Missing
M98-1006,J93-1003,0,0.0371197,"ansport 3 5.63 trainer 2 5.50 Coast guard 3 5.43 Advanced Tactical Fighter 1 5.31 reconnaissance 2 5.12 Qantas 1 5.05 Pan American 1 5.05 training 3 4.97 United Express 1 4.85 Gulfstream 1 4.85 Swissair 1 4.69 PSA 1 4.69 ANA 1 4.69 ground attack 1 4.54 NUM-seat 1 4.21 Alitalia 1 4.12 Lufthansa 1 3.96 PAL 1 3.89 KLM 1 3.89 NUM Syrian 1 3.76 whirlpool 1 3.03 8 2. Retrieve words from the collocation database that were also used as the prenominal modi er of jet"" (shown in Table 5). Freq is the frequency of the word in the context, LogL is the log likelihood ratio between the word and the context [1]. 3. Retrieve the similar words of  ghter"" from an automatically generated thesaurus: jet 0.15; guerrilla 0.14; aircraft 0.12; rebel 0.11; bomber 0.11; soldier 0.11; troop 0.10; plane 0.10; missile 0.09; force 0.09; militia 0.09; helicopter 0.09; leader 0.08; civilian 0.07; faction 0.07; pilot 0.07; airplane 0.07; insurgent 0.07; commander 0.06; tank 0.06; airliner 0.05; militant 0.05; marine 0.05; transport 0.05; reconnaissance 0.05; prisoner 0.05; artillery 0.05; army 0.05; stealth 0.05; victim 0.05; terrorist 0.05; weapon 0.04; rocket 0.04; resistance 0.04; rioter 0.04; gunboat 0.04; colla"
M98-1006,P93-1016,1,0.721268,"cted from the sentence I have a brown dog"" are: (have V:subj:N I) (have V:comp1:N dog) (dog N:jnab:A brown) (dog N:det:D a) (I N:r-subj:V have) (dog N:r-comp1:V have) (brown A:r-jnab:N dog) (a D:r-det:N dog) 1 The identi ers for the dependency types are explained in Table 1. Label Table 1: Dependency types N:det:D N:jnab:A N:nn:N V:comp1:N V:subj:N V:jvab:A Relationship between: a noun and its determiner a noun and its adjectival modi er a noun and its nominal modi er a verb and its noun object a verb and its subject a verb and its adverbial modi er We used MINIPAR, a descendent of PRINCIPAR [2], to parse a text corpus that is made up of 55-million-word Wall Street Journal and 45-million-word San Jose Mercury. Two steps were taken to reduce the number of errors in the parsed corpus. Firstly, only sentences with no more than 25 words are fed into the parser. Secondly, only complete parses are included in the parsed corpus. The 100 million word text corpus is parsed in about 72 hours on a Pentium 200 with 80MB memory. There are about 22 million words in the parse trees. Figure 1 shows an example entry in the resulting collocation database. Each entry contains of all the dependency trip"
M98-1006,P97-1009,1,0.861623,"Missing"
M98-1006,P98-2127,1,0.401642,"Missing"
M98-1006,C98-2122,1,\N,Missing
N03-2017,J00-1004,0,0.0301127,"Missing"
N03-2017,P03-1012,1,0.835917,"rithm uses a best-first search (with fixed beam width and agenda size) to find an alignment that maximizes P (A|E, F ). A state in this search space is a partial alignment. A transition is defined as the addition of a single link to the current state. The algorithm computes P (A|E, F ) based on statistics obtained from a word-aligned corpus. We construct the initial corpus with a system that is similar to the φ2 method. The algorithm then re-aligns the corpus and trains again for three iterations. We will refer to this as the P (A|E, F ) method. The details of this algorithm are described in (Cherry and Lin, 2003). We trained our alignment programs with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. Both the training and testing sentences are from the Hansard corpus. We parsed the training and testing corpora with Minipar.1 We adopted the evaluation methodology in (Och and Ney, 2000), which defines three metrics: precision, recall and alignment error rate (AER). Table 1 shows the results of our experiments. The first four rows correspond to the methods described above. As a reference point, we also provide the results reported in (Och an"
N03-2017,dorr-etal-2002-duster,0,0.0312102,"Missing"
N03-2017,W02-1039,0,0.557949,"alignment can be represented as a binary relation A in [1, l] × [1, m]. A pair (i, j) is in A if ei and fj are a translation (or part of a translation) of each other. We call such pairs links. In Figure 2, the links in the alignment are represented by dashed lines. comp det subj obj subj det pre aux det The reboot causes the host to discover all the devices 1 1 2 Suite à after to 2 3 3 4 4 5 6 5 6 7 7 8 9 10 8 9 10 11 la réinitialisation , l&apos; hôte repère tous les périphériques the reboot the host locate all the peripherals Figure 2: An example pair of aligned sentence The cohesion constraint (Fox, 2002) uses the dependency tree TE (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Let TE (ei ) be the subtree of TE rooted at ei . The phrase span of ei , spanP (ei , TE , A), is the image of the English phrase headed by ei in F given a (partial) alignment A. More precisely, spanP (ei , TE , A) = [k1 , k2 ], where k1 = min{j|(u, j) ∈ A, eu ∈ TE (ei )} k2 = max{j|(u, j) ∈ A, eu ∈ TE (ei )} The head span is the image of ei itself. We define spanH (ei , TE , A) = [k1 , k2 ], where k1 = min{j|(i, j) ∈ A} k2 = max{j|(i, j) ∈ A} In Figure 2, the phrase span of the node di"
N03-2017,H91-1026,0,0.407521,"algorithms take as input an English-French sentence pair and the dependency tree of the English sentence. Both algorithms build an alignment by adding one link at a time. We implement two versions of each algorithm: one with the cohesion constraint and one without. We will describe the versions without cohesion constraint below. For the versions with cohesion constraint, it is understood that each new link must also pass the test described in Section 2. The first algorithm is similar to Competitive Linking (Melamed, 1997). We use a sentence-aligned corpus to compute the φ2 correlation metric (Gale and Church, 1991) between all English-French word pairs. For a given sentence pair, we begin with an empty alignment. We then add links in the order of their φ2 scores so that each word participates in at most one link. We will refer to this as the φ2 method. The second algorithm uses a best-first search (with fixed beam width and agenda size) to find an alignment that maximizes P (A|E, F ). A state in this search space is a partial alignment. A transition is defined as the addition of a single link to the current state. The algorithm computes P (A|E, F ) based on statistics obtained from a word-aligned corpus"
N03-2017,P97-1063,0,0.0280054,"lity of the cohesion constraint, we incorporated it into two alignment algorithms. The algorithms take as input an English-French sentence pair and the dependency tree of the English sentence. Both algorithms build an alignment by adding one link at a time. We implement two versions of each algorithm: one with the cohesion constraint and one without. We will describe the versions without cohesion constraint below. For the versions with cohesion constraint, it is understood that each new link must also pass the test described in Section 2. The first algorithm is similar to Competitive Linking (Melamed, 1997). We use a sentence-aligned corpus to compute the φ2 correlation metric (Gale and Church, 1991) between all English-French word pairs. For a given sentence pair, we begin with an empty alignment. We then add links in the order of their φ2 scores so that each word participates in at most one link. We will refer to this as the φ2 method. The second algorithm uses a best-first search (with fixed beam width and agenda size) to find an alignment that maximizes P (A|E, F ). A state in this search space is a partial alignment. A transition is defined as the addition of a single link to the current st"
N03-2017,P00-1056,0,0.0693053,"aximizes P (A|E, F ). A state in this search space is a partial alignment. A transition is defined as the addition of a single link to the current state. The algorithm computes P (A|E, F ) based on statistics obtained from a word-aligned corpus. We construct the initial corpus with a system that is similar to the φ2 method. The algorithm then re-aligns the corpus and trains again for three iterations. We will refer to this as the P (A|E, F ) method. The details of this algorithm are described in (Cherry and Lin, 2003). We trained our alignment programs with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. Both the training and testing sentences are from the Hansard corpus. We parsed the training and testing corpora with Minipar.1 We adopted the evaluation methodology in (Och and Ney, 2000), which defines three metrics: precision, recall and alignment error rate (AER). Table 1 shows the results of our experiments. The first four rows correspond to the methods described above. As a reference point, we also provide the results reported in (Och and Ney, 2000). They implemented IBM Model 4 by bootstrapping from an HMM model. The rows F→E 1 a"
N03-2017,J97-3002,0,0.458896,"Missing"
N03-2017,P01-1067,0,0.297671,"Missing"
N03-4011,P90-1034,0,0.106721,"nstrate the output of a distributional clustering algorithm called Clustering by Committee that automatically discovers word senses from text1. 1 Introduction Using word senses versus word forms is useful in many applications such as information retrieval (Voorhees 1998), machine translation (Hutchins and Sommers 1992), and question-answering (Pasca and Harabagiu 2001). The Distributional Hypothesis (Harris 1985) states that words that occur in the same contexts tend to be similar. There have been many approaches to compute the similarity between words based on their distribution in a corpus (Hindle 1990; Landauer and Dumais 1997; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin’s approach outputs the following similar words for wine and suit: wine: beer, white wine, red wine, Chardonnay, champagne, fruit, food, coffee, juice, Cabernet, cognac, vinegar, Pinot noir, milk, vodka,… suit: lawsuit, jacket, shirt, pant, dress, case, sweater, coat, trouser, claim, business suit, blouse, skirt, litigation, … The similar words of wine represent the meaning of wine. However, the similar words of suit represent a mixture of its clothing and litigat"
N03-4011,C94-1079,1,0.657444,"ss, soul, mind) ) Each entry shows the clusters to which the headword belongs along with its similarity to the cluster. The lists of words are the top-4 most similar members to the cluster centroid. Each cluster corresponds to a sense of the headword. 2 Feature Representation Following (Lin 1998), we represent each word by a feature vector. Each feature corresponds to a context in which the word occurs. For example, “sip __” is a verbobject context. If the word wine occurred in this context, the context is a feature of wine. These features are obtained by parsing a large corpus using Minipar (Lin 1994), a broad-coverage English parser. The value of the feature is the pointwise mutual information (Manning and Schütze 1999) between the feature and the word. Let c be a context and Fc(w) be the frequency count of a word w occurring in context c. The pointwise mutual information, miw,c, between c and w is defined as: miw,c = Fc ( w ) N References ∑ Fi ( w ) ∑j Fc ( j ) i × N N where N is the total frequency counts of all words and their contexts. We compute the similarity between two words wi and wj using the cosine coefficient (Salton and McGill 1983) of their mutual information vectors: ∑c miw"
N03-4011,P98-2127,1,0.759627,"clustering algorithm called Clustering by Committee that automatically discovers word senses from text1. 1 Introduction Using word senses versus word forms is useful in many applications such as information retrieval (Voorhees 1998), machine translation (Hutchins and Sommers 1992), and question-answering (Pasca and Harabagiu 2001). The Distributional Hypothesis (Harris 1985) states that words that occur in the same contexts tend to be similar. There have been many approaches to compute the similarity between words based on their distribution in a corpus (Hindle 1990; Landauer and Dumais 1997; Lin 1998). The output of these programs is a ranked list of similar words to each word. For example, Lin’s approach outputs the following similar words for wine and suit: wine: beer, white wine, red wine, Chardonnay, champagne, fruit, food, coffee, juice, Cabernet, cognac, vinegar, Pinot noir, milk, vodka,… suit: lawsuit, jacket, shirt, pant, dress, case, sweater, coat, trouser, claim, business suit, blouse, skirt, litigation, … The similar words of wine represent the meaning of wine. However, the similar words of suit represent a mixture of its clothing and litigation senses. Such lists of similar wor"
N03-4011,C98-2122,1,\N,Missing
N12-1095,E09-1010,0,0.0205909,"xplored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method can effectively recover the reference s"
N12-1095,P98-1012,0,0.0486869,"of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Q0 (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clustering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints. After defining each constraint below, we briefly describe its relevance to the translation sense clustering ta"
N12-1095,J92-4003,0,0.0504551,"anslations of the Spanish source word colocar that appear in our input dataset. Clustering with K-Means 4 In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the featu"
N12-1095,J93-2003,0,0.0638348,"with usage examples. 1 Dekang Lin Google lindek@google.com Figure 1: This excerpt from a bilingual dictionary groups English translations of the polysemous Spanish word colocar into three clusters that correspond to different word senses (Vel´azquez de la Cadena et al., 1965). Introduction The ability to learn a bilingual lexicon from a parallel corpus was an early and influential area of success for statistical modeling techniques in natural language processing. Probabilistic word alignment models can induce bilexical distributions over target-language translations of source-language words (Brown et al., 1993). However, word-to-word correspondences do not capture the full structure of a bilingual lexicon. Consider the example bilingual dictionary entry in Figure 1; in addition to enumerating the translations of a word, the dictionary author has grouped those translations into three sense clusters. Inducing such a clustering would prove useful in generating bilingual dictionaries automatically or building tools to assist bilingual lexicographers. ∗ Author was a summer intern with Google Research while conducting this research project. This paper formalizes the task of clustering a set of translation"
N12-1095,P02-1033,0,0.0598201,"k To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised so"
N12-1095,N03-1015,0,0.0600517,"translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means c"
N12-1095,N03-1017,0,0.00542878,"700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Q0 (Dom, 2001), V-measure (Ro"
N12-1095,P09-1116,1,0.590238,"statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the feature vectors of the elements in that cluster. until convergence 4.2 Monolingual Features Following Lin and Wu (2009), each word to be clustered is represented as a feature vector describing the distributional con"
N12-1095,P03-1058,0,0.199405,"sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method"
N12-1095,D07-1043,0,0.115773,"3). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). For evaluation by counting pairs, the popular metrics are the Rand Statistic and Jaccard Coefficient (Halkidi et al., 2001; Meila, 2003). Metrics based on entropy include Cluster Entropy (Steinbach et al., 2000), Class Entropy (Bakus et al., 2002), VI-measure (Meila, 2003), Q0 (Dom, 2001), V-measure (Rosenberg and Hirschberg, 2007) and Mutual Information (Xu et al., 2003). Lastly, there exist the BCubed metrics (Bagga and Baldwin, 1998), a family of metrics that decompose the clustering evaluation by estimating precision and recall for each item in the distribution. Amigo et al. (2009) compares the various clustering metrics mentioned above and their properties. They define four formal but intuitive constraints on such metrics that explain which aspects of clustering quality are captured by the different metric families. Their analysis shows that of the wide range of metrics, only BCubed satisfies those constraints. Aft"
N12-1095,D11-1095,0,0.0160154,"ons of a source word, we apply the CP algorithm (Figure 3), treating the K-Means clusters as synsets (Dt ). 5 Related Work To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and"
N12-1095,C04-1192,0,0.0406943,"task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The underlying notion of “sense” is shared across these tasks, but the way in which we use and evaluate induced senses is novel. 6 Experiments The purpose of our experiments is to assess whether our unsupervised soft K-Means clustering method can effectively reco"
N12-1095,P08-1086,0,0.114418,"with K-Means 4 In this section, we describe an unsupervised method for inducing translation sense clusters from the usage statistics of words in large monolingual and parallel corpora. Our method is language independent. 4.1 Distributed Soft K-Means Clustering As a first step, we cluster all words in the targetlanguage vocabulary in a way that relates words that have similar distributional features. Several methods exist for this task, such as the K-Means algorithm (MacQueen, 1967), the Brown algorithm (Brown et al., 1992) and the exchange algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008). We use a distributed implementation of the “soft” K-Means clustering algorithm described in Lin and Wu (2009). Given a feature vector for each element (a word type) and the number of desired clusters K, the K-Means algorithm proceeds as follows: 1. Select K elements as the initial centroids for K clusters. repeat 2. Assign each element to the top M clusters with the nearest centroid, according to a similarity function in feature space. 3. Recompute each cluster’s centroid by averaging the feature vectors of the elements in that cluster. until convergence 4.2 Monolingual Features Following Li"
N12-1095,C10-1124,0,0.0119987,"of-speech-tag distribution of 38 nouns, 10 verbs and 4 adverbs. The J→E dataset has 369 sourcewords with 319 nouns, 38 verbs and 12 adverbs. We included only these parts of speech because WordNet version 2.1 has adequate coverage for them. Most source words have 3 to 5 translations each. Monolingual features for K-Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001)"
N12-1095,W09-0210,0,0.0168583,"d to a list of up to M clusters. To predict the sense clusters for a set of translations of a source word, we apply the CP algorithm (Figure 3), treating the K-Means clusters as synsets (Dt ). 5 Related Work To our knowledge, the translation sense clustering task has not been explored previously. However, much prior work has explored the related task of monolingual word and phrase clustering. Uszkoreit and Brants (2008) uses an exchange algorithm to cluster words in a language model, Lin and Wu (2009) uses distributed K-Means to cluster phrases for various discriminative classification tasks, Vlachos et al. (2009) uses Dirichlet Process Mixture Models for verb clustering, and Sun and Korhonen (2011) uses a hierarchical Levin-style clustering to cluster verbs. Previous word sense induction work (Diab and Resnik, 2002; Kaji, 2003; Ng et al., 2003; Tufis et al., 2004; Apidianaki, 2009) relates to our work in that these approaches discover word senses automatically through clustering, even using multilingual parallel corpora. However, our task of clustering multiple words produces a different type of output from the standard word sense induction task of clustering in-context uses of a single word. The unde"
N12-1095,C96-2141,0,0.30935,"ourcewords with 319 nouns, 38 verbs and 12 adverbs. We included only these parts of speech because WordNet version 2.1 has adequate coverage for them. Most source words have 3 to 5 translations each. Monolingual features for K-Means clustering were computed from an English corpus of Web documents with 700 billion tokens of text. Bilingual features were computed from 0.78 (S→E) and 1.04 (J→E) billion tokens of parallel text, primarily extracted from the Web using automated parallel document identification (Uszkoreit et al., 2010). Word alignments were induced from the HMMbased alignment model (Vogel et al., 1996), initialized with the bilexical parameters of IBM Model 1 (Brown et al., 1993). Both models were trained using 2 iterations of the expectation maximization algorithm. Phrase pairs were extracted from aligned sentence pairs in the same manner used in phrasebased machine translation (Koehn et al., 2003). 6.2 Clustering Evaluation Metrics The quality of text clustering algorithms can be evaluated using a wide set of metrics. For evaluation by set matching, the popular measures are Purity (Zhao and Karypis, 2001) and Inverse Purity and their harmonic mean (F measure, see Van Rijsbergen (1974)). F"
N12-1095,C98-1012,0,\N,Missing
P00-1014,J95-4004,0,0.0278631,"Missing"
P00-1014,C94-2195,0,0.60927,"Missing"
P00-1014,W95-0103,0,0.849537,"textually similar words. Attachment scores are obtained using a linear combination of features of the 4-tuple. Finally, we combine the average attachment scores with the attachment score of N2 attaching to the original V and the attachment score of N2 attaching to the original N1. The proposed classification represents the attachment site that scored highest. 1 Previous Work Altmann and Steedman (1988) showed that current discourse context is often required for disambiguating attachments. Recent work shows that it is generally sufficient to utilize lexical information (Brill and Resnik, 1994; Collins and Brooks, 1995; Hindle and Rooth, 1993; Ratnaparkhi et al., 1994). One of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions (Hindle and Rooth, 1993). Training data was obtained by extracting all phrases of the form (V, N1, P, N2) from a large parsed corpus. Supervised methods later improved attachment accuracy. Ratnaparkhi et al. (1994) used a maximum entropy model considering only lexical information from within the verb phrase (ignoring N2). They experimented wit"
P00-1014,P98-2177,0,0.59429,"VScore(Wk, Pk, N2k) if Wk = Vk; otherwise Score(Wk, Pk, N2k) = NScore(Wk, Pk, N2k). Suppose that after the initial frequency table is set NScore(man, in, park) = 1.23, VScore(saw, with, telescope) = 3.65, and NScore(dog, with, telescope) = 0.35. Then, the updated cooccurrence frequencies for (man, in, park) and (saw, with, telescope) are: fr(man, in, park) = fr(saw, with, telescope) = 1.23 1.23 3.65 3.65 + 0.35 = 1.0 = 0.913 Table 3 shows the updated frequency table after the first iteration of the algorithm. The resulting database contained 8,900,000 triples. 3.2 Unambiguous Data Set As in (Ratnaparkhi, 1998), we constructed a training data set consisting of only unambiguous attachments of the form (V, P, N2) and (N1, P, N2). We only extract a 3-tuple from a sentence when our program finds no alternative attachment site for its preposition. Each extracted 3-tuple is assigned a frequency count of 1. For example, in the previous sentence, (man, in, park) is extracted since it contains only one attachment site; (dog, with, telescope) is not extracted since with has an alternative attachment site. The resulting database contained 4,400,000 triples. 4 Classification Model Roth (1998) presented a unifie"
P00-1014,P99-1041,1,0.318605,"ion P, we increment the frequency of the corresponding 3-tuple by 1/k. For example, Table 2 shows the initial cooccurrence frequency table for the corresponding 3-tuples of the above sentence. 3Available at www.cs.ualberta.ca/~lindek/minipar.htm. P N2 FREQUENCY man in park 1.0 saw with telescope 0.5 dog with telescope 0.5 Table 3. Co-occurrence frequency table entries for A man in the park saw a dog with a telescope after one iteration. Training Data Extraction We parsed a 125-million word newspaper corpus with Minipar3, a descendent of Principar (Lin, 1994). Minipar outputs dependency trees (Lin, 1999) from the input sentences. For example, the following sentence is decomposed into a dependency tree: V OR N1 V OR N1 P N2 FREQUENCY man in park saw with telescope 0.913 dog with telescope 0.087 1.0 In the following iterations of the algorithm, we update the frequency table as follows. For each k possible attachment site of a preposition P, we refine its attachment score using the formulas described in Section 4: VScore(Vk, Pk, N2k) and NScore(N1k, Pk, N2k). For any tuple (Wk, Pk, N2k), where Wk is either Vk or N2k, we update its frequency as: fr(Wk , Pk , N 2 )= k Score (Wk , Pk , N 2 k ) ∑i ="
P00-1014,P98-2127,1,0.689557,"kid 22, patient 7, refugee 2, rider 1, Russian 1, shark 2, something 19, We 239, wolf 5, ... salad: adj-modifier: assorted 1, crisp 4, fresh 13, good 3, grilled 5, leftover 3, mixed 4, olive 3, prepared 3, side 4, small 6, special 5, vegetable 3, ... object-of: add 3, consume 1, dress 1, grow 1, harvest 2, have 20, like 5, love 1, mix 1, pick 1, place 3, prepare 4, return 3, rinse 1, season 1, serve 8, sprinkle 1, taste 1, test 1, Toss 8, try 3, ... Figure 1. Excepts of entries in the collocation database for eat and salad. Table 1. The top 20 most similar words of eat and salad as given by (Lin, 1998b). WORD SIMILAR WORDS (WITH SIMILARITY SCORE) EAT cook 0.127, drink 0.108, consume 0.101, feed 0.094, taste 0.093, like 0.092, serve 0.089, bake 0.087, sleep 0.086, pick 0.085, fry 0.084, freeze 0.081, enjoy 0.079, smoke 0.078, harvest 0.076, love 0.076, chop 0.074, sprinkle 0.072, Toss 0.072, chew 0.072 SALAD soup 0.172, sandwich 0.169, sauce 0.152, pasta 0.149, dish 0.135, vegetable 0.135, cheese 0.132, dessert 0.13, entree 0.121, bread 0.116, meat 0.116, chicken 0.115, pizza 0.114, rice 0.112, seafood 0.11, dressing 0.109, cake 0.107, steak 0.105, noodle 0.105, bean 0.102 occurrence freque"
P00-1014,C94-1079,1,0.64883,"each k possible attachment site of a preposition P, we increment the frequency of the corresponding 3-tuple by 1/k. For example, Table 2 shows the initial cooccurrence frequency table for the corresponding 3-tuples of the above sentence. 3Available at www.cs.ualberta.ca/~lindek/minipar.htm. P N2 FREQUENCY man in park 1.0 saw with telescope 0.5 dog with telescope 0.5 Table 3. Co-occurrence frequency table entries for A man in the park saw a dog with a telescope after one iteration. Training Data Extraction We parsed a 125-million word newspaper corpus with Minipar3, a descendent of Principar (Lin, 1994). Minipar outputs dependency trees (Lin, 1999) from the input sentences. For example, the following sentence is decomposed into a dependency tree: V OR N1 V OR N1 P N2 FREQUENCY man in park saw with telescope 0.913 dog with telescope 0.087 1.0 In the following iterations of the algorithm, we update the frequency table as follows. For each k possible attachment site of a preposition P, we refine its attachment score using the formulas described in Section 4: VScore(Vk, Pk, N2k) and NScore(N1k, Pk, N2k). For any tuple (Wk, Pk, N2k), where Wk is either Vk or N2k, we update its frequency as: fr(Wk"
P00-1014,H94-1048,0,0.766158,"ained using a linear combination of features of the 4-tuple. Finally, we combine the average attachment scores with the attachment score of N2 attaching to the original V and the attachment score of N2 attaching to the original N1. The proposed classification represents the attachment site that scored highest. 1 Previous Work Altmann and Steedman (1988) showed that current discourse context is often required for disambiguating attachments. Recent work shows that it is generally sufficient to utilize lexical information (Brill and Resnik, 1994; Collins and Brooks, 1995; Hindle and Rooth, 1993; Ratnaparkhi et al., 1994). One of the earliest corpus-based approaches to prepositional phrase attachment used lexical preference by computing co-occurrence frequencies (lexical associations) of verbs and nouns with prepositions (Hindle and Rooth, 1993). Training data was obtained by extracting all phrases of the form (V, N1, P, N2) from a large parsed corpus. Supervised methods later improved attachment accuracy. Ratnaparkhi et al. (1994) used a maximum entropy model considering only lexical information from within the verb phrase (ignoring N2). They experimented with both word features and word class features, their"
P00-1014,W97-0109,0,0.750863,"Missing"
P00-1014,J93-1005,0,\N,Missing
P00-1014,C98-2122,1,\N,Missing
P00-1014,C98-2172,0,\N,Missing
P03-1012,J00-1004,0,0.0884703,"e translation methods, which also employ modular features. Maximum entropy can be used to improve IBM-style translation probabilities by using features, such as improvements to P (f |e) in (Berger et al., 1996). By the same token we can use maximum entropy to improve our estimates of P (lk |eik , fjk , Ck ). We are currently investigating maximum entropy as an alternative to our current feature model which assumes conditional independence among features. 6.2 Grammatical Constraints There have been many recent proposals to leverage syntactic data in word alignment. Methods such as (Wu, 1997), (Alshawi et al., 2000) and (Lopez et al., 2002) employ a synchronous parsing procedure to constrain a statistical alignment. The work done in (Yamada and Knight, 2001) measures statistics on operations that transform a parse tree from one language into another. 7 Future Work The alignment algorithm described here is incapable of creating alignments that are not one-to-one. The model we describe, however is not limited in the same manner. The model is currently capable of creating many-to-one alignments so long as the null probabilities of the words added on the “many” side are less than the probabilities of the lin"
P03-1012,J96-1002,0,0.0547202,"Missing"
P03-1012,J93-2003,0,0.195679,"colinc,lindek}@cs.ualberta.ca Abstract Word alignment plays a crucial role in statistical machine translation. Word-aligned corpora have been found to be an excellent source of translation-related knowledge. We present a statistical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment. 1 Introduction Word alignments were first introduced as an intermediate result of statistical machine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure t"
P03-1012,carbonell-etal-2002-automatic,0,0.022999,"ical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment. 1 Introduction Word alignments were first introduced as an intermediate result of statistical machine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that once a baseline alignment has been created,"
P03-1012,J93-1003,0,0.0605019,"hine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that once a baseline alignment has been created, one can improve results by using a refined scoring metric that is based on the alignment. For example Melamed uses competitive linking along with an explicit noise model in (Melamed, 2000) to produce a new scoring metric, which in turn creates better alignments. In this paper, we present a simple, flexible, statistical model that is de"
P03-1012,W02-1039,0,0.0542962,"linguistic intuitions. 3.1 det det pre det the host discovers all the devices Constraints The reader will note that our alignment model as described above has very few factors to prevent undesirable alignments, such as having all French words align to the same English word. To guide the model to correct alignments, we employ two constraints to limit our search for the most probable alignment. The first constraint is the one-to-one constraint (Melamed, 2000): every word (except the null words e0 and f0 ) participates in exactly one link. The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Given the dependency tree TE , the alignment can induce a dependency tree for F (Hwa et al., 2002). The cohesion constraint requires that this induced dependency tree does not have any crossing dependencies. The details about how the cohesion constraint is implemented are outside the scope of this paper.3 Here we will use a simple example to illustrate the effect of the constraint. Consider the partial alignment in Figure 2. When the system attempts to link of and de, the new link will i"
P03-1012,H91-1026,0,0.764885,"roduced as an intermediate result of statistical machine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that once a baseline alignment has been created, one can improve results by using a refined scoring metric that is based on the alignment. For example Melamed uses competitive linking along with an explicit noise model in (Melamed, 2000) to produce a new scoring metric, which in turn creates better alignments. In this paper, we present a si"
P03-1012,P02-1050,0,0.0142299,"alignments, such as having all French words align to the same English word. To guide the model to correct alignments, we employ two constraints to limit our search for the most probable alignment. The first constraint is the one-to-one constraint (Melamed, 2000): every word (except the null words e0 and f0 ) participates in exactly one link. The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Given the dependency tree TE , the alignment can induce a dependency tree for F (Hwa et al., 2002). The cohesion constraint requires that this induced dependency tree does not have any crossing dependencies. The details about how the cohesion constraint is implemented are outside the scope of this paper.3 Here we will use a simple example to illustrate the effect of the constraint. Consider the partial alignment in Figure 2. When the system attempts to link of and de, the new link will induce the dotted dependency, which crosses a previously induced dependency between service and donn´ees. Therefore, of and de will not be linked. mod pcomp det nn the status of the data service l' état du s"
P03-1012,O97-4004,0,0.0207898,"e status of the data service l' état du service de données Figure 2: An Example of Cohesion Constraint 3.2 obj subj Features In this section we introduce two types of features that we use in our implementation of the probability model described in Section 2. The first feature 1 2 3 4 5 6 1 2 3 4 5 6 l' hôte repère tous les périphériques the host locate all the peripherals Figure 3: Feature Extraction Example type f ta concerns surrounding links. It has been observed that words close to each other in the source language tend to remain close to each other in the translation (Vogel et al., 1996; Ker and Change, 1997). To capture this notion, for any word pair (ei , fj ), if a link l(ei0 , fj 0 ) exists where i − 2 ≤ i0 ≤ i + 2 and j − 2 ≤ j 0 ≤ j + 2, then we say that the feature f ta (i−i0 , j −j 0 , ei0 ) is active for this context. We refer to these as adjacency features. The second feature type f td uses the English parse tree to capture regularities among grammatical relations between languages. For example, when dealing with French and English, the location of the determiner with respect to its governor4 is never swapped during translation, while the location of adjectives is swapped frequently. For"
P03-1012,1996.amta-1.13,0,0.0399712,"knowledge. We present a statistical model for computing the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment. 1 Introduction Word alignments were first introduced as an intermediate result of statistical machine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that onc"
P03-1012,J00-2004,0,0.771457,"learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that once a baseline alignment has been created, one can improve results by using a refined scoring metric that is based on the alignment. For example Melamed uses competitive linking along with an explicit noise model in (Melamed, 2000) to produce a new scoring metric, which in turn creates better alignments. In this paper, we present a simple, flexible, statistical model that is designed to capture the information present in a baseline alignment. This model allows us to compute the probability of an alignment for a given sentence pair. It also allows for the easy incorpor"
P03-1012,W01-1406,0,0.0252005,"the probability of an alignment given a sentence pair. This model allows easy integration of context-specific features. Our experiments show that this model can be an effective tool for improving an existing word alignment. 1 Introduction Word alignments were first introduced as an intermediate result of statistical machine translation systems (Brown et al., 1993). Since their introduction, many researchers have become interested in word alignments as a knowledge source. For example, alignments can be used to learn translation lexicons (Melamed, 1996), transfer rules (Carbonell et al., 2002; Menezes and Richardson, 2001), and classifiers to find safe sentence segmentation points (Berger et al., 1996). In addition to the IBM models, researchers have proposed a number of alternative alignment methods. These methods often involve using a statistic such as φ2 (Gale and Church, 1991) or the log likelihood ratio (Dunning, 1993) to create a score to measure the strength of correlation between source and target words. Such measures can then be used to guide a constrained search to produce word alignments (Melamed, 2000). It has been shown that once a baseline alignment has been created, one can improve results by usi"
P03-1012,P00-1056,0,0.591811,"ote that any sampling method that concentrates on complete, valid and high probability alignments will accomplish the same task. When collecting the statistics needed to calculate P (A|E, F ) from our initial φ2 alignment, we give each s ∈ S a uniform weight. This is reasonable, as we have no probability estimates at this point. When training from the alignments produced by our model, we normalize P (s|E, F ) so P that s∈S P (s|E, F ) = 1. We then count links and features in S according to these normalized probabilities. 5 Experimental Results We adopted the same evaluation methodology as in (Och and Ney, 2000), which compared alignment outputs with manually aligned sentences. Och and Ney classify manual alignments into two categories: Sure (S) and Possible (P ) (S⊆P ). They defined the following metrics to evaluate an alignment A: recall = |A∩S| |S| precision = alignment error rate (AER) = |A∩P | |P | |A∩S|+|A∩P | |S|+|P | We trained our alignment program with the same 50K pairs of sentences as (Och and Ney, 2000) and tested it on the same 500 manually aligned sentences. Both the training and testing sentences are from the Hansard corpus. We parsed the training Table 3: Evaluation of Features Table"
P03-1012,C96-2141,0,0.404202,"mod pcomp det nn the status of the data service l' état du service de données Figure 2: An Example of Cohesion Constraint 3.2 obj subj Features In this section we introduce two types of features that we use in our implementation of the probability model described in Section 2. The first feature 1 2 3 4 5 6 1 2 3 4 5 6 l' hôte repère tous les périphériques the host locate all the peripherals Figure 3: Feature Extraction Example type f ta concerns surrounding links. It has been observed that words close to each other in the source language tend to remain close to each other in the translation (Vogel et al., 1996; Ker and Change, 1997). To capture this notion, for any word pair (ei , fj ), if a link l(ei0 , fj 0 ) exists where i − 2 ≤ i0 ≤ i + 2 and j − 2 ≤ j 0 ≤ j + 2, then we say that the feature f ta (i−i0 , j −j 0 , ei0 ) is active for this context. We refer to these as adjacency features. The second feature type f td uses the English parse tree to capture regularities among grammatical relations between languages. For example, when dealing with French and English, the location of the determiner with respect to its governor4 is never swapped during translation, while the location of adjectives is"
P03-1012,J97-3002,0,0.62674,"based machine translation methods, which also employ modular features. Maximum entropy can be used to improve IBM-style translation probabilities by using features, such as improvements to P (f |e) in (Berger et al., 1996). By the same token we can use maximum entropy to improve our estimates of P (lk |eik , fjk , Ck ). We are currently investigating maximum entropy as an alternative to our current feature model which assumes conditional independence among features. 6.2 Grammatical Constraints There have been many recent proposals to leverage syntactic data in word alignment. Methods such as (Wu, 1997), (Alshawi et al., 2000) and (Lopez et al., 2002) employ a synchronous parsing procedure to constrain a statistical alignment. The work done in (Yamada and Knight, 2001) measures statistics on operations that transform a parse tree from one language into another. 7 Future Work The alignment algorithm described here is incapable of creating alignments that are not one-to-one. The model we describe, however is not limited in the same manner. The model is currently capable of creating many-to-one alignments so long as the null probabilities of the words added on the “many” side are less than the"
P03-1012,P01-1067,0,0.281272,"features, such as improvements to P (f |e) in (Berger et al., 1996). By the same token we can use maximum entropy to improve our estimates of P (lk |eik , fjk , Ck ). We are currently investigating maximum entropy as an alternative to our current feature model which assumes conditional independence among features. 6.2 Grammatical Constraints There have been many recent proposals to leverage syntactic data in word alignment. Methods such as (Wu, 1997), (Alshawi et al., 2000) and (Lopez et al., 2002) employ a synchronous parsing procedure to constrain a statistical alignment. The work done in (Yamada and Knight, 2001) measures statistics on operations that transform a parse tree from one language into another. 7 Future Work The alignment algorithm described here is incapable of creating alignments that are not one-to-one. The model we describe, however is not limited in the same manner. The model is currently capable of creating many-to-one alignments so long as the null probabilities of the words added on the “many” side are less than the probabilities of the links that would be created. Under the current implementation, the training corpus is one-to-one, which gives our model no opportunity to learn many"
P03-1012,2001.mtsummit-ebmt.4,0,\N,Missing
P06-1005,P89-1010,0,0.181915,"Missing"
P06-1005,C90-3063,0,0.414652,"Missing"
P06-1005,W98-1119,0,0.0107725,"d features, perhaps tied to individual words or paths. For example, we could estimate the likelihood each noun belongs to a particular gender/number class by the proportion of times this noun was labelled as the antecedent for a pronoun of this particular gender/number. Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005). We address the drawbacks of these approaches F-Score 85.4 90.4 92.2 88.0 90.3 by using coreferent paths as the assumed resolutions in the bootstrapping. Because we can vary the threshold for defining a coreferent path, we can trade-off coverage for precision. We now outline two potential uses of bootstrapping with coreferent paths: learning gender/number information (Section 4.1) and augmenting a semantic compatibility model (Section 4.2). We bootstrap this data on our automatically-parsed news corpus. The corpus comprises 85 GB of news articles taken from the worl"
P06-1005,N04-1037,0,0.0115813,"mine whether two 33 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 33–40, c Sydney, July 2006. 2006 Association for Computational Linguistics expressions corefer. Coreference is typically only allowed between nouns matching in gender and number, and not violating any intrasentential syntactic principles. Constraints can be applied as a preprocessing step to scoring candidates based on distance, grammatical role, etc., with scores developed either manually (Lappin and Leass, 1994), or through a machine-learning algorithm (Kehler et al., 2004). Constraints and preferences have also been applied together as decision nodes on a decision tree (Aone and Bennett, 1995). When previous resolution systems handle cases like (1) and (2), where no disagreement or syntactic violation occurs, coreference is therefore determined by the weighting of features or learned decisions of the resolution classifier. Without path coreference knowledge, a resolution process would resolve the pronouns in (1) and (2) the same way. Indeed, coreference resolution research has focused on the importance of the strategy for combining well known constraints and pr"
P06-1005,J94-4002,0,0.0550772,"ification task, using various constraints and preferences to determine whether two 33 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 33–40, c Sydney, July 2006. 2006 Association for Computational Linguistics expressions corefer. Coreference is typically only allowed between nouns matching in gender and number, and not violating any intrasentential syntactic principles. Constraints can be applied as a preprocessing step to scoring candidates based on distance, grammatical role, etc., with scores developed either manually (Lappin and Leass, 1994), or through a machine-learning algorithm (Kehler et al., 2004). Constraints and preferences have also been applied together as decision nodes on a decision tree (Aone and Bennett, 1995). When previous resolution systems handle cases like (1) and (2), where no disagreement or syntactic violation occurs, coreference is therefore determined by the weighting of features or learned decisions of the resolution classifier. Without path coreference knowledge, a resolution process would resolve the pronouns in (1) and (2) the same way. Indeed, coreference resolution research has focused on the importa"
P06-1005,W97-1303,0,0.0819434,"ts and preferences have also been applied together as decision nodes on a decision tree (Aone and Bennett, 1995). When previous resolution systems handle cases like (1) and (2), where no disagreement or syntactic violation occurs, coreference is therefore determined by the weighting of features or learned decisions of the resolution classifier. Without path coreference knowledge, a resolution process would resolve the pronouns in (1) and (2) the same way. Indeed, coreference resolution research has focused on the importance of the strategy for combining well known constraints and preferences (Mitkov, 1997; Ng and Cardie, 2002), devoting little attention to the development of new features for these difficult cases. The application of world knowledge to pronoun resolution has been limited to the semantic compatibility between a candidate noun and the pronoun’s context (Yang et al., 2005). We show semantic compatibility can be effectively combined with path coreference information in our experiments below. Our method for determining path coreference is similar to an algorithm for discovering paraphrases in text (Lin and Pantel, 2001). In that work, the beginning and end nodes in the paths are col"
P06-1005,P95-1017,0,0.0158453,"g of the ACL, pages 33–40, c Sydney, July 2006. 2006 Association for Computational Linguistics expressions corefer. Coreference is typically only allowed between nouns matching in gender and number, and not violating any intrasentential syntactic principles. Constraints can be applied as a preprocessing step to scoring candidates based on distance, grammatical role, etc., with scores developed either manually (Lappin and Leass, 1994), or through a machine-learning algorithm (Kehler et al., 2004). Constraints and preferences have also been applied together as decision nodes on a decision tree (Aone and Bennett, 1995). When previous resolution systems handle cases like (1) and (2), where no disagreement or syntactic violation occurs, coreference is therefore determined by the weighting of features or learned decisions of the resolution classifier. Without path coreference knowledge, a resolution process would resolve the pronouns in (1) and (2) the same way. Indeed, coreference resolution research has focused on the importance of the strategy for combining well known constraints and preferences (Mitkov, 1997; Ng and Cardie, 2002), devoting little attention to the development of new features for these diffi"
P06-1005,P01-1006,0,0.0326154,"stem does, however, outperform previously published results on these datasets. Direct comparison of our scoring system to other current top approaches is made difficult by differences in preprocessing. Ideally we would assess the benefit of our probabilistic features using the same state-of-the-art preprocessing modules employed by others such as (Yang et al., 2005) (who additionally use a search engine for compatibility scoring). Clearly, promoting competitive evaluation of pronoun resolution scoring systems by giving competitors equivalent real-world preprocessing output along the lines of (Barbu and Mitkov, 2001) remains the best way to isolate areas for system improvement. Our pronoun resolution system is part of a larger information retrieval project where resolution ac6 Results and Discussion We compare the accuracy of various configurations of our system on the ANC, AQT and MUC datasets (Table 5). We include the score from picking the noun immediately preceding the pronoun (after our hard filters are applied). Due to the hard filters and limited search window, it is not possible for our system to resolve every noun to a correct antecedent. We thus provide the performance upper bound (i.e. the prop"
P06-1005,P02-1014,0,0.679342,"nces have also been applied together as decision nodes on a decision tree (Aone and Bennett, 1995). When previous resolution systems handle cases like (1) and (2), where no disagreement or syntactic violation occurs, coreference is therefore determined by the weighting of features or learned decisions of the resolution classifier. Without path coreference knowledge, a resolution process would resolve the pronouns in (1) and (2) the same way. Indeed, coreference resolution research has focused on the importance of the strategy for combining well known constraints and preferences (Mitkov, 1997; Ng and Cardie, 2002), devoting little attention to the development of new features for these difficult cases. The application of world knowledge to pronoun resolution has been limited to the semantic compatibility between a candidate noun and the pronoun’s context (Yang et al., 2005). We show semantic compatibility can be effectively combined with path coreference information in our experiments below. Our method for determining path coreference is similar to an algorithm for discovering paraphrases in text (Lin and Pantel, 2001). In that work, the beginning and end nodes in the paths are collected, and two paths"
P06-1005,N04-1038,0,0.0323275,"llions of documents with coreference annotations. With such a set, we could extract fine-grained features, perhaps tied to individual words or paths. For example, we could estimate the likelihood each noun belongs to a particular gender/number class by the proportion of times this noun was labelled as the antecedent for a pronoun of this particular gender/number. Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005). We address the drawbacks of these approaches F-Score 85.4 90.4 92.2 88.0 90.3 by using coreferent paths as the assumed resolutions in the bootstrapping. Because we can vary the threshold for defining a coreferent path, we can trade-off coverage for precision. We now outline two potential uses of bootstrapping with coreferent paths: learning gender/number information (Section 4.1) and augmenting a semantic compatibility model (Section 4.2). We bootstrap this data on our automati"
P06-1005,J01-4004,0,0.850069,"rom the accumulated counts. The potential of the bootstrapping approach can best be appreciated by imagining millions of documents with coreference annotations. With such a set, we could extract fine-grained features, perhaps tied to individual words or paths. For example, we could estimate the likelihood each noun belongs to a particular gender/number class by the proportion of times this noun was labelled as the antecedent for a pronoun of this particular gender/number. Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005). We address the drawbacks of these approaches F-Score 85.4 90.4 92.2 88.0 90.3 by using coreferent paths as the assumed resolutions in the bootstrapping. Because we can vary the threshold for defining a coreferent path, we can trade-off coverage for precision. We now outline two potential uses of bootstrapping with coreferent paths: learning gender/number informatio"
P06-1005,P05-1021,0,0.293179,"ing of features or learned decisions of the resolution classifier. Without path coreference knowledge, a resolution process would resolve the pronouns in (1) and (2) the same way. Indeed, coreference resolution research has focused on the importance of the strategy for combining well known constraints and preferences (Mitkov, 1997; Ng and Cardie, 2002), devoting little attention to the development of new features for these difficult cases. The application of world knowledge to pronoun resolution has been limited to the semantic compatibility between a candidate noun and the pronoun’s context (Yang et al., 2005). We show semantic compatibility can be effectively combined with path coreference information in our experiments below. Our method for determining path coreference is similar to an algorithm for discovering paraphrases in text (Lin and Pantel, 2001). In that work, the beginning and end nodes in the paths are collected, and two paths are said to be similar (and thus likely paraphrases of each other) if they have similar terminals (i.e. the paths occur with a similar distribution). Our work does not need to store the terminals themselves, only whether they are from the same pronoun group. Diffe"
P06-1005,W05-0612,1,0.646086,"ps tied to individual words or paths. For example, we could estimate the likelihood each noun belongs to a particular gender/number class by the proportion of times this noun was labelled as the antecedent for a pronoun of this particular gender/number. Since no such corpus exists, researchers have used coarser features learned from smaller sets through supervised learning (Soon et al., 2001; Ng and Cardie, 2002), manually-defined coreference patterns to mine specific kinds of data (Bean and Riloff, 2004; Bergsma, 2005), or accepted the noise inherent in unsupervised schemes (Ge et al., 1998; Cherry and Bergsma, 2005). We address the drawbacks of these approaches F-Score 85.4 90.4 92.2 88.0 90.3 by using coreferent paths as the assumed resolutions in the bootstrapping. Because we can vary the threshold for defining a coreferent path, we can trade-off coverage for precision. We now outline two potential uses of bootstrapping with coreferent paths: learning gender/number information (Section 4.1) and augmenting a semantic compatibility model (Section 4.2). We bootstrap this data on our automatically-parsed news corpus. The corpus comprises 85 GB of news articles taken from the world wide web over a 1-year pe"
P06-1005,J90-1003,0,\N,Missing
P06-1102,H05-1071,0,0.0933931,"Missing"
P06-1102,W99-0613,0,0.0690117,"ies on Web search is enormous. They enable the pursuit of new search paradigms, the processing of database-like queries, and alternative methods of presenting search results. The preparation of exhaustive lists of hand-written extraction rules is impractical given the need for domainindependent extraction of many types of facts from unstructured text. In contrast, the idea of bootstrapping for relation and information extraction was first proposed in (Riloff and Jones, 1999), and successfully applied to the construction of semantic lexicons (Thelen and Riloff, 2002), named entity recognition (Collins and Singer, 1999), extraction of binary relations (Agichtein and Gravano, 2000), and acquisition of structured data for tasks such as Question Answering (Lita and Carbonell, 2004; Fleischman et al., 2003). In the context of fact extraction, the resulting iterative acquisition ∗ Work done during internships at Google Inc. 809 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809–816, c Sydney, July 2006. 2006 Association for Computational Linguistics Acquisition of contextual extraction patterns Occurrences of seed facts Extraction patterns S"
P06-1102,P03-1001,0,0.0168329,"on of exhaustive lists of hand-written extraction rules is impractical given the need for domainindependent extraction of many types of facts from unstructured text. In contrast, the idea of bootstrapping for relation and information extraction was first proposed in (Riloff and Jones, 1999), and successfully applied to the construction of semantic lexicons (Thelen and Riloff, 2002), named entity recognition (Collins and Singer, 1999), extraction of binary relations (Agichtein and Gravano, 2000), and acquisition of structured data for tasks such as Question Answering (Lita and Carbonell, 2004; Fleischman et al., 2003). In the context of fact extraction, the resulting iterative acquisition ∗ Work done during internships at Google Inc. 809 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 809–816, c Sydney, July 2006. 2006 Association for Computational Linguistics Acquisition of contextual extraction patterns Occurrences of seed facts Extraction patterns Seed facts Generalized extraction patterns Text collection Distributional similarities Acquisition of candidate facts Candidate facts Validation of patterns Occurrences of extraction patte"
P06-1102,P04-1053,0,0.0531943,"tion of patterns Occurrences of extraction patterns Validation of candidate facts Validated candidate facts Validated extraction patterns Scoring and ranking Scored candidate facts Scored extraction patterns Figure 1: Large-scale fact extraction architecture Prefix The second contribution of the paper is a method for domain-independent validation and ranking of candidate facts, based on a similarity measure of each candidate fact relative to the set of seed facts. Whereas previous studies assume clean text collections such as news corpora (Thelen and Riloff, 2002; Agichtein and Gravano, 2000; Hasegawa et al., 2004), the validation is essential for low-quality sets of candidate facts collected from noisy Web documents. Without it, the addition of spurious candidate facts to the seed set would result in a quick divergence of the iterative acquisition towards irrelevant information (Agichtein and Gravano, 2000). Furthermore, the finer-grained ranking induced by similarities is necessary in fast-growth iterative acquisition, whereas previously proposed ranking criteria (Thelen and Riloff, 2002; Lita and Carbonell, 2004) are implicitly designed for slow growth of the seed set. Infix Postfix (Irving Berlin, 1"
P06-1102,P90-1034,0,0.0341368,"few instances such as Osasuna, Crewe etc., the patterns containing an instance rather than a class will not be found to be similar to one another. In comparison, the classes and instances are equally useful in our method for generalizing patterns for fact extraction. We merge basic patterns into generalized patterns, regardless of whether the similar words belong, as classes or instances, in any external ontology. that any two numerical values with the same number of digits will overlap during matching. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al., 1993), (Grefenstette, 1994) and (Lin, 1998). Almost all of the methods represent a word by a feature vector, where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. In our approach, we define the features of a word w to be the set of words that occurred within a small window of w in a large corpus. The context window of an instance of w consists of the closest non-stopword on each side of w and the stopwords in between. The value of a"
P06-1102,P98-2127,1,0.322747,"the left (i.e., prefix) and to the right (i.e., postfix) of the seed. The pattern shown at the top of Figure 2, which contains the sequence [CL1 born CL2 00 .], illustrates the use of classes of distributionally similar words within extraction patterns. The first word class in the sequence, CL1, consists of words such as {was, is, could}, whereas the second class includes {February, April, June, Aug., November} and other similar words. The classes of words are computed on the fly over all sequences of terms in the extracted patterns, on top of a large set of pairwise similarities among words (Lin, 1998) extracted in advance from around 50 million news articles indexed by the Google search engine over three years. All digits in both patterns and sentences are replaced with a common marker, such 810 • Jethro Tull: Motley Crue 0.28, Black Crowes 0.26, Pearl Jam 0.26, Silverchair 0.26, Black Sabbath 0.26, Doobie Brothers 0.26, Judas Priest 0.26, Van Halen 0.25, Midnight Oil 0.25, Pere Ubu 0.24, Black Flag 0.24, Godsmack 0.24, Grateful Dead 0.24, Grand Funk Railroad 0.24, Smashing Pumpkins 0.24, Led Zeppelin 0.24, Aerosmith 0.24, Limp Bizkit 0.24, Counting Crows 0.24, Echo And The Bunnymen 0.24,"
P06-1102,W04-3251,0,0.0162867,"en the seed facts, acquisition of candidate facts given the extraction patterns, scoring and ranking of the patterns, and scoring and ranking of the candidate facts, a subset of which is added to the seed set of the next round. Within the existing iterative acquisition framework, our first contribution is a method for automatically generating generalized contextual extraction patterns, based on dynamically-computed classes of similar words. Traditionally, the acquisition of contextual extraction patterns requires hundreds or thousands of consecutive iterations over the entire text collection (Lita and Carbonell, 2004), often using relatively expensive or restrictive tools such as shallow syntactic parsers (Riloff and Jones, 1999; Thelen and Riloff, 2002) or named entity recognizers (Agichtein and Gravano, 2000). Comparatively, generalized extraction patterns achieve exponentially higher coverage in early iterations. The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. 1 Introduction 1.1 Alpa Jain∗ Columbia Un"
P06-1102,P93-1024,0,0.0679963,"ch as Osasuna, Crewe etc., the patterns containing an instance rather than a class will not be found to be similar to one another. In comparison, the classes and instances are equally useful in our method for generalizing patterns for fact extraction. We merge basic patterns into generalized patterns, regardless of whether the similar words belong, as classes or instances, in any external ontology. that any two numerical values with the same number of digits will overlap during matching. Many methods have been proposed to compute distributional similarity between words, e.g., (Hindle, 1990), (Pereira et al., 1993), (Grefenstette, 1994) and (Lin, 1998). Almost all of the methods represent a word by a feature vector, where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. In our approach, we define the features of a word w to be the set of words that occurred within a small window of w in a large corpus. The context window of an instance of w consists of the closest non-stopword on each side of w and the stopwords in between. The value of a feature w 0 is defined"
P06-1102,P05-1047,0,0.0242083,"are replaced with a common marker, such 810 • Jethro Tull: Motley Crue 0.28, Black Crowes 0.26, Pearl Jam 0.26, Silverchair 0.26, Black Sabbath 0.26, Doobie Brothers 0.26, Judas Priest 0.26, Van Halen 0.25, Midnight Oil 0.25, Pere Ubu 0.24, Black Flag 0.24, Godsmack 0.24, Grateful Dead 0.24, Grand Funk Railroad 0.24, Smashing Pumpkins 0.24, Led Zeppelin 0.24, Aerosmith 0.24, Limp Bizkit 0.24, Counting Crows 0.24, Echo And The Bunnymen 0.24, Cold Chisel 0.24, Thin Lizzy 0.24 [..]. To our knowledge, the only previous study that embeds similarities into the acquisition of extraction patterns is (Stevenson and Greenwood, 2005). The authors present a method for computing pairwise similarity scores among large sets of potential syntactic (subject-verb-object) patterns, to detect centroids of mutually similar patterns. By assuming the syntactic parsing of the underlying text collection to generate the potential patterns in the first place, the method is impractical on Web-scale collections. Two patterns, e.g. chairman-resign and CEO-quit, are similar to each other if their components are present in an external hand-built ontology (i.e., WordNet), and the similarity among the components is high over the ontology. Since"
P06-1102,W02-1028,0,0.747025,"g of the candidate facts, a subset of which is added to the seed set of the next round. Within the existing iterative acquisition framework, our first contribution is a method for automatically generating generalized contextual extraction patterns, based on dynamically-computed classes of similar words. Traditionally, the acquisition of contextual extraction patterns requires hundreds or thousands of consecutive iterations over the entire text collection (Lita and Carbonell, 2004), often using relatively expensive or restrictive tools such as shallow syntactic parsers (Riloff and Jones, 1999; Thelen and Riloff, 2002) or named entity recognizers (Agichtein and Gravano, 2000). Comparatively, generalized extraction patterns achieve exponentially higher coverage in early iterations. The extraction of large sets of candidate facts opens the possibility of fast-growth iterative extraction, as opposed to the de-facto strategy of conservatively growing the seed set by as few as five items (Thelen and Riloff, 2002) after each iteration. 1 Introduction 1.1 Alpa Jain∗ Columbia University New York, NY 10027 alpa@cs.columbia.edu Background The potential impact of structured fact repositories containing billions of rel"
P06-1102,A00-1031,0,\N,Missing
P06-1102,C98-2122,1,\N,Missing
P06-2014,P03-1012,1,0.947574,"alignment is said to maintain phrasal cohesion. Fox (2002) measured phrasal cohesion in gold standard alignments by counting crossings. Crossings occur when the projections of two disjoint phrases overlap. For example, Figure 1 shows a head-modifier crossing: the projection of the the tax subtree, impˆot . . . le, is interrupted by the projection of its head, cause. Alignments with no crossings maintain phrasal cohesion. Fox’s experiments show that cohesion is generally maintained for French-English, and that dependency trees produce the highest degree of cohesion among the tested structures. Cherry and Lin (2003) use the phrasal cohesion of a dependency tree as a constraint on a beam search aligner. This constraint produces a significant reduction in alignment error rate. However, as Fox (2002) showed, even in a language pair as close as French-English, there are situations where phrasal cohesion should not be maintained. These include incorrect parses, systematic violations such as not → ne . . . pas, paraphrases, and linguistic exceptions. We aim to create an alignment system that obeys cohesion constraints most of the time, but can violate them when necessary. Unfortunately, Cherry and Lin’s beam s"
P06-2014,E06-1019,1,0.863873,"Missing"
P06-2014,W02-1039,0,0.0448603,"sm, described in (Wu, 1997), is well suited for our purposes. ITGs perform string-to-string alignment, but do so through a parsing algorithm that will allow us to inform the objective function of our dependency tree. Cohesion Constraint Suppose we are given a parse tree for one of the two sentences in our sentence pair. We will refer to the parsed language as English, and the unparsed language as Foreign. Given this information, a reasonable expectation is that English phrases will move together when projected onto Foreign. When this occurs, the alignment is said to maintain phrasal cohesion. Fox (2002) measured phrasal cohesion in gold standard alignments by counting crossings. Crossings occur when the projections of two disjoint phrases overlap. For example, Figure 1 shows a head-modifier crossing: the projection of the the tax subtree, impˆot . . . le, is interrupted by the projection of its head, cause. Alignments with no crossings maintain phrasal cohesion. Fox’s experiments show that cohesion is generally maintained for French-English, and that dependency trees produce the highest degree of cohesion among the tested structures. Cherry and Lin (2003) use the phrasal cohesion of a depend"
P06-2014,H91-1026,0,0.434311,"Missing"
P06-2014,P03-1011,0,0.0426412,"syntax constraints. 2.1 3 Syntax-aware Alignment Search We require an alignment search that can find the globally best alignment under its current objective function, and can account for phrasal cohesion in this objective. IBM Models 1 and 2, HMM (Vogel et al., 1996), and weighted maximum matching alignment all conduct complete searches, but they would not be amenable to monitoring the syntactic interactions of links. The tree-to-string models of (Yamada and Knight, 2001) naturally consider syntax, but special modeling considerations are needed to allow any deviations from the provided tree (Gildea, 2003). The Inversion Transduction Grammar or ITG formalism, described in (Wu, 1997), is well suited for our purposes. ITGs perform string-to-string alignment, but do so through a parsing algorithm that will allow us to inform the objective function of our dependency tree. Cohesion Constraint Suppose we are given a parse tree for one of the two sentences in our sentence pair. We will refer to the parsed language as English, and the unparsed language as Foreign. Given this information, a reasonable expectation is that English phrases will move together when projected onto Foreign. When this occurs, t"
P06-2014,C94-1079,1,0.29748,"quency counts are determined using a sentence-aligned bitext consisting of 50K sentence pairs. Our training set for the discriminative aligners is the first 100 sentence pairs from the FrenchEnglish gold standard provided for the 2003 WPT workshop (Mihalcea and Pedersen, 2003). For evaluation we compare to the remaining 347 gold standard pairs using the alignment evaluation metrics: precision, recall and alignment error rate or AER (Och and Ney, 2003). SVM learning parameters are tuned using the 37-pair development set provided with this data. English dependency trees are provided by Minipar (Lin, 1994). The ψT vector has two new features in addition to those present in the matching system’s ψ. These features can be active only for non-terminal productions, which have the form A → [AA] |hAAi. One feature indicates an inverted production A → hAAi, while the other indicates the use of an invalid span according to a provided English dependency tree, as described in Section 3.2. These are the only features that can be active for nonterminal productions. A terminal production rl that corresponds to a link l is given that link’s features from the match110 Table 1: The effect of hard cohesion const"
P06-2014,P05-1057,0,0.296877,"n Cherry Department of Computing Science University of Alberta Edmonton, AB, Canada, T6G 2E8 colinc@cs.ualberta.ca Abstract to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft co"
P06-2014,J00-2004,0,0.187379,"itself to a soft cohesion constraint. The imperfect beam search may not be able to find the optimal alignment under a soft constraint. Furthermore, it is not clear what penalty to assign to crossings, or how to learn such a penalty from an iterative training process. The remainder of this paper will develop a complete alignment search that is aware of cohesion violations, and use discriminative learning technology to assign a meaningful penalty to those violations. unrest le malaise Figure 1: A cohesion constraint violation. actly one generator in the source. Methods like competitive linking (Melamed, 2000) and maximum matching (Taskar et al., 2005) use a one-toone constraint, where words in either sentence can participate in at most one link. Throughout this paper we assume a one-to-one constraint in addition to any syntax constraints. 2.1 3 Syntax-aware Alignment Search We require an alignment search that can find the globally best alignment under its current objective function, and can account for phrasal cohesion in this objective. IBM Models 1 and 2, HMM (Vogel et al., 1996), and weighted maximum matching alignment all conduct complete searches, but they would not be amenable to monitoring"
P06-2014,W03-0301,0,0.0160597,"nged, defined in terms of the alignment induced by y. 4. A loss-augmented ITG is used to find the max cost. Productions of the form A → e/f that correspond to links have their scores augmented as in the matching system. 5.1 Experimental setup We conduct our experiments using French-English Hansard data. Our φ2 scores, link probabilities and word frequency counts are determined using a sentence-aligned bitext consisting of 50K sentence pairs. Our training set for the discriminative aligners is the first 100 sentence pairs from the FrenchEnglish gold standard provided for the 2003 WPT workshop (Mihalcea and Pedersen, 2003). For evaluation we compare to the remaining 347 gold standard pairs using the alignment evaluation metrics: precision, recall and alignment error rate or AER (Och and Ney, 2003). SVM learning parameters are tuned using the 37-pair development set provided with this data. English dependency trees are provided by Minipar (Lin, 1994). The ψT vector has two new features in addition to those present in the matching system’s ψ. These features can be active only for non-terminal productions, which have the form A → [AA] |hAAi. One feature indicates an inverted production A → hAAi, while the other in"
P06-2014,H05-1011,0,0.692725,"t of Computing Science University of Alberta Edmonton, AB, Canada, T6G 2E8 colinc@cs.ualberta.ca Abstract to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft constraint. We"
P06-2014,J03-1002,0,0.282462,"x-margin syntactic aligner with a soft cohesion constraint. The resulting aligner is the first, to our knowledge, to use a discriminative learning method to train an ITG bitext parser. 1 Introduction Given a parallel sentence pair, or bitext, bilingual word alignment finds word-to-word connections across languages. Originally introduced as a byproduct of training statistical translation models in (Brown et al., 1993), word alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks. The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings. These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology. There have been many attempts to incorporate syntax into alignment; we will not present a complete list here. Some methods parse two flat strings at once using a bitext grammar (Wu, 1997). Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with"
P06-2014,H05-1010,0,0.604001,"g Science University of Alberta Edmonton, AB, Canada, T6G 2E8 colinc@cs.ualberta.ca Abstract to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allows us to include the constraints that would usually be imposed by a tree-to-string alignment method as a feature in our model, creating a powerful soft constraint. We add our syntactic feat"
P06-2014,C96-2141,0,0.970649,"nts for Word Alignment through Discriminative Training Dekang Lin Google Inc. 1600 Amphitheatre Parkway Mountain View, CA, USA, 94043 lindek@google.com Colin Cherry Department of Computing Science University of Alberta Edmonton, AB, Canada, T6G 2E8 colinc@cs.ualberta.ca Abstract to be very different from their IBM counterparts. They model operations that are meaningful at a syntax level, like re-ordering children, but ignore features that have proven useful in IBM models, such as the preference to align words with similar positions, and the HMM preference for links to appear near one another (Vogel et al., 1996). Recently, discriminative learning technology for structured output spaces has enabled several discriminative word alignment solutions (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Discriminative learning allows easy incorporation of any feature one might have access to during the alignment search. Because the features are handled so easily, discriminative methods use features that are not tied directly to the search: the search and the model become decoupled. In this work, we view synchronous parsing only as a vehicle to expose syntactic features to a discriminative model. This allow"
P06-2014,J97-3002,0,0.916039,"alignment has become the first step in training most statistical translation systems, and alignments are useful to a host of other tasks. The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings. These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology. There have been many attempts to incorporate syntax into alignment; we will not present a complete list here. Some methods parse two flat strings at once using a bitext grammar (Wu, 1997). Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with syntactic aligners tend 2 Constrained Alignment Let an alignment be the complete structure that connects two parallel sentences, and a link be one of the word-to-word connections that make up an alignment. All word alignment methods benefit from some set of constraints. These limit the alignment search space and encourage competition between potential links. The IBM models (Brown et al., 1993) benefit from a one-to"
P06-2014,P01-1067,0,0.680579,"ther tasks. The dominant IBM alignment models (Och and Ney, 2003) use minimal linguistic intuitions: sentences are treated as flat strings. These carefully designed generative models are difficult to extend, and have resisted the incorporation of intuitively useful features, such as morphology. There have been many attempts to incorporate syntax into alignment; we will not present a complete list here. Some methods parse two flat strings at once using a bitext grammar (Wu, 1997). Others parse one of the two strings before alignment begins, and align the resulting tree to the remaining string (Yamada and Knight, 2001). The statistical models associated with syntactic aligners tend 2 Constrained Alignment Let an alignment be the complete structure that connects two parallel sentences, and a link be one of the word-to-word connections that make up an alignment. All word alignment methods benefit from some set of constraints. These limit the alignment search space and encourage competition between potential links. The IBM models (Brown et al., 1993) benefit from a one-tomany constraint, where each target word has ex105 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 105–112, c Sydney"
P06-2014,C04-1060,0,0.0228973,"mpared to the matching system. The improved error rate is caused by gains in both precision and recall. This indicates that the invalid span feature is doing more than just ruling out links; perhaps it is de-emphasizing another, less accurate feature’s role. The SD-ITG overrides the cohesion constraint in only 41 of the 347 test sentences, so we can see that it is indeed a soft constraint: it is obeyed nearly all the time, but it can be broken when necessary. The SD-ITG achieves by far the strongest ITG alignment result reported on this French-English set; surpassing the 0.16 AER reported in (Zhang and Gildea, 2004). Training times for this system are quite low; unsupervised statistics can be collected quickly over a large set, while only the 100-sentence training The goal of this experiment is to empirically confirm that the English spans marked invalid by Section 3.2’s dependency-augmented ITG provide useful guidance to an aligner. To do so, we compare an ITG with hard cohesion constraints, an unconstrained ITG, and a weighted maximum matching aligner. All aligners use the same simple objective function. They maximize summed link values v(l), where v(l) is defined as follows for an l = (Ej , Fk ): 2 Pr"
P06-2014,J93-2003,0,\N,Missing
P08-1002,J96-1002,0,0.0251702,"Missing"
P08-1002,P06-1005,1,0.925138,"Missing"
P08-1002,W05-0406,0,0.391038,"Missing"
P08-1002,W05-0612,1,0.83745,"Missing"
P08-1002,C90-3063,0,0.488724,"Missing"
P08-1002,N07-1030,0,0.140065,"textual referents.” M¨uller (2006) summarizes the evolution of computational approaches to nonreferential it detection. In particular, note the pioneering work of Paice and Husk (1987), the inclusion of non-referential it detection in a full anaphora resolution system by Lappin and Leass (1994), and the machine learning approach of Evans (2001). There has recently been renewed interest in non-referential pronouns, driven by three primary sources. First of all, research in coreference resolution has shown the benefits of modules for general noun anaphoricity determination (Ng and Cardie, 2002; Denis and Baldridge, 2007). Unfortunately, these studies handle pronouns inadequately; judging from the decision trees and performance figures, Ng and Cardie (2002)’s system treats all pronouns as anaphoric by default. Secondly, while most pronoun resolution evaluations simply exclude non-referential pronouns, recent unsupervised approaches (Cherry and Bergsma, 2005; Haghighi and Klein, 2007) must deal with all pronouns in unrestricted text, and therefore need robust modules to 11 automatically handle non-referential instances. Finally, reference resolution has moved beyond written text into in spoken dialog. Here, non"
P08-1002,P07-2027,0,0.0347982,"ad in natural language. The es in the German “Wie geht es Ihnen” and the il in the French “S’il vous plaˆıt” are both non-referential. In pro-drop languages that may omit subject pronouns, there remains the question of whether an omitted pronoun is referential (Zhao and Ng, 2007). Although we focus on the English pronoun it, our approach should differentiate any words that have both a structural and a referential role in language, e.g. words like this, there and that (M¨uller, 2007). We believe a distributional approach could also help in related tasks like identifying the generic use of you (Gupta et al., 2007). 3.1 Definition 3.2 Context Distribution Our approach distinguishes contexts where pronouns cannot be replaced by a preceding noun phrase (non-noun-referential) from those where nouns can occur (noun-referential). Although coreference evaluations, such as the MUC (1997) tasks, also make this distinction, it is not necessarily used by all researchers. Evans (2001), for example, distinguishes between “clause anaphoric” and “pleonastic” as in the following two instances: Our method extracts the context surrounding a pronoun and determines which other words can take the place of the pronoun in th"
P08-1002,P07-1107,0,0.0913571,"Missing"
P08-1002,P90-1034,0,0.56559,"Missing"
P08-1002,2005.mtsummit-papers.11,0,0.0429568,"Missing"
P08-1002,J94-4002,0,0.916382,"ing of computational resolution of anaphora. Hobbs (1978) notes his algorithm does not handle pronominal references to sentences nor cases where it occurs in time or weather expressions. Hirst (1981, page 17) emphasizes the importance of detecting non-referential pronouns, “lest precious hours be lost in bootless searches for textual referents.” M¨uller (2006) summarizes the evolution of computational approaches to nonreferential it detection. In particular, note the pioneering work of Paice and Husk (1987), the inclusion of non-referential it detection in a full anaphora resolution system by Lappin and Leass (1994), and the machine learning approach of Evans (2001). There has recently been renewed interest in non-referential pronouns, driven by three primary sources. First of all, research in coreference resolution has shown the benefits of modules for general noun anaphoricity determination (Ng and Cardie, 2002; Denis and Baldridge, 2007). Unfortunately, these studies handle pronouns inadequately; judging from the decision trees and performance figures, Ng and Cardie (2002)’s system treats all pronouns as anaphoric by default. Secondly, while most pronoun resolution evaluations simply exclude non-refer"
P08-1002,P98-2127,1,0.291231,"Missing"
P08-1002,E06-1007,0,0.327132,"Missing"
P08-1002,P07-1103,0,0.0115707,"Missing"
P08-1002,C02-1139,0,0.504958,"ootless searches for textual referents.” M¨uller (2006) summarizes the evolution of computational approaches to nonreferential it detection. In particular, note the pioneering work of Paice and Husk (1987), the inclusion of non-referential it detection in a full anaphora resolution system by Lappin and Leass (1994), and the machine learning approach of Evans (2001). There has recently been renewed interest in non-referential pronouns, driven by three primary sources. First of all, research in coreference resolution has shown the benefits of modules for general noun anaphoricity determination (Ng and Cardie, 2002; Denis and Baldridge, 2007). Unfortunately, these studies handle pronouns inadequately; judging from the decision trees and performance figures, Ng and Cardie (2002)’s system treats all pronouns as anaphoric by default. Secondly, while most pronoun resolution evaluations simply exclude non-referential pronouns, recent unsupervised approaches (Cherry and Bergsma, 2005; Haghighi and Klein, 2007) must deal with all pronouns in unrestricted text, and therefore need robust modules to 11 automatically handle non-referential instances. Finally, reference resolution has moved beyond written text into"
P08-1002,D07-1057,0,0.0447493,"o inject sophisticated “world knowledge” into anaphora resolution. 3 Methodology work, and show it has good inter-annotator reliability (Section 4.1). We henceforth refer to non-nounreferential simply as non-referential, and thus consider the word It in both sentences (3) and (4) as non-referential. Non-referential pronouns are widespread in natural language. The es in the German “Wie geht es Ihnen” and the il in the French “S’il vous plaˆıt” are both non-referential. In pro-drop languages that may omit subject pronouns, there remains the question of whether an omitted pronoun is referential (Zhao and Ng, 2007). Although we focus on the English pronoun it, our approach should differentiate any words that have both a structural and a referential role in language, e.g. words like this, there and that (M¨uller, 2007). We believe a distributional approach could also help in related tasks like identifying the generic use of you (Gupta et al., 2007). 3.1 Definition 3.2 Context Distribution Our approach distinguishes contexts where pronouns cannot be replaced by a preceding noun phrase (non-noun-referential) from those where nouns can occur (noun-referential). Although coreference evaluations, such as the"
P08-1002,C98-2122,1,\N,Missing
P08-1061,J04-3004,0,0.0256817,"2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections between these algorithms and other related machine learning algorithms. Another approach, generative probabilistic models, are a well-studied framework that can be extremely effective. However, generative models use the EM algorithm for parameter estimation in the presence of missing labels, which is notoriously prone to getting stuck in poor local optima. Moreover, EM optimizes a marginal likelihood score that is not discriminative. Consequently, most previous work that has attempted semi-su"
P08-1061,P99-1059,0,0.0671252,"ndency trees are assumed to be projective here, which means that if there is an arc (xi → xj ), then xi is an ancestor of all the words 534 between xi and xj .1 Let Φ(X) denote the set of all the directed, projective trees that span on X. The parser’s goal is then to find the most preferred parse; that is, a projective tree, Y ∈ Φ(X), that obtains the highest “score”. In particular, one would assume that the score of a complete spanning tree Y for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair) (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005a). Given this assumption, the parsing problem reduces to find Y ∗ = arg max score(Y |X) Y ∈Φ(X) = arg max Y ∈Φ(X) X (1) score(xi → xj ) (xi →xj )∈Y where the score(xi → xj ) can depend on any measurable property of xi and xj within the sentence X. This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Eisner, 1996; Wang et al., 2005) as well as non-probabilistic models (McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models, we further assume that t"
P08-1061,C96-1058,0,0.192915,"most one. Dependency trees are assumed to be projective here, which means that if there is an arc (xi → xj ), then xi is an ancestor of all the words 534 between xi and xj .1 Let Φ(X) denote the set of all the directed, projective trees that span on X. The parser’s goal is then to find the most preferred parse; that is, a projective tree, Y ∈ Φ(X), that obtains the highest “score”. In particular, one would assume that the score of a complete spanning tree Y for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair) (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005a). Given this assumption, the parsing problem reduces to find Y ∗ = arg max score(Y |X) Y ∈Φ(X) = arg max Y ∈Φ(X) X (1) score(xi → xj ) (xi →xj )∈Y where the score(xi → xj ) can depend on any measurable property of xi and xj within the sentence X. This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Eisner, 1996; Wang et al., 2005) as well as non-probabilistic models (McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models,"
P08-1061,P02-1017,0,0.0551416,"rithms and other related machine learning algorithms. Another approach, generative probabilistic models, are a well-studied framework that can be extremely effective. However, generative models use the EM algorithm for parameter estimation in the presence of missing labels, which is notoriously prone to getting stuck in poor local optima. Moreover, EM optimizes a marginal likelihood score that is not discriminative. Consequently, most previous work that has attempted semi-supervised or unsupervised approaches to parsing have not produced results beyond the state of the art supervised results (Klein and Manning, 2002; Klein and Manning, 2004). Subsequently, alternative estimation strategies for unsupervised learning have been proposed, such as Contrastive Estimation (CE) by Smith and Eisner (2005). Contrastive Estimation is a generalization of EM, by defining a notion of learner guidance. It makes use of a set of examples (its neighborhood) that are similar in some way to an observed example, requiring the learner to move probability mass to a given example, taking only from the example’s neighborhood. Nevertheless, CE still suffers from shortcomings, including local minima. In recent years, SVMs have dem"
P08-1061,P04-1061,0,0.173137,"machine learning algorithms. Another approach, generative probabilistic models, are a well-studied framework that can be extremely effective. However, generative models use the EM algorithm for parameter estimation in the presence of missing labels, which is notoriously prone to getting stuck in poor local optima. Moreover, EM optimizes a marginal likelihood score that is not discriminative. Consequently, most previous work that has attempted semi-supervised or unsupervised approaches to parsing have not produced results beyond the state of the art supervised results (Klein and Manning, 2002; Klein and Manning, 2004). Subsequently, alternative estimation strategies for unsupervised learning have been proposed, such as Contrastive Estimation (CE) by Smith and Eisner (2005). Contrastive Estimation is a generalization of EM, by defining a notion of learner guidance. It makes use of a set of examples (its neighborhood) that are similar in some way to an observed example, requiring the learner to move probability mass to a given example, taking only from the example’s neighborhood. Nevertheless, CE still suffers from shortcomings, including local minima. In recent years, SVMs have demonstrated state of the art"
P08-1061,J93-2004,0,0.0293617,"tions. 7 Experimental Results Given a convex approach to semi-supervised structured large margin training, and an efficient training 537 algorithm for achieving a global optimum, we now investigate its effectiveness for dependency parsing. In particular, we investigate the accuracy of the results it produces. We applied the resulting algorithm to learn dependency parsers for both English and Chinese. 7.1 Experimental Design Data Sets Since we use a semi-supervised approach, both labeled and unlabeled training data are needed. For experiment on English, we used the English Penn Treebank (PTB) (Marcus et al., 1993) and the constituency structures were converted to dependency trees using the same rules as (Yamada and Matsumoto, 2003). The standard training set of PTB was spit into 2 parts: labeled training data—the first 30k sentences in section 2-21, and unlabeled training data—the remaining sentences in section 2-21. For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al., 2004) and we used the rules in (Bikel, 2004) for conversion. We also divided the standard training set into 2 parts: sentences in section 400-931 and sentences in section 1-270 are used as labeled and unla"
P08-1061,N06-1020,0,0.449364,"erative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections between these algorithms and other related machine learning algorithms. Another approach, generative probabilistic models, are a well-studied framework that can be extremely effective. However, generative models us"
P08-1061,P06-1043,0,0.152557,"erative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections between these algorithms and other related machine learning algorithms. Another approach, generative probabilistic models, are a well-studied framework that can be extremely effective. However, generative models us"
P08-1061,E06-1011,0,0.0628465,"iminative, convex, semi-supervised learning algorithm can be obtained that is applicable to large-scale problems. To demonstrate the benefits of this approach, we apply the technique to learning dependency parsers from combined labeled and unlabeled corpora. Using a stochastic gradient descent algorithm, a parsing model can be efficiently learned from semi-supervised data that significantly outperforms corresponding supervised methods. 1 Introduction Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (McDonald et al., 2005a; McDonald and Pereira, 2006; Wang et al., 2007). However, a key drawback of supervised training algorithms is their dependence on labeled data, which is usually very difficult to obtain. Perceiving the limitation of supervised learning—in particular, the heavy dependence on annotated corpora—many researchers have investigated semi-supervised learning techniques that can take both labeled and unlabeled training data as input. Following the common theme of “more data is better data” we also use both a limited labeled corpora and a plentiful unlabeled data resource. Our goal is to obtain better performance than a purely su"
P08-1061,P05-1012,0,0.800272,"st squares loss, a discriminative, convex, semi-supervised learning algorithm can be obtained that is applicable to large-scale problems. To demonstrate the benefits of this approach, we apply the technique to learning dependency parsers from combined labeled and unlabeled corpora. Using a stochastic gradient descent algorithm, a parsing model can be efficiently learned from semi-supervised data that significantly outperforms corresponding supervised methods. 1 Introduction Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (McDonald et al., 2005a; McDonald and Pereira, 2006; Wang et al., 2007). However, a key drawback of supervised training algorithms is their dependence on labeled data, which is usually very difficult to obtain. Perceiving the limitation of supervised learning—in particular, the heavy dependence on annotated corpora—many researchers have investigated semi-supervised learning techniques that can take both labeled and unlabeled training data as input. Following the common theme of “more data is better data” we also use both a limited labeled corpora and a plentiful unlabeled data resource. Our goal is to obtain better"
P08-1061,H05-1066,0,0.747067,"st squares loss, a discriminative, convex, semi-supervised learning algorithm can be obtained that is applicable to large-scale problems. To demonstrate the benefits of this approach, we apply the technique to learning dependency parsers from combined labeled and unlabeled corpora. Using a stochastic gradient descent algorithm, a parsing model can be efficiently learned from semi-supervised data that significantly outperforms corresponding supervised methods. 1 Introduction Supervised learning algorithms still represent the state of the art approach for inferring dependency parsers from data (McDonald et al., 2005a; McDonald and Pereira, 2006; Wang et al., 2007). However, a key drawback of supervised training algorithms is their dependence on labeled data, which is usually very difficult to obtain. Perceiving the limitation of supervised learning—in particular, the heavy dependence on annotated corpora—many researchers have investigated semi-supervised learning techniques that can take both labeled and unlabeled training data as input. Following the common theme of “more data is better data” we also use both a limited labeled corpora and a plentiful unlabeled data resource. Our goal is to obtain better"
P08-1061,P05-1044,0,0.0455555,"ve models use the EM algorithm for parameter estimation in the presence of missing labels, which is notoriously prone to getting stuck in poor local optima. Moreover, EM optimizes a marginal likelihood score that is not discriminative. Consequently, most previous work that has attempted semi-supervised or unsupervised approaches to parsing have not produced results beyond the state of the art supervised results (Klein and Manning, 2002; Klein and Manning, 2004). Subsequently, alternative estimation strategies for unsupervised learning have been proposed, such as Contrastive Estimation (CE) by Smith and Eisner (2005). Contrastive Estimation is a generalization of EM, by defining a notion of learner guidance. It makes use of a set of examples (its neighborhood) that are similar in some way to an observed example, requiring the learner to move probability mass to a given example, taking only from the example’s neighborhood. Nevertheless, CE still suffers from shortcomings, including local minima. In recent years, SVMs have demonstrated state of the art results in many supervised learning tasks. As a result, many researchers have put effort on developing algorithms for semi-supervised SVMs (S3VMs) (Bennett a"
P08-1061,E03-1008,0,0.0626606,"tigated in the literature (Bennett and Demiriz, 1998; Zhu et al., 2003; Altun et al., 2005; Mann and McCallum, 2007). Among the most prominent approaches are self-training, generative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections between these algorithms and other rela"
P08-1061,W04-3201,0,0.047748,"., 2005) as well as non-probabilistic models (McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models, we further assume that the score of each link in (1) can be decomposed into a weighted linear combination of features score(xi → xj ) = θ · f (xi → xj ) (2) where f (xi → xj ) is a feature vector for the link (xi → xj ), and θ are the weight parameters to be estimated during training. 3 Supervised Structured Large Margin Training Supervised structured large margin training approaches have been applied to parsing and produce promising results (Taskar et al., 2004; McDonald et al., 2005a; Wang et al., 2006). In particular, structured large margin training can be expressed as minimizing a regularized loss (Hastie et al., 2004), as shown below: 1 We assume all the dependency trees are projective in our work (just as some other researchers do), although in the real word, most languages are non-projective. min θ β ⊤ θ θ+ 2 X i (3) max(∆(Li,k , Yi ) − diff(θ, Yi , Li,k )) Li,k where Yi is the target tree for sentence Xi ; Li,k ranges over all possible alternative k trees in Φ(Xi ); diff(θ, Yi , Li,k ) = score(θ, Yi ) − score(θ, Li,k ); P score(θ, Yi ) = (xm"
P08-1061,W05-1516,1,0.887711,"nning tree Y for a given sentence, whether probabilistically motivated or not, can be decomposed as a sum of local scores for each link (a word pair) (Eisner, 1996; Eisner and Satta, 1999; McDonald et al., 2005a). Given this assumption, the parsing problem reduces to find Y ∗ = arg max score(Y |X) Y ∈Φ(X) = arg max Y ∈Φ(X) X (1) score(xi → xj ) (xi →xj )∈Y where the score(xi → xj ) can depend on any measurable property of xi and xj within the sentence X. This formulation is sufficiently general to capture most dependency parsing models, including probabilistic dependency models (Eisner, 1996; Wang et al., 2005) as well as non-probabilistic models (McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models, we further assume that the score of each link in (1) can be decomposed into a weighted linear combination of features score(xi → xj ) = θ · f (xi → xj ) (2) where f (xi → xj ) is a feature vector for the link (xi → xj ), and θ are the weight parameters to be estimated during training. 3 Supervised Structured Large Margin Training Supervised structured large margin training approaches have been applied to parsing and produce promising results (Taskar e"
P08-1061,W06-2904,1,0.882565,"(McDonald et al., 2005a). For standard scoring functions, particularly those used in non-generative models, we further assume that the score of each link in (1) can be decomposed into a weighted linear combination of features score(xi → xj ) = θ · f (xi → xj ) (2) where f (xi → xj ) is a feature vector for the link (xi → xj ), and θ are the weight parameters to be estimated during training. 3 Supervised Structured Large Margin Training Supervised structured large margin training approaches have been applied to parsing and produce promising results (Taskar et al., 2004; McDonald et al., 2005a; Wang et al., 2006). In particular, structured large margin training can be expressed as minimizing a regularized loss (Hastie et al., 2004), as shown below: 1 We assume all the dependency trees are projective in our work (just as some other researchers do), although in the real word, most languages are non-projective. min θ β ⊤ θ θ+ 2 X i (3) max(∆(Li,k , Yi ) − diff(θ, Yi , Li,k )) Li,k where Yi is the target tree for sentence Xi ; Li,k ranges over all possible alternative k trees in Φ(Xi ); diff(θ, Yi , Li,k ) = score(θ, Yi ) − score(θ, Li,k ); P score(θ, Yi ) = (xm →xn )∈Yi θ · f (xm → xn ), as shown in Sect"
P08-1061,W03-3023,0,0.122491,"efficient training 537 algorithm for achieving a global optimum, we now investigate its effectiveness for dependency parsing. In particular, we investigate the accuracy of the results it produces. We applied the resulting algorithm to learn dependency parsers for both English and Chinese. 7.1 Experimental Design Data Sets Since we use a semi-supervised approach, both labeled and unlabeled training data are needed. For experiment on English, we used the English Penn Treebank (PTB) (Marcus et al., 1993) and the constituency structures were converted to dependency trees using the same rules as (Yamada and Matsumoto, 2003). The standard training set of PTB was spit into 2 parts: labeled training data—the first 30k sentences in section 2-21, and unlabeled training data—the remaining sentences in section 2-21. For Chinese, we experimented on the Penn Chinese Treebank 4.0 (CTB4) (Palmer et al., 2004) and we used the rules in (Bikel, 2004) for conversion. We also divided the standard training set into 2 parts: sentences in section 400-931 and sentences in section 1-270 are used as labeled and unlabeled data respectively. For both English and Chinese, we adopted the standard development and test sets throughout the"
P08-1061,P95-1026,0,0.301105,"ining algorithms have been investigated in the literature (Bennett and Demiriz, 1998; Zhu et al., 2003; Altun et al., 2005; Mann and McCallum, 2007). Among the most prominent approaches are self-training, generative models, semisupervised support vector machines (S3VM), graphbased algorithms and multi-view algorithms (Zhu, 2005). Self-training is a commonly used technique for semi-supervised learning that has been ap532 Proceedings of ACL-08: HLT, pages 532–540, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics plied to several natural language processing tasks (Yarowsky, 1995; Charniak, 1997; Steedman et al., 2003). The basic idea is to bootstrap a supervised learning algorithm by alternating between inferring the missing label information and retraining. Recently, McClosky et al. (2006a) successfully applied self-training to parsing by exploiting available unlabeled data, and obtained remarkable results when the same technique was applied to parser adaptation (McClosky et al., 2006b). More recently, Haffari and Sarkar (2007) have extended the work of Abney (2004) and given a better mathematical understanding of self-training algorithms. They also show connections"
P08-1061,J04-4004,0,\N,Missing
P08-1113,D07-1090,0,0.0134055,"Missing"
P08-1113,J93-2003,0,0.0521848,"trimmed the pre-parenthesis text with a length-based constraint. The cut-off point is the first (counting from right to left) potential boundary position (see Sec. 3.2) such that C ≥ 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been 997 proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one"
P08-1113,2007.mtsummit-papers.9,0,0.411576,"is a method for mining parenthetical translations by treating text snippets containing candidate pairs as a partially parallel corpus and using a word alignment algorithm to establish the correspondences between in-parenthesis and pre-parenthesis words. This technique allows us to identify translation pairs even if they only appeared once on the entire web. As a result, we were able to obtain 26.7 million Chinese-English translation pairs from web documents in Chinese. This is over two orders of magnitude more than the number of extracted translation pairs in the previously reported results (Cao, et al. 2007). The next section presents an overview of our algorithm, which is then detailed in Sections 3 and 4. We evaluate our results in Section 5 by comparison with bilingually linked Wikipedia titles and by using the extracted pairs as additional training data in a statistical machine translation system. 2 Mining Parenthetical Translations A parenthetical translation matches the pattern: (4) precede the English term Lower Egypt. Owing to the frequency with which 下埃及 appears as a candidate, and in varying contexts, one has a good reason to believe下埃及is the correct translation of Lower Egypt. … 下游 地区"
P08-1113,J93-1003,0,0.0344834,"w consecutive sequence of words on one side to be linked to the same word on the other side. Specifically, instead of requiring both ei and fj to have no previous linkages, we only require that at least one of them be unlinked and that (suppose ei is unlinked and fj is linked to ek) none of the words between ei and ek be linked to any word other than fj. 4.2 Link scoring We used φ2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as χ2 (Zhang, S. Vogel. 2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005). The φ2 statistics for a pair of words ei and fj is computed as (ad ! bc )2 &quot;2 = (a + b )(a + c )(b + d )(c + d ) where a is the number of sentence pairs containing both ei and fj; a+b is the number of sentence pairs containing ei; a+c is the number of sentence pairs containing fj; d is the number of sentence pairs containing neither ei nor fj. The φ2 score ranges from 0 to 1. We set a threshold at 0.001, below which the φ2 scores are treated as 0. 4.3 Bias in the partially parallel corpus Since only the last few Chinese words in a can"
P08-1113,N03-1017,0,0.00662772,"Missing"
P08-1113,J00-2004,0,0.063267,"ary position (see Sec. 3.2) such that C ≥ 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been 997 proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest precision scores. A disadvantage of Competitive Linking, however, is that the alignments are restricted word-to-word alignme"
P08-1113,W01-1413,0,0.182686,"a candidate, and in varying contexts, one has a good reason to believe下埃及is the correct translation of Lower Egypt. … 下游 地区 … 中心 which is a sequence of m non-English words followed by a sequence of n English words in parentheses. In the remainder of the paper, we assume the non-English text is Chinese, but our technique works for other languages as well. There have been two approaches to finding such parenthetical translations. One is to assume that the English term e1e2…en is given and use a search engine to retrieve text snippets containing e1e2…en from predominately non-English web pages (Nagata et al, 2001, Kwok et al, 2005). Another method (Cao et al, 2007) is to go through a nonEnglish corpus and collect all instances that match the parenthetical pattern in (4). We followed the second approach since it does not require a predefined list of English terms and is amendable for extraction at large scale. In both cases, one can obtain a list of candidate pairs, where the translation of the in-parenthesis terms is a suffix of the pre-parenthesis text. The lengths and frequency counts of the suffixes have been used to determine what is the translation of the in-parenthesis term (Kwok et al, 2005). F"
P08-1113,J03-1002,0,0.00319521,"renthesis text with a length-based constraint. The cut-off point is the first (counting from right to left) potential boundary position (see Sec. 3.2) such that C ≥ 2 E + K, where C is the length of the Chinese text, E is the length of the English text in the parentheses and K is a constant (we used K=6 in our experiments). The lengths C and E are measured in bytes, except when the English text is an abbreviation (in that case, E is multiplied by 5). 4 Word Alignment Word alignment is a well-studied topic in Machine Translation with many algorithms having been 997 proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest prec"
P08-1113,H05-1010,0,0.0238294,"ked to the same word on the other side. Specifically, instead of requiring both ei and fj to have no previous linkages, we only require that at least one of them be unlinked and that (suppose ei is unlinked and fj is linked to ek) none of the words between ei and ek be linked to any word other than fj. 4.2 Link scoring We used φ2 (Gale and Church, 1991) as the link score in the modified competitive linking algorithm, although there are many other possible choices for the link scores, such as χ2 (Zhang, S. Vogel. 2005), log-likelihood ratio (Dunning, 1993) and discriminatively trained weights (Taskar et al, 2005). The φ2 statistics for a pair of words ei and fj is computed as (ad ! bc )2 &quot;2 = (a + b )(a + c )(b + d )(c + d ) where a is the number of sentence pairs containing both ei and fj; a+b is the number of sentence pairs containing ei; a+c is the number of sentence pairs containing fj; d is the number of sentence pairs containing neither ei nor fj. The φ2 score ranges from 0 to 1. We set a threshold at 0.001, below which the φ2 scores are treated as 0. 4.3 Bias in the partially parallel corpus Since only the last few Chinese words in a candidate pair are expected to be translated, there should be"
P08-1113,C04-1031,0,0.0126114,"-studied topic in Machine Translation with many algorithms having been 997 proposed (Brown et al, 1993; Och and Ney 2003). We used a modified version of one of the simplest word alignment algorithms called Competitive Linking (Melamed, 2000). The algorithm assumes that there is a score associated with each pair of words in a bi-text. It sorts the word pairs in descending order of their scores, selecting pairs based on the resultant order. A pair of words is linked if none of the two words were previously linked to any other words. The algorithm terminates when there are no more links to make. Tiedemann (2004) compared a variety of alignment algorithms and found Competitive Linking to have one of the highest precision scores. A disadvantage of Competitive Linking, however, is that the alignments are restricted word-to-word alignments, which implies that multi-word expressions can only be partially linked at best. 4.1 Dealing with multi-word alignment We made a small change to Competitive Linking to allow consecutive sequence of words on one side to be linked to the same word on the other side. Specifically, instead of requiring both ei and fj to have no previous linkages, we only require that at le"
P08-1113,D07-1106,0,0.0415382,"lable-level regularities in transliterated terms. Consider again the Shapiro example in the introduction section. There are numerous correct transliterations for the same English word, some of which are not very frequent. For example, the word 夏布洛happens to have a similar φ2 score with Shapiro as the word 流利 (fluency), which is totally unrelated to Shapiro but happened to have the same co-occurrence statistics in the (partially) parallel corpus. Previous approaches to parenthetical translations relied on specialized algorithms to deal with transliterations (Cao et al, 2007; Jiang et al, 2007; Wu and Chang, 2007). They convert Chinese words into their phonetic representations (Pinyin) and use the known transliterations in a bilingual dictionary to train a transliteration model. We adopted a simpler approach that does not require any additional resources such as pronunciation dictionaries and bilingual dictionaries. In addition to computing the φ2 scores between words, we also compute the φ2 scores of prefixes and suffixes of Chinese and English words. For both languages, the prefix of a word is defined as the first three bytes of the word and the suffix is defined as the last three bytes. Since we use"
P08-1113,W05-0829,0,0.0279943,"Missing"
P08-1113,H91-1026,0,\N,Missing
P09-1116,J92-4003,0,0.221187,"Missing"
P09-1116,W03-0423,0,0.0130528,"el that indicates whether the token at position s is a named entity as well as its type; wu is the word at position u; sfx3 is a word’s threeletter suffix; &lt;SPLç = 8ç@5 are indicators of 1 1033 http://www.reuters.com/researchandstandards/ different word types: wtp1 is true when a word is punctuation; wtp2 indicates whether a word is in lower case, upper case, or all-caps; wtp3 is true when a token is a number; wtp4 is true when a token is a hyphenated word with different capitalization before and after the hyphen. NER systems often have global features to capture discourse-level regularities (Chieu and Ng 2003). For example, documents often have a full mention of an entity at the beginning and then refer to the entity in partial or abbreviated forms. To help in recognizing the shorter versions of the entities, we maintain a history of unigram word features. If a token is encountered again, the word unigram features of the previous instances are added as features for the current instance as well. We have a total of 48 feature templates. In comparison, there are 79 templates in (Suzuki and Isozaki, 2008). Part-of-speech tags were used in the topranked systems in CoNLL 2003, as well as in many follow u"
P09-1116,W03-0425,0,0.126234,"Missing"
P09-1116,W03-0428,0,0.163456,"Missing"
P09-1116,P08-1076,0,0.575432,"ore and after the hyphen. NER systems often have global features to capture discourse-level regularities (Chieu and Ng 2003). For example, documents often have a full mention of an entity at the beginning and then refer to the entity in partial or abbreviated forms. To help in recognizing the shorter versions of the entities, we maintain a history of unigram word features. If a token is encountered again, the word unigram features of the previous instances are added as features for the current instance as well. We have a total of 48 feature templates. In comparison, there are 79 templates in (Suzuki and Isozaki, 2008). Part-of-speech tags were used in the topranked systems in CoNLL 2003, as well as in many follow up studies that used the data set (Ando and Zhang 2005; Suzuki and Isozaki 2008). Our system does not need this information to achieve its peak performance. An important advantage of not needing a POS tagger as a preprocessor is that the system is much easier to adapt to other languages, since training a tagger often requires a larger amount of more extensively annotated data than the training data for NER. 3.2 Phrase cluster features We used hard clustering with 1-word context windows for NER. Fo"
P09-1116,W03-0419,0,0.0990119,"Missing"
P09-1116,N03-1017,0,0.00560605,"Missing"
P09-1116,P08-1086,0,0.0501968,"well. The disambiguation power of phrases is also evidenced by the improvements of phrase-based machine translation systems (Koehn et. al., 2003) over word-based ones. Previous approaches, e.g., (Miller et al. 2004) and (Koo et al. 2008), have all used the Brown algorithm for clustering (Brown et al. 1992). The main idea of the algorithm is to minimize the bigram language-model perplexity of a text corpus. The algorithm is quadratic in the number of elements to be clustered. It is able to cluster tens of thousands of words, but is not scalable enough to deal with tens of millions of phrases. Uszkoreit and Brants (2008) proposed a 1030 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1030–1038, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP distributed clustering algorithm with a similar objective function as the Brown algorithm. It substantially increases the number of elements that can be clustered. However, since it still needs to load the current clustering of all elements into each of the workers in the distributed system, the memory requirement becomes a bottleneck. We present a distributed version of a much simpler K-Means clustering that allows us to"
P09-1116,P08-1068,0,0.398415,"found in the training data, it may still fire cluster-based features as long as it shares cluster assignments with some words in the labeled data. Since the clusters are obtained without any labeled data, they may not correspond directly to concepts that are useful for decision making in the problem domain. However, the supervised learning algorithms can typically identify useful clusters and assign proper weights to them, effectively adapting the clusters to the domain. This method has been shown to be quite successful in named entity recognition (Miller et al. 2004) and dependency parsing (Koo et al., 2008). In this paper, we present a semi-supervised learning algorithm that goes a step further. In addition to word-clusters, we also use phraseclusters as features. Out of context, natural language words are often ambiguous. Phrases are much less so because the words in a phrase provide contexts for one another. Consider the phrase “Land of Odds”. One would never have guessed that it is a company name based on the clusters containing Odds and Land. With phrase-based clustering, “Land of Odds” is grouped with many names that are labeled as company names, which is a strong indication that it is a co"
P09-1116,N04-1043,0,\N,Missing
P09-1116,D08-1044,0,\N,Missing
P09-1116,P08-1113,1,\N,Missing
P10-1089,D07-1090,0,0.00644439,"ularities, such as suffixes and transitivity for adjective ordering. With these benefits, systems trained on labeled data have become the dominant technology in academic NLP. There is a growing recognition, however, that these systems are highly domain dependent. For example, parsers trained on annotated newspaper text perform poorly on other genres (Gildea, 2001). While many approaches have adapted NLP systems to specific domains (Tsuruoka et al., 2005; McClosky et al., 2006; Blitzer 1 Introduction Many NLP systems use web-scale N-gram counts (Keller and Lapata, 2003; Nakov and Hearst, 2005; Brants et al., 2007). Lapata and Keller (2005) demonstrate good performance on eight tasks using unsupervised web-based models. They show web counts are superior to counts from a large corpus. Bergsma et al. (2009) propose unsupervised and supervised systems that use counts from Google’s N-gram corpus (Brants and Franz, 2006). Web-based models perform particularly well on generation tasks, where systems choose between competing sequences of output text (such as different spellings), as opposed to analysis tasks, where systems choose between abstract labels (such as part-of-speech tags or parse trees). In this wor"
P10-1089,A00-1031,0,0.0310258,"Missing"
P10-1089,D07-1021,0,0.0329993,"Missing"
P10-1089,P07-1033,0,0.0708295,"Missing"
P10-1089,W01-0521,0,0.0333006,"Missing"
P10-1089,J03-3001,0,0.0472343,"Missing"
P10-1089,W04-3111,0,0.0145372,"Missing"
P10-1089,P01-1005,0,0.0176981,"Work 80 100 1e3 Of all classifiers, L EX performs worst on all crossdomain tasks. Clearly, many of the regularities that a typical classifier exploits in one domain do not transfer to new genres. N- GM features, however, do not depend directly on training examples, and thus work better cross-domain. Of course, using web-scale N-grams is not the only way to create robust classifiers. Counts from any large auxiliary corpus may also help, but web counts should help more (Lapata and Keller, 2005). Section 6.2 suggests that another way to mitigate domaindependence is having multiple feature views. Banko and Brill (2001) argue “a logical next step for the research community would be to direct efforts towards increasing the size of annotated training collections.” Assuming we really do want systems that operate beyond the specific domains on which they are trained, the community also needs to identify which systems behave as in Figure 2, where the accuracy of the best in-domain system actually decreases with more training examples. Our results suggest better features, such as web pattern counts, may help more than expanding training data. Also, systems using webscale unlabeled data will improve automatically a"
P10-1089,D08-1107,0,0.0628976,"Missing"
P10-1089,P95-1007,0,0.622666,"Missing"
P10-1089,P07-1056,0,0.086428,"Missing"
P10-1089,lin-etal-2010-new,1,0.211312,"Missing"
P10-1089,P00-1012,0,0.0122612,"he observed adjectives such that the training pairs are maximally ordered correctly. The feature weights thus implicitly produce a linear ordering of all observed adjectives. The examples can also be regarded as rank constraints in a discriminative ranker (Joachims, 2002). Transitivity is achieved naturally in that if we correctly order pairs a ≺ b and b ≺ c in the training set, then a ≺ c by virtue of the weights on a and c. While exploiting transitivity has been shown to improve adjective ordering, there are many conflicting pairs that make a strict linear ordering of adjectives impossible (Malouf, 2000). We therefore provide an indicator feature for the pair a1 a2 , so the classifier can memorize exceptions to the linear ordering, breaking strict order transitivity. Our classifier thus operates along the lines of rankers in the preference-based setting as described in Ailon and Mohri (2008). Finally, we also have features for all suffixes of length 1-to-4 letters, as these encode useful information about adjective class (Malouf, 2000). Like the adjective features, the suffix features receive a value of +1 for adjectives in the first position and −1 for those in the second. 3 Prenominal Adjec"
P10-1089,J93-2004,0,0.0333418,"fects downstream parsers and semantic role labelers. The task is difficult because nearby POS tags can be identical in both cases. When the verb follows a noun, tag assignment can hinge on world-knowledge, i.e., the global lexical relation between the noun and verb (E.g., troops tends to be the object of stationed but the subject of vacationed).6 Web-scale N-gram data might help improve the VBN/VBD distinction by providing relational evidence, even if the verb, noun, or verbnoun pair were not observed in training data. We extract nouns followed by a VBN/VBD in the WSJ portion of the Treebank (Marcus et al., 1993), getting 23K training, 1091 development and 1130 test examples from sections 2-22, 24, and 23, respectively. For out-of-domain data, we get 21K 6.1.2 N- GM features For 1), we characterize a noun-verb relation via features for the pair’s distribution in Google V2. Characterizing a word by its distribution has a long history in NLP; we apply similar techniques to relations, like Turney (2006), but with a larger corpus and richer annotations. We extract the 20 most-frequent N-grams that contain both the noun and the verb in the pair. For each of these, we convert the tokens to POS-tags, except"
P10-1089,P06-1043,0,0.0432509,"Missing"
P10-1089,W09-0608,0,0.0286364,"Missing"
P10-1089,W03-1023,0,0.0804367,"Missing"
P10-1089,W05-0603,0,0.125127,"e and leverage other regularities, such as suffixes and transitivity for adjective ordering. With these benefits, systems trained on labeled data have become the dominant technology in academic NLP. There is a growing recognition, however, that these systems are highly domain dependent. For example, parsers trained on annotated newspaper text perform poorly on other genres (Gildea, 2001). While many approaches have adapted NLP systems to specific domains (Tsuruoka et al., 2005; McClosky et al., 2006; Blitzer 1 Introduction Many NLP systems use web-scale N-gram counts (Keller and Lapata, 2003; Nakov and Hearst, 2005; Brants et al., 2007). Lapata and Keller (2005) demonstrate good performance on eight tasks using unsupervised web-based models. They show web counts are superior to counts from a large corpus. Bergsma et al. (2009) propose unsupervised and supervised systems that use counts from Google’s N-gram corpus (Brants and Franz, 2006). Web-based models perform particularly well on generation tasks, where systems choose between competing sequences of output text (such as different spellings), as opposed to analysis tasks, where systems choose between abstract labels (such as part-of-speech tags or par"
P10-1089,D08-1050,0,0.010939,"ut text (such as different spellings), as opposed to analysis tasks, where systems choose between abstract labels (such as part-of-speech tags or parse trees). In this work, we address two natural and related questions which these previous studies leave open: 1. Is there a benefit in combining web-scale counts with the features used in state-of-theart supervised approaches? 865 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 865–874, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics et al., 2007; Daum´e III, 2007; Rimell and Clark, 2008), these techniques assume the system knows on which domain it is being used, and that it has access to representative data in that domain. These assumptions are unrealistic in many real-world situations; for example, when automatically processing a heterogeneous collection of web pages. How well do supervised and unsupervised NLP systems perform when used uncustomized, out-of-the-box on new domains, and how can we best design our systems for robust open-domain performance? Our results show that using web-scale N-gram data in supervised systems advances the state-ofthe-art performance on standa"
P10-1089,P99-1018,0,0.0506656,"Missing"
P10-1089,J06-3003,0,0.00748614,"distinction by providing relational evidence, even if the verb, noun, or verbnoun pair were not observed in training data. We extract nouns followed by a VBN/VBD in the WSJ portion of the Treebank (Marcus et al., 1993), getting 23K training, 1091 development and 1130 test examples from sections 2-22, 24, and 23, respectively. For out-of-domain data, we get 21K 6.1.2 N- GM features For 1), we characterize a noun-verb relation via features for the pair’s distribution in Google V2. Characterizing a word by its distribution has a long history in NLP; we apply similar techniques to relations, like Turney (2006), but with a larger corpus and richer annotations. We extract the 20 most-frequent N-grams that contain both the noun and the verb in the pair. For each of these, we convert the tokens to POS-tags, except for tokens that are among the most frequent 100 unigrams in our corpus, which we include in word form. We mask the noun of interest as N and the verb of interest as V. This converted N-gram is the feature label. The value is the pattern’s log-count. A high count for patterns like (N that V), (N have V) suggests the relation is a VBD, while patterns (N that were V), (N V by), (V some N) indica"
P10-1089,P07-1031,0,0.064906,"Missing"
P10-1089,P05-1021,0,0.022025,"Missing"
P10-1089,J03-3005,0,\N,Missing
P93-1016,W89-0208,0,\N,Missing
P97-1009,P94-1020,0,0.00647606,"ithm that uses the same knowledge sources to disambiguate different words. The algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts. 1 Introduction Given a word, its context and its possible meanings, the problem of word sense disambiguation (WSD) is to determine the meaning of the word in that context. WSD is useful in many natural language tasks, such as choosing the correct word in machine translation and coreference resolution. In several recent proposals (Hearst, 1991; Bruce and Wiebe, 1994; Leacock, Towwell, and Voorhees, 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1994), statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus. Yarowsky (Yarowsky, 1995) proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus, thus avoided the need to hand-annotate any examples. Most previous corpus-based WSD algorithms determine the meanings of polysemous words by exploiting their local c o n t e x t s . A basic intuition that underlies those algorithms is the"
P97-1009,P96-1006,0,0.143665,"ferent words. The algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts. 1 Introduction Given a word, its context and its possible meanings, the problem of word sense disambiguation (WSD) is to determine the meaning of the word in that context. WSD is useful in many natural language tasks, such as choosing the correct word in machine translation and coreference resolution. In several recent proposals (Hearst, 1991; Bruce and Wiebe, 1994; Leacock, Towwell, and Voorhees, 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1994), statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus. Yarowsky (Yarowsky, 1995) proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus, thus avoided the need to hand-annotate any examples. Most previous corpus-based WSD algorithms determine the meanings of polysemous words by exploiting their local c o n t e x t s . A basic intuition that underlies those algorithms is the following: (i) (2) Two different words are likely to ha"
P97-1009,P94-1019,0,0.0104459,"ther be head or rood. The p o s i t i o n indicates whether word is the head or the modifier in depen•to be defined in Section 3.1 65 dency relation. Since a word may be involved in several dependency relationships, each occurrence of a word may have multiple local contexts. The local contexts of the two nouns ""boy"" and ""dog"" in (4) are as follows (the dependency relations between nouns and their determiners are ignored): 3.1 Similarity between Two Concepts There have been several proposed measures for similarity between two concepts (Lee, Kim, and Lee, 1989; Kada et al., 1989; Resnik, 1995b; Wu and Palmer, 1994). All of those similarity measures are defined directly by a formula. We use instead an information-theoretic definition of similarity that can be derived from the following assumptions: (5) Word Local Contexts boy dog (subj chase head) (adjn brown rood) (compl chase head) A s s u m p t i o n 1: The commonality between A and B is measured by I(common(A, B)) Using a broad coverage parser to parse a corpus, we construct a Local Context Database. A n entry in the database is a pair: where common(A, B) is a proposition that states the commonalities between A and B; I(s) is the amount of informatio"
P97-1009,C92-2070,0,0.212723,"algorithm does not require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts. 1 Introduction Given a word, its context and its possible meanings, the problem of word sense disambiguation (WSD) is to determine the meaning of the word in that context. WSD is useful in many natural language tasks, such as choosing the correct word in machine translation and coreference resolution. In several recent proposals (Hearst, 1991; Bruce and Wiebe, 1994; Leacock, Towwell, and Voorhees, 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1994), statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus. Yarowsky (Yarowsky, 1995) proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus, thus avoided the need to hand-annotate any examples. Most previous corpus-based WSD algorithms determine the meanings of polysemous words by exploiting their local c o n t e x t s . A basic intuition that underlies those algorithms is the following: (i) (2) Two different words are likely to have similar meani"
P97-1009,P94-1013,0,0.029119,"ot require a sense-tagged corpus and exploits the fact that two different words are likely to have similar meanings if they occur in identical local contexts. 1 Introduction Given a word, its context and its possible meanings, the problem of word sense disambiguation (WSD) is to determine the meaning of the word in that context. WSD is useful in many natural language tasks, such as choosing the correct word in machine translation and coreference resolution. In several recent proposals (Hearst, 1991; Bruce and Wiebe, 1994; Leacock, Towwell, and Voorhees, 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1994), statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus. Yarowsky (Yarowsky, 1995) proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus, thus avoided the need to hand-annotate any examples. Most previous corpus-based WSD algorithms determine the meanings of polysemous words by exploiting their local c o n t e x t s . A basic intuition that underlies those algorithms is the following: (i) (2) Two different words are likely to have similar meanings if they occur"
P97-1009,P95-1026,0,0.27816,"in identical local contexts. 1 Introduction Given a word, its context and its possible meanings, the problem of word sense disambiguation (WSD) is to determine the meaning of the word in that context. WSD is useful in many natural language tasks, such as choosing the correct word in machine translation and coreference resolution. In several recent proposals (Hearst, 1991; Bruce and Wiebe, 1994; Leacock, Towwell, and Voorhees, 1996; Ng and Lee, 1996; Yarowsky, 1992; Yarowsky, 1994), statistical and machine learning techniques were used to extract classifiers from hand-tagged corpus. Yarowsky (Yarowsky, 1995) proposed an unsupervised method that used heuristics to obtain seed classifications and expanded the results to the other parts of the corpus, thus avoided the need to hand-annotate any examples. Most previous corpus-based WSD algorithms determine the meanings of polysemous words by exploiting their local c o n t e x t s . A basic intuition that underlies those algorithms is the following: (i) (2) Two different words are likely to have similar meanings if they occur in identical local contexts. Consider the sentence: (3) The new facility will employ 500 of the existing 600 employees The word"
P97-1009,H94-1046,0,\N,Missing
P98-2127,P90-1034,0,0.781951,"simgindZe(Wl, W2) = ~'~(r,w)eTCwl)NTCw2)Are{subj.of.obj-of}min(I(Wl, r, w), I(w2, r, w) ) simHindte, (Wl, W2) = ~,(r,w)eT(w,)nT(w2)m i n ( I ( w l , r, w), I(w2, r, w)) ]T(Wl)NT(w2)I simcosine(Wl,W2) = x/IZ(w~)l×lZ(w2)l 2x IT(wl)nZ(w2)l simDice(Wl, W2) = iT(wl)l+lT(w2)I simJacard ( W l , W2) = T(wl) + T(wl )OT(w2)l T(w2)l-IT(Wl)rlT(w2)l Figure 1: Other Similarity Measures puted as follows: I(w,r,w') = _ Iog(PMLE(B)PMLE(A]B)PMLE(CIB)) --(-- log PMLE(A,B, C)) - log IIw,r,wfl×ll*,r,*ll IIw,r,*llxll*,r,w'll - - It is worth noting that I(w,r,w') is equal to the mutual information between w and w' (Hindle, 1990). Let T(w) be the set of pairs (r, w') such that log Iw'r'w'lr×ll*'r'*ll is positive. We define the simw l r ~ * X *~r~w ! ilarity sim(wl, w2) between two words wl and w2 as follows: )""~(r,w)eT(w,)NT(w~)(I(Wl, r, w) + I(w2, r, w) ) ~-,(r,w)eT(wl) I(Wl, r, w) q- ~(r,w)eT(w2) I(w2, r, w) We parsed a 64-million-word corpus consisting of the Wall Street Journal (24 million words), San Jose Mercury (21 million words) and AP Newswire (19 million words). From the parsed corpus, we extracted 56.5 million dependency triples (8.7 million unique). In the parsed corpus, there are 5469 nouns, 2173 verbs, a"
P98-2127,P93-1016,1,0.164797,"thesaurus. In Section 3, we evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri. Section 4 briefly discuss future work in clustering similar words. Finally, Section 5 reviews related work and summarize our contributions. 768 2 Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects. We use a broad-coverage parser (Lin, 1993; Lin, 1994) to extract dependency triples from the text corpus. A dependency triple consists of two words and the grammatical relationship between them in the input sentence. For example, the triples extracted from the sentence ""I have a brown dog"" are: (2) (have subj I), (I subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of dog), (dog det a), (a det-of dog) We use the notation IIw, r, w'll to denote the frequency count of the dependency triple (w, r, w ~) in the parsed corpus. When w, r, or w ~ is the wild card (*), the frequency counts of all the dependency triples tha"
P98-2127,C94-1079,1,0.0817116,"In Section 3, we evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri. Section 4 briefly discuss future work in clustering similar words. Finally, Section 5 reviews related work and summarize our contributions. 768 2 Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects. We use a broad-coverage parser (Lin, 1993; Lin, 1994) to extract dependency triples from the text corpus. A dependency triple consists of two words and the grammatical relationship between them in the input sentence. For example, the triples extracted from the sentence ""I have a brown dog"" are: (2) (have subj I), (I subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of dog), (dog det a), (a det-of dog) We use the notation IIw, r, w'll to denote the frequency count of the dependency triple (w, r, w ~) in the parsed corpus. When w, r, or w ~ is the wild card (*), the frequency counts of all the dependency triples that matches th"
P98-2127,P97-1009,1,0.45251,"thing methods in word sense disambiguation. The remainder of the paper is organized as follows. The next section is concerned with similarities between words based on their distributional patterns. The similarity measure can then be used to create a thesaurus. In Section 3, we evaluate the constructed thesauri by computing the similarity between their entries and entries in manually created thesauri. Section 4 briefly discuss future work in clustering similar words. Finally, Section 5 reviews related work and summarize our contributions. 768 2 Our similarity measure is based on a proposal in (Lin, 1997), where the similarity between two objects is defined to be the amount of information contained in the commonality between the objects divided by the amount of information in the descriptions of the objects. We use a broad-coverage parser (Lin, 1993; Lin, 1994) to extract dependency triples from the text corpus. A dependency triple consists of two words and the grammatical relationship between them in the input sentence. For example, the triples extracted from the sentence ""I have a brown dog"" are: (2) (have subj I), (I subj-of have), (dog obj-of have), (dog adj-mod brown), (brown adj-mod-of d"
P98-2127,P93-1024,0,0.185808,"Missing"
P98-2127,J93-1007,0,0.135808,"tree for ""duty"" Inspection of sample outputs shows that this algorithm works well. However, formal evaluation of its accuracy remains to be future work. 5 Related Work and Conclusion There have been many approaches to automatic detection of similar words from text corpora. Ours is 772 similar to (Grefenstette, 1994; Hindle, 1990; Ruge, 1992) in the use of dependency relationship as the word features, based on which word similarities are computed. Evaluation of automatically generated lexical resources is a difficult problem. In (Hindle, 1990), a small set of sample results are presented. In (Smadja, 1993), automatically extracted collocations are judged by a lexicographer. In (Dagan et al., 1993) and (Pereira et al., ! 993), clusters of similar words are evaluated by how well they are able to recover data items that are removed from the input corpus one at a time. In (Alshawi and Carter, 1994), the collocations and their associated scores were evaluated indirectly by their use in parse tree selection. The merits of different measures for association strength are judged by the differences they make in the precision and the recall of the parser outputs. The main contribution of this paper is a n"
P98-2127,J94-4005,0,\N,Missing
P98-2127,P94-1038,0,\N,Missing
P98-2127,P93-1022,0,\N,Missing
P98-2127,P97-1008,0,\N,Missing
P99-1041,J96-1001,0,0.0255339,"ller et al., 1990) and Roget's Thesaurus. However, since the lexicon used in our parser is based on the WordNet, the phrasal words in WordNet are treated as a single word. For example, &quot;take advantage of&quot; is treated as a transitive verb by the parser. As a result, the extracted non-compositional phrases do not usually overlap with phrasal entries in the WordNet. Therefore, we conducted the evaluation by manually examining sample results. This method was also used to evaluate automatically identified hyponyms (Hearst, 1998), word similarity (Richardson, 1997), and translations of collocations (Smadja et al., 1996). Our evaluation sample consists of 5 most frequent open class words in the our parsed corpus: {have, company, make, do, take} and 5 words whose frequencies are ranked from 2000 to 2004: {path, lock, resort, column, gulf}. We examined three types of dependency relationships: object-verb, noun-noun, and adjective-noun. A total of 216 collocations were extracted, shown in Appendix A. We compared the collocations in Appendix A with the entries for the above 10 words in the NTC's English Idioms Dictionary (henceforth NTC-EID) (Spears and Kirkpatrick, 1993), which contains approximately 6000 defini"
P99-1041,J93-1007,0,0.415465,"d all word pairs that occurred within a 4-word window of each other. The same algorithm and similarity measure for the dependency database are used to construct a thesaurus using the co-occurrence database. Appendix B shows all the word pairs that satisfies the condition (3) and that involve one of the 10 words {have, company, make, do, take, path, lock, resort, column, gulf}. It is clear that Appendix B contains far fewer true non-compositional phrases than Appendix A. 7 Related Work There have been numerous previous research on extracting collocations from corpus, e.g., (Choueka, 1988) and (Smadja, 1993). They do not, however, make a distinction between compositional and noncompositional collocations. Mutual information has often been used to separate systematic associations from accidental ones. It was also used to compute the distributional similarity between words CHindle, 1990; Lin, 1998). A method to determine the compositionality of verb-object pairs is proposed in (Tapanainen et al., 1998). The basic idea in there is that &quot;if an object appears only with one verb (of few verbs) in a large corpus we expect that it has an idiomatic nature&quot; (Tapanainen et al., 1998, p.1290). For each objec"
P99-1041,P98-2210,0,0.115657,"Missing"
P99-1041,P90-1034,0,0.0289385,"t involve one of the 10 words {have, company, make, do, take, path, lock, resort, column, gulf}. It is clear that Appendix B contains far fewer true non-compositional phrases than Appendix A. 7 Related Work There have been numerous previous research on extracting collocations from corpus, e.g., (Choueka, 1988) and (Smadja, 1993). They do not, however, make a distinction between compositional and noncompositional collocations. Mutual information has often been used to separate systematic associations from accidental ones. It was also used to compute the distributional similarity between words CHindle, 1990; Lin, 1998). A method to determine the compositionality of verb-object pairs is proposed in (Tapanainen et al., 1998). The basic idea in there is that &quot;if an object appears only with one verb (of few verbs) in a large corpus we expect that it has an idiomatic nature&quot; (Tapanainen et al., 1998, p.1290). For each object noun o, (Tapanainen et al., 1998) computes the distributed frequency DF(o) and rank the non-compositionality of o according to this value. Using the notation introduced in Section 3, DF(o) is computed as follows: DF(o) = ~ i=1 Iv,, v:compl:~, ola nb where {vl,v2,... ,vn} are verb"
P99-1041,P93-1016,1,0.351288,"a non-compositional expression causes it to have a different distributional characteristic than expressions that are similar to its literal meaning. 2 UMIACS U n i v e r s i t y of M a r y l a n d College P a r k , M a r y l a n d , 20742 lindek@umiacs.umd.edu Input Data The input to our algorithm is a collocation database and a thesaurus. We briefly describe the process of obtaining this input. More details about the construction of the collocation database and the thesaurus can be found in (Lin, 1998). We parsed a 125-million word newspaper corpus with Minipar, 1 a descendent of Principar (Lin, 1993; Lin, 1994), and extracted dependency relationships from the parsed corpus. A dependency relationship is a triple: (head type m o d i f i e r ) , where head and m o d i f i e r are words in the input sentence and type is the type of the dependency relation. For example, (la) is an example dependency tree and the set of dependency triples extracted from (la) are shown in (lb). compl John married Peter's sister b. (marry V:subj:N John), (marry V:compl:N sister), (sister N:gen:N Peter) There are about 80 million dependency relationships in the parsed corpus. The frequency counts of dependency re"
P99-1041,C94-1079,1,0.740715,"ositional expression causes it to have a different distributional characteristic than expressions that are similar to its literal meaning. 2 UMIACS U n i v e r s i t y of M a r y l a n d College P a r k , M a r y l a n d , 20742 lindek@umiacs.umd.edu Input Data The input to our algorithm is a collocation database and a thesaurus. We briefly describe the process of obtaining this input. More details about the construction of the collocation database and the thesaurus can be found in (Lin, 1998). We parsed a 125-million word newspaper corpus with Minipar, 1 a descendent of Principar (Lin, 1993; Lin, 1994), and extracted dependency relationships from the parsed corpus. A dependency relationship is a triple: (head type m o d i f i e r ) , where head and m o d i f i e r are words in the input sentence and type is the type of the dependency relation. For example, (la) is an example dependency tree and the set of dependency triples extracted from (la) are shown in (lb). compl John married Peter's sister b. (marry V:subj:N John), (marry V:compl:N sister), (sister N:gen:N Peter) There are about 80 million dependency relationships in the parsed corpus. The frequency counts of dependency relationships"
P99-1041,P97-1009,1,0.0688511,"allenge to NLP applications. In machine translation, word-for-word translation of non-compositional expressions can result in very misleading (sometimes laughable) translations. In information retrieval, expansion of words in a non-compositional expression can lead to dramatic decrease in precision without any gain in recall. Less obviously, non-compositional expressions need to be treated differently than other phrases in many statistical or corpus-based NLP methods. For example, an underlying assumption in some word sense disambiguation systems, e.g., (Dagan and Itai, 1994; Li et al., 1995; Lin, 1997), is that if two words occurred in the same context, they are probably similar. Suppose we want to determine the intended meaning of &quot;product&quot; in &quot;hot product&quot;. We can find other words that are also modified by &quot;hot&quot; (e.g., &quot;hot car&quot;) and then choose the meaning of &quot;product&quot; that is most similar to meanings of these words. However, this method fails when non-compositional expressions are involved. For instance, using the same algorithm to determine the meaning of &quot;line&quot; in &quot;hot line&quot;, the words &quot;product&quot;, &quot;merchandise&quot;, &quot;car&quot;, etc., would lead the algorithm to choose the &quot;line of product&quot; sens"
P99-1041,P98-2127,1,0.403001,"l properties in a text corpus. The intuitive idea behind the method is that the metaphorical usage of a non-compositional expression causes it to have a different distributional characteristic than expressions that are similar to its literal meaning. 2 UMIACS U n i v e r s i t y of M a r y l a n d College P a r k , M a r y l a n d , 20742 lindek@umiacs.umd.edu Input Data The input to our algorithm is a collocation database and a thesaurus. We briefly describe the process of obtaining this input. More details about the construction of the collocation database and the thesaurus can be found in (Lin, 1998). We parsed a 125-million word newspaper corpus with Minipar, 1 a descendent of Principar (Lin, 1993; Lin, 1994), and extracted dependency relationships from the parsed corpus. A dependency relationship is a triple: (head type m o d i f i e r ) , where head and m o d i f i e r are words in the input sentence and type is the type of the dependency relation. For example, (la) is an example dependency tree and the set of dependency triples extracted from (la) are shown in (lb). compl John married Peter's sister b. (marry V:subj:N John), (marry V:compl:N sister), (sister N:gen:N Peter) There are a"
P99-1041,J94-4003,0,\N,Missing
P99-1041,C98-2122,1,\N,Missing
W03-0302,J93-2003,0,0.0465703,": An Example of Cohesion Constraint To define this notion more formally, let TE (ei ) be the subtree of TE rooted at ei . The phrase span of ei , spanP(ei , TE , A), is the image of the English phrase headed by ei in F given a (partial) alignment A. More precisely, spanP(ei , TE , A) = [k1 , k2 ], where k1 = min{j|l(u, j) ∈ A, eu ∈ TE (ei )} k2 = max{j|l(u, j) ∈ A, eu ∈ TE (ei )} Probability Model We define the word alignment problem as finding the alignment A that maximizes P (A|E, F ). ProAlign models P (A|E, F ) directly, using a different decomposition of terms than the model used by IBM (Brown et al., 1993). In the IBM models of translation, alignments exist as artifacts of a stochastic process, where the words in the English sentence generate the words in the French sentence. Our model does not assume that one sentence generates the other. Instead it takes both sentences as given, and uses the sentences to determine an alignment. An alignment A consists of t links {l1 , l2 , . . . , lt }, where each lk = l(eik , fjk ) for some ik and jk . We will refer to consecutive subsets of A as lij = {li , li+1 , . . . , lj }. Given this notation, P (A|E, F ) can be decomposed as follows: P (A|E, F ) = P ("
W03-0302,P03-1012,1,0.81084,"cal systems. We will consider only a subset FT k of relevant features of Ck . We will make the Na¨ıve Bayes-style assumption that these features ft ∈ FT k are conditionally independent given either lk or (eik , fjk ). This produces a tractable formulation for P (A|E, F ):   t Y Y P (ft|lk )  P (lk |eik , fjk ) × P (ft|eik , fjk ) obj pre subj det det the host discovers all the devices 1 2 3 4 5 6 1 2 3 4 5 6 l&apos; hôte repère tous les périphériques the host locate all the peripherals Figure 2: Feature Extraction Example More details on the probability model used by ProAlign are available in (Cherry and Lin, 2003). In contrast, the incorrect link (the1 , les) will have only ft d (+3, det), which will work to lower the link probability, since most determiners are located before their governors. 3.1 3.2 k=1 ft∈FT k Features used in the shared task For the purposes of the shared task, we use two feature types. Each type could have any number of instantiations for any number of contexts. Note that each feature type is described in terms of the context surrounding a word pair. The first feature type ft a concerns surrounding links. It has been observed that words close to each other in the source language t"
W03-0302,W02-1039,0,0.413507,"orate linguistic features into our model and linguistic intuitions into our constraints. 2 Constraints The model used for scoring alignments has no mechanism to prevent certain types of undesirable alignments, such as having all French words align to the same English word. To guide the search to correct alignments, we employ two constraints to limit our search for the most probable alignment. The first constraint is the one-to-one constraint (Melamed, 2000): every word (except the null words e0 and f0 ) participates in exactly one link. The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Given the dependency tree TE and a (partial) alignment A, the cohesion constraint requires that phrasal cohesion is maintained in the French sentence. If two phrases are disjoint in the English sentence, the alignment must not map them to overlapping intervals in the French sentence. This notion of phrasal constraints on alignments need not be restricted to phrases determined from a dependency structure. However, the experiments conducted in (Fox, 2002) indicate that dependency trees dem"
W03-0302,H91-1026,0,0.482989,"Missing"
W03-0302,N03-2017,1,0.842847,"alignment is cohesive with respect to TE if it does not introduce any head-modifier or modifier-modifier overlaps. For example, the alignment A in Figure 1 is not cohesive because spanP (reboot, TE , A) = [4, 4] intersects spanP (discover, TE , A) = [2, 11]. Since both reboot and discover modify causes, this creates a modifiermodifier overlap. One can check for constraint violations inexpensively by incrementally updating the various spans as new links are added to the partial alignment, and checking for overlap after each modification. More details on the cohesion constraint can be found in (Lin and Cherry, 2003). 3 det aux 2002) as crossings. Given a head node eh and its modifier em , a head-modifier overlap occurs when: peripherals Figure 1: An Example of Cohesion Constraint To define this notion more formally, let TE (ei ) be the subtree of TE rooted at ei . The phrase span of ei , spanP(ei , TE , A), is the image of the English phrase headed by ei in F given a (partial) alignment A. More precisely, spanP(ei , TE , A) = [k1 , k2 ], where k1 = min{j|l(u, j) ∈ A, eu ∈ TE (ei )} k2 = max{j|l(u, j) ∈ A, eu ∈ TE (ei )} Probability Model We define the word alignment problem as finding the alignment A tha"
W03-0302,J00-2004,0,0.126241,"of English-French sentence pairs, along with dependency trees for the English sentences. The presence of the English dependency tree allows us to incorporate linguistic features into our model and linguistic intuitions into our constraints. 2 Constraints The model used for scoring alignments has no mechanism to prevent certain types of undesirable alignments, such as having all French words align to the same English word. To guide the search to correct alignments, we employ two constraints to limit our search for the most probable alignment. The first constraint is the one-to-one constraint (Melamed, 2000): every word (except the null words e0 and f0 ) participates in exactly one link. The second constraint, known as the cohesion constraint (Fox, 2002), uses the dependency tree (Mel’ˇcuk, 1987) of the English sentence to restrict possible link combinations. Given the dependency tree TE and a (partial) alignment A, the cohesion constraint requires that phrasal cohesion is maintained in the French sentence. If two phrases are disjoint in the English sentence, the alignment must not map them to overlapping intervals in the French sentence. This notion of phrasal constraints on alignments need not"
W03-0302,C96-2141,0,0.0886709,"only ft d (+3, det), which will work to lower the link probability, since most determiners are located before their governors. 3.1 3.2 k=1 ft∈FT k Features used in the shared task For the purposes of the shared task, we use two feature types. Each type could have any number of instantiations for any number of contexts. Note that each feature type is described in terms of the context surrounding a word pair. The first feature type ft a concerns surrounding links. It has been observed that words close to each other in the source language tend to remain close to each other in the translation (S. Vogel and Tillmann, 1996). To capture this notion, for any word pair (ei , fj ), if a link l(ei0 , fj 0 ) exists within a window of two words (where i − 2 ≤ i0 ≤ i + 2 and j − 2 ≤ j 0 ≤ j + 2), then we say that the feature ft a (i − i0 , j − j 0 , ei0 ) is active for this context. We refer to these as adjacency features. The second feature type ft d uses the English parse tree to capture regularities among grammatical relations between languages. For example, when dealing with French and English, the location of the determiner with respect to its governor is never swapped during translation, while the location of adje"
W05-1516,P90-1034,0,0.0213261,"maintain the canonical order as required by our model. 3 Similarity-based Smoothing 3.1 Distributional Word Similarity Words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct, ... and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult, ... Many methods have been proposed to compute distributional similarity between words (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. 155 We define the features of a word w to be the set of words that occurred within a small context window of w in a large corpus. The context window of an instance of w consists of the closest nonstop-word on each side of w and the stop-words in between. In our experiments, the set"
W05-1516,W00-1201,0,0.0422616,"sible). For example, there is nothing in such models to prevent the parser from assigning two subjects to a verb. In the DMV model (Klein and Manning, 2004), the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a modifier. Our model is quite similar to the DMV model, except that we compute the conditional probability of the 158 parse tree given the sentence, instead of the joint probability of the parse tree and the sentence. There have been several previous approaches to parsing Chinese with the Penn Chinese Treebank (e.g., Bikel and Chiang, 2000; Levy and Manning, 2003). Both of these approaches employed phrasestructure joint models and used part-of-speech tags in back-off smoothing. Their results were evaluated with the precision and recall of the bracketings implied in the phrase structure parse trees. In contrast, the accuracy of our model is measured in terms of the dependency relationships. A dependency tree may correspond to more than one constituency trees. Our results are therefore not directly comparable with the precision and recall values in previous research. Moreover, it was argued in (Lin 1995) that dependency based eva"
W05-1516,A00-2018,0,0.114214,"the strictly lexicalized conditional model, but higher than the strictly lexicalized joint model. This demonstrated that soft clusters obtained through distributional word similarity perform better than the part-of-speech tags when used appropriately. Models Accuracy (a) Strictly lexicalized conditional model 79.9 (b) At most one word is different in a similar context 77.7 (c) Strictly lexicalized joint model 66.3 (d) Unlexicalized conditional models 71.1 (e) Unlexicalized joint models 71.1 Table 2. Performance of Alternative Models 5 Related Work Previous parsing models (e.g., Collins, 1997; Charniak, 2000) maximize the joint probability P(S, T) of a sentence S and its parse tree T. We maximize the conditional probability P(T |S). Although they are theoretically equivalent, the use of conditional model allows us to take advantage of similarity-based smoothing. Clark et al. (2002) also computes a conditional probability of dependency structures. While the probability space in our model consists of all possible non-projective dependency trees, their probability space is constrained to all the dependency structures that are allowed by a Combinatorial Category Grammar (CCG) and a category dictionary"
W05-1516,P02-1042,0,0.0127526,"rictly lexicalized conditional model 79.9 (b) At most one word is different in a similar context 77.7 (c) Strictly lexicalized joint model 66.3 (d) Unlexicalized conditional models 71.1 (e) Unlexicalized joint models 71.1 Table 2. Performance of Alternative Models 5 Related Work Previous parsing models (e.g., Collins, 1997; Charniak, 2000) maximize the joint probability P(S, T) of a sentence S and its parse tree T. We maximize the conditional probability P(T |S). Although they are theoretically equivalent, the use of conditional model allows us to take advantage of similarity-based smoothing. Clark et al. (2002) also computes a conditional probability of dependency structures. While the probability space in our model consists of all possible non-projective dependency trees, their probability space is constrained to all the dependency structures that are allowed by a Combinatorial Category Grammar (CCG) and a category dictionary (lexicon). They therefore do not need the STOP markers in their model. Another major difference between our model and (Clark et al., 2002) is that the parameters in our model consist exclusively of conditional probabilities of binary variables. Ratnaparkhi’s maximum entropy mo"
W05-1516,P96-1025,0,0.234932,"Missing"
W05-1516,P97-1003,0,0.118748,"bly lower than the strictly lexicalized conditional model, but higher than the strictly lexicalized joint model. This demonstrated that soft clusters obtained through distributional word similarity perform better than the part-of-speech tags when used appropriately. Models Accuracy (a) Strictly lexicalized conditional model 79.9 (b) At most one word is different in a similar context 77.7 (c) Strictly lexicalized joint model 66.3 (d) Unlexicalized conditional models 71.1 (e) Unlexicalized joint models 71.1 Table 2. Performance of Alternative Models 5 Related Work Previous parsing models (e.g., Collins, 1997; Charniak, 2000) maximize the joint probability P(S, T) of a sentence S and its parse tree T. We maximize the conditional probability P(T |S). Although they are theoretically equivalent, the use of conditional model allows us to take advantage of similarity-based smoothing. Clark et al. (2002) also computes a conditional probability of dependency structures. While the probability space in our model consists of all possible non-projective dependency trees, their probability space is constrained to all the dependency structures that are allowed by a Combinatorial Category Grammar (CCG) and a ca"
W05-1516,P03-1054,0,0.0697815,"Missing"
W05-1516,P04-1061,0,0.436015,"Figure 1 is: (1, 2, L), (5, 6, R), (8, 9, L), (7, 9, R), (5, 7, R), (4, 5, R), (3, 4, R), (2, 3, L), (0, 3, L). The generation process according to the canonical order is similar to the head outward generation process in (Collins, 1999), except that it is bottom-up whereas Collins’ models are top-down. Suppose the dependency tree T is constructed in steps G1, …, GN in the canonical order of the dependency links, where N is the number of words in the sentence. We can compute the probability of T as follows: P (T |S ) = P (G1 , G2 ,..., G N |S ) = ∏ i =1 P (Gi |S , G1 ,..., Gi −1 ) N Following (Klein and Manning, 2004), we require that the creation of a dependency link from head h to modifier m be preceded by placing a left STOP and a right STOP around the modifier m and ¬STOP between h and m. Let E wL (and EwR ) denote the event that there are no more modifiers on the left (and right) of a word w. Suppose the dependency link created in the step i is (u, v, d). If d = L, Gi is the conjunction of the four events: EuR , EuL , ¬EvL and linkL(u, v). If d = R, Gi consists of four events: EvL , EvR , ¬EuR and linkR(u, v). The event Gi is conditioned on S , G1 ,..., Gi −1 , which are the words in the sentence and"
W05-1516,P03-1056,0,0.0213886,"re is nothing in such models to prevent the parser from assigning two subjects to a verb. In the DMV model (Klein and Manning, 2004), the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a modifier. Our model is quite similar to the DMV model, except that we compute the conditional probability of the 158 parse tree given the sentence, instead of the joint probability of the parse tree and the sentence. There have been several previous approaches to parsing Chinese with the Penn Chinese Treebank (e.g., Bikel and Chiang, 2000; Levy and Manning, 2003). Both of these approaches employed phrasestructure joint models and used part-of-speech tags in back-off smoothing. Their results were evaluated with the precision and recall of the bracketings implied in the phrase structure parse trees. In contrast, the accuracy of our model is measured in terms of the dependency relationships. A dependency tree may correspond to more than one constituency trees. Our results are therefore not directly comparable with the precision and recall values in previous research. Moreover, it was argued in (Lin 1995) that dependency based evaluation is much more mean"
W05-1516,P98-2127,1,0.218627,"Similarity-based Smoothing 3.1 Distributional Word Similarity Words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct, ... and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult, ... Many methods have been proposed to compute distributional similarity between words (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. 155 We define the features of a word w to be the set of words that occurred within a small context window of w in a large corpus. The context window of an instance of w consists of the closest nonstop-word on each side of w and the stop-words in between. In our experiments, the set of stop-words are defined as the top 100 most frequen"
W05-1516,P05-1012,0,0.133455,"Missing"
W05-1516,P93-1024,0,0.296971,"anonical order as required by our model. 3 Similarity-based Smoothing 3.1 Distributional Word Similarity Words that tend to appear in the same contexts tend to have similar meanings. This is known as the Distributional Hypothesis in linguistics (Harris, 1968). For example, the words test and exam are similar because both of them follow verbs such as administer, cancel, cheat on, conduct, ... and both of them can be preceded by adjectives such as academic, comprehensive, diagnostic, difficult, ... Many methods have been proposed to compute distributional similarity between words (Hindle, 1990; Pereira et al., 1993; Grefenstette, 1994; Lin, 1998). Almost all of the methods represent a word by a feature vector where each feature corresponds to a type of context in which the word appeared. They differ in how the feature vectors are constructed and how the similarity between two feature vectors is computed. 155 We define the features of a word w to be the set of words that occurred within a small context window of w in a large corpus. The context window of an instance of w consists of the closest nonstop-word on each side of w and the stop-words in between. In our experiments, the set of stop-words are def"
W05-1516,C96-1058,0,0.198514,"consist exclusively of conditional probabilities of binary variables. Ratnaparkhi’s maximum entropy model (Ratnaparkhi, 1999) is also a conditional model. However, his model maximizes the probability of the action during each step of the parsing process, instead of overall quality of the parse tree. Yamada and Matsumoto (2002) presented a dependency parsing model using support vector machines. Their model is a discriminative model that maximizes the differences between scores of the correct parse and the scores of the top competing incorrect parses. In many dependency parsing models such as (Eisner, 1996) and (MacDonald et al., 2005), the score of a dependency tree is the sum of the scores of the dependency links, which are computed independently of other links. An undesirable consequence of this is that the parser often creates multiple dependency links that are separately likely but jointly improbable (or even impossible). For example, there is nothing in such models to prevent the parser from assigning two subjects to a verb. In the DMV model (Klein and Manning, 2004), the probability of a dependency link is partly conditioned on whether or not there is a head word of the link already has a"
W05-1516,W03-3023,0,0.46806,"Missing"
W05-1516,W01-0521,0,0.0591328,"Missing"
W05-1516,J04-4004,0,\N,Missing
W05-1516,J03-4003,0,\N,Missing
W05-1516,C98-2122,1,\N,Missing
W07-0403,E06-1019,1,0.866268,"am only. The work described here uses the binary bracketing ITG, which has a single non-terminal: A → [AA] |hAAi |e/f (2) This grammar admits an efficient bitext parsing algorithm, and holds no language-specific biases. (2) cannot represent all possible permutations of concepts that may occur during translation, because some permutations will require discontinuous constituents (Melamed, 2003). This ITG constraint is characterized by the two forbidden structures shown in Figure 1 (Wu, 1997). Empirical studies suggest that only a small percentage of human translations violate these constraints (Cherry and Lin, 2006). 19 Stochastic ITGs are parameterized like their PCFG counterparts (Wu, 1997); productions A → X are assigned probability Pr(X |A). These parameters can be learned from sentence-aligned bitext using the EM algorithm. The expectation task of counting productions weighted by their probability is handled with dynamic programming, using the inside-outside algorithm extended to bitext (Zhang and Gildea, 2004). 3 ITG as a Phrasal Translation Model This paper introduces a phrasal ITG; in doing so, we combine ITG with the JPTM. ITG parsing algorithms consider every possible two-dimensional span of bi"
W07-0403,P06-1097,0,0.0301103,"GIZA++ alignments are trained with no sentence-length limit, using the full 688K corpus. 5.1 Table 1: Inside-outside run-time comparison. Alignment Experiments The goal of this experiment is to compare the Viterbi alignments from the phrasal ITG to gold standard human alignments. We do this to validate our noncompositional constraint and to select good alignments for use with the surface heuristic. 22 Method GIZA++ Intersect GIZA++ Union GIZA++ GDF Phrasal ITG Phrasal ITG + NCC Prec 96.7 82.5 84.0 50.7 75.4 Rec 53.0 69.0 68.2 80.3 78.0 F-measure 68.5 75.1 75.2 62.2 76.7 Following the lead of (Fraser and Marcu, 2006), we hand-aligned the first 100 sentence pairs of our training set according to the Blinker annotation guidelines (Melamed, 1998). We did not differentiate between sure and possible links. We report precision, recall and balanced F-measure (Och and Ney, 2003). For comparison purposes, we include the results of three types of GIZA++ combination, including the grow-diag-final heuristic (GDF). We tested our phrasal ITG with fixed link pruning, and then added the non-compositional constraint (NCC). During development we determined that performance levels off for both of the ITG models after 3 EM i"
W07-0403,N03-1017,0,0.467844,"gorithmically wellfounded method for phrasal analysis of bitext. Section 2 begins by outlining the phrase extraction system we intend to replace and the two methods we combine to do so: the joint phrasal translation model (JPTM) and inversion transduction grammar (ITG). Section 3 describes our proposed solution, a phrasal ITG. Section 4 describes how to apply our phrasal ITG, both as a translation model and as a phrasal word-aligner. Section 5 tests our system in both these capacities, while Section 6 concludes. 2 2.1 Background Phrase Table Extraction Phrasal decoders require a phrase table (Koehn et al., 2003), which contains bilingual phrase pairs and 17 Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 17–24, c Rochester, New York, April 2007. 2007 Association for Computational Linguistics scores indicating their utility. The surface heuristic is the most popular method for phrase-table construction. It extracts all consistent phrase pairs from word-aligned bitext (Koehn et al., 2003). The word alignment provides bilingual links, indicating translation relationships between words. Consistency is defined so that alignment links are never"
W07-0403,N03-1021,0,0.0187489,"h their non-terminals inside square brackets [. . .], produce their symbols in the given order in both streams. Inverted productions, indicated by angled brackets h. . .i, are output in reverse order in the Foreign stream only. The work described here uses the binary bracketing ITG, which has a single non-terminal: A → [AA] |hAAi |e/f (2) This grammar admits an efficient bitext parsing algorithm, and holds no language-specific biases. (2) cannot represent all possible permutations of concepts that may occur during translation, because some permutations will require discontinuous constituents (Melamed, 2003). This ITG constraint is characterized by the two forbidden structures shown in Figure 1 (Wu, 1997). Empirical studies suggest that only a small percentage of human translations violate these constraints (Cherry and Lin, 2006). 19 Stochastic ITGs are parameterized like their PCFG counterparts (Wu, 1997); productions A → X are assigned probability Pr(X |A). These parameters can be learned from sentence-aligned bitext using the EM algorithm. The expectation task of counting productions weighted by their probability is handled with dynamic programming, using the inside-outside algorithm extended"
W07-0403,J03-1002,0,0.0176673,"defined so that alignment links are never broken by phrase boundaries. For each token w in a consistent phrase pair p¯, all tokens linked to w by the alignment must also be included in p¯. Each consistent phrase pair is counted as occurring once per sentence pair. The scores for the extracted phrase pairs are provided by normalizing these flat counts according to common English or Foreign components, producing the conditional distributions p(f¯|¯ e) and p(¯ e|f¯). The surface heuristic can define consistency according to any word alignment; but most often, the alignment is provided by GIZA++ (Och and Ney, 2003). This alignment system is powered by the IBM translation models (Brown et al., 1993), in which one sentence generates the other. These models produce only one-to-many alignments: each generated token can participate in at most one link. Many-to-many alignments can be created by combining two GIZA++ alignments, one where English generates Foreign and another with those roles reversed (Och and Ney, 2003). Combination approaches begin with the intersection of the two alignments, and add links from the union heuristically. The grow-diag-final (GDF) combination heuristic (Koehn et al., 2003) adds"
W07-0403,P03-1021,0,0.0649402,"e phrase tables. Two are conditionalized phrasal models, each EM trained until performance degrades: • C-JPTM3 as described in (Birch et al., 2006) • Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000sentence development set are reported using the BLEU metric (Papineni et al., 2002). For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs. The results of all experiments are shown in Table 3. We see that the Phrasal ITG surpasses the CJPTM by more than 2.5 BLEU points. A large component of this improvement is due to the ITG’s use of inside-outside for expectation calculation, though 3 Supplied by personal communication."
W07-0403,P02-1040,0,0.108799,") • Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000sentence development set are reported using the BLEU metric (Papineni et al., 2002). For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs. The results of all experiments are shown in Table 3. We see that the Phrasal ITG surpasses the CJPTM by more than 2.5 BLEU points. A large component of this improvement is due to the ITG’s use of inside-outside for expectation calculation, though 3 Supplied by personal communication. Run with default parameters, but with maximum phrase length increased to 5. 23 Table 3: Translation Comparison. Method BLEU +M1 Size Conditionalized Phrasal"
W07-0403,2005.eamt-1.36,0,0.0100519,"itionalized phrasal models, each EM trained until performance degrades: • C-JPTM3 as described in (Birch et al., 2006) • Phrasal ITG as described in Section 4.1 Three provide alignments for the surface heuristic: • GIZA++ with grow-diag-final (GDF) • Viterbi Phrasal ITG with and without the noncompositional constraint We use the Pharaoh decoder (Koehn et al., 2003) with the SMT Shared Task baseline system (Koehn and Monz, 2006). Weights for the log-linear model are set using the 500-sentence tuning set provided for the shared task with minimum error rate training (Och, 2003) as implemented by Venugopal and Vogel (2005). Results on the provided 2000sentence development set are reported using the BLEU metric (Papineni et al., 2002). For all methods, we report performance with and without IBM Model 1 features (M1), along with the size of the resulting tables in millions of phrase pairs. The results of all experiments are shown in Table 3. We see that the Phrasal ITG surpasses the CJPTM by more than 2.5 BLEU points. A large component of this improvement is due to the ITG’s use of inside-outside for expectation calculation, though 3 Supplied by personal communication. Run with default parameters, but with maximu"
W07-0403,W05-0835,0,0.061732,"the span under consideration might have been drawn directly from the lexicon. This option can be added to our grammar by altering the definition of a terminal production to include phrases: A → e¯/f¯. This third option is shown in Figure 2 (c). The model implied by this extended grammar is trained using inside-outside and EM. Our approach differs from previous attempts to use ITGs for phrasal bitext analysis. Wu (1997) used a binary bracketing ITG to segment a sentence while simultaneously word-aligning it to its translation, but the model was trained heuristically with a fixed segmentation. Vilar and Vidal (2005) used ITG-like dynamic programming to drive both training and alignment for their recursive translation model, but they employed a conditional model that did not maintain a phrasal lexicon. Instead, they scored phrase pairs using IBM Model 1. Our phrasal ITG is quite similar to the JPTM. Both models are trained with EM, and both employ generative stories that create a sentence and its translation simultaneously. The similarities become more apparent when we consider the canonical-form binary-bracketing ITG (Wu, 1997) shown here: S → A|B|C A → [AB] |[BB] |[CB] | [AC] |[BC] |[CC] B → hAAi |hBAi"
W07-0403,2003.mtsummit-papers.53,0,0.00962983,"n produce the necessary conditional probabilities by conditionalizing the joint table in both directions. We use our p(¯ e/f¯|C) distribution from our stochastic grammar to produce p(¯ e|f¯) and p(f¯|¯ e) values for its phrasal lexicon. 21 Pharaoh also includes lexical weighting parameters that are derived from the alignments used to induce its phrase pairs (Koehn et al., 2003). Using the phrasal ITG as a direct translation model, we do not produce alignments for individual sentence pairs. Instead, we provide a lexical preference with an IBM Model 1 feature pM1 that penalizes unmatched words (Vogel et al., 2003). We include both pM1 (¯ e|f¯) and pM1 (f¯|¯ e). 4.2 Phrasal Word Alignment We can produce a translation model using insideoutside, without ever creating a Viterbi parse. However, we can also examine the maximum likelihood phrasal alignments predicted by the trained model. Despite its strengths derived from using phrases throughout training, the alignments predicted by our phrasal ITG are usually unsatisfying. For example, the fragment pair (order of business, ordre des travaux) is aligned as a phrase pair by our system, linking every English word to every French word. This is frustrating, sin"
W07-0403,J97-3002,0,0.906229,"e statistical analysis used at the word level in a phrasal setting have met with limited success, held back by the sheer size of phrasal alignment space. Hybrid methods that combine well-founded statistical analysis with high-confidence word-level alignments have made some headway (Birch et al., 2006), but suffer from the daunting task of heuristically exploring a still very large alignment space. In the meantime, synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents, but continue to be employed primarily for word-to-word analysis (Wu, 1997). In this paper we unify the probability models for phrasal translation with the algorithms for synchronous parsing, harnessing the benefits of both to create a statistically and algorithmically wellfounded method for phrasal analysis of bitext. Section 2 begins by outlining the phrase extraction system we intend to replace and the two methods we combine to do so: the joint phrasal translation model (JPTM) and inversion transduction grammar (ITG). Section 3 describes our proposed solution, a phrasal ITG. Section 4 describes how to apply our phrasal ITG, both as a translation model and as a phr"
W07-0403,C04-1060,0,0.0445884,"constraint is characterized by the two forbidden structures shown in Figure 1 (Wu, 1997). Empirical studies suggest that only a small percentage of human translations violate these constraints (Cherry and Lin, 2006). 19 Stochastic ITGs are parameterized like their PCFG counterparts (Wu, 1997); productions A → X are assigned probability Pr(X |A). These parameters can be learned from sentence-aligned bitext using the EM algorithm. The expectation task of counting productions weighted by their probability is handled with dynamic programming, using the inside-outside algorithm extended to bitext (Zhang and Gildea, 2004). 3 ITG as a Phrasal Translation Model This paper introduces a phrasal ITG; in doing so, we combine ITG with the JPTM. ITG parsing algorithms consider every possible two-dimensional span of bitext, each corresponding to a bilingual phrase pair. Each multi-token span is analyzed in terms of how it could be built from smaller spans using a straight or inverted production, as is illustrated in Figures 2 (a) and (b). To extend ITG to a phrasal setting, we add a third option for span analysis: that the span under consideration might have been drawn directly from the lexicon. This option can be adde"
W07-0403,W06-3123,0,0.784815,"uch as monolingual agreement and short-range movement, taking pressure off of language and distortion models. Despite the success of phrasal decoders, knowledge acquisition for translation generally begins with a word-level analysis of the training text, taking the form of a word alignment. Attempts to apply the same statistical analysis used at the word level in a phrasal setting have met with limited success, held back by the sheer size of phrasal alignment space. Hybrid methods that combine well-founded statistical analysis with high-confidence word-level alignments have made some headway (Birch et al., 2006), but suffer from the daunting task of heuristically exploring a still very large alignment space. In the meantime, synchronous parsing methods efficiently process the same bitext phrases while building their bilingual constituents, but continue to be employed primarily for word-to-word analysis (Wu, 1997). In this paper we unify the probability models for phrasal translation with the algorithms for synchronous parsing, harnessing the benefits of both to create a statistically and algorithmically wellfounded method for phrasal analysis of bitext. Section 2 begins by outlining the phrase extrac"
W07-0403,N06-1033,0,0.0675552,"Missing"
W07-0403,J93-2003,0,\N,Missing
W07-0403,P05-1059,0,\N,Missing
W07-0403,2006.amta-papers.2,0,\N,Missing
W07-0403,W06-3114,0,\N,Missing
W07-0403,D08-1076,0,\N,Missing
W09-1116,P06-1005,1,0.928216,"uns, like he or his, then Glen is likely a masculine noun. But for most nouns we have no annotated data recording their coreference with pronouns, and thus no data from which we can extract the co-occurrence statistics. Thus previous approaches rely on either hand-crafted coreferenceindicating patterns (Bergsma, 2005), or iteratively guess and improve gender models through expectation maximization of pronoun resolution (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). In statistical approaches, the more frequent the noun, the more accurate the assignment of gender. We use the approach of Bergsma and Lin (2006), both because it achieves state-of-the-art gender classification performance, and because a database of the obtained noun genders is available online.1 Bergsma and Lin (2006) use an unsupervised algorithm to identify syntactic paths along which a noun and pronoun are highly likely to corefer. To extract gender information, they processed a large corpus of news text, and obtained co-occurrence counts for nouns and pronouns connected with these paths in the corpus. In their database, each noun is listed with its corresponding masculine, feminine, neutral, and plural pronoun co-occurrence counts"
W09-1116,P04-1056,0,0.0823546,"Missing"
W09-1116,E09-1018,0,0.0113786,"ed (noun,gender) pairs. All previous statistical approaches rely on a similar observation: if a noun like Glen is often referred to by masculine pronouns, like he or his, then Glen is likely a masculine noun. But for most nouns we have no annotated data recording their coreference with pronouns, and thus no data from which we can extract the co-occurrence statistics. Thus previous approaches rely on either hand-crafted coreferenceindicating patterns (Bergsma, 2005), or iteratively guess and improve gender models through expectation maximization of pronoun resolution (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). In statistical approaches, the more frequent the noun, the more accurate the assignment of gender. We use the approach of Bergsma and Lin (2006), both because it achieves state-of-the-art gender classification performance, and because a database of the obtained noun genders is available online.1 Bergsma and Lin (2006) use an unsupervised algorithm to identify syntactic paths along which a noun and pronoun are highly likely to corefer. To extract gender information, they processed a large corpus of news text, and obtained co-occurrence counts for nouns and pronouns connected with these paths"
W09-1116,W05-0612,1,0.852237,"h to acquire the pseudo-seed (noun,gender) pairs. All previous statistical approaches rely on a similar observation: if a noun like Glen is often referred to by masculine pronouns, like he or his, then Glen is likely a masculine noun. But for most nouns we have no annotated data recording their coreference with pronouns, and thus no data from which we can extract the co-occurrence statistics. Thus previous approaches rely on either hand-crafted coreferenceindicating patterns (Bergsma, 2005), or iteratively guess and improve gender models through expectation maximization of pronoun resolution (Cherry and Bergsma, 2005; Charniak and Elsner, 2009). In statistical approaches, the more frequent the noun, the more accurate the assignment of gender. We use the approach of Bergsma and Lin (2006), both because it achieves state-of-the-art gender classification performance, and because a database of the obtained noun genders is available online.1 Bergsma and Lin (2006) use an unsupervised algorithm to identify syntactic paths along which a noun and pronoun are highly likely to corefer. To extract gender information, they processed a large corpus of news text, and obtained co-occurrence counts for nouns and pronouns"
W09-1116,W99-0613,0,0.0521777,"e: 1) the context of the noun, and 2) the noun’s morphological properties. Bootstrapping with these views is possible in other languages where context is highly predictive of gender class, since contextual words like adjectives and determiners inflect to agree with the grammatical noun gender. We initially attempted a similar system for English noun gender but found context alone to be insufficiently predictive. Bootstrapping is also used in general information extraction. Brin (1998) shows how to alternate between extracting instances of a class and inducing new instance-extracting patterns. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition. Our approach was directly influenced by the hypernym-extractor of Snow et al. (2005) and we provided an analogous summary in Section 1. While their approach uses WordNet to label hypernyms in raw text, our initial labels are generated automatically. Etzioni et al. (2005) also require no labeled data or hand-labeled seeds for their namedentity extractor, but by comparison their classifier only uses a very small number of both features and automatically-generated training examples. 7 Conclusion"
W09-1116,W99-0612,0,0.0498465,"and 2) the noun’s morphological properties. Bootstrapping with these views is possible in other languages where context is highly predictive of gender class, since contextual words like adjectives and determiners inflect to agree with the grammatical noun gender. We initially attempted a similar system for English noun gender but found context alone to be insufficiently predictive. Bootstrapping is also used in general information extraction. Brin (1998) shows how to alternate between extracting instances of a class and inducing new instance-extracting patterns. Collins and Singer (1999) and Cucerzan and Yarowsky (1999) apply bootstrapping to the related task of named-entity recognition. Our approach was directly influenced by the hypernym-extractor of Snow et al. (2005) and we provided an analogous summary in Section 1. While their approach uses WordNet to label hypernyms in raw text, our initial labels are generated automatically. Etzioni et al. (2005) also require no labeled data or hand-labeled seeds for their namedentity extractor, but by comparison their classifier only uses a very small number of both features and automatically-generated training examples. 7 Conclusion We have shown how noun-pronoun c"
W09-1116,N03-1006,0,0.0682873,"an be handled together for pronoun resolution, it might be useful to learn them separately for other applications. Other statistical approaches to English noun gender are discussed in Section 2. In languages with ‘grammatical’ gender and plentiful gold standard data, gender can be tagged along with other word properties using standard supervised tagging techniques (Hajiˇc and Hladk´a, 1997). While our approach is the first to exploit a dual or orthogonal representation of English noun gender, a bootstrapping approach has been applied to 127 determining grammatical gender in other languages by Cucerzan and Yarowsky (2003). In their work, the two orthogonal views are: 1) the context of the noun, and 2) the noun’s morphological properties. Bootstrapping with these views is possible in other languages where context is highly predictive of gender class, since contextual words like adjectives and determiners inflect to agree with the grammatical noun gender. We initially attempted a similar system for English noun gender but found context alone to be insufficiently predictive. Bootstrapping is also used in general information extraction. Brin (1998) shows how to alternate between extracting instances of a class and"
W09-1116,N07-1030,0,0.0662004,"Missing"
W09-1116,A97-1017,0,0.307547,"Missing"
W09-1116,N01-1008,0,0.0392655,"Missing"
W09-1116,ide-suderman-2004-american,0,0.012505,"in Section 4.2. 124 4 Experiments 4.1 Set-up We parsed the 3 GB AQUAINT corpus (Vorhees, 2002) using Minipar (Lin, 1998) to create our unlabeled data. We process this data as described in Section 3, making feature vectors from the first 4 million noun groups. We train from these examples using a linear-kernel SVM via the the efficient SVMmulticlass instance of the SVMstruct software package (Tsochantaridis et al., 2004). To create our gold-standard gender data, we follow Bergsma (2005) in extracting gender information from the anaphora-annotated portion6 of the American National Corpus (ANC) (Ide and Suderman, 2004). In each document, we first group all nouns with a common lower-case string (exactly as done for our example extraction (Section 3.1)). Next, for each group we determine if a third-person pronoun refers to any noun in that group. If so, we label all nouns in the group with the gender of the referring pronoun. For example, if the pronoun he refers to a noun Brown, then all instances of Brown in the document are labeled as masculine. We extract the genders for 2794 nouns in the ANC training set (in 798 noun groups) and 2596 nouns in the ANC test set (in 642 groups). We apply this method to othe"
W09-1116,C96-1021,0,0.0567359,"Missing"
W09-1116,J94-4002,0,0.201255,"Missing"
W09-1116,N04-1041,0,0.0834965,"Missing"
W09-1116,J01-4004,0,0.204485,"Missing"
W09-1116,P95-1026,0,0.0753268,"ent nouns from dominating our training data, we only keep the first 200 groups corresponding to each noun string. Figure 2 gives an example training noun group with some (selected) context sentences. At test time, all nouns in the test documents are converted to this format for further processing. We group nouns because there is a strong tendency for nouns to have only one sense (and hence gender) per discourse. We extract contexts because nearby words provide good clues about which gender is being used. The notion that nouns have only one sense per discourse/collocation was also exploited by Yarowsky (1995) in his seminal work on bootstrapping for word sense disambiguation. 3.2 Feature vectors Once the training instances are extracted, they are converted to labeled feature vectors for supervised learning. The automatically-determined gender provides the class label (e.g., masculine for the group in Figure 2). The features identify properties of the noun and its context that potentially correlate with a particular gender category. We divide the features into two sets: those that depend on the contexts within the document (Context features: features of the tokens in the document), and those that d"
W09-1116,W98-1119,0,\N,Missing
W09-1116,P03-1001,0,\N,Missing
W10-2921,P08-1002,1,0.874272,"Missing"
W10-2921,P07-1056,0,0.00915401,"om the cumulative effect of previous updates. Dredze et al. (2008) maintain the variance of each weight and use this to guide the online updates. However, covariance between weights is not considered. We believe new SVM regularizations in general, and variance regularization in particular, will increasingly be used in combination with related NLP strategies that learn better when labeled data is scarce. These may include: using more-general features, e.g. ones generated from raw text (Miller et al., 2004; Koo et al., 2008), leveraging out-ofdomain examples to improve in-domain classification (Blitzer et al., 2007; Daum´e III, 2007), active learning (Cohn et al., 1994; Tong and Koller, 2002), and approaches that treat unlabeled data as labeled, such as bootstrapping (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and selftraining (McClosky et al., 2006). the VAR-SVM in three directions: efficiency, representational power, and problem domain. While we optimized the VAR-SVM objective in CPLEX, general purpose QP-solvers “do not exploit the special structure of [the SVM optimization] problem,” and consequently often train in time super-linear with the number of training examples (Joachims et al.,"
W10-2921,W07-1604,0,0.0171433,"nsupervised system) that predict that class. We denote these augmented systems with a + as in K - SVM + and CS - SVM + . Web-Scale N-gram VAR-SVM If variance regularization is applied to all weights, attributes C OUNT(in Russia during), C OUNT(Russia during 1997), and C OUNT(during 1997 to) will be encouraged to have similar weights in the score for class during. Furthermore, these will be weighted similarly to other patterns, filled with other prepositions, used in the scores for other classes. 4 Applications 4.1 Preposition Selection Preposition errors are common among new English speakers (Chodorow et al., 2007). Systems that can reliably identify these errors are needed in word processing and educational software. 4 Weights must appear in ≥1 subsets (possibly only in the Cj = I subset). Each occurs in at most one in our experiments. Note it is straightforward to express this as a single covariance matrix regularizer over w; ¯ we omit the details. 176 System O vA -SVM K - SVM K - SVM + CS - SVM CS - SVM + VAR-SVM 10 16.0 13.7 22.2 27.1 39.6 73.8 Training Examples 100 1K 10K 100K 50.6 66.1 71.1 73.5 50.0 65.8 72.0 74.7 56.8 70.5 73.7 75.2 58.8 69.0 73.5 74.2 64.8 71.5 74.0 74.4 74.2 74.7 74.9 74.9 Sys"
W10-2921,D07-1021,0,0.0572833,"Missing"
W10-2921,P08-1068,0,0.0220608,"e, rather than strictly minimizing ||w|| ¯ 2 . The target vector is the vector learned from the cumulative effect of previous updates. Dredze et al. (2008) maintain the variance of each weight and use this to guide the online updates. However, covariance between weights is not considered. We believe new SVM regularizations in general, and variance regularization in particular, will increasingly be used in combination with related NLP strategies that learn better when labeled data is scarce. These may include: using more-general features, e.g. ones generated from raw text (Miller et al., 2004; Koo et al., 2008), leveraging out-ofdomain examples to improve in-domain classification (Blitzer et al., 2007; Daum´e III, 2007), active learning (Cohn et al., 1994; Tong and Koller, 2002), and approaches that treat unlabeled data as labeled, such as bootstrapping (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and selftraining (McClosky et al., 2006). the VAR-SVM in three directions: efficiency, representational power, and problem domain. While we optimized the VAR-SVM objective in CPLEX, general purpose QP-solvers “do not exploit the special structure of [the SVM optimization] problem,” and conseque"
W10-2921,N06-1020,0,0.0362618,"variance regularization in particular, will increasingly be used in combination with related NLP strategies that learn better when labeled data is scarce. These may include: using more-general features, e.g. ones generated from raw text (Miller et al., 2004; Koo et al., 2008), leveraging out-ofdomain examples to improve in-domain classification (Blitzer et al., 2007; Daum´e III, 2007), active learning (Cohn et al., 1994; Tong and Koller, 2002), and approaches that treat unlabeled data as labeled, such as bootstrapping (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and selftraining (McClosky et al., 2006). the VAR-SVM in three directions: efficiency, representational power, and problem domain. While we optimized the VAR-SVM objective in CPLEX, general purpose QP-solvers “do not exploit the special structure of [the SVM optimization] problem,” and consequently often train in time super-linear with the number of training examples (Joachims et al., 2009). It would be useful to fit our optimization problem to efficient SVM training methods, especially for linear classifiers. VAR-SVM ’s representational power could be extended by using non-linear SVMs. Kernels can be used with a covariance regulari"
W10-2921,N04-1043,0,0.0148574,"vector at each update, rather than strictly minimizing ||w|| ¯ 2 . The target vector is the vector learned from the cumulative effect of previous updates. Dredze et al. (2008) maintain the variance of each weight and use this to guide the online updates. However, covariance between weights is not considered. We believe new SVM regularizations in general, and variance regularization in particular, will increasingly be used in combination with related NLP strategies that learn better when labeled data is scarce. These may include: using more-general features, e.g. ones generated from raw text (Miller et al., 2004; Koo et al., 2008), leveraging out-ofdomain examples to improve in-domain classification (Blitzer et al., 2007; Daum´e III, 2007), active learning (Cohn et al., 1994; Tong and Koller, 2002), and approaches that treat unlabeled data as labeled, such as bootstrapping (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and selftraining (McClosky et al., 2006). the VAR-SVM in three directions: efficiency, representational power, and problem domain. While we optimized the VAR-SVM objective in CPLEX, general purpose QP-solvers “do not exploit the special structure of [the SVM optimization] pro"
W10-2921,P07-1033,0,0.0813338,"Missing"
W10-2921,N09-1065,0,0.0445978,"Missing"
W10-2921,P02-1038,0,0.0313626,"eights, and is therefore computationally no harder to optimize than a standard SVM. Variance regularization is shown to enable dramatic improvements in the learning rates of SVMs on three lexical disambiguation tasks. 1 Introduction • I worked in Russia from 1997 to 2001. • I worked in Russia *during 1997 to 2001. Discriminative training is commonly used in NLP and speech to scale the contribution of different models or systems in a combined predictor. For example, discriminative training can be used to scale the contribution of the language model and translation model in machine translation (Och and Ney, 2002). Without training data, it is often reasonable to weight the different models equally. We propose a simple technique that exploits this intuition for better learning with fewer training examples. We regularize the feature weights in a Support Vector Machine (Cortes and Vapnik, 1995) toward a low-variance solution. Since the new SVM quadratic program is convex, it is no harder to optimize than the standard SVM objective. When training data is generated through human effort, faster learning saves time and money. When examples are labeled automatically, through user feedback (Joachims, 2002) or"
W10-2921,P07-1010,0,0.0188003,"ght the different models equally. We propose a simple technique that exploits this intuition for better learning with fewer training examples. We regularize the feature weights in a Support Vector Machine (Cortes and Vapnik, 1995) toward a low-variance solution. Since the new SVM quadratic program is convex, it is no harder to optimize than the standard SVM objective. When training data is generated through human effort, faster learning saves time and money. When examples are labeled automatically, through user feedback (Joachims, 2002) or from textual pseudo-examples (Smith and Eisner, 2005; Okanohara and Tsujii, 2007), faster learning can reduce the lag before a new system is useful. We demonstrate faster learning on lexical disambiguation tasks. For these tasks, a system predicts a label for a word in text, based on the Bergsma et al. (2009) use a variety of web counts to predict the correct preposition. They have features for C OUNT(in Russia from), C OUNT(Russia from 1997), C OUNT(from 1997 to), etc. If these are high, from is predicted. Similarly, they have features for C OUNT(in Russia during), C OUNT(Russia during 1997), C OUNT(during 1997 to). These features predict during. All counts are in the log"
W10-2921,P05-1044,0,0.0298755,"often reasonable to weight the different models equally. We propose a simple technique that exploits this intuition for better learning with fewer training examples. We regularize the feature weights in a Support Vector Machine (Cortes and Vapnik, 1995) toward a low-variance solution. Since the new SVM quadratic program is convex, it is no harder to optimize than the standard SVM objective. When training data is generated through human effort, faster learning saves time and money. When examples are labeled automatically, through user feedback (Joachims, 2002) or from textual pseudo-examples (Smith and Eisner, 2005; Okanohara and Tsujii, 2007), faster learning can reduce the lag before a new system is useful. We demonstrate faster learning on lexical disambiguation tasks. For these tasks, a system predicts a label for a word in text, based on the Bergsma et al. (2009) use a variety of web counts to predict the correct preposition. They have features for C OUNT(in Russia from), C OUNT(Russia from 1997), C OUNT(from 1997 to), etc. If these are high, from is predicted. Similarly, they have features for C OUNT(in Russia during), C OUNT(Russia during 1997), C OUNT(during 1997 to). These features predict duri"
W10-2921,C08-1109,0,0.0269362,"Missing"
W10-2921,W06-2904,1,0.798347,"h consequences discussed below). We thus separately minimize the variance of the it pattern weights and the they pattern weights. We use 1K training, 533 development, and 534 test examples. 5 Related Work There is a large body of work on regularization in machine learning, including work that uses positive semi-definite matrices in the SVM quadratic program. The graph Laplacian has been used to encourage geometrically-similar feature vectors to be classified similarly (Belkin et al., 2006). An appealing property of these approaches is that they incorporate information from unlabeled examples. Wang et al. (2006) use Laplacian regularization for the task of dependency parsing. They regularize such that features for distributionally-similar words have similar weights. Rather than penalize pairwise differences proportional to a similarity function, we simply penalize weight variance. In the field of computer vision, Tefas et al. (2001) (binary) and Kotsia et al. (2009) (multiclass) also regularize weights with respect to a covariance matrix. They use labeled data to find the sum of the sample covariance matrices from each class, similar to linear discriminant analysis. We propose the idea in general, an"
W10-2921,P95-1026,0,0.0752591,"ghts is not considered. We believe new SVM regularizations in general, and variance regularization in particular, will increasingly be used in combination with related NLP strategies that learn better when labeled data is scarce. These may include: using more-general features, e.g. ones generated from raw text (Miller et al., 2004; Koo et al., 2008), leveraging out-ofdomain examples to improve in-domain classification (Blitzer et al., 2007; Daum´e III, 2007), active learning (Cohn et al., 1994; Tong and Koller, 2002), and approaches that treat unlabeled data as labeled, such as bootstrapping (Yarowsky, 1995), co-training (Blum and Mitchell, 1998), and selftraining (McClosky et al., 2006). the VAR-SVM in three directions: efficiency, representational power, and problem domain. While we optimized the VAR-SVM objective in CPLEX, general purpose QP-solvers “do not exploit the special structure of [the SVM optimization] problem,” and consequently often train in time super-linear with the number of training examples (Joachims et al., 2009). It would be useful to fit our optimization problem to efficient SVM training methods, especially for linear classifiers. VAR-SVM ’s representational power could be"
W10-2921,N07-1033,0,0.0684963,"Missing"
Y09-1024,W09-1116,1,0.851404,"Missing"
Y09-1024,W99-0611,0,0.0356644,"will further boost the performance because the difficult cases tackled by these two methods are complementary. 7 Related Work Our method exhibits a fundamental advantage over supervised learning algorithm (including Boschee et al., 2005; Ji and Grishman, 2006; Zitouni and Florian, 2008) as it does not require costly hand-labeled training data. It thrives on web-scale Google n-gram data and discovers semantic knowledge corresponding to the task of mention detection. The use of gender information stems from a lot of prior work on pronoun resolution. Most of these methods (e.g. Ge et al., 1998; Cardie and Wagstaff, 1999) encoded the gender information as hard constraints. Hale and Charniak (1998) obtained gender statistics by using an anaphora algorithm on a large corpus. Bergsma et al. (2005, 2009a) mined gender information from the web and parsed corpora and incorporated gender probabilities as additional features in supervised learning. To the best of our knowledge, this is the first work on exploiting gender information for mention detection and in an unsupervised learning framework. Some very recent work used Google n-gram data for other NLP tasks such as lexical disambiguation (Bergsma et al., 2009b). L"
Y09-1024,P03-1001,0,0.0141265,"additional features in supervised learning. To the best of our knowledge, this is the first work on exploiting gender information for mention detection and in an unsupervised learning framework. Some very recent work used Google n-gram data for other NLP tasks such as lexical disambiguation (Bergsma et al., 2009b). Limited prior work has used manually constructed knowledge resources such as WordNet for Animacy Discovery (Evans and Orasan, 2000). Our offline strategy for acquiring gender and animacy information for online mention detection is similar to that for question answering described in Fleischman et al. (2003). And our approach of using pronoun context to improve mention detection is similar to the idea of refining name tagging based on coreference feedback in (Ji et al., 2005). 8 Conclusion Using mention detection as a case study, we have demonstrated that unsupervised learning methods can achieve comparable performance for some particular tasks if we discover semantic knowledge corresponding to each task. Our method harnesses the probabilistic lexical properties such as gender and animacy discovered from web-scale n-grams, and therefore can identify more rare mentions than the traditional supervi"
Y09-1024,W98-1119,0,0.0248626,"covered knowledge will further boost the performance because the difficult cases tackled by these two methods are complementary. 7 Related Work Our method exhibits a fundamental advantage over supervised learning algorithm (including Boschee et al., 2005; Ji and Grishman, 2006; Zitouni and Florian, 2008) as it does not require costly hand-labeled training data. It thrives on web-scale Google n-gram data and discovers semantic knowledge corresponding to the task of mention detection. The use of gender information stems from a lot of prior work on pronoun resolution. Most of these methods (e.g. Ge et al., 1998; Cardie and Wagstaff, 1999) encoded the gender information as hard constraints. Hale and Charniak (1998) obtained gender statistics by using an anaphora algorithm on a large corpus. Bergsma et al. (2005, 2009a) mined gender information from the web and parsed corpora and incorporated gender probabilities as additional features in supervised learning. To the best of our knowledge, this is the first work on exploiting gender information for mention detection and in an unsupervised learning framework. Some very recent work used Google n-gram data for other NLP tasks such as lexical disambiguatio"
Y09-1024,W06-0206,1,0.704209,"y annotated corpora and gazetteers. Keywords: Knowledge Discovery, Mention Detection, N-Grams, Gender, Animacy 1 Introduction The task of detecting entity mentions (references to entities) is very important to the downstream processing of information extraction such as coreference resolution and event extraction. Entity mentions can be divided into name mentions (e.g. “John Smith”), nominal mentions (e.g. “president”) and pronouns (e.g. “he”, “she”). Typical mention detection systems are based on supervised learning (Boschee et al., 2005; Zitouni and Florian, 2008) or semisupervised learning (Ji and Grishman, 2006). Achieving really high performance for mention detection requires deep semantic knowledge and large costly hand-labeled data. Many systems also exploited lexical gazetteers such as census data with gender information. However, such knowledge is relatively static (it is not updated during the extraction process), expensive to construct, and doesn’t include any probabilistic information. Mention detection is by definition a semantic task: for example, a phrase is a person mention if it refers to a real-world person entity. We should thus expect a successful mention detection system to exploit w"
Y09-1024,H05-1003,1,0.838874,"ning framework. Some very recent work used Google n-gram data for other NLP tasks such as lexical disambiguation (Bergsma et al., 2009b). Limited prior work has used manually constructed knowledge resources such as WordNet for Animacy Discovery (Evans and Orasan, 2000). Our offline strategy for acquiring gender and animacy information for online mention detection is similar to that for question answering described in Fleischman et al. (2003). And our approach of using pronoun context to improve mention detection is similar to the idea of refining name tagging based on coreference feedback in (Ji et al., 2005). 8 Conclusion Using mention detection as a case study, we have demonstrated that unsupervised learning methods can achieve comparable performance for some particular tasks if we discover semantic knowledge corresponding to each task. Our method harnesses the probabilistic lexical properties such as gender and animacy discovered from web-scale n-grams, and therefore can identify more rare mentions than the traditional supervised learning methods based on limited and static semantic resources. Also as an unsupervised learning approach it performs surprisingly well especially on recall. We have"
Y09-1024,D08-1063,0,0.110374,"e-art supervised learning methods which require manually annotated corpora and gazetteers. Keywords: Knowledge Discovery, Mention Detection, N-Grams, Gender, Animacy 1 Introduction The task of detecting entity mentions (references to entities) is very important to the downstream processing of information extraction such as coreference resolution and event extraction. Entity mentions can be divided into name mentions (e.g. “John Smith”), nominal mentions (e.g. “president”) and pronouns (e.g. “he”, “she”). Typical mention detection systems are based on supervised learning (Boschee et al., 2005; Zitouni and Florian, 2008) or semisupervised learning (Ji and Grishman, 2006). Achieving really high performance for mention detection requires deep semantic knowledge and large costly hand-labeled data. Many systems also exploited lexical gazetteers such as census data with gender information. However, such knowledge is relatively static (it is not updated during the extraction process), expensive to construct, and doesn’t include any probabilistic information. Mention detection is by definition a semantic task: for example, a phrase is a person mention if it refers to a real-world person entity. We should thus expect"
