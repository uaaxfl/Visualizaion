2020.inlg-1.2,W18-6507,0,0.063339,"Missing"
2020.inlg-1.22,P19-1483,0,0.0333685,"PYRAMID analysis (Yang et al., 2016). However, PYRAMID focuses on checking whether expected content is present, not finding mistakes in unexpected content. In the context of evaluating computer-generated sports stories, Wiseman et al. (2017) showed sentences (not complete stories) to human subjects, and asked the subjects to count how many facts in the sentence were supported by game data and how many contradicted the game data. These results were then compared to metrics based on information extraction techniques. This was repeated by Puduppully et al. (2019) and extended to other domains by Dhingra et al. (2019). Another metric which semantically analyses generated text and compares this to the source data is SPICE (Anderson et al., 2016), which uses this approach to evaluate the quality of computergenerated image captions, Accuracy-checking is also an issue in fact checking and verification. The FEVER workshops and shared tasks (Thorne et al., 2018b, 2019) asked participants to develop techniques to identify factual The Memphis Grizzlies (5-2) defeated the Phoenix Suns (3 - 2) Monday 102-91 at the Talking Stick Resort Arena in Phoenix. The Grizzlies had a strong first half where they out-scored the"
2020.inlg-1.22,W19-8652,0,0.12006,"Missing"
2020.inlg-1.22,W18-6539,0,0.074573,"Missing"
2020.inlg-1.22,D19-1310,0,0.0870184,"are happy to discuss this. Its also worth noting that from a pragmatic perspective its probably easier for annotators who have domain expertise to detect real-world errors. They do not need to check whether things they already know to be are true are present in the input data, and they can use existing resources (tools, websites, etc) which they are familiar with to find out what actually happened, without worrying about whether all the information in the resource is present in the NLG system’s input data. It is possible that the systems being evaluated used differing input data. For example Gong et al. (2019) include data from games beyond the one which is the focus of the summary. This means that from a practical perspective we would need to create multiple user interfaces to present data to the annotators if we want annotators to check whether facts are in a system’s input data set. 3.2 Categories We ask our annotators to categorise errors into one of the following categories Since this particular NLG system had no information available for the next game, the above sentence is pure hallucination. However, the information could still be correct, by almost sheer luck. If the sentence is factually"
2020.inlg-1.22,W19-8643,0,0.126759,"Missing"
2020.inlg-1.22,2020.acl-main.448,0,0.0356393,"neration, pages 158–168, c Dublin, Ireland, 15-18 December, 2020. 2020 Association for Computational Linguistics sports stories has been released on GitHub1 . 2 Related Work NLG systems can be evaluated using either automatic metrics or human evaluation (Celikyilmaz et al., 2020). Automatic metrics such as BLEU are not very meaningful in NLG (Reiter, 2018), especially when assessing accuracy (Reiter and Belz, 2009). Even in machine translation, BLEU and related metrics are not meaningful unless the differences in metric scores is quite large, much larger than reported in most academic papers (Mathur et al., 2020). Human evaluation of NLG systems is usually done using Likert scales or ratings (van der Lee et al., 2019). In the context of evaluating accuracy, human evaluators are usually asked to assess the overall accuracy of a generated text (Reiter and Belz, 2009), or to compare two texts and say which text is overall most accurate (Reiter et al., 2005; Novikova et al., 2018). The Pyramid method (Nenkova and Passonneau, 2004) in text summarisation is a complex technique for evaluating the quality of a summary from a content perspective. It originally required substantial human input, but recently the"
2020.inlg-1.22,N04-1019,0,0.223646,"2009). Even in machine translation, BLEU and related metrics are not meaningful unless the differences in metric scores is quite large, much larger than reported in most academic papers (Mathur et al., 2020). Human evaluation of NLG systems is usually done using Likert scales or ratings (van der Lee et al., 2019). In the context of evaluating accuracy, human evaluators are usually asked to assess the overall accuracy of a generated text (Reiter and Belz, 2009), or to compare two texts and say which text is overall most accurate (Reiter et al., 2005; Novikova et al., 2018). The Pyramid method (Nenkova and Passonneau, 2004) in text summarisation is a complex technique for evaluating the quality of a summary from a content perspective. It originally required substantial human input, but recently there have been attempts to automate PYRAMID analysis (Yang et al., 2016). However, PYRAMID focuses on checking whether expected content is present, not finding mistakes in unexpected content. In the context of evaluating computer-generated sports stories, Wiseman et al. (2017) showed sentences (not complete stories) to human subjects, and asked the subjects to count how many facts in the sentence were supported by game d"
2020.inlg-1.22,N18-2012,0,0.0412748,"Missing"
2020.inlg-1.22,J18-3002,1,0.810742,"systems, with some manual adjustment to keep the example simple. The materials used to perform the evaluation described below, as well as the small corpus of 21 accuracy-annotated 158 Proceedings of The 13th International Conference on Natural Language Generation, pages 158–168, c Dublin, Ireland, 15-18 December, 2020. 2020 Association for Computational Linguistics sports stories has been released on GitHub1 . 2 Related Work NLG systems can be evaluated using either automatic metrics or human evaluation (Celikyilmaz et al., 2020). Automatic metrics such as BLEU are not very meaningful in NLG (Reiter, 2018), especially when assessing accuracy (Reiter and Belz, 2009). Even in machine translation, BLEU and related metrics are not meaningful unless the differences in metric scores is quite large, much larger than reported in most academic papers (Mathur et al., 2020). Human evaluation of NLG systems is usually done using Likert scales or ratings (van der Lee et al., 2019). In the context of evaluating accuracy, human evaluators are usually asked to assess the overall accuracy of a generated text (Reiter and Belz, 2009), or to compare two texts and say which text is overall most accurate (Reiter et"
2020.inlg-1.22,J09-4008,1,0.712642,"ample simple. The materials used to perform the evaluation described below, as well as the small corpus of 21 accuracy-annotated 158 Proceedings of The 13th International Conference on Natural Language Generation, pages 158–168, c Dublin, Ireland, 15-18 December, 2020. 2020 Association for Computational Linguistics sports stories has been released on GitHub1 . 2 Related Work NLG systems can be evaluated using either automatic metrics or human evaluation (Celikyilmaz et al., 2020). Automatic metrics such as BLEU are not very meaningful in NLG (Reiter, 2018), especially when assessing accuracy (Reiter and Belz, 2009). Even in machine translation, BLEU and related metrics are not meaningful unless the differences in metric scores is quite large, much larger than reported in most academic papers (Mathur et al., 2020). Human evaluation of NLG systems is usually done using Likert scales or ratings (van der Lee et al., 2019). In the context of evaluating accuracy, human evaluators are usually asked to assess the overall accuracy of a generated text (Reiter and Belz, 2009), or to compare two texts and say which text is overall most accurate (Reiter et al., 2005; Novikova et al., 2018). The Pyramid method (Nenko"
2020.inlg-1.22,2020.inlg-1.28,1,0.715671,"age annotators to follow guidelines such as annotating incorrect days-of-week as Name errors. We would also like to annotate human authored texts using our methodology in order to provide a topline for NLG systems. We are also planning to run a shared task for accuracy evaluation, where researchers propose faster and cheaper ways of finding accuracy mistakes (either via a human protocol or with a metric), and these techniques are evaluated by seeing how closely they correlate with the gold-standard accuracy evaluation described in this paper. The shared task is described in a companion paper (Reiter and Thomson, 2020). 6 Conclusion Texts generated by NLG systems need to be accurate, but current neural NLG systems often generate texts with many mistakes (Table 2). In order to fix this problem, we need to be able to measure how accurate texts are. The methodology we present here will allow developers of data-to-text NLG systems to measure the accuracy of texts produced by their systems, It will also make it easier for other researchers to develop cheaper and quicker techniques for measuring accuracy, by giving them a gold standard which they can use to validate their ideas. 7 Acknowledgements Many thanks to"
2020.inlg-1.22,2020.inlg-1.6,1,0.694238,"eraged to optimise systems. RG uses an information extraction model trained to link facts in the text to tuples in the data. It operates on the same training corpus as the language generation model itself. During evaluation, RG extracts tuples from generated text, then compares these with known tuples in the data. Each tuple has a data type, which can be matched to the error types we define in subsection 3.2. RG only reports errors of the ‘name’ and ‘number’ categories; it cannot detect word, context, not-checkable, and other errors. We used an extended version of the RG annotation algorithm (Thomson et al., 2020) which detects additional relations such as the day of the week each game was played on and subsequent opponents for each team. We trained IE models following the general procedure proposed by (Wiseman et al., 2017). We trained with 3 random seeds and 5 learning rates, then manually chose the best 3 LSTM and best 3 Convolutional models to ensemble. We show recall and precision in Table 3. Note that recall in this context is not that of the IE models themselves, but rather the fraction of accuracy errors in our gold standard annotation which were detected by RG. Precision is the fraction of err"
2020.inlg-1.22,N18-1074,0,0.0553831,"Missing"
2020.inlg-1.22,W18-5501,0,0.048447,"Missing"
2020.inlg-1.22,D19-6601,0,0.0285423,"Missing"
2020.inlg-1.22,W19-8639,0,0.0511367,"e accept as correct recall, any instance where RG identifies an error. games. RG does not have access to data for the previous game and therefore cannot detect such an error. The NLG system also does not have access to this data, yet often hallucinates such statements. Semantic control techniques for bringing the data into closer alignment with the text (Duˇsek et al., 2019) have been shown to be useful with less complex datasets such as the E2E Challenge (Duˇsek et al., 2018). However, aligning data and text with the level of complexity shown in the sports summaries is a more difficult task. Wang (2019) aims to prevent generation of sentences not grounded in the data, which does bring the data and generated text into closer alignment, although at the cost of limiting the types of sentence the system is capable of generating. We hope that comparison with gold standard annotations will lead to improved versions of RG and other metrics, as well as a better understanding of where they succeed and where they fail. If RG could reliably detect name and number errors, but not other categories of error, it would still be useful, provided that this limitation was clearly specified and understood. 5 Fu"
2020.inlg-1.28,W19-8643,0,0.13424,"Missing"
2020.inlg-1.28,P19-1195,0,0.0233833,"report performance per season and per game, not performance up to a particular point in a season. Figure 1: Example text with error annotations. Corrections and explanations are not required, but are included here for clarity. Box score data for this game is available at https://www.basketball-reference.com/ boxscores/201411050PHO.html . We also plan to have an ’open’ track where people can submit ideas for evaluating accuracy on our data set which do not fit into the above framework. 4 Data We will use texts produced by three systems that use basketball box score data: Wiseman et al. (2017), Puduppully et al. (2019a), and Rebuffel et al. (2020). We will carefully fact-check, using the protocol of Thomson and Reiter (2020), 60 texts (twenty from each system). The three systems we have chosen all explored different ways of modifying the neural architecture. The system of Wiseman et al. (2017) defined the Rotowire task and provided initial benchmarks for machine translation systems using copy attention, it is included for this reason. Puduppully et al. (2019a) jointly conditioned on a document plan, whilst Rebuffel et al. (2020) used a hierarchical encoder to group attributes (such as statistics) by their"
2020.inlg-1.28,2020.inlg-1.22,1,0.838277,"all games produced from basketball box score and other game data. We welcome submissions based on protocols for human evaluation, automatic metrics, as well as combinations of human evaluations and metrics. 1 Introduction Users expect data-to-text NLG systems to generate textual summaries which are accurate. However, many neural NLG systems in particular generate texts which are factually incorrect. The most reliable way to assess the accuracy of a generated text is to ask human annotators to carefully fact-check the text. However this is a time-consuming process. Our experiences at Aberdeen (Thomson and Reiter, 2020) show that it can take an experienced annotator 30 minutes to factcheck a moderately complex 300-word paragraph produced by a neural data-to-text NLG system. It would be very useful to the NLG community if we could come up with quicker and easier ways of measuring accuracy which have good correlations with careful fact-checking. Such methods could be based on less time-consuming human evaluations, such as asking subjects to rate the accuracy of a text on a Likert-type scale (van der Lee et al., 2019), or on automatic metrics. However, we should only use such techniques if we feel confident tha"
2020.inlg-1.28,D19-1310,0,0.0456106,"e three systems we have chosen all explored different ways of modifying the neural architecture. The system of Wiseman et al. (2017) defined the Rotowire task and provided initial benchmarks for machine translation systems using copy attention, it is included for this reason. Puduppully et al. (2019a) jointly conditioned on a document plan, whilst Rebuffel et al. (2020) used a hierarchical encoder to group attributes (such as statistics) by their respective entities (players/teams). Other systems in this domain which could be used for evaluation include Puduppully et al. (2019b), Wang (2019), Gong et al. (2019), and Iso et al. (2019). Our aim, however, is to assess how well results produced by the participant’s evaluation techniques correlate with the gold-standard fact-checking. Hence we are looking for a set of systems which generate texts that contain a significant number of accuracy errors, not complete coverage of all systems that generate texts from basketball box score data. In Thomson and Reiter (2020), we looked at a number of texts generated by the three systems we have selected. No text was error free (the lowest was 7 errors) and the average number of errors per text is about 20. We will"
2020.inlg-1.29,W19-8642,0,0.0485808,"Missing"
2020.inlg-1.29,W10-4201,0,0.0853033,"Missing"
2020.inlg-1.29,2020.inlg-1.24,1,0.833167,"ievable. We take this as our starting point, i.e. we assume we have been successful in reproducing system outputs. In the shared task we will not try to reproduce system outputs, but start from existing sets of outputs (and inputs, where applicable), and try to reproduce the results of human evaluations performed on them. 3.1 Replicating experiments The second condition is that the experiment that produced the human evaluation results must be replicable which means having access to detailed information about how the experiment was designed and run, but also that it is repeatable in principle. Belz et al. (2020) identify a set of 18 properties with associated value sets for characterising evaluations that are needed for replicability: in addition to defining quality criteria and evaluation mode, papers need to include, or give access to, full details of system outputs (number, how selected), evaluators (number, type, how selected), method for determining effect size and significance of findings, scale or other rating instrument (size, list or range of possible response values), how presented to evaluators, form of response elicitation, information given to evaluators, and experimental conditions. Thi"
2020.inlg-1.29,2020.lrec-1.680,0,0.074984,"Missing"
2020.inlg-1.29,2020.lrec-1.686,0,0.0488062,"Missing"
2020.inlg-1.29,2020.inlg-1.23,1,0.697719,"Missing"
2020.inlg-1.29,R19-1089,0,0.0555606,"Missing"
2020.inlg-1.29,D17-1238,0,0.0548086,"Missing"
2020.inlg-1.29,J18-3002,1,0.828971,"ears, we hope to be able to document an overall increase in levels of replicability and reproducibility over time. 1 Introduction 232 Proceedings of The 13th International Conference on Natural Language Generation, pages 232–236, c Dublin, Ireland, 15-18 December, 2020. 2020 Association for Computational Linguistics This shared task proposal is part of a wider effort to address reproducibility of human evaluations in NLG, a field in which they play a central role and which has always been wary of automatic evaluation metrics and their limitations (Reiter and Belz, 2009; Novikova et al., 2017; Reiter, 2018). Below we start by briefly diagnosing the reproducibility problem in NLG (Section 2), and looking at what the conditions are for reproducibility testing of results from human evaluation of NLG systems (Section 3). We then outline our ideas for a shared task that could help to understand and potentially address the problem (Section 4). We describe related research that has provided inspiration (Section 5), and conclude with next steps (Section 6). 2 Issues with Human Evaluation in NLG In an era dominated by metrics and leaderboards, human evaluation can be an afterthought, often carried out an"
2020.inlg-1.29,J09-4008,1,0.612228,"oducibility. If the task is run over several years, we hope to be able to document an overall increase in levels of replicability and reproducibility over time. 1 Introduction 232 Proceedings of The 13th International Conference on Natural Language Generation, pages 232–236, c Dublin, Ireland, 15-18 December, 2020. 2020 Association for Computational Linguistics This shared task proposal is part of a wider effort to address reproducibility of human evaluations in NLG, a field in which they play a central role and which has always been wary of automatic evaluation metrics and their limitations (Reiter and Belz, 2009; Novikova et al., 2017; Reiter, 2018). Below we start by briefly diagnosing the reproducibility problem in NLG (Section 2), and looking at what the conditions are for reproducibility testing of results from human evaluation of NLG systems (Section 3). We then outline our ideas for a shared task that could help to understand and potentially address the problem (Section 4). We describe related research that has provided inspiration (Section 5), and conclude with next steps (Section 6). 2 Issues with Human Evaluation in NLG In an era dominated by metrics and leaderboards, human evaluation can be"
2020.inlg-1.29,W19-8643,0,0.0365113,"Missing"
2020.intellang-1.4,P17-4012,0,0.015074,"Global Development Group) relational database (Codd, 1970), with (optional) objectrelational mappings (ORM) written in Ruby Sequel (Jeremy Evans). It provides researchers with the ability to query and filter data, in a simple and efficient way. The process of importing data into a normalized relational database also helps to verify the data, clean it, and eliminate redundancy. By writing simple scripts, either in SQL or using the ORM, data can be easily output in the format a researcher requires for their system. We tested this functionality by creating data for a recent OpenNMT based system (Klein et al., 2017; Rebuffel et al., 2020) (see section 3 for details). There are problems with the structure, quality and partitioning of existing Rotowire based datasets. Our investigation focuses on RotowireFG as it contains more games, although the underlying problems are the same in both datasets. Some of these issues are minor, such as the team a player is on being indexed by city rather than name (there are two teams in Los Angeles, the Clippers and the Lakers). Others, like the partition contamination discussed in subsection 2.3 are more serious. The JSON file format also becomes unwieldy as data size a"
2020.intellang-1.4,D14-1179,0,0.00723959,"Missing"
2020.intellang-1.4,W03-1016,0,0.311324,"Missing"
2020.intellang-1.4,D16-1128,0,0.0597374,"Missing"
2020.intellang-1.4,W19-8652,0,0.0331326,"Missing"
2020.intellang-1.4,P02-1040,0,0.12047,"pany such as ESPN might provide data and texts from previous seasons and expect in return an NLG system that generates texts as future seasons play out. We would ideally cross-validate, with different partition setups. However, at about 3 weeks for just one partition setup, we found compute time prohibitive. 3 Initial Experiments In order to confirm that our data can be easily used by existing systems, we exported it to the format of one of the more recent Seq2Seq architectures (Rebuffel et al., 2020) then attempted to replicate the results for BLEU, one of the more popular automated metrics (Papineni et al., 2002). BLEU scores do not correlate with human evaluation of text (Reiter, 2018) but may be useful in early system development. Our aim was to ensure that our data, particularly with the new partition scheme, could be used in place of the existing Rotowire datasets with minimal effort. We used the same hyper-paramaters as the original work, except for the batch size which we reduced from 32 to 16 due to hardware constraints, training 10 models with different random seeds for each of the below partition schemes: There are two problems with the existing partition scheme which need to be overcome. Fir"
2020.intellang-1.4,W18-6539,0,0.0402466,"Missing"
2020.intellang-1.4,W17-3518,0,0.0149157,"ld (Gatt and Krahmer, 2018). Data2Text NLG applications that use classical rules-based methods are currently not compara1 https://github.com/nlgcat/sport sett basketball ble with state-of-the-art Seq2Seq applications. For Seq2Seq applications, the input data is either very shallow, or the output texts exhibit a high degree of factual inaccuracy. The task here is very different to others such as chat bots (Adiwardana et al., 2020), where the focus is on appearing human, rather than conveying concise yet relevant information. A lot of research has been done with datasets created for the WebNLG (Gardent et al., 2017) and E2E (Duˇsek et al., 2018) challenges. There is also the more recent ToTTo dataset (Parikh et al., 2020). Such datasets can provide interesting sentencelevel insights, although the data structures are quite simple (a single table or simple schema) and the human-authored texts are short (usually one or two sentences). This makes them unsuitable for evaluating Data2Text systems which generate summaries based on more complex data analytics. This is not necessarily because the systems are incapable of doing so, but because the data is not available as input. In addition, rules-based systems fo"
2020.intellang-1.4,D19-1310,0,0.178052,"n addition, rules-based systems for problems based on these datasets are neither difficult or timeconsuming to implement, meaning it is harder to investigate their limitations under these conditions. The Rotowire dataset of basketball game summaries (Wiseman et al., 2017), along with the expanded (in terms of game count) Rotowire-FG (Wang, 2019), have become popular resources for investigating the generation of paragraph-sized insightful texts. Several different Seq2Seq models have been proposed, evaluated, and compared using them (Wiseman et al., 2017; Puduppully et al., 2019a,b; Wang, 2019; Gong et al., 2019; Rebuffel et al., 2020). The datasets consist of basketball box scores (see Table 1) and human-authored game summaries (see Figure 2). The example sentence in Figure 1 highlights the level of complexity in these summaries. It includes a set of average statistics for a player over multiple games, as well as the claim that this means the player ‘stayed dominant’. This is just one sentence of many in the full summary. We found Rotowire to be unsuitable for evaluating this in its current format. However, the domain itself is suitably complex and Rotowire provides a foundation upon which we can bu"
2020.intellang-1.4,P19-1195,0,0.0567423,"se the data is not available as input. In addition, rules-based systems for problems based on these datasets are neither difficult or timeconsuming to implement, meaning it is harder to investigate their limitations under these conditions. The Rotowire dataset of basketball game summaries (Wiseman et al., 2017), along with the expanded (in terms of game count) Rotowire-FG (Wang, 2019), have become popular resources for investigating the generation of paragraph-sized insightful texts. Several different Seq2Seq models have been proposed, evaluated, and compared using them (Wiseman et al., 2017; Puduppully et al., 2019a,b; Wang, 2019; Gong et al., 2019; Rebuffel et al., 2020). The datasets consist of basketball box scores (see Table 1) and human-authored game summaries (see Figure 2). The example sentence in Figure 1 highlights the level of complexity in these summaries. It includes a set of average statistics for a player over multiple games, as well as the claim that this means the player ‘stayed dominant’. This is just one sentence of many in the full summary. We found Rotowire to be unsuitable for evaluating this in its current format. However, the domain itself is suitably complex and Rotowire provides"
2020.intellang-1.4,J18-3002,1,0.809832,"eturn an NLG system that generates texts as future seasons play out. We would ideally cross-validate, with different partition setups. However, at about 3 weeks for just one partition setup, we found compute time prohibitive. 3 Initial Experiments In order to confirm that our data can be easily used by existing systems, we exported it to the format of one of the more recent Seq2Seq architectures (Rebuffel et al., 2020) then attempted to replicate the results for BLEU, one of the more popular automated metrics (Papineni et al., 2002). BLEU scores do not correlate with human evaluation of text (Reiter, 2018) but may be useful in early system development. Our aim was to ensure that our data, particularly with the new partition scheme, could be used in place of the existing Rotowire datasets with minimal effort. We used the same hyper-paramaters as the original work, except for the batch size which we reduced from 32 to 16 due to hardware constraints, training 10 models with different random seeds for each of the below partition schemes: There are two problems with the existing partition scheme which need to be overcome. Firstly, the partition contamination needs to be removed. This is as simple as"
2020.intellang-1.4,W19-8639,0,0.0649158,"e or two sentences). This makes them unsuitable for evaluating Data2Text systems which generate summaries based on more complex data analytics. This is not necessarily because the systems are incapable of doing so, but because the data is not available as input. In addition, rules-based systems for problems based on these datasets are neither difficult or timeconsuming to implement, meaning it is harder to investigate their limitations under these conditions. The Rotowire dataset of basketball game summaries (Wiseman et al., 2017), along with the expanded (in terms of game count) Rotowire-FG (Wang, 2019), have become popular resources for investigating the generation of paragraph-sized insightful texts. Several different Seq2Seq models have been proposed, evaluated, and compared using them (Wiseman et al., 2017; Puduppully et al., 2019a,b; Wang, 2019; Gong et al., 2019; Rebuffel et al., 2020). The datasets consist of basketball box scores (see Table 1) and human-authored game summaries (see Figure 2). The example sentence in Figure 1 highlights the level of complexity in these summaries. It includes a set of average statistics for a player over multiple games, as well as the claim that this m"
2020.intellang-1.5,W16-1617,0,0.0197644,"s optimised implementation of of KS test and found it to give the same p-values. The data-set was subdivided into three equal parts, each for training, validation, and testing. We also made sure that each portion had balanced cases of significant and insignificant pairs. 3.1.3 Finalisation of Base Model Architecture A domain-induced restriction of comparative insights is that the number of inputs is two and the number of outputs is one. Here, each input is the histogram of distribution and the output is the statistical significance. Based on previous works on similar input/output constraints (Neculoiu et al., 2016; Berlemont et al., 2015), we came up with three neural network architectures, namely, a recurrent neural network (RNNA), a modified RNN (RNNB) and a siamese network (SIAM). The schematics of the RNNA architecture are shown in Figure 1. The layers Ip1 and Ip2 are input layers, each having a fixed size of 100 elements. The layers F1 and F2, are fully connected layers, each with 50 neurons activated by a Leaky Rectified Linear Unit (ReLU) function. In fact, all layers in the network except the Final layer are activated by the Leaky ReLu function. Another level of Fully connected layers, namely,"
2020.intellang-1.7,D16-1150,0,0.0280415,"moking cessation (Reiter et al., 2003), driving conduct improvement (Braun et al., 2018), diet management (Anselma et al., 2018)(Anselma and Mazzei, 2018)(Anselma and Mazzei, 2017) and therapy recommendation (Skinner et al., 1994). Horizontal approaches deals with more general topics, like the key challenges and model design for behaviour change (Oinas-Kukkonen, 2013)(Kelders et al., 2016)(Oinas-Kukkonen and Harjumaa, 2009), argumentative persuasive communication (Zukerman et al., 2000) (Reed et al., 1996) or the inclusion of generic psychological traits and basic human values into the model (Ding and Pan, 2016). Finally, there have been some attempts to partially merge horizontal and vertical models (Maimone et al., 2018)(Dragoni et al., 2018)(Donadello et al., 2019). When the text must be built according to user traits, it is important to identify the variables which could influence not only the behaviour but also user engagement. These include general psychological tailoring techiques (Hawkins et al., 2008); user numeracy, literacy (Williams and Reiter, 2008) and domain knowledge (McKeown et al., 1993)(Paris, 1988); affect (Mahamood and Reiter, 2011). User stress is a factor which, to our knowledg"
2020.intellang-1.7,N18-5003,0,0.0186386,"at the correlation between stress and eating behaviour; Section 4.1 discusses possible methodologies to assess user stress, prioritising the non-intrusive ones; in Section 4.2 we propose two theoretical ways to tailor the text for a stressed user, along with an example; Section 5 closes the paper with our final considerations. 2 Related Work Natural Language Generation has been used in the healthcare domain in a variety of ways, with different aims. Since the spread of e-health, various works have focused on automating the creation of documents. some examples of these are clinical encounters (Finley et al., 2018), chief complaints (Lee, 2018) and handover reports (Schneider et al., 2013). There has also been some research on producing multiple texts from the same source, as in the BabyTalk project (Portet et al., 2008) which generated premature infants reports for doctors(Portet et al., 2009), nurses (Hunter et al., 2011) and parents (Mahamood and Reiter, 2011) in Neonatal Intensive Care Unit (NICU) environ1 PhilHumans Project page: https://www.philhumans.eu/ ment. Another scenario in which NLG contributed to healthcare is decision support (Binsted et al., 1995) (Hommes et al., 2019), where the text c"
2020.intellang-1.7,W19-8656,0,0.0281973,"Missing"
2020.intellang-1.7,W11-2803,1,0.885422,"al Language Generation has been used in the healthcare domain in a variety of ways, with different aims. Since the spread of e-health, various works have focused on automating the creation of documents. some examples of these are clinical encounters (Finley et al., 2018), chief complaints (Lee, 2018) and handover reports (Schneider et al., 2013). There has also been some research on producing multiple texts from the same source, as in the BabyTalk project (Portet et al., 2008) which generated premature infants reports for doctors(Portet et al., 2009), nurses (Hunter et al., 2011) and parents (Mahamood and Reiter, 2011) in Neonatal Intensive Care Unit (NICU) environ1 PhilHumans Project page: https://www.philhumans.eu/ ment. Another scenario in which NLG contributed to healthcare is decision support (Binsted et al., 1995) (Hommes et al., 2019), where the text can help patients understand their clinical situation, and prepare them for shared decision making (Seminar, 2011) (Stiggelbout et al., 2012). The majority of these systems don’t need any personal information about the reader because their final purpose is purely informative. It is, however, different when we move to behavioural change, as the text aims"
2020.intellang-1.7,P93-1031,0,0.174489,"al., 1996) or the inclusion of generic psychological traits and basic human values into the model (Ding and Pan, 2016). Finally, there have been some attempts to partially merge horizontal and vertical models (Maimone et al., 2018)(Dragoni et al., 2018)(Donadello et al., 2019). When the text must be built according to user traits, it is important to identify the variables which could influence not only the behaviour but also user engagement. These include general psychological tailoring techiques (Hawkins et al., 2008); user numeracy, literacy (Williams and Reiter, 2008) and domain knowledge (McKeown et al., 1993)(Paris, 1988); affect (Mahamood and Reiter, 2011). User stress is a factor which, to our knowledge, is largely ignored in the existing literature. Some NLG works have considered stress, but not in a dynamic way. Some works in tactical NLG (text which tries to induce a certain emotional status) (Van Der Sluis and Mellish, 2008) considered stress, but only to assess the text effect. This makes sense since tactical NLG aims to induce an emotional status instead of detecting or managing it. BabyTalk-Family project (Mahamood and Reiter, 2011) tried to actively tailor the document by estimating stre"
2020.intellang-1.7,J88-3006,0,0.370788,"usion of generic psychological traits and basic human values into the model (Ding and Pan, 2016). Finally, there have been some attempts to partially merge horizontal and vertical models (Maimone et al., 2018)(Dragoni et al., 2018)(Donadello et al., 2019). When the text must be built according to user traits, it is important to identify the variables which could influence not only the behaviour but also user engagement. These include general psychological tailoring techiques (Hawkins et al., 2008); user numeracy, literacy (Williams and Reiter, 2008) and domain knowledge (McKeown et al., 1993)(Paris, 1988); affect (Mahamood and Reiter, 2011). User stress is a factor which, to our knowledge, is largely ignored in the existing literature. Some NLG works have considered stress, but not in a dynamic way. Some works in tactical NLG (text which tries to induce a certain emotional status) (Van Der Sluis and Mellish, 2008) considered stress, but only to assess the text effect. This makes sense since tactical NLG aims to induce an emotional status instead of detecting or managing it. BabyTalk-Family project (Mahamood and Reiter, 2011) tried to actively tailor the document by estimating stress value, but"
2020.intellang-1.7,W13-2119,1,0.712952,"sses possible methodologies to assess user stress, prioritising the non-intrusive ones; in Section 4.2 we propose two theoretical ways to tailor the text for a stressed user, along with an example; Section 5 closes the paper with our final considerations. 2 Related Work Natural Language Generation has been used in the healthcare domain in a variety of ways, with different aims. Since the spread of e-health, various works have focused on automating the creation of documents. some examples of these are clinical encounters (Finley et al., 2018), chief complaints (Lee, 2018) and handover reports (Schneider et al., 2013). There has also been some research on producing multiple texts from the same source, as in the BabyTalk project (Portet et al., 2008) which generated premature infants reports for doctors(Portet et al., 2009), nurses (Hunter et al., 2011) and parents (Mahamood and Reiter, 2011) in Neonatal Intensive Care Unit (NICU) environ1 PhilHumans Project page: https://www.philhumans.eu/ ment. Another scenario in which NLG contributed to healthcare is decision support (Binsted et al., 1995) (Hommes et al., 2019), where the text can help patients understand their clinical situation, and prepare them for s"
2020.intellang-1.7,W00-1408,0,0.157559,"oring was tested for lifestyle improvements and behavioural change in various domains. Vertical approaches includes solutions for smoking cessation (Reiter et al., 2003), driving conduct improvement (Braun et al., 2018), diet management (Anselma et al., 2018)(Anselma and Mazzei, 2018)(Anselma and Mazzei, 2017) and therapy recommendation (Skinner et al., 1994). Horizontal approaches deals with more general topics, like the key challenges and model design for behaviour change (Oinas-Kukkonen, 2013)(Kelders et al., 2016)(Oinas-Kukkonen and Harjumaa, 2009), argumentative persuasive communication (Zukerman et al., 2000) (Reed et al., 1996) or the inclusion of generic psychological traits and basic human values into the model (Ding and Pan, 2016). Finally, there have been some attempts to partially merge horizontal and vertical models (Maimone et al., 2018)(Dragoni et al., 2018)(Donadello et al., 2019). When the text must be built according to user traits, it is important to identify the variables which could influence not only the behaviour but also user engagement. These include general psychological tailoring techiques (Hawkins et al., 2008); user numeracy, literacy (Williams and Reiter, 2008) and domain k"
2020.nl4xai-1.7,J18-3002,1,0.789342,"7. HumanCentric Justification of Machine Learning Predictions. In Proceedings of the Twenty-Sixth International Journal Joint Conferences on Artificial Intelligence, IJCAI 2017, pages 1461–1467. • Many of the explanations that have been generated have not been comprehensively validated to be informative or useful. Intrinsic and extrinsic evaluations should be conducted both by humans and using state of the art automatic metrics where appropriate. Determining how best to evaluate textual explanations of a BN will be a crucial component for their more widespread use in the future (Barros, 2019; Reiter, 2018). Anthony Costa Constantinou, Barbaros Yet, Norman Fenton, Martin Neil, and William Marsh. 2016. Value of information analysis for interventional and counterfactual Bayesian networks in forensic medical sciences. Artificial Intelligence in Medicine, 66:41–52. F. J. D´ıez, J. Mira, E. Iturralde, and S. Zubillaga. 1997. Diaval, a Bayesian expert system for echocardiography. Artificial Intelligence in Medicine, 10(1):59– 73. • It should be evaluated how natural language explanations compare with visual explanations and in which situations a particular style (or a combination of both) should be fa"
2020.nl4xai-1.7,W19-8402,1,0.873903,"ty may suffer due to the explanation in present tense of events that have taken place in different timelines, this example is a marked improvement on past textual explanations of a BN. A narrative is created around the defendant and vague, natural language is used to create arguments to persuade the juror; much more convincing than the common approach of printing observations and probabilistic values. If generated textual explanations are written for a purpose and an audience, have a narrative structure and explicitly communicate uncertainty, they can be a useful aid in explaining AI systems (Reiter, 2019). In early expert systems, explanation was considered a very important component of the system and textual explanations were identified as a solution for explaining reasoning to users (Shortliffe and Buchanan, 1984). Textual explanation was also identified as important for the explanation of Bayesian reasoning; Haddawy et al. (1997) claimed that textual explanation would not require the user to know anything about BNs in order to interact with it effectively. Many of the early textual explanations took the form of basic canned text and offered very stiff output. The developers of the early exp"
2021.eacl-main.29,2020.lrec-1.687,0,0.0233804,"for reproduction via an open call and direct selection. Participants had to ‘reproduce the paper,’ using information contained/linked in it. Participants submitted (a) a report on the reproduction, and (b) the software used to obtain the results as a Docker container (controlling variation from dependencies and run-time environments) on GitLab. Submissions were reviewed in great detail, submitted code was test-run and checked for hard-coding of results. 11 out of 18 submissions were judged to conform with requirements. One original paper (Vajjala and Rama, 2018) attracted four reproductions (Bestgen, 2020; Huber and C ¸ o¨ ltekin, 2020; Caines and Buttery, 2020; Arhiliuc et al., 2020) in what must be a groundbreaking first in NLP. See Table 1 for summaries of all 11 reproductions. An aspect the organisers did not control was how to draw conclusions about reproducibility; most contributions provide binary conclusions but vary in how similar they require results to be for success. E.g. the four papers reproducing Vajjala and Rama (2018) all report similarly large deviations, but only one (Arhiliuc et al., 2020) concludes that the reproduction was not a success. 6 Conclusions It seemed so simple:"
2021.eacl-main.29,J04-4004,0,0.0693029,"Missing"
2021.eacl-main.29,P18-1246,0,0.0589575,"Missing"
2021.eacl-main.29,P11-1028,0,0.0189095,"Missing"
2021.eacl-main.29,2020.lrec-1.680,0,0.543864,"Missing"
2021.eacl-main.29,dakota-kubler-2017-towards,0,0.0219829,"Missing"
2021.eacl-main.29,P13-1166,0,0.340727,"Missing"
2021.eacl-main.29,S19-2131,0,0.0618824,"Missing"
2021.eacl-main.29,2020.lrec-1.682,0,0.0842347,"Missing"
2021.eacl-main.29,J18-3002,1,0.835631,"currently characterised by growing diversity in all these respects. We start below by surveying concepts and definitions in reproducibility research, areas of particular disagreement, and identify categories of work in current NLP reproducibility research. We then use the latter to structure the remainder of the paper. Selection of Papers: We conducted a structured review employing a stated systematic process for identifying all papers in the field that met specific criteria. Structured reviews are a type of metareview more common in fields like medicine but beginning to be used more in NLP (Reiter, 2018; Howcroft et al., 2020). Specifically, we selected papers as follows. We 1 https://2020.emnlp.org/blog/ 2020-05-20-reproducibility 2 There are some situations where it is difficult to share data, e.g. because the data is commercially confidential or because it contains sensitive personal information. But the increasing expectation in NLP is that authors should share as much as possible, and justify cases where it is not possible. 381 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 381–393 April 19 - 23, 2021. ©2021 Association"
2021.eacl-main.29,2020.lrec-1.684,0,0.0641635,"Missing"
2021.eacl-main.29,2020.lrec-1.622,0,0.0321334,", Wieling et al. (2018) randomly select five papers each from ACL’11 and ACL’16 for which code/data was available. In a uniform design, original authors were contacted for help if needed, a maximum time limit of 8h was imposed, and all work was done by the same Masters student. It’s not clear how scores were selected (not all are attempted), and reasons for failure are not always clear even from linked material. Of the 120 score pairs obtained, 60 were the same, 12 reproduction scores were better, 22 were worse, and 26 runs failed (including exceeding the time limit). See Table 1 for summary. Rodrigues et al. (2020) recreated six SemEval’18 systems from the Argument Reasoning Comprehension Task, following system descriptions and/or reusing code, with no time limit. Scores were the same for one system, and within +/- 0.036 points for the other five; the SemEval ranking was exactly the same. Systems were also run on a corrected version of the shared-task data (which contained unwitting clues). This resulted in much lower scores casting doubt on the validity of the original shared task results. REPROLANG (Branco et al., 2020) is so far the only multi-lab (as well as multi-test) study of reproducibility in N"
2021.eacl-main.29,S18-1112,0,0.0289362,"Missing"
2021.eacl-main.29,P11-1036,0,0.092466,"Missing"
2021.eacl-main.29,E17-1046,0,0.0563776,"Missing"
2021.eacl-main.29,J18-4003,0,0.321244,"Missing"
2021.humeval-1.6,2020.acl-main.18,0,0.0223251,"notation from one of the evaluators. When answering question 3 (How many facts in G are correct?) they refer to the clinical report as a ground truth. Based on this set of questions, we gather the following raw counts: R: facts in the reference description G: facts in the generated description R&G: facts in common C: correct facts in the generated description We use these raw counts to compute four derived metrics: • Precision, calculated as R&G G • Recall, calculated as R&G R P recision·Recall P recision+Recall C G • F-Score, calculated as 2 · • Accuracy, calculated as For Coherence, we take Chen et al. (2020) and Juraska et al. (2019) definition: “whether the generated text is grammatically correct and fluent, re58 Bart-Med Pegasus-CNN Metric Precision Recall F-Score Accuracy Coherence Precision Recall F-Score Accuracy Coherence Precision Recall F-Score Accuracy Coherence Precision Recall F-score Accuracy Coherence Eval 1 0.42 0.64 0.49 1.0 0.95 0.58 0.62 0.59 1.0 1.0 0.29 0.43 0.34 0.97 1.0 0.65 1.0 0.77 1.0 1.0 Eval 2 0.43 0.60 0.45 1.0 0.95 0.48 0.61 0.52 1.0 0.95 0.36 0.50 0.40 0.97 1.0 0.58 0.96 0.70 1.0 0.75 Eval 3 0.46 0.73 0.51 1.0 0.90 0.48 0.60 0.51 1.0 1.0 0.31 0.50 0.36 0.98 0.95 0.55"
2021.humeval-1.6,2020.inlg-1.19,0,0.0994133,"Missing"
2021.humeval-1.6,W19-8623,0,0.0127177,"e evaluators. When answering question 3 (How many facts in G are correct?) they refer to the clinical report as a ground truth. Based on this set of questions, we gather the following raw counts: R: facts in the reference description G: facts in the generated description R&G: facts in common C: correct facts in the generated description We use these raw counts to compute four derived metrics: • Precision, calculated as R&G G • Recall, calculated as R&G R P recision·Recall P recision+Recall C G • F-Score, calculated as 2 · • Accuracy, calculated as For Coherence, we take Chen et al. (2020) and Juraska et al. (2019) definition: “whether the generated text is grammatically correct and fluent, re58 Bart-Med Pegasus-CNN Metric Precision Recall F-Score Accuracy Coherence Precision Recall F-Score Accuracy Coherence Precision Recall F-Score Accuracy Coherence Precision Recall F-score Accuracy Coherence Eval 1 0.42 0.64 0.49 1.0 0.95 0.58 0.62 0.59 1.0 1.0 0.29 0.43 0.34 0.97 1.0 0.65 1.0 0.77 1.0 1.0 Eval 2 0.43 0.60 0.45 1.0 0.95 0.48 0.61 0.52 1.0 0.95 0.36 0.50 0.40 0.97 1.0 0.58 0.96 0.70 1.0 0.75 Eval 3 0.46 0.73 0.51 1.0 0.90 0.48 0.60 0.51 1.0 1.0 0.31 0.50 0.36 0.98 0.95 0.55 0.97 0.68 1.0 0.95 Avg 0.4"
2021.humeval-1.6,2020.emnlp-main.750,0,0.0164327,"score. We propose a method for evaluating the quality of generated text by asking evaluators to count facts, and computing precision, recall, fscore, and accuracy from the raw counts. We believe this approach leads to a more objective and easier to reproduce evaluation. We apply this to the task of medical report summarisation, where measuring objective quality and accuracy is of paramount importance. 1 Introduction Natural Language Generation in the medical domain is notoriously hard because of the sensitivity of the content and the potential harm of hallucinations and inaccurate statements (Kryscinski et al., 2020; Falke et al., 2019). This informs the human evaluation of NLG systems, selecting accuracy and overall quality of the generated text as the most valuable aspects to be evaluated. In this paper we carry out a human evaluation of the quality of medical summaries of Clinical Reports generated by state of the art (SOTA) text summarisation models. Our contributions are: (i) a re-purposed parallel dataset of medical reports and summary descriptions for training and evaluating, (ii) an approach for a more objective human evaluation using counts, and (iii) a human evaluation conducted on this dataset"
2021.humeval-1.6,2020.acl-main.703,0,0.0466722,"Missing"
2021.humeval-1.6,2020.inlg-1.22,1,0.71846,"hers define tasks in an XML language and specify the number of annotators. It then generates each individual task and collates the results. The task involves (i) reading the clinical report, (ii) reading the reference description (supplied by the dataset, see Figure 1), (iii) then evaluating 4 generated descriptions by answering 5 questions (for a total of 40 generated descriptions). We ask the evaluators to count the “medical facts” in each generated description and to compare them against those in the reference. Initially, we considered listing the types of facts to be extracted, as done by Thomson and Reiter (2020), but the sheer diversity in the structure and content across the specialties Figure 1: An MTSamples clinical report of specialty ‘Diets and Nutritions’. Note the reference Description at the bottom. Given the brevity of some descriptions, we discard reports with descriptions shorter than 12 words and consider a dataset of 3242 reports. By examining the dataset, we note that descriptions are mostly extractive in nature, meaning they are phrases or entire sentences taken from the report. To quantify this we compute n-gram overlap with Rouge-1 (unigram) and Rouge-L (longest common n-gram) (Lin,"
2021.humeval-1.6,D18-1088,0,0.0235697,"length of each report varies greatly according to the specialty, with an average of 589 words for the body of the report, and 21 words for the description. Figure 1 shows an example of MTSamples reports, inclusive of description. quately represented and then aggregate the data. The dataset, models, and evaluation results can be found on Github3 . 4 Experimental Setup For our experiment, we consider one baseline and three SOTA automatic summarisation models (extractive, abstractive, and fine-tuned on our training set respectively). More specifically: • Lead–3 — this is our baseline. Following Zhang et al. (2018), this model selects the first three sentences of the clinical report as the description; • Bert–Ext — the unsupervised extractive model by Miller (2019) 4 ; • Pegasus–CNN — an abstractive model by Zhang et al. (2019) trained on the CNN/Daily mail dataset and used as is; • Bart–Med — an abstractive model by Lewis et al. (2020), which we fine-tune on our MTSamples training set. We generate descriptions with these 4 models using the entire clinical report text as input. 5 Human Evaluation Protocol We select 10 clinical reports and summary descriptions from our MTSamples test set. Our subjects ar"
2021.humeval-1.7,W05-1615,1,0.620564,"work on how to properly evaluate such a system so that it may be used in the clinical setting. Intrinsic evaluation metrics through Likert scales or ranking methods may help select the best model, but they ∗ 2 Related Work Post-editing has a long history in Machine Translation (MT) (Chander, 1994; Carl et al., 2015; Graham et al., 2017; Koponen, 2016; De Sousa et al., 2011), with a number of production systems and tools using a semi-automatic approach to fix errors and check the output of the system before it is shown to the users (Dowling et al., 2016; Aziz and Specia, 2012). Outside of MT, Sripada et al. (2005) carry out a study on post-editing an NLG system for generating weather forecast from data. As an evaluation metric, Allman et al. (2012) define Productivity as “the quantity of text an experienced translator could translate in a given period of time [compared] with the quantity of text genEqual contribution 62 Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval), pages 62–68 Online, April 19, 2021. ©2021 Association for Computational Linguistics erated by [the system] that the same person could edit in the given time.” To the best of our knowledge, post-editing is not wide"
2021.humeval-1.7,R11-1014,0,0.0602303,"Missing"
2021.humeval-1.7,E17-2057,0,0.0606263,"Missing"
2021.humeval-1.7,W19-1918,0,0.0974235,"Missing"
2021.inlg-1.12,2020.inlg-1.23,0,0.0452387,"Missing"
2021.inlg-1.12,2020.nl4xai-1.12,0,0.0346515,"se explanations address actual user expectations. 2 Related Work In 1990-2000, explanations derived from knowledge bases were enhanced by addressing aspects of users’ reasoning. Specifically, Zukerman and McConachy (1993) and Horacek (1997) considered potential inferences from explanations, omitting easily inferable information and addressing erroneous inferences; Korb et al. (1997) took into account reasoning fallacies when explaining the reasoning of Bayesian Networks; and Stone (2000) generated instructions from which users could draw appropriate inferences about actions to take. Recently, Krause and Vossen (2020) identified additional triggers that should be addressed in explanations. Current research on explanation generation focuses on explaining the predictions made by ML models – a sub-field called Explainable AI (XAI). In particular, neural networks have received a lot of attention owing to their superior performance on one hand, and their opaqueness on the other hand. A common first step in explaining the predictions 1 The participants in our study were told that they have an AI, but they were not informed about the specifics of the ML model. Other explanatory objectives include enhancing trust"
2021.inlg-1.12,W19-8402,1,0.924562,"be addressed in explanations. Current research on explanation generation focuses on explaining the predictions made by ML models – a sub-field called Explainable AI (XAI). In particular, neural networks have received a lot of attention owing to their superior performance on one hand, and their opaqueness on the other hand. A common first step in explaining the predictions 1 The participants in our study were told that they have an AI, but they were not informed about the specifics of the ML model. Other explanatory objectives include enhancing trust in the system, and helping debug a system (Reiter, 2019). of neural networks is to build a local surrogate explainer model that uses a transparent model to approximate the neighbourhood of an instance of interest. Linear regression (Ribeiro et al., 2016; ˇ Strumbelj and Kononenko, 2014; Lundberg and Lee, 2017), decision rules (Ribeiro et al., 2018) and DTs (van der Waa et al., 2018; Guidotti et al., 2019; Sokol and Flach, 2020a) have been employed for this purpose. A DT’s prediction is generally explained by tracing the path from the root to a predicted outcome (Guidotti et al., 2019; Stepin et al., 2020). Recently, researchers have generated class"
2021.inlg-1.12,N16-3020,0,0.0431343,"ural networks have received a lot of attention owing to their superior performance on one hand, and their opaqueness on the other hand. A common first step in explaining the predictions 1 The participants in our study were told that they have an AI, but they were not informed about the specifics of the ML model. Other explanatory objectives include enhancing trust in the system, and helping debug a system (Reiter, 2019). of neural networks is to build a local surrogate explainer model that uses a transparent model to approximate the neighbourhood of an instance of interest. Linear regression (Ribeiro et al., 2016; ˇ Strumbelj and Kononenko, 2014; Lundberg and Lee, 2017), decision rules (Ribeiro et al., 2018) and DTs (van der Waa et al., 2018; Guidotti et al., 2019; Sokol and Flach, 2020a) have been employed for this purpose. A DT’s prediction is generally explained by tracing the path from the root to a predicted outcome (Guidotti et al., 2019; Stepin et al., 2020). Recently, researchers have generated class-contrastive counterfactual explanations to enhance the explanations of DT predictions. Stepin et al. (2020) generated explanations that have a factual and a counterfactual component; the former is"
2021.inlg-1.23,D19-1310,0,0.0183241,"tems we used explored different ways of modifying the neural architecture. The system of Wiseman et al. (2017) defined the Rotowire task and provided initial benchmarks for machine translation systems using copy attention, it is included for this reason. Puduppully et al. (2019a) learned a document plan which was then used to generate text, whilst Rebuffel et al. (2020) used a hierarchical encoder to group attributes (such as statistics) by their respective entities (players/teams). Other systems in this domain which could be used for evaluation include Puduppully et al. (2019b), Wang (2019), Gong et al. (2019), and Iso et al. (2019). Our aim, however, is to assess how well results produced by the participant’s evaluation techniques correlate with the gold-standard fact-checking. Hence we are looking for a set of systems which generate texts that contain a significant number of accuracy errors, not complete coverage of all systems that generate texts from basketball box score data. 3.2 preferred one was not clear to our annotators. For example, in our test we encountered a sentence that was marked up by annotators as shown in Figure 2: Multiple Correct Annotations Sometimes there are multiple correc"
2021.inlg-1.23,P19-1195,0,0.290421,"– Not checkable: A statement which can not be checked, either because the information is not available or because it is too time-consuming to check. – Other: Any other type of mistake. An example is shown in Figure 1. Note that this example combines fragments from texts produced by several different systems, along with some manual adjustments, in order to illustrate different types of mistakes in a simple way. 3 Data We manually annotated, using the procedure of Thomson and Reiter (2020), 90 texts produced by three neural NLG systems that use basketball box score data: Wiseman et al. (2017), Puduppully et al. (2019a), and Rebuffel et al. (2020). In total, 30 texts were annotated from each system. Of these, 60 texts (20 from each system) were given to shared task participants as training data, and 30 texts (10 from each system) were reserved for a separate test 241 List of errors: • 2: incorrect number, should be 0. • Monday: incorrect named entity, should be Wednesday. • Talking Stick Resort Arena: incorrect named entity, should be US Airways Center. • strong: incorrect word, the Grizzlies did not do well in the first half. • out-scored: incorrect word, the Suns had a higher score in first half. • 59: i"
2021.inlg-1.23,2020.inlg-1.22,1,0.86382,"time-consuming human evaluations or on automatic metrics. However, these techniques should only be used if they have good agreement and correlation with careful high-quality human fact-checking by multiple annotators. In this shared task, participating teams submitted techniques (both human and automatic) for evaluating the factual accuracy of summaries of basketball games produced from box score (and other game data) by three neural NLG systems. These techniques were evaluated by computing precision and recall (of identified factual errors) against a goldstandard human annotation produced by Thomson and Reiter (2020)’s protocol. Some of the systems did well overall, but it was also clear that some types of factual errors are difficult to detect. We hope that our shared task encourages researchers from many fields to work on the problem of identifying factual errors in generated texts; progress in this area would be very helpful for NLG. Full details of the shared task requirements, as well as both the training and test corpus can be found at https://github.com/ehudreiter/accuracySharedTask. The Shared Task on Evaluating Accuracy focused on techniques (both manual and automatic) for evaluating the factual"
2021.inlg-1.23,W19-8639,0,0.0191356,"ee neural systems we used explored different ways of modifying the neural architecture. The system of Wiseman et al. (2017) defined the Rotowire task and provided initial benchmarks for machine translation systems using copy attention, it is included for this reason. Puduppully et al. (2019a) learned a document plan which was then used to generate text, whilst Rebuffel et al. (2020) used a hierarchical encoder to group attributes (such as statistics) by their respective entities (players/teams). Other systems in this domain which could be used for evaluation include Puduppully et al. (2019b), Wang (2019), Gong et al. (2019), and Iso et al. (2019). Our aim, however, is to assess how well results produced by the participant’s evaluation techniques correlate with the gold-standard fact-checking. Hence we are looking for a set of systems which generate texts that contain a significant number of accuracy errors, not complete coverage of all systems that generate texts from basketball box score data. 3.2 preferred one was not clear to our annotators. For example, in our test we encountered a sentence that was marked up by annotators as shown in Figure 2: Multiple Correct Annotations Sometimes there"
2021.inlg-1.24,2020.lrec-1.686,0,0.037437,"Missing"
2021.inlg-1.24,W18-6539,0,0.0480415,"Missing"
2021.inlg-1.24,P17-2014,0,0.0518538,"Missing"
2021.inlg-1.24,D17-1238,0,0.0725785,"Missing"
2021.inlg-1.24,2020.coling-main.444,0,0.068114,"Missing"
2021.inlg-1.24,2021.inlg-1.31,1,0.836387,"Missing"
2021.inlg-1.24,W18-6532,0,0.0478464,"Missing"
2021.inlg-1.24,J18-3002,1,0.858118,"Missing"
2021.inlg-1.24,2021.inlg-1.32,0,0.394434,"eland, and one was a collaboration between groups in Spain, Brazil and Ireland. Two of the teams participated in Track A 3 All information and resources relating to ReproGen are available at https://reprogen.github.io/ 4 https://forms.gle/J5ranvXqmfjPDbxLA 5 6 250 https://forms.gle/MgWiKVu7i5UHeMNQ9 https://reprogen.github.io/submissions/ Track A B Team Technical University of Darmstadt (TUDA) UPF Barcelona, UF Minas Gerais, ADAPT Dublin Trivago GmbH, D¨usseldorf ADAPT Dublin Original paper Qader et al. (2018) van der Lee et al. (2017) Mahamood et al. (2007) Popovi´c (2020) Reproduction paper Richter et al. (2021) Mille et al. (2021) Mahamood (2021) Popovi´c and Belz (2021) Table 1: Overview of ReproGen submissions (tracks, teams, original papers and reproduction reports). (Mille et al., 2021; Richter et al., 2021), the other two in Track B (Mahamood, 2021; Popovi´c and Belz, 2021). Three of the four teams are affiliated with universities, one with a commercial company. Each of the submissions reported a reproduction study for a different paper. Two of the evaluated systems produced outputs in English, one in Croatian, and one in Dutch. While Mahamood (2021) and Mille et al. (2021) reproduced human eva"
2021.inlg-1.24,W19-8610,0,0.0179288,"(2017): PASS: A Dutch datato-text system for soccer, targeted towards specific audiences: 1 evaluation study; Dutch; 20 evaluators; 3 quality criteria; reproduction target: primary scores. 2. Duˇsek et al. (2018): Findings of the E2E NLG Challenge: 1 evaluation study; English; MTurk; 2 quality criteria; reproduction target: primary scores. 3. Qader et al. (2018): Generation of Company descriptions using concept-to-text and text-totext deep models: dataset collection and systems evaluation: 1 evaluation study; English; 19 evaluators; 4 quality criteria; reproduction target: primary scores. 4. Santhanam and Shaikh (2019): Towards Best Experiment Design for Evaluating Dialogue System Output: 3 evaluation studies differing in experimental design; English; 40 evaluators; 2 quality criteria; reproduction target: correlation scores between 3 studies. Organisation of Shared Task ReproGen’213 had two tracks, one a shared task in which teams try to reproduce the same prior human evaluation results, the other an ‘unshared task’ in which teams attempt to reproduce their own prior human evaluation results: A Main Reproducibility Track: For a shared set of selected human evaluation studies, participants repeat one or mor"
A92-1009,J86-3001,0,0.0576403,"Missing"
A92-1009,H89-1022,0,0.258816,"Missing"
A92-1009,P89-1025,0,0.354205,"Missing"
A92-1009,C92-1038,1,0.858264,"Missing"
A92-1009,P92-1034,1,0.877652,"Missing"
A92-1009,P83-1023,0,0.0472859,"Missing"
A97-1037,W96-0503,1,0.695751,"ndersen Consulting, a large systems consulting company, and the Software Engineering Laboratory at the Electronic Systems Division of Raytheon, a large Government contractor. Our design is based on initial interviews with software engineers working on a project at Raytheon, and was modified in response to feedback during iterative prototyping when these software engineers were using our system. • MoDEx output integrates tables, text generated automatically, and text entered freely by the user. Automatically generated text includes paragraphs describing the relations between classes, and paral(Lavoie et al., 1996) focuses on an earlier version of MoDEx which did not yet include customization. graphs describing examples. The human-anthored text can capture information not deducible from the model (such as high-level descriptions of purpose associated with the classes). • MoDEx lets the user customize the text plans at run-time, so that the text can reflect individual user or organizational preferences regarding the content and/or layout of the output. • MoDEx uses an interactive hypertext interface (based on standard HTML-based WWW technology) to allow users to browse through the model. • Input to MoDEx"
A97-1037,A97-1039,1,0.740384,"Missing"
C92-1038,P83-1007,0,0.0115018,"COLING-92, NAtCrES,23-28 AO&apos;t~q""1992 232 domain-independent: the core algorithm should work in any domain, once an appropriate knowledge base and user model has been set up. A version of the algorithm has been implemented within the IDAS natural-language generation system [RML92], and it is performing satisfactorily. The algorithm presented in this paper only generates definite noun phrases that identify an object that is in the current focus of attention. Algorithms and models that can be used to generate pronominal and one-anaphoric referring expressions have been presented elsewhere, e.g., [Sid81,GJW83,Da189]. We have recently begun to look at the problem of generating referring expressions for objects that are not in the current focus of attention; this is discussed in the section on Future Work. Background Distinguishing Descriptions The term &apos;referring expression&apos; has been used by different people to mean different things. In this paper, we define a referring expression in intentional terms: a noun phrase is considered to be a referring expression if and only if its only communicative purpose is to identify an object to the hearer, in Kronfeld&apos;s terminology [Kro86], we only use the modal aspect"
C92-1038,H89-1022,0,0.0349963,"Missing"
C92-1038,P86-1029,0,0.0579534,"elsewhere, e.g., [Sid81,GJW83,Da189]. We have recently begun to look at the problem of generating referring expressions for objects that are not in the current focus of attention; this is discussed in the section on Future Work. Background Distinguishing Descriptions The term &apos;referring expression&apos; has been used by different people to mean different things. In this paper, we define a referring expression in intentional terms: a noun phrase is considered to be a referring expression if and only if its only communicative purpose is to identify an object to the hearer, in Kronfeld&apos;s terminology [Kro86], we only use the modal aspect of Donefian&apos;s distinction between attributive and referential descriptions [Don66]; we consider a noun phrase to be referential if it is intended to identify the object it describes to the hearer, and attributive if it is intended to communicate information about that object to the hearer. This usage is similar to that adopted by Reiter [Rei90b] and Dale and Haddock [DH91], but differs from the terminology used by Appelt [App85], who allowed &apos;referring expressions&apos; to satisfy any communicative goal that could be stated in the underlying logical framework. We here"
C92-1038,P90-1013,1,0.900512,"Missing"
C92-1038,A92-1009,1,0.88093,"sensitive to h u m a n preferences: it attempts to use easily perceivable attributes and basic-level [Ros78] attribute values; and • Supported by SERC grant GR/F/36750. E-mail address is E.Reiter@ed. ac.uk. tAiso of the Centre for Cognitive Science at the University of Edinburgh. E-mail address is R. DaleQed. ac .uk. ACRESDECOLING-92, NAtCrES,23-28 AO&apos;t~q""1992 232 domain-independent: the core algorithm should work in any domain, once an appropriate knowledge base and user model has been set up. A version of the algorithm has been implemented within the IDAS natural-language generation system [RML92], and it is performing satisfactorily. The algorithm presented in this paper only generates definite noun phrases that identify an object that is in the current focus of attention. Algorithms and models that can be used to generate pronominal and one-anaphoric referring expressions have been presented elsewhere, e.g., [Sid81,GJW83,Da189]. We have recently begun to look at the problem of generating referring expressions for objects that are not in the current focus of attention; this is discussed in the section on Future Work. Background Distinguishing Descriptions The term &apos;referring expressio"
C92-1038,P89-1009,1,\N,Missing
C92-1038,J86-3001,0,\N,Missing
E03-1021,P83-1022,0,0.365764,"Missing"
E06-1040,W00-1401,0,0.709535,"se than the human evaluations that have traditionally been used to evaluate NLG systems. However, the use of such corpus-based evaluation metrics is only sensible if they are known to be correlated with the results of human-based evaluations. While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments (Papineni et al., 2002; Doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgments; correlation studies have been made of individual components (Bangalore et al., 2000), but not of systems. In this paper we present an empirical study of how well various corpus-based metrics agree with human judgments, when evaluating several NLG systems that generate sentences which describe changes in the wind (for weather forecasts). These systems do not perform content determination (they are limited to microplanning and realisation), so our study does not address corpus-based evaluation of content determination. 2 Background 2.1 Evaluation of NLG systems NLG systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998). NLG evaluations have tend"
E06-1040,W05-1601,1,0.923631,"he success of the BLEU evaluation metric (Papineni et al., 2002) in Machine Translation (MT), which has transformed the MT field by allowing researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets. BLEU and related metrics work by comparing the output of an Ehud Reiter Dept of Computing Science University of Aberdeen UK ereiter@csd.abdn.ac.uk MT system to a set of reference (‘gold standard’) translations, and in principle this kind of evaluation could be done with NLG systems as well. Indeed NLG researchers are already starting to use BLEU (Habash, 2004; Belz, 2005) in their evaluations, as this is much cheaper and easier to organise than the human evaluations that have traditionally been used to evaluate NLG systems. However, the use of such corpus-based evaluation metrics is only sensible if they are known to be correlated with the results of human-based evaluations. While studies have shown that ratings of MT systems by BLEU and similar metrics correlate well with human judgments (Papineni et al., 2002; Doddington, 2002), we are not aware of any studies that have shown that corpus-based evaluation metrics of NLG systems are correlated with human judgm"
E06-1040,C96-1043,0,0.0658248,"NLG systems NLG systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998). NLG evaluations have tended to be of the intrinsic type (Sparck Jones and Galliers, 1996), involving subjects reading and rating texts; usually subjects 313 are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is pote"
E06-1040,E06-1045,0,0.0523936,"8 abilities, pCRU-roulette will tend to use different words and phrases in different texts, whereas the other statistical generators will stick to those with the highest frequency. This behaviour is penalised by the automatic evaluation metrics, but the human evaluators do not seem to mind it. One of the classic rules of writing is to vary lexical and syntactic choices, in order to keep text interesting. However, this behaviour (variation for variation’s sake) will always reduce a system’s score under corpus-similarity metrics, even if it enhances text quality from the perspective of readers. Foster and Oberlander (2006), in their study of facial gestures, have also noted that humans do not mind and indeed in some cases prefer variation, whereas corpus-based evaluations give higher ratings to systems which follow corpus frequency. Using more reference texts does counteract this tendency, but only up to a point: no matter how many reference texts are used, there will still be one, or a small number of, most frequent variants, and using anything else will still worsen corpussimilarity scores. Canvassing expert opinion of text quality and averaging the results is also in a sense frequencybased, as results reflec"
E06-1040,J97-1004,0,0.096838,"systems have traditionally been evaluated using human subjects (Mellish and Dale, 1998). NLG evaluations have tended to be of the intrinsic type (Sparck Jones and Galliers, 1996), involving subjects reading and rating texts; usually subjects 313 are shown both NLG and human-written texts, and the NLG system is evaluated by comparing the ratings of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quic"
E06-1040,N03-1020,0,0.0613763,"l reference translations (four appears to be standard in MT). Properly calculated BLEU scores have been shown to correlate reliably with human judgments (Papineni et al., 2002). The NIST MT evaluation metric (Doddington, 2002) is an adaptation of BLEU, but where BLEU gives equal weight to all n-grams, NIST gives more importance to less frequent (hence more informative) n-grams. BLEU’s ability to detect subtle but important differences in translation quality has been questioned, some research showing NIST to be more sensitive (Doddington, 2002; Riezler and Maxwell III, 2005). The ROUGE metric (Lin and Hovy, 2003) was conceived as document summarisation’s answer to BLEU , but it does not appear to have met with the same degree of enthusiasm. There are several different ROUGE metrics. The simplest is ROUGE-N, which computes the highest proportion in any reference summary of n-grams that are matched by the system-generated summary. A procedure is applied that averages the score across leave-oneout subsets of the set of reference texts. ROUGEN is an almost straightforward n-gram recall metric between two texts, and has several counterintuitive properties, including that even a text composed entirely of se"
E06-1040,P02-1040,0,0.107787,"Missing"
E06-1040,W02-2113,1,0.768323,"in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable. Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts. Similar evaluations have been used e.g. by Bangalore et al. (2000) and Marciniak and Strube (2004). Such corpus-based evaluations have sometimes been criticised in the NLG community, for example by Reiter and Sripada (2002). Grounds for criticism include the fact that regenerating a parsed text is not a realistic NLG task; that texts can be very different from a corpus text but still effectively meet the system’s communicative goal; and that corpus texts are often not of high enough quality to form a realistic test. 2.2 Automatic evaluation of generated texts in MT and Summarisation The MT and document summarisation communities have developed evaluation metrics based on comparing output texts to a corpus of human texts, and have shown that some of these metrics are highly correlated with human judgments. The BLE"
E06-1040,W05-0908,0,0.0333862,"Missing"
E06-1040,W05-1616,1,0.746935,"gs of its texts and human texts. In some cases, subjects are shown texts generated by several NLG systems, including a baseline system which serves as another point of comparison. This methodology was first used in NLG in the mid-1990s by Coch (1996) and Lester and Porter (1997), and continues to be popular today. Other, extrinsic, types of human evaluations of NLG systems include measuring the impact of different generated texts on task performance (Young, 1999), measuring how much experts postedit generated texts (Sripada et al., 2005), and measuring how quickly people read generated texts (Williams and Reiter, 2005). In recent years there has been growing interest in evaluating NLG texts by comparing them to a corpus of human-written texts. As in other areas of NLP, the advantages of automatic corpusbased evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and also that it is repeatable. Corpus-based evaluation was first used in NLG by Langkilde (1998), who parsed texts from a corpus, fed the output of her parser to her NLG system, and then compared the generated texts to the original corpus texts. Similar evaluations have been used e.g. by Bangalore et al. (2000)"
E06-1040,W02-2103,0,\N,Missing
E06-2020,W03-2305,0,0.0635792,"erg et al., 1994) and MultiMeteo (Coch, 1998). 2 forecasts were written. An example of a pollen forecast text is shown in Figure 1, its corresponding data is shown in table 1. A pollen forecast in the map form is shown in Figure 2. Figure 1: Human written pollen forecast text for the pollen data shown in table 1 Figure 2: Pollen forecast map for the pollen data shown in table 1 Analysis of a parallel corpus (texts and their underlying data) can be performed in two stages: Knowledge Acquisition • In the first stage, traditional corpus analysis procedure outlined in (Reiter and Dale, 2000) and (Geldof, 2003) can be used to analyse the pollen forecast texts (the textual component of the parallel corpus). This stage will identify the different message types and uncover the sub language of the pollen forecasts. Our knowledge acquisition activities consisted of corpus studies and discussions with experts. We have collected a parallel corpus (69 data-text pairs) of pollen concentration data and their corresponding human written pollen reports which our industrial collaborator has provided for a local commercial television station. The forecasts were written by two expert meteorologists, one of whom pr"
J00-2005,E87-1001,0,0.0389984,"minalization decisions are typically made earlier than word-ordering decisions; for example in the three-stage pipelined architecture presented by Reiter and Dale (2000), pronominalization decisions are made in the second stage (microplanning), but word ordering is chosen during the third stage (realization). This means that the microplanner will not be able to make optimal pronominalization decisions in cases where le or la are unambiguous, but I&apos; is not, since it does not know word order and hence whether the pronoun will be abbreviated. Many other such cases are described in Danlos&apos;s book (Danlos 1987). The common theme behind many of these examples is that pipelines have difficulties satisfying linguistic constraints (such as unambiguous reference) or performing linguistic optimizations (such as using pronouns instead of longer referring expressions whenever possible) in cases where the constraints or optimizations depend on decisions made in multiple modules. This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module. Despite these arguments, most applied NLG systems use a pipelined archit"
J00-2005,C88-1029,0,0.0253789,"ed into modules. This leads to the question of how modules should interact. In particular, is it acceptable to arrange modules in a simple pipeline, where a later module cannot affect an earlier module? Or is it necessary to allow revision or feedback, where a later module can request that an earlier module modify its results? If a pipeline is used, should modules pass a single solution down the line, or should they pass multiple solutions and let subsequent modules choose between these? Many authors have argued that pipelines cannot optimally handle certain linguistic phenomena. For example, Danlos and Namer (1988) point out that in French, whether a pronoun unambiguously refers to an entity depends on word ordering. This is because the pronouns le or la (which convey gender information) are abbreviated to 1&apos; (which does not contain gender information) when the word following the pronoun starts with a vowel. But in a pipelined NLG system, pronominalization decisions are typically made earlier than word-ordering decisions; for example in the three-stage pipelined architecture presented by Reiter and Dale (2000), pronominalization decisions are made in the second stage (microplanning), but word ordering i"
J00-2005,J98-3004,0,0.0439903,"end on decisions made in multiple modules. This is largely due to the fact that pipelined systems cannot perform general search over a decision space that includes decisions made in more than one module. Despite these arguments, most applied NLG systems use a pipelined architecture; indeed, a pipeline was used in every one of the systems surveyed by Reiter (1994) and Paiva (1998). This may be because pipelines have many engineering advantages, and in practice the sort of problems pointed out by Danlos and other pipeline critics do not seem to be a major problem in current applied NLG systems (Mittal et al. 1998). 3. STOP The STOP system (Reiter, Robertson, and Osman 1999) generates personalized smokingcessation leaflets, based on responses to a questionnaire about smoking likes and dislikes, previous attempts to quit, and so forth. The output of the system is a four-page leaflet; each page is size A5. An example of the two &quot;inside&quot; pages of a leaflet produced by STOP is shown in Figure 1. A STOP leaflet also contains a front page that is only partially generated (the rest is logos and fixed text) and a back page that is selected from a collection of 16 possible back pages, but is not otherwise person"
J00-2005,W94-0319,1,0.836885,"Missing"
J02-4007,P01-1038,0,0.0190241,"Missing"
J02-4007,P93-1031,0,0.0692475,"Missing"
J02-4007,W00-1429,1,0.818461,"Missing"
J02-4007,W01-0802,1,\N,Missing
J02-4007,W02-2113,1,\N,Missing
J07-2013,J07-3008,0,\N,Missing
J09-4008,W05-0909,0,0.0618254,"Missing"
J09-4008,W00-1401,0,0.0147265,"Missing"
J09-4008,2007.mtsummit-ucnlg.14,1,0.593518,"ologist” methodology described earlier (van der Meulen et al. 2009). SKILLSUM (Williams and Reiter 2008), which generates feedback reports from literacy assessments, was evaluated on the basis of educational effectiveness; we gave 200 assessment takers either SKILLSUM texts or control texts, and measured whether they increased the accuracy of self-assessments of their literacy skills. We also evaluated several referring-expression generation algorithms by conducting experiments in which participants were presented with generated referring expressions and asked to identify the target referent (Belz and Gatt 2007; Gatt, Belz, and Kow 2008, 2009); these were carried out in conjunction with shared-task events organized under the Generation Challenges initiative (Generation Challenges is further discussed in Section 2.1.4). Task-based evaluations have traditionally been regarded as the most meaningful kind of evaluation in NLG, especially in contexts where the evaluation needs to convince people in other communities (such as psychologists and doctors). However, they can be expensive and time-consuming. The STOP evaluation cost UK£75,000, and required 20 months to design, carry out, and analyze; the SKILL"
J09-4008,P08-2050,1,0.745331,"ally simulated system “outputs.” Probably the most similar study to our work is that by Stent, Marge, and Singhai (2005), who examined the correlation between human judgments and several automatic metrics when evaluating computer-generated paraphrases; this is further discussed in Section 3.3.3. Very recently some validation studies have been done in the context of the Generation Challenges initiative for shared tasks in NLG, by evaluating systems entered in the shared task using automatic metrics, human ratings, and task-based evaluation, and analyzing correlations between these. For example Belz and Gatt (2008) analyzed correlations between several automatic evaluation metrics and task performance in a referring-expression generation task; they found that there was no signiﬁcant correlation between any of the automatic metrics they looked at (which included specialized metrics for the reference task as well as BLEU and ROUGE) and their task-based measures of effectiveness, such as how long it took human subjects to identify objects from a referring expression, and how many mistakes the subjects made. However the different automatic metrics they examined did tend to correlate with each other, as did"
J09-4008,E06-1040,1,0.801973,"ference for the corpus texts is probably an artefact of the way the reference texts were produced. The forecasters were asked to rewrite the corpus texts, which resulted in considerable similarity between the reference texts and the corpus texts. In calculating correlation ﬁgures (shown in Table 6), we therefore produced two sets of ﬁgures, one for the NLG systems and the corpus texts (I in the table) and one for just the NLG systems (II); set II should be regarded as a post hoc analysis. For set I, none of the metrics signiﬁcantly correlate 5 When we ﬁrst reported results for Experiment 1 in Belz and Reiter (2006), we only had reference texts from two meteorologists, but we have since obtained reference texts from a third meteorologist. This is why the numbers in Tables 5 and 6 differ from the numbers given in Belz and Reiter (2006). 6 The important information in Table 5 is the differences in the scores assigned by the same metric to different systems. Differences in the scores assigned by different metrics to the same system are not meaningful; they are just mathematical artefacts of the formulas used to calculate the metrics. For example, the fact that BLEU-4 gives S UM T IME-Hybrid a higher score t"
J09-4008,P06-1130,0,0.0457868,"Missing"
J09-4008,W08-0309,0,0.0101201,"f NLP Of course, evaluation and experimentation are crucial to all ﬁelds of NLP; here we look at insights from two other NLP subﬁelds which need to evaluate the quality of texts: machine translation and document summarization. 534 Reiter and Belz Validity of Some Metrics for NLG Evaluation 2.2.1 Evaluation in Machine Translation. There is a rich literature in MT evaluation, including a number of specialist workshops on this topic; as in NLG, there is also considerable interest in using shared-task events to provide data about how well different evaluation techniques correlate with each other (Callison-Burch et al. 2008). From an NLG perspective, the most surprising aspect of current MT evaluation is the dominance of BLEU and other automatic corpus-based metrics (Callison-Burch, Osborne, and Koehn 2006). BLEU was ﬁrst proposed as a supplement (the U in BLEU stands for “understudy”) for human evaluation (Papineni et al. 2002), but it is now routinely used as the main technique for evaluating research contributions. It is accepted and indeed the norm for an article on MT in Computational Linguistics to report evaluations that are solely based on automatic corpus-based metrics; this is not the case in NLG, where"
J09-4008,E06-1032,0,0.0484019,"Missing"
J09-4008,W98-1435,0,0.013552,", and then analyzing the correlation between the techniques. One potential weakness of our experiments was that we did not look at correlations with task-effectiveness evaluations. This was because we did not have the resources (money and domain-expert goodwill) to conduct a task-based evaluation. This issue is further discussed in Section 4.2. 536 Reiter and Belz Validity of Some Metrics for NLG Evaluation 3.1 Domain and Systems Our work was done in the domain of computer-generated weather forecasts. This is one of the most popular applications of NLG (Goldberg, Driedger, and Kittredge 1994; Coch 1998; Reiter et al. 2005), and several NLG weather-forecast systems have been ﬁelded and used. Weather forecast generation is probably the closest that NLG comes to a “standard” application domain, and hence seems a good choice for validation studies from this perspective. On the other hand, though, one could also argue that weather-forecast generators are atypical in that the language they generate tends to be very simple, even by the standards of NLG systems: very limited syntax (which differs from conventional English), very small vocabulary, no real text structure above the sentence level, and"
J09-4008,2003.mtsummit-papers.9,0,0.0278439,"t have tried to correlate BLEU-like metrics with the results of task-effectiveness studies. Although a number of studies have analyzed the correlation between BLEU-type metrics and human judgments, most of these have used human judgments from NIST MT evaluations. Human judgments in most of these evaluations were solicited from monolingual subjects who were asked to compare the output of MT systems to a single reference translation, without any context; also in many of these studies the subjects were asked to assess individual sentences or even phrases, not complete texts (Doddington 2002). As Coughlin (2003) and others have pointed out, it is not clear that human judgments solicited in this way would match the judgments of bilingual subjects who were shown complete source and MT texts, and asked to evaluate the quality of the translation in a speciﬁc real-world context. Papineni et al. (2002) in fact found that BLEU scores were more highly correlated with human judgments from monolingual subjects than human judgments from bilingual subjects. In any case, regardless of the effectiveness of BLEU as an MT evaluation metric, another issue is whether an MT evaluation technique can in general be expect"
J09-4008,W06-0707,0,0.016538,"summarization track is the pyramid technique (Nenkova and Passonneau 2004), which is a structured human-based evaluation, based on asking human judges to identify ‘summarization content units’ (SCU) in model and system-generated summaries, and measuring how many SCUs from the model summaries occur in the system summary. This is an interesting technique for evaluating content, and might be worth investigating for evaluating content determination in NLG systems. In terms of validation, a number of studies have claimed that ROUGE correlates with human ratings, for example Lin and Hovy (2003) and Dang (2006). Dorr et al. (2005) checked if ROUGE scores correlated with task effectiveness; they did not ﬁnd a strong correlation. 2.3 Summary In summary, evaluation of NLG texts in the past has primarily been done using human subjects, either by measuring the impact of texts on task performance, or by asking subjects to rate texts. However, a growing number of NLG researchers are using automatic metrics to evaluate their systems, perhaps inspired by the popularity of automatic metrics in other areas of NLP which involve evaluating output texts, most notably machine translation and document summarization"
J09-4008,W02-2116,0,0.0868329,"Missing"
J09-4008,W05-0901,0,0.212341,"track is the pyramid technique (Nenkova and Passonneau 2004), which is a structured human-based evaluation, based on asking human judges to identify ‘summarization content units’ (SCU) in model and system-generated summaries, and measuring how many SCUs from the model summaries occur in the system summary. This is an interesting technique for evaluating content, and might be worth investigating for evaluating content determination in NLG systems. In terms of validation, a number of studies have claimed that ROUGE correlates with human ratings, for example Lin and Hovy (2003) and Dang (2006). Dorr et al. (2005) checked if ROUGE scores correlated with task effectiveness; they did not ﬁnd a strong correlation. 2.3 Summary In summary, evaluation of NLG texts in the past has primarily been done using human subjects, either by measuring the impact of texts on task performance, or by asking subjects to rate texts. However, a growing number of NLG researchers are using automatic metrics to evaluate their systems, perhaps inspired by the popularity of automatic metrics in other areas of NLP which involve evaluating output texts, most notably machine translation and document summarization. This use of metric"
J09-4008,W08-1131,1,0.851702,"Missing"
J09-4008,W08-1120,0,0.0338708,"keholders, in contrast, would be satisﬁed with a single evaluation at the end of the project. The software house would like to know if BabyTalk would be commercially proﬁtable. This partially depends on medical effectiveness (see previous point), which determines the demand for the system. But it also depends on how expensive it is to develop and support BabyTalk; from this perspective the company is especially interested in evaluations of the cost of adapting/porting BabyTalk to different hospitals in the NICU domain in the short term, and to different medical domains in the longer term. See Harris (2008) for a commercial perspective on medical NLG systems. All of these stakeholders are interested in evaluations which assess the quality and effectiveness of generated texts; such evaluations are the focus of our article. The software 530 Reiter and Belz Validity of Some Metrics for NLG Evaluation house and the computer scientists are also interested in engineering-cost evaluations; although this is a very important topic, we will not discuss it here: a separate article would be needed to do justice to this topic. 2.1 Evaluation in NLG The quality of texts generated by NLG systems has been evalu"
J09-4008,W02-2103,0,0.0398413,"has allowed MT researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets. BLEU and related metrics work by comparing the output of an MT system to a set of reference translations (human translations of the source text), and in principle this kind of evaluation could be done with NLG systems as well. As in other areas of NLP, the advantages of automatic corpus-based evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and that it is repeatable. Indeed, NLG researchers have used BLEU in their evaluations for some time (Langkilde 2002; Habash 2004). The use of such automatic evaluation metrics is, however, only sensible if they are known to be correlated with the results of reliable human-based evaluations. Although a number of previous studies have analyzed correlations between human judgments ∗ Department of Computing Science, University of Aberdeen, UK. E-mail: e.reiter@abdn.ac.uk. ∗∗ Natural Language Technology Group, University of Brighton, UK. E-mail: A.S.Belz@brighton.ac.uk. Submission received: 23 March 2007; revised submission received: 6 October 2008; accepted for publication: 29 December 2008. © 2009 Association"
J09-4008,P98-1116,0,0.0453349,"02; Lin and Hovy 2003), much less is known about how well automatic metrics correlate with human judgments in NLG. In this article we present two empirical studies of how well BLEU and various other corpus-based metrics agree with human judgments, when evaluating the outputs of several NLG systems that generate texts which describe changes in the wind (for weather forecasts). We also discuss several caveats that need to be kept in mind when interpreting our study and perhaps other validation studies of automatic metrics as well. 2. Background: Evaluation in NLG and Related Fields As Hirschman (1998), Mellish and Dale (1998), and others have pointed out, evaluations can be used for many purposes, and different evaluations are often needed for different stakeholders. For example, the BabyTalk project at Aberdeen (Portet et al. 2009), which is attempting to create a set of NLG systems which can generate textual summaries of clinical data about babies in a neonatal intensive care unit (NICU), is a collaboration between medical researchers, psychologists, computer scientists, and a commercial software house. Each of these groups has its own evaluation agenda: r r r r The medical researchers w"
J09-4008,J97-1004,0,0.018279,"d time costs, all of these evaluations also depended on goodwill from participants, in most cases busy domain experts who used their own standing in their community to arrange access to subjects and otherwise facilitate the evaluation. Such goodwill in itself is a scarce resource which must be used with care. 2.1.2 Evaluations Based on Human Ratings and Judgments. Another way of evaluating an NLG system is to ask human subjects to rate generated texts on an n-point rating scale; this is an intrinsic form of evaluation (Sp¨arck Jones and Galliers 1995). This methodology was ﬁrst used in NLG by Lester and Porter (1997), who asked eight domain experts to each rate 15 texts on a number of different dimensions: overall quality and coherence, content, organization, writing style, and correctness. Some of the texts were humanwritten and some were computer-generated, but the judges did not know the origin of speciﬁc texts they read. Many more such evaluations have been performed since, often with fewer dimensions. For example, Binsted, Pain, and Ritchie (1997) evaluated a jokegeneration system by asking children to rate the funniness of texts on a 5-point scale; and Walker, Rambow, and Rogati (2002) evaluated the"
J09-4008,N03-1020,0,0.841376,"evious studies have analyzed correlations between human judgments ∗ Department of Computing Science, University of Aberdeen, UK. E-mail: e.reiter@abdn.ac.uk. ∗∗ Natural Language Technology Group, University of Brighton, UK. E-mail: A.S.Belz@brighton.ac.uk. Submission received: 23 March 2007; revised submission received: 6 October 2008; accepted for publication: 29 December 2008. © 2009 Association for Computational Linguistics Computational Linguistics Volume 35, Number 4 and automatic evaluation metrics in machine translation and document summarization (Doddington 2002; Papineni et al. 2002; Lin and Hovy 2003), much less is known about how well automatic metrics correlate with human judgments in NLG. In this article we present two empirical studies of how well BLEU and various other corpus-based metrics agree with human judgments, when evaluating the outputs of several NLG systems that generate texts which describe changes in the wind (for weather forecasts). We also discuss several caveats that need to be kept in mind when interpreting our study and perhaps other validation studies of automatic metrics as well. 2. Background: Evaluation in NLG and Related Fields As Hirschman (1998), Mellish and Da"
J09-4008,N04-1019,0,0.0746361,"luation perspective, an important difference between MT and summarization is that summarization evaluations have placed much more emphasis on content determination. Perhaps in part because of this, the summarization community places more emphasis on human evaluations. Although there are automatic corpus-based metrics for summarization such as ROUGE (Lin and Hovy 2003), they do not seem to dominate summarization evaluation in the same way that BLEU-type metrics dominate MT evaluation. The main summarization evaluation technique in the NIST TAC 2008 summarization track is the pyramid technique (Nenkova and Passonneau 2004), which is a structured human-based evaluation, based on asking human judges to identify ‘summarization content units’ (SCU) in model and system-generated summaries, and measuring how many SCUs from the model summaries occur in the system summary. This is an interesting technique for evaluating content, and might be worth investigating for evaluating content determination in NLG systems. In terms of validation, a number of studies have claimed that ROUGE correlates with human ratings, for example Lin and Hovy (2003) and Dang (2006). Dorr et al. (2005) checked if ROUGE scores correlated with ta"
J09-4008,P03-1021,0,0.0118187,"oblems in a system and suggest improvements. From this 554 Reiter and Belz Validity of Some Metrics for NLG Evaluation perspective, an advantage of human evaluations is that human subjects can be asked to make free-text comments on the texts that they see, and these comments are often extremely useful from a diagnostic perspective. On the other hand, an advantage of automatic metrics is that they allow developers to rapidly evaluate changes to systems and algorithms; indeed, some machine translation researchers use automatic metrics to automatically tune parameters without human intervention (Och 2003). However, as Och points out, this is only sensible if automatic metrics are known to be very accurate predictors of text quality. Because our results suggest that current automatic metrics are not highly accurate predictors of the quality of texts produced by NLG systems, we recommend developers be cautious in using metrics for diagnostic evaluation, and do not use metrics for automatic parameter tuning. On the other hand, automatic metrics do have a potential advantage in small diagnostic evaluations, which is that they are not inﬂuenced by the individual preferences of a small number of hum"
J09-4008,P02-1040,0,0.145102,"sts. Our results suggest that, at least in this domain, metrics may provide a useful measure of language quality, although the evidence for this is not as strong as we would ideally like to see; however, they do not provide a useful measure of content quality. We also discuss a number of caveats which must be kept in mind when interpreting this and other validation studies. 1. Introduction Evaluation is becoming an increasingly important topic in Natural Language Generation (NLG), as in other ﬁelds of computational linguistics. Many NLG researchers are impressed by the BLEU evaluation metric (Papineni et al. 2002) in Machine Translation (MT), which has allowed MT researchers to quickly and cheaply evaluate the impact of new ideas, algorithms, and data sets. BLEU and related metrics work by comparing the output of an MT system to a set of reference translations (human translations of the source text), and in principle this kind of evaluation could be done with NLG systems as well. As in other areas of NLP, the advantages of automatic corpus-based evaluation are that it is potentially much cheaper and quicker than human-based evaluation, and that it is repeatable. Indeed, NLG researchers have used BLEU i"
J09-4008,W02-2113,1,0.816357,"and (in some cases) lexical choices do not vary; this means there is less concern about reference texts not adequately covering the solution space. Automatic corpus-based evaluations are appealing in NLG, as in other areas of NLP, because they are relatively cheap and quick to do if a corpus is available, do not require support from domain experts, and are repeatable. However, their use in NLG is controversial, at least when evaluating systems as a whole instead of just surface realizers, because many people are concerned that the results of such evaluations may not be meaningful. For example Reiter and Sripada (2002) point out that corpus texts are often not of high enough quality to form good reference texts; and Scott and Moore (2007) express concern that metrics will not be able to evaluate many important linguistic properties such as information structure. 533 Computational Linguistics Volume 35, Number 4 A more general concern is that automatic metrics based on comparison to reference texts measure how well a text matches what writers do, whereas most human evaluations (task or judgment-based) measure the impact of a text on readers. Because writers do not always produce optimal texts from a reader’s"
J09-4008,W03-0611,1,0.729602,"ed a corpus and data set, called S UM T IME -M ETEO (Sripada et al. 2003). This consists of a corpus of 1,045 weather forecasts written by professional forecasters, and the numerical predictions of wind, temperature, and so forth, that forecasters examined when they wrote the forecasts. For wind descriptions only, the corpus also contains simple content representations containing information about wind speed and direction, time of day, and position in forecast (we call these “content tuples”). The content tuples were created by parsing the corpus texts and extracting the relevant information (Reiter and Sripada 2003), and are similar to the representations produced by the S UM T IME content-determination system. Figures 1, 2, and 3 show an extract from a numerical data ﬁle, an extract from the corresponding human-written forecast, and the content tuples derived from the human text. 537 Computational Linguistics day/hour 05/06 05/09 05/12 05/15 05/18 05/21 06/00 wind direction SSW S S S SSE SSE VAR Volume 35, Number 4 avg wind speed 18 16 14 14 12 10 6 max (gust) wind speed 22 20 17 17 15 12 7 Figure 1 Extract from meteorological data ﬁle for 05-10-2000 (morning forecast). FORECAST 06-24 GMT, THURSDAY, WIN"
J09-4008,W09-0629,1,\N,Missing
J09-4008,J09-1008,1,\N,Missing
J09-4008,C98-1112,0,\N,Missing
J09-4008,J98-3006,0,\N,Missing
J18-3002,W05-0909,0,0.546624,"(2016a). Colleagues have subsequently pointed out to me several relevant papers that were published after June 2017, such as Novikova et al. (2017), or otherwise were missed by my survey. I have not added these papers to my survey (it is not appropriate to add individual papers to a structured review; the only way to include these papers would be to redo the entire survey with new criteria and end date). However, I have read all of these papers, and they are consistent with the core findings of my survey. My survey is limited to BLEU, and does not look at other popular metrics such as METEOR (Banerjee and Lavie 2005). I focus on BLEU because I believe it is the most popular metric; indeed, Papineni et al. (2002) is one of the most cited NLP papers, according to Google Scholar, and has been given a NAACL Test of Time award. 1 http://aclanthology.info/. 2 https://arxiv.org/. 394 Ehud Reiter BLEU Structured Review 2.2 Screening Papers I screened the candidate papers by reading through them and selecting papers that met the criteria presented in Figure 2. If a paper presented several correlations between BLEU and human evaluations, I looked at the correlations individually and in some cases accepted some but"
J18-3002,P08-2050,0,0.0225446,"outcome of interest. In NLP, human evaluations can be based on human ratings or rankings (intrinsic) or on measurement of an outcome such as task performance (extrinsic); they can also be carried out in laboratory or real-world contexts. The strongest and most meaningful evaluation is a real-world outcome-based evaluation, where a system is operationally deployed and we measure its impact on real-world outcomes.4 From this perspective, it is striking that few of the surveyed papers looked at task/outcome measures. Indeed, only one paper correlated system-level BLEU scores with task outcomes (Belz and Gatt 2008), and all such correlations in that paper were Low or Negative. None of the surveyed papers used real-world human evaluations; that is, they all used human evaluations performed in an artificial context (usually by paid individuals, crowdsourced workers, or the researchers themselves), rather than looking at the impact of systems on real-world users. 4 Examples are given in https://ehudreiter.com/2017/01/19/types-of-nlg-evaluation/. 398 Ehud Reiter BLEU Structured Review Table 1 Correlation of BLEU with ranking-based human evaluation reported in WMT events, for German–English and English–Germa"
J18-3002,E06-1040,1,0.571905,"For example, Callison-Burch, Osborne, and Koehn (2006) present an initial set of correlations based on seven systems (their Figures 2 and 3) and a second set of correlations based on three systems (their Figure 4); I included the former but not the latter. Originality: Did not re-present results that had been presented in another paper; this prevents double-counting. I always preferred the primary source describing a study over a secondary source. However, if a paper presented an improved version of a study piloted in an earlier paper, I only included the later study. For example, I excluded Belz and Reiter (2006) because Reiter and Belz (2009) presents an improved version of that study. Figure 2 Inclusion criteria for studies. 395 Computational Linguistics r Volume 44, Number 3 NLP Systems in the study Type (e.g., MT) and subtype (e.g., Chinese-to-English) of NLP system – Output language (e.g., English) – Domain (e.g., news) – r BLEU scoring details Granularity: Whether BLEU scores were calculated for NLP systems or for individual texts produced by these systems. Note that some papers use the term segment for what I refer to as text granularity. – Number of reference texts (e.g., 1) – Source of refere"
J18-3002,W16-2302,0,0.0422916,"Missing"
J18-3002,W16-2301,0,0.0344138,"Missing"
J18-3002,D14-1026,0,0.0212407,"Missing"
J18-3002,E06-1032,0,0.578491,"Missing"
J18-3002,D10-1055,0,0.0165132,"osed to be better than BLEU. Figure 3 Information extracted from studies. 2.3 Extracting Information from Papers I tried to extract the information described in Figure 3 from each paper that made it through the screening process. However, in many cases I could not find all of this information in the paper. Some of the papers surveyed (as well as many of the papers I excluded) gave interesting qualitative analyses of cases when BLEU provides misleading results. For example, Bouamor et al. (2014) explain BLEU’s weaknesses in evaluating texts in morphologically rich languages such as Arabic, and Espinosa et al. (2010) point out that BLEU inappropriately penalizes texts that have different adverbial placement compared with reference texts. These comments are interesting and valuable research contributions, but in this structured review my focus is on quantitative correlations between BLEU and human evaluations. 3. Results The full results of the survey are presented in the data file associated with this article.3 I summarize key findings here. 3 http://dx.doi.org/10.20392/766c9dd8-75a7-4761-915d-856c0f7cc3c4. 396 Ehud Reiter BLEU Structured Review One important question is what level of correlation is suffi"
J18-3002,D15-1013,0,0.0880207,"Missing"
J18-3002,E17-1019,0,0.113688,"Missing"
J18-3002,D17-1238,0,0.478708,"Missing"
J18-3002,P02-1040,0,0.10979,"specially in machine translation and natural language generation. I present a structured review of the evidence on whether BLEU is a valid evaluation technique—in other words, whether BLEU scores correlate with real-world utility and user-satisfaction of NLP systems; this review covers 284 correlations reported in 34 papers. Overall, the evidence supports using BLEU for diagnostic evaluation of MT systems (which is what it was originally proposed for), but does not support using BLEU outside of MT, for evaluation of individual texts, or for scientific hypothesis testing. 1. Introduction BLEU (Papineni et al. 2002) is a metric that is widely used to evaluate Natural Language Processing (NLP) systems which produce language, especially machine translation (MT) and Natural Language Generation (NLG) systems. Because BLEU itself just computes word-based overlap with a gold-standard reference text, its use as an evaluation metric depends on an assumption that it correlates with and predicts the real-world utility of these systems, measured either extrinsically (e.g., by task performance) or by user satisfaction. From this perspective, it is similar to surrogate endpoints in clinical medicine, such as evaluati"
J18-3002,J09-4008,1,0.928288,"sborne, and Koehn (2006) present an initial set of correlations based on seven systems (their Figures 2 and 3) and a second set of correlations based on three systems (their Figure 4); I included the former but not the latter. Originality: Did not re-present results that had been presented in another paper; this prevents double-counting. I always preferred the primary source describing a study over a secondary source. However, if a paper presented an improved version of a study piloted in an earlier paper, I only included the later study. For example, I excluded Belz and Reiter (2006) because Reiter and Belz (2009) presents an improved version of that study. Figure 2 Inclusion criteria for studies. 395 Computational Linguistics r Volume 44, Number 3 NLP Systems in the study Type (e.g., MT) and subtype (e.g., Chinese-to-English) of NLP system – Output language (e.g., English) – Domain (e.g., news) – r BLEU scoring details Granularity: Whether BLEU scores were calculated for NLP systems or for individual texts produced by these systems. Note that some papers use the term segment for what I refer to as text granularity. – Number of reference texts (e.g., 1) – Source of reference texts (e.g., professional t"
N13-1137,W08-1107,0,0.163416,"n method for non-deterministic REG that aligns generated and observed data and calculates accuracy over alignments. 1174 Proceedings of NAACL-HLT 2013, pages 1174–1184, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Motivation & Overview Most implemented algorithms for referring expression generation focus on unique identification of a referent, determining the set of properties that distinguish a particular target object from the other objects in the scene (the contrast set) (Dale, 1989; Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975; Whitehurst, 1976; Sonnenschein, 1985) as well as later studies on"
N13-1137,P08-2050,0,0.0193456,"he training data. The ordered attribute lists for the algorithm (AP, RP and P) are built in the same way as the preference order list for the IA and GB, listing attributes from the training data in order of 5 We remove location from evaluation in this corpus. Location is not annotated directly, but split such that only xdimension or y-dimension may be marked for a reference. 1180 descending frequency. For these corpora, there are not absolute properties beyond color, so AP is empty. 6 Evaluation Previous evaluation of REG algorithms have used measurements such as Uniqueness, Minimality, Dice (Belz and Gatt, 2008), and Accuracy (Gatt et al., 2009; Reiter and Belz, 2009). Uniqueness is the proportion of outputs that identify the referent uniquely, and Minimality is the proportion of outputs that are both minimal and unique. As our goal is to mimic human reference, these metrics are not as useful for the evaluations as the others. The Dice metric provides a value for the similarity between a generated description and a humanproduced description, and therefore serves as a reasonable objective measure for how human-like the produced sets are. Given the generated property set (DS ) and the human-produced pr"
N13-1137,P89-1009,0,0.334524,"l input; saying an object is “tall” requires further reasoning). 4. An evaluation method for non-deterministic REG that aligns generated and observed data and calculates accuracy over alignments. 1174 Proceedings of NAACL-HLT 2013, pages 1174–1184, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Motivation & Overview Most implemented algorithms for referring expression generation focus on unique identification of a referent, determining the set of properties that distinguish a particular target object from the other objects in the scene (the contrast set) (Dale, 1989; Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains"
N13-1137,W08-1108,0,0.143947,"is represented as a vertex, with properties for an object represented as self-edges on the object vertex, and spatial relations between objects represented as edges between vertices. The algorithm seeks to find the cheapest subgraph, calculated from the edge costs. We use the implementation available from Viethen et al. (2008), which adds a preference order to decide between edges with the same cost during search. This has 3 https://github.com/nltk/nltk contrib/blob/master/ nltk contrib/referring.py retrieved 1.Aug.2012. been one of the best-performing systems in recent generation challenges (Gatt and Belz, 2008; Gatt et al., 2009). An important commonality between these algorithms, and much of the work on REG that they have influenced, is the focus on unique identification and operating deterministically. Both produce one property set (and only one), and stop once a target item has been uniquely identified (or else fail). Their driving goal is to rule out distractor objects. In the approach introduced here, the algorithm produces a distribution over several possible outputs, and the initial driving mechanism is based on likelihood estimates for each attribute independent of the other objects in the"
N13-1137,W09-0629,0,0.0194427,"ertex, with properties for an object represented as self-edges on the object vertex, and spatial relations between objects represented as edges between vertices. The algorithm seeks to find the cheapest subgraph, calculated from the edge costs. We use the implementation available from Viethen et al. (2008), which adds a preference order to decide between edges with the same cost during search. This has 3 https://github.com/nltk/nltk contrib/blob/master/ nltk contrib/referring.py retrieved 1.Aug.2012. been one of the best-performing systems in recent generation challenges (Gatt and Belz, 2008; Gatt et al., 2009). An important commonality between these algorithms, and much of the work on REG that they have influenced, is the focus on unique identification and operating deterministically. Both produce one property set (and only one), and stop once a target item has been uniquely identified (or else fail). Their driving goal is to rule out distractor objects. In the approach introduced here, the algorithm produces a distribution over several possible outputs, and the initial driving mechanism is based on likelihood estimates for each attribute independent of the other objects in the scene, rather than r"
N13-1137,J09-2005,0,0.021447,"a way similar to human visual processing, the generated expression may be overspecified or underspecified. We are limited by available REG corpora to reliably assess methods for generating more complex absolute properties like shape and material, but adding such properties would help advance the generation of human-like reference in visual scenes and offers further points of connection between the generation process and computer vision property detection. Models for generating more complex spatial relations are currently available, and are a natural extension to this framework (e.g., those of Kelleher and Costello (2009)) as object detection becomes more robust. We may also be able to build more sophisticated graphical models as larger corpora become available. For example, modeling the conditional probability of generating reference for a property vn given the previously generated context p(vn |v1 . . . vn−1 ) may bring us closer to human-like output. There are several additional issues that do not arise in this evaluation, but we expect must be accounted for when referring to naturalistic objects in improves performance. 1182 visual domains. These include: • The interconnected nature of properties, where so"
N13-1137,W12-1503,0,0.0289095,"Missing"
N13-1137,J12-1006,1,0.849312,"Missing"
N13-1137,J03-1003,0,0.0294139,"Missing"
N13-1137,W11-2808,1,0.858487,"Missing"
N13-1137,J09-4008,1,0.840197,"lgorithm (AP, RP and P) are built in the same way as the preference order list for the IA and GB, listing attributes from the training data in order of 5 We remove location from evaluation in this corpus. Location is not annotated directly, but split such that only xdimension or y-dimension may be marked for a reference. 1180 descending frequency. For these corpora, there are not absolute properties beyond color, so AP is empty. 6 Evaluation Previous evaluation of REG algorithms have used measurements such as Uniqueness, Minimality, Dice (Belz and Gatt, 2008), and Accuracy (Gatt et al., 2009; Reiter and Belz, 2009). Uniqueness is the proportion of outputs that identify the referent uniquely, and Minimality is the proportion of outputs that are both minimal and unique. As our goal is to mimic human reference, these metrics are not as useful for the evaluations as the others. The Dice metric provides a value for the similarity between a generated description and a humanproduced description, and therefore serves as a reasonable objective measure for how human-like the produced sets are. Given the generated property set (DS ) and the human-produced property set (DH ), Dice is calculated as: 2 × |DS ∩ DH | |"
N13-1137,C92-1038,1,0.627793,"ing an object is “tall” requires further reasoning). 4. An evaluation method for non-deterministic REG that aligns generated and observed data and calculates accuracy over alignments. 1174 Proceedings of NAACL-HLT 2013, pages 1174–1184, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Motivation & Overview Most implemented algorithms for referring expression generation focus on unique identification of a referent, determining the set of properties that distinguish a particular target object from the other objects in the scene (the contrast set) (Dale, 1989; Reiter and Dale, 1992; Dale and Reiter, 1995; Krahmer et al., 2003; Areces et al., 2008). This view of reference was first outlined by Olson (1970), “the specification of an intended referent relative to a set of alternatives”. A substantial body of evidence now shows that contrastive value relative to alternatives is not the only factor motivating speakers’ property choices, specifically in visual domains. The phenomena of overspecification and redundancy, where speakers select properties that have little or no contrastive value, was observed in early developmental studies in visual domains (Ford and Olson, 1975;"
N13-1137,P11-2116,0,0.0738951,"precedes size in the preference orders, in line with recent research showing that this allows the algorithm to perform optimally on the TUNA corpus (van Deemter et al., 2012a). In development, we find that IA performs best with type as the last attribute in the PO, and report on numbers with this approach. 5.2.2 The Graph-Based Algorithm The version of the Graph-Based Algorithm that we use is available from Viethen et al. (2008). This algorithm requires (1) a set of cost functions for each edge, and (2) a PO for deciding between properties in the case of a tie. For (1), we use the method from Theune et al. (2011) to assign two costs (0, 1) to the edges. We first determine the relative frequency with which each property is mentioned for a target object, and then create costs for each property using k-means clustering (k=2) in the Weka toolkit (Hall et al., 2009). We refer interested readers to the Theune et al. paper for further details. For (2), we follow the same method as for the Incremental Algorithm. 5.2.3 The Visual Objects Algorithm The proposed algorithm requires αatt , which we estimate as the relative frequency of each attribute att in the training data. The ordered attribute lists for the al"
N13-1137,W06-1420,1,0.52943,"Missing"
N13-1137,W08-1109,0,0.418851,"colour:green size:(254,254) type:fan loc:(2,2) ori:left Figure 6: Example input scene: TUNA corpus. For IA And GB, gold-standard size values are provided rather than measurements (small, large). As such, although hcolor:grey, type:deski would sufficiently distinguish the intended referent, we instead produce a variety of sets, overspecifying in some instances (e.g., hcolor:grey, ori:front, type:deski), and with a small chance of underspecifying in others (e.g., hsize:large, type:deski). 5 Evaluation Algorithms & Corpora 5.1 Corpora We evaluate on two well-known REG corpora, the GRE3D3 corpus (Viethen and Dale, 2008) and the singular furniture section of the TUNA corpus (van Deemter et al., 2006). Both corpora contain expressions elicited to computer-generated objects, and so provide a reasonable starting point for evaluating reference to visible objects. For all algorithms, we evaluate on the selection of referent attributes. Lexical choice and word order are not taken into account. Example images from GRE3D3 and TUNA are shown in Figure 4, and example algorithm input from these corpora are shown in Figures 5 and 6. In GRE3D3, we evaluate on the selection of type, color, size, and location, but leave asi"
N13-1137,U10-1013,0,0.10772,"respond to general visual attributes and may generate forms for visual properties (attribute:value pairs). That is, a property such as color:red is generated from the attribute node color and a property such as size:tall is generated from the attribute node size. We are limited by existing REG corpora in which properties we can evaluate; in this paper, we examine the effect of the independent selection of color and size, followed by location and orientation.2 Generating human-like expressions in this setting begins to be possible by adopting recent proposals that REG handle speaker variation (Viethen and Dale, 2010) and the non-deterministic nature of reference (van Gompel et al., 2012; van Deemter et al., 2012b). We can capture such variation simply by estimating αatt , the likelihood that an attribute att generates a corresponding visual property. During generation, the algorithm passes through each attribute node, and uses this estimate to stochastically add each property to the output property set. Such a non-deterministic process means that the algorithm will not return the same output every time, which offers new challenges for evaluation. If we run the algorithm 1,000 times, we have a distribution"
N13-1137,viethen-etal-2008-controlling,0,0.376005,"ce a novel algorithm for generating referring expressions, informed by human and computer vision and designed to refer to visible objects. Our method separates absolute properties like color from relative properties like size to stochastically generate a diverse set of outputs. Expressions generated using this method are often overspecified and may be underspecified, akin to expressions produced by people. We call such expressions identifying descriptions. The algorithm outperforms the well-known Incremental Algorithm (Dale and Reiter, 1995) and the GraphBased Algorithm (Krahmer et al., 2003; Viethen et al., 2008) across a variety of images in two domains. We additionally motivate an evaluation method for referring expression generation that takes the proposed algorithm’s non-determinism into account. 1 Introduction Referring expression generation (REG) is the task of generating an expression that can identify a referent to a listener. These expressions generally take the form of a definite noun phrase such as “the large orange plate” or “the furry running dog”. Research in REG primarily focuses on the subtask of selecting a set of properties that may be used to construct the final surface expression,"
N13-1137,J02-4007,1,\N,Missing
P01-1057,P00-1020,0,0.440593,"meeting a communicative goal against the effectiveness of non-NLG control texts. Young’s task evaluation, which may be the most rigorous previous task evaluation of an NLG system, compared the effectiveness of texts generated by different NLG algorithms, while the IDAS task evaluation (Levine and Mellish, 1995) did not include a control text of any kind. Coch (1996) and Lester and Porter (1997) have compared NLG texts to humanwritten and (in Coch’s case) mail-merge texts, but the comparisons were judgements by human domain experts, they did not measure the actual impact of the texts on users. Carenini and Moore (2000) probably came closest to a controlled evaluation of NLG vs non-NLG alternatives, because they compared the impact of NLG argumentative texts to a no-text control (where users had access to the underlying data but were not given any texts arguing for a particular choice). questionnaire is shown in Figure 2. 1 We wish to emphasise that producing personalised health information letters is not a new idea, many previous researchers have worked in this area; see Lennox et al (2001) for a comparison of STOP to previous work in this area. The STOP clinical trial, which is the focus of this paper, was"
P01-1057,C96-1043,0,0.146309,"ubjects different texts, and measuring differences in an outcome variable, such as success in performing a task. However, despite the above work, we are not aware of any previous evaluation which has compared the effectiveness of NLG texts at meeting a communicative goal against the effectiveness of non-NLG control texts. Young’s task evaluation, which may be the most rigorous previous task evaluation of an NLG system, compared the effectiveness of texts generated by different NLG algorithms, while the IDAS task evaluation (Levine and Mellish, 1995) did not include a control text of any kind. Coch (1996) and Lester and Porter (1997) have compared NLG texts to humanwritten and (in Coch’s case) mail-merge texts, but the comparisons were judgements by human domain experts, they did not measure the actual impact of the texts on users. Carenini and Moore (2000) probably came closest to a controlled evaluation of NLG vs non-NLG alternatives, because they compared the impact of NLG argumentative texts to a no-text control (where users had access to the underlying data but were not given any texts arguing for a particular choice). questionnaire is shown in Figure 2. 1 We wish to emphasise that produc"
P01-1057,J97-1004,0,0.0999464,") for a summary of NLG evaluation. As Mellish and Dale point out, we can evaluate the effectiveness of underlying theories, general properties of NLG systems and texts (such as computational speed, or text understandability), or the effectiveness of the generated texts in an actual task or application context. Theory evaluations are typically done by comparing predictions of a theory to what is observed in a humanauthored corpus (for example, (Yeh and Mellish, 1997)). Evaluations of text properties are typically done by asking human judges to rate the quality of generated texts (for example, (Lester and Porter, 1997)); sometimes human-authored texts are included in the rated set (without judges knowing which texts are human-authored) to provide a baseline. Task evaluations (for example, (Young, 1999)) are typically done by showing human subjects different texts, and measuring differences in an outcome variable, such as success in performing a task. However, despite the above work, we are not aware of any previous evaluation which has compared the effectiveness of NLG texts at meeting a communicative goal against the effectiveness of non-NLG control texts. Young’s task evaluation, which may be the most rig"
P01-1057,J00-2005,1,0.864175,"Missing"
P01-1057,W00-1429,1,0.914751,"k much better if it has access to both positive and negative examples; in other words, if researchers publish their failures as well as their successes. We believe that negative results are also important in NLG, NLP, and AI, even if it is not possible to draw straightforward lessons from them; and we hope that more such results are reported in the future. 6 Other Evaluation Techniques in STOP The clinical trial was by far the biggest evaluation exercise in STOP, but we also performed some smaller evaluations in order to test our algorithms and knowledge acquisition methodology (Reiter, 2000; Reiter et al., 2000). These included: 1. Asking smokers or domain experts to read two letters, and state which one they thought was superior; 2. Statistical analyses of characteristics of smokers; and 3. Comparing the effectiveness of different algorithms at filling up but not exceeding 4 A5 pages. These evaluations were much smaller, simpler, and cheaper than the clinical trial, and often gave easier to interpret results. For example, the letter-comparison experiments suggested (although they did not prove) that older people preferred a more formal writing style than younger people; the statistical analysis sugg"
P01-1057,J97-1007,0,0.0645733,"te their systems. 2 Evaluation of NLG Systems Evaluation is becoming increasingly important in NLG, as in other areas of NLP; see Mellish and Dale (1998) for a summary of NLG evaluation. As Mellish and Dale point out, we can evaluate the effectiveness of underlying theories, general properties of NLG systems and texts (such as computational speed, or text understandability), or the effectiveness of the generated texts in an actual task or application context. Theory evaluations are typically done by comparing predictions of a theory to what is observed in a humanauthored corpus (for example, (Yeh and Mellish, 1997)). Evaluations of text properties are typically done by asking human judges to rate the quality of generated texts (for example, (Lester and Porter, 1997)); sometimes human-authored texts are included in the rated set (without judges knowing which texts are human-authored) to provide a baseline. Task evaluations (for example, (Young, 1999)) are typically done by showing human subjects different texts, and measuring differences in an outcome variable, such as success in performing a task. However, despite the above work, we are not aware of any previous evaluation which has compared the effecti"
P90-1013,P89-1009,0,\N,Missing
P92-1034,P90-1020,0,0.0659593,"Missing"
P92-1034,C90-3052,0,0.0496593,"Missing"
P92-1034,E89-1009,0,0.122749,"Missing"
P92-1034,H89-1022,0,0.125367,"Missing"
P92-1034,P83-1012,0,0.0531459,"Missing"
P92-1034,J81-1002,0,0.0840723,"Missing"
P92-1034,P89-1025,0,0.204415,"Missing"
P92-1034,C92-1038,1,0.877902,"Missing"
P92-1034,A92-1009,1,0.883591,"Missing"
P92-1034,C88-2128,0,0.0683055,"Missing"
W00-1429,A94-1002,0,\N,Missing
W00-1429,J00-2005,1,\N,Missing
W00-1429,A97-1037,1,\N,Missing
W01-0802,W98-1409,0,0.0205125,"d ultimately is responsible for generating the overview. Communication Goal (CG) is the input to the data summarisation system in response to which it accesses DDS to produce an overview of the data using the DR. In the context of the overview produced by DR, the Communication Reasoner (CR) system generates the final content specification taking into account the influence of the User Constraints (UC) and other pragmatic factors. This content is then sent to subsequent NLG modules (not shown), such as microplanning and surface realisation. Our model has some similarities to the one proposed by Barzilay et al (1998), in that the Domain Reasoner uses general domain knowledge similar to their RDK, while the Communication Reasoner uses communication knowledge similar to their CDK and DCK. The central feature of the above model is the idea of data overview and its effect on content selection. One possible use of overviews is to trigger context-dependent content rules. The time-series analysis part of SUMTIME is largely based on Shahar&apos;s model (1997), which makes heavy use of such rules. In Shahar&apos;s model contexts are inferred by separate mechanisms; we believe that these should be incorporated into the overv"
W01-0802,W98-1410,0,0.0124585,"content. From a theoretical perspective content determination should probably be based on deep reasoning about the system&apos;s communicative goal, the user&apos;s intentions, and the current context (Allen and Perrault 1980), but this requires an enormous amount of knowledge and reasoning, and is difficult to do robustly in real applications. In recent years many new content determination strategies have been proposed, ranging from the use of sophisticated signal-processing techniques (Boyd 1997) to complex planning algorithms (Mittal et al 1998) to systems which exploit cognitive models of the user (Fiedler 1998). However, most of these strategies have only been demonstrated in one application. Furthermore, as far as we can tell these strategies are usually based on the intuition and experiences of the developers. While realisation, microplanning, and document structuring techniques are increasingly based on analyses of how humans perform these tasks (including corpus analysis, psycholinguistic studies, and KA activities), most papers on content determination make little reference to how human experts determine the content of a text. Human experts are often consulted with regard to the details of cont"
W01-0802,J98-3004,0,0.026457,"resses appropriate content to a text which nicely expresses inappropriate content. From a theoretical perspective content determination should probably be based on deep reasoning about the system&apos;s communicative goal, the user&apos;s intentions, and the current context (Allen and Perrault 1980), but this requires an enormous amount of knowledge and reasoning, and is difficult to do robustly in real applications. In recent years many new content determination strategies have been proposed, ranging from the use of sophisticated signal-processing techniques (Boyd 1997) to complex planning algorithms (Mittal et al 1998) to systems which exploit cognitive models of the user (Fiedler 1998). However, most of these strategies have only been demonstrated in one application. Furthermore, as far as we can tell these strategies are usually based on the intuition and experiences of the developers. While realisation, microplanning, and document structuring techniques are increasingly based on analyses of how humans perform these tasks (including corpus analysis, psycholinguistic studies, and KA activities), most papers on content determination make little reference to how human experts determine the content of a text."
W01-0802,W00-1429,1,0.87951,"Missing"
W01-0802,A94-1002,0,\N,Missing
W03-0611,W02-1022,0,0.0313547,"the NLP community, models of word meanings are typically either entered by a user or developer (for example in Microsoft’s English Query natural-language interday 25-10-00 25-10-00 25-10-00 25-10-00 25-10-00 25-10-00 25-10-00 25-10-00 26-10-00 hour 0 3 6 9 12 15 18 21 0 wind dir SSW SSE ESE ESE E ENE ENE NNE NNW wind speed 12 11 18 16 15 15 18 20 26 Table 1: Wind (at 10m) extract from 24-Oct-00 data file face) or derived from a hand-built knowledge base (eg, (Reiter, 1991)). There is growing interest in trying to learn word meanings from parallel text-data corpora, for example (Siskind, 2001; Barzilay and Lee, 2002; Roy, 2002). We believe our work is unusual because we are using naturally occurring texts and data. Siskind (2001), in contrast, used data which was explicitly created for his experiments; Barzilay and Lee (2002) used texts which subjects had written for a previous experiment; and Roy (2002) used both data and texts that were created for his experiments. 3 SumTime Project and Corpora The S UM T IME project is investigating better technology for building software systems that automatically generate textual summaries of time-series data. One of the domains S UM T IME is working in is weather f"
W03-0611,J90-2002,0,0.0626816,"ng of time phrases in weather forecasts by analysing a parallel corpus of (A) manually-written weather forecast texts and (B) the numerical data (from a weather simulation) that the human forecasters examined when writing the textual forecasts. The analysis procedure first aligns (associates) text fragments with data segments, and then infers the meaning of each time phrase by statistically analysing the time of data segments that are aligned to textual phrases that contain this time phrase. This is broadly similar in concept to the use of parallel multilingual corpora in machine translation (Brown et al., 1990), except that our parallel corpus consists of texts and underlying numeric data, not texts and their translations. In other words, we are trying to learn what words mean in terms of non-linguistic data, not the best translations of words in another language. Probably the biggest surprise in our analysis was the substantial variation we saw between individuals. For Linguists and lexicographers have used a number of different techniques to determine the meanings of words. These include asking native-speaker informants to judge the acceptability and oddness of test sentences (Cruse, 1986); defini"
W03-0611,C00-2163,0,0.0260422,"Missing"
W03-0611,W02-2113,1,0.917041,"ss our current work on using parallel text-data corpora to learn the meanings of other types of words. Somayajulu Sripada Department of Computing Science University of Aberdeen ssripada@csd.abdn.ac.uk example, by evening apparently meant 1800 to some people, but 0000 to others. Although the possibility of such variation in individual idiolects has been acknowledged in the past (for example, (Nunberg, 1978; Parikh, 1994)), it seems to be ignored by most recent work on lexical semantics. We have published other papers that have summarised our key findings, notably variation between individuals (Reiter and Sripada, 2002a; Reiter and Sripada, 2002b); and also described the corpus itself (Sripada et al., 2003b). The purpose of this paper is to describe our analysis procedure (including alignment) and results in detail, and to also discuss our current work on using parallel text-data corpora to learn the meanings of other types of words. 2 Previous Research 1 Introduction NLP systems that interact with the world often need models of what words mean in terms of the non-linguistic world. In this paper, we describe how we have determined the meaning of time phrases in weather forecasts by analysing a parallel corp"
W03-0611,W00-1429,1,0.827359,"Missing"
W03-0611,J02-4007,1,\N,Missing
W03-0611,E03-1021,1,\N,Missing
W03-0611,J02-2001,0,\N,Missing
W03-2312,J02-4007,1,\N,Missing
W03-2312,P89-1025,0,\N,Missing
W05-1615,W00-1401,0,0.140785,"Missing"
W05-1615,J97-1004,0,0.0604181,"Missing"
W05-1615,W03-0203,0,0.0779521,"Missing"
W05-1615,W03-0611,1,0.90299,"Missing"
W05-1615,C92-2067,0,0.123395,"Missing"
W05-1615,J02-4007,1,\N,Missing
W05-1616,A97-1039,0,0.10172,"Missing"
W05-1616,P93-1031,0,0.0862738,"Missing"
W05-1616,J88-3006,0,0.0742543,"Missing"
W05-1616,C00-2093,0,0.0612363,"Missing"
W05-1616,W03-2317,1,0.867427,"Missing"
W05-1616,W01-1605,0,\N,Missing
W05-1616,J02-2001,0,\N,Missing
W06-1422,W00-1401,0,0.194158,"s, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)). One of the best-known comparative studies of evaluation techniques was by Papineni et al. (2002) who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems. Several other studies of this type have been carried out in the MT and Summarisation communities. The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al. (2000). The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human judgements and several corpus-based metrics. They used linear regression to suggest a combination of the corpus-based metrics which they be136 Proceedings of the Fourth International Natural Language Generation Conference, pages 136–138, c Sydney, July 2006. 2006 Association for Computational Linguistics We will create the semantic-level representations by parsing the corpus texts, probably using a LinGO parser1 . We will create the content representations u"
W06-1422,W06-1421,1,0.829864,"ems with similar functionalities. Correlating the results of the different evaluation techniques will give us empirical insight as 2 Comparative Evaluations in NLG There is a long history of shared task initiatives in NLP, of which the best known is perhaps MUC (Hirschman, 1998); others include TREC, PARSE VAL, SENSEVAL , and the range of shared tasks organised by CoNLL. Such exercises are now common in most areas of NLP, and have had a major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)). One of the best-known comparative studies of evaluation techniques was by Papineni et al. (2002) who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems. Several other studies of this type have been carried out in the MT and Summarisation communities. The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al. (2000). The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human"
W06-1422,E06-1040,1,0.926429,"l summaries of statistical data. The actual data will come from opinion polls or national statistics offices. The corpus will also include data about the authors (e.g., age, sex, domain expertise). Nurses’ reports: As part of a new project at Aberdeen, Babytalk2 , we will be acquiring a corpus of texts written by nurses to summarise the status of a baby in a neonatal intensive care unit, along with the raw data this is based on (sensor readings, records of actions taken such as giving medication). lieve is a better predictor of human judgements than any of the individual metrics. In our work (Belz and Reiter, 2006), we used several different evaluation techniques (human and corpus-based) to evaluate the output of five NLG systems which generated wind descriptions for weather forecasts. We then analysed how well the corpus-based evaluations correlated with the human-based evaluations. Amongst other things, we concluded that BLEU-type metrics work reasonably well when comparing statistical NLG systems, but less well when comparing statistical NLG systems to knowledge-based NLG systems. We worked in this domain because of the availability of the SumTime corpus (Sripada et al., 2003), which contains both nu"
W06-1422,P02-1040,0,0.087069,"ive us empirical insight as 2 Comparative Evaluations in NLG There is a long history of shared task initiatives in NLP, of which the best known is perhaps MUC (Hirschman, 1998); others include TREC, PARSE VAL, SENSEVAL , and the range of shared tasks organised by CoNLL. Such exercises are now common in most areas of NLP, and have had a major impact on many areas, including machine translation and information extraction (see discussion of history of shared-task initiatives and their impact in Belz and Kilgarriff (2006)). One of the best-known comparative studies of evaluation techniques was by Papineni et al. (2002) who proposed the BLEU metric for machine translation and showed that BLEU correlated well with human judgements when comparing several machine translation systems. Several other studies of this type have been carried out in the MT and Summarisation communities. The first comparison of NLG evaluation techniques which we are aware of is by Bangalore et al. (2000). The authors manually created several variants of sentences from the Wall Street Journal, and evaluated these sentences using both human judgements and several corpus-based metrics. They used linear regression to suggest a combination"
W06-1422,W02-2113,1,\N,Missing
W07-2315,W02-2119,0,\N,Missing
W07-2315,E06-2020,1,\N,Missing
W07-2315,A94-1002,0,\N,Missing
W07-2315,P83-1022,0,\N,Missing
W07-2325,W00-1429,1,0.87135,"Missing"
W07-2325,W07-2315,1,0.871054,"s how texts that communicate emotionally sensitive information should be worded. In this paper we focus on the usage of ’hedge phrases’ which communicate empathetic information, such as ""unfortunately."" An experiment in the domain of communicating exam results to students suggests that such emphathetic hedge phrases are appreciated by non-native English speakers, but disliked by native English speakers. 2 Motivation Some NLG systems produce texts that communicate emotionally sensitive information. In particular, the BT-Parent system, which is part of the BabyTalk project (Portet et al., 2007; Reiter, 2007), produces texts that summarise the condition of a baby in a neonatal intensive care unit, for the baby’s parents. Such texts must be worded in a way which minimises emotional distress, while of course still being truthful. The work here is an initial attempt to explore one aspect of how sensitive information should be communicated. Because of ethical considerations, we have conducted this initial experiment with a different group, students who are being told about exam results. 155 Hedge Phrases Hedging can be described as a strategy by which speakers mitigate and soften the force of their ut"
W08-1104,I05-5005,0,0.0127376,"erence frames. This approach has been implemented in a data-to-text system we have deployed in the weather forecasting domain. 1 Ian P Davy Aerospace and Marine Intl., Banchory, Aberdeenshire, UK idavy@weather3000.com Introduction Data-to-text systems are NLG systems that generate texts from raw input data. Many examples of such systems have been reported in the literature, which have been applied in a number of domains and to different types of input. For example, BabyTalk (Portet et al., 2007) generates medical reports from sensors monitoring a baby in a Neonatal Intensive Care Unit, while (Hallett and Scott, 2005) describe a system for generating reports from events in medical records. SumTime (Reiter et al., 2005), (Coch, 1998) and Fog (Goldberg et al., 1994) generate weather forecasts from the output of weather computer simulation models, while (Iordanskaja et al., 1992) and (R¨osner, 1987) both generate summaries from employment statistics. As the above examples show most work in data-totext up to now has concentrated almost exclusively on time series data. Work on generating text from spatial data has been reported in Coral (Dale et al., 2005), which generates route descriptions of a path construct"
W08-1104,W06-1408,0,0.0461773,"Missing"
W08-1104,C92-3158,0,0.0930965,"Missing"
W08-1104,J03-1003,0,0.0996369,"Missing"
W08-1104,W07-2315,1,0.740029,"Authored Corpus Text 3 Architecture As noted in the previous section, the input data to our system contains only limited spatial information: a point identifier that ties the measurement site to a particular route and a latitude longitude coordinate. Therefore it is necessary for our system to perform additional spatial reasoning to characterise the input in terms of its underlying geography. The architecture of our system shown in Figure 4, extends 1. The input data has to be analysed, this is non17 Figure 4: RoadSafe System Architecture the architecture for data-to-text systems proposed in (Reiter, 2007) to include this additional processing. In Section 3.1 we explain some of the rationale behind these design decisions based on observations from our knowledge acquisition(KA) Studies. In Sections 3.2 and 3.3 we explain the additional modules we have introduced in more detail. 3.1 3. Coastal Proximity e.g. ‘strong winds along the coast’. 4. Population e.g. e.g. ‘Roads through the Urban areas holding just above freezing’. Communicative Purpose of Spatial Descriptions From our studies we have found that experts generally follow 4 steps when writing road ice forecasts: Observations from Knowledge"
W08-1104,W05-1615,1,0.756383,"ating the quality of the output of the current system using post edit techniques and feedback from expert meteorologists at AMI. Our prototype has been installed at AMI since the start of the year and is being used to generate draft road ice forecasts for one of their local council clients. One forecast is generated per day which is then post-edited by an on duty forecaster before it is sent to the client. While common in Machine Translation post-edit evaluations are still relatively rare in NLG. The only large scale post-edit evaluation of an NLG system to our knowledge has been reported in (Sripada et al., 2005). Our current evaluation is small in comparison to that evaluation; SumTime-Mousam, the system being evaluated in that work was generating 150 draft forecasts per day. However, it does try to address some of the problems the authors encountered during that evaluation. The main issue outlined by (Sripada et al., 2005) was that their analysis was post-hoc and therefore not supported by authors or by an editing tool, which made it difficult to analyse why post-edits were made. We have accounted for this by including post-editing as part of our development process and making use of a simple online"
W08-1104,J06-2002,0,0.187473,"Missing"
W08-1104,W01-0802,1,\N,Missing
W08-1119,E06-1040,1,0.50769,"t al., 2005). Most of these systems have generated short (paragraph-length or smaller) summaries of relatively small data sets (less than 1KB). Some research has been on systems that summarise larger data sets (Yu et al., 2007; Turner et al., 2008), but these systems have also generated paragraph-length summaries; we are not aware of any previous research on generating multi-paragraph summaries in a data-to-text system. Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al., 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al., 2005), and persuasive effectiveness (Carenini and Moore, 2006). However, again to the best of our knowledge no previous data-to-text system has been evaluated by asking users to make decisions based on the generated texts, and measuring the quality of these decisions. 2 BabyTalk and BT-45 Law et al. (2005) showed that human-written textual summaries were effective decision-support aids in NICU , but of course it is not practical to expect medical professionals to routinely write such summaries, especially considering that the summaries used by Law et al. i"
W08-1119,J88-2003,0,0.0454049,"was inserted successfully. In other words, the time given would still be the time that the abstract event started; but this is misleading, because readers of the above text expect that the stated time is the time of the successful insertion (14.08), not the time at which the sequence of insert/remove events started. We need a much better model of how to communicate time, and how this communication depends on the semantics and linguistic expression of the events being described. An obvious first step, which we are currently working on, is to include a linguisticallymotivated temporal ontology (Moens and Steedman, 1988), which will be separate from the existing domain ontology. We also need better techniques for communicating the temporal relationships between events in cases where they are not listed in chronological order (Oberlander and Lascarides, 1992). 6 Discussion Two discourse analysts from Edinburgh University, Dr. Andy McKinlay and Dr Chris McVittie, kindly examined and compared some of the human and BT45 texts. Their top-level comment was that the human texts had much better narrative structures than the BT-45 texts. They use the term ‘narrative’ in 153 the sense of Labov (1972, Chapter 9); that i"
W08-1119,C92-2108,0,0.0578845,"rtion (14.08), not the time at which the sequence of insert/remove events started. We need a much better model of how to communicate time, and how this communication depends on the semantics and linguistic expression of the events being described. An obvious first step, which we are currently working on, is to include a linguisticallymotivated temporal ontology (Moens and Steedman, 1988), which will be separate from the existing domain ontology. We also need better techniques for communicating the temporal relationships between events in cases where they are not listed in chronological order (Oberlander and Lascarides, 1992). 6 Discussion Two discourse analysts from Edinburgh University, Dr. Andy McKinlay and Dr Chris McVittie, kindly examined and compared some of the human and BT45 texts. Their top-level comment was that the human texts had much better narrative structures than the BT-45 texts. They use the term ‘narrative’ in 153 the sense of Labov (1972, Chapter 9); that is storylike structures which describe real experiences, and which go beyond just describing the events and include information that helps listeners make sense of what happened, such as abstracts, evaluatives, correlatives, and explicatives. D"
W08-1119,passonneau-2006-measuring,0,0.0161877,"ation is not possible. But it also suggests that if we can improve the technology so that computer-generated texts are as effective as human texts, we should have a very effective decisionsupport technology. 4 Quantitative Comparison of BT-45 and Corpus Texts In addition to the task-based evaluation described above, we also quantitatively compared the BT-45 and human texts, and qualitatively analysed problems in the BT-45 texts. Quantitative comparison was done by annotating the BT-45 and human texts to identify which events they mentioned. For each scenario, we computed the MASI coefficient (Passonneau, 2006) between the set of events mentioned in the BT-45 and human texts. The average MASI score was 0.21 (SD = 0.13), which is low; this suggests that BT-45 and the human writers choose different content. We also checked whether similar human and BT-45 texts (as judged by MASI score) obtained similar evaluation scores; in fact there was no significant correlation between MASI similarity of human and BT-45 texts and the difference between their evaluation scores. We performed a second analysis based on comparing the structure (e.g., number and size of paragraphs) of the BT-45 and human texts, using a"
W08-1119,W07-2315,1,0.734978,"ation of the system suggested that these summaries are useful, but not as effective as they could be. In this paper we present a qualitative analysis of problems that the evaluation highlighted in BT-45 texts. Many of these problems are due to the fact that BT45 does not generate good narrative texts; this is a topic which has not previously received much attention from the NLG research community, but seems to be quite important for creating good data-to-text systems. 1 Introduction Data-to-text NLG systems produce textual output based on the analysis and interpretation of nonlinguistic data (Reiter, 2007). Systems which produce short summaries of small amounts of data, such as weather-forecast generators (Reiter et al., 2005), have been one of the most successful applications of NLG , and there is growing interest in creating systems which produce longer summaries of larger data sets. We have recently carried out an evaluation of one such system, BT-45 (Portet et al., 2007), which generates multi-paragraph summaries of clinical data from a Neonatal Intensive Care Unit (NICU). The summaries cover a period of roughly 45 minutes, and describe both sensor data (heart rate, blood oxygen saturation,"
W08-1119,W05-1615,1,0.787348,"erated short (paragraph-length or smaller) summaries of relatively small data sets (less than 1KB). Some research has been on systems that summarise larger data sets (Yu et al., 2007; Turner et al., 2008), but these systems have also generated paragraph-length summaries; we are not aware of any previous research on generating multi-paragraph summaries in a data-to-text system. Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al., 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al., 2005), and persuasive effectiveness (Carenini and Moore, 2006). However, again to the best of our knowledge no previous data-to-text system has been evaluated by asking users to make decisions based on the generated texts, and measuring the quality of these decisions. 2 BabyTalk and BT-45 Law et al. (2005) showed that human-written textual summaries were effective decision-support aids in NICU , but of course it is not practical to expect medical professionals to routinely write such summaries, especially considering that the summaries used by Law et al. in some cases took several hours to write. T"
W08-1119,W08-1104,1,0.812292,"which showed that medical professionals were more likely to make the correct treatment deci147 sion when shown a human-written textual summary of the data than when they were shown a graphical visualisation of the data. A number of data-to-text systems have been developed and indeed fielded, especially in the domain of weather forecasts (Goldberg et al., 1994; Reiter et al., 2005). Most of these systems have generated short (paragraph-length or smaller) summaries of relatively small data sets (less than 1KB). Some research has been on systems that summarise larger data sets (Yu et al., 2007; Turner et al., 2008), but these systems have also generated paragraph-length summaries; we are not aware of any previous research on generating multi-paragraph summaries in a data-to-text system. Data-to-texts systems have been evaluated in a number of ways, including human ratings (the most common technique) (Reiter et al., 2005), BLEU-like scores against human texts (Belz and Reiter, 2006), post-edit analyses (Sripada et al., 2005), and persuasive effectiveness (Carenini and Moore, 2006). However, again to the best of our knowledge no previous data-to-text system has been evaluated by asking users to make decis"
W09-0601,W07-2315,1,0.790502,"ewell, Langer, and Hickey (1998) suggest that NLG could be used to generate complete utterances from the limited input that AAC users are able to provide. For example, the Compansion project (McCoy, Pennington, Badman 1998) used NLP and NLG techniques to expand telegraphic user input, such as Mary go store?, into complete utterances, such as Did Mary go to the store? Netzer and Elhadad (2006) allowed users to author utterances in the symbolic language BLISS, and used NLG to translate this to English and Hebrew texts. In recent years there has been growing interest in data-to-text NLG systems (Reiter, 2007); these systems generate texts based on sensor and other numerical data, supplemented with ontologies that specify domain knowledge. In principle, it seems that data-to-text techniques should allow NLG systems to provide more assistance than the syntactic help provided by Compansion. For example, if the user wanted to talk about a recent football (soccer) match, a data-to-text system could get actual data about the match from the web, and generate potential utterances from this data, such as Arsenal beat Chelsea 2-1 and Van Persie scored two goals; the user could then select one of these to ut"
W09-0601,N06-2027,0,0.10986,"brary of fixed texts and templates. 2.2 NLG and AAC Natural language generation (NLG) systems generate texts in English and other human languages from non-linguistic input (Reiter and Dale, 2000). In their review of NLP and AAC, Newell, Langer, and Hickey (1998) suggest that NLG could be used to generate complete utterances from the limited input that AAC users are able to provide. For example, the Compansion project (McCoy, Pennington, Badman 1998) used NLP and NLG techniques to expand telegraphic user input, such as Mary go store?, into complete utterances, such as Did Mary go to the store? Netzer and Elhadad (2006) allowed users to author utterances in the symbolic language BLISS, and used NLG to translate this to English and Hebrew texts. In recent years there has been growing interest in data-to-text NLG systems (Reiter, 2007); these systems generate texts based on sensor and other numerical data, supplemented with ontologies that specify domain knowledge. In principle, it seems that data-to-text techniques should allow NLG systems to provide more assistance than the syntactic help provided by Compansion. For example, if the user wanted to talk about a recent football (soccer) match, a data-to-text sy"
W09-0601,W08-1119,1,\N,Missing
W09-0601,W07-2322,0,\N,Missing
W09-0607,E06-2020,1,0.936165,"ic descripIntroduction Disciplines such as environmental studies, geography, geology, planning and business marketing make extensive use of Geographical Information Systems (GIS); however, despite an explosion of available mapping software, GIS remains a specialist tool with specialist skills required to analyse and understand the information presented using map displays. Complementing such displays with textual summaries therefore provides an immediate niche for NLG systems. Recently, research into NLG systems that generate text from georeferenced data has begun to emerge (Dale et al., 2005; Turner et al., 2006; Turner et al., 2008b; Thomas and Sripada, 2008). These systems are required to textually describe the geographic distribution of domain variables such as road surface temperature and unemployment rates. For example, descriptions such as ’road surface temperatures will fall below zero in some places in the southwest’ and ’unemployment is highest in the rural areas’ need to be generated Proceedings of the 12th European Workshop on Natural Language Generation, pages 42–49, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 42 tions from knowledge acquisition (K"
W09-0607,W08-1104,1,0.651892,"es within a frame of reference. Because geographic concepts are inherently vague our approach does not guarantee a distinguishing description. Our evaluation studies show that NLG systems, because they can analyse input data exhaustively, can produce more fine-grained geographic descriptions that are more useful to end users than those generated by human experts. 1 • many geographic concepts are inherently vague (see for example (Varzi, 2001) for a discussion on this topic); • often the underlying data sets contain little explicit geographic information for a generation system to make use of (Turner et al., 2008b); • as input to a generation system, georeferenced data is often complex, constraints imposed on the output text (such as length) may make the traditional approach to the Referring Expression Generation (REG) problem in NLG of finding a distinguishing description implausible (Turner et al., 2008b). This paper looks at the problem in the context of work the authors have carried out on summarising georeferenced data sets in the meteorology domain. The main feature of our approach is that geographic descriptions perform the dual function of referring to a specific geographic locations unambiguo"
W09-0607,W05-1627,0,0.0235919,"and the early morning, turning freezing in some places above 400M. Background Much work on generation of spatial descriptions has concentrated on smaller scale spaces that are immediately perceivable. For example, spatial descriptions have been studied from the perspective of robot communication (Kelleher and Kruijff, 2006), 3D animation (Towns et al., 1998) and basic visual scenes (Viethen and Dale, 2008; Ebert et al., 1996). In a more geographical context route description generation systems such as (Dale et al., 2005) and (Moulin and Kettani, 1999) have had wide appeal to NLG researchers. (Varges, 2005) also generate landmark based spatial descriptions using maps from the map task dialogue corpus. RoadSafe is an NLG system that has been operationally deployed at Aerospace and Marine International (AMI) to produce weather forecast texts for winter road maintenance. It generates forecast texts describing various weather conditions on a road network as shown in Figure 1. The input to the system is a data set consisting of numerical weather predictions (NWP) calculated over a large set of point locations across a road network. An example static snapshot of the input to RoadSafe for one parameter"
W09-0607,P06-1131,0,0.0459532,"rning. Hoar frost will affect some southwestern and central routes by early morning. Road surface temperatures will fall slowly during the evening and tonight, reaching zero in some far southern and southwestern places by 21:00. Fog will affect some northeastern and southwestern routes during tonight and the early morning, turning freezing in some places above 400M. Background Much work on generation of spatial descriptions has concentrated on smaller scale spaces that are immediately perceivable. For example, spatial descriptions have been studied from the perspective of robot communication (Kelleher and Kruijff, 2006), 3D animation (Towns et al., 1998) and basic visual scenes (Viethen and Dale, 2008; Ebert et al., 1996). In a more geographical context route description generation systems such as (Dale et al., 2005) and (Moulin and Kettani, 1999) have had wide appeal to NLG researchers. (Varges, 2005) also generate landmark based spatial descriptions using maps from the map task dialogue corpus. RoadSafe is an NLG system that has been operationally deployed at Aerospace and Marine International (AMI) to produce weather forecast texts for winter road maintenance. It generates forecast texts describing variou"
W09-0607,W08-1115,1,0.819392,"environmental studies, geography, geology, planning and business marketing make extensive use of Geographical Information Systems (GIS); however, despite an explosion of available mapping software, GIS remains a specialist tool with specialist skills required to analyse and understand the information presented using map displays. Complementing such displays with textual summaries therefore provides an immediate niche for NLG systems. Recently, research into NLG systems that generate text from georeferenced data has begun to emerge (Dale et al., 2005; Turner et al., 2006; Turner et al., 2008b; Thomas and Sripada, 2008). These systems are required to textually describe the geographic distribution of domain variables such as road surface temperature and unemployment rates. For example, descriptions such as ’road surface temperatures will fall below zero in some places in the southwest’ and ’unemployment is highest in the rural areas’ need to be generated Proceedings of the 12th European Workshop on Natural Language Generation, pages 42–49, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 42 tions from knowledge acquisition (KA) studies we have carried out are discussed in §"
W09-0607,W06-1413,0,\N,Missing
W09-0607,W00-1416,0,\N,Missing
W09-0607,W08-1109,0,\N,Missing
W09-0607,J03-1003,0,\N,Missing
W09-0607,J02-1003,0,\N,Missing
W09-0607,P02-1013,0,\N,Missing
W09-0613,W96-0507,0,0.137301,"st interfaces to generate syntactic structures and linearise them. The library is also flexible in allowing the use of mixed (canned and noncanned) representations. 1 2. Since the tactical problem involves search through a space of linguistic choices, the broader the coverage, the more efficiency may be compromised. Where real-time deployment is a goal, this may be an obstacle. Introduction Over the past several years, a significant consensus has emerged over the definition of the realisation task, through the development of realisers such as R EAL P RO (Lavoie and Rambow, 1997), A LETH G EN (Coch, 1996), KPML (Bateman, 1997), FUF / SURGE (Elhadad and Robin, 1996), H ALO GEN (Langkilde, 2000), YAG (McRoy et al., 2000), and O PEN CCG (White, 2006). Realisation involves two logically distinguishable tasks. Tactical generation involves making appropriate linguistic choices given the semantic input. However, once tactical decisions have been taken, building a syntactic representation, applying the right morphological operations, and linearising the sentence as a string are comparatively mechanical tasks. With the possible exception of template-based realisers, such as YAG, existing wide-coverage"
W09-0613,W96-0501,0,0.0303404,"nd linearise them. The library is also flexible in allowing the use of mixed (canned and noncanned) representations. 1 2. Since the tactical problem involves search through a space of linguistic choices, the broader the coverage, the more efficiency may be compromised. Where real-time deployment is a goal, this may be an obstacle. Introduction Over the past several years, a significant consensus has emerged over the definition of the realisation task, through the development of realisers such as R EAL P RO (Lavoie and Rambow, 1997), A LETH G EN (Coch, 1996), KPML (Bateman, 1997), FUF / SURGE (Elhadad and Robin, 1996), H ALO GEN (Langkilde, 2000), YAG (McRoy et al., 2000), and O PEN CCG (White, 2006). Realisation involves two logically distinguishable tasks. Tactical generation involves making appropriate linguistic choices given the semantic input. However, once tactical decisions have been taken, building a syntactic representation, applying the right morphological operations, and linearising the sentence as a string are comparatively mechanical tasks. With the possible exception of template-based realisers, such as YAG, existing wide-coverage realisers usually carry out both tasks. By contrast, a realis"
W09-0613,W07-2315,1,0.599809,"patient information (Portet et al., to appear) differs from standard usage, and does not always allow variation to the same extent. Since realisers don’t typically address such requirements, their use in a particular application may require the alteration of the realiser’s rule-base or, in the case of statistical realisers, re-training on large volumes of appropruately annotated data. This paper describes SimpleNLG, a realisation engine which grew out of recent experiences in building large-scale data-to-text NLG systems, whose goal is to summarise large volumes of numeric and symbolic data (Reiter, 2007). Sublanguage requirements and efficiency are important considerations in such systems. Although meeting these requirements was the initial motivation behind SimpleNLG, it has since been developed into an engine with significant coverage of English syntax and morphology, while at the same time providing a simple API that offers users direct programmatic control over the realisation process. Proceedings of the 12th European Workshop on Natural Language Generation, pages 90–93, c Athens, Greece, 30 – 31 March 2009. 2009 Association for Computational Linguistics 90 lexical phrasal Feature A DJ P"
W09-0613,A00-2023,0,\N,Missing
W09-0613,W00-1437,0,\N,Missing
W09-0613,W06-1403,0,\N,Missing
W09-0613,A97-1039,0,\N,Missing
W10-1301,W08-1120,0,0.0278155,"ies of sensor and other numerical data. The most popular application area has been weather forecasting (generating textual weather forecasts from the results of a numerical atmosphere simulation model), and indeed several weather forecast generators have been fielded and used operationally (Goldberg, Driedger et al. 1994; Reiter, Sripada et al. 2005). A number of data-to-text systems have also been developed in the medical community, such as BabyTalk (Gatt, Portet et al. 2009), which generates summaries of clinical data from a neonatal intensive care unit, and the commercial Narrative Engine (Harris 2008) which summarizes data acquired during a doctor/patient encounter. Most previous research in data-to-text has focused on summarizing technical data for expert users, with the goal of effectively communicating key information. In our work, in contrast, the focus is on summarizing data about everyday events, with the goal of having something interesting to talk about. There has been considerable work in the computational creativity community on generating interesting stories (Péréz and Sharples 2004), but it has focused on fictional written stories, where the computer system can say whatever it"
W10-1301,W07-2315,1,0.819946,"CCN. 2.3 NLG, Data-to-text NLG systems generate texts in English (or other human languages) from non-linguistic data (Reiter and Dale 2000). Our vision is to use an NLG system to generate a draft story, which the child can edit. The nonlinguistic input to our story-generator is sensor data about the child’s activities, including location data (where the child was) and interaction data (what people and objects the child interacted with). We also want to allow teachers and school staff to enter information about the child’s activities (such as voice messages). A number of data-to-text systems (Reiter 2007) have been developed in recent years, which generate English summaries of sensor and other numerical data. The most popular application area has been weather forecasting (generating textual weather forecasts from the results of a numerical atmosphere simulation model), and indeed several weather forecast generators have been fielded and used operationally (Goldberg, Driedger et al. 1994; Reiter, Sripada et al. 2005). A number of data-to-text systems have also been developed in the medical community, such as BabyTalk (Gatt, Portet et al. 2009), which generates summaries of clinical data from a"
W10-1301,W09-0601,1,\N,Missing
W10-1302,N06-2027,0,0.19947,"of NLP in AAC NLP technology has provided many benefits to AAC system designers. Possibly the first technology to be included in many commercial systems to date was word prediction and completion. There have also been many research prototypes exploring the applicability of more emerging technologies such as named entity recognition from synthesized speech (Wisenburn and Higginbotham 2008), the generation of well-formed utterances from 11 telegraphic input (McCoy, Pennington et al. 1998) and the automatic identification of contextual vocabulary from the web (Higginbotham, Bisantz et al. 2008). Netzer and Elhadad (2006) used NLG to allow the semantic authoring of utterances. However, NLG, in the sense of data-to-text (Reiter and Dale 2000), has had limited application within AAC thus far, although Reiter et al. (2009) showed it is possible to generate stories from sensor data which allow a child using AAC to tell others about their day at school. 2.3 System Rationale This project is exploring the use of NLG to produce conversational utterances in AAC systems designed for social interaction. At the outset it was hoped that using NLG might address some of the difficulties observed in pre-storage systems. For i"
W10-1302,W09-0601,1,0.887603,"Missing"
W10-1302,J05-1002,0,\N,Missing
W10-4210,W08-1107,0,0.0536568,"gly little research on how people refer to objects in a real-world setting. This paper addresses this issue, and we begin formulating the requirements for an REG algorithm that refers to visible three-dimensional objects in the real world. Reference to objects in a visual domain provides a straightforward extension of the sorts of reference REG research already tends to consider. Toy examples outline reference to objects, people, and animals that are perceptually available before the speaker begins generating an utterance (Dale and Reiter, 1995; Krahmer et al., 2003; van Deemter et al., 2006; Areces et al., 2008). Example referents may be referred to by their color, size, type (“dog” or “cup”), whether or not they have a beard, etc. Typically, the reference process proceeds by comparing the properties of the referent with the properties of all the other items in the set. The final expression roughly conforms to the Gricean maxims (Grice, 1975). However, when the goal is to generate natural reference, this framework is too simple. The form reference takes is profoundly affected by modality, task, and audience (Chapanis et al., 1977; Cohen, 1984; Clark and Wilkes-Gibbs, 1986), and even when these aspect"
W10-4210,E06-1041,0,0.0178676,"of object prototypes. 4.1 Spatial and Visual Properties It is perhaps unsurprising to find reference that exhibits spatial knowledge in a study where objects are presented in three-dimensional space. Human behavior is anchored in space, and spatial information is essential for our ability to navigate the world we live in. However, referring expression generation algorithms geared towards spatial representations have oversimplified this tendency, keeping objects within the realm of twodimensions and only looking at the spatial relations between objects. For example, Funakoshi et al. (2004) and Gatt (2006) focus on how objects should be clustered together to form groups. This utilizes some of the spatial information between objects, but does not address the spatial, three-dimensional nature of objects themselves. Rather, objects exist as entities that may be grouped with other entities in a set or singled out as individual objects; they do not have their own spatial characteristics. Similarly, one of the strengths of the Graph-Based Algorithm (Krahmer et al., 2003) is its ability to generate expressions that involve relations between objects, and these include spatial ones (“next to”, “on top o"
W10-4210,J95-3003,0,0.072875,"hin language, and language is often a joint activity. However, most research on referring expression generation supposes a solitary generating agent.1 This tacitly assumes that reference will be taking place in a monologue setting, rather than a dialogue or group setting. Indeed, the goal of most REG algorithms is to produce uniquely distinguishing, one-shot referring expressions. Studies on natural reference usually use a two person (speaker-listener) communication task (e.g., Flavell et al., 1968; Krauss and Glucksberg, 1969; Ford and Olson, 1975). This research has 1 A notable exception is Heeman and Hirst (1995). shown that reference is more accurate and efficient when it incorporates things like gesture and gaze (Clark and Krych, 2004). There is a trade-off in effort between initiating a noun phrase and refashioning it so that both speakers understand the referent (Clark and Wilkes-Gibbs, 1986), and speakers communicate to form lexical pacts on how to refer to an object (Sacks and Schegloff, 1979; Brennan and Clark, 1996). Mutual understanding of referents is achieved in part by referring within a subset of potential referents (Clark et al., 1983; Beun and Cremers, 1998). A few studies have compared"
W10-4210,J84-2002,0,0.436048,"; Krahmer et al., 2003; van Deemter et al., 2006; Areces et al., 2008). Example referents may be referred to by their color, size, type (“dog” or “cup”), whether or not they have a beard, etc. Typically, the reference process proceeds by comparing the properties of the referent with the properties of all the other items in the set. The final expression roughly conforms to the Gricean maxims (Grice, 1975). However, when the goal is to generate natural reference, this framework is too simple. The form reference takes is profoundly affected by modality, task, and audience (Chapanis et al., 1977; Cohen, 1984; Clark and Wilkes-Gibbs, 1986), and even when these aspects are controlled, different people will refer differently to the same object (Mitchell, 2008). In light of this, we isolate one kind of natural reference and begin building the algorithmic framework necessary to generate the observed language. Psycholinguistic research has examined reference in a variety of settings, which may inform research on natural REG, but it is not always clear how to extend this work to a computational model. This is true in part because these studies favor an analysis of reference in the context of collaborati"
W10-4210,J03-1003,0,0.0598886,"Missing"
W10-4210,W06-1420,1,0.41927,"Missing"
W10-4210,W08-1109,0,\N,Missing
W11-2704,P05-1007,0,0.0769164,"Missing"
W11-2803,P07-1063,0,0.0534255,"). One approach proposed by de Rosis and Grasso (2000) was to introduce models at the sentence planning stage that adapts the message for the intended recipient’s communicative goal and also employs rule-based heuristics for the usage of empathy in the resultant text. Other ANLG systems have used emotional or physiological models to define the type of affective text generation. For example, the PERSONAGE system, whilst strictly not dealing with emotion, has shown that by using the ‘Big Five’ personality traits model it is possible to generate tailored output for particular personality traits (Mairesse and Walker, 2007). A review of past work in ANLG by Belz (2003) concluded that the research in ANLG has not yet been successful in making the connection between emotion and NLG. Empirical testing of ANLG systems can also pose many challenges as well, with very few past systems being tested. Work by van der Sluis and Mellish (2009) on measuring the emotions of recipients when given positively slanted texts has recently shown it is possible to measure the emotional effect. However, the overall lack of empirical testing from past ANLG systems makes it hard to determine the effectiveness of previous ANLG implement"
W11-2803,W00-1429,1,0.790333,"Missing"
W11-2803,W07-2315,1,0.671419,"to query for the existence of particular factual details about the baby’s records to help determine the score for each of the thirteen separate stress factors. This score is stored and made accessible to all other components of the BT-Family system. However, the PNSS score could be calculated by other means or even directly entered by a parent or medical staff; this would not affect the rest of the system. 4 Current problems Implementing ANLG in BT-Family The ANLG architecture (illustrated in Figure 1) used in BT-Family is an extension of the NLG data-totext architecture that was proposed by Reiter (2007), in which natural language text is generated from a non-linguistic data source. Most of the core parts of this system are based upon the BT-Nurse system, in which there are six core components: Badger EMR Database, BabyTalk Ontology, Signal Analysis, Data Interpretation, Document Planner, and Microplanner & Realisation. A detailed explanation of how these core modules function can be found in Gatt et al., (2009) and Mahamood (2010). The system presented in this section is built upon the BT-Nurse system, but there are crucial differences between these two systems that make both unique from eac"
W11-2803,W09-0625,0,0.0530657,"Missing"
W11-2804,P02-1040,0,0.100706,"Missing"
W11-2804,J09-4008,1,0.825783,"performance, human opinions on Likert-like scales, and/or similarity to a goldstandard corpus. While such evaluations are essential, we believe there is also a role for qualitative evaluations, especially when the goal of the evaluation is formative that is, assessing weaknesses and identifying how the NLG system could be improved. In this paper we describe how we used two qualitative methodologies, content analysis and discourse analysis, to evaluate texts produced by Background 2.1 Evaluation in NLG The great majority of published evaluations of NLG systems are quantitative: as described by Reiter and Belz (2009), they either measure the impact of a generated text on task performance, ask human subjects to rate generated texts on a Likert-like scale, or compare the similarity of generated texts to corpus texts using automatic metrics such as BLEU (Papineni et al., 2002). Reiter and Belz point out that many human-based quantitative NLG evaluations also solicit free-text comments from their subjects, and these are very helpful in diagnosing and fixing problems in generated texts. Soliciting such comments, however is usually a secondary goal of evaluations of NLG systems, the primary goal being quantitat"
W11-2804,W08-1119,1,\N,Missing
W11-2808,W08-1108,0,0.25444,"put and output of these systems are a novel characterization of the factors that affect referring expression generation, and the methods described here may serve as one building block in future work connecting vision to language. 1 Introduction The task of referring expression generation (REG) has often been contextualized as a problem of generating uniquely identifying reference to visible items. Properties such as COLOR, SIZE, LOCATION, and ORIENTATION have been treated as exemplars of attributes used to distinguish a referent (Dale and Reiter, 1995; Krahmer et al., 2003; van Deemter, 2006; Gatt and Belz, 2008). This paper is no exception. However, we approach the task of REG by examining in depth what it means to uniquely identify something that is visible. We specifically address the attribute of size and explore ways to connect the dimensional properties of real-world objects to surface forms used by people to pick out a referent. This work contributes to recent research examining naturalistic reference in visual domains explicitly (Kelleher et al., 2005; Viethen and Dale, 2010; Koolen et al., 2011). Traditionally, to create an algorithm for the generation of reference, one considers a set of dif"
W11-2808,J03-1003,0,0.358267,"Missing"
W11-2808,W10-4210,1,0.891997,"Missing"
W11-2808,J06-2002,1,0.917165,"Missing"
W11-2808,U10-1013,0,0.1766,"as exemplars of attributes used to distinguish a referent (Dale and Reiter, 1995; Krahmer et al., 2003; van Deemter, 2006; Gatt and Belz, 2008). This paper is no exception. However, we approach the task of REG by examining in depth what it means to uniquely identify something that is visible. We specifically address the attribute of size and explore ways to connect the dimensional properties of real-world objects to surface forms used by people to pick out a referent. This work contributes to recent research examining naturalistic reference in visual domains explicitly (Kelleher et al., 2005; Viethen and Dale, 2010; Koolen et al., 2011). Traditionally, to create an algorithm for the generation of reference, one considers a set of different properties and develops ways to decide which properties to include in a final surface string. This may be considered a breadth-based methodology, where many properties are considered, but the details of how those properties are input to the algorithm is left unspecified. Here, we begin creating an algorithm for the generation of naturalistic reference by considering a single property – size – and tracing how it is realized based on a variety of different inputs and ou"
W12-1516,W11-2803,1,0.888421,"dard” corpus of domain texts exists. Indeed, in many cases domain experts may find it difficult to give detailed requirements and knowledge until they can see a version of the NLG system working on concrete examples. This suggests that an iterative software development methodology should be used, where domain experts repeatedly try out an NLG system, revise underlying domain (communication) knowledge and request changes to the system’s functionality, and wait for developers to implement these changes before repeating the process. We describe how we carried out this process on BabyTalk-Family (Mahamood and Reiter, 2011), an NLG system which generates summaries of clinical data about a baby in a neonatal intensive care unit (NICU), for the babys parents. Over a 6 month period, this process enabled us to improve an initial version of the system (essentially the result of a PhD) to the point where the system was good enough to be deployable live in a hospital context. We also describe how the feedback from the clinicians changed over the course of this period. 2 Previous Research Reiter et al. (2003) describe a knowledge acquisition strategy for building NLG systems which includes 4 stages: directly asking doma"
W13-2119,bouayad-agha-etal-2002-pills,0,0.0465122,"olves the description of the “treatment and findings”, which describes the events that happen whilst the patient is being cared for and relevant parts of the sensor data (see Figure 1). For this section of the report, the document planning algorithm is based on that of (Portet et al., 2007), which identifies a number of key events and creates a paragraph for each key event. Events that are explicitly linked to the key event or events that happen at the same time are added to the relevant paragraph. This is based on the earlier work of (Hallett et al., 2006). STOP (Reiter et al., 2003), PILLS (Bouayad-Agha et al., 2002), MIGRANE (Buchanan et al., 1992), and Healthdoc (Hirst et al., 1997). Other systems, such as TOPAZ (Kahn et al., 1991) and Suregen (H¨uske-Kraus, 2003), aim to summarise information in order to support medical decision-making. In the case of MIME, the challenge is to summarise large amounts of sensor data, in the context of carer observations and actions, in a coherent way that supports quick decision making by the reader. The problem of describing the data relates to previous work on summarising time series data (e.g. (Yu et al., 2007)). In many ways, though, our problem is most similar to t"
W13-2119,E03-2008,0,0.0946256,"Missing"
W14-4419,W08-1103,0,0.0311747,"and Gao (2007) conducted a small study where they showed scuba divers different possible outputs from their ScubaText system, including text-only, graph-only and annotated graphs. They found that divers preferred the annotated graph presentation. The ScubaText software could not in practice produce annotated graphs for arbitrary input data sets and automatically set the scale based on detected events, so this was primarily a study of user preferences. McCoy and colleagues have been developing techniques to automatically generate textual summaries of data graphics for visually impaired users (Demier et al., 2008). This differs from our work because their goal is to replace the graph, whereas our goal is to generate an integrated text/graphics presentation. There were several early systems in the 1990s (Wahlster et al., 1993; Feiner and McKeown, 1990, for example), which generated integrated presentations of figures and texts, but these systems focused on annotating static pictures and diagrams, not data graphics. The WIP system, which combined static diagram images and text, used a deep planning approach to produce tightly integrated multimodal documents; it is not clear how robustly this approach han"
W14-4419,W08-1119,1,\N,Missing
W15-4723,P08-2050,0,0.0765701,"he ContentSelection algorithm in detail (p. 122), so below we highlight its main steps: 4.1 A small-scaled quantitative evaluation To test how the algorithm currently performs, we ran it using 7 weather forecast datasets provided by the UK’s meteorology agency: MetOffice. The data contained numerical predictions for a region in the UK (Grampian), and each dataset also accompanies a textual summary, against which we used to compare our algorithm. We chose DICE to evaluate how comparable each output was. This metrics has been widely used by the Referring Expression community (Gatt et al., 2008; Belz and Gatt, 2008). The results are displayed in Table 2. To compare MetOffice’s FoR choices with those by our algorithm, we ran it using 6 different density thresholds: 0.0, 0.2, 0.4, 0.6, 0.8 and 1.0. A density threshold is in this sense the minimum event density a descriptor can have to be accepted as a candidate. If you recall the explanation of the algorithm above, a Frame of Reference is rejected if all its descriptors are rejected, but equally if all its descriptors cannot be rejected. For example, it only makes sense to select Inland as a descriptor if Coastal is not a candidate; if both Inland and Coas"
W15-4723,W08-1131,0,0.0271944,"Turner describes the ContentSelection algorithm in detail (p. 122), so below we highlight its main steps: 4.1 A small-scaled quantitative evaluation To test how the algorithm currently performs, we ran it using 7 weather forecast datasets provided by the UK’s meteorology agency: MetOffice. The data contained numerical predictions for a region in the UK (Grampian), and each dataset also accompanies a textual summary, against which we used to compare our algorithm. We chose DICE to evaluate how comparable each output was. This metrics has been widely used by the Referring Expression community (Gatt et al., 2008; Belz and Gatt, 2008). The results are displayed in Table 2. To compare MetOffice’s FoR choices with those by our algorithm, we ran it using 6 different density thresholds: 0.0, 0.2, 0.4, 0.6, 0.8 and 1.0. A density threshold is in this sense the minimum event density a descriptor can have to be accepted as a candidate. If you recall the explanation of the algorithm above, a Frame of Reference is rejected if all its descriptors are rejected, but equally if all its descriptors cannot be rejected. For example, it only makes sense to select Inland as a descriptor if Coastal is not a candidate; i"
W15-4723,P06-1131,0,0.3495,"Missing"
W15-4723,J12-1006,0,0.288328,"Missing"
W15-4723,J02-1003,0,0.374887,"Missing"
W15-4723,W09-0629,0,\N,Missing
W15-4723,W09-0607,1,\N,Missing
W15-4726,W07-2315,1,0.717634,"of all incidents, we use an agglomerative clustering algorithm, where the distance between two incidents is defined by the weighted similarity of all above mentioned features. The algorithm also has a minimal cluster size, which is influenced by the total number of incidents, and a maximum distance, which are used to decide, when to stop the agglomeration and which clusters are irrelevant. In this way we try to balance the interest between greatest possible and tightest possible clus3.3 NLG The Data-2-Text module of our prototype follows the three-stage pipelined architecture, as described by Reiter (2007), and uses simpleNLG (Gatt and Reiter, 2009) as surface realiser. 3.3.1 Psychological Background Since we try to achieve a behaviour change, we use different psychological techniques for the verbalisation of feedback, which have been shown to be useful in the literature (cf. Section 2.1) to maximize the likelihood of achieving this goal. This is reflected particularly in the document plan, which follows mainly the three techniques described in Section 2.2. Another psychological aspect was already taken into account during the specification of relevant behaviour. We try to avoid unnecessary fru"
W15-4726,W09-0613,1,0.883422,"rative clustering algorithm, where the distance between two incidents is defined by the weighted similarity of all above mentioned features. The algorithm also has a minimal cluster size, which is influenced by the total number of incidents, and a maximum distance, which are used to decide, when to stop the agglomeration and which clusters are irrelevant. In this way we try to balance the interest between greatest possible and tightest possible clus3.3 NLG The Data-2-Text module of our prototype follows the three-stage pipelined architecture, as described by Reiter (2007), and uses simpleNLG (Gatt and Reiter, 2009) as surface realiser. 3.3.1 Psychological Background Since we try to achieve a behaviour change, we use different psychological techniques for the verbalisation of feedback, which have been shown to be useful in the literature (cf. Section 2.1) to maximize the likelihood of achieving this goal. This is reflected particularly in the document plan, which follows mainly the three techniques described in Section 2.2. Another psychological aspect was already taken into account during the specification of relevant behaviour. We try to avoid unnecessary frustration by only reporting behaviour that ca"
W15-4726,W13-2115,0,0.0306267,"g spatio-temporal data in other domains (Turner et al., 2008; Ponnamperuma et al., 2013). Hattie and Timperley (2007) pointed out, that “specific goals are more effective than general or nonspecific ones” (emphasis added). Ye and Johnson (1995), Teach and Shortliffe (1987), Weiner (1980) and many others pointed out, that it is crucial for the acceptance of feedback from computer systems, that the feedback is justified in a way that allows the user to reconstruct how conclusions were drawn. 2 2.3 Related Work NLG systems that generate feedback have proven to be helpful in many different areas. Gkatzia et al. (2013) for example showed that an NLG system can provide students with feedback that is perceived as helpful as feedback from lecturers, using reinforcement learning. The SkillSum system (Williams and Reiter, 2008), which generates feedback about basic reading skills and performed significantly better than a comparable system that used canned texts. In the context of citizen science, automatically generated feedback has been shown to improve both skill levels and motivation levels among participants (Blake et al., 2012; van der Wal et al., 2016). As Eugenio et al. (2005) have shown, aggregation is o"
W15-4726,P13-4029,1,0.828847,"Missing"
W15-4726,C12-1020,1,\N,Missing
W15-4726,P05-1007,0,\N,Missing
W16-6643,P08-2050,0,0.0323925,"gorithm, and subsequently used a Gold Standard from M2 to test its performance. For each phase, we ran the algorithm with 3 distinct combinations of spatial operations: a) no operation, so only absolute descriptions such as (COASTAL) and non-specific directions such as (NORTH) were generated; b) mereology only, where 262 mereological descriptions such as (NORTH, MORAY) were generated in addition to the ones above; c) both mereology and intersection, where the most complex descriptions such as (COASTAL u (NORTH u MORAY)) were also generated. The evaluation method was intrinsic, as described by Belz and Gatt (2008), whereby we computed the similarity between corpus descriptions and the output of the algorithm using the DICE coefficient of similarity. The Gold Standard testbeds excluded descriptions with direction modifiers such as far and central, because the current algorithm does not have an implementation for these concepts. The Gold Standard from M2 contained 44 entries, and that from M2, 36. 0.7 0.6 0.6 0.5 0.5 0.4 0.4 0.3 0.3 0 20 40 60 80 100 (a) Training scores (M1). 0 20 40 60 80 100 (b) Test scores (M2). Figure 4: DICE similarity scores when running the algorithm against both sub-corpora (M1 a"
W16-6643,W15-4723,1,0.747831,"Missing"
W16-6643,P06-1131,0,0.0451432,"y (Reiter, 2007), which automatically write reports in natural language such as English, given structured data such as those we typically store in databases. Our domain is weather forecast and our input data conforms with that typically found in Geographic Information Systems (Worboys and Duckham, 2004). The many algorithms for doing Referring Expression Generation (REG) as outlined in Krahmer and Van Deemter (2012) assume that Knowledge Bases (KBs) exhaustively specify all properties that are inherent (i.e. absolute) to entities. The REG style we propose here is inspired in alternative work (Kelleher and Kruijff, 2006; Viethen and Dale, 2008) that computes relational properties, rather than storing them in KBs. We base our approach on evidence observed in human-authored texts, as it shall be explained in Section 4. The underlying philosophy is that some properties are absolute, i.e. inherent to entities, while some properties are relative to other properties. An example of the relative type of properties in the spatial domain is the part-whole relation, henceforth mereology (Cohn and Renz, 2008, 577). For example, a given city will absolutely be a part of a country (or continent) or not, so the properties"
W16-6643,C08-1055,0,0.0555592,"Missing"
W16-6643,J12-1006,0,0.0572796,"Missing"
W16-6643,J09-4008,1,0.819488,"ithm employs 2 spatial operations – intersection and mereology – when processing point-based data. We described the compilation of a data-and263 text aligned corpus, which we used as a testbed to guide development and to test the final system. We have shown that employing spatial operations makes the machine-generated output more similar to the human-generated descriptions. We increased the overall average of similarity between the computer output and human descriptions from a 0.38 (DICE), when no operations are used, to a score of 0.66, when computing mereology and intersection. In line with Reiter and Belz (2009), we believe that our metrics-based evaluation was valuable but only a ‘development-stage’ guidance. A task-based evaluation shall be more revealing of the algorithm’s performance. Thus, our next study will evaluate how well users accomplish a task given the descriptions generated by our algorithm. Nonetheless we are convinced that spatial operations are employed by humans when producing descriptions, which makes the algorithm described here to be more human-like than previous approaches. Above all, our results show that relative properties are paramount when generating referring expressions i"
W16-6643,W07-2315,1,0.667139,"ion of the algorithm suggests that part-whole relations are key in geographic expressions. 1 Introduction This paper discusses the role of spatial operations in ‘creating’ properties to be used for generating geographic expressions. For example, we generate the expression “northern France” by retrieving the property FRANCE from our knowledge base, and subsequently computing (or creating) the property NORTH at run-time. The algorithm we describe in this article is meant to be used by Natural Language Generation (NLG) systems (Reiter and Dale, 2000), especially those in the Data-to-Text family (Reiter, 2007), which automatically write reports in natural language such as English, given structured data such as those we typically store in databases. Our domain is weather forecast and our input data conforms with that typically found in Geographic Information Systems (Worboys and Duckham, 2004). The many algorithms for doing Referring Expression Generation (REG) as outlined in Krahmer and Van Deemter (2012) assume that Knowledge Bases (KBs) exhaustively specify all properties that are inherent (i.e. absolute) to entities. The REG style we propose here is inspired in alternative work (Kelleher and Kru"
W16-6643,J02-1003,0,0.110607,"Missing"
W16-6643,W08-1109,0,0.0297895,"matically write reports in natural language such as English, given structured data such as those we typically store in databases. Our domain is weather forecast and our input data conforms with that typically found in Geographic Information Systems (Worboys and Duckham, 2004). The many algorithms for doing Referring Expression Generation (REG) as outlined in Krahmer and Van Deemter (2012) assume that Knowledge Bases (KBs) exhaustively specify all properties that are inherent (i.e. absolute) to entities. The REG style we propose here is inspired in alternative work (Kelleher and Kruijff, 2006; Viethen and Dale, 2008) that computes relational properties, rather than storing them in KBs. We base our approach on evidence observed in human-authored texts, as it shall be explained in Section 4. The underlying philosophy is that some properties are absolute, i.e. inherent to entities, while some properties are relative to other properties. An example of the relative type of properties in the spatial domain is the part-whole relation, henceforth mereology (Cohn and Renz, 2008, 577). For example, a given city will absolutely be a part of a country (or continent) or not, so the properties COUNTRY and CONTINENT are"
W16-6643,W09-0607,1,\N,Missing
W17-3519,W16-6615,0,0.0149109,"ria’s perspective is research on specific types of reference which are common in the domains Arria works in, focusing on algorithms which are sensitive to linguistic and discourse context, configurable, usable in hybrid systems which include some canned text, and which support variation. There are definitely encouraging signs, for example the recent resurgence of interest in contextually appropriate named entity reference (e.g., Belz and Kow 2010, van Deemter 2016b), although this has mostly focused on people rather than companies. It is also encouraging to see recent work on variation (e.g., Baltaretu and Ferreira 2016) and on configuring reference for different genres and domains (e.g. Koolen et al, 2012). Of course NLG researchers do not need to focus on Arria’s needs. But there are many interesting research issues in specific types of reference, variation, etc. Also human speakers arguably use different reference strategies for different types of entities, vary reference strategies depending on domain and genre, insert referring expressions into fixed (formulaic) language, and vary reference in order to keep text interesting. Investigating these issues could lead to important insights about language and r"
W17-3519,W16-6605,0,0.0251857,"Missing"
W17-3519,W08-1108,0,0.0556088,"Missing"
W17-3519,J12-1006,0,0.0679764,"Missing"
W17-3519,N13-1137,1,0.908614,"Missing"
W17-3519,W15-4723,1,0.908518,"Missing"
W17-3519,W07-2315,1,0.729827,"Missing"
W17-3535,W09-0613,1,0.850932,"Missing"
W18-6544,W09-0613,1,0.810123,"ID, HINDER, REQUIRE and PRECLUDE. These relationships can be seen on Figure 2. The experiment presented the below task descriptions to participants. Figure 3: Input for text in Figure 1. 5.2.2 Experiments • Hillwalking - Walking in the hills during every season except for winter. There may be some rough ground and it may be wet. • Trail Walking - Walking on forest paths or other well kept trails. Usually in warmer weather although there may be some light rain. Realization In order to keep the system as simple as possible just four relations were used. Realization is performed using SimpleNLG (Gatt and Reiter, 2009). The realizer functions themselves use helper functions which construct commonly occurring patterns of text. An introductory sentence (labeled M0 in Figure 1) is included at the beginning of the output text detailing the product and its components. This is the only fixed ordering rule. The conjunction of components is realized in the order that the components would otherwise first be mentioned. • AID - Attribute aids in the completion of the task, but is not essential. This is realized as ‘is good for’ or ‘suits’. • HINDER - Attribute hinders the completion of the task, but not to the point w"
W18-6544,J95-2003,0,0.808272,"Missing"
W18-6544,J06-4002,0,0.0577411,"al nodes to this network which are not present in the text. These can be thought of as inferences based on the context of the propositions. Kintsch calls these ‘knowledge elaborations’ and relations between them and the proposition nodes from the text are added to form a complex highly interconnected network in the readers mental model. them across domains. They also require much manual work and each relation needs to be defined by annotators who often do not agree even within a domain. Machine Learning has been investigated as a method for both Content Determination and Document Structuring (Lapata, 2006), (Barzilay and Lee, 2004), (Liang et al., 2009). Such models rely on existing corpora within the domain and often paired data-corpus. If the domain is changed, or if there is disagreement as to what the correct corpus should be within a domain then such approaches experience difficulty. 3 The Construction-Integration Model RST and Schemata focus on the relations between messages. Focus mechanisms can be used to check if different messages contain identical subjects or objects, utilizing this information when ordering messages. Focus (Sidner, 1979), Centering Theory (Grosz et al., 1995), (Poes"
W18-6544,P09-1011,0,0.0447002,"sent in the text. These can be thought of as inferences based on the context of the propositions. Kintsch calls these ‘knowledge elaborations’ and relations between them and the proposition nodes from the text are added to form a complex highly interconnected network in the readers mental model. them across domains. They also require much manual work and each relation needs to be defined by annotators who often do not agree even within a domain. Machine Learning has been investigated as a method for both Content Determination and Document Structuring (Lapata, 2006), (Barzilay and Lee, 2004), (Liang et al., 2009). Such models rely on existing corpora within the domain and often paired data-corpus. If the domain is changed, or if there is disagreement as to what the correct corpus should be within a domain then such approaches experience difficulty. 3 The Construction-Integration Model RST and Schemata focus on the relations between messages. Focus mechanisms can be used to check if different messages contain identical subjects or objects, utilizing this information when ordering messages. Focus (Sidner, 1979), Centering Theory (Grosz et al., 1995), (Poesio et al., 2004) and Scripts (Schank and Abelson"
W18-6544,Q17-1022,0,0.0225614,"Missing"
W18-6544,D14-1162,0,0.0814861,"ach. 5 5.1 Vector Space Model The Vector Space Model (VSM) was created using the Python Gensim implementation of Word2Vec. The corpus was stripped of all characters which were not within the alphabet for the given language. The corpus was lemmatized (using spaCy). The VSM is trained on English Wikipedia using Word2Vec. The training settings were skip-gram with 600 dimensions, a window of 5, negative sampling of 5 and all words with a lower total frequency than 5 were discarded. Whilst Word2Vec has been used as a starting point, it is possible that models generated using systems such as GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) would improve an NLG system which relies upon distributional semantics. Vector Space Specialisation (Mrksic et al., 2017) may also be useful. 5.1.1 System Input Figure 2 shows nodes and relations from the graph database (Neo4j) for a Product which consists of Components, with each Component being made of Materials. All Products, Components and Materials (collectively Items) may have Attributes which have suitabilities for different Tasks. Items may also have a direct suitability for a Task. The system will describe the suitability of a product for a task. The in"
W18-6544,N04-1015,0,0.0688666,"network which are not present in the text. These can be thought of as inferences based on the context of the propositions. Kintsch calls these ‘knowledge elaborations’ and relations between them and the proposition nodes from the text are added to form a complex highly interconnected network in the readers mental model. them across domains. They also require much manual work and each relation needs to be defined by annotators who often do not agree even within a domain. Machine Learning has been investigated as a method for both Content Determination and Document Structuring (Lapata, 2006), (Barzilay and Lee, 2004), (Liang et al., 2009). Such models rely on existing corpora within the domain and often paired data-corpus. If the domain is changed, or if there is disagreement as to what the correct corpus should be within a domain then such approaches experience difficulty. 3 The Construction-Integration Model RST and Schemata focus on the relations between messages. Focus mechanisms can be used to check if different messages contain identical subjects or objects, utilizing this information when ordering messages. Focus (Sidner, 1979), Centering Theory (Grosz et al., 1995), (Poesio et al., 2004) and Scrip"
W18-6544,N18-1202,0,0.0155289,"ector Space Model (VSM) was created using the Python Gensim implementation of Word2Vec. The corpus was stripped of all characters which were not within the alphabet for the given language. The corpus was lemmatized (using spaCy). The VSM is trained on English Wikipedia using Word2Vec. The training settings were skip-gram with 600 dimensions, a window of 5, negative sampling of 5 and all words with a lower total frequency than 5 were discarded. Whilst Word2Vec has been used as a starting point, it is possible that models generated using systems such as GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018) would improve an NLG system which relies upon distributional semantics. Vector Space Specialisation (Mrksic et al., 2017) may also be useful. 5.1.1 System Input Figure 2 shows nodes and relations from the graph database (Neo4j) for a Product which consists of Components, with each Component being made of Materials. All Products, Components and Materials (collectively Items) may have Attributes which have suitabilities for different Tasks. Items may also have a direct suitability for a Task. The system will describe the suitability of a product for a task. The input to the system is an unorder"
W18-6544,J04-3003,0,0.166395,"Missing"
W18-6548,W17-3535,1,0.890675,"Missing"
W18-6548,W17-3525,0,0.0277796,"n, 1996) of “Overview first, zoom and filter, then details-ondemand”. One of the main ideas presented there is that it is beneficial for a reader to be exposed to an overview of the information before diving into specific details of interest. There have been related NLG research about sets of objects, although with different goals or focuses. For example, to refer to or identify a set of objects within a larger set (Van Deemter, 2002), to perform a data-to-text analysis of tabularized data by records1 , to generate a page title for set items with shared characteristics from existing metadata (Mathur et al., 2017), or to address the issue of missing data encountered in summarisation (Inglis et al., 2017). In contrast, our work explores summaries that describe commonalities and differences within a set in order to help a user make informed decisions in selecting an object from the set. Our work focuses particularly on Content Determination step in the NLG pipe-line (Reiter and Dale, 2000), including selecting features and values to be presented. We explored the task of creating a textual summary describing a large set of objects characterised by a small number of features using an e-commerce dataset. Wh"
W18-6548,J02-1003,1,0.573446,"Missing"
W18-6551,W08-1104,1,0.845298,"Missing"
W18-6551,W17-5525,0,0.0638373,"Missing"
W18-6551,W15-4723,1,0.903448,"Missing"
W18-6551,W07-2315,1,0.567353,"of 7 descriptors (including cardinal points, coast, inland, and a proper name). Figure 2 shows a representation of the answers given by the students for “Northern Galicia” and a contour map that illustrates the percentages of overlapping answers. The second survey was addressed to meteorologists in the Galician Weather Agency (MeteoGalicia, 2018). Its purpose was to gather data to create Language grounding, i.e., understanding how words and expressions are anchored in data, is one of the initial tasks that are essential for the conception of a data-to-text (D2T) system (Roy and Reiter, 2005; Reiter, 2007). This can be achieved through different means, such as using heuristics or machine learning algorithms on an available parallel corpora of text and data (Novikova et al., 2017) to obtain a mapping between the expressions of interest and the underlying data (Reiter et al., 2005), getting experts to provide these mappings, or running surveys on writers or readers that provide enough data for the application of mapping algorithms (Ramos-Soto et al., 2017). Performing language grounding ensures that generated texts include words whose meaning is aligned with what writers understand or what reader"
W18-6551,W02-2113,1,0.63836,"Missing"
W18-6551,W09-0607,1,\N,Missing
W19-8402,P02-1040,0,0.106064,"luation Challenge: Can we get reliable estimates of scrutabilty, trust (etc) by simply asking users to read explanations and estimate scrutability (etc)? What experimental design (subjects, questions, etc) gives the best results? Do we need to first check explanations for accuracy before doing the above? Other challenges include creating good experimental designs for task-based evaluation, such as the study Biran and McKeown (2017) did to assess whether explanations improved financial decision making because of increased scrutability; and also exploring whether automatic metrics such as BLEU (Papineni et al., 2002) give meaningful insights about trust, scrutability, etc. • Helping developers debug their AI systems. This is not a common goal in NLG, but seems to be one of the most common goals in Explainable AI. The popular LIME model (Ribeiro et al., 2016), for example, is largely presented as a way of helping ML developers choose between models, and also improve models via feature engineering. 3 Appropriate Explanation for Audience A fundamental principle of NLG is that texts are produced for users, and hence should use appropriate content, terminology, etc for the intended audience (Paris, 2015; Walke"
W19-8402,W08-1119,1,0.603896,"Missing"
W19-8402,W15-4708,0,0.0310789,"ent on the texts in various ways. This is leads to my first challenge A core principle of NLG is that generated texts have a communicative goal. That is, they have a purpose such as helping users make decisions (perhaps the most common goal), encouraging users to change their behaviour, or entertaining users. Evaluations of NLG systems are based on how well they achieve these goals, as well as the accuracy and fluency of generated texts. Typically we either directly measure success in achieving the goal or we ask human subjects how effective they think the texts will be at achieving the goal (Gkatzia and Mahamood, 2015). Real-world explanations of AI systems similarly have purposes, which include • Evaluation Challenge: Can we get reliable estimates of scrutabilty, trust (etc) by simply asking users to read explanations and estimate scrutability (etc)? What experimental design (subjects, questions, etc) gives the best results? Do we need to first check explanations for accuracy before doing the above? Other challenges include creating good experimental designs for task-based evaluation, such as the study Biran and McKeown (2017) did to assess whether explanations improved financial decision making because of"
W19-8402,N16-3020,0,0.0397925,"st check explanations for accuracy before doing the above? Other challenges include creating good experimental designs for task-based evaluation, such as the study Biran and McKeown (2017) did to assess whether explanations improved financial decision making because of increased scrutability; and also exploring whether automatic metrics such as BLEU (Papineni et al., 2002) give meaningful insights about trust, scrutability, etc. • Helping developers debug their AI systems. This is not a common goal in NLG, but seems to be one of the most common goals in Explainable AI. The popular LIME model (Ribeiro et al., 2016), for example, is largely presented as a way of helping ML developers choose between models, and also improve models via feature engineering. 3 Appropriate Explanation for Audience A fundamental principle of NLG is that texts are produced for users, and hence should use appropriate content, terminology, etc for the intended audience (Paris, 2015; Walker et al., 2004). For example, the Babytalk systems generated very different summaries from the same data for doctors (Portet et al., 2009), nurses (Hunter et al., 2012), and parents (Mahamood and Reiter, 2011). Explanations should also present in"
W19-8402,W17-3535,1,0.890879,"Missing"
W19-8402,W11-2803,1,0.751143,"n Explainable AI. The popular LIME model (Ribeiro et al., 2016), for example, is largely presented as a way of helping ML developers choose between models, and also improve models via feature engineering. 3 Appropriate Explanation for Audience A fundamental principle of NLG is that texts are produced for users, and hence should use appropriate content, terminology, etc for the intended audience (Paris, 2015; Walker et al., 2004). For example, the Babytalk systems generated very different summaries from the same data for doctors (Portet et al., 2009), nurses (Hunter et al., 2012), and parents (Mahamood and Reiter, 2011). Explanations should also present information in appropriate ways for their audience, using features, terminology, and content that make sense to the user (Lacave and D´ıez, 2002; Biran and McKeown, 2017). For example, a few years ago I helped some colleagues evaluate a system which generated explanations for an AI system which classified leaves (Alonso et al., 2017). We showed these explanations to a domain expert (Professor of Ecology at the University of Aberdeen), and he struggled to understand some explanations because the features used in these explanation were not the ones that he norm"
W90-0104,H86-1023,0,0.0225806,"sifier is given an object or event, and is asked to find an appropriate lexical unit that fits that object or event. Discrimination nets (e.g., Goldman 1975; Pustejovsky and Nirenburg 1987) are basically decision trees. They are typically used as high-speed 'compiled' classifiers that select the most specific lexical unit that subsumes t Currently at the Department of Artificial Intelligence, University of Edinburgh, 80 South Bridge, Edinburgh EH1 1HN, Scotland. E-maih reitel@aitma.edinburgh.ac.uk I This paper does not exmnine the kind of oollocational and selectional constraints discussed by Cumming (1986) and Nirenburg and Nirenburg (1988). 23 Objec~ C Animal Machine Vertebrate Network Fish t Ethernet Dam-rate:lOMbiffsec l Circuit-type:Packet [ Physical-medium:Coaxial-cableJ Breathes:Air t Shark (Dangerous:True) ~ Brca~es.-Air (Can-fly:True)j tL 0 Human ) ( Pekingese larrow caOStrich n-fly:FalseJ I Age-status:Adult Adult I f I ~, I [ ~=ma~oI I -.@ Bachelor Married:False (Key) cPrimitive 3 lass [ Class Defined [ Basic Level Class definingroleVR Lexical Unit Class (default role filler) Object Class is ~ y 24 Figure 1: Objects and Lexical Units in a Taxonomy the target object or event. For insta"
W90-0104,C88-2100,0,0.106656,"object or event, and is asked to find an appropriate lexical unit that fits that object or event. Discrimination nets (e.g., Goldman 1975; Pustejovsky and Nirenburg 1987) are basically decision trees. They are typically used as high-speed 'compiled' classifiers that select the most specific lexical unit that subsumes t Currently at the Department of Artificial Intelligence, University of Edinburgh, 80 South Bridge, Edinburgh EH1 1HN, Scotland. E-maih reitel@aitma.edinburgh.ac.uk I This paper does not exmnine the kind of oollocational and selectional constraints discussed by Cumming (1986) and Nirenburg and Nirenburg (1988). 23 Objec~ C Animal Machine Vertebrate Network Fish t Ethernet Dam-rate:lOMbiffsec l Circuit-type:Packet [ Physical-medium:Coaxial-cableJ Breathes:Air t Shark (Dangerous:True) ~ Brca~es.-Air (Can-fly:True)j tL 0 Human ) ( Pekingese larrow caOStrich n-fly:FalseJ I Age-status:Adult Adult I f I ~, I [ ~=ma~oI I -.@ Bachelor Married:False (Key) cPrimitive 3 lass [ Class Defined [ Basic Level Class definingroleVR Lexical Unit Class (default role filler) Object Class is ~ y 24 Figure 1: Objects and Lexical Units in a Taxonomy the target object or event. For instance, looking at some of Goldman's e"
W90-0104,P87-1028,0,0.0242182,"), which generates certain kinds of natural language object descriptions. FN uses some additional preference rules that primarily affect NP formarion; these rules are not discussed in this paper. 2. Lexical Choice as Classification The two major approaches (to date) for lexical choice have been discrimination nets and structure mapping systems. Both of these approaches can be regarded as classification/matching architectures, where a classifier is given an object or event, and is asked to find an appropriate lexical unit that fits that object or event. Discrimination nets (e.g., Goldman 1975; Pustejovsky and Nirenburg 1987) are basically decision trees. They are typically used as high-speed 'compiled' classifiers that select the most specific lexical unit that subsumes t Currently at the Department of Artificial Intelligence, University of Edinburgh, 80 South Bridge, Edinburgh EH1 1HN, Scotland. E-maih reitel@aitma.edinburgh.ac.uk I This paper does not exmnine the kind of oollocational and selectional constraints discussed by Cumming (1986) and Nirenburg and Nirenburg (1988). 23 Objec~ C Animal Machine Vertebrate Network Fish t Ethernet Dam-rate:lOMbiffsec l Circuit-type:Packet [ Physical-medium:Coaxial-cableJ B"
W94-0319,P84-1107,0,0.270212,"Missing"
W94-0319,C88-1029,0,0.149365,"Missing"
W94-0319,C90-1008,0,0.101905,"Missing"
W94-0319,P88-1020,0,0.0111439,"Missing"
W94-0319,H89-1022,0,0.0186117,"Missing"
W94-0319,C92-1038,1,0.526232,"Missing"
W94-0319,P92-1034,1,0.844207,"Missing"
W94-0319,A92-1009,1,0.786017,"Missing"
W94-0319,J90-1004,0,0.0561373,"Missing"
W94-0319,T75-2027,0,0.449293,"Missing"
W94-0319,P89-1025,0,0.0155977,"Missing"
W94-0319,A92-1006,0,0.34282,"Missing"
W96-0503,W96-0503,1,0.0547219,"wo descriptions of is s e c t i o n o f his institution, each section belongs to exactly one course. (We have observed such cardinality mistakes in many OO models.) The analyst fixes this and reruns M o D E x on this relation, obtaining the description shown in Figure 3 (bottom). The text now contains a new section with negative examples, which makes it clear that it is no longer possible for a SECTION to belong either to z e r o COURSES or to multiple COURSES. Several other types of text can be generated, such as path descriptions and comparisons and texts about several classes. We refer to (Lavoie et al., 1996) for more detailed information. 4 How MODEx Works MODEx is implemented using the now fairly standard, modular pipeline architecture. Several modules are part of C O G E N T , CoGenTex&apos;s generator shell. M o D E x operates as a &apos;Web server&apos; which generates H T M L files that can be viewed by any Web browser. For lack of space we refrain from giving details here and refer to (Lavoie et al., 1996) for details. M o D E x is designed for use independent of the domain of the OO d a t a model that is being described: it lacks domain knowledge. This means that the system is fully portable between mode"
W96-0503,A92-1006,1,0.548089,"ent feedback from actual users during an iterative prototyping approach. • MoDEx includes examples in its texts: as well as conventional descriptions. The need for examples in documentation has been pointed out in recent work by Paris and Mittal (see for example (Mittal and Paris, 1&apos;293) and the references cited therein). However, none of the specification paraphrasers proposed to date have included examples. • M o D E x uses an interactive hypertext interface to allow users to browse through the model. Such interfaces have been used in other NLG applications, (e.g., (Reiter e t a l . , 1995; Rambow and Korelsky, 1992)), but ours is based on (now) standard html-based W W W technology. • M o D E x uses a simple modeling language, which is based on the ODL standard developed by the Object Database Management Group (OMC) (Cattail, 1994). Some previous systems have paraphrased complex modeling languages that are not widely used outside the research community (GIST, P P P ) . • M o D E x does not have access to knowledge about the domain of the data model (beyond the data model itself). At least one previous system has used such knowledge (GEMA). 3 A MoDEx Scenario Figure 1: The University O-O Diagram Suppose th"
