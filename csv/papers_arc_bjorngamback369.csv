2020.semeval-1.99,{S}em{E}val-2020 Task 8: Memotion Analysis- the Visuo-Lingual Metaphor!,2020,-1,-1,8,0,15133,chhavi sharma,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"Information on social media comprises of various modalities such as textual, visual and audio. NLP and Computer Vision communities often leverage only one prominent modality in isolation to study social media. However, computational processing of Internet memes needs a hybrid approach. The growing ubiquity of Internet memes on social media platforms such as Facebook, Instagram, and Twitter further suggests that we can not ignore such multimodal content anymore. To the best of our knowledge, there is not much attention towards meme emotion analysis. The objective of this proposal is to bring the attention of the research community towards the automatic processing of Internet memes. The task Memotion analysis released approx 10K annotated memes- with human annotated labels namely sentiment(positive, negative, neutral), type of emotion(sarcastic,funny,offensive, motivation) and their corresponding intensity. The challenge consisted of three subtasks: sentiment (positive, negative, and neutral) analysis of memes,overall emotion (humor, sarcasm, offensive, and motivational) classification of memes, and classifying intensity of meme emotion. The best performances achieved were F1 (macro average) scores of 0.35, 0.51 and 0.32, respectively for each of the three subtasks."
2020.semeval-1.100,{S}em{E}val-2020 Task 9: Overview of Sentiment Analysis of Code-Mixed Tweets,2020,-1,-1,6,0,11186,parth patwa,Proceedings of the Fourteenth Workshop on Semantic Evaluation,0,"In this paper, we present the results of the SemEval-2020 Task 9 on Sentiment Analysis of Code-Mixed Tweets (SentiMix 2020). We also release and describe our Hinglish (Hindi-English)and Spanglish (Spanish-English) corpora annotated with word-level language identification and sentence-level sentiment labels. These corpora are comprised of 20K and 19K examples, respectively. The sentiment labels are - Positive, Negative, and Neutral. SentiMix attracted 89 submissions in total including 61 teams that participated in the Hinglish contest and 28 submitted systems to the Spanglish competition. The best performance achieved was 75.0{\%} F1 score for Hinglish and 80.6{\%} F1 for Spanglish. We observe that BERT-like models and ensemble methods are the most common and successful approaches among the participants."
2020.icon-main.33,Sentimental Poetry Generation,2020,-1,-1,2,0,19146,kasper rostvold,Proceedings of the 17th International Conference on Natural Language Processing (ICON),0,"The paper investigates how well poetry can be generated to contain a specific sentiment, and whether readers of the poetry experience the intended sentiment. The poetry generator consists of a bi-directional Long Short-Term Memory (LSTM) model, combined with rhyme pair generation, rule-based word prediction methods, and tree search for extending generation possibilities. The LSTM network was trained on a set of English poetry written and published by users on a public website. Human judges evaluated poems generated by the system, both with a positive and negative sentiment. The results indicate that while there are some weaknesses in the system compared to other state-of-the-art solutions, it is fully capable of generating poetry with an inherent sentiment that is perceived by readers."
2020.icon-main.35,Native-Language Identification with Attention,2020,-1,-1,2,0,19148,stian steinbakken,Proceedings of the 17th International Conference on Natural Language Processing (ICON),0,"The paper explores how an attention-based approach can increase performance on the task of native-language identification (NLI), i.e., to identify an author{'}s first language given information expressed in a second language. Previously, Support Vector Machines have consistently outperformed deep learning-based methods on the TOEFL11 data set, the de facto standard for evaluating NLI systems. The attention-based system BERT (Bidirectional Encoder Representations from Transformers) was first tested in isolation on the TOEFL11 data set, then used in a meta-classifier stack in combination with traditional techniques to produce an accuracy of 0.853. However, more labelled NLI data is now available, so BERT was also trained on the much larger Reddit-L2 data set, containing 50 times as many examples as previously used for English NLI, giving an accuracy of 0.902 on the Reddit-L2 in-domain test scenario, improving the state-of-the-art by 21.2 percentage points."
2020.alw-1.3,Using Transfer-based Language Models to Detect Hateful and Offensive Language Online,2020,-1,-1,2,0,22383,vebjorn isaksen,Proceedings of the Fourth Workshop on Online Abuse and Harms,0,"Distinguishing hate speech from non-hate offensive language is challenging, as hate speech not always includes offensive slurs and offensive language not always express hate. Here, four deep learners based on the Bidirectional Encoder Representations from Transformers (BERT), with either general or domain-specific language models, were tested against two datasets containing tweets labelled as either {`}Hateful{'}, {`}Normal{'} or {`}Offensive{'}. The results indicate that the attention-based models profoundly confuse hate speech with offensive and normal language. However, the pre-trained models outperform state-of-the-art results in terms of accurately predicting the hateful instances."
W19-3516,A Platform Agnostic Dual-Strand Hate Speech Detector,2019,0,0,2,0,24443,johannes meyer,Proceedings of the Third Workshop on Abusive Language Online,0,"Hate speech detectors must be applicable across a multitude of services and platforms, and there is hence a need for detection approaches that do not depend on any information specific to a given platform. For instance, the information stored about the text{'}s author may differ between services, and so using such data would reduce a system{'}s general applicability. The paper thus focuses on using exclusively text-based input in the detection, in an optimised architecture combining Convolutional Neural Networks and Long Short-Term Memory-networks. The hate speech detector merges two strands with character n-grams and word embeddings to produce the final classification, and is shown to outperform comparable previous approaches."
S19-2124,{NIT}{\\_}{A}gartala{\\_}{NLP}{\\_}{T}eam at {S}em{E}val-2019 Task 6: An Ensemble Approach to Identifying and Categorizing Offensive Language in {T}witter Social Media Corpora,2019,0,0,3,0,15211,steve swamy,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"The paper describes the systems submitted to OffensEval (SemEval 2019, Task 6) on {`}Identifying and Categorizing Offensive Language in Social Media{'} by the {`}NIT{\_}Agartala{\_}NLP{\_}Team{'}. A Twitter annotated dataset of 13,240 English tweets was provided by the task organizers to train the individual models, with the best results obtained using an ensemble model composed of six different classifiers. The ensemble model produced macro-averaged F1-scores of 0.7434, 0.7078 and 0.4853 on Subtasks A, B, and C, respectively. The paper highlights the overall low predictive nature of various linguistic features and surface level count features, as well as the limitations of a traditional machine learning approach when compared to a Deep Learning counterpart."
K19-1088,Studying Generalisability across Abusive Language Detection Datasets,2019,0,0,3,0,15211,steve swamy,Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL),0,"Work on Abusive Language Detection has tackled a wide range of subtasks and domains. As a result of this, there exists a great deal of redundancy and non-generalisability between datasets. Through experiments on cross-dataset training and testing, the paper reveals that the preconceived notion of including more non-abusive samples in a dataset (to emulate reality) may have a detrimental effect on the generalisability of a model trained on that data. Hence a hierarchical annotation model is utilised here to reveal redundancies in existing datasets and to help reduce redundancy in future efforts."
W18-6215,Ternary {T}witter Sentiment Classification with Distant Supervision and Sentiment-Specific Word Embeddings,2018,0,1,3,0,27765,mats byrkjeland,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"The paper proposes the Ternary Sentiment Embedding Model, a new model for creating sentiment embeddings based on the Hybrid Ranking Model of Tang et al. (2016), but trained on ternary-labeled data instead of binary-labeled, utilizing sentiment embeddings from datasets made with different distant supervision methods. The model is used as part of a complete Twitter Sentiment Analysis system and empirically compared to existing systems, showing that it outperforms Hybrid Ranking and that the quality of the distant-supervised dataset has a great impact on the quality of the produced sentiment embeddings."
W18-5110,The Effects of User Features on {T}witter Hate Speech Detection,2018,0,5,2,0,28029,elise unsvaag,Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2),0,"The paper investigates the potential effects user features have on hate speech classification. A quantitative analysis of Twitter data was conducted to better understand user characteristics, but no correlations were found between hateful text and the characteristics of the users who had posted it. However, experiments with a hate speech classifier based on datasets from three different languages showed that combining certain user features with textual features gave slight improvements of classification performance. While the incorporation of user features resulted in varying impact on performance for the different datasets used, user network-related features provided the most consistent improvements."
W18-3215,Named Entity Recognition on Code-Switched Data Using Conditional Random Fields,2018,0,1,3,1,28345,utpal sikdar,Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching,0,"Named Entity Recognition is an important information extraction task that identifies proper names in unstructured texts and classifies them into some pre-defined categories. Identification of named entities in code-mixed social media texts is a more difficult and challenging task as the contexts are short, ambiguous and often noisy. This work proposes a Conditional Random Fields based named entity recognition system to identify proper names in code-switched data and classify them into nine categories. The system ranked fifth among nine participant systems and achieved a 59.25{\%} F1-score."
S18-1138,{NTNU} at {S}em{E}val-2018 Task 7: Classifier Ensembling for Semantic Relation Identification and Classification in Scientific Papers,2018,0,0,3,0,28346,biswanath barik,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"The paper presents NTNU{'}s contribution to SemEval-2018 Task 7 on relation identification and classification. The class weights and parameters of five alternative supervised classifiers were optimized through grid search and cross-validation. The outputs of the classifiers were combined through voting for the final prediction. A wide variety of features were explored, with the most informative identified by feature selection. The best setting achieved F1 scores of 47.4{\%} and 66.0{\%} in the relation classification subtasks 1.1 and 1.2. For relation identification and classification in subtask 2, it achieved F1 scores of 33.9{\%} and 17.0{\%},"
S18-1144,"{F}lytxt{\\_}{NTNU} at {S}em{E}val-2018 Task 8: Identifying and Classifying Malware Text Using Conditional Random Fields and Na{\\\\\i}ve {B}ayes Classifiers""",2018,0,1,3,1,28345,utpal sikdar,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"Cybersecurity risks such as malware threaten the personal safety of users, but to identify malware text is a major challenge. The paper proposes a supervised learning approach to identifying malware sentences given a document (subTask1 of SemEval 2018, Task 8), as well as to classifying malware tokens in the sentences (subTask2). The approach achieved good results, ranking second of twelve participants for both subtasks, with F-scores of 57{\%} for subTask1 and 28{\%} for subTask2."
L18-1447,Utilizing Large {T}witter Corpora to Create Sentiment Lexica,2018,0,0,3,0,29998,valerij fredriksen,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-4424,A Feature-based Ensemble Approach to Recognition of Emerging and Rare Named Entities,2017,13,2,2,1,28345,utpal sikdar,Proceedings of the 3rd Workshop on Noisy User-generated Text,0,"Detecting previously unseen named entities in text is a challenging task. The paper describes how three initial classifier models were built using Conditional Random Fields (CRFs), Support Vector Machines (SVMs) and a Long Short-Term Memory (LSTM) recurrent neural network. The outputs of these three classifiers were then used as features to train another CRF classifier working as an ensemble. 5-fold cross-validation based on training and development data for the emerging and rare named entity recognition shared task showed precision, recall and F1-score of 66.87{\%}, 46.75{\%} and 54.97{\%}, respectively. For surface form evaluation, the CRF ensemble-based system achieved precision, recall and F1 scores of 65.18{\%}, 45.20{\%} and 53.30{\%}. When applied to unseen test data, the model reached 47.92{\%} precision, 31.97{\%} recall and 38.55{\%} F1-score for entity level evaluation, with the corresponding surface form evaluation values of 44.91{\%}, 30.47{\%} and 36.31{\%}."
W17-3013,Using Convolutional Neural Networks to Classify Hate-Speech,2017,14,53,1,1,15137,bjorn gamback,Proceedings of the First Workshop on Abusive Language Online,0,"The paper introduces a deep learning-based Twitter hate-speech text classification system. The classifier assigns each tweet to one of four predefined categories: racism, sexism, both (racism and sexism) and non-hate-speech. Four Convolutional Neural Network models were trained on resp. character 4-grams, word vectors based on semantic information built using word2vec, randomly generated word vectors, and word vectors combined with character n-grams. The feature set was down-sized in the networks by max-pooling, and a softmax function used to classify tweets. Tested by 10-fold cross-validation, the model based on word2vec embeddings performed best, with higher precision than recall, and a 78.3{\%} F-score."
W17-0210,{T}witter Topic Modeling by Tweet Aggregation,2017,14,11,3,0,32166,asbjorn steinskog,Proceedings of the 21st Nordic Conference on Computational Linguistics,0,None
E17-1069,A Societal Sentiment Analysis: Predicting the Values and Ethics of Individuals by Analysing Social Media Content,2017,32,2,6,0,29751,tushar maheshwari,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 1, Long Papers",0,"To find out how users{'} social media behaviour and language are related to their ethical practices, the paper investigates applying Schwartz{'} psycholinguistic model of societal sentiment to social media text. The analysis is based on corpora collected from user essays as well as social media (Facebook and Twitter). Several experiments were carried out on the corpora to classify the ethical values of users, incorporating Linguistic Inquiry Word Count analysis, n-grams, topic models, psycholinguistic lexica, speech-acts, and non-linguistic information, while applying a range of machine learners (Support Vector Machines, Logistic Regression, and Random Forests) to identify the best linguistic and non-linguistic features for automatic classification of values and ethics."
W16-6326,{T}witter Named Entity Extraction and Linking Using Differential Evolution,2016,0,0,2,1,28345,utpal sikdar,Proceedings of the 13th International Conference on Natural Language Processing,0,None
W16-5817,Language Identification in Code-Switched Text Using Conditional Random Fields and Babelnet,2016,0,0,2,1,28345,utpal sikdar,Proceedings of the Second Workshop on Computational Approaches to Code Switching,0,None
W16-3922,Feature-Rich {T}witter Named Entity Recognition and Classification,2016,0,4,2,1,28345,utpal sikdar,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"Twitter named entity recognition is the process of identifying proper names and classifying them into some predefined labels/categories. The paper introduces a Twitter named entity system using a supervised machine learning approach, namely Conditional Random Fields. A large set of different features was developed and the system was trained using these. The Twitter named entity task can be divided into two parts: i) Named entity extraction from tweets and ii) Twitter name classification into ten different types. For Twitter named entity recognition on unseen test data, our system obtained the second highest F1 score in the shared task: 63.22{\%}. The system performance on the classification task was worse, with an F1 measure of 40.06{\%} on unseen test data, which was the fourth best of the ten systems participating in the shared task."
S16-1014,{NTNUS}ent{E}val at {S}em{E}val-2016 Task 4: Combining General Classifiers for Fast {T}witter Sentiment Analysis,2016,9,4,3,0,29999,brage jahren,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1292,Comparing the Level of Code-Switching in Corpora,2016,20,12,1,1,15137,bjorn gamback,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Social media texts are often fairly informal and conversational, and when produced by bilinguals tend to be written in several different languages simultaneously, in the same way as conversational speech. The recent availability of large social media corpora has thus also made large-scale code-switched resources available for research. The paper addresses the issues of evaluation and comparison these new corpora entail, by defining an objective measure of corpus level complexity of code-switched texts. It is also shown how this formal measure can be used in practice, by applying it to several code-switched corpora."
W15-5906,Self-Organizing Maps for Classification of a Multi-Labeled Corpus,2015,0,0,2,1,34205,lars bungum,Proceedings of the 12th International Conference on Natural Language Processing,0,None
W15-5938,Sentence Boundary Detection for Social Media Text,2015,14,4,5,0,32463,dwijen rudrapal,Proceedings of the 12th International Conference on Natural Language Processing,0,None
W15-2914,Negation Scope Detection for {T}witter Sentiment Analysis,2015,36,19,3,0,36878,johan reitan,"Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,The paper describes the first sophisticated negation scope detection system for Twitter sentiment analysis. The system has been evaluated both on existing corpora from other domains and on a corpus of English Twitter data (tweets) annotated for negation. It produces better results than what has been reported in other domains and improves the performance on tweets containing negation when incorporated into a state-of-the-art Twitter sentiment analyser.
R15-1033,Part-of-Speech Tagging for Code-Mixed {E}nglish-{H}indi {T}witter and {F}acebook Chat Messages,2015,36,33,2,1,15215,anupam jamatia,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"The paper reports work on collecting and annotating code-mixed English-Hindi social media text (Twitter and Facebook messages), and experiments on automatic tagging of these corpora, using both a coarse-grained and a fine-grained part-ofspeech tag set. We compare the performance of a combination of language specific taggers to that of applying four machine learning algorithms to the task (Conditional Random Fields, Sequential Minimal Optimization, Naive Bayes and Random Forests), using a range of different features based on word context and wordinternal information."
W14-5152,Identifying Languages at the Word Level in Code-Mixed {I}ndian Social Media Text,2014,43,47,2,1,14329,amitava das,Proceedings of the 11th International Conference on Natural Language Processing,0,"Language identification at the document level has been considered an almost solved problem in some application areas, but language detectors fail in the social media context due to phenomena such as utterance internal code-switching, lexical borrowings, and phonetic typing; all implying that language identification in social media has to be carried out at the word level. The paper reports a study to detect language boundaries at the word level in chat message corpora in mixed EnglishBengali and English-Hindi. We introduce a code-mixing index to evaluate the level of blending in the corpora and describe the performance of a system developed to separate multiple languages."
W14-0510,Agent-based modeling of language evolution,2014,-1,-1,2,0,38846,torvald lekvam,Proceedings of the 5th Workshop on Cognitive Aspects of Computational Language Learning ({C}og{ACLL}),0,None
S14-2078,{NTNU}: Measuring Semantic Similarity with Sublexical Feature Representations and Soft Cardinality,2014,20,12,3,0,39003,andre lynum,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"The paper describes the approaches taken by the NTNU team to the SemEval 2014 Semantic Textual Similarity shared task. The solutions combine measures based on lexical soft cardinality and character n-gram feature representations with lexical distance metrics from TakeLabxe2x80x99s baseline system. The final NTNU system is based on bagged support vector machine regression over the datasets from previous shared tasks and shows highly competitive performance, being the best system on three of the datasets and third best overall (on weighted mean over all six datasets)."
W13-3210,Towards Dynamic Word Sense Discrimination with Random Indexing,2013,17,7,3,0,23682,hans moen,Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,0,"Most distributional models of word similarity represent a word type by a single vector of contextual features, even though, words commonly have more than one sense. The multiple senses can be captured by employing several vectors per word in a multi-prototype distributional model, prototypes that can be obtained by first constructing all the context vectors for the word and then clustering similar vectors to create sense vectors. Storing and clustering context vectors can be expensive though. As an alternative, we introduce Multi-Sense Random Indexing, which performs on-the-fly (incremental) clustering. To evaluate the method, a number of measures for word similarity are proposed, both contextual and non-contextual, including new measures based on optimal alignment of word senses. Experimental results on the task of predicting semantic textual similarity do, however, not show a systematic difference between singleprototype and multi-prototype models."
W13-1003,Improving Word Translation Disambiguation by Capturing Multiword Expressions with Dictionaries,2013,17,8,2,1,34205,lars bungum,Proceedings of the 9th Workshop on Multiword Expressions,0,"The paper describes a method for identifying and translating multiword expressions using a bi-directional dictionary. While a dictionarybased approach suffers from limited recall, precision is high; hence it is best employed alongside an approach with complementing properties, such as an n-gram language model. We evaluate the method on data from the English-German translation part of the crosslingual word sense disambiguation task in the 2010 semantic evaluation exercise (SemEval). The output of a baseline disambiguation system based on n-grams was substantially improved by matching the target words and their immediate contexts against compound and collocational words in a dictionary."
S13-2071,{NTNU}: Domain Semi-Independent Short Message Sentiment Classification,2013,19,1,3,0,41228,oyvind selmer,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"The paper describes experiments using grid searches over various combinations of machine learning algorithms, features and preprocessing strategies in order to produce the optimal systems for sentiment classification of microblog messages. The approach is fairly domain independent, as demonstrated by the systems achieving quite competitive results when applied to short text message data, i.e., input they were not originally trained on."
S13-1008,{NTNU}-{CORE}: Combining strong features for semantic similarity,2013,27,4,5,0,32380,erwin marsi,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"The paper outlines the work carried out at NTNU as part of the *SEMxe2x80x9913 shared task on Semantic Textual Similarity, using an approach which combines shallow textual, distributional and knowledge-based features by a support vector regression model. Feature sets include (1) aggregated similarity based on named entity recognition with WordNet and Levenshtein distance through the calculation of maximum weighted bipartite graphs; (2) higher order word co-occurrence similarity using a novel method called xe2x80x9cMultisense Random Indexingxe2x80x9d; (3) deeper semantic relations based on the RelEx semantic dependency relationship extraction system; (4) graph edit-distance on dependency trees; (5) reused features of the TakeLab and DKPro systems from the STSxe2x80x9912 shared task. The NTNU systems obtained 9th place overall (5th best team) and 1st place on the SMT data set."
W12-3707,{S}entimantics: Conceptual Spaces for Lexical Sentiment Polarity Representation with Contextuality,2012,26,12,2,1,14329,amitava das,Proceedings of the 3rd Workshop in Computational Approaches to Subjectivity and Sentiment Analysis,0,"Current sentiment analysis systems rely on static (context independent) sentiment lexica with proximity based fixed-point prior polarities. However, sentiment-orientation changes with context and these lexical resources give no indication of which value to pick at what context. The general trend is to pick the highest one, but which that is may vary at context. To overcome the problems of the present proximity-based static sentiment lexicon techniques, the paper proposes a new way to represent sentiment knowledge in a Vector Space Model. This model can store dynamic prior polarity with varying contextual information. The representation of the sentiment knowledge in the Conceptual Spaces of distributional Semantics is termed Sentimantics."
W09-0715,Methods for {A}mharic Part-of-Speech Tagging,2009,24,19,1,1,15137,bjorn gamback,Proceedings of the First Workshop on Language Technologies for {A}frican Languages,0,"The paper describes a set of experiments involving the application of three state-of-the-art part-of-speech taggers to Ethiopian Amharic, using three different tagsets. The taggers showed worse performance than previously reported results for English, in particular having problems with unknown words. The best results were obtained using a Maximum Entropy approach, while HMM-based and SVM-based taggers got comparable results."
E09-2017,A Mobile Health and Fitness Companion Demonstrator,2009,7,14,2,0,47379,olov staahl,Proceedings of the Demonstrations Session at {EACL} 2009,0,"Multimodal conversational spoken dialogues using physical and virtual agents provide a potential interface to motivate and support users in the domain of health and fitness. The paper presents a multimodal conversational Companion system focused on health and fitness, which has both a stationary and a mobile component."
W05-0710,Classifying {A}mharic News Text Using Self-Organizing Maps,2005,21,12,2,0,50805,samuel eyassu,Proceedings of the {ACL} Workshop on Computational Approaches to {S}emitic Languages,0,"The paper addresses using artificial neural networks for classification of Amharic news items. Amharic is the language for countrywide communication in Ethiopia and has its own writing system containing extensive systematic redundancy. It is quite dialectally diversified and probably representative of the languages of a continent that so far has received little attention within the language processing field.n n The experiments investigated document clustering around user queries using Self-Organizing Maps, an unsupervised learning neural network strategy. The best ANN model showed a precision of 60.0% when trying to cluster unseen data, and a 69.5% precision when trying to classify it."
W05-0109,Natural Language Processing at the {S}chool of {I}nformation {S}tudies for {A}frica,2005,12,2,1,1,15137,bjorn gamback,Proceedings of the Second {ACL} Workshop on Effective Tools and Methodologies for Teaching {NLP} and {CL},0,"The lack of persons trained in computational linguistic methods is a severe obstacle to making the Internet and computers accessible to people all over the world in their own languages. The paper discusses the experiences of designing and teaching an introductory course in Natural Language Processing to graduate computer science students at Addis Ababa University, Ethiopia, in order to initiate the education of computational linguists in the Horn of Africa region."
W03-2701,"{I}ntroduction: Dialogue Systems: Interaction, Adaptation and Styles of Management",2003,0,2,2,0,15952,kristiina jokinen,"Proceedings of the 2003 {EACL} Workshop on Dialogue Systems: interaction, adaptation and styes of management",0,None
W00-1502,Composing a General-Purpose Toolbox for {S}wedish,2000,18,2,2,0,2702,fredrik olsson,Proceedings of the {COLING}-2000 Workshop on Using Toolsets and Architectures To Build {NLP} Systems,0,"The paper discusses the lessons we have learned from the work on building a reusable toolset for Swedish within the framework of GATE, the General Architecture for Text Engineering, from the University of Sheffield, UK.n n We describe our toolbox SVENSK and the reasons behind the choices made in the design, as well as the overall conclusions for language processing toolbox design which can be drawn."
gamback-olsson-2000-experiences,Experiences of Language Engineering Algorithm Reuse,2000,8,12,1,1,15137,bjorn gamback,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Abstractn Traditionally, the level of reusability of language processing resources within the research community has been veryn low. Most of the recycling of linguistic resources has been concerned with reuse of data, e.g., corpora, lexica, andn grammars, while the algorithmic resources far too seldom have been shared between dierent projects and institutions.n As a consequence, researchers who are willing to reuse somebody else's processing components have been forced to investn major eorts into issues of integration, inter-process communication, and interface design.n In this paper, we discuss the experiences drawn from the svensk project regarding the issues on reusability of languagen engineering software as well as some of the challenges for the research community which are prompted by them. Their mainn characteristics can be laid out along three dimensions; technical/software challenges, linguistic challenges, and `political'n challenges. In the end, the unavoidable conclusion is that it denitely is time to bring more aspects of engineering inton the Computational Linguistic community!"
W99-1017,Designing a System for {S}wedish Spoken Document Retrieval,2000,-1,-1,2,0,52580,botond pakucs,Proceedings of the 12th Nordic Conference of Computational Linguistics ({NODALIDA} 1999),0,None
P98-1072,Semantic-Head Based Resolution of Scopal Ambiguities,1998,10,5,1,1,15137,bjorn gamback,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"We introduce an algorithm for scope resolution in underspecified semantic representations. Scope preferences are suggested on the basis of semantic argument structure. The major novelty of this approach is that, while maintaining an (scopally) underspecified semantic representation, we at the same time suggest a resolution possibility. The algorithm has been implemented and tested in a large-scale system and fared quite well: 28% of the utterances were ambiguous, 80% of these were correctly interpreted, leaving errors in only 5.7% of the utterance set."
C98-1069,Semantic-Head Based Resolution of Scopal Ambiguities,1998,10,5,1,1,15137,bjorn gamback,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"We introduce an algorithm for scope resolution in underspecified semantic representations. Scope preferences are suggested on the basis of semantic argument structure. The major novelty of this approach is that, while maintaining an (scopally) underspecified semantic representation, we at the same time suggest a resolution possibility. The algorithm has been implemented and tested in a large-scale system and fared quite well: 28% of the utterances were ambiguous, 80% of these were correctly interpreted, leaving errors in only 5.7% of the utterance set."
Y96-1006,Underspecified {J}apanese Semantics in a Machine Translation System,1996,10,2,1,1,15137,bjorn gamback,"Proceedings of the 11th Pacific Asia Conference on Language, Information and Computation",0,"Semantic representations which are underspecified with respect to, for example, scope, have recently attracted much attention. Most research in this area has focused on treating English language phenomena in a theoretical fashion. Our paper deviates from this twofold: We take Japanese as the language of our investigations and describe how our ideas about underspecified Japanese semantics (e.g., on modality adverbs) have been implemented in a spoken-language machine translation system."
C96-1024,Compositional Semantics in Verbmobil,1996,6,40,2,0,6245,johan bos,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"The paper discusses how compositional semantics is implemented in the Verbmobil speech-to-speech translation system using LUD, a description language for underspecified discourse representation structures. The description language and its formal interpretation in DRT are described as well as its implementation together with the architecture of the system's entire syntactic-semantic processing module. We show that a linguistically sound theory and formalism can be properly implemented in a system with (near) real-time requirements."
W95-0203,{S}wedish Language Processing in the Spoken Language Translator,1995,0,12,1,1,15137,bjorn gamback,Proceedings of the 10th Nordic Conference of Computational Linguistics ({NODALIDA} 1995),0,"Abstractn This paper provides an overview of the colloquium's discussion session on natural language understanding, which followed presentations by M. Bates [Bates, M. (1995) Proc. Natl. Acad. Sci. USA 92, 9977-9982] and R. C. Moore [Moore, R. C. (1995) Proc. Natl. Acad. Sci. USA 92, 9983-9988]. The paper reviews the dual role of language processing in providing understanding of the spoken input and an additional source of constraint in the recognition process. To date, language processing has successfully provided understanding but has provided only limited (and computationally expensive) constraint. As a result, most current systems use a loosely coupled, unidirectional interface, such as N-best or a word network, with natural language constraints as a postprocess, to filter or resort the recognizer output. However, the level of discourse context provides significant constraint on what people can talk about and how things can be referred to; when the system becomes an active participant, it can influence this order. But sources of discourse constraint have not been extensively explored, in part because these effects can only be seen by studying systems in the context of their use in interactive problem solving. This paper argues that we need to study interactive systems to understand what kinds of applications are appropriate for the current state of technology and how the technology can move from the laboratory toward real applications."
1994.amta-1.12,Complex Verb Transfer Phenomena in the {SLT} System,1994,-1,-1,1,1,15137,bjorn gamback,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
W93-0406,Tagging Experiments Using Neural Networks,1994,-1,-1,2,0,55262,martin eineborg,Proceedings of the 9th Nordic Conference of Computational Linguistics ({NODALIDA} 1993),0,None
W93-0408,On Implementing {S}wedish Tense and Aspect,1994,-1,-1,1,1,15137,bjorn gamback,Proceedings of the 9th Nordic Conference of Computational Linguistics ({NODALIDA} 1993),0,None
W93-0412,Clustering Sentences {--} Making Sense of Synonymous Sentences,1994,-1,-1,2,0,21633,jussi karlgren,Proceedings of the 9th Nordic Conference of Computational Linguistics ({NODALIDA} 1993),0,None
H93-1042,A Speech to Speech Translation System Built From Standard Components,1993,16,24,6,0.952381,11421,manny rayner,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"This paper describes a speech to speech translation system using standard components and a suite of generalizable customization techniques. The system currently translates air travel planning queries from English to Swedish. The modular architecture is designed to be easy to port to new domains and languages, and consists of a pipelined series of processing phases. The output of each phase consists of multiple hypotheses; statistical preference mechanisms, the data for which is derived from automatic processing of domain corpora, are used between each pair of phases to filter hypotheses. Linguistic knowledge is represented throughout the system in declarative form. We summarize the architectures of the component systems and the interfaces between them, and present initial performance results."
C92-4186,Ebl{\\mbox{$^2$}}: An Approach to Automatic Lexical Acquisition,1992,21,6,2,0,36920,lars asker,{COLING} 1992 Volume 4: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"A method for automatic lexical acquisition is outlined. An existing lexicon that, in addition to ordinary lexical entries, contains prototypical entries for various non-exclusive paradigms of open-class words, is extended by inferring new lexical entries from texts containing unknown words. This is done by comparing the constraints placed on the unknown words by the natural language system's grammar with the prototypes and a number of hand-coded phrase templates specific for each paradigm. Once a sufficient number of observations of the word in different contexts have been made, a lexical entry is constructed for the word by assigning it to one or several paradigm(s).Parsing sentences with unknown words is normally very time-consuming due to the large number of grammatically possible analyses. To circumvent this problem, other phrase templates are extracted automatically from the grammar and domain-specific texts using an explanation-based learning method. These templates represent grammatically correct sentence patterns. When a sentence matches a template, the original parsing component can be bypassed, reducing parsing times dramatically."
1992.tc-1.14,{E}nglish-{S}wedish translation of dialogue software,1992,9,2,5,0,41856,hiyan alshawi,Proceedings of Translating and the Computer 14: Quality standards and the implementation of technology in translation,0,"The paper describes the BCI, a prototype interactive machine-translation system, constructed by connecting English and Swedish versions of the SRI Core Language Engine through a transfer component. Transfer takes place at the level of Quasi Logical Form (QLF), a contextually sensitive logical form representation which is deep enough for dealing with cross-linguistic differences. Theoretical arguments are presented to support the claim that QLF transfer represents a good compromise between the opposing paradigms of syntactic transfer and semantic interlinguabased MT. An annotated example dialogue is shown. A follow-on project, in which the BCI is used as the core of a spoken-language translation system, is briefly described."
P91-1021,Translation by Quasi Logical Form Transfer,1991,11,40,4,0,41856,hiyan alshawi,29th Annual Meeting of the Association for Computational Linguistics,1,"The paper describes work on applying a general purpose natural language processing system to transfer-based interactive translation. Transfer takes place at the level of Quasi Logical Form (QLF), a contextually sensitive logical form representation which is deep enough for dealing with cross-linguistic differences. Theoretical arguments and experimental results are presented to support the claim that this framework has good properties in terms of modularity, compositionality, reversibility and monotonicity."
