2021.acl-short.106,Learning to Solve {NLP} Tasks in an Incremental Number of Languages,2021,-1,-1,4,1,10811,giuseppe castellucci,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In real scenarios, a multilingual model trained to solve NLP tasks on a set of languages can be required to support new languages over time. Unfortunately, the straightforward retraining on a dataset containing annotated examples for all the languages is both expensive and time-consuming, especially when the number of target languages grows. Moreover, the original annotated material may no longer be available due to storage or business constraints. Re-training only with the new language data will inevitably result in Catastrophic Forgetting of previously acquired knowledge. We propose a Continual Learning strategy that updates a model to support new languages over time, while maintaining consistent results on previously learned languages. We define a Teacher-Student framework where the existing model {``}teaches{''} to a student model its knowledge about the languages it supports, while the student is also trained on a new language. We report an experimental evaluation in several tasks including Sentence Classification, Relational Learning and Sequence Labeling."
2020.acl-main.191,{GAN}-{BERT}: Generative Adversarial Learning for Robust Text Classification with a Bunch of Labeled Examples,2020,-1,-1,3,0,12619,danilo croce,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Recent Transformer-based architectures, e.g., BERT, provide impressive results in many Natural Language Processing tasks. However, most of the adopted benchmarks are made of (sometimes hundreds of) thousands of examples. In many real scenarios, obtaining high- quality annotated data is expensive and time consuming; in contrast, unlabeled examples characterizing the target task can be, in general, easily collected. One promising method to enable semi-supervised learning has been proposed in image processing, based on Semi- Supervised Generative Adversarial Networks. In this paper, we propose GAN-BERT that ex- tends the fine-tuning of BERT-like architectures with unlabeled data in a generative adversarial setting. Experimental results show that the requirement for annotated examples can be drastically reduced (up to only 50-100 annotated examples), still obtaining good performances in several sentence classification tasks."
D19-1415,Auditing Deep Learning processes through Kernel-based Explanatory Models,2019,0,1,3,1,12619,danilo croce,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"While NLP systems become more pervasive, their accountability gains value as a focal point of effort. Epistemological opaqueness of nonlinear learning methods, such as deep learning models, can be a major drawback for their adoptions. In this paper, we discuss the application of Layerwise Relevance Propagation over a linguistically motivated neural architecture, the Kernel-based Deep Architecture, in order to trace back connections between linguistic properties of input instances and system decisions. Such connections then guide the construction of argumentations on network{'}s inferences, i.e., explanations based on real examples, semantically related to the input. We propose here a methodology to evaluate the transparency and coherence of analogy-based explanations modeling an audit stage for the system. Quantitative analysis on two semantic tasks, i.e., question classification and semantic role labeling, show that the explanatory capabilities (native in KDAs) are effective and they pave the way to more complex argumentation methods."
W18-5403,Explaining non-linear Classifier Decisions within Kernel-based Deep Architectures,2018,0,4,3,1,12619,danilo croce,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"Nonlinear methods such as deep neural networks achieve state-of-the-art performances in several semantic NLP tasks. However epistemologically transparent decisions are not provided as for the limited interpretability of the underlying acquired neural models. In neural-based semantic inference tasks epistemological transparency corresponds to the ability of tracing back causal connections between the linguistic properties of a input instance and the produced classification output. In this paper, we propose the use of a methodology, called \textit{Layerwise Relevance Propagation}, over linguistically motivated neural architectures, namely \textit{Kernel-based Deep Architectures} (KDA), to guide argumentations and explanation inferences. In such a way, each decision provided by a KDA can be linked to real examples, linguistically related to the input instance: these can be used to motivate the network output. Quantitative analysis shows that richer explanations about the semantic and syntagmatic structures of the examples characterize more convincing arguments in two tasks, i.e. question classification and semantic role labeling."
W17-2804,Structured Learning for Context-aware Spoken Language Understanding of Robotic Commands,2017,21,0,3,1,10798,andrea vanzo,Proceedings of the First Workshop on Language Grounding for Robotics,0,"Service robots are expected to operate in specific environments, where the presence of humans plays a key role. A major feature of such robotics platforms is thus the ability to react to spoken commands. This requires the understanding of the user utterance with an accuracy able to trigger the robot reaction. Such correct interpretation of linguistic exchanges depends on physical, cognitive and language-dependent aspects related to the environment. In this work, we present the empirical evaluation of an adaptive Spoken Language Understanding chain for robotic commands, that explicitly depends on the operational environment during both the learning and recognition stages. The effectiveness of such a context-sensitive command interpretation is tested against an extension of an already existing corpus of commands, that introduced explicit perceptual knowledge: this enabled deeper measures proving that more accurate disambiguation capabilities can be actually obtained."
P17-1032,Deep Learning in Semantic Kernel Spaces,2017,23,8,4,1,12619,danilo croce,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Kernel methods enable the direct usage of structured representations of textual data during language learning and inference tasks. Expressive kernels, such as Tree Kernels, achieve excellent performance in NLP. On the other side, deep neural networks have been demonstrated effective in automatically learning feature representations during training. However, their input is tensor data, i.e., they can not manage rich structured information. In this paper, we show that expressive kernels and deep neural networks can be combined in a common framework in order to (i) explicitly model structured information and (ii) learn non-linear decision functions. We show that the input layer of a deep architecture can be pre-trained through the application of the Nystrom low-rank approximation of kernel spaces. The resulting {``}kernelized{''} neural network achieves state-of-the-art accuracy in three different tasks."
S16-1172,{K}e{LP} at {S}em{E}val-2016 Task 3: Learning Semantic Relations between Questions and Answers,2016,20,39,4,0.833333,10810,simone filice,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1007,A Language Independent Method for Generating Large Scale Polarity Lexicons,2016,24,0,3,1,10811,giuseppe castellucci,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Sentiment Analysis systems aims at detecting opinions and sentiments that are expressed in texts. Many approaches in literature are based on resources that model the prior polarity of words or multi-word expressions, i.e. a polarity lexicon. Such resources are defined by teams of annotators, i.e. a manual annotation is provided to associate emotional or sentiment facets to the lexicon entries. The development of such lexicons is an expensive and language dependent process, making them often not covering all the linguistic sentiment phenomena. Moreover, once a lexicon is defined it can hardly be adopted in a different language or even a different domain. In this paper, we present several Distributional Polarity Lexicons (DPLs), i.e. large-scale polarity lexicons acquired with an unsupervised methodology based on Distributional Models of Lexical Semantics. Given a set of heuristically annotated sentences from Twitter, we transfer the sentiment information from sentences to words. The approach is mostly unsupervised, and experimental evaluations on Sentiment Analysis tasks in two languages show the benefits of the generated resources. The generated DPLs are publicly available in English and Italian."
P15-4004,{K}e{LP}: a Kernel-based Learning Platform for Natural Language Processing,2015,27,30,4,0.833333,10810,simone filice,Proceedings of {ACL}-{IJCNLP} 2015 System Demonstrations,0,"Kernel-based learning algorithms have been shown to achieve state-of-the-art results in many Natural Language Processing (NLP) tasks. We present KELP, a Java framework that supports the implementation of both kernel-based learning algorithms and kernel functions over generic data representation, e.g. vectorial data or discrete structures. The framework has been designed to decouple kernel functions and learning algorithms: once a new kernel function has been implemented it can be adopted in all the available kernelmachine algorithms. The platform includes different Online and Batch Learning algorithms for Classification, Regression and Clustering, as well as several Kernel functions, ranging from vector-based to structural kernels. This paper will show the main aspects of the framework by applying it to different NLP tasks."
S14-2135,{UNITOR}: Aspect Based Sentiment Analysis with Structured Learning,2014,25,11,4,1,10811,giuseppe castellucci,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In this paper, the UNITOR system participating in the SemEval-2014 Aspect Based Sentiment Analysis competition is presented. The task is tackled exploiting Kernel Methods within the Support Vector Machine framework. The Aspect Term Extraction is modeled as a sequential tagging task, tackled through SVM hmm . The Aspect Term Polarity, Aspect Category and Aspect Category Polarity detection are tackled as a classification problem where multiple kernels are linearly combined to generalize several linguistic information. In the challenge, UNITOR system achieves good results, scoring in almost all rankings between the 2 nd and the 8 th position within about 30 competitors."
bastianelli-etal-2014-huric,{H}u{RIC}: a Human Robot Interaction Corpus,2014,20,25,5,1,10797,emanuele bastianelli,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Recent years show the development of large scale resources (e.g. FrameNet for the Frame Semantics) that supported the definition of several state-of-the-art approaches in Natural Language Processing. However, the reuse of existing resources in heterogeneous domains such as Human Robot Interaction is not straightforward. The generalization offered by many data driven methods is strongly biased by the employed data, whose performance in out-of-domain conditions exhibit large drops. In this paper, we present the Human Robot Interaction Corpus (HuRIC). It is made of audio files paired with their transcriptions referring to commands for a robot, e.g. in a home environment. The recorded sentences are annotated with different kinds of linguistic information, ranging from morphological and syntactic information to rich semantic information, according to the Frame Semantics, to characterize robot actions, and Spatial Semantics, to capture the robot environment. All texts are represented through the Abstract Meaning Representation, to adopt a simple but expressive representation of commands, that can be easily translated into the internal representation of the robot."
C14-1221,A context-based model for Sentiment Analysis in {T}witter,2014,31,44,3,1,10798,andrea vanzo,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Most of the recent literature on Sentiment Analysis over Twitter is tied to the idea that the sentiment is a function of an incoming tweet. However, tweets are filtered through streams of posts, so that a wider context, e.g. a topic, is always available. In this work, the contribution of this contextual information is investigated. We modeled the polarity detection problem as a sequential classification task over streams of tweets. A Markovian formulation of the Support Vector Machine discriminative model as embodied by the SVM hmm algorithm has been here employed to assign the sentiment polarity to entire sequences. The experimental evaluation proves that sequential tagging effectively embodies evidence about the contexts and is able to reach a relative increment in detection accuracy of around 20% in F1 measure. These results are particularly interesting as the approach is flexible and does not require manually coded resources."
W13-3814,Towards Compositional Tree Kernels,2013,26,2,3,0,40818,paolo annesi,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,"English. Several textual inference tasks rely on kernel-based learning. In particular Tree Kernels (TKs) proved to be suitable to the modeling of syntactic and semantic similarity between linguistic instances. In order to generalize the meaning of linguistic phrases, Distributional Compositional Semantics (DCS) methods have been defined to compositionally combine the meaning of words in semantic spaces. However, TKs still do not account for compositionality. A novel kernel, i.e. the Compositional Tree Kernel, is presented integrating DCS operators in the TK estimation. The evaluation over Question Classification and Metaphor Detection shows the contribution of semantic compositions w.r.t. traditional TKs. Italiano. Sono numerosi i problemi di interpretazione del testo che beneficiano dallxe2x80x99applicazione di metodi di apprendimento automatico basato su funzioni kernel. In particolare, i Tree Kernel (TK) sono applicati alla modellazione di metriche di similarita sintattica e semantica tra espressioni linguistiche. Allo scopo di generalizzare i significati legati a sintagmi complessi, i metodi di Distributional Compositional Semantics combinano algebricamente i vettori associati agli elementi lessicali costituenti. Ad oggi i modelli di TK non esprimono criteri di composizionalita. In questo lavoro dimostriamo il beneficio di modelli di composizionalita applicati ai TK, in problemi di Question Classification e Metaphor Detection."
W13-3820,Textual Inference and Meaning Representation in Human Robot Interaction,2013,17,17,4,1,10797,emanuele bastianelli,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,"This paper provides a first investigation over existing textual inference paradigms in order to propose a generic framework able to capture major semantic aspects in Human Robot Interaction (HRI). We investigate the use of general semantic paradigms used in Natural Language Understanding (NLU) tasks, such as Semantic Role Labeling, over typical robot commands. The semantic information obtained is then represented under the Abstract Meaning Representation. AMR is a general representation language useful to express different level of semantic information without a strong dependence to the syntactic structure of an underlying sentence. The final aim of this work is to find an effective synergy between HRI and NLU."
S13-2060,{UNITOR}: Combining Syntactic and Semantic Kernels for {T}witter Sentiment Analysis,2013,26,7,4,1,10811,giuseppe castellucci,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"In this paper, the UNITOR system participating in the SemEval-2013 Sentiment Analysis in Twitter task is presented. The polarity detection of a tweet is modeled as a classification task, tackled through a Multiple Kernel approach. It allows to combine the contribution of complex kernel functions, such as the Latent Semantic Kernel and Smoothed Partial Tree Kernel, to implicitly integrate syntactic and lexical information of annotated examples. In the challenge, UNITOR system achieves good results, even considering that no manual feature engineering is performed and no manually coded resources are employed. These kernels in-fact embed distributional models of lexical semantics to determine expressive generalization of tweets."
S13-2096,{UNITOR}-{HMM}-{TK}: Structured Kernel-based learning for Spatial Role Labeling,2013,17,13,3,1,10797,emanuele bastianelli,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"In this paper the UNITOR-HMM-TK system participating in the Spatial Role Labeling task at SemEval 2013 is presented. The spatial roles classification is addressed as a sequence-based word classification problem: the SVM learning algorithm is applied, based on a simple feature modeling and a robust lexical generalization achieved through a Distributional Model of Lexical Semantics. In the identification of spatial relations, roles are combined to generate candidate relations, later verified by a SVM classifier. The Smoothed Partial Tree Kernel is applied, i.e. a convolution kernel that enhances both syntactic and lexical properties of the examples, avoiding the need of a manual feature engineering phase. Finally, results on three of the five tasks of the challenge are reported."
S13-1007,{UNITOR}-{CORE}{\\_}{TYPED}: Combining Text Similarity and Semantic Filters through {SV} Regression,2013,19,8,3,1,12619,danilo croce,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,"This paper presents the UNITOR system that participated in the *SEM 2013 shared task on Semantic Textual Similarity (STS). The task is modeled as a Support Vector (SV) regression problem, where a similarity scoring function between text pairs is acquired from examples. The proposed approach has been implemented in a system that aims at providing high applicability and robustness, in order to reduce the risk of over-fitting over a specific datasets. Moreover, the approach does not require any manually coded resource (e.g. WordNet), but mainly exploits distributional analysis of unlabeled corpora. A good level of accuracy is achieved over the shared task: in the Typed STS task the proposed system ranks in 1st and 2nd position."
S12-1088,{UNITOR}: Combining Semantic Text Similarity functions through {SV} Regression,2012,18,9,4,1,12619,danilo croce,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"This paper presents the UNITOR system that participated to the SemEval 2012 Task 6: Semantic Textual Similarity (STS). The task is here modeled as a Support Vector (SV) regression problem, where a similarity scoring function between text pairs is acquired from examples. The semantic relatedness between sentences is modeled in an unsupervised fashion through different similarity functions, each capturing a specific semantic aspect of the STS, e. g. syntactic vs. lexical or topical vs. paradigmatic similarity. The SV regressor effectively combines the different models, learning a scoring function that weights individual scores in a unique resulting STS. It provides a highly portable method as it does not depend on any manually built resource (e.g. WordNet) nor controlled, e. g. aligned, corpus."
P12-1028,Verb Classification using Distributional Similarity in Syntactic and Semantic Structures,2012,49,11,3,1,12619,danilo croce,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"In this paper, we propose innovative representations for automatic classification of verbs according to mainstream linguistic theories, namely VerbNet and FrameNet. First, syntactic and semantic structures capturing essential lexical and syntactic properties of verbs are defined. Then, we design advanced similarity functions between such structures, i.e., semantic tree kernel functions, for exploiting distributional and grammatical information in Support Vector Machines. The extensive empirical analysis on VerbNet class and frame detection shows that our models capture meaningful syntactic/semantic structures, which allows for improving the state-of-the-art."
D11-1096,Structured Lexical Similarity via Convolution Kernels on Dependency Trees,2011,66,97,3,1,12619,danilo croce,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"A central topic in natural language processing is the design of lexical and syntactic features suitable for the target application. In this paper, we study convolution dependency tree kernels for automatic engineering of syntactic and semantic patterns exploiting lexical similarities. We define efficient and powerful kernels for measuring the similarity between dependency structures, whose surface forms of the lexical nodes are in part or completely different. The experiments with such kernels for question classification show an unprecedented results, e.g. 41% of error reduction of the former state-of-the-art. Additionally, semantic role classification confirms the benefit of semantic smoothing for dependency kernels."
W10-2304,Robust and Efficient Page Rank for Word Sense Disambiguation,2010,25,11,2,1,45330,diego cao,Proceedings of {T}ext{G}raphs-5 - 2010 Workshop on Graph-based Methods for Natural Language Processing,0,"Graph-based methods that are en vogue in the social network analysis area, such as centrality models, have been recently applied to linguistic knowledge bases, including unsupervised Word Sense Disambiguation. Although the achievable accuracy is rather high, the main drawback of these methods is the high computational demanding whenever applied to the large scale sense repositories. In this paper an adaptation of the PageRank algorithm recently proposed for Word Sense Disambiguation is presented that preserves the reachable accuracy while significantly reducing the requested processing time. Experimental analysis over well-known benchmarks will be presented in the paper and the results confirm our hypothesis."
P10-1025,Towards Open-Domain Semantic Role Labeling,2010,22,20,4,1,12619,danilo croce,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Current Semantic Role Labeling technologies are based on inductive algorithms trained over large scale repositories of annotated examples. Frame-based systems currently make use of the FrameNet database but fail to show suitable generalization capabilities in out-of-domain scenarios. In this paper, a state-of-art system for frame-based SRL is extended through the encapsulation of a distributional model of semantic similarity. The resulting argument classification model promotes a simpler feature space that limits the potential overfitting effects. The large scale empirical study here discussed confirms that state-of-art accuracy can be obtained for out-of-domain evaluations."
de-cao-etal-2010-extensive,Extensive Evaluation of a {F}rame{N}et-{W}ord{N}et mapping resource,2010,15,5,3,1,45330,diego cao,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Lexical resources are basic components of many text processing system devoted to information extraction, question answering or dialogue. In paste years many resources have been developed such as FrameNet and WordNet. FrameNet describes prototypical situations (i.e. Frames) while WordNet defines lexical meaning (senses) for the majority of English nouns, verbs, adjectives and adverbs. A major difference between FrameNet and WordNet refers to their coverage. Due of this lack of coverage, in recent years some approaches have been studied to make a bridge between this two resources, so a resource is used to extend the coverage of the other one. The nature of these approaches leave from supervised to supervised methods. The major problem is that there is not a standard in evaluation of the mapping. Each different work have tested own approach with a custom gold standard. This work give an extensive evaluation of the model proposed in (De Cao et al., 2008) using gold standard proposed in other works. Moreover this work give an empirical comparison between other available resources. As outcome of this work we also release the full mapping resource made according to the model proposed in (De Cao et al., 2008)."
W08-2208,Combining Word Sense and Usage for Modeling Frame Semantics,2008,18,29,4,1,45330,diego cao,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"Models of lexical semantics are core paradigms in most NLP applications, such as dialogue, information extraction and document understanding. Unfortunately, the coverage of currently available resources (e.g. FrameNet) is still unsatisfactory. This paper presents a largely applicable approach for extending frame semantic resources, combining word sense information derived from WordNet and corpus-based distributional information. We report a large scale evaluation over the English FrameNet, and results on extending FrameNet to the Italian language, as the basis of the development of a full FrameNet for Italian."
pennacchiotti-etal-2008-towards,Towards a Vector Space Model for {F}rame{N}et-like Resources,2008,30,5,4,0,44420,marco pennacchiotti,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper, we present an original framework to model frame semantic resources (namely, FrameNet) using minimal supervision. This framework can be leveraged both to expand an existing FrameNet with new knowledge, and to induce a FrameNet in a new language. Our hypothesis is that a frame semantic resource can be modeled and represented by a suitable semantic space model. The intuition is that semantic spaces are an effective model of the notion of Âbeing characteristic of a frameÂ for both lexical elements and full sentences. The paper gives two main contributions. First, it shows that our hypothesis is valid and can be successfully implemented. Second, it explores different types of semantic VSMs, outlining which one is more suitable for representing a frame semantic resource. In the paper, VSMs are used for modeling the linguistic core of a frame, the lexical units. Indeed, if the hypothesis is verified for these units, the proposed framework has a much wider application."
J08-2003,Tree Kernels for Semantic Role Labeling,2008,42,142,3,0.712893,4033,alessandro moschitti,Computational Linguistics,0,"The availability of large scale data sets of manually annotated predicate-argument structures has recently favored the use of machine learning approaches to the design of automated semantic role labeling (SRL) systems. The main research in this area relates to the design choices for feature representation and for effective decompositions of the task in different learning models. Regarding the former choice, structural properties of full syntactic parses are largely employed as they represent ways to encode different principles suggested by the linking theory between syntax and semantics. The latter choice relates to several learning schemes over global views of the parses. For example, re-ranking stages operating over alternative predicate-argument sequences of the same sentence have shown to be very effective.n n In this article, we propose several kernel functions to model parse tree properties in kernel-based machines, for example, perceptrons or support vector machines. In particular, we define different kinds of tree kernels as general approaches to feature engineering in SRL. Moreover, we extensively experiment with such kernels to investigate their contribution to individual stages of an SRL architecture both in isolation and in combination with other traditional manually coded features. The results for boundary recognition, classification, and re-ranking stages provide systematic evidence about the significant impact of tree kernels on the overall accuracy, especially when the amount of training data is small. As a conclusive result, tree kernels allow for a general and easily portable feature engineering method which is applicable to a large family of natural language processing tasks."
D08-1048,Automatic induction of {F}rame{N}et lexical units,2008,22,47,3,0,44420,marco pennacchiotti,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Most attempts to integrate FrameNet in NLP systems have so far failed because of its limited coverage. In this paper, we investigate the applicability of distributional and WordNet-based models on the task of lexical unit induction, i.e. the expansion of FrameNet with new lexical units. Experimental results show that our distributional and WordNet-based models achieve good level of accuracy and coverage, especially when combined."
S07-1062,{RTV}: Tree Kernels for Thematic Role Classification,2007,9,2,3,1,20383,daniele pighin,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We present a simple, two-steps supervised strategy for the identification and classification of thematic roles in natural language texts. We employ no external source of information but automatic parse trees of the input sentences. We use a few attribute-value features and tree kernel functions applied to specialized structured features. The resulting system has an F1 of 75.44 on the SemEval2007 closed task on semantic role labeling."
P07-1098,Exploiting Syntactic and Shallow Semantic Kernels for Question Answer Classification,2007,14,183,3,0.888158,4033,alessandro moschitti,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"We study the impact of syntactic and shallow semantic information in automatic classification of questions and answers and answer re-ranking. We define (a) new tree structures based on shallow semantics encoded in Predicate Argument Structures (PASs) and (b) new kernel functions to exploit the representational power of such structures with Support Vector Machines. Our experiments suggest that syntactic information helps tasks such as question/answer classification and that shallow semantics gives remarkable contribution when a reliable set of PASs can be extracted, e.g. from answers."
W06-2909,Semantic Role Labeling via Tree Kernel Joint Inference,2006,18,50,3,0.967742,4033,alessandro moschitti,Proceedings of the Tenth Conference on Computational Natural Language Learning ({C}o{NLL}-X),0,"Recent work on Semantic Role Labeling (SRL) has shown that to achieve high accuracy a joint inference on the whole predicate argument structure should be applied. In this paper, we used syntactic subtrees that span potential argument structures of the target predicate in tree kernel functions. This allows Support Vector Machines to discern between correct and incorrect predicate structures and to re-rank them based on the joint probability of their arguments. Experiments on the PropBank data show that both classification and re-ranking based on tree kernels can improve SRL systems."
W06-2607,Tree Kernel Engineering in Semantic Role Labeling Systems,2006,19,15,3,0.967742,4033,alessandro moschitti,Proceedings of the Workshop on Learning Structured Information in Natural Language Applications,0,"Recent work on the design of automatic systems for semantic role labeling has shown that feature engineering is a complex task from a modeling and implementation point of view. Tree kernels alleviate such complexity as kernel functions generate features automatically and require less software development for data extraction. In this paper, we study several tree kernel approaches for both boundary detection and argument classification. The comparative experiments on Support Vector Machines with such kernels on the CoNLL 2005 dataset show that very simple tree manipulations trigger automatic feature engineering that highly improves accuracy and efficiency in both phases. Moreover, the use of different classifiers for internal and pre-terminal nodes maintains the same accuracy and highly improves efficiency."
moschitti-basili-2006-tree,A Tree Kernel approach to Question and Answer Classification in Question Answering Systems,2006,10,22,2,0.967742,4033,alessandro moschitti,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"A critical step in Question Answering design is the definition of the models for question focus identification and answer extraction. In case of factoid questions, we can use a question classifier (trained according to a target taxonomy) and a named entity recognizer. Unfortunately, this latter cannot be applied to generate answers related to non-factoid questions. In this paper, we tackle such problem by designing classifiers of non-factoid answers. As the feature design for this learning task is very complex, we take advantage of tree kernels to generate large feature set from the syntactic parse trees of passages relevant to the target question. Such kernels encode syntactic and lexical information in Support Vector Machines which can decide if a sentence focuses on a target taxonomy subject. The experiments with SVMs on the TREC 10 dataset show that our approach is an interesting future research."
W05-1002,Verb Subcategorization Kernels for Automatic Semantic Labeling,2005,14,7,2,0.967742,4033,alessandro moschitti,Proceedings of the {ACL}-{SIGLEX} Workshop on Deep Lexical Acquisition,0,"Recently, many researches in natural language learning have considered the representation of complex linguistic phenomena by means of structural kernels. In particular, tree kernels have been used to represent verbal subcategorization frame (SCF) information for predicate argument classification. As the SCF is a relevant clue to learn the relation between syntax and semantic, the classification algorithm accuracy was remarkable enhanced. In this article, we extend such work by studying the impact of the SCF tree kernel on both PropBank and FrameNet semantic roles. The experiments with Support Vector Machines (SVMs) confirm a strong link between the SCF and the semantics of the verbal predicates as well as the benefit of using kernels in diverse and complex test conditions, e.g. classification of unseen verbs."
W05-0601,Effective use of {W}ord{N}et Semantics via Kernel-Based Learning,2005,22,30,1,1,12620,roberto basili,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"Research on document similarity has shown that complex representations are not more accurate than the simple bag-of-words. Term clustering, e.g. using latent semantic indexing, word co-occurrences or synonym relations using a word ontology have been shown not very effective. In particular, when to extend the similarity function external prior knowledge is used, e.g. WordNet, the retrieval system decreases its performance. The critical issues here are methods and conditions to integrate such knowledge.n n In this paper we propose kernel functions to add prior knowledge to learning algorithms for document classification. Such kernels use a term similarity measure based on the WordNet hierarchy. The kernel trick is used to implement such space in a balanced and statistically coherent way. Cross-validation results show the benefit of the approach for the Support Vector Machines when few training data is available."
W05-0630,Hierarchical Semantic Role Labeling,2005,8,31,4,0.967742,4033,alessandro moschitti,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"We present a four-step hierarchical SRL strategy which generalizes the classical two-level approach (boundary detection and classification). To achieve this, we have split the classification step by grouping together roles which share linguistic properties (e.g. Core Roles versus Adjuncts). The results show that the non-optimized hierarchical approach is computationally more efficient than the traditional systems and it preserves their accuracy."
W05-0407,Engineering of Syntactic Features for Shallow Semantic Parsing,2005,19,12,4,0.967742,4033,alessandro moschitti,Proceedings of the {ACL} Workshop on Feature Engineering for Machine Learning in Natural Language Processing,0,"Recent natural language learning research has shown that structural kernels can be effectively used to induce accurate models of linguistic phenomena.n n In this paper, we show that the above properties hold on a novel task related to predicate argument classification. A tree kernel for selecting the subtrees which encodes argument structures is applied. Experiments with Support Vector Machines on large data sets (i.e. the PropBank collection) show that such kernel improves the recognition of argument boundaries."
W04-2510,Ontological resources and question answering,2004,16,17,1,1,12620,roberto basili,Proceedings of the Workshop on Pragmatics of Question Answering at {HLT}-{NAACL} 2004,0,This paper discusses the possibility of building an ontology-based question answering system in the context of the Semantic Web presenting a proof-of-concept system. The system is under development in the MOSES European Project.
guthrie-etal-2004-large,Large Scale Experiments for Semantic Labeling of Noun Phrases in Raw Text,2004,8,2,2,0,37309,louise guthrie,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,This paper gives a brief overview of the results of our work during the Summer 2003 Workshop of the Center for Language and Speech Processing at the Johns Hopkins University in Baltimore Maryland. The goal of the project was to determine the feasibility of extending named entity recognition to common nouns and determine whether or not it is possible to assign automatically a predetermined set of semantic tags and approach human performance in the task.
basili-etal-2004-a2q,{A}2{Q}: An Agent-based Architecure for Multilingual {Q}{\\&}{A},2004,16,0,1,1,12620,roberto basili,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,In this paper we describe the agent based architecture and extensively report the design of the shallow processing model in it. We present the general model describing the data flow and the expected activities that have to be carried out. The notion of question session will be introduced as a means to control the communication among the different agents. We then present a shallow model mainly based on an IR engine and a passage re-ranking that uses the notion of expanded query. We will report the pilot investigation on the performances of
basili-etal-2004-similarity,A Similarity Measure for Unsupervised Semantic Disambiguation,2004,5,11,1,1,12620,roberto basili,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents an unsupervised method for the resolution of lexical ambiguity of nouns. The method relies on the topological structure of the noun taxonomy of WordNet where a notion of semantic distance is defined. An unsupervised semantic tagger, based on the above measure, is evaluated over an hand-annotated portion of the British National Corpus and compared with a supervised approach based on the Maximum Entropy Model."
J03-4005,"Book Reviews: Learning to Classify Text Using Support Vector Machines: Methods, Theory and Algorithms by Thorsten Joachims; Anaphora Resolution by Ruslan Mitkov",2003,0,2,1,1,12620,roberto basili,Computational Linguistics,0,None
C02-1129,Decision Trees as Explicit Domain Term Definitions,2002,4,1,1,1,12620,roberto basili,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,Terminology Acquisition (TA) methods are viable solutions for the knowledge bottleneck problem that confines knowledge-intensive information access systems (such as Information Extraction systems) to restricted application scenarios. TA can be seen as a way to inspect large text collections for extracting concise domain knowledge. In this paper we argue that major insights over the notion of term can be obtained by investigating a more domain-based term definition. We propose a decision tree learning approach as an interesting model of the human TA activity. An incremental model is proposed to study the evolution of the term definition during the TA process over a particular implicit domain model. The experimental apparatus is based on robust text processing tools that support a large scale investigation. The good results suggest that the proposed automatic TA model can support the development of conceptual domain dictionaries as required by knowledge-based information systems.
W01-1005,Identification of Relevant Terms to Support the Construction of Domain Ontologies,2001,17,69,3,0,17227,paola velardi,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,"Though the utility of domain Ontologies is now widely acknowledged in the IT (Information Technology) community, several barriers must be overcome before Ontologies become practical and useful tools. One important achievement would be to reduce the cost of identifying and manually entering several thousand-concept descriptions. This paper describes a text mining technique to aid an Ontology Engineer to identify the important concepts in a Domain Ontology."
W01-1013,Multilingual Authoring: the {NAMIC} Approach,2001,11,7,1,1,12620,roberto basili,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,"With increasing amounts of electronic information available, and the increase in the variety of languages used to produce documents of the same type, the problem of how to manage similar documents in different languages arises. This paper proposes an approach to processing/structuring text so that Multilingual Authoring (creating hypertext links) can be effectively carried out. This work, funded by the European Union, is applied to the Multilingual Authoring of news agency text. We have applied methods from Natural Language Processing, especially Information Extraction technology, to both monolingual and Multilingual Authoring."
basili-etal-2000-tuning,Tuning Lexicons to New Operational Scenarios,2000,10,0,1,1,12620,roberto basili,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,In this paper the role of the lexicon within typical application tasks based on NLP is analysed. A large scale semantic lexicon is studied within the framework of a NLP application. The coverage of the lexicon with respect the target domain and a (semi)automatic tuning approach have been evaluated. The impact of a corpus-driven inductive architecture aiming to compensate lacks in lexical information are thus measured and discussed.
W98-0711,Automatic Adaptation of {W}ord{N}et to Sublanguages and to Computational Tasks,1998,8,3,1,1,12620,roberto basili,Usage of {W}ord{N}et in Natural Language Processing Systems,0,"Semantically tagging a corpus is useful for many intermediate NLP tasks such as: acquisition of word argument structures in sublanguages, acquisition of syntactic disambiguation cues, terminology learning, etc. Semantic categories allow the generalization of observed word patterns, and facilitate the discovery of irecurrent sublanguage phenomena and selectional rules of various types. Yet, as opposed to POS tags in morphology, there is no consensus in literature about the type and granularity of the category inventory. In addition, most available on-line taxonomies, as WordNet, are over ambiguous and, at the same time, may not include many domain-dependent senses of words. In this paper we describe a method to adapt a general purpose taxonomy to an application sub[anguage: flint, we prune branches of the Wordnet hierarchy that are too  fine grained for the domain: then. a statistical model of classes is built from corpus contexts to sort the different classifications or assign a classification to known and unknown words, respectively. 1 I n t r o d u c t i o n Lexical learning methods based on the use of semantic categories are faced with the problem of overambiguity and entangled structures of Thesaura and dictionaries. WordNet and Roget's Thesaura were not initially conceived, despite their success among researchers in lexical statistics, as tools for automatic language processing. The purpose was rather to provide the linguists with a very refined, general purpose, linguistically motivated source of taxonomic knowledge. As a consequence, in most on-fine Thesaura words are extremely ambiguous. with very subtle distinctions among senses. High ambiguity, entangled nodes, and asymmetry have already been emphasized in (Hearst and Shutze, 1993) as being an obstacle to the effective use of on-line Thesaura in corpus linguistics. In most cases, the noise introduced by overambiguity almost overrides the positive effect of semantic clustering. For example, in (BriIl and Resnik, 1994) clustering PP heads according to WordNet synsets produced only a [% improvement in a PP disambiguation task. with respect to the non-clustered method. A subsequent paper (Resnik. 1997) reports of a 40% precision in a sense disambiguation task. always based on generalization through WordNet synsets. Context-based sense clisambiguation becomes a prohibitive task on a wide-scale basis, because when words in the context of unambiguous word are replaced by their s.vnsets, there is a multiplication of possible contexts, rather than a generalization. [n (Agirre and Rigau. 1996) a method called Conceptual Distance is proposed to reduce this problem, but the reported performance in disambiguation still does not reach 50%. On the other hand, (Dolan. 1994) and (Krovetz and Croft. 1992) claim that fine-grained semantic distinctions are unlikely to be of practical value for many applications. Our experience supports this claim: often, what matters is to be able to distinguish among contrastive (Pustejowsky. 1995) ambiguities of the bank_river bank_organisation flavor. The problem however is that the notion ofcoutrast ive is domain-dependent. Depending upon the sublanguage (e.g. medicine, finance, computers. etc.) and upon the specific NLP application (e.g. Information Extraction, Dialogue etc.) a given semantic label may be too general or too specific for the task at hand. For example, the word line has 27 senses in WordNet. many of which draw subtle distinctions e.g. line of ~cork (sense 26) and line of products (sense [9). In aa"
W97-0314,Inducing Terminology for Lexical Acquisition,1997,10,15,1,1,12620,roberto basili,Second Conference on Empirical Methods in Natural Language Processing,0,None
W97-0211,Towards a Bootstrapping Framework for Corpus Semantic Tagging,1997,12,14,1,1,12620,roberto basili,"Tagging Text with Lexical Semantics: Why, What, and How?",0,Availability of source information for semantic tagging (or disambignating) words in corpora is problematic. A framework to produce a semantically tagged corpus in a domain specific perspective using as source a general purpose taxonomy (i.e. WordNet) is here proposed. The tag set is derived from higher level Wordnet synsets. A methodology aiming to support semantic bootstrapping in a NLP application is defined. Results from large scale experiments are reported 1.
J96-4006,Integrating General-purpose and Corpus-based Verb Classification,1996,8,14,1,1,12620,roberto basili,Computational Linguistics,0,"Le probleme de la generalite des taxonomies lexicales (notamment verbales) est depuis longtemps source de debats en linguistique informatique. Les relations similaires suggerees par la structure thematique des mots dans les phrases dependent en effet fortement du domaine et il est difficile de trouver des invariants communs entre les sous-langages. Les As. analysent ici les relations entre une methode purement inductive, telle que le systeme de regroupement conceptuel CIAULA, base sur des corpus, et une classification de domaine general, encodee a la main, telle que WordNet. Leur objectif est d'identifier leurs points communs et leurs differences et d'examiner la possibilte d'une integration fructueuse des 2 approches"
W94-0102,The Noisy Channel and the Braying Donkey,1994,-1,-1,1,1,12620,roberto basili,The Balancing Act: Combining Symbolic and Statistical Approaches to Language,0,None
A94-1029,Might a semantic lexicon support hypertextual authoring?,1994,9,5,1,1,12620,roberto basili,Fourth Conference on Applied Natural Language Processing,0,"It is common opinion that current hypertextual systems do not allow to express objectively the information content of documents, but only the view of the author. The hyperlink building requires an heavy and highly specialised human intervention: this task is very expensive whenever possible!A different approach, based on NLP methodologies, aiming at automatizing the development of an hypertext, is hereafter proposed. Anchorage points are inferred both from content and structure of documents. A semantic lexicon based on conceptual graph structures is used to guide text understanding. Contextual roles are introduced to model domain specific concepts relevant to the navigation. An off-line activation of useful links has been defined according to explicit user specifications. A simple declarative language (HyDeL) for the definition of such links is availale to the user to create his own views on the document base. HERMES is a prototype system implementing our approach. The paper discusses the semantic processing of a document base and highlights the performance of different hypertextual systems derived by HERMES over different languages and knowledge domains."
W93-0107,Hierarchical Clustering of Verbs,1993,17,2,1,1,12620,roberto basili,Acquisition of Lexical Knowledge from Text,0,This paper addresses the problem of performing a hierarchical cluster analysis on objects that are measured on the same variable on a number of equally spaced points. Such data are typically collected in longitudinal studies or in experiments where electro-physiological measurements are registered (such as EEG or EMG). A generalized inter-object distance measure is defined that takes into account various aspects of the similarity between the functions from which the data are sampled. A mathematical programming procedure is developed for weighting these aspects in such a way that the resulting inter-object distances optimally satisfy the ultrametric inequality. These optimally weighted distances can then be subjected to any existing hierarchical clustering procedure. The new approach is illustrated on an artificial data set and some possible limitations and extensions of the new method are discussed.
A92-1013,Computational Lexicons: the Neat Examples and the Odd Exemplars,1992,24,27,1,1,12620,roberto basili,Third Conference on Applied Natural Language Processing,0,"When implementing computational lexicons it is important to keep in mind the texts that a NLP system must deal with. Words relate to each other in many different, often queer, ways: this information is rarely found in dictionaries, and it is quite hard to be invented a priori, despite the imagination that linguists exhibit at inventing esoteric examples.In this paper we present the results of an experiment in learning from corpora the frequent selectional restrictions holding between content words. The method is based on the analysis of word associations augmented with syntactic markers and semantic tags. Word pairs are extracted by a morphosyntactic analyzer and clustered according to their semantic tags. A statistical measure is applied to the data to evaluate the significance of a detected relation. Clustered association data render the study of word associations more interesting with several respects: data are more reliable even for smaller corpora, more easy to interpret, and have many practical applications in NLP."
