2014.lilt-9.9,W09-2510,0,0.0182342,"ing) headlines collected by Karl Stratos, and rendered into EL by him to demonstrate inferences based on implicatives. A significant point to note is that human readers seem to (unconsciously) generate and internalize the corresponding inferences; this point stands in contrast with the current emphasis in NLP on recognizing textual entailment, i.e., on making boolean judgements where both the premise and the hypothetical conclusion are at hand, so that piecemeal alignments and transformations as well as statistical guesswork can be employed in the task. 7 As we noted in (Stratos et al. 2011), Clausen and Manning (2009) proposed a way of projecting presuppositions in NLog in accord with the plug-hole-filter scheme of Karttunen (1973). In this scheme, plugs (e.g., ‘say’) block all projections, filters (e.g., ‘if-then’) allow only certain ones, and holes (e.g., ‘probably’) allow all. But the approach does not fully handle the e↵ects of discourse context. 54 / Lenhart Schubert . Vatican refused to engage with child sex abuse inquiry (The Dec 11, 2010). . Guardian: A homeless Irish man was forced to eat part of his ear (The Hu↵Post: Feb 18, 2011). . ington Oprah is shocked that President Obama gets no respect (F"
2014.lilt-9.9,N09-1016,0,0.0427345,"Missing"
2014.lilt-9.9,W12-3023,1,0.903673,"Missing"
2014.lilt-9.9,W11-2408,1,0.89998,"Missing"
2014.lilt-9.9,W10-0724,1,0.909532,"Missing"
2014.lilt-9.9,P08-1067,0,0.0256033,"f verbal predicates more generally) depend on the types of the arguments. When persons wander, they physically move about; when eyes wander, it is the target of the gaze that shifts; when minds wander, it is the subject matter contemplated that shifts. An alternative to this position is that argument patterns, potentially in combination with other contextual cues, disambiguate the sense of the verb, and it is the disambiguated senses that carry the preceding entailments. 64 / Lenhart Schubert (This view seems in principle compatible with feature-based discriminative parsing models such as in (Huang 2008), though the features in the latter involve specific word and structural patterns, rather than any meaningful semantic patterns.) From this perspective, conformity with a particular abstract syntax is just one factor in the final analysis of a sentence; recognition of stock phrases, idioms, and patterns of predication and modifications are powerfully influential as well. The second application of general factoids, which we have been actively exploring, is as a starting point for generating inference-enabling, quantified knowledge, as discussed in the following subsection. 5.3 Acquiring inferen"
2014.lilt-9.9,S12-1020,0,0.020121,"high temperatures, that it is prized as material for jewelry, and so on. This observation ties in with our comments below on the need for “object-oriented” knowledge in verb-based inference. Axiomatizing implicative verbs: As already noted in our reference to Karl Stratos’ work, we have undertaken a knowledge engineering effort to collect factive and implicative verbal predicates (along with some antifactive and belief- or want-implying ones) and axiomatize them for use in Epilog. The 250 entries were gleaned from various sources and expanded via VerbNet, thesauri, etc. But as pointed out by Karttunen (2012), there are also numerous phrasal implicatives, such as make a futile attempt to, make no e↵ort to, take the trouble to, use the opportunity to, or fulfill one’s duty to. It is doubtful that such cases can be adequately treated by an enumerative approach; rather it appears that multiple items of lexical knowledge will need to be used in concert to derive the desired entailments or implicatures. Axiomatizing Verbnet classes: After an initial attempt to directly render VerbNet semantic annotations into Epilog axioms, which led to generally weak and often flawed axioms, we switched to a more meti"
2014.lilt-9.9,C08-1066,0,0.187189,"inferences, but beyond that, also support inferences that require a combination of lexical knowledge and world knowledge. However, broad language understanding and commonsense reasoning are still thwarted by the “knowledge acquisition bottleneck”, and we summarize some of our ongoing and contemplated attacks on that persistent difficulty. 1 Introduction: Natural Logic, Episodic Logic, and Natural Logic (NLog, as we will abbreviate it) provides an attractive framework for implementing many “obvious” entailment inferences, starting with little more than a syntactically analyzed sentence (e.g., MacCartney and Manning 2008). But in itself, NLog is certainly insufficient for language understanding or commonsense inference. We advocate here a formalized (not necessarily fully disambiguated) representation that is very close to language, along with an inference methodology that easily accommodates NLog inferences, but beyond that enables more general reasoning based on both lexical knowledge and world knowledge. This approach to representation and inference has been pursued by our group at the University of Rochester (and 1 University of Rochester LiLT Volume 9 Perspectives on Semantic Representations for Textual I"
2014.lilt-9.9,W09-3714,0,0.18016,"Missing"
2014.lilt-9.9,W06-3907,0,0.257248,"e-of x])]) From this Epilog would easily infer that there is no member of the CSC faculty named Alan Turing. Incidentally, contrary to the notion that high expressivity entails low efficiency, experimental application of Epilog to large-scale theorem proving in first-order logic showed it to be competitive with the best theorem provers, especially for relatively NLog-like Inference and Commonsense Reasoning / 51 large axiom bases (Morbini and Schubert 2009). 3 NLog-like inference in The essential ideas behind NLog are the following (e.g., van Benthem 1991, 2007; Valencia 1991; van Eijck 2005; Nairn et al. 2006; MacCartney and Manning 2008): 1. Starting with a syntactically structured natural language sentence, we can replace phrases by more general [more specific] ones in positive- [negative-] polarity environments; identity or equivalence replacements are permissible in both types of environments as well as in (transparent) environments that are neither upward nor downward entailing; e.g., Several trucks are on their way ! Several vehicles are on their way; If a vehicle is on its way, turn it back ! If a truck is on its way, turn it back 2. We exploit implicatives/factives; e.g., X manages to do Y"
2014.lilt-9.9,W12-0803,1,0.833514,"Missing"
2014.lilt-9.9,J91-4003,0,0.0703963,"e bottle, a mouth, a briefcase, a store, a festive event, etc. Our assumption is that capturing such entailments will require an “object-oriented” approach, i.e., one that draws upon the properties and methods associated with particular object types. We know the physical form, kinematics, dynamics, and function of doors, books, wine bottles, and so forth, and the use of a verb like open with a particular type of object seems to align the entailments of the verb with the known potentialities for that type of object. The work of James Pustejovsky and his collaborators on the Generative Lexicon (Pustejovsky 1991) is clearly relevant here. For example, he points out the di↵erent interpretations of the verb use in phrases such as use the new knife (on the turkey), use soft contact lenses, use unleaded gasoline (in a car), use the subway, etc., where these interpretations depend on the “telic qualia” of the artifacts referred to, i.e., their purpose and function. 5.2 Acquiring pattern-like world knowledge A project we initiated more than 10 years ago is dubbed Knext (General KNowledge EXtraction from Text). Unlike KE e↵orts that glean specific facts about named entities from textual data – birth dates, o"
2014.lilt-9.9,P84-1054,1,0.323819,"Missing"
2014.lilt-9.9,E09-1092,1,0.903183,"Missing"
2014.lilt-9.9,W08-2219,1,0.913307,"Missing"
2020.sigdial-1.16,W19-0402,1,0.807495,"eprocessing the input. However, it is only used for handling casual aspects of dialogue such as greetings, and for “tidying up” some inputs in preparation for further processing. Additional regularization is done with a limited coreference module, which can resolve anaphora and referring expressions such as “it”, “that block”, etc., by detecting and storing discourse entities in context and employing recency and syntactic salience heuristics. This allows Eta to answer some simple follow-up questions like “Where is it now?” From the tidied-up inputs, Eta derives an unscoped logical form (ULF) (Kim and Schubert, 2019). ULF is closely related to the logical syntax used in schemas – it is a preliminary form of that syntax, when mapping English to logic. ULF differs from analogs, e.g., AMR, in that it is close to the surface form of English, covers a richer set of semantic phenomena, and does so in a typeconsistent way. For example, ULF for the sentence “Which blocks are on two other blocks?” will be (((Which.d (plur block.n)) ((pres be.v) (on.p (two.d (other.a (plur block.n)))))) ?). Resulting ULF retains much of the surface structure, but uses semantic typing and adds operators to indicate plurality, tense,"
2020.sigdial-1.16,N19-1058,0,0.0284134,"alistic understanding of spatial relations. Modern efforts in blocks worlds include work by Perera et al. (Perera et al., 2018), which is focused on learning spatial concepts (such as staircases, towers, etc.) based on verbally-conveyed structural constraints, e.g., “The height is at most 3”, as well as explicit examples and counterexamples, given by the user. Bisk et al. (Bisk et al., 2018) use deep learning to transduce verbal instructions into block displacements in a simulated environment. Some deep learning based studies achieve nearperfect scores on the CLEVR question answering dataset (Kottur et al., 2019; Mao et al., 2019). Common limitation of these approaches is reliance on unrealistically simple spatial models and domainspecific language formalisms, and in relation to our 128 Proceedings of the SIGdial 2020 Conference, pages 128–131 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics work, there is no question answering functionality or episodic memory. Our work is inspired by the psychologically and linguistically oriented studies (Garrod et al., 1999; Herskovits, 1985; Tyler and Evans, 2003). Studies of human judgements of spatial relations show that no"
2020.sigdial-1.16,W18-1402,0,0.018509,"onents in order to perform a simple task of spatial question answering about block configurations. Our goal is dialogue-based question answering about spatial configurations of blocks on a table, Related Work Early studies featuring the blocks world include (Winograd, 1972) and (Fahlman, 1974), both of which maintained symbolic memory of blocksworld states. They demonstrated impressive planning capabilities, but their worlds were simulated, interaction was text-based, and they lacked a realistic understanding of spatial relations. Modern efforts in blocks worlds include work by Perera et al. (Perera et al., 2018), which is focused on learning spatial concepts (such as staircases, towers, etc.) based on verbally-conveyed structural constraints, e.g., “The height is at most 3”, as well as explicit examples and counterexamples, given by the user. Bisk et al. (Bisk et al., 2018) use deep learning to transduce verbal instructions into block displacements in a simulated environment. Some deep learning based studies achieve nearperfect scores on the CLEVR question answering dataset (Kottur et al., 2019; Mao et al., 2019). Common limitation of these approaches is reliance on unrealistically simple spatial mod"
2020.sigdial-1.16,W18-1403,1,0.835378,"unrealistically simple spatial models and domainspecific language formalisms, and in relation to our 128 Proceedings of the SIGdial 2020 Conference, pages 128–131 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics work, there is no question answering functionality or episodic memory. Our work is inspired by the psychologically and linguistically oriented studies (Garrod et al., 1999; Herskovits, 1985; Tyler and Evans, 2003). Studies of human judgements of spatial relations show that no crisp, qualitative models can do justice to those judgments. The study (Platonov and Schubert, 2018) explored computational models for prepositions using imagistic modeling, akin to the current work. Another study (Bigelow et al., 2015) applied imagistic approach to a story understanding task and employed Blender to create 3D scenes and reason about the relative configuration and visibility of objects in the scene. 3 Figure 1: The blocks world apparatus setup. Blocks World System Overview Fig. 1, 2 depict our physical blocks world (consisting of a square table with several cubical blocks, two Kinect sensors and a display) and the system’s software architecture1 . The blocks are color-coded a"
2020.sigdial-1.16,H89-1033,0,0.257516,"s implementation of a diverse range of capabilities in a virtual interactive agent aware of physical blocks on a table, including visual scene analysis, spatial reasoning, planning, learning of new concepts, dialogue management and voice interaction, and more. In this work, we describe an end-to-end system that integrates several such components in order to perform a simple task of spatial question answering about block configurations. Our goal is dialogue-based question answering about spatial configurations of blocks on a table, Related Work Early studies featuring the blocks world include (Winograd, 1972) and (Fahlman, 1974), both of which maintained symbolic memory of blocksworld states. They demonstrated impressive planning capabilities, but their worlds were simulated, interaction was text-based, and they lacked a realistic understanding of spatial relations. Modern efforts in blocks worlds include work by Perera et al. (Perera et al., 2018), which is focused on learning spatial concepts (such as staircases, towers, etc.) based on verbally-conveyed structural constraints, e.g., “The height is at most 3”, as well as explicit examples and counterexamples, given by the user. Bisk et al. (Bisk"
2021.reinact-1.8,W19-0402,1,0.810554,", the DM uses hierarchical pattern transduction, similarly to the mechanism used by the LISSA system (Razavi et al., 2017) to extract context-independent gist clauses given the prior utterance. Transduction hierarchies specify patterns at their nodes to be matched to input, with terminal nodes providing result templates, or specifying a subschema. The pattern templates look for particular words or word features (including “wildcards” matching any word sequence of some length). Eta uses gist clause extraction for tidying-up the user’s utterance, and then derives an unscoped logical form (ULF) (Kim and Schubert, 2019) (a preliminary form of the episodic logic syntax of the dialogue schema) from the tidied-up input. ULF differs from similar semantic representations, e.g., AMR, in that it is close to the surface form of English, type-consistent, and covers a rich set of phenomena. To derive ULFs, we introduced semantic composition into the transduction trees. The resulting parser is quite efficient and accurate for the domain at hand. The input is recursively broken into constituents, such as a VP segment, until a lexical subroutine supplies ULFs for individual words, which are propagated back up and compose"
2021.reinact-1.8,W18-1403,1,0.837959,"rious additional blocks to generate explanations, e.g., in the form of plain English text, based directly on the model’s inner state. Our approach to spatial preposition modeling is inspired by the criteria that have been discussed in linguistically oriented studies (Garrod et al., 1999; Herskovits, 1985; Tyler and Evans, 2003). Studies of human judgements of spatial relations show that overly formal qualitative models with sharp boundaries generally cannot do justice to the usage of locative expressions in natural settings. Our models are implemented along the same general lines as those in (Platonov and Schubert, 2018) and (Richard-Bollans et al., 2020b,a). These studies model prepositions as constructed from more basic physico-geometrical primitives. Modern neural work on spatial relation-learning in the BW domain is exemplified by (Bisk et al., 2018). 3 (a) Blocks world setup (b) Dialogue pipeline Blocks World System and Eta Dialogue Manager Figure 1: System overview. Fig. 1a, 1b depict our physical blocks world (consisting of a square table with several cubical blocks, two Kinect sensors and a display) and the system’s software architecture. The blocks are color-coded as green, red, or blue, and marked w"
2021.splurobonlp-1.4,D14-1217,0,0.0332522,"cceptability is a three dimensional rectangular region (more precisely, a prism with a rectangular base) representing the set of points for which the given spatial relation holds. For example if one has a pair of two objects, A and B, and wants to determine whether A is on top of B, A is checked to determine whether it is in the region of acceptability located directly above B. Probabilistic reasoning is supported by using values from 0 to 1 to represent the portion of the relatum that falls into a particular region of acceptability. Other noteworthy recent examples of datasetdriven work are (Chang et al., 2014) and (Yu and Siskind, 2017). The former inverts the learning problem, in a sense; the task was not to learn how to describe object relationships, but rather to automatically generate a scene based on a textual description. The latter employed models of spatial relations to locate and identify similar objects in several video streams. We should separately mention the spatial modelling studies by Malinowski and Fritz (2014) and, especially, Collell et al. (2017), which apply deep neural networks to learning spatial templates for triplets of form (relatum, relation, referent). The latter work doe"
2021.splurobonlp-1.4,W18-1403,1,0.746464,"tter block” into block displacements in a simulated environment. This system, unlike ours, relies on deep learning and does not use high-level 34 understanding task, allowing reasoning about the relative configuration and visibility of objects in the scene. Another example of an imagistic reasoning system was implemented as part of the planning system for the robot Ripley (Roy et al., 2004). Ripley used three-dimensional representation of its body, operator and workspace, reconstructed from two-dimensional view coming from Ripley’s cameras. Our work is very similar in spirit and execution to (Platonov and Schubert, 2018) and (RichardBollans et al., 2020b,a). All these studies model prepositions using specially designed 3D environments in Blender or Unity and employ similar sets of metrics to define the meaning of the prepositions. The studies by Platonov & Schubert differ from the present work in that the rules were less flexible (fewer parameters) and parameter values were hand-adjusted, while in our work we use gradient descent-based optimization to learn optimal values. The studies by Richard-Bollans ete al. relied on the prototype and exemplar approaches, using learning from data to estimate the prototype"
2021.splurobonlp-1.4,N19-1058,0,0.0264937,"atial configurations of objects. While mathematically appealing and facilitating rigorous inference, these qualitative methods are too strict and unable to capture the semantic richness of natural language descriptions of spatial configurations of objects, since they neglect aspects such as orientation, size, shape, and argument types. cognitively-motivated spatial relation models. The CLEVR dataset (Johnson et al., 2017) and its modified versions, such as (Liu et al., 2019), lays out an explicit spatial question answering challenge that has inspired a flurry of visual reasoning works, e.g., (Kottur et al., 2019) and (Mao et al., 2019), which achieves near-perfect scores on the CLEVR questions. Common shortcomings of these approaches are reliance on synthetic data of limited variety (only a few simple geometric shapes are present), two-dimensional image-based model of the world, very limited ground-truth models of spatial relations (e.g., left means any amount laterally to the left, regardless of depth or intervening objects, etc.), and use of domain-specific procedural formalisms for linguistic semantics. Conceptually, the way we define the spatial relations in our model is similar to the spatial tem"
C08-1116,H05-1071,0,0.026429,"Missing"
C08-1116,W04-3221,0,0.126528,"e generation of abstracted logical forms through compositional linguistic analysis. The following provides an overview of K NEXT and its target knowledge representation, Episodic Logic. 2.1 Episodic Logic Automatically acquiring general world knowledge from text is not a task that provides an immediate solution to any real world problem.2 Rather, the motivation for acquiring large stores of background knowledge is to enable research within other areas of artificial intelligence, e.g., the construction of systems that can engage in dialogues about everyday topics in unrestricted English, use 1 Almuhareb and Poesio (2004) treat unary attributes as values of binary attributes; e.g., illegal might be the value of a legality attribute. But for many unary attributes, this is a stretch. 2 Unless one regularly needs reminding of facts such as, A WOMAN MAY BOIL A GOAT. 921 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 921–928 Manchester, August 2008 (Some e0: [e0 at-about Now0] [(Many.det x : [x ((attr athletic.a) (plur youngster.n))] [x want.v (Ka (become.v (plur ((attr professional.a) athlete.n))))]) ** e0]) Figure 1: Example EL formula; square brackets indicate"
C08-1116,P97-1003,0,0.035938,"r the class Drug. We extracted such unary attributes by focusing on verbalizations of the form, A(N) X CAN BE Y as in AN ANIMAL CAN BE WARM-BLOODED. 3 3.1 Experimental Setting Corpus Processing Initial reports on the use of K NEXT were focused on the processing of manually created parse trees, on a corpus of limited size (the Brown corpus of Kucera and Francis (1967)). Since that time the system has been modified into a fully automatic extraction system, making use of syntactic parse trees generated by parsers trained on the Penn Treebank. For our studies here, the parser employed was that of Collins (1997) applied to the sentences of the British National Corpus (BNC Consortium, 2001). Our choice of the BNC was motivated by its breadth of genre, its substantial size (100 million words) and its familiarity (and accessibility) to the community. 3.2 p/a ratio (r) r ≥ .06 0 &lt; r &lt; .06 otherwise Gazetteers KNEXT’s gazetteers were used as-is, and were defined based on a variety of sources: miscellaneous publicly available lists, as well as manual enumeration. The classes covered can be seen in the Results section in table 2, where the minimum, maximum and mean size were 2, 249, and 41, respectively. 3."
C08-1116,N03-1011,0,0.0210997,"ized propositions, e.g.: A CANDY-COMPANY MAY HAVE BARS, and A CAR-COMPANY MAY HAVE CUSTOMERS. These examples point to additional areas for improvement beyond sense disambiguation: noncompositional phrase filtering for all NPs, rather than just in the cases of adjectival modification (Mars bar is a Wikipedia topic); and relative discounting of patterns used in the extraction process4 . This later technique is commonly used in specialized extraction systems, such as constructed by Snow et al. (2005) who fit a logistic regression model for hypernym (X is-a Y) classification based on WordNet, and Girju et al. (2003) who trained a classifier to look specifically for part-whole relations. 4 For example, (NP (NNP X) (NNS Y)) may be more semantically ambiguous than, e.g., the possessive construction (NP (NP (NNP X) (POS ’s)) (NP (NNS Y))). 4.2 Unary Attributes Table 4 shows how filtering non-compositional phrases from CAN BE propositions affects extraction volume. Table 5 shows the difference between such post-filtered propositions and those that were deleted. As our filter lists were not built fully automatically, evaluation was performed exclusively by an author with negligible direct involvement in the li"
C08-1116,P99-1041,0,0.0864005,"Missing"
E09-1092,W97-0209,0,0.0865147,"ierarchy (Fellbaum, 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure. In the work discussed here we explore the potential of WordNet as an underlying concept hierarchy on which to base generalization decisions. The use of WordNet raises the challenge of dealing with multiple semantic concepts associated with the same word, i.e., employing WordNet requires word sense disambiguation in order to associate terms observed in text with concepts (synsets) within the hierarchy. In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al. (2007).2 Others within the knowledge acquisition community have favored taking the first, most dominant sense of each word (e.g., see Suchanek et al. (2007) and Pas¸ca (2008)). As will be seen, our algorithm does not select word senses prior to generalizing them, but rather as a byproduct of the abstraction process. Moreover, it potentially selects multiple senses of a word deemed equally appropriate in a given context, and in that sen"
E09-1092,E95-1016,0,0.15787,"Missing"
E09-1092,A00-2018,0,0.00905794,"UAL) HAVE[V] (:Q DET ROOM[N])) (:I (:Q DET FEMALE-INDIVIDUAL) SLEEP[V]) (:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET (:F PLUR CLOTHE[N]))) (:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A])) Here the upper-case sentences are automatically generated verbalizations of the abstracted LFs shown beneath them.1 The initial development of K NEXT was based on the hand-constructed parse trees in the Penn Treebank version of the Brown corpus, but subsequently Schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers (e.g., that of Collins (1997) or Charniak (2000)) applied to larger corpora, such as the British National Corpus (BNC), a 100 million-word, mixed genre collection, along with Web corpora of comparable size (see work of Van Durme et al. (2008) and Van Durme and Schubert (2008) for details). The BNC yielded over 2 3.2 Propositional Templates While the procedure given here is not tied to a particular formalism in representing semantic con1 Keywords like :i, :q, and :f are used to indicate infix predication, unscoped quantification, and function application, but these details need not concern us here. 2 809 Personal communication function S COR"
E09-1092,J02-2003,0,0.126698,"Missing"
E09-1092,P97-1003,0,0.0353813,"DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET ROOM[N])) (:I (:Q DET FEMALE-INDIVIDUAL) SLEEP[V]) (:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET (:F PLUR CLOTHE[N]))) (:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A])) Here the upper-case sentences are automatically generated verbalizations of the abstracted LFs shown beneath them.1 The initial development of K NEXT was based on the hand-constructed parse trees in the Penn Treebank version of the Brown corpus, but subsequently Schubert and collaborators refined and extended the system to work with parse trees obtained with statistical parsers (e.g., that of Collins (1997) or Charniak (2000)) applied to larger corpora, such as the British National Corpus (BNC), a 100 million-word, mixed genre collection, along with Web corpora of comparable size (see work of Van Durme et al. (2008) and Van Durme and Schubert (2008) for details). The BNC yielded over 2 3.2 Propositional Templates While the procedure given here is not tied to a particular formalism in representing semantic con1 Keywords like :i, :q, and :f are used to indicate infix predication, unscoped quantification, and function application, but these details need not concern us here. 2 809 Personal communica"
E09-1092,W03-0902,1,0.522822,"ntial source of such knowledge. However, current natural language understanding (NLU) methods are not general and reliable enough to enable broad assimilation, in a formalized representation, of explicitly stated knowledge in encyclopedias or similar sources. As well, such sources typically do not cover the most obvious facts of the world, such as that ice cream may be delicious and may be coated with chocolate, or that children may play in parks. Methods currently exist for extracting simple “factoids” like those about ice cream and children just mentioned (see in particular (Schubert, 2002; Schubert and Tong, 2003)), but these are quite weak as general claims, and – being unconditional Proceedings of the 12th Conference of the European Chapter of the ACL, pages 808–816, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 808 corpus statistics, concluding that future endeavors exploring knowledge extraction and WordNet should go beyond the heuristics employed in recent work. 2 factoids per sentence on average, resulting in a total collection of several million. Human judging of the factoids indicates that about 2 out of 3 factoids are perceived as reasonable claims."
E09-1092,P08-2047,0,0.0438215,"Missing"
E09-1092,J98-2002,0,0.652414,", 1998) continues to be the resource of choice for many computational linguists requiring an ontology-like structure. In the work discussed here we explore the potential of WordNet as an underlying concept hierarchy on which to base generalization decisions. The use of WordNet raises the challenge of dealing with multiple semantic concepts associated with the same word, i.e., employing WordNet requires word sense disambiguation in order to associate terms observed in text with concepts (synsets) within the hierarchy. In their work on determining selectional preferences, both Resnik (1997) and Li and Abe (1998) relied on uniformly distributing observed frequencies for a given word across all its senses, an approach later followed by Pantel et al. (2007).2 Others within the knowledge acquisition community have favored taking the first, most dominant sense of each word (e.g., see Suchanek et al. (2007) and Pas¸ca (2008)). As will be seen, our algorithm does not select word senses prior to generalizing them, but rather as a byproduct of the abstraction process. Moreover, it potentially selects multiple senses of a word deemed equally appropriate in a given context, and in that sense provides coarse-gra"
E09-1092,C02-1105,0,0.250617,"Missing"
E09-1092,W08-2212,0,0.0411382,"Missing"
E09-1092,W08-2219,1,0.845166,"Missing"
E09-1092,W04-0837,0,0.011971,"ding situations in the world; they simply tell us what is generally possible and worth mentioning. 5 5.1 Experiments Tuning to WordNet Our method as described thus far is not tied to a particular word sense taxonomy. Experiments reported here relied on the following model adjustments in order to make use of WordNet (version 3.0). The function P was set to return the union of a synset’s hypernym and instance hypernym relations. Regarding the function L , WordNet is constructed such that always picking the first sense of a given nominal tends to be correct more often than not (see discussion by McCarthy et al. (2004)). To exploit this structural bias, we employed a modified version of L that results in a preference for nodes corresponding to the first sense of words to be covered, especially when the number of distinct observations were low (such as earlier, with crash helmet): Non-reliance on Frequency As can be observed, our approach makes no use of the relative or absolute frequencies of the words in W , even though such frequencies could be added as, e.g., relative weights on length in S CORE. This is a purposeful decision motivated both by practical and theoretical concerns. Practically, a large port"
E09-1092,C08-1116,1,0.894287,"Missing"
E09-1092,P06-1014,0,0.0128233,"ife or mistress or girlfriend) in the life of a particular man; ”he was faithful to his woman” 3 WOMAN : a human female employed to do housework; ”the char will clean the carpet”; ”I have a woman who comes in four hours a day while I write” *4 WOMAN : women as a class; ”it’s an insult to American womanhood”; ”woman is the glory of creation”; ”the fair sex gathered on the veranda” Figure 2: Example of a context and senses provided for 7 Allowing for multiple fine-grained senses to be judged as appropriate in a given context goes back at least to Sussna (1993); discussed more recently by, e.g., Navigli (2006). evaluation, with the fourth sense being judged as inappropriate. 813 If something is famous, it is probably a person1 , an artifact1 , or a communication2 If ? writes something, it is probably a communication2 If a person is happy with something, it is probably a communication2 , a work1 , a final result1 , or a state of affairs1 If a fish has something, it is probably a cognition1 , a torso1 , an interior2 , or a state2 If something is fast growing, it is probably a group1 or a business3 If a message undergoes something, it is probably a message2 , a transmission2 , a happening1 , or a crea"
E09-1092,N07-1071,0,\N,Missing
E09-1092,W08-2208,0,\N,Missing
H90-1008,J88-2002,0,0.0462281,"iginal English text. Thus we will have some assurance that inferences based on the MR will be those intuitively warranted. With respect to this schema, most previous studies of tense and aspect have been more formally explicit about (~) than about (~). The MR is often some dialect of firstorder logic, or a modal or intensional logic, with formal semantics in the traditional vein (e.g., possible-worlds models with multiple time indices). The description of (~), however, is often very informal or limited to a very restricted set of constructions. Furthermore, the formalized accounts (e.g., [4], [6], [14]) tend to take context for granted - the focus is on truth conditions, and so the model is simply assumed to supply the needed reference times. AI-motivated work on tense and aspect has emphasized their pragmatic significance, e.g., for discourse segment structure (see [1], cA. 13 & 14), and their role in the practical extraction of temporal relationships from discourse in applied NL systems (e.g., see several of the articles in Computational Linguistics 11(2), 1988). Much of this work has treated (~) heuristically, restricting attention to simple types of sentences and viewing tense and"
H90-1008,J88-2003,0,0.32509,"he orients relationship in 4th stage (italicized) sample result. This is intended to echo Leech&apos;s notion of a &quot;point of orientation&quot; (as cited in [19]). We refer to this relation, and certain others derived from context (including has-possible-antecedents above) as context-charged. The idea is that their meaning is to be &quot;discharged&quot; using uncertain (probabilistic) inference. For instance, the fact that e0 in the example orients (serves as point of orientation for) e suggests among other possibilities that e immediately follows e0 (in e0&apos;s &quot;consequent&quot; or &quot;result&quot; phase, in the terminology of [10] or [11]). Given the assumed types of e0 (Mary and Sue being surrounded by their fans) and e (a man kissing Mary), this is a very plausible inference, but in other cases the most plausible conclusion from the orients relation may be a subepisode relation (&quot;It was a great party. Mary did her Chef impression&quot;), an explanatory relation (&quot;John went shopping. Mary&apos;s birthday was in a week&quot;), or any of the discourse relations that have been discussed in the literature (e.g., [7]). The de-indexicalization stage is followed by inference stages which discharge context-charged relations and more general"
H90-1008,J88-2005,0,0.183795,"ts relationship in 4th stage (italicized) sample result. This is intended to echo Leech&apos;s notion of a &quot;point of orientation&quot; (as cited in [19]). We refer to this relation, and certain others derived from context (including has-possible-antecedents above) as context-charged. The idea is that their meaning is to be &quot;discharged&quot; using uncertain (probabilistic) inference. For instance, the fact that e0 in the example orients (serves as point of orientation for) e suggests among other possibilities that e immediately follows e0 (in e0&apos;s &quot;consequent&quot; or &quot;result&quot; phase, in the terminology of [10] or [11]). Given the assumed types of e0 (Mary and Sue being surrounded by their fans) and e (a man kissing Mary), this is a very plausible inference, but in other cases the most plausible conclusion from the orients relation may be a subepisode relation (&quot;It was a great party. Mary did her Chef impression&quot;), an explanatory relation (&quot;John went shopping. Mary&apos;s birthday was in a week&quot;), or any of the discourse relations that have been discussed in the literature (e.g., [7]). The de-indexicalization stage is followed by inference stages which discharge context-charged relations and more generally do in"
H90-1008,J82-1003,1,0.840167,"istently with our compositional view of tense and aspect. The way the past perfect auxiliary connects up with a pre-existing &quot;point of orientation&quot; is not by direct anaphoric reference, but by a mechanism that applies to all statives. II. A Formal View of the Understanding Process The following illustrates our current view of the stages of the understanding process, at a theoretical level. (At the procedural level these stages are interleaved.) This view incorporates ideas from GPSG, HPSG, DRT and situation semantics, and evolved in the course of our own research on mapping English into logic [17], [18] and story understanding [15], [16]. 0. Assume that an episode e0 has just been described. e0: After their doubles victory, Mary and Sue were surrounded by their fans. N e w s e n t e n c e : A man kissed Mary 1. G P S G p a r s e r : [s [NP [Det a] [N man]] [vP Iv kissed] [NP Mary]]] 2. R u l e - b y - r u l e L F c o m p u t a t i o n : [<3 m a n &gt; <past kiss&gt; Mary] 3. Scoping: (past (3:e: [z man][x kiss Mary])) 4. D e - i n d e x i c a l i z a t i o n (using context structure C): (3e:[[e before Now3] A [e0 orients e]] [(3x:[z man][x kiss Mary]) ** e]) 5. P r a g m a t i c i n f e r e"
H90-1008,J88-2006,0,0.321585,"es make definite or indefinite reference to episodes (or times)? Davidson[3], Harman[5], Reichenbach[13] and a majority of writers after them took event reference in simple past and present to be indefinite (existential), and this seems in accord with intuition in such sentences as (6) John got married last year. where there is no definite commitment as to the time of the event (apart from its being confined to last year). On the other hand, Partee[12] pointed out the seemingly anaphoric character of tense reference in sentences like (7) I left the stove on. Also, Leech[8], McCawley[9], Webber[19] and many others have noted that the simple past normally involves a &quot;point of orientation&quot; such as an immedi(1) John grinned at Mary. She frowned. (2) John knocked, but soon realized Mary had left. Each of the verbs in (1) appears to introduce an episode; the past inflection places them before the utterance event (or speech time), and the surface ordering suggests temporal sequencing, and probably a causal connection. In (2) we appear to have at least three episodes, a &quot;knocking,&quot; a &quot;realizing&quot; and a &quot;leaving.&quot; The past perfect auxiliary relates the &quot;leaving&quot; to a past episode serving as refe"
H93-1027,J88-2002,0,0.0248245,"scope of the quantifier No and the adverbial for a week). Note now that in (9a), the inner durative adverbial for eight hours transforms the unbounded VP to a bounded one. Being another durative adverbial, however, the outer for a week requires that its argument be unbounded. This is not a problem as shown in ELFs (9b, c, d). That is, in (9b), the argument is a negated formula which is normally considered to be stative-unbounded, and in (9c) and (9d), the iter operator produces stative-unbounded formulas. 4. C O N C L U S I O N Much theoretical work has been done on temporal adverbials (e.g., [4, 5, 7, 14, 16, 19]). There is also some computationally oriented work. For instance, H0bbs [8] provided simple rules for some temporal adverbials, including frequency ones. Moens and Steedman [15], among others, discussed the interaction of adverbials and aspectual categories. Our work goes further, in terms of (1) the scope of syntactic coverage, (2) interaction of adverbials with each other and with tense and aspect, (3) systematic (and compositional) transduction from syntax to logical form (with logical-form deindexing), (4) formal interpretability of the resulting logical forms, and (5) demonstrable use of"
H93-1027,P85-1008,0,0.0246361,"ative adverbial for eight hours transforms the unbounded VP to a bounded one. Being another durative adverbial, however, the outer for a week requires that its argument be unbounded. This is not a problem as shown in ELFs (9b, c, d). That is, in (9b), the argument is a negated formula which is normally considered to be stative-unbounded, and in (9c) and (9d), the iter operator produces stative-unbounded formulas. 4. C O N C L U S I O N Much theoretical work has been done on temporal adverbials (e.g., [4, 5, 7, 14, 16, 19]). There is also some computationally oriented work. For instance, H0bbs [8] provided simple rules for some temporal adverbials, including frequency ones. Moens and Steedman [15], among others, discussed the interaction of adverbials and aspectual categories. Our work goes further, in terms of (1) the scope of syntactic coverage, (2) interaction of adverbials with each other and with tense and aspect, (3) systematic (and compositional) transduction from syntax to logical form (with logical-form deindexing), (4) formal interpretability of the resulting logical forms, and (5) demonstrable use of the resulting logical forms for inference. Our initial focus in the analysi"
H93-1027,J88-2003,0,0.908112,"em resembles Passonneau&apos;s [18] in that it makes use of two orthogonal feature hierarchies, although the actual division of features is different from hers. is not ill). Conversely, a formula is unbounded if the episode it characterizes does not terminate in a distinctive result state. For instance, was ill in &quot;John was ill when I saw him last week&quot; is unbounded as the sentence does not entail that John was not ill right after the described episode. However, when we say &quot;John was ill twice last year,&quot; we are talking about bounded &quot;ill&quot; episodes.7 As has been discussed by many authors (e.g., in [3, 6, 15, 17, 26, 27]), vPs and temporal adverbials may not arbitrarily combine. Normally, durative adverbials combine with unbounded VPs; cardinal and frequency adverbials with bounded VPs; and adverbials of time-span with telic VPs. Thus, for instance, Mary studied for an hour. s *Mary finished the homework for a second. Mary called John twice Irepeatedly l every five minutes. Mary wrote the paper/n two weeks. or throughout last month (corresponding to bounded and unbounded readings of the VP). Synchronized cyclic adverbials like every spring or every time I saw Mary may combine with bounded or unbounded formula"
H93-1027,J88-2005,0,0.071759,"[(3e2:[e2 irnmed-suecessor-in el s] [(dist (begin-of el) (begin-of e2)) = p]) v [(dist (begin-of e 1) (end-of e)) &lt; p]] ^ [(3e3:[e3 immed-predecessor-in el s] [(dist (begin-of • 1) (begin-of e3)) = p]) v [(dist (begin-of el) (begin-of e)) &lt; p]]])))) A component episode of a sequence of episodes with periodp has an immediate predecessor/successor that is apart from it by p unless it is the firstllast element of the sequence. The distance between the first/last element and the begirdend point of the episode the sequence permeates is less than p. 6Our aspecmal class system resembles Passonneau&apos;s [18] in that it makes use of two orthogonal feature hierarchies, although the actual division of features is different from hers. is not ill). Conversely, a formula is unbounded if the episode it characterizes does not terminate in a distinctive result state. For instance, was ill in &quot;John was ill when I saw him last week&quot; is unbounded as the sentence does not entail that John was not ill right after the described episode. However, when we say &quot;John was ill twice last year,&quot; we are talking about bounded &quot;ill&quot; episodes.7 As has been discussed by many authors (e.g., in [3, 6, 15, 17, 26, 27]), vPs a"
H93-1027,H90-1008,1,0.824335,"&gt;], written in the square-bracketed, infixed form that is the preferred sentence syntax in EL.3 The above indexical (context-dependent) logical form is obtained quite directly as a byproduct of parsing, and is subsequently further processed-- first, by scoping of ambiguously scoped quantifiers, logical connectives, and tense operators, and then by applying a set of formal deindexing rules, which introduce explicit episodic variables into the LF, and temporally relate these based on tense operators, temporal adverbials, and context structures called tense trees. These tense trees, described in [10, 22], supply &quot;orienting relations&quot; between episodes introduced by different clauses, such as the relation that exists between successively reported events in a narrative. We should emphasize that our treatment of time adverbials is fully compatible and integrated with the treatment of tense, but we will neglect tense o~rators and tense trees herein as far as possible. We do need to mention, though, that tense operators are generally assumed to take wide scope over adverbials in the same clause. Thus, after scoping, we get 2Certain feature principles are assumed in the grammar--namely, certain vers"
H93-1027,P92-1030,1,\N,Missing
J82-1003,P82-1014,0,0.0939297,"Missing"
J82-1003,P81-1023,0,0.0189496,"y a recent panel discussion on parsing issues (Robinson 1981). In the words of one of the panelists, ""I take it to be uncontroversial that, other things being equal, a homogenized system is less preferable on both practical and scientific grounds than one that naturally decomposes. Practically, such a system is easier to build and maintain, since the parts can be designed, developed, and understood to a certain extent in isolation... Scientifically, a decomposable system is much more likely to provide insight into the process of natural language comprehension, whether by machines or people."" (Kaplan 1981) The panelists also emphasized that structural decomposition by no means precludes interleaving or paralCopyright 1982 by the Association for C o m p u t a t i o n a l Linguistics. Permission to copy without fee all or part of this material is g r a n t e d provided that the copies are not made for direct commercial a d v a n t a g e and the Journal reference and this copyright notice are included on the first page. To copy otherwise, or to republish, requires a fee a n d / o r specific permission. 0362-613X/82/010026-19501.00 26 American Journal of Computational Linguistics, Volume 8, Number"
J82-1003,P83-1009,0,0.234842,"Missing"
J82-1003,P81-1020,0,0.0142427,"ng and comprehension. In our view, the achievement of the former objective calls for a careful structural separation of the subsystems that specify possible constituent structure (syntax), possible mappings from constituent structure to underlying logical form (part of semantics), and possible mappings from logical form to deeper, unambiguous representations as a function of discourse context and world knowledge (part of pragmatics and l Submitted A u g u s t 1981; revised July 1982. inference). This sort of view is now widely held, as evidenced by a recent panel discussion on parsing issues (Robinson 1981). In the words of one of the panelists, ""I take it to be uncontroversial that, other things being equal, a homogenized system is less preferable on both practical and scientific grounds than one that naturally decomposes. Practically, such a system is easier to build and maintain, since the parts can be designed, developed, and understood to a certain extent in isolation... Scientifically, a decomposable system is much more likely to provide insight into the process of natural language comprehension, whether by machines or people."" (Kaplan 1981) The panelists also emphasized that structural de"
J82-1003,P82-1001,0,0.0337927,"ht syntactic track. Neither parser at present handles slash categories and coordination (although they could be handled inefficiently by resort to closure of the g r a m m a r under metarules and rule schemata). Extraction of quantifiers f r o m the logical-form translations is at present based on the level of syntactic embedding and left-to-right order alone, and no o t h e r f o r m of postprocessing is a t t e m p t e d . l 5 15 Since submission of this paper for publication, we have become aware of several additional papers on p a r s e r - t r a n s l a t o r s similar to ours. One is by Rosenschein & Shieber (1982), another by G a w r o n et al. (1982); in conception these are based quite directly on the generalized phrase structure grammar of Gazdar and his collaborators, and use reeursive descent parsers. A related Prolog-based approach is described by McCord (1981, 1982). 42 It has b e e n gratifyingly easy to write these parsertranslators, confirming us in the conviction that G a z d a r - s t y l e g r a m m a r s hold great p r o m i s e for the design of natural language understanding systems. It is particularly n o t e w o r t h y that we found the design of the t r a n s l a t o r c o m p o n e"
J82-1003,T75-2008,0,0.0242496,"g u e 1970c, Bennett 1974, Partee 1974, D o w t y 1978, 1979, Dowty, Wall & Peters 1981). A review of this literature would be out of place here; but we would like to indicate that the case against decomposition (and hence against conventional translations) is not closed, by offering the fol9 With regard to our system-building objectives, such resort to lexical decomposition is no liability: the need for some use of lexical decomposition to obtain ""canonical"" representations that facilitate inference is widely acknowledged by AI researchers, and carried to extremes by some (e.g., Wilks 1974, Schank 1975). V o l u m e 8, N u m b e r 1, J a n u a r y - M a r c h 1982 31 Lenhart K. Schubert and Francis Jeffry Pelletier lowing paraphrases of the three sample sentences. (Paraphrase (1)w is well-known, except perhaps for the particular form of adverbial (Quine 1960, B e n n e t t 1974, Partee 1974), while ( 2 ) I - ( 3 ) &apos;&apos; are original). These could be formalized within a conventional logical framework allowing for non-truth-functional sentential operators: John tries to find a unicorn (by looking around), John forms a mental description which could apply to a unicorn, (3) 1 John acts, thinks and"
J82-1003,P81-1024,0,\N,Missing
J82-1003,P81-1036,0,\N,Missing
N09-3007,P90-1034,0,0.362427,"using as features argument positions within fragments from a syntactic dependency parser. 6 Conclusion We have presented a bootstrapping approach for creating semantically tagged lexicons. The method can effectively classify nouns with contrasting semantic properties, even when the initial training set is a very small. Further classification is possible with both manual and automatic methods by utilizing individual contextual features in the optimal model. Acknowledgments Table 7: Top-10 features that promote activation of the physical-object target in the model. glish nouns first appeared in Hindle (1990). Roark and Charniak (1998) constructed a semantic lexicon using co-occurrence statistics of nouns within noun phrases. More recently, Liakata and Pulman (2008) induced a hierarchy over nominals using as features knowledge fragments similar to the sort given by K NEXT. Our work might be viewed as aiming for the same goal (a lexico-semantic based partitioning over nominals, tied to corpus-based knowledge), but allowing for an a priori bias regarding preferred structure. The idea of bootstrapping lexical semantic properties goes back at least to Hearst (1998), where the idea is suggested of usin"
N09-3007,P06-1103,0,0.0133972,"n iterative training process, the classifier first learns from a small seed set, which contains examples of all categories (in binary classification, both positive and negative examples) manually selected to reflect human knowledge of semantic categories. The classifier then discovers new instances (and corresponding features) of each category. Based on activation values, these newly discovered instances are selectively admitted into the original training set, which increases the size of training examples for the next iteration. The iterative training algorithm described above is adopted from Klementiev and Roth (2006). The advantage of bootstrapping is the ability to automatically learn from new discoveries, which saves both time and effort required to manually examine a source lexicon. However, if implemented exactly as described above, this process has two apparent disadvantages: New examples may be wrongly classified by the model; and it is difficult to evaluate the discriminative models produced in successive iterations, as there are no standard data against which to judge them (the new examples are by definition previously unexamined). We propose two measures to alleviate these problems. First, we adm"
N09-3007,W08-2212,0,0.0527038,"r creating semantically tagged lexicons. The method can effectively classify nouns with contrasting semantic properties, even when the initial training set is a very small. Further classification is possible with both manual and automatic methods by utilizing individual contextual features in the optimal model. Acknowledgments Table 7: Top-10 features that promote activation of the physical-object target in the model. glish nouns first appeared in Hindle (1990). Roark and Charniak (1998) constructed a semantic lexicon using co-occurrence statistics of nouns within noun phrases. More recently, Liakata and Pulman (2008) induced a hierarchy over nominals using as features knowledge fragments similar to the sort given by K NEXT. Our work might be viewed as aiming for the same goal (a lexico-semantic based partitioning over nominals, tied to corpus-based knowledge), but allowing for an a priori bias regarding preferred structure. The idea of bootstrapping lexical semantic properties goes back at least to Hearst (1998), where the idea is suggested of using seed examples of a relation to discover lexico-syntactic extraction patterns and then using these to discover further examples of the desired relation. The Ba"
N09-3007,P98-2182,0,0.0513576,"es argument positions within fragments from a syntactic dependency parser. 6 Conclusion We have presented a bootstrapping approach for creating semantically tagged lexicons. The method can effectively classify nouns with contrasting semantic properties, even when the initial training set is a very small. Further classification is possible with both manual and automatic methods by utilizing individual contextual features in the optimal model. Acknowledgments Table 7: Top-10 features that promote activation of the physical-object target in the model. glish nouns first appeared in Hindle (1990). Roark and Charniak (1998) constructed a semantic lexicon using co-occurrence statistics of nouns within noun phrases. More recently, Liakata and Pulman (2008) induced a hierarchy over nominals using as features knowledge fragments similar to the sort given by K NEXT. Our work might be viewed as aiming for the same goal (a lexico-semantic based partitioning over nominals, tied to corpus-based knowledge), but allowing for an a priori bias regarding preferred structure. The idea of bootstrapping lexical semantic properties goes back at least to Hearst (1998), where the idea is suggested of using seed examples of a relati"
N09-3007,W02-1028,0,0.0762365,"Missing"
N09-3007,C08-1116,1,0.882175,"Missing"
N09-3007,C98-2177,0,\N,Missing
P84-1025,P82-1001,0,0.0677009,"Missing"
P84-1025,J82-1003,1,0.778438,"nguage are systematically--and simply--determined by the syntactic form of those sentences. This view is in contrast with a tacit assumption often made in AI, that computation of logical translations requires throngs of more or less arbitrary rules operating upon syntactic forms.* The following are a few grammar rules in approximately the style of Gazdar&apos;s Generalized Phrase Structure Grammar (GPSG). They differ from Gazdar&apos;s primarily in that they are designed to produce more or less ""conventional"" logical translations, rather than the intensional ones of Montague and Gazdar (for details see Schubert & Pelletier 1982). Each rule consists of a rule number, a phrase structure rule, and a semantic (logical translation) rule. Applying the rules of translation is even simpler. In essence, all that is needed is a mechanism for arranging logical expressions into larger expressions in conformity with the semantic rules. (For examples of parsers see Thompson 1981, Schubert & Pelletier 1982, Gawron Rosenschein & Shieber 1982). et al 1982, The topic of mass terms and predicates has a substantial literature within both linguistics and philosophical logic, with much of the recent research deriving inspiration from Mont"
P84-1025,P81-1036,0,0.0291638,"&apos;s Generalized Phrase Structure Grammar (GPSG). They differ from Gazdar&apos;s primarily in that they are designed to produce more or less ""conventional"" logical translations, rather than the intensional ones of Montague and Gazdar (for details see Schubert & Pelletier 1982). Each rule consists of a rule number, a phrase structure rule, and a semantic (logical translation) rule. Applying the rules of translation is even simpler. In essence, all that is needed is a mechanism for arranging logical expressions into larger expressions in conformity with the semantic rules. (For examples of parsers see Thompson 1981, Schubert & Pelletier 1982, Gawron Rosenschein & Shieber 1982). et al 1982, The topic of mass terms and predicates has a substantial literature within both linguistics and philosophical logic, with much of the recent research deriving inspiration from Montague Grammar (e.g., see Pellefier 1979, ter Meulen 1980, Bunt 1981, Chierchia 1982). There are three views on the mass/count distinction, namely that the distinction is (a) syntactic, (b) semantic,, and (c) pragmatic, Orthogonal to these views we have the further possibilities (i) that the mass/count distinction is lexical. and (ii) that it"
P84-1025,P82-1014,0,\N,Missing
P84-1054,P83-1017,0,0.153015,"Missing"
P84-1054,J82-1003,1,\N,Missing
P92-1030,J88-2003,0,0.250505,"Missing"
P92-1030,T87-1035,0,0.058531,"Missing"
P92-1030,T75-2026,0,0.234653,"Missing"
P92-1030,C92-1052,0,0.0200552,"Missing"
P92-1030,J86-3001,0,0.252627,"Missing"
P92-1030,P91-1008,0,0.0511871,"Missing"
P92-1030,J88-2005,0,0.534084,"Missing"
P92-1030,H90-1008,1,0.910456,"Missing"
P92-1030,P87-1021,0,0.143368,"Missing"
P92-1030,J88-2006,0,0.703686,"Missing"
P99-1053,P83-1019,0,0.893873,"Missing"
P99-1053,P98-2185,0,0.0554911,"Missing"
P99-1053,P92-1000,0,0.283982,"Missing"
P99-1053,W96-0204,0,0.0277413,"Missing"
P99-1053,P92-1008,0,\N,Missing
P99-1053,P97-1033,0,\N,Missing
P99-1053,C98-2180,0,\N,Missing
S16-2004,W13-0103,0,0.0550809,"Missing"
S16-2004,W99-0501,0,0.262868,"d incorporates frames, hand-tagged word senses, and examples from WordNet to achieve highly consistent semantic interpretations. EL allows the full content of glosses to be incorporated into the formal lexical axioms, without sacrificing interpretive accuracy, or verb-to-verb inference accuracy on a standard test set. Figure 1: Example of rule extraction from machine readable dictionaries for WordNet entry of slam2.v. there have been many attempts to transduce informal lexical knowledge from machine readable dictionaries into a formally structured form (Calzolari, 1984; Chodorow et al., 1985; Harabagiu et al., 1999; Moldovan and Rus, 2001; Hobbs, 2008; Allen et al., 2013). Consider an example of the types of knowledge these approaches seek to extract in Figure 1. WordNet defines slam2.v, i.e., sense 2 of the verb slam, as “strike violently”. This gloss states an implication that if “x slams y” characterizes an event e, then “x strikes y violently” also characterizes event e. All language phenomena must be able to be represented and reasoned about for such axioms to be useful in a language understanding system. This is where previous approaches share a common shortcoming: the logical representations that"
S16-2004,W13-2322,0,0.0432128,"e the representation used by smatch is the same as that of AMR, we can map the triple representation into a graph representation in the same manner as AMR formulas. Figure 5 shows an example of the use of the new instance triple in mapping the EL formula for “I am very happy” into these representations. However, this mapping does not relate the semantics of EL to AMR since the interpretation of the triples differ for AMR and EL formulas. EL-smatch In this section we introduce EL-smatch, a generalized formulation of smatch (Cai and Knight, 2013), the standard evaluation metric for AMR parsing (Banarescu et al., 2013). Smatch represents each logical form as a conjunction of triples of three types: 1. instance(variable, type) 2. relation(variable, variable) 3. attribute(variable, value) Every node instance of the logical form is associated with a variable, and the nodes are described and related to each other using the above triples. Thus, type and value can both only be atomic constants. The smatch score is then defined as the maximum f-score (of triples) obtainable via a oneto-one matching of variables between the two formulas (Cai and Knight, 2013). In order to capture complex types of EL, we introduce a"
S16-2004,P85-1008,0,0.802018,"glosses are often abstract, sometimes miss important information (such as arguments), and may be inconsistent with one another. Evidently there is a need for sophisticated extraction techniques to acquire accurate and consistent knowledge from dictionaries. Most modern approaches to this problem use WordNet (Miller, 1995) as the lexical resource because of the linguistic and semantic annotations that accompany the glosses. Some work encodes WordNet glosses into variants of first-order logic (FOL) (Harabagiu et al., 1999; Moldovan and Rus, 2001; Hobbs, 2008), such as Hobbs Logical Form (HLF) (Hobbs, 1985), while other work encodes them into OWL-DL (OWL Working Group, 2004; Allen et al., 2011; Allen et al., 2013; Orfan and Allen, 2015; Mostafazadeh and Allen, 2015). A particularly noteworthy line of work is that by Allen et al. (2013), which integrates information from a high-level ontology with glosses of semantically related clusters of words to construct inference-supporting micro-theories of con• Have associated methods of inference; • Have a formal interpretation. The semantic representations used by previous approaches fall short on at least one of the above requirements. FOL struggles to"
S16-2004,P13-2131,0,0.049017,"and then describing and relating the components using the triples. Since the representation used by smatch is the same as that of AMR, we can map the triple representation into a graph representation in the same manner as AMR formulas. Figure 5 shows an example of the use of the new instance triple in mapping the EL formula for “I am very happy” into these representations. However, this mapping does not relate the semantics of EL to AMR since the interpretation of the triples differ for AMR and EL formulas. EL-smatch In this section we introduce EL-smatch, a generalized formulation of smatch (Cai and Knight, 2013), the standard evaluation metric for AMR parsing (Banarescu et al., 2013). Smatch represents each logical form as a conjunction of triples of three types: 1. instance(variable, type) 2. relation(variable, variable) 3. attribute(variable, value) Every node instance of the logical form is associated with a variable, and the nodes are described and related to each other using the above triples. Thus, type and value can both only be atomic constants. The smatch score is then defined as the maximum f-score (of triples) obtainable via a oneto-one matching of variables between the two formulas (Cai a"
S16-2004,1993.eamt-1.2,0,0.161514,"Missing"
S16-2004,P84-1036,0,0.558223,"ove parsing precision of coordinators and incorporates frames, hand-tagged word senses, and examples from WordNet to achieve highly consistent semantic interpretations. EL allows the full content of glosses to be incorporated into the formal lexical axioms, without sacrificing interpretive accuracy, or verb-to-verb inference accuracy on a standard test set. Figure 1: Example of rule extraction from machine readable dictionaries for WordNet entry of slam2.v. there have been many attempts to transduce informal lexical knowledge from machine readable dictionaries into a formally structured form (Calzolari, 1984; Chodorow et al., 1985; Harabagiu et al., 1999; Moldovan and Rus, 2001; Hobbs, 2008; Allen et al., 2013). Consider an example of the types of knowledge these approaches seek to extract in Figure 1. WordNet defines slam2.v, i.e., sense 2 of the verb slam, as “strike violently”. This gloss states an implication that if “x slams y” characterizes an event e, then “x strikes y violently” also characterizes event e. All language phenomena must be able to be represented and reasoned about for such axioms to be useful in a language understanding system. This is where previous approaches share a commo"
S16-2004,W04-0804,0,0.0489787,"an et al., 2012; Chklovski and 6.3 relations of WordNet. A similar experiment by us is unlikely to shed additional light on the topic. 7 Due to time constraints, this evaluation was performed on a gold standard developed primarily by only one annotator. We hope to remedy this in future work including an analysis of interannotator agreement. Error Analysis In the semantic parsing evaluation, most of the parsing errors arose from a failure in the sentence 8 http://www.cs.rochester.edu/u/gkim21/papers/highfidelity-lex-supplementary.pdf 41 techniques, such as those from the SENSEVAL3 task on WSD (Litkowski, 2004), could be used to improve semantic precision, and argument coherence could be improved using techniques from Mostafazadeh & Allen (Mostafazadeh and Allen, 2015). Another possible avenue is concurrent use of information from multiple dictionaries, such as Wiktionary, VerbNet, and WordNet, to construct more complete and reliable axioms, in particular with respect to argument structure and types. We argued that the semantic representations used in previous approaches to extracting lexical axioms from dictionaries are insufficient for achieving a natural language understanding system. We presente"
S16-2004,A00-2018,0,0.169291,"Missing"
S16-2004,P01-1052,0,0.17895,"and-tagged word senses, and examples from WordNet to achieve highly consistent semantic interpretations. EL allows the full content of glosses to be incorporated into the formal lexical axioms, without sacrificing interpretive accuracy, or verb-to-verb inference accuracy on a standard test set. Figure 1: Example of rule extraction from machine readable dictionaries for WordNet entry of slam2.v. there have been many attempts to transduce informal lexical knowledge from machine readable dictionaries into a formally structured form (Calzolari, 1984; Chodorow et al., 1985; Harabagiu et al., 1999; Moldovan and Rus, 2001; Hobbs, 2008; Allen et al., 2013). Consider an example of the types of knowledge these approaches seek to extract in Figure 1. WordNet defines slam2.v, i.e., sense 2 of the verb slam, as “strike violently”. This gloss states an implication that if “x slams y” characterizes an event e, then “x strikes y violently” also characterizes event e. All language phenomena must be able to be represented and reasoned about for such axioms to be useful in a language understanding system. This is where previous approaches share a common shortcoming: the logical representations that the lexical knowledge i"
S16-2004,W04-3205,0,0.178693,"Missing"
S16-2004,P85-1037,0,0.439097,"sion of coordinators and incorporates frames, hand-tagged word senses, and examples from WordNet to achieve highly consistent semantic interpretations. EL allows the full content of glosses to be incorporated into the formal lexical axioms, without sacrificing interpretive accuracy, or verb-to-verb inference accuracy on a standard test set. Figure 1: Example of rule extraction from machine readable dictionaries for WordNet entry of slam2.v. there have been many attempts to transduce informal lexical knowledge from machine readable dictionaries into a formally structured form (Calzolari, 1984; Chodorow et al., 1985; Harabagiu et al., 1999; Moldovan and Rus, 2001; Hobbs, 2008; Allen et al., 2013). Consider an example of the types of knowledge these approaches seek to extract in Figure 1. WordNet defines slam2.v, i.e., sense 2 of the verb slam, as “strike violently”. This gloss states an implication that if “x slams y” characterizes an event e, then “x strikes y violently” also characterizes event e. All language phenomena must be able to be represented and reasoned about for such axioms to be useful in a language understanding system. This is where previous approaches share a common shortcoming: the logi"
S16-2004,D12-1018,0,0.0236114,"ils of semantically rich lexical axioms. Therefore, in order to compare our results to previous work, we evaluate a stripped-down version of our inference mechanism on a manually created verb entailment dataset (Weisman et al., 2012). This dataset contains 812 directed verb pairs, v1 → v2, which are annotated ‘yes’ if the annotator could think of plausible contexts under which the entailment from v1 to v2 holds. For example, identify entails recognize in some contexts, does not entail describe is any context. Though the dataset is not rich, many previous systems (Mostafazadeh and Allen, 2015; Weisman et al., 2012; Chklovski and 6.3 relations of WordNet. A similar experiment by us is unlikely to shed additional light on the topic. 7 Due to time constraints, this evaluation was performed on a gold standard developed primarily by only one annotator. We hope to remedy this in future work including an analysis of interannotator agreement. Error Analysis In the semantic parsing evaluation, most of the parsing errors arose from a failure in the sentence 8 http://www.cs.rochester.edu/u/gkim21/papers/highfidelity-lex-supplementary.pdf 41 techniques, such as those from the SENSEVAL3 task on WSD (Litkowski, 2004"
S16-2004,W12-0803,1,0.735822,"e for semantic parsing of the glosses (Schubert, 2002; Schubert and Tong, 2003; Gordon and Schubert, 2010; Schubert, 2014). For the parser to be effective, some preprocessing of the glosses is necessary because glosses often omit arguments, resulting in an incomplete sentence. There are also some serious shortcomings to general semantic parsers, particularly in handling coordinators and/ or. In this section, we describe the complete semantic parsing process of glosses and the details of each step. Throughout our semantic parsing implementation, we use the tree-to-tree transduction tool (TTT) (Purtee and Schubert, 2012) for transArgument Structure Inference We initially use the frames in WordNet to hypothesize the argument structures. For example, the frames for quarrel1.v are [Somebody quarrel1.v] and [Somebody quarrel1.v PP]. From this we hypothesize that quarrel1.v has a subject argument that is a person, no object argument, and may include a prepositional phrase adjunct. Then we refine the frames by looking at the examples and gloss(es) available for the synset. 2 However, EL’s quantifier syntax also allows, e.g., (most.det x Φ(x) Ψ(x)), ducible to FOL. Semantic Parsing of Glosses 3 quarrel1.v and scrap2"
S16-2004,W03-0902,1,0.603504,"m construction replaces the dummy arguments with variables and constructs a scoped axiom relating the entry word and the semantic parse of the gloss using the characterization operator ‘**’. In the simple example slam2.v, most of the subroutines used in each step have no effect. All transformations outside the scope of the BLLIP parser are performed with hand-written rules, which were fine-tuned using a development set of 550 verb synset entries. 4.1 4.2 Sentence-level semantic parsers for EL have been developed previously, which we can use for semantic parsing of the glosses (Schubert, 2002; Schubert and Tong, 2003; Gordon and Schubert, 2010; Schubert, 2014). For the parser to be effective, some preprocessing of the glosses is necessary because glosses often omit arguments, resulting in an incomplete sentence. There are also some serious shortcomings to general semantic parsers, particularly in handling coordinators and/ or. In this section, we describe the complete semantic parsing process of glosses and the details of each step. Throughout our semantic parsing implementation, we use the tree-to-tree transduction tool (TTT) (Purtee and Schubert, 2012) for transArgument Structure Inference We initially"
S16-2004,W14-2411,1,0.455906,"iables and constructs a scoped axiom relating the entry word and the semantic parse of the gloss using the characterization operator ‘**’. In the simple example slam2.v, most of the subroutines used in each step have no effect. All transformations outside the scope of the BLLIP parser are performed with hand-written rules, which were fine-tuned using a development set of 550 verb synset entries. 4.1 4.2 Sentence-level semantic parsers for EL have been developed previously, which we can use for semantic parsing of the glosses (Schubert, 2002; Schubert and Tong, 2003; Gordon and Schubert, 2010; Schubert, 2014). For the parser to be effective, some preprocessing of the glosses is necessary because glosses often omit arguments, resulting in an incomplete sentence. There are also some serious shortcomings to general semantic parsers, particularly in handling coordinators and/ or. In this section, we describe the complete semantic parsing process of glosses and the details of each step. Throughout our semantic parsing implementation, we use the tree-to-tree transduction tool (TTT) (Purtee and Schubert, 2012) for transArgument Structure Inference We initially use the frames in WordNet to hypothesize the"
S16-2004,E09-1092,1,0.890864,"Missing"
W03-0902,W01-0703,0,0.00620929,"simple, approximate English form; and (6) heuristic routines for filtering out many ill-formed or vacuous propositions. In addition, semantic interpretation of individual words involves some simple morphological analysis, for instance to allow the interpretation of (VBD SLEPT) in terms of a predicate SLEEP[V]. In (Schubert, 2002) we made some comparisons between our project and earlier work in knowledge extraction (e.g., (muc, 1993; muc, 1995; muc, 1998; Berland and Charniak, 1999; Clark and Weir, 1999; Hearst, 1998; Riloff and Jones, 1999)) and in discovery of selectional preferences (e.g., (Agirre and Martinez, 2001; Grishman and Sterling, 1992; Resnik, 1992; Resnik, 1993; Zernik, 1992; Zernik and Jacobs, 1990)). Reiterating briefly, we note that knowledge extraction work has generally employed carefully tuned extraction patterns to locate and extract some predetermined, specific kinds of facts; our goal, instead, is to process every phrase and sentence that is encountered, abstracting from it miscellaneous general knowledge whenever possible. Methods for discovering selectional preferences do seek out conventional patterns of verb-argument combination, but tend to “lose the connection” between argument"
W03-0902,P99-1008,0,0.00661056,"shed clothes”); (5) routines for deriving propositional patterns from the resulting miscellaneous semantic patterns, and rendering them in a simple, approximate English form; and (6) heuristic routines for filtering out many ill-formed or vacuous propositions. In addition, semantic interpretation of individual words involves some simple morphological analysis, for instance to allow the interpretation of (VBD SLEPT) in terms of a predicate SLEEP[V]. In (Schubert, 2002) we made some comparisons between our project and earlier work in knowledge extraction (e.g., (muc, 1993; muc, 1995; muc, 1998; Berland and Charniak, 1999; Clark and Weir, 1999; Hearst, 1998; Riloff and Jones, 1999)) and in discovery of selectional preferences (e.g., (Agirre and Martinez, 2001; Grishman and Sterling, 1992; Resnik, 1992; Resnik, 1993; Zernik, 1992; Zernik and Jacobs, 1990)). Reiterating briefly, we note that knowledge extraction work has generally employed carefully tuned extraction patterns to locate and extract some predetermined, specific kinds of facts; our goal, instead, is to process every phrase and sentence that is encountered, abstracting from it miscellaneous general knowledge whenever possible. Methods for discovering"
W03-0902,J96-2004,0,0.0545116,"Missing"
W03-0902,W99-0631,0,0.0644261,"for deriving propositional patterns from the resulting miscellaneous semantic patterns, and rendering them in a simple, approximate English form; and (6) heuristic routines for filtering out many ill-formed or vacuous propositions. In addition, semantic interpretation of individual words involves some simple morphological analysis, for instance to allow the interpretation of (VBD SLEPT) in terms of a predicate SLEEP[V]. In (Schubert, 2002) we made some comparisons between our project and earlier work in knowledge extraction (e.g., (muc, 1993; muc, 1995; muc, 1998; Berland and Charniak, 1999; Clark and Weir, 1999; Hearst, 1998; Riloff and Jones, 1999)) and in discovery of selectional preferences (e.g., (Agirre and Martinez, 2001; Grishman and Sterling, 1992; Resnik, 1992; Resnik, 1993; Zernik, 1992; Zernik and Jacobs, 1990)). Reiterating briefly, we note that knowledge extraction work has generally employed carefully tuned extraction patterns to locate and extract some predetermined, specific kinds of facts; our goal, instead, is to process every phrase and sentence that is encountered, abstracting from it miscellaneous general knowledge whenever possible. Methods for discovering selectional preferenc"
W03-0902,C92-2099,0,0.0374463,"form; and (6) heuristic routines for filtering out many ill-formed or vacuous propositions. In addition, semantic interpretation of individual words involves some simple morphological analysis, for instance to allow the interpretation of (VBD SLEPT) in terms of a predicate SLEEP[V]. In (Schubert, 2002) we made some comparisons between our project and earlier work in knowledge extraction (e.g., (muc, 1993; muc, 1995; muc, 1998; Berland and Charniak, 1999; Clark and Weir, 1999; Hearst, 1998; Riloff and Jones, 1999)) and in discovery of selectional preferences (e.g., (Agirre and Martinez, 2001; Grishman and Sterling, 1992; Resnik, 1992; Resnik, 1993; Zernik, 1992; Zernik and Jacobs, 1990)). Reiterating briefly, we note that knowledge extraction work has generally employed carefully tuned extraction patterns to locate and extract some predetermined, specific kinds of facts; our goal, instead, is to process every phrase and sentence that is encountered, abstracting from it miscellaneous general knowledge whenever possible. Methods for discovering selectional preferences do seek out conventional patterns of verb-argument combination, but tend to “lose the connection” between argument types (e.g., that a road may"
W03-0902,J93-2004,0,0.0281577,"HAVE A ROOM. A FEMALE-INDIVIDUAL MAY SLEEP. A FEMALE-INDIVIDUAL MAY HAVE CLOTHES. CLOTHES CAN BE WASHED. ((:I (:I (:I (:I (:Q (:Q (:Q (:Q DET DET DET DET NAMED-ENTITY) ENTER[V] (:Q THE ROOM[N])) FEMALE-INDIVIDUAL) HAVE[V] (:Q DET ROOM[N])) FEMALE-INDIVIDUAL) SLEEP[V]) FEMALE-INDIVIDUAL) HAVE[V] (:Q DET (:F PLUR CLOTHE[N]))) (:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A])) The results are produced as logical forms (the last five lines above – see Schubert, 2002, for some details), from which the English glosses are generated automatically. Our work so far has focused on data in the Penn Treebank (Marcus et al., 1993), particularly the Brown corpus and some examples from the Wall Street Journal corpus. The advantage is that Treebank annotations allow us to postpone the challenges of reasonably accurate parsing, though we will soon be experimenting with “industrial strength” parsers on unannotated texts. We reported some specifics of our approach and some preliminary results in (Schubert, 2002). Since then we have refined our extraction methods to the point where we can reliably apply them the Treebank corpora, on average extracting more than 2 generalized propositions per sentence. Applying these methods t"
W03-0902,P92-1053,0,0.0711197,"ines for filtering out many ill-formed or vacuous propositions. In addition, semantic interpretation of individual words involves some simple morphological analysis, for instance to allow the interpretation of (VBD SLEPT) in terms of a predicate SLEEP[V]. In (Schubert, 2002) we made some comparisons between our project and earlier work in knowledge extraction (e.g., (muc, 1993; muc, 1995; muc, 1998; Berland and Charniak, 1999; Clark and Weir, 1999; Hearst, 1998; Riloff and Jones, 1999)) and in discovery of selectional preferences (e.g., (Agirre and Martinez, 2001; Grishman and Sterling, 1992; Resnik, 1992; Resnik, 1993; Zernik, 1992; Zernik and Jacobs, 1990)). Reiterating briefly, we note that knowledge extraction work has generally employed carefully tuned extraction patterns to locate and extract some predetermined, specific kinds of facts; our goal, instead, is to process every phrase and sentence that is encountered, abstracting from it miscellaneous general knowledge whenever possible. Methods for discovering selectional preferences do seek out conventional patterns of verb-argument combination, but tend to “lose the connection” between argument types (e.g., that a road may carry traffic,"
W03-0902,H93-1054,0,0.0303011,"ring out many ill-formed or vacuous propositions. In addition, semantic interpretation of individual words involves some simple morphological analysis, for instance to allow the interpretation of (VBD SLEPT) in terms of a predicate SLEEP[V]. In (Schubert, 2002) we made some comparisons between our project and earlier work in knowledge extraction (e.g., (muc, 1993; muc, 1995; muc, 1998; Berland and Charniak, 1999; Clark and Weir, 1999; Hearst, 1998; Riloff and Jones, 1999)) and in discovery of selectional preferences (e.g., (Agirre and Martinez, 2001; Grishman and Sterling, 1992; Resnik, 1992; Resnik, 1993; Zernik, 1992; Zernik and Jacobs, 1990)). Reiterating briefly, we note that knowledge extraction work has generally employed carefully tuned extraction patterns to locate and extract some predetermined, specific kinds of facts; our goal, instead, is to process every phrase and sentence that is encountered, abstracting from it miscellaneous general knowledge whenever possible. Methods for discovering selectional preferences do seek out conventional patterns of verb-argument combination, but tend to “lose the connection” between argument types (e.g., that a road may carry traffic, a newspaper m"
W03-0902,C90-1005,0,0.0164205,"vacuous propositions. In addition, semantic interpretation of individual words involves some simple morphological analysis, for instance to allow the interpretation of (VBD SLEPT) in terms of a predicate SLEEP[V]. In (Schubert, 2002) we made some comparisons between our project and earlier work in knowledge extraction (e.g., (muc, 1993; muc, 1995; muc, 1998; Berland and Charniak, 1999; Clark and Weir, 1999; Hearst, 1998; Riloff and Jones, 1999)) and in discovery of selectional preferences (e.g., (Agirre and Martinez, 2001; Grishman and Sterling, 1992; Resnik, 1992; Resnik, 1993; Zernik, 1992; Zernik and Jacobs, 1990)). Reiterating briefly, we note that knowledge extraction work has generally employed carefully tuned extraction patterns to locate and extract some predetermined, specific kinds of facts; our goal, instead, is to process every phrase and sentence that is encountered, abstracting from it miscellaneous general knowledge whenever possible. Methods for discovering selectional preferences do seek out conventional patterns of verb-argument combination, but tend to “lose the connection” between argument types (e.g., that a road may carry traffic, a newspaper may carry a story, but a road is unlikely"
W03-0902,C92-4212,0,0.407246,"ill-formed or vacuous propositions. In addition, semantic interpretation of individual words involves some simple morphological analysis, for instance to allow the interpretation of (VBD SLEPT) in terms of a predicate SLEEP[V]. In (Schubert, 2002) we made some comparisons between our project and earlier work in knowledge extraction (e.g., (muc, 1993; muc, 1995; muc, 1998; Berland and Charniak, 1999; Clark and Weir, 1999; Hearst, 1998; Riloff and Jones, 1999)) and in discovery of selectional preferences (e.g., (Agirre and Martinez, 2001; Grishman and Sterling, 1992; Resnik, 1992; Resnik, 1993; Zernik, 1992; Zernik and Jacobs, 1990)). Reiterating briefly, we note that knowledge extraction work has generally employed carefully tuned extraction patterns to locate and extract some predetermined, specific kinds of facts; our goal, instead, is to process every phrase and sentence that is encountered, abstracting from it miscellaneous general knowledge whenever possible. Methods for discovering selectional preferences do seek out conventional patterns of verb-argument combination, but tend to “lose the connection” between argument types (e.g., that a road may carry traffic, a newspaper may carry a sto"
W08-2219,W03-0901,0,0.2588,"Missing"
W08-2219,W99-0631,0,0.0305971,"ed for Open Knowledge Extraction: the conversion of arbitrary input sentences into general world knowledge represented in a logical form possibly usable for inference. Results show the feasibility of extraction via the use of sophisticated natural language processing as applied to web texts. 2 Previous Work Given that the concern here is with open knowledge extraction, the myriad projects that target a few prespecified types of relations occurring in a large corpus are set aside. Among early efforts, one might count work on deriving selectional preferences (e.g., Zernik (1992); Resnik (1993); Clark and Weir (1999)) or partial predicateargument structure (e.g., Abney (1996)) as steps in the direction of open knowledge extraction, though typically few of the tuples obtained (often a type of subject plus a verb, or a verb plus a type of object) can be interpreted as complete items of world knowledge. Another somewhat relevant line of research was initiated by Zelle and Mooney (1996), concerned with learning to map NL database queries into formal DB queries (a kind of semantic interpretation). This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at lea"
W08-2219,P97-1003,0,0.121537,"ms (e.g., leaving just a head noun phrase). 2.1 K NEXT K NEXT (Schubert, 2002) was originally designed for application to collections of manually annotated parse trees, such as the Brown corpus. In order to extract knowledge from larger text collections, the system has been extended for processing arbitrary text through the use of third-party parsers. In addition, numerous improvements have been made to the semantic interpretation rules, the filtering techniques, and other components of the system. The extraction procedure is as follows: 1. Parse each sentence using a Treebank-trained parser (Collins, 1997; Charniak, 1999). 2. Preprocess the parse tree, for better interpretability (e.g., distinguish different types of SBAR phrases and different types of PPs, identify temporal phrases, etc.). 3. Apply a set of 80 interpretive rules for computing unscoped logical forms (ULFs) of the sentence and all lower-level constituents in a bottom-up sweep; at the same time, abstract and collect phrasal logical forms that promise to yield stand-alone propositions (e.g., ULFs of clauses and of pre- or post-modified nominals are prime candidates). The ULFs are rendered in Episodic Logic (e.g., (Schubert and Hw"
W08-2219,J02-3001,0,0.00751587,", this approach requires annotation of texts with logical forms, and extending this approach to general texts would seemingly require a massive corpus of hand-annotated text — and the logical forms would have to cover far more phenomena than are found in DB queries (e.g., attitudes, generalized quantifiers, etc.). Another line of relevant work is that on semantic role labelling. One early example was MindNet (Richardson et al., 1998), which was based on collecting 24 semantic role relations from MRDs such as the American Heritage Dictionary. More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al. (2008). The relevance of this work comes from the fact that identifying the arguments of the verbs in a sentence is a first step towards forming predications, and these may in many cases correspond to items of world knowledge. Open Knowledge Extraction through Compositional Language Processing 241 Liakata and Pulman (2002) built a system for recovering Davidsonian predicateargument structures from the Penn Treebank through the application of a small set of syntactic templates targeting head nodes of verb arguments. The authors illustrate their"
W08-2219,P02-1031,0,0.109263,"ing general world knowledge, given in a logical form potentially usable for inference, may be extracted in high volume from arbitrary input sentences. We compare these results with those obtained in recent work on Open Information Extraction, indicating with some examples the quite different kinds of output obtained by the two approaches. Finally, we observe that portions of the extracted knowledge are comparable to results of recent work on class attribute extraction. 239 240 Van Durme and Schubert 1 Introduction Several early studies in large-scale text processing (Liakata and Pulman, 2002; Gildea and Palmer, 2002; Schubert, 2002) showed that having access to a sentence’s syntax enabled credible, automated semantic analysis. These studies suggest that the use of increasingly sophisticated linguistic analysis tools could enable an explosion in available symbolic knowledge. Nonetheless, much of the subsequent work in extraction has remained averse to the use of the linguistic deep structure of text; this decision is typically justified by a desire to keep the extraction system as computationally lightweight as possible. The acquisition of background knowledge is not an activity that needs to occur online"
W08-2219,C02-1105,0,0.282128,"sonable quality, representing general world knowledge, given in a logical form potentially usable for inference, may be extracted in high volume from arbitrary input sentences. We compare these results with those obtained in recent work on Open Information Extraction, indicating with some examples the quite different kinds of output obtained by the two approaches. Finally, we observe that portions of the extracted knowledge are comparable to results of recent work on class attribute extraction. 239 240 Van Durme and Schubert 1 Introduction Several early studies in large-scale text processing (Liakata and Pulman, 2002; Gildea and Palmer, 2002; Schubert, 2002) showed that having access to a sentence’s syntax enabled credible, automated semantic analysis. These studies suggest that the use of increasingly sophisticated linguistic analysis tools could enable an explosion in available symbolic knowledge. Nonetheless, much of the subsequent work in extraction has remained averse to the use of the linguistic deep structure of text; this decision is typically justified by a desire to keep the extraction system as computationally lightweight as possible. The acquisition of background knowledge is not an activity t"
W08-2219,H93-1054,0,0.198084,"paper is designed for Open Knowledge Extraction: the conversion of arbitrary input sentences into general world knowledge represented in a logical form possibly usable for inference. Results show the feasibility of extraction via the use of sophisticated natural language processing as applied to web texts. 2 Previous Work Given that the concern here is with open knowledge extraction, the myriad projects that target a few prespecified types of relations occurring in a large corpus are set aside. Among early efforts, one might count work on deriving selectional preferences (e.g., Zernik (1992); Resnik (1993); Clark and Weir (1999)) or partial predicateargument structure (e.g., Abney (1996)) as steps in the direction of open knowledge extraction, though typically few of the tuples obtained (often a type of subject plus a verb, or a verb plus a type of object) can be interpreted as complete items of world knowledge. Another somewhat relevant line of research was initiated by Zelle and Mooney (1996), concerned with learning to map NL database queries into formal DB queries (a kind of semantic interpretation). This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Moon"
W08-2219,P98-2180,0,0.0358415,"and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. However, this approach requires annotation of texts with logical forms, and extending this approach to general texts would seemingly require a massive corpus of hand-annotated text — and the logical forms would have to cover far more phenomena than are found in DB queries (e.g., attitudes, generalized quantifiers, etc.). Another line of relevant work is that on semantic role labelling. One early example was MindNet (Richardson et al., 1998), which was based on collecting 24 semantic role relations from MRDs such as the American Heritage Dictionary. More recent representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer (2002), and Punyakanok et al. (2008). The relevance of this work comes from the fact that identifying the arguments of the verbs in a sentence is a first step towards forming predications, and these may in many cases correspond to items of world knowledge. Open Knowledge Extraction through Compositional Language Processing 241 Liakata and Pulman (2002) built a system for recovering Davids"
W08-2219,W03-0902,1,0.416496,"UR person.n)) want.v (Ka (rid.a (of.p (DET dictator.n))))] which is verbalized as PERSONS ported by the text fragment: MAY WANT TO BE RID OF A DICTATOR and is sup... and that if the Spanish people wanted to be rid of Franco, they must achieve this by ... Later examples will be translated into a more conventional logical form. One larger collection we have processed since the 2002-3 work on Treebank corpora is the British National Corpus (BNC), consisting of 100 million words of mixedgenre text passages. The quality of resulting propositions has been assessed by the hand-judging methodology of Schubert and Tong (2003), yielding positive judgements almost as frequently as for the Brown Treebank corpus. The next section, concerned with the web corpus collected and used by Banko et al. (2007), contains a fuller description of the judging method. The BNC-based KB, containing 6,205,877 extracted propositions, is publicly searchable via a recently developed online knowledge browser.3 2 Where Ka is an action/attribute reification operator. 3 http://www.cs.rochester.edu/u/vandurme/epik Van Durme and Schubert 244 3 Experiments The experiments reported here were aimed at a comparative assessment of linguistically ba"
W08-2219,P07-1121,0,0.0130814,"Resnik (1993); Clark and Weir (1999)) or partial predicateargument structure (e.g., Abney (1996)) as steps in the direction of open knowledge extraction, though typically few of the tuples obtained (often a type of subject plus a verb, or a verb plus a type of object) can be interpreted as complete items of world knowledge. Another somewhat relevant line of research was initiated by Zelle and Mooney (1996), concerned with learning to map NL database queries into formal DB queries (a kind of semantic interpretation). This was pursued further, for instance, by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning log-linear models, or (in the latter case) synchronous CF grammars augmented with lambda operators, for mapping English queries to DB queries. However, this approach requires annotation of texts with logical forms, and extending this approach to general texts would seemingly require a massive corpus of hand-annotated text — and the logical forms would have to cover far more phenomena than are found in DB queries (e.g., attitudes, generalized quantifiers, etc.). Another line of relevant work is that on semantic role labelling. One early example was MindNet (Richardson et al."
W08-2219,C92-4212,0,0.392104,"plored in this paper is designed for Open Knowledge Extraction: the conversion of arbitrary input sentences into general world knowledge represented in a logical form possibly usable for inference. Results show the feasibility of extraction via the use of sophisticated natural language processing as applied to web texts. 2 Previous Work Given that the concern here is with open knowledge extraction, the myriad projects that target a few prespecified types of relations occurring in a large corpus are set aside. Among early efforts, one might count work on deriving selectional preferences (e.g., Zernik (1992); Resnik (1993); Clark and Weir (1999)) or partial predicateargument structure (e.g., Abney (1996)) as steps in the direction of open knowledge extraction, though typically few of the tuples obtained (often a type of subject plus a verb, or a verb plus a type of object) can be interpreted as complete items of world knowledge. Another somewhat relevant line of research was initiated by Zelle and Mooney (1996), concerned with learning to map NL database queries into formal DB queries (a kind of semantic interpretation). This was pursued further, for instance, by Zettlemoyer and Collins (2005) an"
W08-2219,A00-2018,0,\N,Missing
W08-2219,J08-2005,0,\N,Missing
W08-2219,C98-2175,0,\N,Missing
W10-0724,C02-1144,0,0.0346537,"S CAN BE LIKELY TO GO ON FOR SOME HOURS ’. While it is expected that eventually sufficiently clean knowledge bases will be produced for inferences to be made about everyday things and events, currently the average quality of automatically acquired knowledge is not good enough to be used in traditional reasoning systems. An obstacle for knowledge extraction is the lack of an easy method for evaluating – and thus improving – the quality of results. Evaluation in acquisition systems is typically done by human judging of random samples of output, usually by the reporting authors themselves (e.g., Lin and Pantel, 2002; Schubert and Tong, 2003; Banko et al., 2007). This is time-consuming, and it has the potential for bias: it would be preferable to have people other than AI researchers label whether an output is commonsense knowledge or not. We explore the use of Amazon’s Mechanical Turk service, an online labor market, as a means of acquiring many non-expert judgements for little cost. 2 Related Work While Open Mind Commons (Speer, 2007) asks users to vote for or against commonsense statements contributed by others users in order to come to a consensus, we seek to evaluate an automatic system. Snow et al."
W10-0724,W03-0902,1,0.726268,"ON FOR SOME HOURS ’. While it is expected that eventually sufficiently clean knowledge bases will be produced for inferences to be made about everyday things and events, currently the average quality of automatically acquired knowledge is not good enough to be used in traditional reasoning systems. An obstacle for knowledge extraction is the lack of an easy method for evaluating – and thus improving – the quality of results. Evaluation in acquisition systems is typically done by human judging of random samples of output, usually by the reporting authors themselves (e.g., Lin and Pantel, 2002; Schubert and Tong, 2003; Banko et al., 2007). This is time-consuming, and it has the potential for bias: it would be preferable to have people other than AI researchers label whether an output is commonsense knowledge or not. We explore the use of Amazon’s Mechanical Turk service, an online labor market, as a means of acquiring many non-expert judgements for little cost. 2 Related Work While Open Mind Commons (Speer, 2007) asks users to vote for or against commonsense statements contributed by others users in order to come to a consensus, we seek to evaluate an automatic system. Snow et al. (2008) compared the quali"
W10-0724,W08-2219,1,0.817803,"; Schubert and Tong, 2003; Banko et al., 2007). This is time-consuming, and it has the potential for bias: it would be preferable to have people other than AI researchers label whether an output is commonsense knowledge or not. We explore the use of Amazon’s Mechanical Turk service, an online labor market, as a means of acquiring many non-expert judgements for little cost. 2 Related Work While Open Mind Commons (Speer, 2007) asks users to vote for or against commonsense statements contributed by others users in order to come to a consensus, we seek to evaluate an automatic system. Snow et al. (2008) compared the quality of labels produced by non-expert Turkers against those made by experts for a variety of NLP tasks and found that they required only four responses per item to emulate expert annotations. Kittur et al. (2008) describe the use and 1 Public release of the basic K NEXT engine is forthcoming. 159 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 159–162, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics The statement above is a reasonably clear, entirely plausible, generic cla"
W10-0724,D08-1027,0,\N,Missing
W11-2408,W08-2222,0,0.0182933,"not lexical items. We are simply not likely to find multiple occurrences of the same pair of clauses in a variety of syntactic configurations, all indicating a consequence relation – you’re unlikely to find multiple redundant patterns relating clauses, as in ‘Went up to the door but didn’t knock on it’. There is more work to be done to arrive at a reliable, inference-ready knowledge base of such rules. The primary desideratum is to produce a logical representation for the rules such that they can be used in the E PILOG reasoner (Schubert and Hwang, 2000). Computing logical forms (as, e.g., in Bos (2008)) and then deriving logically formulated rules from these rather than deriving sentential forms directly from text should also allow us to be more precise about dropping modifiers, reshaping into generic present tense from other tenses, and other issues that affect the quality of the statements. We have a preliminary version of a logical form generator that derives LFs from TreeBank parses that can support this direction. Further filtering techniques (based both on the surface form and the logical form) should keep the desired inference rules while improving quality. Acknowledgements This work"
W11-2408,A00-2018,0,0.031132,"above is a reasonably clear, entirely plausible, generic claim and seems neither too specific nor too general or vague to be useful: 1. I agree. 2. I lean towards agreement. 3. I’m not sure. 4. I lean towards disagreement. 5. I disagree. Figure 2: Instructions for judging of unsharpened factoids. Judge 1 Judge 2 Correlation 1.84 2.45 0.55 Table 1: Average ratings and Pearson correlation for rules from the personal stories corpus. Lower ratings are better; see Fig. 2. For evaluation, we used a corpus of personal stories from weblogs (Gordon and Swanson, 2009), parsed with a statistical parser (Charniak, 2000). We sampled 100 output rules and rated them on a scale of 1–5 (1 being best) based on the criteria in Fig. 2. To decide if a rule meets the criteria, it is helpful to imagine a dialogue with a computer agent. Told an instantiated form of the antecedent, the agent asks for confirmation of a potential conclusion. E.g., for If attacks are brief, then they may not be intense, the dialogue would go: “The attacks (on Baghdad) were brief.” “So I suppose they weren’t intense, were they?” If this is a reasonable follow-up, then the rule is probably good, although we also disprefer very unlikely antece"
W11-2408,W04-3205,0,0.0935967,"l rules obtained are largely limited to ones expressing some rough synonymy or similarity relation. Pekar (2006) developed related methods for learning the implications of an event based on the regular co-occurrence of two verbs within “locally coherent text”, acquiring rules like ‘x was appointed as y’ suggests that ‘x became y’, but, as in DIRT, we lack information about the types of x and y, and only acquire binary relations. Girju (2003) applied Hearst’s (1998) procedure for finding lexico-syntactic patterns to discover causal relations between nouns, as in ‘Earthquakes generate tsunami’. Chklovski and Pantel (2004) used pat59 Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 59–63, c Edinburgh, Scotland, UK, July 30, 2011. 2011 Association for Computational Linguistics (S < (NP $. (VP < (/,/ $. ( S < ( VP < (VBG < hoping) < (S < (VP < TO)))))))) (S < (NP $. (VP < (( CC < but) $.. (VP < (AUX < did) < (RB < /n[’o]t/)))))) (S < (NP $. (VP < ( AUX $. (ADJP < (JJ $. ((CC < /(but|yet)/) $. JJ))))))) (S < ( NP $. (VP < (/,/ $. ( S < (VP < (( VBG < expecting) $. ( S < (VP < TO))))))))) Figure 1: Examples of TGrep2 patterns for finding parse tree fragments that might be abstract"
W11-2408,W03-1210,0,0.140435,"(inferential selectional preferences – ISP ) based on lexical abstraction from empirically observed argument types. A limitation of the approach is that the conditional rules obtained are largely limited to ones expressing some rough synonymy or similarity relation. Pekar (2006) developed related methods for learning the implications of an event based on the regular co-occurrence of two verbs within “locally coherent text”, acquiring rules like ‘x was appointed as y’ suggests that ‘x became y’, but, as in DIRT, we lack information about the types of x and y, and only acquire binary relations. Girju (2003) applied Hearst’s (1998) procedure for finding lexico-syntactic patterns to discover causal relations between nouns, as in ‘Earthquakes generate tsunami’. Chklovski and Pantel (2004) used pat59 Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 59–63, c Edinburgh, Scotland, UK, July 30, 2011. 2011 Association for Computational Linguistics (S < (NP $. (VP < (/,/ $. ( S < ( VP < (VBG < hoping) < (S < (VP < TO)))))))) (S < (NP $. (VP < (( CC < but) $.. (VP < (AUX < did) < (RB < /n[’o]t/)))))) (S < (NP $. (VP < ( AUX $. (ADJP < (JJ $. ((CC < /(but|yet)/) $. JJ)))))"
W11-2408,N07-1071,0,0.0690266,"n a car crash might be injured and the car damaged is a matter of common sense, and, as such, is rarely stated directly. However, it can be found in sentences where this expectation Related Work One well-known approach to conditional knowledge acquisition is that of Lin and Pantel (2001), where inference rules are learned using distributional similarity between dependency tree paths. These results include entailment rules like ‘x is the author of y ⇔ x wrote y’ (which is true provided x is a literary work) and less dependable ones like ‘x caused y ⇔ y is blamed on x’. This work was refined by Pantel et al. (2007) by assigning the x and y terms semantic types (inferential selectional preferences – ISP ) based on lexical abstraction from empirically observed argument types. A limitation of the approach is that the conditional rules obtained are largely limited to ones expressing some rough synonymy or similarity relation. Pekar (2006) developed related methods for learning the implications of an event based on the regular co-occurrence of two verbs within “locally coherent text”, acquiring rules like ‘x was appointed as y’ suggests that ‘x became y’, but, as in DIRT, we lack information about the types"
W11-2408,N06-1007,0,0.105886,"ributional similarity between dependency tree paths. These results include entailment rules like ‘x is the author of y ⇔ x wrote y’ (which is true provided x is a literary work) and less dependable ones like ‘x caused y ⇔ y is blamed on x’. This work was refined by Pantel et al. (2007) by assigning the x and y terms semantic types (inferential selectional preferences – ISP ) based on lexical abstraction from empirically observed argument types. A limitation of the approach is that the conditional rules obtained are largely limited to ones expressing some rough synonymy or similarity relation. Pekar (2006) developed related methods for learning the implications of an event based on the regular co-occurrence of two verbs within “locally coherent text”, acquiring rules like ‘x was appointed as y’ suggests that ‘x became y’, but, as in DIRT, we lack information about the types of x and y, and only acquire binary relations. Girju (2003) applied Hearst’s (1998) procedure for finding lexico-syntactic patterns to discover causal relations between nouns, as in ‘Earthquakes generate tsunami’. Chklovski and Pantel (2004) used pat59 Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2"
W11-2408,D10-1106,0,0.2236,"ted to inference rules. See Rohde (2001) for an explanation of the syntax. terns like ‘x-ed by y-ing’ (‘obtained by borrowing’) to get co-occurrence data on candidate pairs from the Web. They used these co-occurrence counts to obtain a measure of mutual information between pairs of verbs, and hence to assess the strengths of the relations. A shortcoming of rules obtained in this way is their lack of detailed predicative structure. For inference purposes, it would be insufficient to know that ‘crashes cause injuries’ without having any idea of what is crashing and who or what is being injured. Schoenmackers et al. (2010) derived first-order Horn clauses from the tuple relations found by T EXTRUNNER (Banko et al., 2007). Their system produces rules like ‘IsHeadquarteredIn(Company, State) :- IsBasedIn(Company, City) ∧ IsLocatedIn(City, State)’, which are intended to improve inference for questionanswering. A limitation of this approach is that, operating on the facts discovered by an information extraction system, it largely obtains relations among simple attributes like locations or roles rather than consequences or reasons. 3 Method Our method first uses TGrep2 (Rohde, 2001) to find parse trees matching hand-"
W11-2408,W08-2219,1,0.90079,"Missing"
W11-2408,E09-1092,1,0.915033,"Missing"
W11-2408,baccianella-etal-2010-sentiwordnet,0,\N,Missing
W12-0803,P08-1064,0,0.0341187,"Missing"
W12-0803,P05-1022,0,0.0127459,"or trees are fundamental tools in AI. They facilitate many symbol manipulation tasks, including operations on parse trees and logical forms, and even inference and aspects of dialogue and translation. The TTT system allows concise and transparent specification of rules for such tasks, in particular (as we will show), parse tree refinement and correction, predicate disambiguation, logical form refinement, inference, and verbalization into English. In parse tree refinement, our particular focus has been on repair of malformed parses of image captions, as obtained by the Charniak-Johnson parser (Charniak and Johnson, 2005). This has encompassed such tasks as distinguishing passive participles from past participles and temporal nominals from non-temporal ones, among other tasks which will be discussed later. For example, standard treebank parses tag both past participles (as in “has written”) and passive participles (as in “was written”) as VBN. This is undesirLenhart Schubert University of Rochester Department of Computer Science schubert@cs.rochester.edu able for subsequent compositional interpretation, as the meanings of past and passive participles are distinct. We can easily relabel the past participles as"
W12-0803,P10-1004,0,0.0529658,"Missing"
W12-0803,levy-andrew-2006-tregex,0,0.183118,"rhs must occur at a depth less than two on the left, and Tiburon cannot easily simulate our vertical path or sequence operators. Timbuk is a system for deciding reachability with term rewriting systems and tree automata (Genet, 2003), and it also performs intersection, union, and determinization of tree automata. Though variables can appear at arbitrary locations in terms, they always match exactly one term from a fixed set, and therefore do not match sequences or vertical paths. The three related tools Tgrep, Tregex, and Tsurgeon provide powerful tree matching and restructuring capabilities (Levy and Andrew, 2006). However, Tgrep and Tregex provide no transduction mechanism, and Tsurgeon’s modifications are limited to local transformations on trees. Also, it presupposes list structures that begin with an atom (as in Treebank trees, but not in parse trees with explicit phrasal features), and its patterns are fundamentally tree traversal patterns rather than tree templates, and can be quite hard to read. Xpath and XSLT are languages for manipulation of XML trees (World Wide Web Consortium, 1999; World Wide Web Consortium, 1999). As its name indicates, Xpath expressions describe paths in trees to the rele"
W12-0803,N10-1130,0,0.014495,"mal tree transducers is given in (Knight, 2007). The main properties are restrictions on the height of the tree fragments allowed in rules, linearity, and whether the rules can delete arbitrary subtrees. Among the more popular and recent ones, synchronous tree substitution grammars (STSG), synchronous tree sequence substitution grammars (STSSG), and multi bottom-up tree transducers (MBOT) constrain their rules to be linear and non-deleting, which is important for efficient rule learning and transduction execution (Chiang, 2004; Galley et. al, 2004; Yamada and Knight, 2001; Zhang et. al, 2008; Maletti, 2010). The language TTT does not have any such restrictions, as it is intended as a general programming aid, with a concise syntax for potentially radical transformations, rather than a model of particular classes of linguistic operations. Thus, for example, the 5-element pattern (! ((* A) B) ((* A) C) ((* A) D) ((* A) E) ((* A))) applied to the expression (A A A A A) rescans the latter 5 times, implying quadratic complexity. (Our current implementation does not attempt regular expression reduction for efficient recognition.) With the addition of the permutation operator {}, we can force all permut"
W12-0803,N04-1035,0,\N,Missing
W12-0803,P01-1067,0,\N,Missing
W12-3023,W11-2408,1,0.634901,"Missing"
W12-3023,W11-0116,0,0.0295847,"to live in. They do mal temporal frequency for the event would be. For this by using buckets of numeric phrases in hand- instance, to say that ‘Mary snacks constantly’ (or crafted extraction patterns like ‘(I|he|she) hwordi+ ‘frequently’ or ‘occasionally’) only makes sense if hnumerici hnouni’, which would match ‘she visited you already have in mind some range of frequencies four countries’. They apply these patterns to Google’s that would be normal or unremarkable. More absolute frequency adverbials, such as daily, Web1Tgram Corpus of n-grams. weekly, or every other week avoid the problem of Gusev et al. (2011) presented a similar approach to learning event durations using query patterns sent to depending on a person’s expectations for their meana Web search engine, e.g., ‘heventpast for * hbucketi’, ing. However, these tend to occur with extraordiwhere the bucket is a category in [seconds, minutes, nary rather than ordinary claims. For instance, in the hours, . . . , decades] for classifying the event’s ex- British National Corpus we see pected duration. Both of these papers are notable for ‘Clashes between security forces and students gaining wide coverage by indirectly using Web-scale had occurre"
W12-3023,C92-2082,0,0.0373555,"ther adverbs of quantification like twice. Sentences systematically learning the expected or normal fre- that contain a frequency adverb are referred to as frequency of events in the world. However, our basic quency statements, e.g., ‘John sometimes jogs in the approach to this problem aligns with a long-running park.’ Frequency statements are interesting because line of work using textual references to learn spe- their truth depends not just on the existence of some cific kinds of world knowledge. This approach has past events that support them but on a regular disbeen popular at least since Hearst (1992) used lexico- tribution of events in time. That is, saying that John syntactic patterns like ‘NP0 such as {NP1 , NP2 , . . . , ‘sometimes jogs’ means that it is a habitual rather (and|or)} NPn ’ to learn hyponym relations, such as than incidental activity. ‘Bambara ndang is a bow lute’ from large text corAs Cohen (1999) observes, much of our knowlpora. edge about the world is expressed through frequency In addressing the problem of quantificational dis- statements, but it’s not entirely clear what these senambiguation, Srinivasan and Yates (2009) learn the tences mean. From the perspective of"
W12-3023,D09-1152,0,0.0181858,"support them but on a regular disbeen popular at least since Hearst (1992) used lexico- tribution of events in time. That is, saying that John syntactic patterns like ‘NP0 such as {NP1 , NP2 , . . . , ‘sometimes jogs’ means that it is a habitual rather (and|or)} NPn ’ to learn hyponym relations, such as than incidental activity. ‘Bambara ndang is a bow lute’ from large text corAs Cohen (1999) observes, much of our knowlpora. edge about the world is expressed through frequency In addressing the problem of quantificational dis- statements, but it’s not entirely clear what these senambiguation, Srinivasan and Yates (2009) learn the tences mean. From the perspective of knowledge exexpected sizes of sets of entities that participate in traction, they can seem quite opaque as their meaning a relation; e.g., how many capitals a country has or seems to rely on our pre-existing ideas of what a norhow many cities a person tends to live in. They do mal temporal frequency for the event would be. For this by using buckets of numeric phrases in hand- instance, to say that ‘Mary snacks constantly’ (or crafted extraction patterns like ‘(I|he|she) hwordi+ ‘frequently’ or ‘occasionally’) only makes sense if hnumerici hnouni’"
W12-3023,J93-2004,0,\N,Missing
W14-2411,W13-0103,0,0.158857,"Missing"
W14-2411,Q13-1005,0,0.031704,"d at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge & Mooney 2009, Artzi & Zettlemoyer 2011, In the first-reader domain (where we are using 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi & Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis & Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet, and Draicchio et al.’s (2013) FRED system for mapping from NL to OWL ontology. Boxer2 is highly developed, but interpretations are limited to FOL, so that the kinds of general quantification, reification and modification that pervade ordinary language cannot be adequately captured. The CCG-Distributional approach combines logical and distributional semantics in an"
W14-2411,D13-1160,0,0.0313119,"le and a senior gray-haired female respectively. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge & Mooney 2009, Artzi & Zettlemoyer 2011, In the first-reader domain (where we are using 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi & Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis & Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet, and Draicchio et al.’s (2013) FRED system for mapping from NL to OWL ont"
W14-2411,W08-2222,0,0.0243986,"up (e.g., Ge & Mooney 2009, Artzi & Zettlemoyer 2011, In the first-reader domain (where we are using 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi & Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis & Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet, and Draicchio et al.’s (2013) FRED system for mapping from NL to OWL ontology. Boxer2 is highly developed, but interpretations are limited to FOL, so that the kinds of general quantification, reification and modification that pervade ordinary language cannot be adequately captured. The CCG-Distributional approach combines logical and distributional semantics in an interesting way, but apart from the FOL limitation, the induced cluster-based predicates lose"
W14-2411,P09-1069,0,0.0313948,"senior, hence quite possibly gray-haired, and this enabled correct alignment of the names with the persons detected in the image, determined via image processing to be a young dark-haired female and a senior gray-haired female respectively. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge & Mooney 2009, Artzi & Zettlemoyer 2011, In the first-reader domain (where we are using 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi & Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis & Steedma"
W14-2411,P13-1092,0,0.0371614,"ed via image processing to be a young dark-haired female and a senior gray-haired female respectively. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge & Mooney 2009, Artzi & Zettlemoyer 2011, In the first-reader domain (where we are using 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi & Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis & Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet, and Draicc"
W14-2411,W12-0803,1,0.660465,"d even when parses are deemed correct according to “gold standard"" annotated corpora, they often conflate semantically disparate word and phrase types. For example, prepositional phrases (PPs) functioning as predicates are not distinguished from ones functioning as adverbial modifiers; the roles of wh-words that form questions, relative clauses, or wh-nominals are not distinguished; and constituents parsed as SBARs (subordinate clauses) can be relative clauses, adverbials, question clauses, or clausal nominals. Our approach to these problems makes use of a new tree transduction language, TTT (Purtee & Schubert 2012) that allows concise, modular, declarative representation of tree transductions. (As indicated below, TTT also plays a key role in logical form postprocessing.) While we cannot ex• the use of a tree transduction language, TTT, to partially disambiguate, refine (and sometimes repair) raw Treebank parses, and also to perform many deindexing and logical canonicalization tasks; • the use of EL, a Montague-inspired logical framework for semantic representation and knowledge representation; • allowance for nonclassical restricted quantifiers, several forms of modification and reification, quasi-quot"
W14-2411,D13-1161,0,0.0149599,"-haired female respectively. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge & Mooney 2009, Artzi & Zettlemoyer 2011, In the first-reader domain (where we are using 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi & Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis & Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet, and Draicchio et al.’s (2013) FRED system for mapping from NL to OWL ontology. Boxer2 is highly de"
W14-2411,J82-1003,1,0.636961,"ily complex sentences, rather than just with atomic predications, as in Davidsonian event semantics. For example, the initial sentence in each of the following pairs is interpreted as directly characterizing an episode, which then serves as antecedent for a 4. Comprehensive scoping and tense deindexing Though EL is Montague-inspired, one difference from a Montague-style intensional logic is that we treat noun phrase (NP) interpretations as unscoped elements, rather than second-order predicates. These elements are heuristically scoped to the sentence level in LF postprocessing, as proposed in (Schubert & Pelletier 1982). The latter proposal also covered scoping of logical connectives, which exhibit the same scope ambiguities as quantifiers. Our current heuristic scoping algorithm handles these phenomena as well as tense scope, allowing for such factors as syntactic ordering, island constraints, and differences in widescoping tendencies among different operators. Episodes characterized by sentences remain implicit until application of a “deindexing"" algorithm. This algorithm makes use of a contextual element called a tense tree which is built and traversed in accordance with simple recursive rules applied to"
W14-2411,D11-1140,0,0.0224149,"the persons detected in the image, determined via image processing to be a young dark-haired female and a senior gray-haired female respectively. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge & Mooney 2009, Artzi & Zettlemoyer 2011, In the first-reader domain (where we are using 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi & Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis & Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb on"
W14-2411,Q13-1015,0,0.0520515,"Missing"
W14-2411,P11-1060,0,0.0364024,"the image, determined via image processing to be a young dark-haired female and a senior gray-haired female respectively. 6. Related work Most current projects in semantic parsing either single out domains that assure highly restricted natural language usage, or greatly limit the semantic content that is extracted from text. For example, projects may be aimed at question-answering over relational databases, with themes such as geography, air travel planning, or robocup (e.g., Ge & Mooney 2009, Artzi & Zettlemoyer 2011, In the first-reader domain (where we are using 58 Kwiatkowski et al. 2011, Liang et al. 2011, Poon 2013). Impressive thematic scope is achieved in (Berant et al. 2013, Kwiatkowski et al. 2013), but the target semantic language (for Freebase access) is still restricted to database operations such as join, intersection, and set cardinality. Another popular domain is command execution by robots (e.g., Tellex 2011, Howard et al. 2013, Artzi & Zettlemoyer 2013). Examples of work aimed at broader linguistic coverage are Johan Bos’ Boxer project (Bos 2008), Lewis & Steedman’s (2013) CCGDistributional system, James Allen et al.’s (2013) work on extracting an OWL-DL verb ontology from WordNet"
W14-2411,W08-2219,1,0.883917,"Missing"
W14-2411,W09-3714,0,0.0269821,"ctability tradeoff. We reject such motivations – tools should be made to fit the phenomenon rather than the other way around. The tractability argument, for example, is simply mistaken: Efficient inference algorithms for subsets of an expressive representation can also be implemented within a more comprehensive inference framework, without forfeiting the advantages of expressiveness. Moreover, recent work in Natural Logic, which uses phrase-structured NL directly for inference, indicates that the richness of language is no obstacle to rapid inference of many obvious lexical entailments (e.g., MacCartney & Manning 2009). Thus our target representation, EL, taking its cue from Montague allows directly for the kinds of quantification, intensionality, modification, and reification found in all natural languages (e.g., Schubert & Hwang 2000, Schubert, to appear). In addition, EL associates episodes (events, situations, processes) directly with arbitrarily complex sentences, rather than just with atomic predications, as in Davidsonian event semantics. For example, the initial sentence in each of the following pairs is interpreted as directly characterizing an episode, which then serves as antecedent for a 4. Comp"
W14-2411,D11-1039,0,\N,Missing
W14-2411,2014.lilt-9.9,1,\N,Missing
W17-1802,W15-0128,0,0.0189568,"ntages over directly annotating EL logical forms. ULF enables the separation of determining the semantic type structure from replacing indexial expressions and disambiguating quantifier scopes, word senses, and anaphora – tasks which in general require the context of the sentence to resolve. Since we are tackling a range of subtle semantic phenomena beyond those ordinarily considered, this decomposition is likely to achieve better results than a fellswoop approach. An undisambiguated representation also has the advantage of adaptability to a wide range of tasks – a topic discussed in depth by Bender et al. (2015). 3 Intensional Modifiers Attitude Predicates Attitude predicates such as assert, believe, and assume relate an individual to a proposition. Propositions are treated as abstract entities, namely, reified sentence intensions. Of course an attitude predication can be true without the proposition being true. Unlike some semantic representations, EL does not conflate propositions with episodes. Episodes are real (often physical) entities occupying time intervals, whereas propositions are informational entities. Propositions are formed from sentences using a that operator, since they are most commo"
W17-1802,P03-1011,0,0.0141943,"eep.v])) In sentence (a) confidently is a predicate modifier whereas in sentence (b) undoubtedly is a sentence modifier. Clearly, this is entirely determined by the lexical entry since the syntax trees of the 1 Some clauses used as arguments denote episode types, e.g., For Mary to be late is unusual; we distinguish such cases but omit details here. 2 Formal details of the treatment of tense and temporal adverbials in EL are given in (Hwang and Schubert, 1994). 12 (e) “He may have been sleeping” we expect machine translation methods such as Synchronous Tree Substitution Grammars (Eisner, 2003; Gildea, 2003) to be successful in automating this annotation because of the close syntactic correspondence to the surface form. (&lt;pres may.aux&gt; (perf (prog [he.pro sleep.v]))) Sentence (a) is a simple sentence where the tense is determined by the verb. Sentence (b), (c), and (d) show how had and has determine the tense of the sentence. Note that in all three cases the perfect auxiliary is followed by the past participle form of the verb. This is simply a syntactic requirement in English. Sentence (e) shows an example where the modal auxiliary determines the tense. 7 8 In view of its English-like syntax, ou"
W17-1802,N06-2015,0,0.0104054,"s, the annotation guidelines will not yet be publicly released. Also, since the phenomena described in this document cannot be annotated in isolation in our framework, there are no semantic category-specific preliminary annotations to report. We expect the annotation effort to be successful because ULF is syntactically close to surface English and the annotator tools under development will simplify the annotation task. Similarly, Related Work Previous efforts have been made toward training a transducer for broad coverage meaning representation of sentences, perhaps most prominently OntoNotes (Hovy et al., 2006) and AMR (Banarescu et al., 2013). These representations employed PropBank, WordNet, VerbNet, and FrameNet as semantic resources, but were not designed to be formally interpretable. Semantic types of nodes are not defined, there is no distiction between extension and intension (or between what is real and what is hypothetical), and thus there is no clear basis for inference. The representations also set aside some important linguistic phenomena, such as tense (hence, how events are temporally linked); and quantifiers are added in modifier-like fashion, much as if they were attributes of entiti"
W17-1802,S16-2004,1,0.780789,"m and Lenhart Schubert University of Rochester Department of Computer Science {gkim21,schubert}@cs.rochester.edu Abstract icates, as long as we restrict ourselves to positive, atomic predications. But it also allows for logically complex characterizations of episodes, such as episodes of not eating anything all day, or of each superpower menacing the other with its nuclear arsenal (Schubert, 2000). EL has been shown to be suitable for deductive inference, uncertain inference, and Natural-Logiclike inference (Morbini and Schubert, 2009; Schubert and Hwang, 2000; Schubert, 2014). Most recently, Kim and Schubert (2016) developed a system that generated EL verb gloss axioms from WordNet, which enabled inferences that were competitive with the state-of-the-art even with greater expressivity. In a supplementary document for the above paper, Kim and Schubert present an illustration of EL appropriately handling the intensional predicate modifier nearly. The illustration uses the gloss for the second sense of stumble, which is miss a step and fall or nearly fall and shows that using EL as the representation enables inferences that are not possible using intersective predicate modification. We are currently underw"
W17-1802,W13-2322,0,0.0302634,"will not yet be publicly released. Also, since the phenomena described in this document cannot be annotated in isolation in our framework, there are no semantic category-specific preliminary annotations to report. We expect the annotation effort to be successful because ULF is syntactically close to surface English and the annotator tools under development will simplify the annotation task. Similarly, Related Work Previous efforts have been made toward training a transducer for broad coverage meaning representation of sentences, perhaps most prominently OntoNotes (Hovy et al., 2006) and AMR (Banarescu et al., 2013). These representations employed PropBank, WordNet, VerbNet, and FrameNet as semantic resources, but were not designed to be formally interpretable. Semantic types of nodes are not defined, there is no distiction between extension and intension (or between what is real and what is hypothetical), and thus there is no clear basis for inference. The representations also set aside some important linguistic phenomena, such as tense (hence, how events are temporally linked); and quantifiers are added in modifier-like fashion, much as if they were attributes of entities. DeepBank is a corpus of annot"
W17-1802,W14-2411,1,0.779122,"Semantic Representation Gene Kim and Lenhart Schubert University of Rochester Department of Computer Science {gkim21,schubert}@cs.rochester.edu Abstract icates, as long as we restrict ourselves to positive, atomic predications. But it also allows for logically complex characterizations of episodes, such as episodes of not eating anything all day, or of each superpower menacing the other with its nuclear arsenal (Schubert, 2000). EL has been shown to be suitable for deductive inference, uncertain inference, and Natural-Logiclike inference (Morbini and Schubert, 2009; Schubert and Hwang, 2000; Schubert, 2014). Most recently, Kim and Schubert (2016) developed a system that generated EL verb gloss axioms from WordNet, which enabled inferences that were competitive with the state-of-the-art even with greater expressivity. In a supplementary document for the above paper, Kim and Schubert present an illustration of EL appropriately handling the intensional predicate modifier nearly. The illustration uses the gloss for the second sense of stumble, which is miss a step and fall or nearly fall and shows that using EL as the representation enables inferences that are not possible using intersective predica"
W17-1802,P03-2041,0,\N,Missing
W18-1403,D14-1217,0,0.27769,"y include locations, sizes and distances. Nongeometric factors include background knowledge about the relata—their physical properties, roles, the way we interact with them—as well as the perceived “frame” and the presence and characteristics of other objects within that frame. We use a 3D modeling approach in our work. Thus geometric factors can be directly inferred from the coordinates of the polygonal meshes In recent years, attempts have been made to use statistical learning models, especially deep neural networks, to learn spatial relations. Noteworthy examples are Bisk et al. (2017) and Chang et al. (2014). The first study was dedicated to learning spatial prepositions from images with accompanying textual annotation data within a blocks world domain. The experimental task was based on a se1 The implementation and all the accompanying data can be found at https://github.com/gplatono/SRP/tree/master/blender project 23 comprising the object’s model. We add additional geometric and non-geometric knowledge about the objects by manually attaching labels or tags to the meshes. Our approach is a rule-based one. Each spatial relation takes two (or three, in case of between) arguments and applies a sequ"
W19-0402,P89-1004,0,0.132505,"Missing"
W19-0402,D15-1198,0,0.0154101,"MR deliberately neglected issues such as articles, tense, the distinction between real and hypothetical entities, and nonintersective modification. In the context of inference, this risks making false conclusions such as that a “big ant” is bigger than a “small elephant”. Still, this development was an inspiration to us in terms of both the quest for broad coverage and methods of learning and evaluating semantic parsers. There has also been much activity in developing semantic parsers that derive logical representations, raising the possibility of making inferences with those representations (Artzi et al., 2015; Artzi and Zettlemoyer, 2013; Howard et al., 2014; Kate and Mooney, 2006; Konstas et al., 2017; Kwiatkowski et al., 2011; Liang et al., 2011; Poon, 2013; Popescu et al., 2004; Tellex et al., 2011). The techniques and formalisms employed are interesting (e.g., learning of CCG grammars that generate λ-calculus expressions), but the targeted tasks have generally been question-answering in domains consisting of numerous monadic and dyadic ground facts (“triples""), or simple robotic or human action descriptions.6 Noteworthy examples of formal logic-based approaches, not targeting specific applicat"
W19-0402,Q13-1005,0,0.0302904,"ected issues such as articles, tense, the distinction between real and hypothetical entities, and nonintersective modification. In the context of inference, this risks making false conclusions such as that a “big ant” is bigger than a “small elephant”. Still, this development was an inspiration to us in terms of both the quest for broad coverage and methods of learning and evaluating semantic parsers. There has also been much activity in developing semantic parsers that derive logical representations, raising the possibility of making inferences with those representations (Artzi et al., 2015; Artzi and Zettlemoyer, 2013; Howard et al., 2014; Kate and Mooney, 2006; Konstas et al., 2017; Kwiatkowski et al., 2011; Liang et al., 2011; Poon, 2013; Popescu et al., 2004; Tellex et al., 2011). The techniques and formalisms employed are interesting (e.g., learning of CCG grammars that generate λ-calculus expressions), but the targeted tasks have generally been question-answering in domains consisting of numerous monadic and dyadic ground facts (“triples""), or simple robotic or human action descriptions.6 Noteworthy examples of formal logic-based approaches, not targeting specific applications are Bos’ (2008) and Drai"
W19-0402,W13-2322,0,0.16264,"y surface-like LFs is a practical way to generate fully resolved interpretations of natural language in EL. Figure 1 shows a diagram of our divide-and-conquer approach, which is elaborated upon in Section 3.3. We also outline a framework for quickly and reliably collecting ULF annotations for a corpus in a multi-pronged approach. Our evaluation of the annotation framework shows that we achieve annotation speeds and agreement comparable to those for the abstract meaning representation (AMR) project, which has successfully built a large enough corpus to drive research into corpus-based parsing (Banarescu et al., 2013). Further resources relating to this project, including a more in-depth description of ULFs, the annotation guidelines, and related code are available from the project website http://cs.rochester.edu/u/gkim21/ulf/. 2 Episodic Logic EL is a semantic representation that extends FOL to more closely match the expressivity of natural languages. It echoes both the surface form of language, and more crucially, the semantic types that are found in all languages. Some semantic theorists view the fact that noun phrases denoting both concrete and abstract entities can appear as predicate arguments (Arist"
W19-0402,W08-2222,0,0.0338219,"Missing"
W19-0402,P17-1014,0,0.017688,"pothetical entities, and nonintersective modification. In the context of inference, this risks making false conclusions such as that a “big ant” is bigger than a “small elephant”. Still, this development was an inspiration to us in terms of both the quest for broad coverage and methods of learning and evaluating semantic parsers. There has also been much activity in developing semantic parsers that derive logical representations, raising the possibility of making inferences with those representations (Artzi et al., 2015; Artzi and Zettlemoyer, 2013; Howard et al., 2014; Kate and Mooney, 2006; Konstas et al., 2017; Kwiatkowski et al., 2011; Liang et al., 2011; Poon, 2013; Popescu et al., 2004; Tellex et al., 2011). The techniques and formalisms employed are interesting (e.g., learning of CCG grammars that generate λ-calculus expressions), but the targeted tasks have generally been question-answering in domains consisting of numerous monadic and dyadic ground facts (“triples""), or simple robotic or human action descriptions.6 Noteworthy examples of formal logic-based approaches, not targeting specific applications are Bos’ (2008) and Draiccio et al.’s (2013), whose hand-built semantic parsers respective"
W19-0402,D11-1140,0,0.0140544,"nd nonintersective modification. In the context of inference, this risks making false conclusions such as that a “big ant” is bigger than a “small elephant”. Still, this development was an inspiration to us in terms of both the quest for broad coverage and methods of learning and evaluating semantic parsers. There has also been much activity in developing semantic parsers that derive logical representations, raising the possibility of making inferences with those representations (Artzi et al., 2015; Artzi and Zettlemoyer, 2013; Howard et al., 2014; Kate and Mooney, 2006; Konstas et al., 2017; Kwiatkowski et al., 2011; Liang et al., 2011; Poon, 2013; Popescu et al., 2004; Tellex et al., 2011). The techniques and formalisms employed are interesting (e.g., learning of CCG grammars that generate λ-calculus expressions), but the targeted tasks have generally been question-answering in domains consisting of numerous monadic and dyadic ground facts (“triples""), or simple robotic or human action descriptions.6 Noteworthy examples of formal logic-based approaches, not targeting specific applications are Bos’ (2008) and Draiccio et al.’s (2013), whose hand-built semantic parsers respectively generate FOL formulas a"
W19-0402,C02-1150,0,0.252517,"annotations 3 0.69/0.75 0.63/0.73 marked as certain, indicates room for improvement in 4 0.62/0.71 the annotation guidelines and training of some annotators. We have so far collected 927 certain annotations and have 1,580 in total. The full annotation breakdown is in Table 1. We started with the English portion of the Tatoeba dataset (https://tatoeba.org/ eng/), a crowd-sourced translation dataset. This source tends to have shorter sentences, but they are more varied in topic and form. We then added text from Project Gutenberg (http://gutenberg.org), the UIUC Question Classification dataset (Li and Roth, 2002), and the Discourse Graphbank (Wolf, 2005). Preliminary parsing experiments on a small dataset (900 sentences) show promising results and we expect to be able to build an accurate parser with a moderately-sized dataset and representation-specific engineering (Kim, 2019). 7 Related Work A notable development in general representations of semantic content has been the design of AMR (Banarescu et al., 2013) followed by numerous research studies on generating AMR from English and on using it for downstream tasks. AMR is intended as a kind of intuitive normal form for the relational context of Engl"
W19-0402,P11-1060,0,0.01194,"ation. In the context of inference, this risks making false conclusions such as that a “big ant” is bigger than a “small elephant”. Still, this development was an inspiration to us in terms of both the quest for broad coverage and methods of learning and evaluating semantic parsers. There has also been much activity in developing semantic parsers that derive logical representations, raising the possibility of making inferences with those representations (Artzi et al., 2015; Artzi and Zettlemoyer, 2013; Howard et al., 2014; Kate and Mooney, 2006; Konstas et al., 2017; Kwiatkowski et al., 2011; Liang et al., 2011; Poon, 2013; Popescu et al., 2004; Tellex et al., 2011). The techniques and formalisms employed are interesting (e.g., learning of CCG grammars that generate λ-calculus expressions), but the targeted tasks have generally been question-answering in domains consisting of numerous monadic and dyadic ground facts (“triples""), or simple robotic or human action descriptions.6 Noteworthy examples of formal logic-based approaches, not targeting specific applications are Bos’ (2008) and Draiccio et al.’s (2013), whose hand-built semantic parsers respectively generate FOL formulas and OWL-DL expression"
W19-0402,P13-1007,0,0.01591,"ators) can generally “float"" to more than one possible position. Following a view of scope ambiguity developed by Schubert and Pelletier (1982) elaborated by Hurum and Schubert (1986), these constituents always float to pre-sentential positions, and determiner phrases leave behind a variable that is then bound at the sentential level. The accessible positions are constrained by linguistic restrictions, such as scope island constraints in subordinate clauses (Ruys and Winter, 2010). Beyond this, many factors influence preferred scoping possibilities, with surface form playing a prominent role (Manshadi et al., 2013). The proximity of ULF to surface syntax enables the use of these constraints. Deindexing and Canonicalization: Much of the past work relating to EL has been concerned with the principles of deindexing (Hwang, 1992; Hwang and Schubert, 1994; Schubert and Hwang, 2000). Deindexing corresponds to the introduction of event variables for explicitly characterizing the sentence it is linked to via the ‘**’ operator (this variable becomes |E|.sk in Figure 1 after Skolemization). Hwang and Schubert’s approach to tense-aspect processing, constructing tense trees for temporally relating event variables,"
W19-0402,C02-2025,0,0.073174,"it focuses on binary structural relations such as restrictor, body, or modifier between semantic components, rather than operator-operand type structure. It is not directly intended for inference, but readily lends itself to incremental disambiguation. We are not aware of any work on inference generation of the type ULFs targets, based on these projects. A couple of yet-unmentioned but notable semantic annotation projects are the Groningen Meaning Bank (Bos et al., 2017), with discourse representation structure (DRS) annotations (Kamp, 1981) and the Redwoods treebank (Flickinger et al., 2012; Oepen et al., 2002) with Minimal Recursion Semantics (MRS) (Copestake et al., 2005) annotations. DRSs have the same representational limitations as Bos’ (2008) system. MRS is descriptively powerful and linguistically motivated, with significant resources including a hand-built grammar, multiple parsers, and a large annotated dataset (Bub et al., 1997; Callmeier, 2001). Given that MRS and Manshadi and Allen’s graphical representation are objectlanguage agnostic, meta-level semantic representations, inference systems cannot be built directly for them based on model-theoretic notions of interpretation, truth, satis"
W19-0402,P13-1092,0,0.0170156,"t of inference, this risks making false conclusions such as that a “big ant” is bigger than a “small elephant”. Still, this development was an inspiration to us in terms of both the quest for broad coverage and methods of learning and evaluating semantic parsers. There has also been much activity in developing semantic parsers that derive logical representations, raising the possibility of making inferences with those representations (Artzi et al., 2015; Artzi and Zettlemoyer, 2013; Howard et al., 2014; Kate and Mooney, 2006; Konstas et al., 2017; Kwiatkowski et al., 2011; Liang et al., 2011; Poon, 2013; Popescu et al., 2004; Tellex et al., 2011). The techniques and formalisms employed are interesting (e.g., learning of CCG grammars that generate λ-calculus expressions), but the targeted tasks have generally been question-answering in domains consisting of numerous monadic and dyadic ground facts (“triples""), or simple robotic or human action descriptions.6 Noteworthy examples of formal logic-based approaches, not targeting specific applications are Bos’ (2008) and Draiccio et al.’s (2013), whose hand-built semantic parsers respectively generate FOL formulas and OWL-DL expressions. But these"
W19-0402,C04-1021,0,0.112482,"ce, this risks making false conclusions such as that a “big ant” is bigger than a “small elephant”. Still, this development was an inspiration to us in terms of both the quest for broad coverage and methods of learning and evaluating semantic parsers. There has also been much activity in developing semantic parsers that derive logical representations, raising the possibility of making inferences with those representations (Artzi et al., 2015; Artzi and Zettlemoyer, 2013; Howard et al., 2014; Kate and Mooney, 2006; Konstas et al., 2017; Kwiatkowski et al., 2011; Liang et al., 2011; Poon, 2013; Popescu et al., 2004; Tellex et al., 2011). The techniques and formalisms employed are interesting (e.g., learning of CCG grammars that generate λ-calculus expressions), but the targeted tasks have generally been question-answering in domains consisting of numerous monadic and dyadic ground facts (“triples""), or simple robotic or human action descriptions.6 Noteworthy examples of formal logic-based approaches, not targeting specific applications are Bos’ (2008) and Draiccio et al.’s (2013), whose hand-built semantic parsers respectively generate FOL formulas and OWL-DL expressions. But these representations precl"
W19-0402,D18-1285,0,0.0330046,"Missing"
W19-0402,W14-2411,1,0.902708,"otator agreement of 0.88 on confident annotations. We hypothesize that a divide-and-conquer approach to semantic parsing starting with derivation of ULFs will lead to semantic analyses that do justice to subtle aspects of linguistic meaning, and will enable construction of more accurate semantic parsers. 1 Introduction Episodic Logic (EL) is a semantic representation extending FOL, designed to closely match the expressivity and surface form of natural language and to enable deductive inference, uncertain inference, and NLog-like inference (Morbini and Schubert, 2009; Schubert and Hwang, 2000; Schubert, 2014). Kim and Schubert (2016) developed a system that transforms annotated WordNet glosses into EL axioms which were competitive with state-of-the-art lexical inference systems while achieving greater expressivity. While EL is representationally appropriate for language understanding, the current EL parser is too unreliable for general text: The phrase structures produced by the underlying Treebank parser leave many ambiguities in the semantic type structure, which are disambiguated incorrectly by the hand-coded compositional rules; moreover, errors in the phrase structures can further disrupt the"
W19-0402,J82-1003,1,0.452233,"straints. While ULF 3 (for.p me.pro) has type D Ñ pS Ñ 2q and |Snoopy |has type D , so (|Snoopy |(for.p me.pro)) has a type that resolves to S Ñ 2 (i.e. a sentence intension). constrains the word senses and coreferences through adicity and syntactic structure, WSD and anaphora resolution should not be applied to isolated sentences since word sense patterns and coreference chains often span multiple sentences. Scoping: Unscoped constituents (determiners, tense operators, and coordinators) can generally “float"" to more than one possible position. Following a view of scope ambiguity developed by Schubert and Pelletier (1982) elaborated by Hurum and Schubert (1986), these constituents always float to pre-sentential positions, and determiner phrases leave behind a variable that is then bound at the sentential level. The accessible positions are constrained by linguistic restrictions, such as scope island constraints in subordinate clauses (Ruys and Winter, 2010). Beyond this, many factors influence preferred scoping possibilities, with surface form playing a prominent role (Manshadi et al., 2013). The proximity of ULF to surface syntax enables the use of these constraints. Deindexing and Canonicalization: Much of t"
W19-0402,P06-1115,0,\N,Missing
W19-0402,P13-2131,0,\N,Missing
W19-0402,2014.lilt-9.9,1,\N,Missing
W19-0402,S16-2004,1,\N,Missing
W19-1102,D13-1185,0,0.0187515,", 1975; Fillmore and Baker, 2010). Schank and Abelson’s scripts successfully answered questions in the restaurant domain (among others) and Minsky’s frames formed the basis for a number of AI systems for the remainder of the 20th century (Bobrow and Winograd, 1976; Fikes and Kehler, 1985; MacGregor and Burstein, 1991). However, these systems were limited to generating inferences from manually constructed frames. Recent progress in learning schema-like knowledge has primarily been driven by applying statistical or neural network-based methods to large text corpora (Chambers and Jurafsky, 2011; Chambers, 2013; Pichotta and Mooney, 2016; Yuan et al., 2018). The learned schemas are quite limited in their capacity to enable inferences since they only describe high-level roles or temporal sequences. Perhaps this is all that can be expected from methods that are given minimal guidance and rely on many similar examples to find patterns. Wanzare et al. (2017) use crowdsourcing to help improve their clustering results and their scripts are made up of graph-ordered event clusters, but the clusters are groups of unstructured text segments. The goal of the schema framework we describe is to generate rich inf"
W19-1102,P11-1098,0,0.03981,"son, 1977) and frames (Minsky, 1975; Fillmore and Baker, 2010). Schank and Abelson’s scripts successfully answered questions in the restaurant domain (among others) and Minsky’s frames formed the basis for a number of AI systems for the remainder of the 20th century (Bobrow and Winograd, 1976; Fikes and Kehler, 1985; MacGregor and Burstein, 1991). However, these systems were limited to generating inferences from manually constructed frames. Recent progress in learning schema-like knowledge has primarily been driven by applying statistical or neural network-based methods to large text corpora (Chambers and Jurafsky, 2011; Chambers, 2013; Pichotta and Mooney, 2016; Yuan et al., 2018). The learned schemas are quite limited in their capacity to enable inferences since they only describe high-level roles or temporal sequences. Perhaps this is all that can be expected from methods that are given minimal guidance and rely on many similar examples to find patterns. Wanzare et al. (2017) use crowdsourcing to help improve their clustering results and their scripts are made up of graph-ordered event clusters, but the clusters are groups of unstructured text segments. The goal of the schema framework we describe is to g"
W19-1102,P92-1030,1,0.482852,"d each story includes similar words used in similar contexts: “These men 8 fish in the sea”, “We will take the long rod, and the hook and line”, “Here is Tom with his rod and line”, “Sometimes they sit on the bank of the river”, and many more thematically parallel sentences in the stories strongly suggest a schema where people are near water, have a rod, and catch fish with a rod or a net. After extracting a set of recurring events from the stories—like going to water, having a rod, catching fish, and putting fish in a basket—we plan to experiment with using narrative models like tense trees (Hwang and Schubert, 1992) to find subsequences of the events that could be interpreted as steps in a schema. After extracting subsequences of similar events that occur in two or more stories, we can use knowledge of basic motivations—e.g. people often want to possess things—to infer a teleology of those events. If some unknown action “catch ?x” always occurs before “possess ?x”, we might hypothesize that the catching something has an effect of possessing that thing. If that sequence is only ever seen with fish and crabs as ?x, our confidence in the “catch to possess” schema might be lower when the object is not a mari"
W19-1102,S16-2004,1,0.862114,"r an example sentence. domain of individuals and includes categories such as basic individuals (e.g., John, the Blarney Stone, or the Earth’s magnetic field), episodes (events, situations, processes), sets, numbers, propositions, and kinds. Schubert and Hwang (2000) provide a complete description of the ontology. EL has been shown to be suitable for deductive inference, uncertain inference, and Natural-Logic-like inference and has been used successfully to represent inference-enabling verb axioms (Morbini and Schubert, 2009; Schubert and Hwang, 2000; Schubert, 2014; Purtee and Schubert, 2017; Kim and Schubert, 2016). Inferences can be generated using the E PILOG inference engine (Morbini and Schubert, 2009; Schaeffer et al., 1993). Most lexical items in EL are represented in the form [word][sense num].[lexical type], e.g., run1.v.2 Lexical types in EL are closely related to POS tags (e.g. .v, .p, and .d for verbs, prepositions, and determiners, respectively) but are constrained in their use by the EL semantic type system; e.g., modal can becomes can.aux-s or can.aux-v depending on its function as a sentence-level possibility operator or a VP-level ability operator. Names (denoting basic individuals) are"
W19-1102,W19-0402,1,0.836198,"herence within the logical formalism. It turns out that ULF provides enough semantic resolution for enabling schema inferences in most of the first-reader stories we have considered. For a small number of cases, the ambiguous components in ULFs are resolved on an as-needed basis. Here we describe ULF to the extent necessary to understand its application within the presented schema description and examples. 2 We 3 ka use WordNet senses in this document (Miller, 1995), but EL is not strictly tied to WordNet. can be expressed in terms of k, forming a kind whose instances are agent-event pairs. 3 Kim and Schubert (2019) provides a more complete description of ULF and its uses outside of schemas. Figure 1 shows the ULF for a sentence alongside the EL interpretation and demonstrates a few key differences between EL and ULF. 1. ULF does not have episode variables. ULF preserves type coherence in the face of implicit episodes and actions by introducing operators to form episode- and action-modifying adverbials from predicate intensions (adv-e, adv-a). Tense and aspectual operators are also implicitly episode-modifiers. 2. Scope, anaphora, and word sense are unresolved. Without scope or anaphora resolution, “the"
W19-1102,W16-6003,0,0.0193751,"and Baker, 2010). Schank and Abelson’s scripts successfully answered questions in the restaurant domain (among others) and Minsky’s frames formed the basis for a number of AI systems for the remainder of the 20th century (Bobrow and Winograd, 1976; Fikes and Kehler, 1985; MacGregor and Burstein, 1991). However, these systems were limited to generating inferences from manually constructed frames. Recent progress in learning schema-like knowledge has primarily been driven by applying statistical or neural network-based methods to large text corpora (Chambers and Jurafsky, 2011; Chambers, 2013; Pichotta and Mooney, 2016; Yuan et al., 2018). The learned schemas are quite limited in their capacity to enable inferences since they only describe high-level roles or temporal sequences. Perhaps this is all that can be expected from methods that are given minimal guidance and rely on many similar examples to find patterns. Wanzare et al. (2017) use crowdsourcing to help improve their clustering results and their scripts are made up of graph-ordered event clusters, but the clusters are groups of unstructured text segments. The goal of the schema framework we describe is to generate rich inferences which can be used t"
W19-1102,W14-2411,1,0.783079,"ark.n))))) Figure 1: EL and ULF formulas for an example sentence. domain of individuals and includes categories such as basic individuals (e.g., John, the Blarney Stone, or the Earth’s magnetic field), episodes (events, situations, processes), sets, numbers, propositions, and kinds. Schubert and Hwang (2000) provide a complete description of the ontology. EL has been shown to be suitable for deductive inference, uncertain inference, and Natural-Logic-like inference and has been used successfully to represent inference-enabling verb axioms (Morbini and Schubert, 2009; Schubert and Hwang, 2000; Schubert, 2014; Purtee and Schubert, 2017; Kim and Schubert, 2016). Inferences can be generated using the E PILOG inference engine (Morbini and Schubert, 2009; Schaeffer et al., 1993). Most lexical items in EL are represented in the form [word][sense num].[lexical type], e.g., run1.v.2 Lexical types in EL are closely related to POS tags (e.g. .v, .p, and .d for verbs, prepositions, and determiners, respectively) but are constrained in their use by the EL semantic type system; e.g., modal can becomes can.aux-s or can.aux-v depending on its function as a sentence-level possibility operator or a VP-level abili"
W19-1102,W17-0901,0,0.0295384,"were limited to generating inferences from manually constructed frames. Recent progress in learning schema-like knowledge has primarily been driven by applying statistical or neural network-based methods to large text corpora (Chambers and Jurafsky, 2011; Chambers, 2013; Pichotta and Mooney, 2016; Yuan et al., 2018). The learned schemas are quite limited in their capacity to enable inferences since they only describe high-level roles or temporal sequences. Perhaps this is all that can be expected from methods that are given minimal guidance and rely on many similar examples to find patterns. Wanzare et al. (2017) use crowdsourcing to help improve their clustering results and their scripts are made up of graph-ordered event clusters, but the clusters are groups of unstructured text segments. The goal of the schema framework we describe is to generate rich inferences which can be used to learn further schemas from a relatively limited number of examples. 3 Episodic Logic and Its Underspecified Form Before diving into the details of our schemas, we first must describe the logical formalism in which the schemas are encoded, called Episodic Logic (EL) (Hwang, 1992; Hwang and Schubert, 1993; Schubert and Hw"
W19-3306,W15-3801,0,0.0961532,"Missing"
W19-3306,W13-2322,0,0.0434817,"in the BLEU scores, and for structurally oriented inferences, incorrect inferences are likely to have misleadingly high scores. Representation Structures and Minimal Recursion Semantics (MRS) can both be mapped to FOL and run on FOL theorem provers (Kamp and Reyle, 1993; Copestake et al., 2005). MRS has been successfully used for the task of recognizing textual entailment (RTE) (Lien and Kouylekov, 2015). Similarly, EL has been shown successful in generating FOL inferences (Morbini and Schubert, 2009) and self-aware metareasoning (Morbini and Schubert, 2011). Abstract Meaning Representation (Banarescu et al., 2013) focuses on event structure, resolution of anaphora, and word senses rather than logical inference and has been demonstrated to support event extraction and summarization (Rao et al., 2017; Wang et al., 2017; Dohare et al., 2017). TRIPS LF (Allen, 1994; Manshadi et al., 2008) is an unscoped modal logic directly integrated with a lexical ontology and has been used for dialogue and biomedical event extraction (Perera et al., 2018; Allen et al., 2015). Distributional representations have been shown to be very effective for RTE, such as in the SNLI and MultiNLI datasets (Bowman et al., 2015; Willi"
W19-3306,D15-1075,0,0.0460981,"n (Banarescu et al., 2013) focuses on event structure, resolution of anaphora, and word senses rather than logical inference and has been demonstrated to support event extraction and summarization (Rao et al., 2017; Wang et al., 2017; Dohare et al., 2017). TRIPS LF (Allen, 1994; Manshadi et al., 2008) is an unscoped modal logic directly integrated with a lexical ontology and has been used for dialogue and biomedical event extraction (Perera et al., 2018; Allen et al., 2015). Distributional representations have been shown to be very effective for RTE, such as in the SNLI and MultiNLI datasets (Bowman et al., 2015; Williams et al., 2018). These datasets are much larger than previous RTE datasets and both provide classification tasks supporting the use of an implicit distributional representation in a neural network system. The discourse inferences we demonstrated with ULFs, which require access to some syntactic information, as well our evaluations based on reliable English generation, are a challenge to all of the semantic representations discussed, because of their relative remoteness from syntax. 8 Conclusions We presented the first known method of generating inferences from ULF and an evaluation of"
W19-3306,W18-5010,0,0.0232252,"own successful in generating FOL inferences (Morbini and Schubert, 2009) and self-aware metareasoning (Morbini and Schubert, 2011). Abstract Meaning Representation (Banarescu et al., 2013) focuses on event structure, resolution of anaphora, and word senses rather than logical inference and has been demonstrated to support event extraction and summarization (Rao et al., 2017; Wang et al., 2017; Dohare et al., 2017). TRIPS LF (Allen, 1994; Manshadi et al., 2008) is an unscoped modal logic directly integrated with a lexical ontology and has been used for dialogue and biomedical event extraction (Perera et al., 2018; Allen et al., 2015). Distributional representations have been shown to be very effective for RTE, such as in the SNLI and MultiNLI datasets (Bowman et al., 2015; Williams et al., 2018). These datasets are much larger than previous RTE datasets and both provide classification tasks supporting the use of an implicit distributional representation in a neural network system. The discourse inferences we demonstrated with ULFs, which require access to some syntactic information, as well our evaluations based on reliable English generation, are a challenge to all of the semantic representations dis"
W19-3306,C94-2149,0,0.112575,"es structurally. The annotators are additionally instructed to keep the inference as fluent as possible, preserve the original sentence as much as possible, and keep the perspective of the speaker of the sentence. We also included an option for annotators to add new rules, to extend the dataset into categories we did not anticipate. This category will be referred to as Other. conjugations of “will”. Tags for closed classes of words and shorthands for common non-word patterns were hand-curated. Tags for open classes such as &lt;past&gt; and &lt;ppart&gt; are generated from the XTAG morphological database (Doran et al., 1994) with minor edits during the development process. 4.3 Sentence selection After performing filtering, we still have far too many sentences to feasibly annotate, so we build a balanced set of 800 sentences split evenly among the four sentence types we filtered for, clausetaking verbs, counterfactuals, requests, and questions. For each sentence type, we select the sentence round-robin between the four datasets to balance out the genres. Some types of sentences appear more that 200 times in this sampling because some sentences pass multiple filters. For example, “Could you open the door?” passes b"
W19-3306,W12-0803,1,0.771922,"d uninvert-sent! which transforms an subject-auxiliary inverted sentence, e.g. a question, to the uninverted form. We indicate transformation rules by ending the name with an exclamantion mark, !. Here are a couple of examples of negate-vp! transformations for clarity. The inference rules that we define are tree transductions that respect the EL type system in both the antecedent and consequent clauses, ensuring semantic coherence in the concluded formulas. By using high-level predicates and transformations over ULF expressions, these are simple and interpretable at the top level. We use TTT (Purtee and Schubert, 2012) to define our tree-transductions rules as it provides a powerful and flexible way to declare tree transductions and supports custom predicate and mapping functions. 3.1 Named ULF Expression Predicates (2) left the house → did not leave the house The foundation of the interpretable predicates correspond to the ULF semantic types with syntactic features, e.g. lex-pronoun? which is true for any atom with a .pro suffix—a ULF pronoun. In line with TTT notation, we indicate predicates by ending the name with a question mark, ?. These are defined over the possible compositions of ULF expressions whi"
W19-3306,W17-2315,0,0.0164612,"n both be mapped to FOL and run on FOL theorem provers (Kamp and Reyle, 1993; Copestake et al., 2005). MRS has been successfully used for the task of recognizing textual entailment (RTE) (Lien and Kouylekov, 2015). Similarly, EL has been shown successful in generating FOL inferences (Morbini and Schubert, 2009) and self-aware metareasoning (Morbini and Schubert, 2011). Abstract Meaning Representation (Banarescu et al., 2013) focuses on event structure, resolution of anaphora, and word senses rather than logical inference and has been demonstrated to support event extraction and summarization (Rao et al., 2017; Wang et al., 2017; Dohare et al., 2017). TRIPS LF (Allen, 1994; Manshadi et al., 2008) is an unscoped modal logic directly integrated with a lexical ontology and has been used for dialogue and biomedical event extraction (Perera et al., 2018; Allen et al., 2015). Distributional representations have been shown to be very effective for RTE, such as in the SNLI and MultiNLI datasets (Bowman et al., 2015; Williams et al., 2018). These datasets are much larger than previous RTE datasets and both provide classification tasks supporting the use of an implicit distributional representation in a neur"
W19-3306,W19-0402,1,0.834007,"pport, but give no description of how to achieve this, nor a demonstration of it in practice. ULF, being a pre-canonicalized semantic form, makes available many possible structures for similar semantic meanings, which leads to a challenge 56 Proceedings of the First International Workshop on Designing Meaning Representations, pages 56–65 c Florence, Italy, August 1st, 2019 2019 Association for Computational Linguistics 2.1 with some refinement to the inference rules and the ULF-to-English generation system. 2 Expected Inferences from ULF Here we briefly describe the classes of inferences that Kim and Schubert (2019) propose could be generated with ULF. 1 Unscoped Episodic Logical Form Inferences based on clause-taking verbs – For example, “She managed to quit smoking"" entails that “She quit smoking"" and “John suspects that I am lying” entails “John believes that I am probably lying”. Stratos et al. (2011) have demonstrated such inferences using fully resolved EL formulas. ULF is an underspecified variant of EL which captures the predicate-argument structure within the EL type-system while leaving operator scope, anaphora, and word sense unresolved (Kim and Schubert, 2019). All atoms in ULF, with the exce"
W19-3306,C02-1150,0,0.140632,"IUC Question Classification tree transduction rule. These rules can be formulated as EL meta-axioms (Morbini and Schubert, 2008) generalized with the named ULF expression predications and transformations to interface with the looser syntax of ULF and its representational idiosyncrasies inherited from English. Since the inferential categories we are exploring are a mixture of entailments, presuppositions, and implicatures their use in a general inference framework warrants additional management of projecting presuppositions and defusing implicatures. 4 The UIUC Question Classification dataset (Li and Roth, 2002) consists of questions from the TREC question answering competition. It covers a wide range of question structures on a wide variety of topics, but focuses on factoid questions. This dataset consists of 15,452 questions. 4.2 As the phenomena that we want to focus on are relatively infrequent, we wrote filtering patterns to reduce the number of human annotations needed to get a sufficient dataset for evaluation. Requests, for example, occur once in roughly every 100 to 1000 sentences, depending on the genre. The filtering is performed by first sentence-delimiting and tokenizing the source texts"
W19-3306,W15-2205,0,0.0194221,"ores tend to correlate with correct inferences in practice, using it as a metric of evaluation is fraught with danger. Small changes that dramatically alter the meaning of a sentence (e.g., negation) are not reflected in the BLEU scores, and for structurally oriented inferences, incorrect inferences are likely to have misleadingly high scores. Representation Structures and Minimal Recursion Semantics (MRS) can both be mapped to FOL and run on FOL theorem provers (Kamp and Reyle, 1993; Copestake et al., 2005). MRS has been successfully used for the task of recognizing textual entailment (RTE) (Lien and Kouylekov, 2015). Similarly, EL has been shown successful in generating FOL inferences (Morbini and Schubert, 2009) and self-aware metareasoning (Morbini and Schubert, 2011). Abstract Meaning Representation (Banarescu et al., 2013) focuses on event structure, resolution of anaphora, and word senses rather than logical inference and has been demonstrated to support event extraction and summarization (Rao et al., 2017; Wang et al., 2017; Dohare et al., 2017). TRIPS LF (Allen, 1994; Manshadi et al., 2008) is an unscoped modal logic directly integrated with a lexical ontology and has been used for dialogue and bi"
W19-3306,N18-1101,0,0.0363488,"2013) focuses on event structure, resolution of anaphora, and word senses rather than logical inference and has been demonstrated to support event extraction and summarization (Rao et al., 2017; Wang et al., 2017; Dohare et al., 2017). TRIPS LF (Allen, 1994; Manshadi et al., 2008) is an unscoped modal logic directly integrated with a lexical ontology and has been used for dialogue and biomedical event extraction (Perera et al., 2018; Allen et al., 2015). Distributional representations have been shown to be very effective for RTE, such as in the SNLI and MultiNLI datasets (Bowman et al., 2015; Williams et al., 2018). These datasets are much larger than previous RTE datasets and both provide classification tasks supporting the use of an implicit distributional representation in a neural network system. The discourse inferences we demonstrated with ULFs, which require access to some syntactic information, as well our evaluations based on reliable English generation, are a challenge to all of the semantic representations discussed, because of their relative remoteness from syntax. 8 Conclusions We presented the first known method of generating inferences from ULF and an evaluation of inferences, focusing on"
W99-0622,W98-1114,0,0.0419509,"Missing"
W99-0622,P96-1025,0,0.0868214,"Missing"
W99-0622,A97-1053,0,0.0227929,"Missing"
W99-0622,J93-2004,0,\N,Missing
W99-0622,P96-1006,0,\N,Missing
W99-0622,P92-1023,0,\N,Missing
