W18-1406,"Points, Paths, and Playscapes: Large-scale Spatial Language Understanding Tasks Set in the Real World",2018,0,2,6,0,1071,jason baldridge,Proceedings of the First International Workshop on Spatial Language Understanding,0,"Spatial language understanding is important for practical applications and as a building block for better abstract language understanding. Much progress has been made through work on understanding spatial relations and values in images and texts as well as on giving and following navigation instructions in restricted domains. We argue that the next big advances in spatial language understanding can be best supported by creating large-scale datasets that focus on points and paths based in the real world, and then extending these to create online, persistent playscapes that mix human and bot players, where the bot players must learn, evolve, and survive according to their depth of understanding of scenes, navigation, and interactions."
P16-1059,Collective Entity Resolution with Multi-Focal Attention,2016,34,42,6,0,10988,amir globerson,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,None
Q15-1036,{P}lato: A Selective Context Model for Entity Resolution,2015,40,24,4,0,34464,nevena lazic,Transactions of the Association for Computational Linguistics,0,"We present Plato, a probabilistic model for entity resolution that includes a novel approach for handling noisy or uninformative features, and supplements labeled training data derived from Wikipedia with a very large unlabeled text corpus. Training and inference in the proposed model can easily be distributed across many servers, allowing it to scale to over 107 entities. We evaluate Plato on three standard datasets for entity resolution. Our approach achieves the best results to-date on TAC KBP 2011 and is highly competitive on both the CoNLL 2003 and TAC KBP 2012 datasets."
D12-1093,Reading The Web with Learned Syntactic-Semantic Inference Rules,2012,17,64,3,0,32594,ni lao,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"We study how to extend a large knowledge base (Freebase) by reading relational information from a large Web text corpus. Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base. We describe a distributed, Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the parsed text and the knowledge base. Experiments show significant accuracy improvements in binary relation prediction over methods that consider only text, or only the existing knowledge base."
P11-1080,Large-Scale Cross-Document Coreference Using Distributed Inference and Hierarchical Models,2011,35,108,3,0,3252,sameer singh,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"Cross-document coreference, the task of grouping all the mentions of each entity in a document collection, arises in information extraction and automated knowledge base construction. For large collections, it is clearly impractical to consider all possible groupings of mentions into distinct entities. To solve the problem we propose two ideas: (a) a distributed inference technique that uses parallelism to enable large scale processing, and (b) a hierarchical model of coreference that represents uncertainty over multiple granularities of entities to facilitate more effective approximate inference. To evaluate these ideas, we constructed a labeled corpus of 1.5 million disambiguated mentions in Web pages by selecting link anchors referring to Wikipedia entities. We show that the combination of the hierarchical model with distributed inference quickly obtains high accuracy (with error reduction of 38%) on this large dataset, demonstrating the scalability of our approach."
P10-2036,Sparsity in Dependency Grammar Induction,2010,27,39,4,0,40853,jennifer gillenwater,Proceedings of the {ACL} 2010 Conference Short Papers,0,"A strong inductive bias is essential in unsupervised grammar induction. We explore a particular sparsity bias in dependency grammars that encourages a small number of unique dependency types. Specifically, we investigate sparsity-inducing penalties on the posterior distributions of parent-child POS tag pairs in the posterior regularization (PR) framework of Graca et al. (2007). In experiments with 12 languages, we achieve substantial gains over the standard expectation maximization (EM) baseline, with average improvement in attachment accuracy of 6.3%. Further, our method outperforms models based on a standard Bayesian sparsity-inducing prior by an average of 4.9%. On English in particular, we show that our approach improves on several other state-of-the-art techniques."
P10-1149,Experiments in Graph-Based Semi-Supervised Learning Methods for Class-Instance Acquisition,2010,20,85,2,1,8056,partha talukdar,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Graph-based semi-supervised learning (SSL) algorithms have been successfully used to extract class-instance pairs from large unstructured and structured text collections. However, a careful comparison of different graph-based SSL algorithms on that task has been lacking. We compare three graph-based SSL algorithms for class-instance acquisition on a variety of graphs constructed from different domains. We find that the recently proposed MAD algorithm is the most effective. We also show that class-instance extraction can be significantly improved by adding semantic information in the form of instance-attribute edges derived from an independently developed knowledge base. All of our code and data will be made publicly available to encourage reproducible research in this area."
D10-1017,Efficient Graph-Based Semi-Supervised Learning of Structured Tagging Models,2010,30,102,3,1,34465,amarnag subramanya,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We describe a new scalable algorithm for semi-supervised training of conditional random fields (CRF) and its application to part-of-speech (POS) tagging. The algorithm uses a similarity graph to encourage similar n-grams to have similar POS tags. We demonstrate the efficacy of our approach on a domain adaptation task, where we assume that we have access to large amounts of unlabeled data from the target domain, but no additional labeled data. The similarity graph is used during training to smooth the state posteriors on the target domain. Standard inference can be used at test time. Our approach is able to scale to very large problems and yields significantly improved target domain accuracy."
D08-1061,Weakly-Supervised Acquisition of Labeled Class Instances using Graph Random Walks,2008,17,125,6,1,8056,partha talukdar,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We present a graph-based semi-supervised label propagation algorithm for acquiring open-domain labeled classes and their instances from a combination of unstructured and structured text sources. This acquisition method significantly improves coverage compared to a previous set of labeled classes and instances derived from free text, while achieving comparable precision."
C08-1060,Reading the Markets: Forecasting Public Opinion of Political Candidates by News Analysis,2008,17,35,4,1,47313,kevin lerman,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Media reporting shapes public opinion which can in turn influence events, particularly in political elections, in which candidates both respond to and shape public perception of their campaigns. We use computational linguistics to automatically predict the impact of news on public perception of political candidates. Our system uses daily newspaper articles to predict shifts in public opinion as reflected in prediction markets. We discuss various types of features designed for this problem. The news system improves market prediction over baseline market systems."
W07-1509,Semi-Automated Named Entity Annotation,2007,10,21,2,0.490196,30689,kuzman ganchev,Proceedings of the Linguistic Annotation Workshop,0,"We investigate a way to partially automate corpus annotation for named entity recognition, by requiring only binary decisions from an annotator. Our approach is based on a linear sequence model trained using a k-best MIRA learning algorithm. We ask an annotator to decide whether each mention produced by a high recall tagger is a true mention or a false positive. We conclude that our approach can reduce the effort of extending a seed training corpus by up to 58%."
W07-0206,Transductive Structured Classification through Constrained {M}in-{C}uts,2007,-1,-1,2,0.490196,30689,kuzman ganchev,Proceedings of the Second Workshop on {T}ext{G}raphs: Graph-Based Algorithms for Natural Language Processing,0,None
P07-1056,"Biographies, {B}ollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification",2007,12,1392,3,1,35606,john blitzer,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Automatic sentiment classification has been extensively studied and applied in recent years. However, sentiment is expressed differently in different domains, and annotating corpora for every possible domain of interest is impractical. We investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. First, we extend to sentiment classification the recently-proposed structural correspondence learning (SCL) algorithm, reducing the relative error due to adaptation between domains by an average of 30% over the original SCL algorithm and 46% over a supervised baseline. Second, we identify a measure of domain similarity that correlates well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains."
D07-1112,Frustratingly Hard Domain Adaptation for Dependency Parsing,2007,10,61,6,0.52381,245,mark dredze,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,We describe some challenges of adaptation in the 2007 CoNLL Shared Task on Domain Adaptation. Our error analysis for this task suggests that a primary source of error is differences in annotation guidelines between treebanks. Our suspicions are supported by the observation that no team was able to improve target domain performance substantially over a state of the art baseline.
W06-2919,A Context Pattern Induction Method for Named Entity Extraction,2006,14,70,4,1,8056,partha talukdar,Proceedings of the Tenth Conference on Computational Natural Language Learning ({C}o{NLL}-X),0,"We present a novel context pattern induction method for information extraction, specifically named entity extraction. Using this method, we extended several classes of seed entity lists into much larger high-precision lists. Using token membership in these extended lists as additional features, we improved the accuracy of a conditional random field-based named entity tagger. In contrast, features derived from the seed lists decreased extractor accuracy."
W06-2932,Multilingual Dependency Analysis with a Two-Stage Discriminative Parser,2006,20,202,3,1,10634,ryan mcdonald,Proceedings of the Tenth Conference on Computational Natural Language Learning ({C}o{NLL}-X),0,"We present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages. The first stage is based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages. The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis."
W06-1615,Domain Adaptation with Structural Correspondence Learning,2006,25,1026,3,1,35606,john blitzer,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent. In such cases, we seek to adapt existing models from a resource-rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger."
E06-1011,Online Learning of Approximate Dependency Parsing Algorithms,2006,19,418,2,1,10634,ryan mcdonald,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper we extend the maximum spanning tree (MST) dependency parsing framework of McDonald et al. (2005c) to incorporate higher-order feature representations and allow dependency structures with multiple parents per word. We show that those extensions can make the MST framework computationally intractable, but that the intractability can be circumvented with new approximate parsing algorithms. We conclude with experiments showing that discriminative online learning using those approximate algorithms achieves the best reported parsing accuracy for Czech and Danish."
P05-1012,Online Large-Margin Training of Dependency Parsers,2005,29,698,3,1,10634,ryan mcdonald,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"We present an effective training algorithm for linearly-scored dependency parsers that implements online large-margin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements."
P05-1061,Simple Algorithms for Complex Relation Extraction with Applications to Biomedical {IE},2005,16,78,2,1,10634,ryan mcdonald,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"A complex relation is any n-ary relation in which some of the arguments may be be unspecified. We present here a simple two-stage method for extracting complex relations between named entities in text. The first stage creates a graph from pairs of entities that are likely to be related, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text."
H05-1066,Non-Projective Dependency Parsing using Spanning Tree Algorithms,2005,21,705,2,1,10634,ryan mcdonald,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n3) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies."
H05-1124,Flexible Text Segmentation with Structured Multilabel Classification,2005,19,75,3,1,10634,ryan mcdonald,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Many language processing tasks can be reduced to breaking the text into segments with prescribed properties. Such tasks include sentence splitting, tokenization, named-entity extraction, and chunking. We present a new model of text segmentation based on ideas from multilabel classification. Using this model, we can naturally represent segmentation problems involving overlapping and non-contiguous segments. We evaluate the model on entity extraction and noun-phrase chunking and show that it is more accurate for overlapping and non-contiguous segments, but it still performs well on simpler data sets for which sequential tagging has been the best method."
N03-1028,Shallow Parsing with Conditional Random Fields,2003,33,1137,2,0,4445,fei sha,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position. Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods. We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model. Improved training methods based on modern optimization algorithms were critical in achieving these results. We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models."
P99-1070,Relating Probabilistic Grammars and Automata,1999,10,74,3,0,14766,steven abney,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"Both probabilistic context-free grammars (PCFGs) and shift-reduce probabilistic pushdown automata (PPDAs) have been used for language modeling and maximum likelihood parsing. We investigate the precise relationship between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases."
P98-2147,Dynamic Compilation of Weighted Context-Free Grammars,1998,17,32,2,0,39166,mehryar mohri,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"Weighted context-free grammars are a convenient formalism for representing grammatical constructions and their likelihoods in a variety of language-processing applications. In particular, speech understanding applications require appropriate grammars both to constrain speech recognition and to help extract the meaning of utterances. In many of those applications, the actual languages described are regular, but context-free representations are much more concise and easier to create. We describe an efficient algorithm for compiling into weighted finite automata an interesting class of weighted context-free grammars that represent regular languages. The resulting automata can then be combined with other speech recognition components. Our method allows the recognizer to dynamically activate or deactivate grammar rules and substitute a new regular language for some terminal symbols, depending on previously recognized inputs, all without recompilation. We also report experimental results showing the practicality of the approach."
C98-2142,Dynamic compilation of weighted context-free grammars,1998,17,32,2,0,39166,mehryar mohri,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"Weighted context-free grammars are a convenient formalism for representing grammatical constructions and their likelihoods in a variety of language-processing applications. In particular, speech understanding applications require appropriate grammars both to constrain speech recognition and to help extract the meaning of utterances. In many of those applications, the actual languages described are regular, but context-free representations are much more concise and easier to create. We describe an efficient algorithm for compiling into weighted finite automata an interesting class of weighted context-free grammars that represent regular languages. The resulting automata can then be combined with other speech recognition components. Our method allows the recognizer to dynamically activate or deactivate grammar rules and substitute a new regular language for some terminal symbols, depending on previously recognized inputs, all without recompilation. We also report experimental results showing the practicality of the approach."
W97-0309,Aggregate and mixed-order {M}arkov models for statistical language processing,1997,12,135,2,0,55599,lawrence saul,Second Conference on Empirical Methods in Natural Language Processing,0,We consider the use of language models whose size and accuracy are intermediate between different order n-gram models. Two types of models are studied in particular. Aggregate Markov models are classbased bigram models in which the mapping from words to classes is probabilistic. Mixed-order Markov models combine bigram models whose predictions are conditioned on different words. Both types of models are trained by ExpectationMaximization (EM) algorithms for maximum likelihood estimation. We examine smoothing procedures in which these models are interposed between different order n-grams. This is found to significantly reduce the perplexity of unseen word combinations.
P97-1008,Similarity-Based Methods for Word Sense Disambiguation,1997,14,116,3,0.543546,955,ido dagan,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency. The similarity-based methods perform up to 40% better on this particular task. We also conclude that events that occur only once in the training set have major impact on similarity-based estimates.
J97-4009,Book Reviews: Type Logical Grammar: Categorial Logic of Signs Glyn {V}. Morrill (Polytechnic {U}niversity of {C}atalonia),1997,0,0,1,1,28562,fernando pereira,Computational Linguistics,0,None
W95-0108,Beyond Word N-Grams,1995,-1,-1,1,1,28562,fernando pereira,Third Workshop on Very Large Corpora,0,None
P94-1038,Similarity-Based Estimation of Word Cooccurrence Probabilities,1994,13,113,2,0.543546,955,ido dagan,32nd Annual Meeting of the Association for Computational Linguistics,1,"In many applications of natural language processing it is necessary to determine the likelihood of a given word combination. For example, a speech recognizer may need to determine which of the two word combinations eat a peach and eat a beach is more likely. Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus. However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus. In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on most similar words.We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model. The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error."
H94-1050,Weighted Rational Transductions and their Application to Human Language Processing,1994,17,74,1,1,28562,fernando pereira,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"We present the concepts of weighted language, transduction and automaton from algebraic automata theory as a general framework for describing and implementing decoding cascades in speech and language processing. This generality allows us to represent uniformly such information sources as pronunciation dictionaries, language models and lattices, and to use uniform algorithms for building decoding stages and for optimizing and combining them. In particular, a single automata join algorithm can be used either to combine information sources such as a pronunciation dictionary and a context-dependency model during the construction of a decoder, or dynamically during the operation of the decoder. Applications to speech recognition and to Chinese text segmentation will be discussed."
P93-1024,Distributional Clustering of {E}nglish Words,1993,11,967,1,1,28562,fernando pereira,31st Annual Meeting of the Association for Computational Linguistics,1,"We describe and evaluate experimentally a method for clustering words according to their distribution in particular syntactic contexts. Words are represented by the relative frequency distributions of contexts in which they appear, and relative entropy between those distributions is used as the similarity measure for clustering. Clusters are represented by average context distributions derived from the given words according to their probabilities of cluster membership. In many cases, the clusters can be thought of as encoding coarse sense distinctions. Deterministic annealing is used to find lowest distortion sets of clusters: as the annealing parameter increases, existing clusters become unstable and subdivide, yielding a hierarchical soft clustering of the data. Clusters are used as the basis for class models of word coocurrence, and the models evaluated with respect to held-out test data."
J93-3006,Book Reviews: The Logic of Typed Feature Structures,1993,-1,-1,1,1,28562,fernando pereira,Computational Linguistics,0,None
P92-1017,Inside-Outside Reestimation From Partially Bracketed Corpora,1992,11,67,1,1,28562,fernando pereira,30th Annual Meeting of the Association for Computational Linguistics,1,"The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences."
H92-1024,Inside-Outside Reestimation From Partially Bracketed Corpora,1992,11,67,1,1,28562,fernando pereira,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences."
P91-1032,Finite-State Approximation of Phrase Structure Grammars,1991,18,76,1,1,28562,fernando pereira,29th Annual Meeting of the Association for Computational Linguistics,1,"Phrase-structure grammars are an effective representation for important syntactic and semantic aspects of natural languages, but are computationally too demanding for use as language models in real-time speech recognition. An algorithm is described that computes finite-state approximations for context-free grammars and equivalent augmented phrase-structure grammar formalisms. The approximation is exact for certain context-free grammars generating regular languages, including all left-linear and right-linear context-free grammars. The algorithm has been used to construct finite-state language models for limited-domain speech recognition tasks."
J90-1001,Categorial Semantics and Scoping,1990,33,63,1,1,28562,fernando pereira,Computational Linguistics,0,"Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings."
J90-1004,Semantic-Head-Driven Generation,1990,0,0,3,0.376654,12905,stuart shieber,Computational Linguistics,0,We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applic...
H90-1005,Finite-State Approximations of Grammars,1990,8,12,1,1,28562,fernando pereira,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"Grammars for spoken language systems are subject to the conflicting requirements of language modeling for recognition and of language analysis for sentence interpretation. Current recognition algorithms can most directly use finite-state acceptor (FSA) language models. However, these models are inadequate for language interpretation, since they cannot express the relevant syntactic and semantic regularities. Augmented phrase structure grammar (APSG) formalisms, such as unification grammars, can express many of those regularities, but they are computationally less suitable for language modeling, because of the inherent cost of computing state transitions in APSG parsers."
P89-1002,A Semantic-Head-Driven Generation Algorithm for Unification-Based Formalisms,1989,16,78,4,0.376654,12905,stuart shieber,27th Annual Meeting of the Association for Computational Linguistics,1,"We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable. In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion."
P89-1019,A Calculus for Semantic Composition and Scoping,1989,26,8,1,1,28562,fernando pereira,27th Annual Meeting of the Association for Computational Linguistics,1,"Certain restrictions on possible scopings of quantified noun phrases in natural language are usually expressed in terms of formal constraints on binding at a level of logical form. Such reliance on the form rather than the content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings."
H89-1043,Integrating Speech and Natural-Language Processing,1989,1,26,2,0,19878,robert moore,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"SRI has developed a new architecture for integrating speech and natural-language processing that applies linguistic constraints during recognition by incrementally expanding the state-transition network embodied in a unification grammar. We compare this dynamic-grammar-network (DGN) approach to its principal alternative, word-lattice parsing, presenting preliminary experimental results that suggest the DGN approach requires much less computation time than word-lattice parsing, while maintaining a very tractable recognition search space."
P88-1010,An Integrated Framework for Semantic and Pragmatic Interpretation,1988,25,25,2,0,56971,martha pollack,26th Annual Meeting of the Association for Computational Linguistics,1,"We report on a mechanism for semantic and pragmatic interpretation that has been designed to take advantage of the generally compositional nature of semantic analysis, without unduly constraining the order in which pragmatic decisions are made. To achieve this goal, we introduce the idea of a conditional interpretation: one that depends upon a set of assumptions about subsequent pragmatic processing. Conditional interpretations are constructed compositionally according to a set of declaratively specified interpretation rules. The mechanism can handle a wide range of pragmatic phenomena and their interactions."
T87-1008,"Information, Unification and Locality",1987,0,1,1,1,28562,fernando pereira,Theoretical Issues in Natural Language Processing 3,0,"In a unification framework we deal with a domain of descriptions P tha t are used to classify objects from some class under analysis 0 , u t terances and their f ragments in the case tha t concerns us here. Classification is given by a description relation ~ between objects in 0 and elements of P. If d is a [partial] description of e, we write e ~ d, e satisfies (or is described by) d. The set of all objects tha t satisfy a description d is wri t ten [dl. Descriptions are in general partial, that is, a description d may in general be extended to a more specific (more informative) description d I. With suitable technical assumptions, this gives a partial order d ~ d' on descriptions. In terms of the description relation ~ , d E d' iff for every object e, e ~ d whenever e ~ d ~. Two descriptions d and d' are compatible if there is a description d such tha t d _C d and d ~ _C d, tha t is if d and d' can both be extended to a single description more informative than both. If two descriptions d and d ~ are compatible, it is reasonable to assume tha t there is a least specific description d U d' more specific tha t both d and d'. In other words, d U d' contains all the information in d and d', bu t no more. For historical reasons, d t.J d' is called the unification of d and d'. In more s tandard mathemat ica l terminology, d u d' is the join of d and d'. In terms of the description relation, if e ~ d it d', e ~ d and e ~ d'. Fur thermore , we want unification to behave like logical conjunction: if e ~ d and e ~ d', e ~ d U d'. Thus lid U d'~ = [[d] n [d'~ holds for any compatible descriptions d and d'. The domains of objects, descriptions and the description relation are usually infinite, even though there may be some way of finitely characterizing the description relation. Such a characterization is a grammar . To write grammars, we need to be able to constrain entities to satisfy arbitrari ly complex descriptions without giving the descriptions in full. Our main ins t ruments for this are parameterized descriptions and rules. A parameter ized description d(pl,. . . ,pk) is not a description itself, but ra ther an encoding of a function from k-tuples of descriptions to descriptions. An object e satisfies such a parameter ized description iff there are descriptions f l , . . . , fk such tha t e satisfies d( f l , . . . , f~). Given a family of parameter ized descriptions (di)iel with parameters (Pi)i~s and a set C of constraints involving the parameters , a family of objects (ei)ie I satisfies the parameter ized descriptions relative to the constraints iff there are descriptions (fi)ieJ tha t can be uniformly replaced for the parameters in such a"
P85-1017,A Structure-Sharing Representation for Unification-Based Grammar Formalisms,1985,6,51,1,1,28562,fernando pereira,23rd Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a structure-sharing method for the representation of complex phrase types in a parser for PATR-II, a unification-based grammar formalism.In parsers for unification-based grammar formalisms, complex phrase types are derived by incremental refinement of the phrase types defined in grammar rules and lexical entries. In a naive implementation, a new phrase type is built by copying older ones and then combining the copies according to the constraints stated in a grammar rule. The structure-sharing method was designed to eliminate most such copying; indeed, practical tests suggest that the use of this technique reduces parsing time by as much as 60%.The present work is inspired by the structure-sharing method for theorem proving introduced by Boyer and Moore and on the variant of it that is used in some Prolog implementations."
P84-1027,The Semantics of Grammar Formalisms Seen as Computer Languages,1984,15,54,1,1,28562,fernando pereira,10th International Conference on Computational Linguistics and 22nd Annual Meeting of the Association for Computational Linguistics,1,"The design, implementation, and use of grammar formalisms for natural language have constituted a major branch of computational linguistics throughout its development. By viewing grammar formalisms as just a special case of computer languages, we can take advantage of the machinery of denotational semantics to provide a precise specification of their meaning. Using Dana Scott's domain theory, we elucidate the nature of the feature systems used in augmented phrase-structure grammar formalisms, in particular those of recent versions of generalized phrase structure grammar, lexical functional grammar and PATR-II, and provide a denotational semantics for a simple grammar formalism. We find that the mathematical structures developed for this purpose contain an operation of feature generalization, not available in those grammar formalisms, that can be used to give a partial account of the effect of coordination on syntactic features."
P83-1021,Parsing as Deduction,1983,13,246,1,1,28562,fernando pereira,21st Annual Meeting of the Association for Computational Linguistics,1,"By exploring the relationship between parsing and deduction, a new and more general view of chart parsing is obtained, which encompasses parsing for grammar formalisms based on unification, and is the basis of the Earley Deduction proof procedure for definite clauses. The efficiency of this approach for an interesting class of grammars is discussed."
J82-3002,An Efficient Easily Adaptable System for Interpreting Natural Language Queries,1982,15,276,2,0,34521,david warren,American Journal of Computational Linguistics,0,"This paper gives an overall account of a prototype natural language question answering system, called Chat-80. Chat-80 has been designed to be both efficient and easily adaptable to a variety of applications. The system is implemented entirely in Prolog, a programming language based on logic. With the aid of a logic-based grammar formalism called extraposition grammars, Chat-80 translates English questions into the Prolog subset of logic. The resulting logical expression is then transformed by a planning algorithm into efficient Prolog, cf. query optimisation in a relational database. Finally, the Prolog form is executed to yield the answer. On a domain of world geography, most questions within the English subset are answered in well under one second, including relatively complex queries."
J81-4003,Extraposition Grammars,1981,3,102,1,1,28562,fernando pereira,American Journal of Computational Linguistics,0,"Extraposition grammars are an extension of definite clause grammars, and are similarly defined in terms of logic clauses. The extended formalism makes it easy to describe left extraposition of constituents, an important feature of natural language syntax."
