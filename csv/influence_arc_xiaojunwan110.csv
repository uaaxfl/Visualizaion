2020.acl-main.21,D18-1241,0,0.0499939,"Missing"
2020.acl-main.21,D16-1245,0,0.0290891,"a [˜ tanh(W ai ; yi vi ]) Decoder (7) 230 Coarse-To-Fine Generation Since the semi-autoregressive generation scenario makes it more challenging to deal with coreferences between questions (especially questions in different groups), we perform question generation in a coarse-to-fine manner. The decoder only needs to generate “coarse questions” where all pronouns are replaced by a placeholder “[p]”. To get final results, we use an additional pre-trained coreference resolution model to fill pronouns into different placeholders. To make a fair comparison, we use the coreference resolution model (Clark and Manning, 2016) adopted by prior works CoreNQG (Du and Cardie, 2018) and CorefNet (Gao et al., 2019). Model Seq2seq (Du et al., 2017) CopyNet (See et al., 2017) CoreNQG (Du and Cardie, 2018) VHRED (Serban et al., 2017) HRAN (Xing et al., 2018) ReDR (Pan et al., 2019) CorefNet (Gao et al., 2019) Ours BLEU1 28.72 29.40 33.84 30.51 30.18 30.84 32.72 35.70 BLEU2 10.16 12.14 14.69 11.95 12.53 15.17 16.01 19.64 BLEU3 6.30 6.53 8.72 6.94 7.65 9.81 10.97 12.06 ROUGE 31.75 33.71 34.38 31.93 35.06 35.58 37.48 38.15 METEOR 13.10 14.20 14.05 12.42 12.95 15.41 16.09 17.26 Length 5.78 5.77 6.08 4.83 5.02 5.58 5.96 6.03 Ta"
2020.acl-main.21,P15-1086,0,0.0177943,"that SQG is not regarded as a dialog generation task. We also propose an answer-aware attention mechanism and a coarse-to-fine generation scenario for better performance. • We use extensive experiments to show that our model outperforms previous work by a substantial margin. Further analysis illustrated the impact of different components. Dataset for this paper is available at https:// github.com/ChaiZ-pku/Sequential-QG. 2 2.1 Related Work Traditional Question Generation TQG was traditionally tackled by rule-based methods (Lindberg et al., 2013; Mazidi and Nielsen, 2014; Hussein et al., 2014; Labutov et al., 2015), e.g., filling handcrafted templates under certain transformation rules. With the rise of data-driven learning approaches, neural networks (NN) have gradually taken the mainstream. Du et al. (2017) pioneered NN-based QG by adopting the Seq2seq architecture (Sutskever et al., 2014). Many ideas were proposed since then to make it more powerful, including answer position features (Zhou et al., 2017), specialized pointer mechanism (Zhao et al., 2018), self-attention (Scialom et al., 2019), answer separation (Kim et al., 2019), etc. In addition, enhancing the Seq2seq model into more complicated st"
2020.acl-main.21,P18-1177,0,0.036122,"Missing"
2020.acl-main.21,W07-0734,0,0.00938679,"man annotations denoting the relationship between input sentences and target questions. So it is unfair to compare CFNet with other methods. It is worth mentioning that when generating questions using the second and third groups of baselines, only previously generated outputs were used as dialog history, i.e., the gold standard questions are remain unknown (in some prior works, they were directly used as dialog history, which we think is inappropriate in practice). 5.2 Automatic Evaluation Metrics Following the conventions, we used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) as automatic evaluation metrics. We also computed the average word-number of generated questions. As shown in Table 2, our semi-autoregressive model outperformed other methods substantially. When we focus on the second and third groups of baselines regarding SQG as multi-turn dialog generation tasks, we can find that models from the third group are more powerful since they make better use of information from input passages. Besides, models from the second group tend to generate shortest questions. Finally, similar to the problem that dialog systems often generate dull and responses, these mod"
2020.acl-main.21,P17-1123,0,0.233768,"o different groups and generates each group of them in parallel. During this process, it builds two graphs focusing on information from passages, answers respectively and performs dual-graph interaction to get information for generation. Besides, we design an answer-aware attention mechanism and the coarse-to-fine generation scenario. Experiments on our new dataset containing 81.9K questions show that our model substantially outperforms prior works. 1 Introduction Question Generation (QG) aims to teach machines to ask human-like questions from a range of inputs such as natural language texts (Du et al., 2017), images (Mostafazadeh et al., 2016) and knowledge bases (Serban et al., 2016). In recent years, QG has received increasing attention due to its wide applications. Asking questions in dialog systems can enhance the interactiveness and persistence of humanmachine interactions (Wang et al., 2018). QG benefits Question Answering (QA) models through data augmentation (Duan et al., 2017) and joint learning (Sun et al., 2019). It also plays an important role in education (Heilman and Smith, 2010) and clinical (Weizenbaum et al., 1966) systems. Traditional Question Generation (TQG) is defined as the"
2020.acl-main.21,D17-1090,0,0.161009,"K questions show that our model substantially outperforms prior works. 1 Introduction Question Generation (QG) aims to teach machines to ask human-like questions from a range of inputs such as natural language texts (Du et al., 2017), images (Mostafazadeh et al., 2016) and knowledge bases (Serban et al., 2016). In recent years, QG has received increasing attention due to its wide applications. Asking questions in dialog systems can enhance the interactiveness and persistence of humanmachine interactions (Wang et al., 2018). QG benefits Question Answering (QA) models through data augmentation (Duan et al., 2017) and joint learning (Sun et al., 2019). It also plays an important role in education (Heilman and Smith, 2010) and clinical (Weizenbaum et al., 1966) systems. Traditional Question Generation (TQG) is defined as the reverse task of QA, i.e., a passage and an answer (often a certain span from the passage) are provided as inputs, and the output is a question grounded in the input passage targeting on the given answer. When there is a sequence of answers, we can perform Sequential Question Generation (SQG) to produce a series of interconnected questions. Table 1 shows an example comparing the two"
2020.acl-main.21,P19-1480,0,0.629327,"graph are constructed and iteratively updated with each other. The passage-info graph is used for better capturing context dependencies, and the answer-info graph is used to make generated questions more relevant to given answers with the help of our answer-aware attention mechanism. Besides, a coarse-to-fine text generation scenario is adopted for the coreference resolution between questions. Prior works performed SQG on CoQA (Reddy et al., 2019), a high-quality dataset for conversational QA. As will be further illustrated, a number of data in CoQA are not suitable for SQG. Some researchers (Gao et al., 2019) directly discarded these data, but the remaining questions may become incoherent, e.g., the antecedent words for many pronouns are unclear. To this end, we build a new dataset from CoQA containing 81.9K relabeled questions. Above all, the main contributions of our work are: • We build a new dataset containing 7.2K passages and 81.9K questions from CoQA. It is the first dataset specially built for SQG as far as we know. • We perform semi-autoregressive SQG under dual-graph interaction. This is the first time that SQG is not regarded as a dialog generation task. We also propose an answer-aware"
2020.acl-main.21,N10-1086,0,0.0343953,"n (QG) aims to teach machines to ask human-like questions from a range of inputs such as natural language texts (Du et al., 2017), images (Mostafazadeh et al., 2016) and knowledge bases (Serban et al., 2016). In recent years, QG has received increasing attention due to its wide applications. Asking questions in dialog systems can enhance the interactiveness and persistence of humanmachine interactions (Wang et al., 2018). QG benefits Question Answering (QA) models through data augmentation (Duan et al., 2017) and joint learning (Sun et al., 2019). It also plays an important role in education (Heilman and Smith, 2010) and clinical (Weizenbaum et al., 1966) systems. Traditional Question Generation (TQG) is defined as the reverse task of QA, i.e., a passage and an answer (often a certain span from the passage) are provided as inputs, and the output is a question grounded in the input passage targeting on the given answer. When there is a sequence of answers, we can perform Sequential Question Generation (SQG) to produce a series of interconnected questions. Table 1 shows an example comparing the two tasks. Intuitively, questions in SQG are much more concise and we can regard them with given answers as QA-sty"
2020.acl-main.21,W13-2114,0,0.0266891,"utoregressive SQG under dual-graph interaction. This is the first time that SQG is not regarded as a dialog generation task. We also propose an answer-aware attention mechanism and a coarse-to-fine generation scenario for better performance. • We use extensive experiments to show that our model outperforms previous work by a substantial margin. Further analysis illustrated the impact of different components. Dataset for this paper is available at https:// github.com/ChaiZ-pku/Sequential-QG. 2 2.1 Related Work Traditional Question Generation TQG was traditionally tackled by rule-based methods (Lindberg et al., 2013; Mazidi and Nielsen, 2014; Hussein et al., 2014; Labutov et al., 2015), e.g., filling handcrafted templates under certain transformation rules. With the rise of data-driven learning approaches, neural networks (NN) have gradually taken the mainstream. Du et al. (2017) pioneered NN-based QG by adopting the Seq2seq architecture (Sutskever et al., 2014). Many ideas were proposed since then to make it more powerful, including answer position features (Zhou et al., 2017), specialized pointer mechanism (Zhao et al., 2018), self-attention (Scialom et al., 2019), answer separation (Kim et al., 2019),"
2020.acl-main.21,P14-2053,0,0.0214557,"dual-graph interaction. This is the first time that SQG is not regarded as a dialog generation task. We also propose an answer-aware attention mechanism and a coarse-to-fine generation scenario for better performance. • We use extensive experiments to show that our model outperforms previous work by a substantial margin. Further analysis illustrated the impact of different components. Dataset for this paper is available at https:// github.com/ChaiZ-pku/Sequential-QG. 2 2.1 Related Work Traditional Question Generation TQG was traditionally tackled by rule-based methods (Lindberg et al., 2013; Mazidi and Nielsen, 2014; Hussein et al., 2014; Labutov et al., 2015), e.g., filling handcrafted templates under certain transformation rules. With the rise of data-driven learning approaches, neural networks (NN) have gradually taken the mainstream. Du et al. (2017) pioneered NN-based QG by adopting the Seq2seq architecture (Sutskever et al., 2014). Many ideas were proposed since then to make it more powerful, including answer position features (Zhou et al., 2017), specialized pointer mechanism (Zhao et al., 2018), self-attention (Scialom et al., 2019), answer separation (Kim et al., 2019), etc. In addition, enhanci"
2020.acl-main.21,P16-1170,0,0.0170945,"erates each group of them in parallel. During this process, it builds two graphs focusing on information from passages, answers respectively and performs dual-graph interaction to get information for generation. Besides, we design an answer-aware attention mechanism and the coarse-to-fine generation scenario. Experiments on our new dataset containing 81.9K questions show that our model substantially outperforms prior works. 1 Introduction Question Generation (QG) aims to teach machines to ask human-like questions from a range of inputs such as natural language texts (Du et al., 2017), images (Mostafazadeh et al., 2016) and knowledge bases (Serban et al., 2016). In recent years, QG has received increasing attention due to its wide applications. Asking questions in dialog systems can enhance the interactiveness and persistence of humanmachine interactions (Wang et al., 2018). QG benefits Question Answering (QA) models through data augmentation (Duan et al., 2017) and joint learning (Sun et al., 2019). It also plays an important role in education (Heilman and Smith, 2010) and clinical (Weizenbaum et al., 1966) systems. Traditional Question Generation (TQG) is defined as the reverse task of QA, i.e., a passage"
2020.acl-main.21,P19-1203,0,0.0669137,"mar et al., 2019) have also gained much attention. There are also some works performing TQG under certain constraints, e.g., controlling the 226 topic (Hu et al., 2018) and difficulty (Gao et al., 2018) of questions. Besides, combining QG with QA (Wang et al., 2017; Tang et al., 2017; Sun et al., 2019) is also focused by many researchers. 2.2 Sequential Question Generation As human beings tend to use coherent questions for knowledge testing or information seeking, SQG plays an important role in many applications. Prior works regarded SQG as a dialog generation task (namely conversational QA). Pan et al. (2019) pretrained a model performing dialog generation, and then fine-tuned its parameters by reinforcement learning to make generated questions relevant to given answers. Gao et al. (2019) iteratively generated questions from previous outputs and leveraged off-the-shelf coreference resolution models to introduce a coreference loss. Besides, additional human annotations were performed on sentences from input passages for conversation flow modeling. Since SQG is essentially different from dialog generation, we discard its dialog view and propose the first semi-autoregressive SQG model. Compared with"
2020.acl-main.21,P02-1040,0,0.10784,"this paper got better results, it required additional human annotations denoting the relationship between input sentences and target questions. So it is unfair to compare CFNet with other methods. It is worth mentioning that when generating questions using the second and third groups of baselines, only previously generated outputs were used as dialog history, i.e., the gold standard questions are remain unknown (in some prior works, they were directly used as dialog history, which we think is inappropriate in practice). 5.2 Automatic Evaluation Metrics Following the conventions, we used BLEU (Papineni et al., 2002), ROUGE-L (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) as automatic evaluation metrics. We also computed the average word-number of generated questions. As shown in Table 2, our semi-autoregressive model outperformed other methods substantially. When we focus on the second and third groups of baselines regarding SQG as multi-turn dialog generation tasks, we can find that models from the third group are more powerful since they make better use of information from input passages. Besides, models from the second group tend to generate shortest questions. Finally, similar to the problem that d"
2020.acl-main.21,D16-1264,0,0.123073,"Missing"
2020.acl-main.21,Q19-1016,0,0.311801,".e., a passage and an answer (often a certain span from the passage) are provided as inputs, and the output is a question grounded in the input passage targeting on the given answer. When there is a sequence of answers, we can perform Sequential Question Generation (SQG) to produce a series of interconnected questions. Table 1 shows an example comparing the two tasks. Intuitively, questions in SQG are much more concise and we can regard them with given answers as QA-style conversations. Since it is more natural for human beings to test knowledge or seek information through coherent questions (Reddy et al., 2019), SQG has wide applications, e.g., enabling virtual assistants to ask questions based on previous discussions to get better user experiences. SQG is a challenging task in two aspects. First, information omissions between questions lead to complex context dependencies. Second, there are frequently occurred coreference between questions. Prior works regarded SQG as a dialog generation task (namely conversational QG) where questions are generated autoregressively (recurrently), i.e., a new question is produced based on previous outputs. Although many powerful dialog generation models can be adopt"
2020.acl-main.21,P19-1604,0,0.0147915,"ionally tackled by rule-based methods (Lindberg et al., 2013; Mazidi and Nielsen, 2014; Hussein et al., 2014; Labutov et al., 2015), e.g., filling handcrafted templates under certain transformation rules. With the rise of data-driven learning approaches, neural networks (NN) have gradually taken the mainstream. Du et al. (2017) pioneered NN-based QG by adopting the Seq2seq architecture (Sutskever et al., 2014). Many ideas were proposed since then to make it more powerful, including answer position features (Zhou et al., 2017), specialized pointer mechanism (Zhao et al., 2018), self-attention (Scialom et al., 2019), answer separation (Kim et al., 2019), etc. In addition, enhancing the Seq2seq model into more complicated structures using variational inference, adversarial training and reinforcement learning (Yao et al., 2018; Kumar et al., 2019) have also gained much attention. There are also some works performing TQG under certain constraints, e.g., controlling the 226 topic (Hu et al., 2018) and difficulty (Gao et al., 2018) of questions. Besides, combining QG with QA (Wang et al., 2017; Tang et al., 2017; Sun et al., 2019) is also focused by many researchers. 2.2 Sequential Question Generation As huma"
2020.acl-main.21,P17-1099,0,0.228841,"with coreferences between questions (especially questions in different groups), we perform question generation in a coarse-to-fine manner. The decoder only needs to generate “coarse questions” where all pronouns are replaced by a placeholder “[p]”. To get final results, we use an additional pre-trained coreference resolution model to fill pronouns into different placeholders. To make a fair comparison, we use the coreference resolution model (Clark and Manning, 2016) adopted by prior works CoreNQG (Du and Cardie, 2018) and CorefNet (Gao et al., 2019). Model Seq2seq (Du et al., 2017) CopyNet (See et al., 2017) CoreNQG (Du and Cardie, 2018) VHRED (Serban et al., 2017) HRAN (Xing et al., 2018) ReDR (Pan et al., 2019) CorefNet (Gao et al., 2019) Ours BLEU1 28.72 29.40 33.84 30.51 30.18 30.84 32.72 35.70 BLEU2 10.16 12.14 14.69 11.95 12.53 15.17 16.01 19.64 BLEU3 6.30 6.53 8.72 6.94 7.65 9.81 10.97 12.06 ROUGE 31.75 33.71 34.38 31.93 35.06 35.58 37.48 38.15 METEOR 13.10 14.20 14.05 12.42 12.95 15.41 16.09 17.26 Length 5.78 5.77 6.08 4.83 5.02 5.58 5.96 6.03 Table 2: Experimental results. In each column, we bold / underline the best performance over all / baseline methods, respectively. Under the evalua"
2020.acl-main.21,P18-1204,0,0.38892,"and the coarse-to-fine generation scenario. Experiments on our new dataset containing 81.9K questions show that our model substantially outperforms prior works. 1 Introduction Question Generation (QG) aims to teach machines to ask human-like questions from a range of inputs such as natural language texts (Du et al., 2017), images (Mostafazadeh et al., 2016) and knowledge bases (Serban et al., 2016). In recent years, QG has received increasing attention due to its wide applications. Asking questions in dialog systems can enhance the interactiveness and persistence of humanmachine interactions (Wang et al., 2018). QG benefits Question Answering (QA) models through data augmentation (Duan et al., 2017) and joint learning (Sun et al., 2019). It also plays an important role in education (Heilman and Smith, 2010) and clinical (Weizenbaum et al., 1966) systems. Traditional Question Generation (TQG) is defined as the reverse task of QA, i.e., a passage and an answer (often a certain span from the passage) are provided as inputs, and the output is a question grounded in the input passage targeting on the given answer. When there is a sequence of answers, we can perform Sequential Question Generation (SQG) to"
2020.acl-main.21,D18-1424,0,0.0166263,"l Question Generation TQG was traditionally tackled by rule-based methods (Lindberg et al., 2013; Mazidi and Nielsen, 2014; Hussein et al., 2014; Labutov et al., 2015), e.g., filling handcrafted templates under certain transformation rules. With the rise of data-driven learning approaches, neural networks (NN) have gradually taken the mainstream. Du et al. (2017) pioneered NN-based QG by adopting the Seq2seq architecture (Sutskever et al., 2014). Many ideas were proposed since then to make it more powerful, including answer position features (Zhou et al., 2017), specialized pointer mechanism (Zhao et al., 2018), self-attention (Scialom et al., 2019), answer separation (Kim et al., 2019), etc. In addition, enhancing the Seq2seq model into more complicated structures using variational inference, adversarial training and reinforcement learning (Yao et al., 2018; Kumar et al., 2019) have also gained much attention. There are also some works performing TQG under certain constraints, e.g., controlling the 226 topic (Hu et al., 2018) and difficulty (Gao et al., 2018) of questions. Besides, combining QG with QA (Wang et al., 2017; Tang et al., 2017; Sun et al., 2019) is also focused by many researchers. 2.2"
2020.acl-main.400,W18-6402,0,0.0829212,"ts and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics. 1 Figure 1: An Example for Multimodal Machine Translation. Introduction Multimodal machine translation (MMT) is a novel machine translation (MT) task which aims at designing better translation systems using context from an additional modality, usually images (See Figure 1). It initially organized as a shared task within the First Conference on Machine Translation (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Current works focus on the dataset named Multi30k (Elliott et al., 2016), a multilingual extension of Flickr30k dataset with translations of the English image descriptions into different languages. Previous works propose various incorporation methods. Calixto and Liu (2017) utilize global image features to initialize the encoder/decoder hidden states of RNN. Elliott and K´ad´ar (2017) model the source sentence and reconstruct the image representation jointly via multi-task learning. Recently, Ive et al. (2019) propose a translate-and-refine approach using two-stage decoder based on Transform"
2020.acl-main.400,D17-1105,0,0.0738858,"hine translation (MMT) is a novel machine translation (MT) task which aims at designing better translation systems using context from an additional modality, usually images (See Figure 1). It initially organized as a shared task within the First Conference on Machine Translation (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Current works focus on the dataset named Multi30k (Elliott et al., 2016), a multilingual extension of Flickr30k dataset with translations of the English image descriptions into different languages. Previous works propose various incorporation methods. Calixto and Liu (2017) utilize global image features to initialize the encoder/decoder hidden states of RNN. Elliott and K´ad´ar (2017) model the source sentence and reconstruct the image representation jointly via multi-task learning. Recently, Ive et al. (2019) propose a translate-and-refine approach using two-stage decoder based on Transformer (Vaswani et al., 2017). Calixto et al. (2019) put forward a latent variable model to learn the interaction between visual and textual features. However, in multimodal tasks the different modalities usually are not equally important. For example, in MMT the text is obviousl"
2020.acl-main.400,P17-1175,0,0.15371,"Missing"
2020.acl-main.400,I17-1014,0,0.082076,"Missing"
2020.acl-main.400,P19-1653,0,0.172981,"Conference on Machine Translation (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Current works focus on the dataset named Multi30k (Elliott et al., 2016), a multilingual extension of Flickr30k dataset with translations of the English image descriptions into different languages. Previous works propose various incorporation methods. Calixto and Liu (2017) utilize global image features to initialize the encoder/decoder hidden states of RNN. Elliott and K´ad´ar (2017) model the source sentence and reconstruct the image representation jointly via multi-task learning. Recently, Ive et al. (2019) propose a translate-and-refine approach using two-stage decoder based on Transformer (Vaswani et al., 2017). Calixto et al. (2019) put forward a latent variable model to learn the interaction between visual and textual features. However, in multimodal tasks the different modalities usually are not equally important. For example, in MMT the text is obviously more important than images. Although the image carries richer information, it also contains more irrelevant content. If we directly encode the image features, it may introduce a lot of noise. To address the issues above, we propose the mul"
2020.acl-main.400,P02-1040,0,0.106863,"l. Instead, they adjust the attention of each word to compute the hidden representations of the image. In each encoder layer we also employ 4347 residual connections between each layer as well as layer normalization. And the decoder are followed the standard implemention of Transformer. 3 3.1 Experiment Baselines and Metrics We compare the performance of our model with previous kinds of models: (1) sequence-to-sequence model only trained on text data (LSTM, Transformer). (2) Previous works trained on both text and image data. We evaluated the translation quality of our model in terms of BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014), which have been used in most previous works. 3.2 Datasets We build and test our model on the Multi30k dataset (Elliott et al., 2016), which consists of two multilingual expansions of the original Flickr30k dataset referred to as M30kT and M30KC , respectively. Multi30k contains 30k images, and for each of the images, M30kT has one of its English description manually translated into German by a professional translator. M30KC has five English descriptions and five German descriptions, but the German descriptions were crowdsourced independently from their"
2020.acl-main.400,P16-1009,0,0.045624,"five English descriptions and five German descriptions, but the German descriptions were crowdsourced independently from their English versions. The training, validation, test sets of Multi30k contain 29k, 1014 and 1k instances respectively. We use M30kT as the original training data and M30kC for building additional back-translated training data following Calixto et al. (2019). We present our experiment results on English-German (En-De) Test2016. We use LSTM trained on the textual part of the M30KT dataset (De-En, the original 29k sentences) without images to build a back-translation model (Sennrich et al., 2016), and then apply this model to translate 145k monolingual German description in M30kC into English as additional training data. This part of data we refer to as back-translated data. 3.3 Model Settings We preprocess the data by tokenizing and lowercasing. Word embeddings are initialized using pretrained 300-dimensional Glove vectors. we extract spatial image features from the last convolutional layer of ResNet-50. The spatial features are 7 × 7 × 2048-dimensional vectors which are representations of local spatial regions of the image. Our encoder and decoder have both 6 layers with 300-dimensi"
2020.acl-main.400,W16-2346,0,0.292524,"irrelevant information in images. Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics. 1 Figure 1: An Example for Multimodal Machine Translation. Introduction Multimodal machine translation (MMT) is a novel machine translation (MT) task which aims at designing better translation systems using context from an additional modality, usually images (See Figure 1). It initially organized as a shared task within the First Conference on Machine Translation (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Current works focus on the dataset named Multi30k (Elliott et al., 2016), a multilingual extension of Flickr30k dataset with translations of the English image descriptions into different languages. Previous works propose various incorporation methods. Calixto and Liu (2017) utilize global image features to initialize the encoder/decoder hidden states of RNN. Elliott and K´ad´ar (2017) model the source sentence and reconstruct the image representation jointly via multi-task learning. Recently, Ive et al. (2019) propose a translate-and-refine appro"
2020.acl-main.400,2020.acl-main.640,1,0.831637,"n ) of n elements where xi ∈ Rd , and computes a new sequence z = (z1 , ..., zn ) of the same length where z ∈ Rd : zi = n X αij xj W V  (1) j=1 where αij is weight coefficient computed by a softmax function:  T ! xi W Q xj W K √ (2) αij = softmax d W V , W Q , W V ∈ Rd×d are layer-specific trainable parameter matrices. Thus we can see that each word representation is induced from all the other words. If we consider every word to be a node, then Transformer can be regarded as a variant of GNN which treats each sentence as a fully-connected graph with words as nodes (Battaglia et al., 2018; Yao et al., 2020). In traditional MT tasks, the source sentence graph only contains nodes with text information. If we want to incorporate information from other modality, we should add the nodes with other modality information into the source graph. Therefore, as the words are local semantic representations of the sentence, we extract the spatial features which are the semantic representations of local spatial regions of the image. We add the spatial features of the image as pseudo-words in the source sentence and feed it into the multimodal self-attention layer. 2.2 Multimodal Self-attention As stated before"
2020.acl-main.400,P19-1642,0,0.357132,"e dataset named Multi30k (Elliott et al., 2016), a multilingual extension of Flickr30k dataset with translations of the English image descriptions into different languages. Previous works propose various incorporation methods. Calixto and Liu (2017) utilize global image features to initialize the encoder/decoder hidden states of RNN. Elliott and K´ad´ar (2017) model the source sentence and reconstruct the image representation jointly via multi-task learning. Recently, Ive et al. (2019) propose a translate-and-refine approach using two-stage decoder based on Transformer (Vaswani et al., 2017). Calixto et al. (2019) put forward a latent variable model to learn the interaction between visual and textual features. However, in multimodal tasks the different modalities usually are not equally important. For example, in MMT the text is obviously more important than images. Although the image carries richer information, it also contains more irrelevant content. If we directly encode the image features, it may introduce a lot of noise. To address the issues above, we propose the multimodal Transformer. The proposed model does not directly encode image features. Instead, the hidden representations of images are"
2020.acl-main.400,W14-3348,0,0.014593,"on of each word to compute the hidden representations of the image. In each encoder layer we also employ 4347 residual connections between each layer as well as layer normalization. And the decoder are followed the standard implemention of Transformer. 3 3.1 Experiment Baselines and Metrics We compare the performance of our model with previous kinds of models: (1) sequence-to-sequence model only trained on text data (LSTM, Transformer). (2) Previous works trained on both text and image data. We evaluated the translation quality of our model in terms of BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014), which have been used in most previous works. 3.2 Datasets We build and test our model on the Multi30k dataset (Elliott et al., 2016), which consists of two multilingual expansions of the original Flickr30k dataset referred to as M30kT and M30KC , respectively. Multi30k contains 30k images, and for each of the images, M30kT has one of its English description manually translated into German by a professional translator. M30KC has five English descriptions and five German descriptions, but the German descriptions were crowdsourced independently from their English versions. The training, validat"
2020.acl-main.400,D18-1329,0,0.0405931,"ranslation. Full Model - replace with self-attention - replace with blank images - replace with random images To further study the influence of the individual components in our model, we conduct ablation experiments to better understand their relative importance. The results are presented in Table 2. Firstly, we investigate the effect of multimodal self-attention. As shown in the second columns (replace with selfattention) in Table 2. If we simply concatenate the word vectors with the image features and then perform self-attention, we will lose 0.6 BLEU score and 0.4 METEOR score. Inspired by Elliott (2018), we further examine the utility of the image by the adversarial evaluation. When we replace all input images with a blank picture, the performance of the model drops a lot. When we replace all input MEMTEOR 38.7 38.1 37.1 36.7 55.7 55.3 54.8 54.5 Table 2: Influence of different components in our model. 4 Ablation Study BLEU4 Conclusion In this paper, we propose the multimodal selfattention to consider the relative importance between different modalities in the MMT task. The hidden representations of less important modality (image) are induced from the important modality (text) under the guide"
2020.acl-main.400,W17-4718,0,0.178223,"n in images. Experiments and visualization analysis demonstrate that our model benefits from visual information and substantially outperforms previous works and competitive baselines in terms of various metrics. 1 Figure 1: An Example for Multimodal Machine Translation. Introduction Multimodal machine translation (MMT) is a novel machine translation (MT) task which aims at designing better translation systems using context from an additional modality, usually images (See Figure 1). It initially organized as a shared task within the First Conference on Machine Translation (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Current works focus on the dataset named Multi30k (Elliott et al., 2016), a multilingual extension of Flickr30k dataset with translations of the English image descriptions into different languages. Previous works propose various incorporation methods. Calixto and Liu (2017) utilize global image features to initialize the encoder/decoder hidden states of RNN. Elliott and K´ad´ar (2017) model the source sentence and reconstruct the image representation jointly via multi-task learning. Recently, Ive et al. (2019) propose a translate-and-refine approach using two-stage de"
2020.acl-main.400,W16-3210,0,0.225852,"Missing"
2020.acl-main.550,W12-4303,0,0.0191729,"ummary of paper B given the context of paper A. The difficulty lies in that given different A or different contexts of A, the task aims to produce different citation texts for the same B. Most commonly, the citation text is a single sentence, but sometimes it may consist of several sentences (Jebari et al., 2018; Qazvinian and Radev, 2010; Sondhi and Zhai, 2014). Like (Small, 2011), we define citation text as a block of text composed of one or more consecutive sentences surrounding the reference sign. Each citation sentence can be classified as explicit or implicit (Qazvinian and Radev, 2010; Athar and Teufel, 2012; Yasunaga et al., 2019). Explicit citation is a citation sentence that contains explicit reference to the cited paper. An implicit (or non-explicit) citation sentence appears around the explicit citation sentence and it does not attach any explicit reference to the cited paper but supplies additional information about the cited paper. The citation text generation task in this study aims to generate both explicit and implicit 6181 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6181–6190 c July 5 - 10, 2020. 2020 Association for Computational Ling"
2020.acl-main.550,C10-2049,0,0.301878,"vinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian and Radev, 2010; Cohan and Goharian, 2018; Yasunaga et al., 2019). Several benchmark tests have been set up for scientific summarization, including TAC 2014 Biomedical Summarization track and the CL-SciSumm Shared Task (Jaidka et al., 2016). A few other studies have investigated the task of summarizing multiple scholarly papers, i.e., multi-document summarization in the scientific domain (Mohammad et al., 2009; Yeloglu et al., 2011; Chen and Zhuge, 2014). Related work generation is a special case of multi-document scientific summarization (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019). However, the above related work about scholarly paper summarization is different from the task of citation text generation, which aims to generate a usually very short text to describe the cited paper in the given context of the citing paper. 3 Problem and Corpus Formally, given a citing paper A, a cited paper B and the context C in A, the task aims to generate the citation text T to describe B. The context C refers to the sentences surrounding the target citation text in A and it is provided to distinguish different mentions of B in different positio"
2020.acl-main.550,D14-1170,1,0.916554,"8; Mei and Zhai, 2008; Qazvinian and Radev, 2010; Cohan and Goharian, 2018; Yasunaga et al., 2019). Several benchmark tests have been set up for scientific summarization, including TAC 2014 Biomedical Summarization track and the CL-SciSumm Shared Task (Jaidka et al., 2016). A few other studies have investigated the task of summarizing multiple scholarly papers, i.e., multi-document summarization in the scientific domain (Mohammad et al., 2009; Yeloglu et al., 2011; Chen and Zhuge, 2014). Related work generation is a special case of multi-document scientific summarization (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019). However, the above related work about scholarly paper summarization is different from the task of citation text generation, which aims to generate a usually very short text to describe the cited paper in the given context of the citing paper. 3 Problem and Corpus Formally, given a citing paper A, a cited paper B and the context C in A, the task aims to generate the citation text T to describe B. The context C refers to the sentences surrounding the target citation text in A and it is provided to distinguish different mentions of B in different positions of A. The follo"
2020.acl-main.550,W16-1511,0,0.0142011,"fic domain, which is relevant to the citation text generation task. Early works include (Luhn, 1958; Baxendale, 1958; Edmundson, 1969), and they tried to use various features specific to scientific articles for summary extraction. Later on, citation information has shown its usefulness for scientific paper summarization (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian and Radev, 2010; Cohan and Goharian, 2018; Yasunaga et al., 2019). Several benchmark tests have been set up for scientific summarization, including TAC 2014 Biomedical Summarization track and the CL-SciSumm Shared Task (Jaidka et al., 2016). A few other studies have investigated the task of summarizing multiple scholarly papers, i.e., multi-document summarization in the scientific domain (Mohammad et al., 2009; Yeloglu et al., 2011; Chen and Zhuge, 2014). Related work generation is a special case of multi-document scientific summarization (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019). However, the above related work about scholarly paper summarization is different from the task of citation text generation, which aims to generate a usually very short text to describe the cited paper in the given context of the cit"
2020.acl-main.550,W09-3611,0,0.0930147,"Missing"
2020.acl-main.550,D08-1082,0,0.0352188,"he above related work about scholarly paper summarization is different from the task of citation text generation, which aims to generate a usually very short text to describe the cited paper in the given context of the citing paper. 3 Problem and Corpus Formally, given a citing paper A, a cited paper B and the context C in A, the task aims to generate the citation text T to describe B. The context C refers to the sentences surrounding the target citation text in A and it is provided to distinguish different mentions of B in different positions of A. The following example shows a paragraph of (Lu et al., 2008) and this article cites paper (Wong and Mooney, 2006). In this example, A refers to (Lu et al., 2008) and B refers to (Wong and Mooney, 2006). The sentence underlined (i.e., the second sentence) is an explicit citation, and the sentence in italics (i.e., the third sentence) is an implicit citationand both of them compose the citation text. The remaining two 6182 sentences (i.e., the first and last sentences) compose the context C of A. The phrase in bold which indicates the explicit citation to paper B is called reference sign. And the explicit citation text can be defined as the sentence with"
2020.acl-main.550,P08-1093,0,0.282705,"burden of researchers, we propose and try to address the task of automatic citation text generation. Automatic generation of citation texts in scholarly papers is a challenging and meaningful task, however, there are very few studies investigating this problem. Given a cited paper B and the context in a citing paper A (i.e., the sentences before and after a specific position in paper A), the task aims to generate a short text to describe B with respect to the given context in A. The task is like the task of scholarly paper summarization (Luhn, 1958; Edmundson, 1969; Qazvinian and Radev, 2008; Mei and Zhai, 2008). Both of the two tasks aim to produce a text to describe the cited paper B. The major difference between the two tasks is that the citation texts reflect not only the salient content of B, but also the context of A. Different citing papers usually have different descriptions of the same cited paper. Sometimes one paper may cite another paper several times in different positions but give different descriptions because the specific contexts are different. Another difference between the two tasks is the length of the text. A citation text is usually much shorter than a paper summary. Generally,"
2020.acl-main.550,N09-1066,0,0.279357,"es specific to scientific articles for summary extraction. Later on, citation information has shown its usefulness for scientific paper summarization (Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian and Radev, 2010; Cohan and Goharian, 2018; Yasunaga et al., 2019). Several benchmark tests have been set up for scientific summarization, including TAC 2014 Biomedical Summarization track and the CL-SciSumm Shared Task (Jaidka et al., 2016). A few other studies have investigated the task of summarizing multiple scholarly papers, i.e., multi-document summarization in the scientific domain (Mohammad et al., 2009; Yeloglu et al., 2011; Chen and Zhuge, 2014). Related work generation is a special case of multi-document scientific summarization (Hoang and Kan, 2010; Hu and Wan, 2014; Chen and Zhuge, 2019). However, the above related work about scholarly paper summarization is different from the task of citation text generation, which aims to generate a usually very short text to describe the cited paper in the given context of the citing paper. 3 Problem and Corpus Formally, given a citing paper A, a cited paper B and the context C in A, the task aims to generate the citation text T to describe B. The co"
2020.acl-main.550,C08-1087,0,0.305403,"ng. In order to reduce the burden of researchers, we propose and try to address the task of automatic citation text generation. Automatic generation of citation texts in scholarly papers is a challenging and meaningful task, however, there are very few studies investigating this problem. Given a cited paper B and the context in a citing paper A (i.e., the sentences before and after a specific position in paper A), the task aims to generate a short text to describe B with respect to the given context in A. The task is like the task of scholarly paper summarization (Luhn, 1958; Edmundson, 1969; Qazvinian and Radev, 2008; Mei and Zhai, 2008). Both of the two tasks aim to produce a text to describe the cited paper B. The major difference between the two tasks is that the citation texts reflect not only the salient content of B, but also the context of A. Different citing papers usually have different descriptions of the same cited paper. Sometimes one paper may cite another paper several times in different positions but give different descriptions because the specific contexts are different. Another difference between the two tasks is the length of the text. A citation text is usually much shorter than a paper"
2020.acl-main.550,P10-1057,0,0.134888,"rent descriptions because the specific contexts are different. Another difference between the two tasks is the length of the text. A citation text is usually much shorter than a paper summary. Generally, citation text generation can be considered as a task of generating a very short summary of paper B given the context of paper A. The difficulty lies in that given different A or different contexts of A, the task aims to produce different citation texts for the same B. Most commonly, the citation text is a single sentence, but sometimes it may consist of several sentences (Jebari et al., 2018; Qazvinian and Radev, 2010; Sondhi and Zhai, 2014). Like (Small, 2011), we define citation text as a block of text composed of one or more consecutive sentences surrounding the reference sign. Each citation sentence can be classified as explicit or implicit (Qazvinian and Radev, 2010; Athar and Teufel, 2012; Yasunaga et al., 2019). Explicit citation is a citation sentence that contains explicit reference to the cited paper. An implicit (or non-explicit) citation sentence appears around the explicit citation sentence and it does not attach any explicit reference to the cited paper but supplies additional information abo"
2020.acl-main.550,P17-1099,0,0.0626367,"generation dataset based on the ACL Anthology Network corpus (AAN) (Radev et al., 2013). We first perform human annotation and get 1,000 citation texts (including explicit and implicit citation sentences). We randomly select 400 citation texts as test set, and use the other 600 citation texts to first train a citation text extraction model and then use the extraction model to automatically extract many more citation texts to build a large-scale training dataset. With the training dataset we construct, we can train our citation generation model. In this paper, we use pointer-generator network (See et al., 2017) as the baseline model. We believe that the key to dealing with citation text generation problem is modelling the relationship between the context of citing paper A and the content of cited paper B. So we encode the context of paper A and the abstract of paper B separately, and add cross attention mechanism by making context and abstract attend to each other. We call our model multi-source pointergenerator network with cross attention mechanism. The evaluation results show that our model outperforms the baseline models. Our contributions are summarized as follows: • We propose a new task of au"
2020.acl-main.550,N06-1056,0,0.0186545,"marization is different from the task of citation text generation, which aims to generate a usually very short text to describe the cited paper in the given context of the citing paper. 3 Problem and Corpus Formally, given a citing paper A, a cited paper B and the context C in A, the task aims to generate the citation text T to describe B. The context C refers to the sentences surrounding the target citation text in A and it is provided to distinguish different mentions of B in different positions of A. The following example shows a paragraph of (Lu et al., 2008) and this article cites paper (Wong and Mooney, 2006). In this example, A refers to (Lu et al., 2008) and B refers to (Wong and Mooney, 2006). The sentence underlined (i.e., the second sentence) is an explicit citation, and the sentence in italics (i.e., the third sentence) is an implicit citationand both of them compose the citation text. The remaining two 6182 sentences (i.e., the first and last sentences) compose the context C of A. The phrase in bold which indicates the explicit citation to paper B is called reference sign. And the explicit citation text can be defined as the sentence with a reference sign to the cited paper. The implicit ci"
2020.acl-main.554,N19-1253,0,0.0266161,"ross-lingual summarization is a challenging task as it requires learning to understand different languages and learning how to summarize at the same time. It would be difficult for the model to directly learn cross-lingual summarization. In this paper, we explore this question: can we ease the training and enhance the cross-lingual summarization by establishing alignment of context representations between two languages? Learning cross-lingual representations has been proven a beneficial method for cross-lingual transfer for some downstream tasks (Klementiev et al., 2012; Artetxe et al., 2018; Ahmad et al., 2019; Chen et al., 2019). The underlying idea is to learn a shared embedding space for two languages to improve the model’s ability for cross-lingual transfer. Recently, it has been shown that this method can also be applied to context representations (Aldarmaki and Diab, 2019; Schuster et al., 2019). In this paper, we show that the learning of cross-lingual representations is also beneficial for neural crosslingual summarization models. We propose a multi-task framework that jointly learns to summarize and align context-level representations. Concretely, we first integrate monolingual summarizati"
2020.acl-main.554,N19-1391,0,0.0936231,"s question: can we ease the training and enhance the cross-lingual summarization by establishing alignment of context representations between two languages? Learning cross-lingual representations has been proven a beneficial method for cross-lingual transfer for some downstream tasks (Klementiev et al., 2012; Artetxe et al., 2018; Ahmad et al., 2019; Chen et al., 2019). The underlying idea is to learn a shared embedding space for two languages to improve the model’s ability for cross-lingual transfer. Recently, it has been shown that this method can also be applied to context representations (Aldarmaki and Diab, 2019; Schuster et al., 2019). In this paper, we show that the learning of cross-lingual representations is also beneficial for neural crosslingual summarization models. We propose a multi-task framework that jointly learns to summarize and align context-level representations. Concretely, we first integrate monolingual summarization models and cross-lingual summarization models into one unified model and then 6220 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6220–6231 c July 5 - 10, 2020. 2020 Association for Computational Linguistics build two line"
2020.acl-main.554,D18-1549,0,0.0759801,"summarization model. Cross-lingual summarization is a challenging task as it requires learning to understand different languages and learning how to summarize at the same time. It would be difficult for the model to directly learn cross-lingual summarization. In this paper, we explore this question: can we ease the training and enhance the cross-lingual summarization by establishing alignment of context representations between two languages? Learning cross-lingual representations has been proven a beneficial method for cross-lingual transfer for some downstream tasks (Klementiev et al., 2012; Artetxe et al., 2018; Ahmad et al., 2019; Chen et al., 2019). The underlying idea is to learn a shared embedding space for two languages to improve the model’s ability for cross-lingual transfer. Recently, it has been shown that this method can also be applied to context representations (Aldarmaki and Diab, 2019; Schuster et al., 2019). In this paper, we show that the learning of cross-lingual representations is also beneficial for neural crosslingual summarization models. We propose a multi-task framework that jointly learns to summarize and align context-level representations. Concretely, we first integrate mon"
2020.acl-main.554,P19-1299,0,0.0406016,"zation is a challenging task as it requires learning to understand different languages and learning how to summarize at the same time. It would be difficult for the model to directly learn cross-lingual summarization. In this paper, we explore this question: can we ease the training and enhance the cross-lingual summarization by establishing alignment of context representations between two languages? Learning cross-lingual representations has been proven a beneficial method for cross-lingual transfer for some downstream tasks (Klementiev et al., 2012; Artetxe et al., 2018; Ahmad et al., 2019; Chen et al., 2019). The underlying idea is to learn a shared embedding space for two languages to improve the model’s ability for cross-lingual transfer. Recently, it has been shown that this method can also be applied to context representations (Aldarmaki and Diab, 2019; Schuster et al., 2019). In this paper, we show that the learning of cross-lingual representations is also beneficial for neural crosslingual summarization models. We propose a multi-task framework that jointly learns to summarize and align context-level representations. Concretely, we first integrate monolingual summarization models and cross-"
2020.acl-main.554,W12-3018,0,0.0455508,"Missing"
2020.acl-main.554,P19-1305,0,0.1929,"lingual abstractive summarization are mainly based on the summarization-translation or translationsummarization pipeline paradigm and adopt different strategies to incorporate bilingual features (Leuski et al., 2003; Orasan and Chiorean, 2008; Wan et al., 2010; Wan, 2011) into the pipeline model. Recently, Shen et al. (2018) first propose a neural cross-lingual summarization system based on a large-scale corpus. They first translate the texts automatically from the source language into the target language and then use the teacher-student framework to train a cross-lingual summarization model. Duan et al. (2019) further improve this teacher-student framework by using genuine summaries paired with the translated pseudo source sentences to train the cross-lingual summarization model. Zhu et al. (2019) propose a multi-task learning framework to train a neural cross-lingual summarization model. Cross-lingual summarization is a challenging task as it requires learning to understand different languages and learning how to summarize at the same time. It would be difficult for the model to directly learn cross-lingual summarization. In this paper, we explore this question: can we ease the training and enhanc"
2020.acl-main.554,orasan-chiorean-2008-evaluation,0,0.120218,"Missing"
2020.acl-main.554,P19-1018,0,0.0218515,"Missing"
2020.acl-main.554,N19-1162,0,0.0124956,"e training and enhance the cross-lingual summarization by establishing alignment of context representations between two languages? Learning cross-lingual representations has been proven a beneficial method for cross-lingual transfer for some downstream tasks (Klementiev et al., 2012; Artetxe et al., 2018; Ahmad et al., 2019; Chen et al., 2019). The underlying idea is to learn a shared embedding space for two languages to improve the model’s ability for cross-lingual transfer. Recently, it has been shown that this method can also be applied to context representations (Aldarmaki and Diab, 2019; Schuster et al., 2019). In this paper, we show that the learning of cross-lingual representations is also beneficial for neural crosslingual summarization models. We propose a multi-task framework that jointly learns to summarize and align context-level representations. Concretely, we first integrate monolingual summarization models and cross-lingual summarization models into one unified model and then 6220 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6220–6231 c July 5 - 10, 2020. 2020 Association for Computational Linguistics build two linear mappings to project t"
2020.acl-main.554,D15-1229,0,0.031659,"ranslated Chinese source sentences provided by (Duan et al., 2019) to do Zh-to-En tests. DUC2004 DUC2004 corpus only contains test sets. We use the model trained on gigaword corpus to generate summaries on DUC2004 test sets. We use the 500 human-translated test samples provided by (Duan et al., 2019) to do Zh-to-En tests. • Pipe-ST The Pipe-ST baseline first uses a monolingual summarization model to generate the summaries, then uses a translation model to translate the summaries to the other language. We replace the translation model with the Google translation system as PipeST*. LCSTS LCSTS (Hu et al., 2015) is a Chinese summarization corpus, which contains 2.40M training pairs, 10,666 validation pairs, and 725 test pairs. We use 3K cross-lingual test samples provided by Zhu et al. (2019) to do Zh-to-En tests. • Pseudo The Pseudo baseline directly trains a cross-lingual summarization model by using the pseudo parallel cross-lingual summarization data. CNN/DM CNN/DM (Hermann et al., 2015) contains 287.2K training pairs, 13.3K validation pairs, and 11.5K test pairs. We use the 3K cross-lingual test samples provided by Zhu et al. (2019) to do En-to-Zh cross-lingual tests. 6.3 • XLM Pretraining This"
2020.acl-main.554,P17-1099,0,0.0605722,"earning to align and summarize. We design relevant loss functions to train this framework and propose several methods to enhance the isomorphism and cross-lingual transfer between languages. Experimental results show that our model can outperform competitive models in most cases. In addition, we show that our model even has the ability to generate cross-lingual summaries without access to any cross-lingual corpus. 1 Introduction Neural abstractive summarization has witnessed rapid growth in recent years. Variants of sequenceto-sequence models have shown to obtain promising results on English (See et al., 2017) or Chinese summarization datasets. However, Cross-lingual summarization, which aims at generating a summary in one language from input text in a different language, has been rarely studied because of the lack of parallel corpora. Early researches on cross-lingual abstractive summarization are mainly based on the summarization-translation or translationsummarization pipeline paradigm and adopt different strategies to incorporate bilingual features (Leuski et al., 2003; Orasan and Chiorean, 2008; Wan et al., 2010; Wan, 2011) into the pipeline model. Recently, Shen et al. (2018) first propose a"
2020.acl-main.554,C12-1089,0,0.0407293,"n a neural cross-lingual summarization model. Cross-lingual summarization is a challenging task as it requires learning to understand different languages and learning how to summarize at the same time. It would be difficult for the model to directly learn cross-lingual summarization. In this paper, we explore this question: can we ease the training and enhance the cross-lingual summarization by establishing alignment of context representations between two languages? Learning cross-lingual representations has been proven a beneficial method for cross-lingual transfer for some downstream tasks (Klementiev et al., 2012; Artetxe et al., 2018; Ahmad et al., 2019; Chen et al., 2019). The underlying idea is to learn a shared embedding space for two languages to improve the model’s ability for cross-lingual transfer. Recently, it has been shown that this method can also be applied to context representations (Aldarmaki and Diab, 2019; Schuster et al., 2019). In this paper, we show that the learning of cross-lingual representations is also beneficial for neural crosslingual summarization models. We propose a multi-task framework that jointly learns to summarize and align context-level representations. Concretely,"
2020.acl-main.554,P18-1072,0,0.0235341,"Missing"
2020.acl-main.554,P11-1155,1,0.953596,"uence models have shown to obtain promising results on English (See et al., 2017) or Chinese summarization datasets. However, Cross-lingual summarization, which aims at generating a summary in one language from input text in a different language, has been rarely studied because of the lack of parallel corpora. Early researches on cross-lingual abstractive summarization are mainly based on the summarization-translation or translationsummarization pipeline paradigm and adopt different strategies to incorporate bilingual features (Leuski et al., 2003; Orasan and Chiorean, 2008; Wan et al., 2010; Wan, 2011) into the pipeline model. Recently, Shen et al. (2018) first propose a neural cross-lingual summarization system based on a large-scale corpus. They first translate the texts automatically from the source language into the target language and then use the teacher-student framework to train a cross-lingual summarization model. Duan et al. (2019) further improve this teacher-student framework by using genuine summaries paired with the translated pseudo source sentences to train the cross-lingual summarization model. Zhu et al. (2019) propose a multi-task learning framework to train a neural cros"
2020.acl-main.554,P10-1094,1,0.93813,"of sequenceto-sequence models have shown to obtain promising results on English (See et al., 2017) or Chinese summarization datasets. However, Cross-lingual summarization, which aims at generating a summary in one language from input text in a different language, has been rarely studied because of the lack of parallel corpora. Early researches on cross-lingual abstractive summarization are mainly based on the summarization-translation or translationsummarization pipeline paradigm and adopt different strategies to incorporate bilingual features (Leuski et al., 2003; Orasan and Chiorean, 2008; Wan et al., 2010; Wan, 2011) into the pipeline model. Recently, Shen et al. (2018) first propose a neural cross-lingual summarization system based on a large-scale corpus. They first translate the texts automatically from the source language into the target language and then use the teacher-student framework to train a cross-lingual summarization model. Duan et al. (2019) further improve this teacher-student framework by using genuine summaries paired with the translated pseudo source sentences to train the cross-lingual summarization model. Zhu et al. (2019) propose a multi-task learning framework to train a"
2020.acl-main.554,N15-1104,0,0.0631925,"Missing"
2020.acl-main.554,D15-1012,1,0.937014,"Missing"
2020.acl-main.554,D19-1302,0,0.167021,"Missing"
2020.acl-main.556,N18-1150,0,0.0638061,"Missing"
2020.acl-main.556,P18-1063,0,0.0196254,"document clusters into account, while the too-long input often leads to the degradation in document summarization (Cohan et al., 2018; Liu and Lapata, 2019). Recently, hierarchical frameworks have shown their effectiveness on multi-document summarization (Zhang et al., 2018; Liu and Lapata, 2019). These approaches usually use multiple encoders to model hierarchical relationships in the discourse structure, but other methods to incorporate the structural semantic knowledge have not been explored. The combination of extractive and abstractive has been explored in single document summarization. Chen and Bansal (2018) use the extracted sentences as the input of the abstractive summarization. Subramanian et al. (2019) concatenate the extracted summary to the original document as the input of the abstractive summarization. In this work, we treat documents, sentences, and words as the different granularity of semantic units, and connect these semantic units within a three-granularity hierarchical relation graph. With the multi-granularity hierarchical structure, we can unify extractive and abstractive summarization into one architecture simultaneously. Extractive summarization operates on sentence-granularity"
2020.acl-main.556,N13-1136,0,0.178653,"Missing"
2020.acl-main.556,N18-2097,0,0.0286639,"phs to improve the performance for the extractive summarization. Abstractive methods can generate new words and new sentences, but it is technically more difficult than extractive methods. Some works on multidocument summarization simply concatenate multiple source documents into a long flat sequence and model multi-document summarization as a long sequence-to-sequence task (Liu et al., 2018; Fabbri et al., 2019). However, these approaches don’t take the hierarchical structure of document clusters into account, while the too-long input often leads to the degradation in document summarization (Cohan et al., 2018; Liu and Lapata, 2019). Recently, hierarchical frameworks have shown their effectiveness on multi-document summarization (Zhang et al., 2018; Liu and Lapata, 2019). These approaches usually use multiple encoders to model hierarchical relationships in the discourse structure, but other methods to incorporate the structural semantic knowledge have not been explored. The combination of extractive and abstractive has been explored in single document summarization. Chen and Bansal (2018) use the extracted sentences as the input of the abstractive summarization. Subramanian et al. (2019) concatenat"
2020.acl-main.556,N19-1409,0,0.0279046,"learning rate α = 0.0001, momentum β1 = 0.9, β2 = 0.999 and weight decay  = 10−5 . When the valid loss on the development set increases for two consecutive epochs, the learning rate is halved. We use a mini-batch size of 10, and set the hyper-parameter k = 5 and λ = 2. Given the salience score predicted by the sentence extractor, we apply a simple greedy procedure to select sentences. We select one sentence based on the descending order of the salience scores and append to the extracted summary until the summary reaches 300 words. We disallow repeating the same trigram (Paulus et al., 2018; Edunov et al., 2019) and use beam search with a beam size of 5 for summary generator. 4.3 Metrics and Baselines We use ROUGE (Lin, 2004) to evaluate the produced summary in our experiments. Following previous work, we report ROUGE F11 on MultiNews dataset. We compare our model with several typical baselines and several baselines proposed in the latest years. Lead-3 is an extractive baseline which concatenates the first-3 sentences of each source document as a summary. LexRank (Erkan and Radev, 2004) 1 The ROUGE evaluation option: -c 95 -2 4 -U -r 1000 -n 4 -w 1.2 -a R-1 39.41 38.27 38.44 38.77 43.86 41.85 43.57 4"
2020.acl-main.556,P19-1102,0,0.509866,"ginal, which are relatively simple. Cao et al. (2015) rank sentences with a recursive neural network. Yasunaga et al. (2017) employ a Graph Convolutional Network (GCN) to incorporate sentence relation graphs to improve the performance for the extractive summarization. Abstractive methods can generate new words and new sentences, but it is technically more difficult than extractive methods. Some works on multidocument summarization simply concatenate multiple source documents into a long flat sequence and model multi-document summarization as a long sequence-to-sequence task (Liu et al., 2018; Fabbri et al., 2019). However, these approaches don’t take the hierarchical structure of document clusters into account, while the too-long input often leads to the degradation in document summarization (Cohan et al., 2018; Liu and Lapata, 2019). Recently, hierarchical frameworks have shown their effectiveness on multi-document summarization (Zhang et al., 2018; Liu and Lapata, 2019). These approaches usually use multiple encoders to model hierarchical relationships in the discourse structure, but other methods to incorporate the structural semantic knowledge have not been explored. The combination of extractive"
2020.acl-main.556,W04-3252,0,0.595976,"paired with a documents cluster of average 2103 words discussing a topic. The number of source documents per summary presents as shown in Table 1 . While the dataset contains abstractive gold summaries, it is not readily suited to training extractive models. So we follow the work of Zhou et al. (2018) on extractive summary labeling, constructing gold-label sequences by greedily optimizing ROUGE-2 F1 on the gold-standard summary. 6248 # of source Frequency # of source Frequency 2 23,894 7 382 3 12,707 8 209 4 5,022 9 89 5 1,873 10 33 6 763 Model Lead-3 LexRank (Erkan and Radev, 2004) TextRank (Mihalcea and Tarau, 2004) MMR(Carbonell and Goldstein, 1998) HIBERT (Zhang et al., 2019) PGN (See et al., 2017) CopyTransformer(Gehrmann et al., 2018) Hi-MAP(Fabbri et al., 2019) HF(Liu and Lapata, 2019) MGSum-ext MGSum-abs oracle ext Table 1: The distribution of number of source articles per instance in Multi-News dataset. 4.2 Implementation Details We set our model parameters based on preliminary experiments on the development set. We prune the vocabulary to 50k and use the word in the source documents with maximum weight in copy attention to replace the unknown word of the generated summary. We set the dimension of"
2020.acl-main.556,D18-1443,0,0.0425212,"Missing"
2020.acl-main.556,P16-1154,0,0.0484194,"The masking mechanism ensures that the prediction of the position t depends only on the known output of the position before t.  g˜ = LayerNorm g l−1 +MSAttn g l−1 , g l−1 (9) The cross-attention sub-layer take the selfattention output g˜ as the queries and the multigranularity encoder output ow as keys and values to performs multi-head sparse attention. The feedforward network is used to further transform the outputs. c = LayerNorm (˜ g + MSAtt (˜ g , ow )) g l = LayerNorm (c + FFN(c)) where Wg ∈ Rdmodel ×dvocab , bg ∈ Rdvocab and dvocab is the size of target vocabulary. The copy mechanism (Gu et al., 2016) is employed to tackle the problem of out-of-vocabulary (OOV) words. We compute the copy attention εt with the decoder output g L2 and the input representations ow , and further obtain copy distribution pct . i,j,k where σ is the sigmoid function, Wη Rdmodel ×1 , bη ∈ R. 3.4 ∈ Objective Function We train the sentence extractor and the summary generator in a unified architecture in an end-toend manner. We use the cross entropy as both the extractor loss and the generator loss. N 1 X  (n) Lext = − ys log y˜s(n) + N n=1     (n) 1 − ys log 1 − y˜s(n) Labs = − (14) N 1 X (n) log p(yw ) N n=1"
2020.acl-main.556,D18-1446,0,0.164947,"rs into account, while the too-long input often leads to the degradation in document summarization (Cohan et al., 2018; Liu and Lapata, 2019). Recently, hierarchical frameworks have shown their effectiveness on multi-document summarization (Zhang et al., 2018; Liu and Lapata, 2019). These approaches usually use multiple encoders to model hierarchical relationships in the discourse structure, but other methods to incorporate the structural semantic knowledge have not been explored. The combination of extractive and abstractive has been explored in single document summarization. Chen and Bansal (2018) use the extracted sentences as the input of the abstractive summarization. Subramanian et al. (2019) concatenate the extracted summary to the original document as the input of the abstractive summarization. In this work, we treat documents, sentences, and words as the different granularity of semantic units, and connect these semantic units within a three-granularity hierarchical relation graph. With the multi-granularity hierarchical structure, we can unify extractive and abstractive summarization into one architecture simultaneously. Extractive summarization operates on sentence-granularity"
2020.acl-main.556,N03-1020,0,0.28121,"ove methods from this paper. HT (Liu and Lapata, 2019) is a Transformer based model with an attention mechanism to share information cross-document for abstractive multi-document summarization. It is used 6249 initially to generate Wikipedia, and we reproduce their method for the multi-document news summarization. 4.4 3.38 3.29 3.22 3.07 2.98 2.81 3.05 2.89 2.82 2.97 3.06 2.95 2.96 3.03 2.73 Automatic Evaluation Following previous work, we report ROUGE-1 (unigram), ROUGE-2 (bigram), and ROUGE-SU4 (skip bigrams with a maximum distance of 4 words) scores as the metrics for automatic evaluation (Lin and Hovy, 2003). In Table 2, we report the results on the Multi-News test set and our proposed multi-granularity model (denoted as MGSum) outperforms various previous models. Our abstractive method achieves scores of 46.00, 16.81, and 20.09 on the three ROUGE metrics while our extractive method achieves scores of 44.75, 15.75, and 19.30 on the three ROUGE metrics. We can also see that the abstractive methods perform better than the extractive methods. We attribute this result to the observation that the gold summary of this dataset tends to use new expressions to summarize the original input documents. Owing"
2020.acl-main.556,P19-1500,0,0.299016,"erformance for the extractive summarization. Abstractive methods can generate new words and new sentences, but it is technically more difficult than extractive methods. Some works on multidocument summarization simply concatenate multiple source documents into a long flat sequence and model multi-document summarization as a long sequence-to-sequence task (Liu et al., 2018; Fabbri et al., 2019). However, these approaches don’t take the hierarchical structure of document clusters into account, while the too-long input often leads to the degradation in document summarization (Cohan et al., 2018; Liu and Lapata, 2019). Recently, hierarchical frameworks have shown their effectiveness on multi-document summarization (Zhang et al., 2018; Liu and Lapata, 2019). These approaches usually use multiple encoders to model hierarchical relationships in the discourse structure, but other methods to incorporate the structural semantic knowledge have not been explored. The combination of extractive and abstractive has been explored in single document summarization. Chen and Bansal (2018) use the extracted sentences as the input of the abstractive summarization. Subramanian et al. (2019) concatenate the extracted summary"
2020.acl-main.556,P17-1099,0,0.779213,"nt granularity of semantic representations, which helps to capture multi-granularity key information and improves the performance of both abstractive and extractive summarization. Experiment results show that our proposed model substantially outperforms all strong baseline methods and achieves the best results on the Multi-News dataset. 1 Introduction Document summarization aims at producing a fluent, condensed summary for given documents. Single document summarization has shown promising results with sequence-to-sequence models that encode a source document and then decode it into a summary (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; C ¸ elikyilmaz et al., 2018). Multi-document summarization requires producing a summary from a cluster of thematically related documents, where the given documents complement and overlap each other. Multi-document summarization involves identifying important information and filtering out redundant information from multiple input sources. There are two primary methodologies for multidocument summarization: extractive and abstractive. Extractive methods directly select important sentences from the original, which are relatively simple. Cao et al. (20"
2020.acl-main.556,K17-1045,0,0.0469282,"Missing"
2020.acl-main.556,P19-1499,0,0.339558,"ohan et al., 2018; Liu and Lapata, 2019). Recently, hierarchical frameworks have shown their effectiveness on multi-document summarization (Zhang et al., 2018; Liu and Lapata, 2019). These approaches usually use multiple encoders to model hierarchical relationships in the discourse structure, but other methods to incorporate the structural semantic knowledge have not been explored. The combination of extractive and abstractive has been explored in single document summarization. Chen and Bansal (2018) use the extracted sentences as the input of the abstractive summarization. Subramanian et al. (2019) concatenate the extracted summary to the original document as the input of the abstractive summarization. In this work, we treat documents, sentences, and words as the different granularity of semantic units, and connect these semantic units within a three-granularity hierarchical relation graph. With the multi-granularity hierarchical structure, we can unify extractive and abstractive summarization into one architecture simultaneously. Extractive summarization operates on sentence-granularity and directly supervises the sentence representations while abstractive summarization operates on wor"
2020.acl-main.556,P18-1061,0,0.0249751,".1 Experiment Dataset We experiment with the latest released Multi-News dataset (Fabbri et al., 2019), which is the first large scale multi-document news summarization dataset. It contains about 44972 pairs for training, 5622 pairs for development, and 5622 for the test. Each summary of the average of 264 words is paired with a documents cluster of average 2103 words discussing a topic. The number of source documents per summary presents as shown in Table 1 . While the dataset contains abstractive gold summaries, it is not readily suited to training extractive models. So we follow the work of Zhou et al. (2018) on extractive summary labeling, constructing gold-label sequences by greedily optimizing ROUGE-2 F1 on the gold-standard summary. 6248 # of source Frequency # of source Frequency 2 23,894 7 382 3 12,707 8 209 4 5,022 9 89 5 1,873 10 33 6 763 Model Lead-3 LexRank (Erkan and Radev, 2004) TextRank (Mihalcea and Tarau, 2004) MMR(Carbonell and Goldstein, 1998) HIBERT (Zhang et al., 2019) PGN (See et al., 2017) CopyTransformer(Gehrmann et al., 2018) Hi-MAP(Fabbri et al., 2019) HF(Liu and Lapata, 2019) MGSum-ext MGSum-abs oracle ext Table 1: The distribution of number of source articles per instance"
2020.acl-main.606,P13-1023,0,0.0240186,"meaning representations. 4 Two State-of-the-art Parsers Existing work in data-driven semantic graph parsing can be roughly divided into four types, namely composition-, factorization-, transitionand translation-based ones (Koller et al., 2019). According to experimental results obtained on benchmark datasets with various target structures including Abstract Meaning Representation(AMR; Langkilde and Knight, 1998; Banarescu et al., 2013), Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), Semantic Dependency Parsing (SDP) as well as Universal Conceptual Cognitive Annotatio (UCCA; Abend and Rappoport, 2013), the compositionand factorization-based approaches are the leading approaches obtained by now (Lindemann et al., 2019; Zhang et al., 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the result of a derivation process, in which a set of lexical and syntactico-semantic rules are iteratively applied and evaluated. The core engine of the composition-based parser is a graph rewriting system that expli"
2020.acl-main.606,W13-2322,0,0.0372891,"plorations of variable alignments. The numerical results are displayed in Table 2. The modest SMATCH scores indicate the existence of great divergence between the literal and intended meaning representations. 4 Two State-of-the-art Parsers Existing work in data-driven semantic graph parsing can be roughly divided into four types, namely composition-, factorization-, transitionand translation-based ones (Koller et al., 2019). According to experimental results obtained on benchmark datasets with various target structures including Abstract Meaning Representation(AMR; Langkilde and Knight, 1998; Banarescu et al., 2013), Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), Semantic Dependency Parsing (SDP) as well as Universal Conceptual Cognitive Annotatio (UCCA; Abend and Rappoport, 2013), the compositionand factorization-based approaches are the leading approaches obtained by now (Lindemann et al., 2019; Zhang et al., 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the result of a derivation proc"
2020.acl-main.606,W15-0128,0,0.0592677,"king with ERG As there is no gold semantics-annotated corpus for learner English and building such a corpus from scratch is tedious and time-consuming, we exploit ERG to establish a large-scale sembanking with informative semantic representations. To be specific, for each input sentence S, we generate K-best semantic graphs G1 , G2 , ..., GK with an ERG-based processor, i.e. ACE2 . The created grammar-licensed analyses contain both a derivation tree recording the used grammar rules and lexical entries, and the associated semantic representation constructed compositionally via this derivation (Bender et al., 2015). The elaborate grammar rules enable sembanking reusable, automatically derivable and task-independent, and it can benefit many NLP systems by incorporating domainspecific knowledge and reasoning. 6785 2 http://sweaglesw.org/linguistics/ace/ 3.3.3 Reranking ERG Analyses with Gold UD Previous work has proved that high-quality syntax makes a large impact on semantic parsing tasks such as SRL (Hermann and Blunsom, 2013; He et al., 2017; Qian et al., 2017). The exploratory work in Lin et al. (2018) draws the same conclusion in an L2 situation. We assume that the incorporation of syntactic trees he"
2020.acl-main.606,P16-1070,0,0.550433,"y to annotate such large-scale atypical data with in-depth linguistic analysis. High-performance automatic annotation of learner texts, from an engineering point of view, enables it possible to derive high-quality information by structuring the specific type of data, and from a scientific point of view, facilitates quantitative studies for Second Language Acquisition (SLA), which is complementary to hands-on experiences in interpreting interlanguage phenom∗ Now works at Alibaba Group. ena (Gass, 2013). This direction has been recently explored by the NLP community (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Lin et al., 2018). Different from standard English, ESL may preserve many features of learners’ first languages1 . The difference between learner texts and benchmark training data, e.g. Penn TreeBank (PTB; Marcus et al., 1993), is more related to linguistic competence, rather than performance (Chomsky, 2014). This makes processing ESL different from almost all the existing discussions on domain adaptation in NLP. Despite the ubiquity and importance of interlanguages at both the scientific and engineering levels, it is only partially understood how NLP models perform on them. In this paper,"
2020.acl-main.606,P13-2131,0,0.15408,"raphs, we can pick out graph Gp with the highest score SCORE(Gp , T ). Our goal is to ensure SCORE(Gg , T ) > SCORE(Gp , T ), which can be achieved with the help of the averaged structured perceptron learning algorithm. 3.3.4 Effectiveness of the Reranking Model To evaluate the capability of our proposed reranking model, we randomly extract 10,000 and 2,476 sentences from DeepBank (Flickinger et al., 2012) as the training and validation data respectively. The gold UD analyses are derived from the original PTB (Marcus et al., 1993) annotations. With regard to evaluation metrics, we use SMATCH (Cai and Knight, 2013) and Elementary Dependency Matching (EDM; Dridan and Oepen, 2011). Results are shown in Table 1. The first three rows EDM Node Edge All All Top-1 92.8 90.0 91.4 87.8 Rerank (50) Rerank (500) 94.7 95.1 93.4 93.9 94.1 94.5 92.0 92.7 Oracle (50) Oracle (500) 97.6 98.7 96.9 98.5 97.2 98.6 95.6 97.6 – – – 94-95 Inter-Annotator Agreement Table 1: Results of reranking. “Top-1” means the most preferable graph generated by the ACE parser. “Rerank (50)” and “Rerank (500)” means that K is set to 50 and 500 during reranking respectively. “Oracle” means directly selecting the best-performing graph for each"
2020.acl-main.606,K18-1054,1,0.929555,", 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the result of a derivation process, in which a set of lexical and syntactico-semantic rules are iteratively applied and evaluated. The core engine of the composition-based parser is a graph rewriting system that explicitly explores the syntacticosemantic recursive derivations that are governed by a Synchronous Hyperedge Replacement Grammar (SHRG; Chen et al., 2018b). The parser constructs DMRS graphs by explicitly modeling such derivations. It utilizes a constituent parser to build a syntactic derivation, and then selects semantic HRG rules associated to syntactic CFG rules to generate a graph. When multiple rules are applicable for a single phrase, a neural network is used to rank them. We use the parser in Chen et al. (2019) based on both the lexicalized grammar and the constructional grammar (refer to Chen et al. (2018b) for the distinction). Henceforth, they are called lexicalized and constructional compositionbased parsers respectively. Figure 3 s"
2020.acl-main.606,P18-1038,1,0.92732,", 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the result of a derivation process, in which a set of lexical and syntactico-semantic rules are iteratively applied and evaluated. The core engine of the composition-based parser is a graph rewriting system that explicitly explores the syntacticosemantic recursive derivations that are governed by a Synchronous Hyperedge Replacement Grammar (SHRG; Chen et al., 2018b). The parser constructs DMRS graphs by explicitly modeling such derivations. It utilizes a constituent parser to build a syntactic derivation, and then selects semantic HRG rules associated to syntactic CFG rules to generate a graph. When multiple rules are applicable for a single phrase, a neural network is used to rank them. We use the parser in Chen et al. (2019) based on both the lexicalized grammar and the constructional grammar (refer to Chen et al. (2018b) for the distinction). Henceforth, they are called lexicalized and constructional compositionbased parsers respectively. Figure 3 s"
2020.acl-main.606,P81-1022,0,0.679333,"Missing"
2020.acl-main.606,P18-2077,0,0.0151891,"ed nodes) in a sub-graph are marked as communication channels to other meaning parts. According to the construction rule (shown in double-framed box), we glue the two sub-parts via the filled nodes, forming a larger graph with the syntactic label “NP”. More details are illustrated in Chen et al. (2019) possible candidates. The parser works with a twostage pipeline structure, for concept identification and relation detection, as illustrated in Figure 4. In the first phase, sequence labeling models are used to predict nodes, and in the second phase, we utilize the dependency model introduced by Dozat and Manning (2018) to link nodes. The two models in both stages use a multi-layer BiLSTM to encode tokens. In the first stage, another softmax layer is utilized to predict concept-related labels, while in the second stage, the dependency model is utilized to calculate a score for selecting token pairs. 5 5.1 Parsing to Literal Meanings Robustness of Parsing Models We experiment with three different parsers introduced in last section, i.e., lexicalized and constructional composition-based parsers and the factorization-based parser. We train these parsers on DeepBank version 1.1, corresponding to ERG 1214, and us"
2020.acl-main.606,W11-2927,0,0.0317885,", T ). Our goal is to ensure SCORE(Gg , T ) > SCORE(Gp , T ), which can be achieved with the help of the averaged structured perceptron learning algorithm. 3.3.4 Effectiveness of the Reranking Model To evaluate the capability of our proposed reranking model, we randomly extract 10,000 and 2,476 sentences from DeepBank (Flickinger et al., 2012) as the training and validation data respectively. The gold UD analyses are derived from the original PTB (Marcus et al., 1993) annotations. With regard to evaluation metrics, we use SMATCH (Cai and Knight, 2013) and Elementary Dependency Matching (EDM; Dridan and Oepen, 2011). Results are shown in Table 1. The first three rows EDM Node Edge All All Top-1 92.8 90.0 91.4 87.8 Rerank (50) Rerank (500) 94.7 95.1 93.4 93.9 94.1 94.5 92.0 92.7 Oracle (50) Oracle (500) 97.6 98.7 96.9 98.5 97.2 98.6 95.6 97.6 – – – 94-95 Inter-Annotator Agreement Table 1: Results of reranking. “Top-1” means the most preferable graph generated by the ACE parser. “Rerank (50)” and “Rerank (500)” means that K is set to 50 and 500 during reranking respectively. “Oracle” means directly selecting the best-performing graph for each sentence from the K-best list. The inter-annotator agreement of"
2020.acl-main.606,D19-1222,0,0.0165987,"identify concepts and also to determine dependency relations together with the resulted conceptual embeddings (in yellow). Comparing different models, we can see that the factorization-based approach performs better on all setups, which is consistent with previous studies (Koller et al., 2019). The gap between results on DeepBank and the other two datasets demonstrates the existence of cross-domain effect, which has been observed in plenty of NLP tasks, including but not limited to semantic parsing (Chen et al., 2018a; Lindemann et al., 2019; Blitzer and Pereira, 2007; Ben-David et al., 2010; Elsahar and Gallé, 2019). Furthermore, it is clear that there is a drop from L1 to L2 data. The gap is marked in the last row, the average of which is about 4 points, indicating the insufficiency of using standard models to parse learner texts. Still, the factorization-based model yields a little bit more robust results on non-native data. We hold that the poor performance of compositionbased model is caused by the explicit syntacticosemantic derivation process. Since the interface between syntax and semantics of learner languages is somewhat unclear, directly applying rewriting rules extracted from L1 data may be pa"
2020.acl-main.606,flickinger-etal-2014-towards,0,0.0602175,"Missing"
2020.acl-main.606,N16-4001,0,0.028172,"parsing on learner English. 3.3.1 2014). ERS helps to reveal much deeper semantic analysis than other shallow target structures such as the predicate-argument relations in the semantic role labeling (SRL) task. Moreover, it can be derived into several different forms, like the logicalform-based representation Minimal Recursion Semantics (MRS) and the graph-shaped structure Elementary Dependency Structures (EDS). We resort to this resource to build an informative analysis for learner English and choose EDS as the target structure. Target Meaning Representation English Resource Semantics (ERS; Flickinger et al., 2016) is an important resource of semantic representations produced by the English Resource Grammar (ERG; Flickinger, 1999), a broad-coverage, linguistically motivated precision Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) of English (Flickinger, 2000, 2011). It provides rich semantic representations including the semantic roles and other detailed information such as the scope of quantifiers and scopal operators including negation, as well as semantic representations of linguistically complex phenomena such as time and date expressions, conditionals, and comparatives (Flicking"
2020.acl-main.606,P17-1044,0,0.0396301,"ivation tree recording the used grammar rules and lexical entries, and the associated semantic representation constructed compositionally via this derivation (Bender et al., 2015). The elaborate grammar rules enable sembanking reusable, automatically derivable and task-independent, and it can benefit many NLP systems by incorporating domainspecific knowledge and reasoning. 6785 2 http://sweaglesw.org/linguistics/ace/ 3.3.3 Reranking ERG Analyses with Gold UD Previous work has proved that high-quality syntax makes a large impact on semantic parsing tasks such as SRL (Hermann and Blunsom, 2013; He et al., 2017; Qian et al., 2017). The exploratory work in Lin et al. (2018) draws the same conclusion in an L2 situation. We assume that the incorporation of syntactic trees helps improve the quality of our evaluation data. We conduct a reranking procedure on the Kbest candidates derived under the ERG framework with the aid of gold Universal Dependencies (UD; Berzak et al., 2016b) trees and select the graph which best fits into the gold syntactic tree (represented as T ). Our reranking model can be formulated into: demonstrates that the parsing performance has been greatly improved after reranking, provin"
2020.acl-main.606,P13-1088,0,0.0163775,"analyses contain both a derivation tree recording the used grammar rules and lexical entries, and the associated semantic representation constructed compositionally via this derivation (Bender et al., 2015). The elaborate grammar rules enable sembanking reusable, automatically derivable and task-independent, and it can benefit many NLP systems by incorporating domainspecific knowledge and reasoning. 6785 2 http://sweaglesw.org/linguistics/ace/ 3.3.3 Reranking ERG Analyses with Gold UD Previous work has proved that high-quality syntax makes a large impact on semantic parsing tasks such as SRL (Hermann and Blunsom, 2013; He et al., 2017; Qian et al., 2017). The exploratory work in Lin et al. (2018) draws the same conclusion in an L2 situation. We assume that the incorporation of syntactic trees helps improve the quality of our evaluation data. We conduct a reranking procedure on the Kbest candidates derived under the ERG framework with the aid of gold Universal Dependencies (UD; Berzak et al., 2016b) trees and select the graph which best fits into the gold syntactic tree (represented as T ). Our reranking model can be formulated into: demonstrates that the parsing performance has been greatly improved after"
2020.acl-main.606,P19-4002,1,0.830458,"tences are parallel, we can compare the graph structures directly. We use SMATCH (Cai and Knight, 2013) as the evaluation metric which provides the token-wise evaluation along with effective explorations of variable alignments. The numerical results are displayed in Table 2. The modest SMATCH scores indicate the existence of great divergence between the literal and intended meaning representations. 4 Two State-of-the-art Parsers Existing work in data-driven semantic graph parsing can be roughly divided into four types, namely composition-, factorization-, transitionand translation-based ones (Koller et al., 2019). According to experimental results obtained on benchmark datasets with various target structures including Abstract Meaning Representation(AMR; Langkilde and Knight, 1998; Banarescu et al., 2013), Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), Semantic Dependency Parsing (SDP) as well as Universal Conceptual Cognitive Annotatio (UCCA; Abend and Rappoport, 2013), the compositionand factorization-based approaches are the leading approaches obtained by now (Lindemann et al., 2019; Zhang et al., 2019). In this paper, we use these two kinds of parsers (compositionand factorizatio"
2020.acl-main.606,P98-1116,0,0.46553,"tion along with effective explorations of variable alignments. The numerical results are displayed in Table 2. The modest SMATCH scores indicate the existence of great divergence between the literal and intended meaning representations. 4 Two State-of-the-art Parsers Existing work in data-driven semantic graph parsing can be roughly divided into four types, namely composition-, factorization-, transitionand translation-based ones (Koller et al., 2019). According to experimental results obtained on benchmark datasets with various target structures including Abstract Meaning Representation(AMR; Langkilde and Knight, 1998; Banarescu et al., 2013), Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), Semantic Dependency Parsing (SDP) as well as Universal Conceptual Cognitive Annotatio (UCCA; Abend and Rappoport, 2013), the compositionand factorization-based approaches are the leading approaches obtained by now (Lindemann et al., 2019; Zhang et al., 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the re"
2020.acl-main.606,D18-1414,1,0.674505,"ge-scale atypical data with in-depth linguistic analysis. High-performance automatic annotation of learner texts, from an engineering point of view, enables it possible to derive high-quality information by structuring the specific type of data, and from a scientific point of view, facilitates quantitative studies for Second Language Acquisition (SLA), which is complementary to hands-on experiences in interpreting interlanguage phenom∗ Now works at Alibaba Group. ena (Gass, 2013). This direction has been recently explored by the NLP community (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Lin et al., 2018). Different from standard English, ESL may preserve many features of learners’ first languages1 . The difference between learner texts and benchmark training data, e.g. Penn TreeBank (PTB; Marcus et al., 1993), is more related to linguistic competence, rather than performance (Chomsky, 2014). This makes processing ESL different from almost all the existing discussions on domain adaptation in NLP. Despite the ubiquity and importance of interlanguages at both the scientific and engineering levels, it is only partially understood how NLP models perform on them. In this paper, we present, to the b"
2020.acl-main.606,P19-1450,0,0.0684563,"y divided into four types, namely composition-, factorization-, transitionand translation-based ones (Koller et al., 2019). According to experimental results obtained on benchmark datasets with various target structures including Abstract Meaning Representation(AMR; Langkilde and Knight, 1998; Banarescu et al., 2013), Elementary Dependency Structures (EDS; Oepen and Lønning, 2006), Semantic Dependency Parsing (SDP) as well as Universal Conceptual Cognitive Annotatio (UCCA; Abend and Rappoport, 2013), the compositionand factorization-based approaches are the leading approaches obtained by now (Lindemann et al., 2019; Zhang et al., 2019). In this paper, we use these two kinds of parsers (compositionand factorization-based parsers) described in Chen et al. (2019) as state-of-the-art representatives. Following the principle of compositionality, a semantic graph can be viewed as the result of a derivation process, in which a set of lexical and syntactico-semantic rules are iteratively applied and evaluated. The core engine of the composition-based parser is a graph rewriting system that explicitly explores the syntacticosemantic recursive derivations that are governed by a Synchronous Hyperedge Replacement G"
2020.acl-main.606,J93-2004,0,0.0732856,"structuring the specific type of data, and from a scientific point of view, facilitates quantitative studies for Second Language Acquisition (SLA), which is complementary to hands-on experiences in interpreting interlanguage phenom∗ Now works at Alibaba Group. ena (Gass, 2013). This direction has been recently explored by the NLP community (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Lin et al., 2018). Different from standard English, ESL may preserve many features of learners’ first languages1 . The difference between learner texts and benchmark training data, e.g. Penn TreeBank (PTB; Marcus et al., 1993), is more related to linguistic competence, rather than performance (Chomsky, 2014). This makes processing ESL different from almost all the existing discussions on domain adaptation in NLP. Despite the ubiquity and importance of interlanguages at both the scientific and engineering levels, it is only partially understood how NLP models perform on them. In this paper, we present, to the best of our knowledge, the first study on Semantic Parsing for English as a Second Language. Motivated by the Interface Hypothesis (Sorace, 2011) in SLA, we emphasize on the divergence between literal and inten"
2020.acl-main.606,P16-1173,0,0.52004,"e need an automatic machinery to annotate such large-scale atypical data with in-depth linguistic analysis. High-performance automatic annotation of learner texts, from an engineering point of view, enables it possible to derive high-quality information by structuring the specific type of data, and from a scientific point of view, facilitates quantitative studies for Second Language Acquisition (SLA), which is complementary to hands-on experiences in interpreting interlanguage phenom∗ Now works at Alibaba Group. ena (Gass, 2013). This direction has been recently explored by the NLP community (Nagata and Sakaguchi, 2016; Berzak et al., 2016a; Lin et al., 2018). Different from standard English, ESL may preserve many features of learners’ first languages1 . The difference between learner texts and benchmark training data, e.g. Penn TreeBank (PTB; Marcus et al., 1993), is more related to linguistic competence, rather than performance (Chomsky, 2014). This makes processing ESL different from almost all the existing discussions on domain adaptation in NLP. Despite the ubiquity and importance of interlanguages at both the scientific and engineering levels, it is only partially understood how NLP models perform on"
2020.acl-main.606,P11-1121,0,0.0754947,"Missing"
2020.acl-main.606,W14-1701,0,0.0176324,"icit syntacticosemantic derivation process. Since the interface between syntax and semantics of learner languages is somewhat unclear, directly applying rewriting rules extracted from L1 data may be partly misleading. 5.2 Relatedness to Grammatical Errors It is crucial to understand whether and to what extent parsers are indeed robust to learner errors. We re-analyse the results from two aspects. First, we modify the original SMATCH evaluation metric and enable it to be sensitive to distances from errors. Then we make a distinction among typical error types proposed in CoNLL-2014 Shared Task (Ng et al., 2014). Results show that standard parsers can not handle learner errors well enough and their behaviors vary among different 6788 Node LEX Edge All Node CXG Edge All Node FAC Edge All DeepBank 94.05 92.96 93.50 95.83 92.87 94.34 96.85 95.19 96.01 L1 L2 ∆ 88.41 84.38 4.03 86.44 82.23 4.21 87.41 83.29 4.12 90.32 86.47 3.85 86.04 81.70 4.34 88.14 84.04 4.10 92.28 88.68 3.60 89.12 84.45 4.67 90.91 86.91 4.00 Data Table 3: SMATCH scores of semantic parsing on different test data. Henceforth, LEX, CXG and FAC refer to lexicalized and constructional composition-based parsers and the factorization-based pa"
2020.acl-main.606,W17-4305,0,0.0126699,"rding the used grammar rules and lexical entries, and the associated semantic representation constructed compositionally via this derivation (Bender et al., 2015). The elaborate grammar rules enable sembanking reusable, automatically derivable and task-independent, and it can benefit many NLP systems by incorporating domainspecific knowledge and reasoning. 6785 2 http://sweaglesw.org/linguistics/ace/ 3.3.3 Reranking ERG Analyses with Gold UD Previous work has proved that high-quality syntax makes a large impact on semantic parsing tasks such as SRL (Hermann and Blunsom, 2013; He et al., 2017; Qian et al., 2017). The exploratory work in Lin et al. (2018) draws the same conclusion in an L2 situation. We assume that the incorporation of syntactic trees helps improve the quality of our evaluation data. We conduct a reranking procedure on the Kbest candidates derived under the ERG framework with the aid of gold Universal Dependencies (UD; Berzak et al., 2016b) trees and select the graph which best fits into the gold syntactic tree (represented as T ). Our reranking model can be formulated into: demonstrates that the parsing performance has been greatly improved after reranking, proving the power of the p"
2020.acl-main.606,C12-2094,0,0.0257966,"directly annotate the linguistic properties in learner sentences (Dickinson and Ragheb, 2009; DıazNegrillo et al., 2010; Rastelli, 2013). The lack of precisely annotated data has limited the systematic analysis of interlanguages. There are several attempts to set up annotation schemes for different linguistic layers of learner languages, such as POS tags and syntactic information (Hirschmann et al., 2007; Dıaz-Negrillo et al., 2010; Rosen et al., 2014; Nagata and Sakaguchi, 2016; Berzak et al., 2016b). But it is challenging to elucidate the exact definition of “syntax” for learner languages. Ragheb and Dickinson (2012) defines multiple layers (morphological dependencies, distributional dependencies, and subcategorization) based on different evidence to capture non-canonical properties. Similarly, motivated by the Interface Hypothesis (Sorace, 2011), we employ a principled method to create parallel semantic representations for learner English by discriminating between the literal and intended meanings. With regard to the semantic analysis for learner languages, Lin et al. (2018) takes the first step in this direction. Based on a parallel semantic role labeling (SRL) corpus, they prove the importance of synta"
2020.acl-main.606,W10-1004,0,0.0433926,"eaker meaning or interpretation). The former puts an emphasis on the linguistic code features appearing in the sentence, while the latter is derived from the author’s intention. When we consider an interlanguage, the divergence between literal and intended meanings is much larger due to various cross-lingual influences. It is reasonable to consider both aspects to develop a principled method to process outputs from L2 learners. 3.1 SLA at the Syntax-Semantics Interface VP 2 Related Work PP VP Early work regarding the collection of learner corpora mainly concentrates on tagging alleged errors (Rozovskaya and Roth, 2010; Nagata et al., 2011). The past decade has seen a tendency to directly annotate the linguistic properties in learner sentences (Dickinson and Ragheb, 2009; DıazNegrillo et al., 2010; Rastelli, 2013). The lack of precisely annotated data has limited the systematic analysis of interlanguages. There are several attempts to set up annotation schemes for different linguistic layers of learner languages, such as POS tags and syntactic information (Hirschmann et al., 2007; Dıaz-Negrillo et al., 2010; Rosen et al., 2014; Nagata and Sakaguchi, 2016; Berzak et al., 2016b). But it is challenging to eluc"
2020.acl-main.606,P19-1009,0,0.0217843,"Missing"
2020.acl-main.606,N19-1014,0,0.0549157,"n resort to Grammatical Error Correction (GEC), the task of correcting different kinds of errors in text. It has attracted a lot of attention and considerable effort has been made to promote the performance on specific benchmark data. We utilize two off-the-shelf GEC models. One is a multilayer convolutional encoder-decoder neural network proposed in Chollampatt and Ng (2018). We choose the basic model introduced in the paper. The other model copies the unchanged words from the source sentence to the target sentence using a pretrained copy-augmented architecture with a denoising auto-encoder (Zhao et al., 2019). It achieves the state-of-the-art performance without extra pseudo data. Performances of the two GEC models on CoNLL-2014 test set are shown in Table 5. We train the factorization-based model on DeepBank and examine the performance on L2 and L1 sentences as well as the revised sentences by two GEC models. The produced graphs are compared with I-silver which represents the intended meaning. We notice that during the computation of SMATCH, some disagreements of nodes result from the discrepancy of morphological variation or different collocations between the input and the standard sentence. Hen"
2020.acl-main.640,D17-1209,0,0.0624897,"Missing"
2020.acl-main.640,P18-1026,0,0.531726,"l. (2018b) introduced the Graph2Seq problems which aim to generate target sequence from graph-structured data. The main challenge for Graph2Seq learning is to build a powerful encoder which is able to capture the inherent structure in the given graph and learn good representations for generating the target text. Early work relies on statistical methods or sequence-to-sequence (Seq2Seq) models where input graphs are linearized (Lu et al., 2009; Song et al., 2017; Konstas et al., 2017). Recent studies propose various models based on graph neural network (GNN) to encode graphs (Xu et al., 2018b; Beck et al., 2018; Guo et al., 2019; Damonte and Cohen, 2019; Ribeiro et al., 2019). However, these approaches only consider the relations between directly connected nodes, ignore the indirect relations between distance nodes. Inspired by the success of Transformer (Vaswani et al., 2017) which can learn the dependencies between all tokens without regard to their distance, the current state-of-theart Graph2Seq models (Zhu et al., 2019; Cai and Lam, 2020) are based on Transformer and learn the relations between all nodes no matter they are connected or not. These approaches use shortest relation path between nod"
2020.acl-main.640,2020.acl-main.640,1,0.0609248,"., 2009; Song et al., 2017; Konstas et al., 2017). Recent studies propose various models based on graph neural network (GNN) to encode graphs (Xu et al., 2018b; Beck et al., 2018; Guo et al., 2019; Damonte and Cohen, 2019; Ribeiro et al., 2019). However, these approaches only consider the relations between directly connected nodes, ignore the indirect relations between distance nodes. Inspired by the success of Transformer (Vaswani et al., 2017) which can learn the dependencies between all tokens without regard to their distance, the current state-of-theart Graph2Seq models (Zhu et al., 2019; Cai and Lam, 2020) are based on Transformer and learn the relations between all nodes no matter they are connected or not. These approaches use shortest relation path between nodes to encode semantic relationships. However, they ignore the information of nodes in the relation path and encode the direct relations and indirect relations without distinction. It may disturb the information propagation process when aggregate information from direct neighbors. To solve the issues above, we propose the Heterogeneous Graph Transformer (HetGT) to encode the graph, which independently model the different relations in the"
2020.acl-main.640,N19-1366,0,0.495363,"roblems which aim to generate target sequence from graph-structured data. The main challenge for Graph2Seq learning is to build a powerful encoder which is able to capture the inherent structure in the given graph and learn good representations for generating the target text. Early work relies on statistical methods or sequence-to-sequence (Seq2Seq) models where input graphs are linearized (Lu et al., 2009; Song et al., 2017; Konstas et al., 2017). Recent studies propose various models based on graph neural network (GNN) to encode graphs (Xu et al., 2018b; Beck et al., 2018; Guo et al., 2019; Damonte and Cohen, 2019; Ribeiro et al., 2019). However, these approaches only consider the relations between directly connected nodes, ignore the indirect relations between distance nodes. Inspired by the success of Transformer (Vaswani et al., 2017) which can learn the dependencies between all tokens without regard to their distance, the current state-of-theart Graph2Seq models (Zhu et al., 2019; Cai and Lam, 2020) are based on Transformer and learn the relations between all nodes no matter they are connected or not. These approaches use shortest relation path between nodes to encode semantic relationships. Howeve"
2020.acl-main.640,W14-3348,0,0.137279,"eck et al., 2018) DGCN (Guo et al., 2019) GTransformer (Cai and Lam, 2020) 16.1 16.7 19.0 21.3 42.4 44.1 47.9 - 9.6 9.8 12.1 14.1 33.3 37.1 41.1 - GGNN2Seqensenmble (Beck et al., 2018) DGCNensemble (Guo et al., 2019) 19.6 20.5 45.1 45.8 - 11.7 13.1 35.9 37.8 - Transformer HetGTdot-product (ours) HetGTadditive (ours) 23.18 25.39 25.44 49.54 51.55 51.27 26.00 27.37 27.26 14.83 16.15 16.29 39.27 41.10 41.14 19.12 20.18 20.35 Table 3: Results for syntax-based NMT on the test sets of En-De and En-Cs. 3.3 Metrics and Baselines For performance evaluation, we use BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and sentence-level CHRF++ (Popovi´c, 2015) with default hyperparameter settings as evaluation metrics. Meanwhile, we use the tools in Neubig et al. (2019) for the statistical significance tests. Our baseline is the original Transformer 2 . For AMR-to-text generation, Transformer takes linearized graphs as inputs. For syntax-based NMT, Transformer is trained on the preprocessed translation dataset without syntactic information. We also compare the performance of HetGT with previous single/ensenmble approaches which can be grouped into three categories: (1) Recurrent neu2 Parameters were chosen"
2020.acl-main.640,D18-1198,0,0.0675724,"ttention in recent years. Many Natural Language Process (NLP) problems involve learning from not only sequential data but also more complex structured data, such as semantic graphs. For example, AMR-to-text generation is a task of generating text from Abstract Meaning Representation (AMR) graphs, where nodes denote semantic concepts and edges refer to relations between concepts (see Figure 1 (a)). In addition, it has been shown that even if the sequential input can be augmented by additional structural information, bringing benefits for some tasks, such as semantic parsing (Pust et al., 2015; Guo and Lu, 2018) and machine translation (Bastings et al., 2017). Therefore, Xu et al. (2018b) introduced the Graph2Seq problems which aim to generate target sequence from graph-structured data. The main challenge for Graph2Seq learning is to build a powerful encoder which is able to capture the inherent structure in the given graph and learn good representations for generating the target text. Early work relies on statistical methods or sequence-to-sequence (Seq2Seq) models where input graphs are linearized (Lu et al., 2009; Song et al., 2017; Konstas et al., 2017). Recent studies propose various models base"
2020.acl-main.640,Q19-1019,0,0.691803,"ed the Graph2Seq problems which aim to generate target sequence from graph-structured data. The main challenge for Graph2Seq learning is to build a powerful encoder which is able to capture the inherent structure in the given graph and learn good representations for generating the target text. Early work relies on statistical methods or sequence-to-sequence (Seq2Seq) models where input graphs are linearized (Lu et al., 2009; Song et al., 2017; Konstas et al., 2017). Recent studies propose various models based on graph neural network (GNN) to encode graphs (Xu et al., 2018b; Beck et al., 2018; Guo et al., 2019; Damonte and Cohen, 2019; Ribeiro et al., 2019). However, these approaches only consider the relations between directly connected nodes, ignore the indirect relations between distance nodes. Inspired by the success of Transformer (Vaswani et al., 2017) which can learn the dependencies between all tokens without regard to their distance, the current state-of-theart Graph2Seq models (Zhu et al., 2019; Cai and Lam, 2020) are based on Transformer and learn the relations between all nodes no matter they are connected or not. These approaches use shortest relation path between nodes to encode seman"
2020.acl-main.640,P17-4012,0,0.0328636,"n the preprocessing steps. 3.2 Parameter Settings Both our encoder and decoder have 6 layers with 512-dimensional word embeddings and hidden states. We employ 8 heads and dropout with a rate of 0.3. For optimization, we use Adam optimizer with β2 = 0.998 and set batch size to 4096 tokens. Meanwhile, we increase learning rate linearly for the first warmup steps, and decrease it thereafter proportionally to the inverse square root of the step number. We set warmup steps to 8000. The similar learning rate schedule is adopted in (Vaswani et al., 2017). Our implementation uses the openNMT library (Klein et al., 2017). We train the models for 250K steps on a single GeForce GTX 1080 Ti GPU. Our code is available at https://github.com/QAQ-v/HetGT. 7149 LDC2015E86 (AMR15) Model LDC2017T10 (AMR17) BLEU CHRF++ METEOR BLEU CHRF++ METEOR GGNN2Seq (Beck et al., 2018) GraphLSTM (Song et al., 2018) GCNSEQ (Damonte and Cohen, 2019) DGCN (Guo et al., 2019) G2S-GGNN (Ribeiro et al., 2019) Transformer-SA (Zhu et al., 2019) Transformer-CNN (Zhu et al., 2019) GTransformer (Cai and Lam, 2020) 23.3 24.40 25.9 24.32 29.66 29.10 27.4 63.00 62.10 56.4 23.60 30.53 35.45 35.00 32.9 23.3 24.54 27.9 27.87 31.54 31.82 29.8 50.4 57."
2020.acl-main.640,P17-1014,0,0.495418,"s, such as semantic parsing (Pust et al., 2015; Guo and Lu, 2018) and machine translation (Bastings et al., 2017). Therefore, Xu et al. (2018b) introduced the Graph2Seq problems which aim to generate target sequence from graph-structured data. The main challenge for Graph2Seq learning is to build a powerful encoder which is able to capture the inherent structure in the given graph and learn good representations for generating the target text. Early work relies on statistical methods or sequence-to-sequence (Seq2Seq) models where input graphs are linearized (Lu et al., 2009; Song et al., 2017; Konstas et al., 2017). Recent studies propose various models based on graph neural network (GNN) to encode graphs (Xu et al., 2018b; Beck et al., 2018; Guo et al., 2019; Damonte and Cohen, 2019; Ribeiro et al., 2019). However, these approaches only consider the relations between directly connected nodes, ignore the indirect relations between distance nodes. Inspired by the success of Transformer (Vaswani et al., 2017) which can learn the dependencies between all tokens without regard to their distance, the current state-of-theart Graph2Seq models (Zhu et al., 2019; Cai and Lam, 2020) are based on Transformer and l"
2020.acl-main.640,D09-1042,0,0.248588,"ion, bringing benefits for some tasks, such as semantic parsing (Pust et al., 2015; Guo and Lu, 2018) and machine translation (Bastings et al., 2017). Therefore, Xu et al. (2018b) introduced the Graph2Seq problems which aim to generate target sequence from graph-structured data. The main challenge for Graph2Seq learning is to build a powerful encoder which is able to capture the inherent structure in the given graph and learn good representations for generating the target text. Early work relies on statistical methods or sequence-to-sequence (Seq2Seq) models where input graphs are linearized (Lu et al., 2009; Song et al., 2017; Konstas et al., 2017). Recent studies propose various models based on graph neural network (GNN) to encode graphs (Xu et al., 2018b; Beck et al., 2018; Guo et al., 2019; Damonte and Cohen, 2019; Ribeiro et al., 2019). However, these approaches only consider the relations between directly connected nodes, ignore the indirect relations between distance nodes. Inspired by the success of Transformer (Vaswani et al., 2017) which can learn the dependencies between all tokens without regard to their distance, the current state-of-theart Graph2Seq models (Zhu et al., 2019; Cai and"
2020.acl-main.640,N19-4007,0,0.0405167,"Missing"
2020.acl-main.640,P02-1040,0,0.108312,"stings et al., 2017) GGNN2Seq (Beck et al., 2018) DGCN (Guo et al., 2019) GTransformer (Cai and Lam, 2020) 16.1 16.7 19.0 21.3 42.4 44.1 47.9 - 9.6 9.8 12.1 14.1 33.3 37.1 41.1 - GGNN2Seqensenmble (Beck et al., 2018) DGCNensemble (Guo et al., 2019) 19.6 20.5 45.1 45.8 - 11.7 13.1 35.9 37.8 - Transformer HetGTdot-product (ours) HetGTadditive (ours) 23.18 25.39 25.44 49.54 51.55 51.27 26.00 27.37 27.26 14.83 16.15 16.29 39.27 41.10 41.14 19.12 20.18 20.35 Table 3: Results for syntax-based NMT on the test sets of En-De and En-Cs. 3.3 Metrics and Baselines For performance evaluation, we use BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and sentence-level CHRF++ (Popovi´c, 2015) with default hyperparameter settings as evaluation metrics. Meanwhile, we use the tools in Neubig et al. (2019) for the statistical significance tests. Our baseline is the original Transformer 2 . For AMR-to-text generation, Transformer takes linearized graphs as inputs. For syntax-based NMT, Transformer is trained on the preprocessed translation dataset without syntactic information. We also compare the performance of HetGT with previous single/ensenmble approaches which can be grouped into three categories: (1) R"
2020.acl-main.640,W15-3049,0,0.0835325,"Missing"
2020.acl-main.640,D15-1136,0,0.0313125,"attracted lots of attention in recent years. Many Natural Language Process (NLP) problems involve learning from not only sequential data but also more complex structured data, such as semantic graphs. For example, AMR-to-text generation is a task of generating text from Abstract Meaning Representation (AMR) graphs, where nodes denote semantic concepts and edges refer to relations between concepts (see Figure 1 (a)). In addition, it has been shown that even if the sequential input can be augmented by additional structural information, bringing benefits for some tasks, such as semantic parsing (Pust et al., 2015; Guo and Lu, 2018) and machine translation (Bastings et al., 2017). Therefore, Xu et al. (2018b) introduced the Graph2Seq problems which aim to generate target sequence from graph-structured data. The main challenge for Graph2Seq learning is to build a powerful encoder which is able to capture the inherent structure in the given graph and learn good representations for generating the target text. Early work relies on statistical methods or sequence-to-sequence (Seq2Seq) models where input graphs are linearized (Lu et al., 2009; Song et al., 2017; Konstas et al., 2017). Recent studies propose"
2020.acl-main.640,D19-1314,0,0.364457,"rate target sequence from graph-structured data. The main challenge for Graph2Seq learning is to build a powerful encoder which is able to capture the inherent structure in the given graph and learn good representations for generating the target text. Early work relies on statistical methods or sequence-to-sequence (Seq2Seq) models where input graphs are linearized (Lu et al., 2009; Song et al., 2017; Konstas et al., 2017). Recent studies propose various models based on graph neural network (GNN) to encode graphs (Xu et al., 2018b; Beck et al., 2018; Guo et al., 2019; Damonte and Cohen, 2019; Ribeiro et al., 2019). However, these approaches only consider the relations between directly connected nodes, ignore the indirect relations between distance nodes. Inspired by the success of Transformer (Vaswani et al., 2017) which can learn the dependencies between all tokens without regard to their distance, the current state-of-theart Graph2Seq models (Zhu et al., 2019; Cai and Lam, 2020) are based on Transformer and learn the relations between all nodes no matter they are connected or not. These approaches use shortest relation path between nodes to encode semantic relationships. However, they ignore the info"
2020.acl-main.640,P16-1162,0,0.0841277,"ad. 2.2 Input Graph Transformation Following Beck et al. (2018), we transform the original graph into the Levi graph. The transformation equivalently turns edges into additional nodes so we can encode the original edge labels in the same way as for nodes. We also add a reverse edge between each pair of connected nodes as well as a self-loop edge for each node. These strategies can make the model benefit from the information propagation from different directions (See Figure 1 (b)). In order to alleviate the data sparsity problem in the corpus, we further introduce the Byte Pair Encoding (BPE) (Sennrich et al., 2016) into the Levi Graph. We split the original node into multiple subword nodes. Besides adding default connections, we also add the reverse and self-loop edges among subwords. For example, the word country in Figure 2 is segmented into co@@, un@@, try with Heterogeneous Graph Transformer Our model is also an encoder-decoder architecture, consisting of stacked encoder and decoder layers. Given a preprocessed extended Levi graph, we split the extended Levi graph into multiple subgraphs according to its heterogeneity. In each graph encoder block, the node representation in different subgraphs is up"
2020.acl-main.640,D19-1548,0,0.702884,"nearized (Lu et al., 2009; Song et al., 2017; Konstas et al., 2017). Recent studies propose various models based on graph neural network (GNN) to encode graphs (Xu et al., 2018b; Beck et al., 2018; Guo et al., 2019; Damonte and Cohen, 2019; Ribeiro et al., 2019). However, these approaches only consider the relations between directly connected nodes, ignore the indirect relations between distance nodes. Inspired by the success of Transformer (Vaswani et al., 2017) which can learn the dependencies between all tokens without regard to their distance, the current state-of-theart Graph2Seq models (Zhu et al., 2019; Cai and Lam, 2020) are based on Transformer and learn the relations between all nodes no matter they are connected or not. These approaches use shortest relation path between nodes to encode semantic relationships. However, they ignore the information of nodes in the relation path and encode the direct relations and indirect relations without distinction. It may disturb the information propagation process when aggregate information from direct neighbors. To solve the issues above, we propose the Heterogeneous Graph Transformer (HetGT) to encode the graph, which independently model the differ"
2020.acl-main.640,N18-2074,0,0.0565404,"nformation in the encoder. Beck et al. (2018) employ Gated Graph Neural Networks (GGNN) as the encoder and Song et al. (2018) propose the graph-state LSTM to incorporate the graph structure. Their works belong to the family of recurrent neural network (RNN). In addition, there are some works are build upon the GNN. Damonte and Cohen (2019) propose stacking encoders including LSTM and GCN. Guo et al. (2019) introduce the densely connected GCN to encode richer local and non-local information for better graph representation. Recent studies also extend Transformer to encode structure information. Shaw et al. (2018) propose the relation-aware self-attention which learns explicit embeddings for pair-wise relationships between input elements. Zhu et al. (2019) and Cai and Lam (2020) both extend the relation-aware selfattention to generate text from AMR graph. Our model is also based on Transformer. However, we do not employ the relative position encoding to incorporate structural information. Instead, we directly mask the non-neighbor nodes attention when updating each nodes representation. Moreover, we introduce the heterogeneous information and jump connection to help model learn a better graph represent"
2020.acl-main.640,P17-2002,0,0.452056,"efits for some tasks, such as semantic parsing (Pust et al., 2015; Guo and Lu, 2018) and machine translation (Bastings et al., 2017). Therefore, Xu et al. (2018b) introduced the Graph2Seq problems which aim to generate target sequence from graph-structured data. The main challenge for Graph2Seq learning is to build a powerful encoder which is able to capture the inherent structure in the given graph and learn good representations for generating the target text. Early work relies on statistical methods or sequence-to-sequence (Seq2Seq) models where input graphs are linearized (Lu et al., 2009; Song et al., 2017; Konstas et al., 2017). Recent studies propose various models based on graph neural network (GNN) to encode graphs (Xu et al., 2018b; Beck et al., 2018; Guo et al., 2019; Damonte and Cohen, 2019; Ribeiro et al., 2019). However, these approaches only consider the relations between directly connected nodes, ignore the indirect relations between distance nodes. Inspired by the success of Transformer (Vaswani et al., 2017) which can learn the dependencies between all tokens without regard to their distance, the current state-of-theart Graph2Seq models (Zhu et al., 2019; Cai and Lam, 2020) are bas"
2020.acl-main.640,P18-1150,0,0.0868193,"eq model. Lu et al. (2009) propose an NLG approach built on top of tree conditional random fields to use the tree-structured meaning representation. Song et al. (2017) use synchronous node replacement grammar to generate text. Konstas et al. (2017) linearize the input graph and feed it to the seq2seq model for text-to-AMR parsing and AMR-to-text generation. However, linearizing AMR graphs into sequences may incurs in loss of information. Recent efforts consider to capture the structural information in the encoder. Beck et al. (2018) employ Gated Graph Neural Networks (GGNN) as the encoder and Song et al. (2018) propose the graph-state LSTM to incorporate the graph structure. Their works belong to the family of recurrent neural network (RNN). In addition, there are some works are build upon the GNN. Damonte and Cohen (2019) propose stacking encoders including LSTM and GCN. Guo et al. (2019) introduce the densely connected GCN to encode richer local and non-local information for better graph representation. Recent studies also extend Transformer to encode structure information. Shaw et al. (2018) propose the relation-aware self-attention which learns explicit embeddings for pair-wise relationships bet"
2020.coling-main.121,P19-1331,0,0.158428,"e DRESS model which rewards the simple, fluent output sentences with proper meaning. Vu et al. (2018) used memory-augmented neural networks to adapt the existing architecture and Guo et al. (2018) used multi-task learning to improve the entailment and paraphrase capabilities. Recently, Kriz et al. (2019) used two techniques to solve the problem that the model tends to copy words directly, resulting in a long and complicated output sentence. Nishihara et al. (2019) proposed a method to simplify the original sentences to different level sentences. Different from most sequenceto-sequence models, Dong et al. (2019) proposed a neural programmer-interpreter approach to predict explicit edit operations directly. Jiang et al. (2020) proposed the neural CRF model to get better sentence alignment. However, up to now, most of the research on text simplification is limited to sentence level, ignoring the influence of document context on sentence simplification. Pitler and Nenkova (2008) are the first to empirically demonstrate that discourse relations are closely related to the perceived quality of the text. So far, Zhong et al. (2020) are the first and the only ones to focus on discourse level factors of text"
2020.coling-main.121,C18-1039,0,0.0133887,"ed on statistic machine learning. Using a small number of manual simplifications and a large number of paraphrases, Xu et al. (2016) adapted the method of the statistical machine translation. Nisioi et al. (2017) were the first to apply the sequence-to-sequence model to automatic text simplification, using the framework of neural machine translation and making specific improvements. Zhang and Lapata (2017) proposed the DRESS model which rewards the simple, fluent output sentences with proper meaning. Vu et al. (2018) used memory-augmented neural networks to adapt the existing architecture and Guo et al. (2018) used multi-task learning to improve the entailment and paraphrase capabilities. Recently, Kriz et al. (2019) used two techniques to solve the problem that the model tends to copy words directly, resulting in a long and complicated output sentence. Nishihara et al. (2019) proposed a method to simplify the original sentences to different level sentences. Different from most sequenceto-sequence models, Dong et al. (2019) proposed a neural programmer-interpreter approach to predict explicit edit operations directly. Jiang et al. (2020) proposed the neural CRF model to get better sentence alignmen"
2020.coling-main.121,W15-3014,0,0.0273284,"he FKGL value, the simpler the output sentences are. In this paper, we regard SARI as the most important criterion to judge the effect of simplification. We also employ human judges to conduct more reliable evaluation for this task. 4.3 Training Details Our model is based on the transformer model (Vaswani et al., 2017). The addition encoder, encoder and decoder in our model have 4 layers with 4 multi-heads. We set the size of the vocabulary to 45800 and other uncommon words in the training set are replaced with the out-of-vocabulary token UNK. When predicting, we follow the method proposed by Jean et al. (2015) to replace UNK. We use the Adagrad optimizer (Duchi et al., 2011) to train our model and we train 50 epochs. We set the learning rate to 0.1 and the training batch size to 16. Following BERT (Devlin et al., 2019), we replace the ReLU activation function with a GELU activation function (Hendrycks and Gimpel, 2016) that performs better on transformer. When training, we firstly use the sentence pairs with context information to train the whole model, then we fix the parameters of the document-level module. Next, we use the sentence pairs without context information to train the sentence-level mo"
2020.coling-main.121,2020.acl-main.709,0,0.0323026,"ugmented neural networks to adapt the existing architecture and Guo et al. (2018) used multi-task learning to improve the entailment and paraphrase capabilities. Recently, Kriz et al. (2019) used two techniques to solve the problem that the model tends to copy words directly, resulting in a long and complicated output sentence. Nishihara et al. (2019) proposed a method to simplify the original sentences to different level sentences. Different from most sequenceto-sequence models, Dong et al. (2019) proposed a neural programmer-interpreter approach to predict explicit edit operations directly. Jiang et al. (2020) proposed the neural CRF model to get better sentence alignment. However, up to now, most of the research on text simplification is limited to sentence level, ignoring the influence of document context on sentence simplification. Pitler and Nenkova (2008) are the first to empirically demonstrate that discourse relations are closely related to the perceived quality of the text. So far, Zhong et al. (2020) are the first and the only ones to focus on discourse level factors of text simplification. Their results have shown that using discourse level factors is useful for predicting sentence deleti"
2020.coling-main.121,O13-1007,0,0.0192576,"Missing"
2020.coling-main.121,P13-1151,0,0.148936,"nal sentence. The phrase “sing as well as play” in the context may provide additional information to help the simplification. The phrase “at the bar” is retained because of the presence of “at the Midtown Bar” in the context. Correspondingly, there is no mention of information related to “fan base” in the context, so adjectives such as “loyal”, “small” are deleted. In this paper, we are committed to investigating the influence of document context on text simplification and proposing a neural model to improve simplification using context. Using the Wikipedia datasets (Coster and Kauchak, 2011; Kauchak, 2013), we first construct a dataset in which the document context This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 1411 Proceedings of the 28th International Conference on Computational Linguistics, pages 1411–1423 Barcelona, Spain (Online), December 8-13, 2020 Context Original Simplified To fund her private lessons , Simone performed at the Midtown Bar & Grill on Pacific Avenue in Atlantic City , whose owner insisted that she sing as well as play the piano . Simone ’s mixture of jazz , blues , and c"
2020.coling-main.121,N19-1317,0,0.0145411,"hrases, Xu et al. (2016) adapted the method of the statistical machine translation. Nisioi et al. (2017) were the first to apply the sequence-to-sequence model to automatic text simplification, using the framework of neural machine translation and making specific improvements. Zhang and Lapata (2017) proposed the DRESS model which rewards the simple, fluent output sentences with proper meaning. Vu et al. (2018) used memory-augmented neural networks to adapt the existing architecture and Guo et al. (2018) used multi-task learning to improve the entailment and paraphrase capabilities. Recently, Kriz et al. (2019) used two techniques to solve the problem that the model tends to copy words directly, resulting in a long and complicated output sentence. Nishihara et al. (2019) proposed a method to simplify the original sentences to different level sentences. Different from most sequenceto-sequence models, Dong et al. (2019) proposed a neural programmer-interpreter approach to predict explicit edit operations directly. Jiang et al. (2020) proposed the neural CRF model to get better sentence alignment. However, up to now, most of the research on text simplification is limited to sentence level, ignoring the"
2020.coling-main.121,D17-1222,0,0.0158121,"ame meaning (Sulem et al., 2018b), which will benefit young children (Kajiwara et al., 2013) and non-native English speakers (Paetzold, 2015; Paetzold and Specia, 2016). It includes many ways to deal with the input text, such as deletion, reordering, paraphrase, and sentence separation (Saggion, 2017). Besides, text simplification is closely related to many natural ˇ language processing (NLP) tasks, such as machine translation (Stajner and Popovi´c, 2016; Hasler et al., 2017), paraphrase generation (Pavlick and Callison-Burch, 2016; Cao et al., 2017; Zhao et al., 2018) and text summarization (Li et al., 2017; Ma and Sun, 2017; Jin et al., 2020). In recent years, many researchers have proposed various models to improve the performance of text simplification. However, few people have explored the impact of document context (i.e., the preceding and following sentences of the original sentence to be simplified in a document) on text simplification, let alone establish a large-scale dataset containing context. Many examples like the one in Table 1 arouse our interest in exploring the influence of context. In the simplified sentence, “played and sang” is the simplification of “in her performances” in t"
2020.coling-main.121,P14-1041,0,0.0241967,"of document context on sentence simplification and build a large dataset for training and testing. (2) We propose and train a new model named simplification using context (SUC), which makes full use of context information. SUC outperforms the baselines on both automatic evaluation and human evaluation. (3) We use ablation experiments to illustrate the different effects of the preceding sentences and the following sentences on the current sentences to be simplified. 2 Related works Text simplification has developed rapidly in the past decade. Wubben et al. (2012) proposed the PBMTR model while Narayan and Gardent (2014) put forward the Hybrid model. Both models were based on statistic machine learning. Using a small number of manual simplifications and a large number of paraphrases, Xu et al. (2016) adapted the method of the statistical machine translation. Nisioi et al. (2017) were the first to apply the sequence-to-sequence model to automatic text simplification, using the framework of neural machine translation and making specific improvements. Zhang and Lapata (2017) proposed the DRESS model which rewards the simple, fluent output sentences with proper meaning. Vu et al. (2018) used memory-augmented neur"
2020.coling-main.121,P19-2036,0,0.0176933,"l to automatic text simplification, using the framework of neural machine translation and making specific improvements. Zhang and Lapata (2017) proposed the DRESS model which rewards the simple, fluent output sentences with proper meaning. Vu et al. (2018) used memory-augmented neural networks to adapt the existing architecture and Guo et al. (2018) used multi-task learning to improve the entailment and paraphrase capabilities. Recently, Kriz et al. (2019) used two techniques to solve the problem that the model tends to copy words directly, resulting in a long and complicated output sentence. Nishihara et al. (2019) proposed a method to simplify the original sentences to different level sentences. Different from most sequenceto-sequence models, Dong et al. (2019) proposed a neural programmer-interpreter approach to predict explicit edit operations directly. Jiang et al. (2020) proposed the neural CRF model to get better sentence alignment. However, up to now, most of the research on text simplification is limited to sentence level, ignoring the influence of document context on sentence simplification. Pitler and Nenkova (2008) are the first to empirically demonstrate that discourse relations are closely"
2020.coling-main.121,P17-2014,0,0.0168056,"c evaluation and human evaluation. (3) We use ablation experiments to illustrate the different effects of the preceding sentences and the following sentences on the current sentences to be simplified. 2 Related works Text simplification has developed rapidly in the past decade. Wubben et al. (2012) proposed the PBMTR model while Narayan and Gardent (2014) put forward the Hybrid model. Both models were based on statistic machine learning. Using a small number of manual simplifications and a large number of paraphrases, Xu et al. (2016) adapted the method of the statistical machine translation. Nisioi et al. (2017) were the first to apply the sequence-to-sequence model to automatic text simplification, using the framework of neural machine translation and making specific improvements. Zhang and Lapata (2017) proposed the DRESS model which rewards the simple, fluent output sentences with proper meaning. Vu et al. (2018) used memory-augmented neural networks to adapt the existing architecture and Guo et al. (2018) used multi-task learning to improve the entailment and paraphrase capabilities. Recently, Kriz et al. (2019) used two techniques to solve the problem that the model tends to copy words directly,"
2020.coling-main.121,W16-4912,0,0.0328495,"improve sentence simplification. In the ablation experiment, we show that using either the preceding sentences or the following sentences as context can significantly improve simplification. 1 Introduction Text simplification is a hot issue in the field of natural language generation (NLG). It is also one of the critical needs of society (Woodsend and Lapata, 2011). Text simplification aims to adapt a complex text into a more readable version with the same meaning (Sulem et al., 2018b), which will benefit young children (Kajiwara et al., 2013) and non-native English speakers (Paetzold, 2015; Paetzold and Specia, 2016). It includes many ways to deal with the input text, such as deletion, reordering, paraphrase, and sentence separation (Saggion, 2017). Besides, text simplification is closely related to many natural ˇ language processing (NLP) tasks, such as machine translation (Stajner and Popovi´c, 2016; Hasler et al., 2017), paraphrase generation (Pavlick and Callison-Burch, 2016; Cao et al., 2017; Zhao et al., 2018) and text summarization (Li et al., 2017; Ma and Sun, 2017; Jin et al., 2020). In recent years, many researchers have proposed various models to improve the performance of text simplification."
2020.coling-main.121,N15-2002,0,0.0229122,"ext indeed helps improve sentence simplification. In the ablation experiment, we show that using either the preceding sentences or the following sentences as context can significantly improve simplification. 1 Introduction Text simplification is a hot issue in the field of natural language generation (NLG). It is also one of the critical needs of society (Woodsend and Lapata, 2011). Text simplification aims to adapt a complex text into a more readable version with the same meaning (Sulem et al., 2018b), which will benefit young children (Kajiwara et al., 2013) and non-native English speakers (Paetzold, 2015; Paetzold and Specia, 2016). It includes many ways to deal with the input text, such as deletion, reordering, paraphrase, and sentence separation (Saggion, 2017). Besides, text simplification is closely related to many natural ˇ language processing (NLP) tasks, such as machine translation (Stajner and Popovi´c, 2016; Hasler et al., 2017), paraphrase generation (Pavlick and Callison-Burch, 2016; Cao et al., 2017; Zhao et al., 2018) and text summarization (Li et al., 2017; Ma and Sun, 2017; Jin et al., 2020). In recent years, many researchers have proposed various models to improve the performa"
2020.coling-main.121,P02-1040,0,0.109486,"t set. There is no repetition among the sentences in the test, validation and training sets. Previous study on machine translation has shown that using too much context information will not only not improve the results, but increase the computational complexity (Tu et al., 2018). Following Zhang et al. (2018), we take two preceding sentences and two following sentences of the current sentence as context information. If there is only one preceding sentence or following sentence, we choose to keep it. 4.2 Evaluation Metrics We use SARI (Xu et al., 2016) and FKGL (Kincaid et al., 1975) and BLEU (Papineni et al., 2002) as automatic evaluation metrics in our work. The SARI metric may be the most important criteria to measure the result of text simplification.4 The SARI metric compares the simplified sentence with the original one and the reference one at the same time. The SARI value comes from three parts: adding words, deleting words properly and keeping words properly. The values of the three parts are also reported in our results. The BLEU metric is commonly used previously and used to measure the similarity of output to a reference sentence (Zhao et al., 2020), so we decide to use this metric.5 It is wo"
2020.coling-main.121,P16-2024,0,0.0183315,"2011). Text simplification aims to adapt a complex text into a more readable version with the same meaning (Sulem et al., 2018b), which will benefit young children (Kajiwara et al., 2013) and non-native English speakers (Paetzold, 2015; Paetzold and Specia, 2016). It includes many ways to deal with the input text, such as deletion, reordering, paraphrase, and sentence separation (Saggion, 2017). Besides, text simplification is closely related to many natural ˇ language processing (NLP) tasks, such as machine translation (Stajner and Popovi´c, 2016; Hasler et al., 2017), paraphrase generation (Pavlick and Callison-Burch, 2016; Cao et al., 2017; Zhao et al., 2018) and text summarization (Li et al., 2017; Ma and Sun, 2017; Jin et al., 2020). In recent years, many researchers have proposed various models to improve the performance of text simplification. However, few people have explored the impact of document context (i.e., the preceding and following sentences of the original sentence to be simplified in a document) on text simplification, let alone establish a large-scale dataset containing context. Many examples like the one in Table 1 arouse our interest in exploring the influence of context. In the simplified s"
2020.coling-main.121,D08-1020,0,0.0343549,"nds to copy words directly, resulting in a long and complicated output sentence. Nishihara et al. (2019) proposed a method to simplify the original sentences to different level sentences. Different from most sequenceto-sequence models, Dong et al. (2019) proposed a neural programmer-interpreter approach to predict explicit edit operations directly. Jiang et al. (2020) proposed the neural CRF model to get better sentence alignment. However, up to now, most of the research on text simplification is limited to sentence level, ignoring the influence of document context on sentence simplification. Pitler and Nenkova (2008) are the first to empirically demonstrate that discourse relations are closely related to the perceived quality of the text. So far, Zhong et al. (2020) are the first and the only ones to focus on discourse level factors of text simplification. Their results have shown that using discourse level factors is useful for predicting sentence deletion. Nevertheless, different from our research focusing on sentence simplification, this work mainly 1 The dataset we used in the experiment is available at https://github.com/RLSNLP/Document-Context-to-SentenceSimplification 1412 analyzes and predicts the"
2020.coling-main.121,P17-1099,0,0.0485968,"third component is a multi-head attention but we take the output RE of the encoder as matrices K and V . The matrix Q is the output Attn0 of the previous multi-head attention. The formula can be defined as: Attn00 = M ultiHead(Attn0 , RE , RE ) (13) The last component is a neural network, which is the same as the one in encoder. Define RD as the output of the decoder, and the formula can be expressed as: RD = F F N (Attn00 ) 3.3 (14) Pointer-Generator Network and Coverage Mechanism The pointer-generator network copies words from the original sentence to solve the problem of out-ofvocabulary (See et al., 2017). Following the implementation2 , the pointer-generator network used in our model contains a multi-head attention. RD is taken as matrix Q and RE is taken as matrices K and V , which is defined as: A = M ultiHead(RD , RE , RE ) (15) The generation probability Pgen can be calculated as: Pgen = Sigmoid(W1 A + W2 RD + W3 Xsimple + b) (16) The vectors W1 , W2 , W3 and the scalar b are all learnable parameters. The final probability distribution can be defined as: P (W ) = Pgen Pvocab + (1 − Pgen )D (17) Where D can be obtained from multi-head attention and Pvocab is the vocabulary. The coverage mo"
2020.coling-main.121,W16-3411,0,0.0440338,"Missing"
2020.coling-main.121,D18-1081,0,0.0113454,"result of text simplification.4 The SARI metric compares the simplified sentence with the original one and the reference one at the same time. The SARI value comes from three parts: adding words, deleting words properly and keeping words properly. The values of the three parts are also reported in our results. The BLEU metric is commonly used previously and used to measure the similarity of output to a reference sentence (Zhao et al., 2020), so we decide to use this metric.5 It is worth noting, however, that the BLEU metric has been found often negatively correlates with simplicity recently (Sulem et al., 2018a). Following Dong et al. (2019), we choose to use the FKGL method to measure the readability of the output sentences.6 The lower the FKGL value, the simpler the output sentences are. In this paper, we regard SARI as the most important criterion to judge the effect of simplification. We also employ human judges to conduct more reliable evaluation for this task. 4.3 Training Details Our model is based on the transformer model (Vaswani et al., 2017). The addition encoder, encoder and decoder in our model have 4 layers with 4 multi-heads. We set the size of the vocabulary to 45800 and other uncom"
2020.coling-main.121,N18-1063,0,0.0157926,"result of text simplification.4 The SARI metric compares the simplified sentence with the original one and the reference one at the same time. The SARI value comes from three parts: adding words, deleting words properly and keeping words properly. The values of the three parts are also reported in our results. The BLEU metric is commonly used previously and used to measure the similarity of output to a reference sentence (Zhao et al., 2020), so we decide to use this metric.5 It is worth noting, however, that the BLEU metric has been found often negatively correlates with simplicity recently (Sulem et al., 2018a). Following Dong et al. (2019), we choose to use the FKGL method to measure the readability of the output sentences.6 The lower the FKGL value, the simpler the output sentences are. In this paper, we regard SARI as the most important criterion to judge the effect of simplification. We also employ human judges to conduct more reliable evaluation for this task. 4.3 Training Details Our model is based on the transformer model (Vaswani et al., 2017). The addition encoder, encoder and decoder in our model have 4 layers with 4 multi-heads. We set the size of the vocabulary to 45800 and other uncom"
2020.coling-main.121,P16-1008,0,0.0238173,"lti-head attention. RD is taken as matrix Q and RE is taken as matrices K and V , which is defined as: A = M ultiHead(RD , RE , RE ) (15) The generation probability Pgen can be calculated as: Pgen = Sigmoid(W1 A + W2 RD + W3 Xsimple + b) (16) The vectors W1 , W2 , W3 and the scalar b are all learnable parameters. The final probability distribution can be defined as: P (W ) = Pgen Pvocab + (1 − Pgen )D (17) Where D can be obtained from multi-head attention and Pvocab is the vocabulary. The coverage model focuses on solving the problem of generating text repeatedly in sequence-tosequence model (Tu et al., 2016). The general coverage model is given by: Ci = gupdate (Ci−1 , αi , Φ(h), Ψ) (18) Ci is the coverage vector which summarizes the previous attentions at time step i to help adjust future attention. gupdate updates Ci after new attention αi when decoding at time step i. Φ(h) is a word-specific feature and Ψ are different auxiliary inputs. 2 The code is available at https://github.com/lipiji/TranSummar. 1416 4 Experiments 4.1 Dataset Since the contexts are not provided in commonly used datasets such as Wikismall (Zhu et al., 2010) and Newsela (Xu et al., 2015), we need to build appropriate datase"
2020.coling-main.121,Q18-1029,0,0.0229382,"oose to retain them to train the sentence-level modules of our model. In the training set, there are around 110K aligned sentence pairs with context information and around 41K aligned sentence pairs without context information. From the remaining sentence pairs with context information, we use 5K as the validation set and 1K as the test set. There is no repetition among the sentences in the test, validation and training sets. Previous study on machine translation has shown that using too much context information will not only not improve the results, but increase the computational complexity (Tu et al., 2018). Following Zhang et al. (2018), we take two preceding sentences and two following sentences of the current sentence as context information. If there is only one preceding sentence or following sentence, we choose to keep it. 4.2 Evaluation Metrics We use SARI (Xu et al., 2016) and FKGL (Kincaid et al., 1975) and BLEU (Papineni et al., 2002) as automatic evaluation metrics in our work. The SARI metric may be the most important criteria to measure the result of text simplification.4 The SARI metric compares the simplified sentence with the original one and the reference one at the same time. Th"
2020.coling-main.121,N18-2013,0,0.0203631,"BMTR model while Narayan and Gardent (2014) put forward the Hybrid model. Both models were based on statistic machine learning. Using a small number of manual simplifications and a large number of paraphrases, Xu et al. (2016) adapted the method of the statistical machine translation. Nisioi et al. (2017) were the first to apply the sequence-to-sequence model to automatic text simplification, using the framework of neural machine translation and making specific improvements. Zhang and Lapata (2017) proposed the DRESS model which rewards the simple, fluent output sentences with proper meaning. Vu et al. (2018) used memory-augmented neural networks to adapt the existing architecture and Guo et al. (2018) used multi-task learning to improve the entailment and paraphrase capabilities. Recently, Kriz et al. (2019) used two techniques to solve the problem that the model tends to copy words directly, resulting in a long and complicated output sentence. Nishihara et al. (2019) proposed a method to simplify the original sentences to different level sentences. Different from most sequenceto-sequence models, Dong et al. (2019) proposed a neural programmer-interpreter approach to predict explicit edit operati"
2020.coling-main.121,D18-1325,0,0.0192554,"analyzes and predicts the sentence deletion in document simplification. In addition to deletion operation, sentence simplification also includes reservation, separation, synonym replacement, and other operations (Xu et al., 2016). Whether the preceding sentences and the following sentences have different effects on the original sentence has not been taken into account. Even in a similar field of machine translation, most of the research focuses on the preceding sentences. Based on the transformer model, Zhang et al. (2018) used a new encoder to represent the context and proposed a new model. Werlen et al. (2018) proposed a hierarchical attention model that captures the context in a structured and dynamic way. Both of the models that have received widespread attention only focuses on the preceding sentences. In addition to the preceding sentences, the following sentences also contain the information of keeping, paraphrasing, and deleting the words in the original sentence in text simplification. To the best of our knowledge, we are the first to study the effects of the preceding sentences and the following sentences and apply them to a sequence-to-sequence model in the field of text simplification. 3"
2020.coling-main.121,P12-1107,0,0.0851227,"Missing"
2020.coling-main.121,Q15-1021,0,0.021479,"atedly in sequence-tosequence model (Tu et al., 2016). The general coverage model is given by: Ci = gupdate (Ci−1 , αi , Φ(h), Ψ) (18) Ci is the coverage vector which summarizes the previous attentions at time step i to help adjust future attention. gupdate updates Ci after new attention αi when decoding at time step i. Φ(h) is a word-specific feature and Ψ are different auxiliary inputs. 2 The code is available at https://github.com/lipiji/TranSummar. 1416 4 Experiments 4.1 Dataset Since the contexts are not provided in commonly used datasets such as Wikismall (Zhu et al., 2010) and Newsela (Xu et al., 2015), we need to build appropriate datasets first. With the help of the Wikipedia datasets (Coster and Kauchak, 2011; Kauchak, 2013), we successfully construct a dataset for our research3 . The Wikipedia datasets contain about 167K aligned sentence pairs. We extract the context sentences of each original sentence from the document-aligned data. For those sentences without the preceding sentences or the following sentences, we choose to retain them to train the sentence-level modules of our model. In the training set, there are around 110K aligned sentence pairs with context information and around"
2020.coling-main.121,Q16-1029,0,0.338676,"l use of context information. SUC outperforms the baselines on both automatic evaluation and human evaluation. (3) We use ablation experiments to illustrate the different effects of the preceding sentences and the following sentences on the current sentences to be simplified. 2 Related works Text simplification has developed rapidly in the past decade. Wubben et al. (2012) proposed the PBMTR model while Narayan and Gardent (2014) put forward the Hybrid model. Both models were based on statistic machine learning. Using a small number of manual simplifications and a large number of paraphrases, Xu et al. (2016) adapted the method of the statistical machine translation. Nisioi et al. (2017) were the first to apply the sequence-to-sequence model to automatic text simplification, using the framework of neural machine translation and making specific improvements. Zhang and Lapata (2017) proposed the DRESS model which rewards the simple, fluent output sentences with proper meaning. Vu et al. (2018) used memory-augmented neural networks to adapt the existing architecture and Guo et al. (2018) used multi-task learning to improve the entailment and paraphrase capabilities. Recently, Kriz et al. (2019) used"
2020.coling-main.121,D17-1062,0,0.0797829,"ified. 2 Related works Text simplification has developed rapidly in the past decade. Wubben et al. (2012) proposed the PBMTR model while Narayan and Gardent (2014) put forward the Hybrid model. Both models were based on statistic machine learning. Using a small number of manual simplifications and a large number of paraphrases, Xu et al. (2016) adapted the method of the statistical machine translation. Nisioi et al. (2017) were the first to apply the sequence-to-sequence model to automatic text simplification, using the framework of neural machine translation and making specific improvements. Zhang and Lapata (2017) proposed the DRESS model which rewards the simple, fluent output sentences with proper meaning. Vu et al. (2018) used memory-augmented neural networks to adapt the existing architecture and Guo et al. (2018) used multi-task learning to improve the entailment and paraphrase capabilities. Recently, Kriz et al. (2019) used two techniques to solve the problem that the model tends to copy words directly, resulting in a long and complicated output sentence. Nishihara et al. (2019) proposed a method to simplify the original sentences to different level sentences. Different from most sequenceto-seque"
2020.coling-main.121,D18-1049,0,0.052826,"is available at https://github.com/RLSNLP/Document-Context-to-SentenceSimplification 1412 analyzes and predicts the sentence deletion in document simplification. In addition to deletion operation, sentence simplification also includes reservation, separation, synonym replacement, and other operations (Xu et al., 2016). Whether the preceding sentences and the following sentences have different effects on the original sentence has not been taken into account. Even in a similar field of machine translation, most of the research focuses on the preceding sentences. Based on the transformer model, Zhang et al. (2018) used a new encoder to represent the context and proposed a new model. Werlen et al. (2018) proposed a hierarchical attention model that captures the context in a structured and dynamic way. Both of the models that have received widespread attention only focuses on the preceding sentences. In addition to the preceding sentences, the following sentences also contain the information of keeping, paraphrasing, and deleting the words in the original sentence in text simplification. To the best of our knowledge, we are the first to study the effects of the preceding sentences and the following sente"
2020.coling-main.121,D18-1355,0,0.0163614,"ext into a more readable version with the same meaning (Sulem et al., 2018b), which will benefit young children (Kajiwara et al., 2013) and non-native English speakers (Paetzold, 2015; Paetzold and Specia, 2016). It includes many ways to deal with the input text, such as deletion, reordering, paraphrase, and sentence separation (Saggion, 2017). Besides, text simplification is closely related to many natural ˇ language processing (NLP) tasks, such as machine translation (Stajner and Popovi´c, 2016; Hasler et al., 2017), paraphrase generation (Pavlick and Callison-Burch, 2016; Cao et al., 2017; Zhao et al., 2018) and text summarization (Li et al., 2017; Ma and Sun, 2017; Jin et al., 2020). In recent years, many researchers have proposed various models to improve the performance of text simplification. However, few people have explored the impact of document context (i.e., the preceding and following sentences of the original sentence to be simplified in a document) on text simplification, let alone establish a large-scale dataset containing context. Many examples like the one in Table 1 arouse our interest in exploring the influence of context. In the simplified sentence, “played and sang” is the simp"
2020.coling-main.121,C10-1152,0,0.0500667,"problem of generating text repeatedly in sequence-tosequence model (Tu et al., 2016). The general coverage model is given by: Ci = gupdate (Ci−1 , αi , Φ(h), Ψ) (18) Ci is the coverage vector which summarizes the previous attentions at time step i to help adjust future attention. gupdate updates Ci after new attention αi when decoding at time step i. Φ(h) is a word-specific feature and Ψ are different auxiliary inputs. 2 The code is available at https://github.com/lipiji/TranSummar. 1416 4 Experiments 4.1 Dataset Since the contexts are not provided in commonly used datasets such as Wikismall (Zhu et al., 2010) and Newsela (Xu et al., 2015), we need to build appropriate datasets first. With the help of the Wikipedia datasets (Coster and Kauchak, 2011; Kauchak, 2013), we successfully construct a dataset for our research3 . The Wikipedia datasets contain about 167K aligned sentence pairs. We extract the context sentences of each original sentence from the document-aligned data. For those sentences without the preceding sentences or the following sentences, we choose to retain them to train the sentence-level modules of our model. In the training set, there are around 110K aligned sentence pairs with c"
2020.coling-main.200,P17-1074,0,0.0723204,"(hx ) J (hx ) = − L X log P (xt |˜ x&lt;t , hx ) (3) (4) t=1 We choose the powerful Transformer(Vaswani et al., 2017) as encoder and decoder. Both the encoder and the decoder consist of Transformer blocks with multi-head self-attention layer followed by feedforward layer. As for the classifier, it has several feed-forward layers as the classification layers. The classifier will determine whether the sentence is correct. If not, it will predict the specific type of grammatical errors. We define six types of grammatical errors based on the 25 main types defined in automatic annotation tool ERRANT(Bryant et al., 2017): ADJ/ADV(ADJ, ADJ:FORM and ADV), DET, PREP, NOUN(NOUN, NOUN:INFL, NOUN:NUM and NOUN:POSS), VERB(VERB, VERB:FORM, VERB:INFL, VERB:SVA and VERB:TENSE), OTHER(Errors that do not fall into above categories). These error types are common in human writing and more difficult to be corrected by GEC model. 3.1.2 Generating Synthetic Training Samples We want to add a perturbation vector r to the latent representation hx of input sentence x, and use decoder φD to generate additional training samples from hx + r. The algorithm for generating synthetic samples is summarized in Algorithm 1. Given a correct"
2020.coling-main.200,W19-4406,0,0.23407,"e, and use a decoder to generate a sentence with target grammatical error type. In this way, diverse errors can be generated by assigning different target error types. To further improve the performance, we adopt some rules to assist the generation of some local grammatical errors, such as spelling errors, wrong punctuation, etc. We apply this data augmentation method to the existing GEC model Copy-transformer (Zhao et al., 2019) to evaluate the results. Experiments are conducted on the following widely used benchmarks: CoNLL-2014 (Ng et al., 2014), FCE (Yannakoudakis et al., 2011), BEA-2019 (Bryant et al., 2019). Experimental results show the efficacy of our proposed method which outperforms several existing models. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2202 Proceedings of the 28th International Conference on Computational Linguistics, pages 2202–2212 Barcelona, Spain (Online), December 8-13, 2020 Our contributions are summarized as follows: 1. We propose a new data augmentation method to generate synthetic samples by editing latent representations of grammatical sentences, which is able to"
2020.coling-main.200,W19-4423,0,0.780259,"synthetic sample x0 with its original sentence x, we use similarity discriminator ψ to get a score p ∈ [0, 1] which reflects the degree of semantic similarity between x and x0 . We set a threshold t that if p is greater than this threshold, x0 can be selected as the augmented sample. Our method can generate more natural sentences compared to methods that directly apply noise to tokens. Since we edit latent representations of sentences, we can get more diverse samples with different errors which can not be obtained by only applying noise to tokens. 3.2 Pre-defined Rules Based on previous works(Choe et al., 2019; Lichtarge et al., 2018), the rule-based method can generate local grammatical errors with high quality. We propose five rules to assist in generating synthetic training data. 2205 Delete. Randomly delete a token with a probability of 0.15. Add. Firstly, randomly select a word from a word list (Google-10000-English1 ), and then add the selected word to random position with a probability of 0.15. Replace. Randomly replace a token with its possible forms with a probability of 0.5. If the picked token is a word, we use Word forms2 to generate all possible forms (adverb, adjective, noun and verb)"
2020.coling-main.200,W17-5037,0,0.0133417,"on these sentence pairs. Finally, we fine-tune the model on respective entire training dataset corresponding to each test set. 5.4 Post-processing To further improve the performance, we incorporate the following techniques that are widely used in GEC task: Features Re-scoring (FR). Following Chollampatt and Ng(2018), we use edit operation (EO) features and language model (LM) features to re-score the final beam candidates. EO features denote three features about token-level edit operation. LM features include the score of a language model which is trained on the web-scale Common Crawl corpus (Chollampatt and Ng, 2017; Junczys-Dowmunt and Grundkiewicz, 2016), and the length of the output sequences. Right-to-left Re-ranking (R2L). Following Sennrich et al. (2016a; 2017), we use the right-to-left re-ranking method to build the ensemble of independently trained models. We pass n-best candidates generated from four left-to-right models to four right-to-left models, and re-rank the n-best candidates based on their corresponding scores. 3 https://github.com/pyenchant/pyenchant 2207 6 6.1 Results and Analysis Compared with Existing Methods We evaluate the performance of our method on public benchmarks and compare"
2020.coling-main.200,N12-1067,0,0.208928,"9). NUCLE is a collection of essays written by students who are non-native English speakers. Professional English instructors were invited to correct the grammatical errors in these essays. There are 28 common grammatical error types. The Lang-8 corpus is a cleaned English subset of the language learning websites. FCE and W&I+LOCNESS are public GEC datasets. Bryant et al.(2019) use an automatic annotation tool ERRANT to annotate the types of grammatical errors. There are 25 main grammatical error types. Evaluation data. We report results on CoNLL-2014 benchmark evaluated by official M2 scorer(Dahlmeier and Ng, 2012), and on BEA-2019 and FCE benchmarks evaluated by ERRANT. Seed Corpus Following the Kiyono et al.(2019), we choose the large English corpus Gigaword as seed corpus for data augmentation. The datasets used in the experiments are summarized in Table 1. 5.2 Pre-processing We first tokenize the data by NLTK (Bird et al., 2009). Then we apply byte-pair encoding (BPE) (Sennrich et al., 2016b) to sentences using subword-nmt(Sennrich et al., 2016b) before feeding the texts into 1 2 https://github.com/first20hours/google-10000-english https://github.com/gutfeeling/word forms 2206 Dataset Gigaword NUCLE"
2020.coling-main.200,W13-1703,0,0.0675883,"choose copy-augmented Transformer (Zhao et al., 2019) as GEC model to test our data augmentation method. Copy-augmented Transformer is a kind of Transformer that incorporates an attention-based copy mechanism in the decoder. It can generate word from a fixed vocabulary and the source input tokens. Considering the similarity between input and output, this copy mechanism leads to great performance of models in GEC task. 5 Experiment Setup 5.1 Datasets Training data. We use the following GEC datasets as original training corpus: National University of Singapore Corpus of Learner English (NUCLE)(Dahlmeier et al., 2013), Lang-8 Corpus of Learner English (Lang-8)(Tajiri et al., 2012), FCE dataset(Yannakoudakis et al., 2011), and Write & Improve + LOCNESS Corpus(W&I+LOCNESS)(Bryant et al., 2019). NUCLE is a collection of essays written by students who are non-native English speakers. Professional English instructors were invited to correct the grammatical errors in these essays. There are 28 common grammatical error types. The Lang-8 corpus is a cleaned English subset of the language learning websites. FCE and W&I+LOCNESS are public GEC datasets. Bryant et al.(2019) use an automatic annotation tool ERRANT to a"
2020.coling-main.200,N19-1423,0,0.0171933,"ct grammatical errors. Yuan and Briscoe (2016) used a classical bidirectional recurrent neural network (Graves et al., 2013) with attention. Chollampatt and Ng (2018) proposed a convolution neural network (Kim, 2014) to capture the local context. Many recent works (Junczys-Dowmunt et al., 2018) made use of the powerful machine translation architecture Transformer (Vaswani et al., 2017). Zhao et al. (2019) further applied copying mechanism(Gu et al., 2016; Jia and Liang, 2016) to Transformer. Considering the tremendous performance of pre-trained methods, pretrained language model, such as BERT(Devlin et al., 2019), can be incorporated into the encoder-decoder model (Kaneko et al., 2020). An encoder-decoder GEC model requires a large amount of training data, and the available training corpora usually failed to train a good GEC model. To address this problem, many data augmentation methods have been proposed (Ge et al., 2018). Many works adopted pre-defined rules to generate local grammatical errors. Grundkiewicz et al. (2019) applied a confusion set built by spellchecker to the corpus. Choe et al.(2019) extracted some common text editing operations from human writing habits and got synthetic samples by"
2020.coling-main.200,W19-4427,0,0.352579,"her applied copying mechanism(Gu et al., 2016; Jia and Liang, 2016) to Transformer. Considering the tremendous performance of pre-trained methods, pretrained language model, such as BERT(Devlin et al., 2019), can be incorporated into the encoder-decoder model (Kaneko et al., 2020). An encoder-decoder GEC model requires a large amount of training data, and the available training corpora usually failed to train a good GEC model. To address this problem, many data augmentation methods have been proposed (Ge et al., 2018). Many works adopted pre-defined rules to generate local grammatical errors. Grundkiewicz et al. (2019) applied a confusion set built by spellchecker to the corpus. Choe et al.(2019) extracted some common text editing operations from human writing habits and got synthetic samples by these extracted operations. Lichtarge et al.(Lichtarge et al., 2018) made use of models trained on large amounts of weakly supervised text. Inspired by back-translation procedure for machine translation (Sennrich et al., 2015) , Xie et al.(2018) proposed a model that can learn to generate erroneous sentences from correct ones. Based on this work, Kiyono et al. (2019) further studied the data augmentation methods and"
2020.coling-main.200,P16-1154,0,0.032521,"ral network models to deal with GEC task. Some treated the GEC task as a translation problem and applied neural machine translation model to detect and correct grammatical errors. Yuan and Briscoe (2016) used a classical bidirectional recurrent neural network (Graves et al., 2013) with attention. Chollampatt and Ng (2018) proposed a convolution neural network (Kim, 2014) to capture the local context. Many recent works (Junczys-Dowmunt et al., 2018) made use of the powerful machine translation architecture Transformer (Vaswani et al., 2017). Zhao et al. (2019) further applied copying mechanism(Gu et al., 2016; Jia and Liang, 2016) to Transformer. Considering the tremendous performance of pre-trained methods, pretrained language model, such as BERT(Devlin et al., 2019), can be incorporated into the encoder-decoder model (Kaneko et al., 2020). An encoder-decoder GEC model requires a large amount of training data, and the available training corpora usually failed to train a good GEC model. To address this problem, many data augmentation methods have been proposed (Ge et al., 2018). Many works adopted pre-defined rules to generate local grammatical errors. Grundkiewicz et al. (2019) applied a confusio"
2020.coling-main.200,P16-1002,0,0.0256898,"s to deal with GEC task. Some treated the GEC task as a translation problem and applied neural machine translation model to detect and correct grammatical errors. Yuan and Briscoe (2016) used a classical bidirectional recurrent neural network (Graves et al., 2013) with attention. Chollampatt and Ng (2018) proposed a convolution neural network (Kim, 2014) to capture the local context. Many recent works (Junczys-Dowmunt et al., 2018) made use of the powerful machine translation architecture Transformer (Vaswani et al., 2017). Zhao et al. (2019) further applied copying mechanism(Gu et al., 2016; Jia and Liang, 2016) to Transformer. Considering the tremendous performance of pre-trained methods, pretrained language model, such as BERT(Devlin et al., 2019), can be incorporated into the encoder-decoder model (Kaneko et al., 2020). An encoder-decoder GEC model requires a large amount of training data, and the available training corpora usually failed to train a good GEC model. To address this problem, many data augmentation methods have been proposed (Ge et al., 2018). Many works adopted pre-defined rules to generate local grammatical errors. Grundkiewicz et al. (2019) applied a confusion set built by spellch"
2020.coling-main.200,D16-1161,0,0.0192672,"inally, we fine-tune the model on respective entire training dataset corresponding to each test set. 5.4 Post-processing To further improve the performance, we incorporate the following techniques that are widely used in GEC task: Features Re-scoring (FR). Following Chollampatt and Ng(2018), we use edit operation (EO) features and language model (LM) features to re-score the final beam candidates. EO features denote three features about token-level edit operation. LM features include the score of a language model which is trained on the web-scale Common Crawl corpus (Chollampatt and Ng, 2017; Junczys-Dowmunt and Grundkiewicz, 2016), and the length of the output sequences. Right-to-left Re-ranking (R2L). Following Sennrich et al. (2016a; 2017), we use the right-to-left re-ranking method to build the ensemble of independently trained models. We pass n-best candidates generated from four left-to-right models to four right-to-left models, and re-rank the n-best candidates based on their corresponding scores. 3 https://github.com/pyenchant/pyenchant 2207 6 6.1 Results and Analysis Compared with Existing Methods We evaluate the performance of our method on public benchmarks and compare the scores with the current top models w"
2020.coling-main.200,N18-1055,0,0.0793694,"istical machine learning method(Knight and Chander, 1994; Minnen et al., 2000; Izumi et al., 2003). With the development of deep learning, recent works proposed a variety of neural network models to deal with GEC task. Some treated the GEC task as a translation problem and applied neural machine translation model to detect and correct grammatical errors. Yuan and Briscoe (2016) used a classical bidirectional recurrent neural network (Graves et al., 2013) with attention. Chollampatt and Ng (2018) proposed a convolution neural network (Kim, 2014) to capture the local context. Many recent works (Junczys-Dowmunt et al., 2018) made use of the powerful machine translation architecture Transformer (Vaswani et al., 2017). Zhao et al. (2019) further applied copying mechanism(Gu et al., 2016; Jia and Liang, 2016) to Transformer. Considering the tremendous performance of pre-trained methods, pretrained language model, such as BERT(Devlin et al., 2019), can be incorporated into the encoder-decoder model (Kaneko et al., 2020). An encoder-decoder GEC model requires a large amount of training data, and the available training corpora usually failed to train a good GEC model. To address this problem, many data augmentation met"
2020.coling-main.200,2020.acl-main.391,0,0.354895,"nal recurrent neural network (Graves et al., 2013) with attention. Chollampatt and Ng (2018) proposed a convolution neural network (Kim, 2014) to capture the local context. Many recent works (Junczys-Dowmunt et al., 2018) made use of the powerful machine translation architecture Transformer (Vaswani et al., 2017). Zhao et al. (2019) further applied copying mechanism(Gu et al., 2016; Jia and Liang, 2016) to Transformer. Considering the tremendous performance of pre-trained methods, pretrained language model, such as BERT(Devlin et al., 2019), can be incorporated into the encoder-decoder model (Kaneko et al., 2020). An encoder-decoder GEC model requires a large amount of training data, and the available training corpora usually failed to train a good GEC model. To address this problem, many data augmentation methods have been proposed (Ge et al., 2018). Many works adopted pre-defined rules to generate local grammatical errors. Grundkiewicz et al. (2019) applied a confusion set built by spellchecker to the corpus. Choe et al.(2019) extracted some common text editing operations from human writing habits and got synthetic samples by these extracted operations. Lichtarge et al.(Lichtarge et al., 2018) made"
2020.coling-main.200,D14-1181,0,0.0107789,"number of language learners of English, there has been increasing attention to the English GEC in the past few years. Considering the outstanding performance of neural network models in machine translation tasks, many studies have tackled GEC as a machine translation task. They regard ungrammatical sentences as the source language and grammatical sentences as the target language. This approach allows cutting-edge neural machine translation models to be applied to GEC. Many encoder-decoder models, such as recurrent neural network (RNN)(Graves et al., 2013), convolutional neural networks (CNN)(Kim, 2014), have been widely applied to GEC. A challenge in applying neural machine translation models to GEC is the requirement of a large amount of training data, i.e., the source-target pairs. To address this problem, many data augmentation methods have been proposed. Existing methods, however, are often only able to generate sentences with limited error types, and can only improve the performance of GEC model on these few error types, while it is still hard for the model to correct the sentences with other types of errors. To address the above problem, we propose a new data augmentation method to ge"
2020.coling-main.200,D19-1119,0,0.12236,"rules to generate local grammatical errors. Grundkiewicz et al. (2019) applied a confusion set built by spellchecker to the corpus. Choe et al.(2019) extracted some common text editing operations from human writing habits and got synthetic samples by these extracted operations. Lichtarge et al.(Lichtarge et al., 2018) made use of models trained on large amounts of weakly supervised text. Inspired by back-translation procedure for machine translation (Sennrich et al., 2015) , Xie et al.(2018) proposed a model that can learn to generate erroneous sentences from correct ones. Based on this work, Kiyono et al. (2019) further studied the data augmentation methods and got some empirical conclusions. 3 Our Data Augmentation Method Our data augmentation method applies noise to the latent space and it can generate sentences with various error types by editing latent representations of grammatical sentences. To further improve the performance, we adopt some rules to assist the generation of some local grammatical errors. Synthetic training samples generated from our method are used to train neural GEC model and enable the model to detect and correct most error types and improve its performance and robustness. 3"
2020.coling-main.200,W00-0708,0,0.0726037,"ethod achieves the state-of-the-art performance on CoNLL-2014 and FCE benchmarks. It outperforms not only all previous single models but also all ensemble models. On BEA-2019 benchmark, our method achieves very competitive performance as well. 2 Related Work Early GEC models are mainly based on manually designed grammar rules(Murata and Nagao, 1994; Bond et al., 1996; Siegel, 1996). Han et al. (2006) pointed out the limitation of rule-based method and proposed a statistical model. Later, some researchers proposed solutions based on statistical machine learning method(Knight and Chander, 1994; Minnen et al., 2000; Izumi et al., 2003). With the development of deep learning, recent works proposed a variety of neural network models to deal with GEC task. Some treated the GEC task as a translation problem and applied neural machine translation model to detect and correct grammatical errors. Yuan and Briscoe (2016) used a classical bidirectional recurrent neural network (Graves et al., 2013) with attention. Chollampatt and Ng (2018) proposed a convolution neural network (Kim, 2014) to capture the local context. Many recent works (Junczys-Dowmunt et al., 2018) made use of the powerful machine translation ar"
2020.coling-main.200,W14-1701,0,0.0523882,"turbation vector to the latent representation of input sentence, and use a decoder to generate a sentence with target grammatical error type. In this way, diverse errors can be generated by assigning different target error types. To further improve the performance, we adopt some rules to assist the generation of some local grammatical errors, such as spelling errors, wrong punctuation, etc. We apply this data augmentation method to the existing GEC model Copy-transformer (Zhao et al., 2019) to evaluate the results. Experiments are conducted on the following widely used benchmarks: CoNLL-2014 (Ng et al., 2014), FCE (Yannakoudakis et al., 2011), BEA-2019 (Bryant et al., 2019). Experimental results show the efficacy of our proposed method which outperforms several existing models. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2202 Proceedings of the 28th International Conference on Computational Linguistics, pages 2202–2212 Barcelona, Spain (Online), December 8-13, 2020 Our contributions are summarized as follows: 1. We propose a new data augmentation method to generate synthetic samples by editing"
2020.coling-main.200,2020.bea-1.16,0,0.194961,"Missing"
2020.coling-main.200,N19-4009,0,0.0249265,"arallel’ means if the dataset has parallel GEC pairs. ’Annotated’ means if the types of grammatical errors are annotated. models. It allows us to avoid &lt; unk &gt; tokens in most datasets. Following Choe et al. (2019), we use spellcheck Enchant3 to assist the correction of spelling errors. Besides, we use ERRANT to annotate the types of grammatical errors in Lang-8 dataset. In this way, we can get a large amount of annotated data for the training of the grammatical error type classifier. 5.3 Model Training Details In this paper, we use the Transformer implementation in the public Fairseq Toolkit (Ott et al., 2019). For the Transformer model, the hidden size of embedding is 512. The encoder and decoder have 6 layers and 8 attention heads. For the inner layer in the feed-forward network, the size of is 4096. The number of feed-forward layers in classifier is 3. The classifier model is trained using the Adam optimization method (Kingma and Ba, 2015). The learning rate is initially set as 0.001 ,the decay factor is set as 0.99 for every epoch. To avoid overfitting, we adopt dropout mechanism (Srivastava et al., 2014). The dropout rate is 0.1. During decoding, models are optimized with Nesterovs Accelerated"
2020.coling-main.200,D16-1244,0,0.0942872,"Missing"
2020.coling-main.200,W16-2323,0,0.102834,"utomatic annotation tool ERRANT to annotate the types of grammatical errors. There are 25 main grammatical error types. Evaluation data. We report results on CoNLL-2014 benchmark evaluated by official M2 scorer(Dahlmeier and Ng, 2012), and on BEA-2019 and FCE benchmarks evaluated by ERRANT. Seed Corpus Following the Kiyono et al.(2019), we choose the large English corpus Gigaword as seed corpus for data augmentation. The datasets used in the experiments are summarized in Table 1. 5.2 Pre-processing We first tokenize the data by NLTK (Bird et al., 2009). Then we apply byte-pair encoding (BPE) (Sennrich et al., 2016b) to sentences using subword-nmt(Sennrich et al., 2016b) before feeding the texts into 1 2 https://github.com/first20hours/google-10000-english https://github.com/gutfeeling/word forms 2206 Dataset Gigaword NUCLE Lang-8 FCE W&I+LOCNESS #Sentences 131.8M 57.2K 1.04M 33.2K 34.3K Parallel no yes yes yes yes Annotated yes no yes yes Table 1: Summary of datasets. ’Parallel’ means if the dataset has parallel GEC pairs. ’Annotated’ means if the types of grammatical errors are annotated. models. It allows us to avoid &lt; unk &gt; tokens in most datasets. Following Choe et al. (2019), we use spellcheck Enc"
2020.coling-main.200,P16-1162,0,0.314813,"utomatic annotation tool ERRANT to annotate the types of grammatical errors. There are 25 main grammatical error types. Evaluation data. We report results on CoNLL-2014 benchmark evaluated by official M2 scorer(Dahlmeier and Ng, 2012), and on BEA-2019 and FCE benchmarks evaluated by ERRANT. Seed Corpus Following the Kiyono et al.(2019), we choose the large English corpus Gigaword as seed corpus for data augmentation. The datasets used in the experiments are summarized in Table 1. 5.2 Pre-processing We first tokenize the data by NLTK (Bird et al., 2009). Then we apply byte-pair encoding (BPE) (Sennrich et al., 2016b) to sentences using subword-nmt(Sennrich et al., 2016b) before feeding the texts into 1 2 https://github.com/first20hours/google-10000-english https://github.com/gutfeeling/word forms 2206 Dataset Gigaword NUCLE Lang-8 FCE W&I+LOCNESS #Sentences 131.8M 57.2K 1.04M 33.2K 34.3K Parallel no yes yes yes yes Annotated yes no yes yes Table 1: Summary of datasets. ’Parallel’ means if the dataset has parallel GEC pairs. ’Annotated’ means if the types of grammatical errors are annotated. models. It allows us to avoid &lt; unk &gt; tokens in most datasets. Following Choe et al. (2019), we use spellcheck Enc"
2020.coling-main.200,W17-4739,0,0.0446205,"Missing"
2020.coling-main.200,Y96-1005,0,0.0337478,"enerate errors with high quality and diversity. 2. Additional synthetic training samples enable training neural GEC model to detect and correct most error types, and improving the performance and robustness of the model. 3. Our method achieves the state-of-the-art performance on CoNLL-2014 and FCE benchmarks. It outperforms not only all previous single models but also all ensemble models. On BEA-2019 benchmark, our method achieves very competitive performance as well. 2 Related Work Early GEC models are mainly based on manually designed grammar rules(Murata and Nagao, 1994; Bond et al., 1996; Siegel, 1996). Han et al. (2006) pointed out the limitation of rule-based method and proposed a statistical model. Later, some researchers proposed solutions based on statistical machine learning method(Knight and Chander, 1994; Minnen et al., 2000; Izumi et al., 2003). With the development of deep learning, recent works proposed a variety of neural network models to deal with GEC task. Some treated the GEC task as a translation problem and applied neural machine translation model to detect and correct grammatical errors. Yuan and Briscoe (2016) used a classical bidirectional recurrent neural network (Grav"
2020.coling-main.200,P12-2039,0,0.0347932,"to test our data augmentation method. Copy-augmented Transformer is a kind of Transformer that incorporates an attention-based copy mechanism in the decoder. It can generate word from a fixed vocabulary and the source input tokens. Considering the similarity between input and output, this copy mechanism leads to great performance of models in GEC task. 5 Experiment Setup 5.1 Datasets Training data. We use the following GEC datasets as original training corpus: National University of Singapore Corpus of Learner English (NUCLE)(Dahlmeier et al., 2013), Lang-8 Corpus of Learner English (Lang-8)(Tajiri et al., 2012), FCE dataset(Yannakoudakis et al., 2011), and Write & Improve + LOCNESS Corpus(W&I+LOCNESS)(Bryant et al., 2019). NUCLE is a collection of essays written by students who are non-native English speakers. Professional English instructors were invited to correct the grammatical errors in these essays. There are 28 common grammatical error types. The Lang-8 corpus is a cleaned English subset of the language learning websites. FCE and W&I+LOCNESS are public GEC datasets. Bryant et al.(2019) use an automatic annotation tool ERRANT to annotate the types of grammatical errors. There are 25 main gramm"
2020.coling-main.200,N18-1057,0,0.1535,"Missing"
2020.coling-main.200,P11-1019,0,0.520656,"latent representation of input sentence, and use a decoder to generate a sentence with target grammatical error type. In this way, diverse errors can be generated by assigning different target error types. To further improve the performance, we adopt some rules to assist the generation of some local grammatical errors, such as spelling errors, wrong punctuation, etc. We apply this data augmentation method to the existing GEC model Copy-transformer (Zhao et al., 2019) to evaluate the results. Experiments are conducted on the following widely used benchmarks: CoNLL-2014 (Ng et al., 2014), FCE (Yannakoudakis et al., 2011), BEA-2019 (Bryant et al., 2019). Experimental results show the efficacy of our proposed method which outperforms several existing models. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2202 Proceedings of the 28th International Conference on Computational Linguistics, pages 2202–2212 Barcelona, Spain (Online), December 8-13, 2020 Our contributions are summarized as follows: 1. We propose a new data augmentation method to generate synthetic samples by editing latent representations of grammati"
2020.coling-main.200,N16-1042,0,0.0992749,"ually designed grammar rules(Murata and Nagao, 1994; Bond et al., 1996; Siegel, 1996). Han et al. (2006) pointed out the limitation of rule-based method and proposed a statistical model. Later, some researchers proposed solutions based on statistical machine learning method(Knight and Chander, 1994; Minnen et al., 2000; Izumi et al., 2003). With the development of deep learning, recent works proposed a variety of neural network models to deal with GEC task. Some treated the GEC task as a translation problem and applied neural machine translation model to detect and correct grammatical errors. Yuan and Briscoe (2016) used a classical bidirectional recurrent neural network (Graves et al., 2013) with attention. Chollampatt and Ng (2018) proposed a convolution neural network (Kim, 2014) to capture the local context. Many recent works (Junczys-Dowmunt et al., 2018) made use of the powerful machine translation architecture Transformer (Vaswani et al., 2017). Zhao et al. (2019) further applied copying mechanism(Gu et al., 2016; Jia and Liang, 2016) to Transformer. Considering the tremendous performance of pre-trained methods, pretrained language model, such as BERT(Devlin et al., 2019), can be incorporated into"
2020.coling-main.200,N19-1014,0,0.468783,"the corresponding grammatical error type classifier, we can get a perturbation vector in latent space. Then we add the perturbation vector to the latent representation of input sentence, and use a decoder to generate a sentence with target grammatical error type. In this way, diverse errors can be generated by assigning different target error types. To further improve the performance, we adopt some rules to assist the generation of some local grammatical errors, such as spelling errors, wrong punctuation, etc. We apply this data augmentation method to the existing GEC model Copy-transformer (Zhao et al., 2019) to evaluate the results. Experiments are conducted on the following widely used benchmarks: CoNLL-2014 (Ng et al., 2014), FCE (Yannakoudakis et al., 2011), BEA-2019 (Bryant et al., 2019). Experimental results show the efficacy of our proposed method which outperforms several existing models. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 2202 Proceedings of the 28th International Conference on Computational Linguistics, pages 2202–2212 Barcelona, Spain (Online), December 8-13, 2020 Our contri"
2020.emnlp-main.229,J90-1003,0,0.523416,"traction Incongruity is a leading theory in computational humor. We achieve it by rewriting the sentences with lexical constraints. For each pair of homophones, we obtain a set of candidate sentences C. For the i-th candidate sentence, we extract words weakly related to the alternative word to compose the weak word vocabulary Wi . The words in the whole corpus which have the same POS tag with each word w in Wi and are strongly related to the pun word compose the support word vocabulary Si,w . We use Point-wise Mutual Information (PMI) to evaluate the relatedness between two words (e.g. x, y) (Church and Hanks, 1990) : P M I(x, y) = log2 p(x, y) . p(x) · p(y) (1) To make the pun words more plausible, we hope to replace the weak word with a support word. Keep ||Wi ||= nw words with lowest PMI scores in Wi and ||Si,w ||= ns words with highest PMI scores in Si,w . For each sentence, there are nw ∗ ns pairs of possible constraints. To keep the edited sentences grammatical, we need to select one pair which results in the most reasonable modifications. As Algorithm 1 shows, for each homophone pair, Algorithm 1 Constraint Selection Algorithm. Require: C: a set of candidate sentences containing the alternative wo"
2020.emnlp-main.229,S17-2011,0,0.0148411,"eplacing verbs can result in the transformations of other words, which changes the candidate sentence to a great extent. keep the top-k hypotheses. In this way, both positive constraints and negative constraints can be satisfied without repeating. We call our model Lexically Constrained Rewriting Model (LCR). 3 Experiments 3.1 Data Set We use PARABANK (Hu et al., 2019b), a largescale English paraphrase dataset to train the rewriting model. Following previous work (He et al., 2019), we use BookCorpus (Zhu et al., 2015) as retrieval corpus. And we use the homophone pairs in 2017 SemEval task 7 (Doogan et al., 2017) for testing. 3.2 Experimental Setting We train a CBOW model on BookCorpus with a context window (width = 5) to learn 300dimensional word vectors. In our constraint selection algorithm, we keep w = 3 weak words for each sentence and s = 5 support words for each weak word. In the decoding phrase, we set the beam size k = 5. 3.3 Baseline Models RE: Retrieve a sentence containing the pun word. RE+S: Sample one sentence containing the alternative word and replace it with the corresponding pun word. NJD (Yu et al., 2018): Retrained the model on the BookCorpus. Use two homophones as the inputs for d"
2020.emnlp-main.229,N19-1172,0,0.0143829,"ence scores. We 2 IN vectors are input vectors of a trained CBOW model. OUT vectors are output vectors of a trained CBOW model. 3 Replacing verbs can result in the transformations of other words, which changes the candidate sentence to a great extent. keep the top-k hypotheses. In this way, both positive constraints and negative constraints can be satisfied without repeating. We call our model Lexically Constrained Rewriting Model (LCR). 3 Experiments 3.1 Data Set We use PARABANK (Hu et al., 2019b), a largescale English paraphrase dataset to train the rewriting model. Following previous work (He et al., 2019), we use BookCorpus (Zhu et al., 2015) as retrieval corpus. And we use the homophone pairs in 2017 SemEval task 7 (Doogan et al., 2017) for testing. 3.2 Experimental Setting We train a CBOW model on BookCorpus with a context window (width = 5) to learn 300dimensional word vectors. In our constraint selection algorithm, we keep w = 3 weak words for each sentence and s = 5 support words for each weak word. In the decoding phrase, we set the beam size k = 5. 3.3 Baseline Models RE: Retrieve a sentence containing the pun word. RE+S: Sample one sentence containing the alternative word and replace i"
2020.emnlp-main.229,P17-1141,0,0.0159201,"ng With the extracted constraints, we further generate homophonic puns by rewriting the candidate sentence. Following Hu et al. (2019a), we train a generator in an end-to-end way. Given a source sentence x, the generator aims to rewrite it as y˜. In the end-to-end model, y˜ maximizes the conditional probability given by a model θ and an input sequence x: y˜ = argmaxy pθ (y|x), y ∈ Y (2) where Y is the space of possible outputs. Traditional beam search can not guarantee the outputs conformed to the constraints. Researchers propose algorithms to place constraints in natural and meaningful ways (Hokamp and Liu, 2017; Post and Vilar, 2018). However, previous works ignore the situation that some constraints may share a prefix. To avoid repeating, the constraints that have not been generated are organized into a trie. For positive constraints, there is a counter to indicate that how many times the constraint must be generated. When a constraint is generated, its counter is decremented. For negative constraints, the trie does not need any counters. At each time step, the generation of an active phrase is blocked by setting the costs of all word IDs marked in the current node to infinity. Hypotheses are ranke"
2020.emnlp-main.229,W09-2004,0,0.0973919,"Missing"
2020.emnlp-main.229,N19-1090,0,0.0476532,"Missing"
2020.emnlp-main.229,P18-1113,0,0.0422464,"Missing"
2020.emnlp-main.229,S17-2005,0,0.0413353,"Missing"
2020.emnlp-main.229,N18-1119,0,0.0120402,"constraints, we further generate homophonic puns by rewriting the candidate sentence. Following Hu et al. (2019a), we train a generator in an end-to-end way. Given a source sentence x, the generator aims to rewrite it as y˜. In the end-to-end model, y˜ maximizes the conditional probability given by a model θ and an input sequence x: y˜ = argmaxy pθ (y|x), y ∈ Y (2) where Y is the space of possible outputs. Traditional beam search can not guarantee the outputs conformed to the constraints. Researchers propose algorithms to place constraints in natural and meaningful ways (Hokamp and Liu, 2017; Post and Vilar, 2018). However, previous works ignore the situation that some constraints may share a prefix. To avoid repeating, the constraints that have not been generated are organized into a trie. For positive constraints, there is a counter to indicate that how many times the constraint must be generated. When a constraint is generated, its counter is decremented. For negative constraints, the trie does not need any counters. At each time step, the generation of an active phrase is blocked by setting the costs of all word IDs marked in the current node to infinity. Hypotheses are ranked by sentence number, n"
2020.emnlp-main.229,P18-1153,1,0.915163,"Missing"
2020.emnlp-main.311,H05-1042,0,0.0776894,"preference records into consideration. They generate personalized recipes from incomplete input specifications (name and incomplete ingredient details). Different from the existing methods on recipe generation which focus on covering all of the given ingredients, we extend the previous generation task with a selection of given items. Selective generation is a task to produce the natural language description for a salient subset of a rich records (Mei et al., 2016). A lot of attention has been paid to individual content selection and selective realization sub-problems (Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Liang et al., 2009). Recent works (Chen and Mooney, 2008; Chen et al., 2010; Mei et al., 2016) explore full selective generation and learn alignments between generated texts and input data using a translation model. We find appropriate set of ingredients by selective routing algorithm. And our model can generate texts according to the user constraints on both ingredients and categories. The core inspiration for our routing module comes from following works. Hinton et al. (2011) propose transformation matrices that learn to encode the intrinsic spatial relationship between a part and a whole"
2020.emnlp-main.311,N04-1015,0,0.116914,"take the historical user preference records into consideration. They generate personalized recipes from incomplete input specifications (name and incomplete ingredient details). Different from the existing methods on recipe generation which focus on covering all of the given ingredients, we extend the previous generation task with a selection of given items. Selective generation is a task to produce the natural language description for a salient subset of a rich records (Mei et al., 2016). A lot of attention has been paid to individual content selection and selective realization sub-problems (Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Liang et al., 2009). Recent works (Chen and Mooney, 2008; Chen et al., 2010; Mei et al., 2016) explore full selective generation and learn alignments between generated texts and input data using a translation model. We find appropriate set of ingredients by selective routing algorithm. And our model can generate texts according to the user constraints on both ingredients and categories. The core inspiration for our routing module comes from following works. Hinton et al. (2011) propose transformation matrices that learn to encode the intrinsic spatial relationship"
2020.emnlp-main.311,D15-1090,0,0.0142505,"M-for-RecipeGeneration sideration in recipe generation process. We propose a novel algorithm to calculate the ingredient collocation weights to enforce recipe generation model. • Given ingredients with noises, our model can satisfy personalized user demands by taking ingredients and category constraints into consideration. • Our approach yields significant improvements on both automatic and human evaluation. 2 Related Work Recipes have been gaining interest in recent researches, including recipe processing (Mori et al., 2012, 2014; Bosselut et al., 2017), recipe parsing (Malmaud et al., 2014; Jermsurawong and Habash, 2015), recipe retrieval (Chen and Ngo, 2016; Min et al., 2017), regional cuisine style transformation (Kazama et al., 2018), recipe QA (Yagcioglu et al., 2018) and recipe generation (Kiddon et al., 2016; Yang et al., 2017). Among previous efforts towards recipes researches, our study is closer to recipe generation. Kiddon et al. (2016) define the task as given a goal (recipe title) and an agenda (ingredient list) to generate a complete recipe. They present the neural checklist model to improve the semantic coverage of the agenda in the generated texts. Yang et al. (2017) develop a language model th"
2020.emnlp-main.311,D16-1032,0,0.0990225,"of instructional language to teach people how to prepare delicious food. They have been gaining interests in recent researches as recipes contain immensely rich information about the real world (Yagcioglu et al., 2018). Among previous efforts towards computational recipe studies, there are two lines in obtaining cooking recipes for users: recipe retrieval and recipe generation. Recipe retrieval (Chen and Ngo, 2016; Min et al., 2017) matches the entities from the given dish pictures or text inputs to find the corresponding recipes. Provided with ingredients (Yang et al., 2017), recipe titles (Kiddon et al., 2016), or dish photos (Salvador et al., 2019), recipe generation models introduce additional mechanisms to assure the generated recipes containing as much ∗ The two authors contributed equally to this paper. Contribution was done at Peking University. given ingredients as possible. In previous studies, the target recipes are exactly composed of the given ingredients. However, in practice, people usually have a number of ingredients at hand and do not know what to cook. They have difficulty in choosing appropriate set of ingredients. And it can be hard for them to input an accurate recipe title for"
2020.emnlp-main.311,N16-1014,0,0.0890516,"Missing"
2020.emnlp-main.311,P09-1011,0,0.0846396,"Missing"
2020.emnlp-main.311,D15-1166,0,0.118547,"nsideration as Eq 6 shows. In this way, the undesired ingredients (with lower routing weights) get lower attentions. We produce an attention hidden state by concatenating the context vector and the hidden state. And then we use an LSTM decoder to get the word distribution pt formulated as Eq 7, where Yˆ = {Yˆ1 , ..., Yˆ|Yˆ |} denotes the word sequences of the target text. The word with highest probability in the distribution is selected as the generated word. Routing Enforced Generative Model (RGM) The generation module shares the same LSTM encoder with RM. We augment the attention mechanism (Luong et al., 2015) to capture relevant ingredient information to help with predicting the current target word. The alignments ai ∈ IRni between the last target hidden state ht−1 ∈ IRz and hidden states H (i) ∈ IRni ×z of the ingredient i is calculated as Eq 5, where W T ∈ IRz×z is a linear transformation matrix for the ingredient representations. Different from the previous attention mechanism, we obtain the context vector ct ∈ IRz taking both routing weights and alignment vector (5) ct = Σni=1 oi,ˆj ai H (i) . (6) pt = sof tmax(LST M (Yˆt−1 , [ct ; ht−1 ])). (7) 3.3 (4) ai = sof tmax(ht−1 W T (H (i) )&gt; ). Mode"
2020.emnlp-main.311,N19-1423,0,0.0174165,") is the ratio of distinct bi-grams in the generated recipes, which depicts the diversity7 . We define the set of used ingredients in the model outputs and gold reference are SO and SG respectively. Prec. denotes the ratio of SO∩SG in SO. Rec. denotes Baseline Models 5 In this work, we investigate how to improve recipe generation over strong baselines in both our setting 3 (Mixed Inputs) and common setting (Standard Inputs). We compare our routing enforced generative model against the baseline models below. Attseq: As the bidirectional LSTM encoder has proved strong representation capability (Devlin et al., 2019). We use the model with bidirectional encoder and Luong attention decoder (Luong et al., 2015) as our baseline model. Pointer: As the vocabulary used in instructional language is limited and there is a strong relationship between given ingredients and recipes, seq2seq model with the pointer network performs particularly well in previous recipe generation work (Yang et al., 2017). We use the pointer network (Vinyals et al., 2015) with ingredient attention, which provides comparable performance to reference-aware language model (Yang et al., 2017) and higher BLEU 5 . Retrieve: The model retrieve"
2020.emnlp-main.311,D19-1613,0,0.0796318,", 2019), recipe generation models introduce additional mechanisms to assure the generated recipes containing as much ∗ The two authors contributed equally to this paper. Contribution was done at Peking University. given ingredients as possible. In previous studies, the target recipes are exactly composed of the given ingredients. However, in practice, people usually have a number of ingredients at hand and do not know what to cook. They have difficulty in choosing appropriate set of ingredients. And it can be hard for them to input an accurate recipe title for the models (Kiddon et al., 2016; Majumder et al., 2019). What’s more, users may have preferences on some ingredients (e.g. “olive oil”) or categories (e.g. “Low Sugar”). As Figure 1 shows, given the same ingredient list, there can be different sets of ingredients contributing to recipes with different user demands. Previous researches have not discussed this common scene of life. There is a clear need in finding suitable cooking recipes that match user demands. As increasing the variety and creativity of daily dishes can promote our happiness, in this work, we manage to obtain the desired recipes in a generation manner. Our task is generally defin"
2020.emnlp-main.311,W14-2407,0,0.0116509,".com/ArleneYuZhiwei/RGM-for-RecipeGeneration sideration in recipe generation process. We propose a novel algorithm to calculate the ingredient collocation weights to enforce recipe generation model. • Given ingredients with noises, our model can satisfy personalized user demands by taking ingredients and category constraints into consideration. • Our approach yields significant improvements on both automatic and human evaluation. 2 Related Work Recipes have been gaining interest in recent researches, including recipe processing (Mori et al., 2012, 2014; Bosselut et al., 2017), recipe parsing (Malmaud et al., 2014; Jermsurawong and Habash, 2015), recipe retrieval (Chen and Ngo, 2016; Min et al., 2017), regional cuisine style transformation (Kazama et al., 2018), recipe QA (Yagcioglu et al., 2018) and recipe generation (Kiddon et al., 2016; Yang et al., 2017). Among previous efforts towards recipes researches, our study is closer to recipe generation. Kiddon et al. (2016) define the task as given a goal (recipe title) and an agenda (ingredient list) to generate a complete recipe. They present the neural checklist model to improve the semantic coverage of the agenda in the generated texts. Yang et al. (2"
2020.emnlp-main.311,N16-1086,0,0.0187554,"and create mentions of entities together with their 3798 attributes by accessing external databases. Majumder et al. (2019) take the historical user preference records into consideration. They generate personalized recipes from incomplete input specifications (name and incomplete ingredient details). Different from the existing methods on recipe generation which focus on covering all of the given ingredients, we extend the previous generation task with a selection of given items. Selective generation is a task to produce the natural language description for a salient subset of a rich records (Mei et al., 2016). A lot of attention has been paid to individual content selection and selective realization sub-problems (Barzilay and Lee, 2004; Barzilay and Lapata, 2005; Liang et al., 2009). Recent works (Chen and Mooney, 2008; Chen et al., 2010; Mei et al., 2016) explore full selective generation and learn alignments between generated texts and input data using a translation model. We find appropriate set of ingredients by selective routing algorithm. And our model can generate texts according to the user constraints on both ingredients and categories. The core inspiration for our routing module comes fr"
2020.emnlp-main.311,mori-etal-2014-flow,0,0.039853,"Missing"
2020.emnlp-main.311,D18-1350,0,0.0265563,"lgorithm. And our model can generate texts according to the user constraints on both ingredients and categories. The core inspiration for our routing module comes from following works. Hinton et al. (2011) propose transformation matrices that learn to encode the intrinsic spatial relationship between a part and a whole constitute viewpoint. Later, Sabour et al. (2017) propose an iterative routing-by-agreement mechanism to learn the intrinsic relationship between two layers. Hinton et al. (2018) propose a new iterative routing procedure based on the EM algorithm. Inspired by the previous work, Yang et al. (2018) firstly investigate the performance of dynamic routing on text classification. They propose three strategies (orphan category, leaky-softmax, coefficient amendment) to stabilize the dynamic routing process and alleviate the disturbance of some noises. 3 Our Approach The basic structure of our model is shown in Figure 2. Generally speaking, we take two steps to achieve the recipe generation: Select with Routing: In this part, our task is to select an appropriate set (soft selection as weight distribution) of ingredients to support a dish for each category with the given constraints. The proced"
2020.emnlp-main.311,D17-1197,0,0.303416,"1999). Recipes are a specific genre of instructional language to teach people how to prepare delicious food. They have been gaining interests in recent researches as recipes contain immensely rich information about the real world (Yagcioglu et al., 2018). Among previous efforts towards computational recipe studies, there are two lines in obtaining cooking recipes for users: recipe retrieval and recipe generation. Recipe retrieval (Chen and Ngo, 2016; Min et al., 2017) matches the entities from the given dish pictures or text inputs to find the corresponding recipes. Provided with ingredients (Yang et al., 2017), recipe titles (Kiddon et al., 2016), or dish photos (Salvador et al., 2019), recipe generation models introduce additional mechanisms to assure the generated recipes containing as much ∗ The two authors contributed equally to this paper. Contribution was done at Peking University. given ingredients as possible. In previous studies, the target recipes are exactly composed of the given ingredients. However, in practice, people usually have a number of ingredients at hand and do not know what to cook. They have difficulty in choosing appropriate set of ingredients. And it can be hard for them t"
2020.emnlp-main.311,P02-1040,0,0.107002,"ining set M T . 4.2 4 www.allrecipes.com www.yummly.com Training Details Attseq, Pointer and our model RGM all use 2layer LSTM encoders and decoders. All the hidden sizes in three generation models are 512. To avoid over-fitting, we set the dropout rates to 0.3 in these models. We use Adam (Kingma and Ba, 2014) as the optimizer and the learning rate is 0.0001. As for hyper parameters, we set the routing iteration r = 3,6 weight increase factor α = 100. 4.4 Automatic Evaluation Metrics In order to evaluate the effectiveness of our methods, we introduce the automatic metrics as follows: BLEU-4 (Papineni et al., 2002) is a commonly used metric to measure the quality of machine generated texts. Dis2 (Li et al., 2016) is the ratio of distinct bi-grams in the generated recipes, which depicts the diversity7 . We define the set of used ingredients in the model outputs and gold reference are SO and SG respectively. Prec. denotes the ratio of SO∩SG in SO. Rec. denotes Baseline Models 5 In this work, we investigate how to improve recipe generation over strong baselines in both our setting 3 (Mixed Inputs) and common setting (Standard Inputs). We compare our routing enforced generative model against the baseline mo"
2020.emnlp-main.560,P19-1448,0,0.0776132,"ts. Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder. 1 Introduction The Text-to-SQL task aims to translate natural language texts into SQL queries. Users who do not understand SQL grammars can benefit from this task and acquire information from databases by just inputting natural language texts. Previous works (Li and Jagadish, 2014; Xu et al., 2017; Yu et al., 2018a; Bogin et al., 2019b; Huo et al., 2019) focus on context-independent text-to-SQL generation. However, in practice, users usually interact with systems for several turns to acquire information, which extends the text-to-SQL task to the context-dependent text-to-SQL task in a conversational scenario. Throughout the interaction, user inputs may omit some information that appeared before. This phenomenon brings difficulty for context-dependent text-to-SQL task. Recently, context-dependent text-to-SQL task has attracted more attention. Suhr et al. (2018) conduct experiments on ATIS dataset (Dahl et al., 1994). Beside"
2020.emnlp-main.560,D19-1378,0,0.0341578,"Missing"
2020.emnlp-main.560,D19-5319,0,0.0227358,"orms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder. 1 Introduction The Text-to-SQL task aims to translate natural language texts into SQL queries. Users who do not understand SQL grammars can benefit from this task and acquire information from databases by just inputting natural language texts. Previous works (Li and Jagadish, 2014; Xu et al., 2017; Yu et al., 2018a; Bogin et al., 2019b; Huo et al., 2019) focus on context-independent text-to-SQL generation. However, in practice, users usually interact with systems for several turns to acquire information, which extends the text-to-SQL task to the context-dependent text-to-SQL task in a conversational scenario. Throughout the interaction, user inputs may omit some information that appeared before. This phenomenon brings difficulty for context-dependent text-to-SQL task. Recently, context-dependent text-to-SQL task has attracted more attention. Suhr et al. (2018) conduct experiments on ATIS dataset (Dahl et al., 1994). Besides, two cross-domain"
2020.emnlp-main.560,N18-1203,0,0.0501556,"Missing"
2020.emnlp-main.560,H94-1010,0,0.876714,"2018a; Bogin et al., 2019b; Huo et al., 2019) focus on context-independent text-to-SQL generation. However, in practice, users usually interact with systems for several turns to acquire information, which extends the text-to-SQL task to the context-dependent text-to-SQL task in a conversational scenario. Throughout the interaction, user inputs may omit some information that appeared before. This phenomenon brings difficulty for context-dependent text-to-SQL task. Recently, context-dependent text-to-SQL task has attracted more attention. Suhr et al. (2018) conduct experiments on ATIS dataset (Dahl et al., 1994). Besides, two cross-domain contextdependent datasets SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a) are released. Crossdomain means databases in test set differ from that in training set, which is more challenging. EditSQL (Zhang et al., 2019) is the previous state-of-the-art model on SParC and CoSQL datasets and it focuses on taking advantages of previous utterance texts and previously predicted query to predict the query for current turn. Table 1 shows the user inputs, ground truth queries and predicted queries of EditSQL for an interaction. In the second turn, EditSQL views “Kacey”"
2020.emnlp-main.560,N19-1423,0,0.0234375,"umn affiliation). Let X = {x1 , x2 , ..., x|X |}, where |X |is the number of utterances. xi is the i-th utterance and xij is the j-th token of it. y i is the i-th SQL query corresponding to xi and yji is the j-th token of y i . S consists of schema items {S 1 , ..., S |S |}, where |S| is the number of database schema items. At turn i, the model should make use of current and previous utterances {x1 , x2 , ..., xi }, database schema items S and their relations R to predict a SQL query y˜i . The objective of the model is to maximize the Q|I| probability of i=1 P (y i |x1 , x2 , ..., xi ). BERT (Devlin et al., 2019) is a pre-trained language model. Employing BERT output as embeddings of user inputs and database schema items has proved effective in context-dependent text-to-SQL task (Hwang et al., 2019; Guo et al., 2019; Wang et al., 2019; Choi et al., 2020). Therefore, we leverage BERT to get the embeddings of user inputs and database schema items as other context-dependent text-to-SQL models do. We concatenate user inputs and database schema items by separating with a “[SEP]” token following (Hwang et al., 2019). The output of BERT model is used as the embeddings of user inputs and schema items. 4 4.2 I"
2020.emnlp-main.560,P19-1444,0,0.0853087,"studies have focused on context-independent text-to-SQL task. Zhong et al. (2017) split the vocabulary and use reinforcement learning. Xu et al. (2017) propose a sketched-based model, which decomposes the token prediction process into SELECT-clause prediction and WHERE-clause prediction, aiming at taking previous predictions into consideration. Yu et al. (2018a) further employ a tree-based SQL decoder so as to decode SQL queries with the help of SQL grammar. In order to encode database schemas, schemas are regarded as graphs and graph neural networks have been applied (Bogin et al., 2019a,b). Guo et al. (2019) design an intermediate representation to bridge the gap between natural language texts and SQL queries. Choi et al. (2020) utilize a sketch-based slot filling approach to synthesize SQL queries. Wang et al. (2019) attempt to align the database columns and their mentions in user inputs by using a relation-aware self attention. Recently, context-dependent text-to-SQL task has drawn people’s attention. In-domain contextdependent benchmarks ATIS (Suhr et al., 2018) have been proposed. For ATIS, Suhr et al. (2018) utilize a sequence to sequence framework. Besides, they introduce an interaction-lev"
2020.emnlp-main.560,D18-1193,0,0.400235,"text-to-SQL datasets. Our model outperforms previous state-of-the-art model by a large margin and achieves new state-of-the-art results on the two datasets. The comparison and ablation results demonstrate the efficacy of our model and the usefulness of the database schema interaction graph encoder. 1 Introduction The Text-to-SQL task aims to translate natural language texts into SQL queries. Users who do not understand SQL grammars can benefit from this task and acquire information from databases by just inputting natural language texts. Previous works (Li and Jagadish, 2014; Xu et al., 2017; Yu et al., 2018a; Bogin et al., 2019b; Huo et al., 2019) focus on context-independent text-to-SQL generation. However, in practice, users usually interact with systems for several turns to acquire information, which extends the text-to-SQL task to the context-dependent text-to-SQL task in a conversational scenario. Throughout the interaction, user inputs may omit some information that appeared before. This phenomenon brings difficulty for context-dependent text-to-SQL task. Recently, context-dependent text-to-SQL task has attracted more attention. Suhr et al. (2018) conduct experiments on ATIS dataset (Dahl"
2020.emnlp-main.560,D19-1204,0,0.265671,"t-to-SQL generation. However, in practice, users usually interact with systems for several turns to acquire information, which extends the text-to-SQL task to the context-dependent text-to-SQL task in a conversational scenario. Throughout the interaction, user inputs may omit some information that appeared before. This phenomenon brings difficulty for context-dependent text-to-SQL task. Recently, context-dependent text-to-SQL task has attracted more attention. Suhr et al. (2018) conduct experiments on ATIS dataset (Dahl et al., 1994). Besides, two cross-domain contextdependent datasets SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a) are released. Crossdomain means databases in test set differ from that in training set, which is more challenging. EditSQL (Zhang et al., 2019) is the previous state-of-the-art model on SParC and CoSQL datasets and it focuses on taking advantages of previous utterance texts and previously predicted query to predict the query for current turn. Table 1 shows the user inputs, ground truth queries and predicted queries of EditSQL for an interaction. In the second turn, EditSQL views “Kacey” as the name of a dog owner. However, since the context of the interaction is"
2020.emnlp-main.560,D18-1425,0,0.0547293,"Missing"
2020.emnlp-main.560,P19-1443,0,0.297053,"t-to-SQL generation. However, in practice, users usually interact with systems for several turns to acquire information, which extends the text-to-SQL task to the context-dependent text-to-SQL task in a conversational scenario. Throughout the interaction, user inputs may omit some information that appeared before. This phenomenon brings difficulty for context-dependent text-to-SQL task. Recently, context-dependent text-to-SQL task has attracted more attention. Suhr et al. (2018) conduct experiments on ATIS dataset (Dahl et al., 1994). Besides, two cross-domain contextdependent datasets SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a) are released. Crossdomain means databases in test set differ from that in training set, which is more challenging. EditSQL (Zhang et al., 2019) is the previous state-of-the-art model on SParC and CoSQL datasets and it focuses on taking advantages of previous utterance texts and previously predicted query to predict the query for current turn. Table 1 shows the user inputs, ground truth queries and predicted queries of EditSQL for an interaction. In the second turn, EditSQL views “Kacey” as the name of a dog owner. However, since the context of the interaction is"
2020.findings-emnlp.218,P19-1602,0,0.0276801,"n-based methods have also been proposed to generate paraphrases (Mallinson et al., 2017; Wieting et al., 2017; Guo et al., 2019). The main philosophy of these methods is to translate a text into another language (often referred to as “pivot language”), and translate it back to the original language. Then the original text and backtranslated text are considered as a pair of paraphrases. There are also some works trying to generate paraphrase in an unsupervised way. For example, Roy and Grangier (2019) adopt the vectorquantized VAE framework to discrete the latent space to generate paraphrases. Bao et al. (2019) decompose the latent space into syntactic and semantic space, and sample in the syntactic space while keeping semantics unchanged when generating paraphrases. 2.2 Generative Adversarial Nets Generative Adversarial Nets was proposed by Goodfellow et al. (2014). The main idea of GAN is to train the generator and discriminator via minimax optimization, where the generator tries to generate realistic samples that match the real distribution, and the discriminator tries to distinguish between generated and real samples. GAN was first applied in the computer vision area. Some recent work have appli"
2020.findings-emnlp.218,P19-1599,0,0.0553652,"Gupta et al., 2018; Shakeri and Sethy, 2019; Yang et al., 2019) have become the dominant technique in the field of paraphrase generation. Paraphrases should be diversified in nature, i.e., an input sentence can correspond to multiple plausible paraphrases. Traditional seq2seq-based methods tend to generate highly similar outputs since the maximum likelihood estimation (MLE)-based objective function mostly cares about the validity rather than the diversity of outputs. Some works introduce control mechanisms over seq2seq models to produce diverse outputs (Iyyer et al., 2018; Park et al., 2019; Chen et al., 2019). However, the templates or exemplars in control mechanism cannot cover all the possibility of paraphrase, and the introduction of control mechanism is inflexible. Xu et al. (2018b) propose to use a shared decoder with different decoder embeddings to generate different outputs, but the decoder embeddings are not explicitly encouraged and learned to produce different outputs. Generative models, such as Variational Autoencoder (VAE) (Kingma and Welling, 2014) and Generative Adversarial Network (GAN) (Goodfellow et al., 2014), which learn distributions over the latent space, can generate diverse"
2020.findings-emnlp.218,N18-1170,0,0.0273583,"2019; Kajiwara, 2019; Li et al., 2018; Gupta et al., 2018; Shakeri and Sethy, 2019; Yang et al., 2019) have become the dominant technique in the field of paraphrase generation. Paraphrases should be diversified in nature, i.e., an input sentence can correspond to multiple plausible paraphrases. Traditional seq2seq-based methods tend to generate highly similar outputs since the maximum likelihood estimation (MLE)-based objective function mostly cares about the validity rather than the diversity of outputs. Some works introduce control mechanisms over seq2seq models to produce diverse outputs (Iyyer et al., 2018; Park et al., 2019; Chen et al., 2019). However, the templates or exemplars in control mechanism cannot cover all the possibility of paraphrase, and the introduction of control mechanism is inflexible. Xu et al. (2018b) propose to use a shared decoder with different decoder embeddings to generate different outputs, but the decoder embeddings are not explicitly encouraged and learned to produce different outputs. Generative models, such as Variational Autoencoder (VAE) (Kingma and Welling, 2014) and Generative Adversarial Network (GAN) (Goodfellow et al., 2014), which learn distributions over"
2020.findings-emnlp.218,P19-1607,0,0.142518,"has been widely used in many downstream applications, such as information retrieval, question answering, machine translation, and so on. Early works on paraphrase generation mainly focus on rule-based (McKeown, 1983; Meteer and Shaked, 1988), grammar-based (Narayan et al., 2016), lexicon-based (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and statistical machine translation (SMT)-based methods (Kauchak and Barzilay, 2006; Zhao et al., 2009). Recently, with the release of large-scale paraphrase datasets, sequence-to-sequence (seq2seq) models (Prakash et al., 2016; Li et al., 2019; Kajiwara, 2019; Li et al., 2018; Gupta et al., 2018; Shakeri and Sethy, 2019; Yang et al., 2019) have become the dominant technique in the field of paraphrase generation. Paraphrases should be diversified in nature, i.e., an input sentence can correspond to multiple plausible paraphrases. Traditional seq2seq-based methods tend to generate highly similar outputs since the maximum likelihood estimation (MLE)-based objective function mostly cares about the validity rather than the diversity of outputs. Some works introduce control mechanisms over seq2seq models to produce diverse outputs (Iyyer et al., 2018; P"
2020.findings-emnlp.218,N06-1058,0,0.328444,"paraphrase sentence, which requires that the generated sentence and input sentence are different in expression form, but have the same expressed meaning. Paraphrase generation is a fundamental task of natural language processing (NLP). The technique of paraphrase generation has been widely used in many downstream applications, such as information retrieval, question answering, machine translation, and so on. Early works on paraphrase generation mainly focus on rule-based (McKeown, 1983; Meteer and Shaked, 1988), grammar-based (Narayan et al., 2016), lexicon-based (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and statistical machine translation (SMT)-based methods (Kauchak and Barzilay, 2006; Zhao et al., 2009). Recently, with the release of large-scale paraphrase datasets, sequence-to-sequence (seq2seq) models (Prakash et al., 2016; Li et al., 2019; Kajiwara, 2019; Li et al., 2018; Gupta et al., 2018; Shakeri and Sethy, 2019; Yang et al., 2019) have become the dominant technique in the field of paraphrase generation. Paraphrases should be diversified in nature, i.e., an input sentence can correspond to multiple plausible paraphrases. Traditional seq2seq-based methods tend to generate highly simi"
2020.findings-emnlp.218,D14-1181,0,0.00237762,"us we can only list their BLEU4 scores for reference.3 4.4 Implementation Details For the generator, the encoder is set as a onelayer bidirectional GRU network with inner self2 https://github.com/lancopku/DPGAN 3 They do not release their codes, so we cannot get their results of generating multiple paraphrases. attention, and the decoder is set as a two-layer unidirectional GRU network. The dimension of the input and hidden size is set to 512. The latent code dimension is set to 512, and the latent code is concatenated to each input token. For the discriminator, the CNN network is the same as Kim (2014), where the size of filter windows are set as 3, 4, 5 with 100 feature maps each. Following previous work on GAN-based text generation, we pre-train the generator using standard MLE loss for 25 epochs, and pre-train the discriminator using the objective in Eq. 3 for 5 epochs. After pre-training, the generator and discriminator are trained alternatively, where each iteration consists of a G-step followed by a D-step. We use the NLTK4 tool to process the English texts. The vocabulary sizes are set as 50,000 and 80,000 for Quora and MSCOCO datasets, respectively. We set α = 0.8 and β = 0.8 in Eq."
2020.findings-emnlp.218,D18-1421,0,0.491539,"used in many downstream applications, such as information retrieval, question answering, machine translation, and so on. Early works on paraphrase generation mainly focus on rule-based (McKeown, 1983; Meteer and Shaked, 1988), grammar-based (Narayan et al., 2016), lexicon-based (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and statistical machine translation (SMT)-based methods (Kauchak and Barzilay, 2006; Zhao et al., 2009). Recently, with the release of large-scale paraphrase datasets, sequence-to-sequence (seq2seq) models (Prakash et al., 2016; Li et al., 2019; Kajiwara, 2019; Li et al., 2018; Gupta et al., 2018; Shakeri and Sethy, 2019; Yang et al., 2019) have become the dominant technique in the field of paraphrase generation. Paraphrases should be diversified in nature, i.e., an input sentence can correspond to multiple plausible paraphrases. Traditional seq2seq-based methods tend to generate highly similar outputs since the maximum likelihood estimation (MLE)-based objective function mostly cares about the validity rather than the diversity of outputs. Some works introduce control mechanisms over seq2seq models to produce diverse outputs (Iyyer et al., 2018; Park et al., 2019;"
2020.findings-emnlp.218,P19-1332,0,0.460867,"hrase generation has been widely used in many downstream applications, such as information retrieval, question answering, machine translation, and so on. Early works on paraphrase generation mainly focus on rule-based (McKeown, 1983; Meteer and Shaked, 1988), grammar-based (Narayan et al., 2016), lexicon-based (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and statistical machine translation (SMT)-based methods (Kauchak and Barzilay, 2006; Zhao et al., 2009). Recently, with the release of large-scale paraphrase datasets, sequence-to-sequence (seq2seq) models (Prakash et al., 2016; Li et al., 2019; Kajiwara, 2019; Li et al., 2018; Gupta et al., 2018; Shakeri and Sethy, 2019; Yang et al., 2019) have become the dominant technique in the field of paraphrase generation. Paraphrases should be diversified in nature, i.e., an input sentence can correspond to multiple plausible paraphrases. Traditional seq2seq-based methods tend to generate highly similar outputs since the maximum likelihood estimation (MLE)-based objective function mostly cares about the validity rather than the diversity of outputs. Some works introduce control mechanisms over seq2seq models to produce diverse outputs (Iyyer"
2020.findings-emnlp.218,D10-1090,0,0.0756768,"Missing"
2020.findings-emnlp.218,D16-1230,0,0.0315045,"N 3.12 3.80 3.95 3.52 Table 4: Results of the human evaluation on the MSCOCO dataset. shown in Table 1. For the other baselines, we also show the BLEU4 scores in Table 2 for reference. In terms of BLEU4 score, our DivGAN (average) performs worse than RbM-SL, MC-WGAN, VAE-SVG, D-PAGE and those transformer-based methods. However, we strongly argue that this does not mean that the quality of our generated paraphrases is worse than those generated by these models. Previous works have shown that BLEU is not a good measure for evaluating several text generation tasks, including dialogue generation (Liu et al., 2016), sentence simplification (Sulem et al., 2018) and paraphrase generation (Liu et al., 2010; An and Liu, 2019). First, we also think that the BLEU itself is not is a perfectly reasonable metric for the paraphrase generation task. The paraphrases are highly diversified in nature, but there is only one reference in these paraphrase datasets. Taking the sentences “what can i do to overcome anxiety” with the human reference “what do i do to reduce my anxiety” for example, our model generates sentences like “how do i overcome anxiety” or “what’s the best way to overcome anxiety” which are low in BLE"
2020.findings-emnlp.218,E17-1083,0,0.0295868,"019; Kajiwara, 2019). Li et al. (2018) further adopt reinforcement learning with policy gradient technique to generate semantically consistent paraphrases. Gupta et al. (2018) propose a conditional VAE-based framework to generate paraphrases from the latent space. Shakeri and Sethy (2019) improve the VAE framework by conditioning the generator on a label which specifies whether the paraphrases are semantically consistent or not. Yang et al. (2019) further introduce the CVAE-GAN framework for paraphrase generation. Some translation-based methods have also been proposed to generate paraphrases (Mallinson et al., 2017; Wieting et al., 2017; Guo et al., 2019). The main philosophy of these methods is to translate a text into another language (often referred to as “pivot language”), and translate it back to the original language. Then the original text and backtranslated text are considered as a pair of paraphrases. There are also some works trying to generate paraphrase in an unsupervised way. For example, Roy and Grangier (2019) adopt the vectorquantized VAE framework to discrete the latent space to generate paraphrases. Bao et al. (2019) decompose the latent space into syntactic and semantic space, and sam"
2020.findings-emnlp.218,J83-1001,0,0.634801,"es compared with baselines. 1 Introduction The task of paraphrase generation refers to rewriting a given sentence to a new paraphrase sentence, which requires that the generated sentence and input sentence are different in expression form, but have the same expressed meaning. Paraphrase generation is a fundamental task of natural language processing (NLP). The technique of paraphrase generation has been widely used in many downstream applications, such as information retrieval, question answering, machine translation, and so on. Early works on paraphrase generation mainly focus on rule-based (McKeown, 1983; Meteer and Shaked, 1988), grammar-based (Narayan et al., 2016), lexicon-based (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and statistical machine translation (SMT)-based methods (Kauchak and Barzilay, 2006; Zhao et al., 2009). Recently, with the release of large-scale paraphrase datasets, sequence-to-sequence (seq2seq) models (Prakash et al., 2016; Li et al., 2019; Kajiwara, 2019; Li et al., 2018; Gupta et al., 2018; Shakeri and Sethy, 2019; Yang et al., 2019) have become the dominant technique in the field of paraphrase generation. Paraphrases should be diversified in nature,"
2020.findings-emnlp.218,C88-2088,0,0.763683,"h baselines. 1 Introduction The task of paraphrase generation refers to rewriting a given sentence to a new paraphrase sentence, which requires that the generated sentence and input sentence are different in expression form, but have the same expressed meaning. Paraphrase generation is a fundamental task of natural language processing (NLP). The technique of paraphrase generation has been widely used in many downstream applications, such as information retrieval, question answering, machine translation, and so on. Early works on paraphrase generation mainly focus on rule-based (McKeown, 1983; Meteer and Shaked, 1988), grammar-based (Narayan et al., 2016), lexicon-based (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and statistical machine translation (SMT)-based methods (Kauchak and Barzilay, 2006; Zhao et al., 2009). Recently, with the release of large-scale paraphrase datasets, sequence-to-sequence (seq2seq) models (Prakash et al., 2016; Li et al., 2019; Kajiwara, 2019; Li et al., 2018; Gupta et al., 2018; Shakeri and Sethy, 2019; Yang et al., 2019) have become the dominant technique in the field of paraphrase generation. Paraphrases should be diversified in nature, i.e., an input sentence c"
2020.findings-emnlp.218,W16-6625,0,0.170907,"ch as Transformer (Vaswani et al., 2017). Our work is orthogonal to those works that focus on designing sophisticated encoder and decoder architectures. 3.1.2 Discriminator The discriminator D adopts a CNN network since CNN has recently been shown of great effectiveness in short text classification. Given a text x and the paraphrase y, the CNN network encodes them into C(x) and C(y) of the same dimension respectively. Then the quality of the paraphrase is measured by a one-layer feed-forward network with sigmoid activation: q(x, y) = σ(w[C(x); C(y)] + b) input sentence. Similar to Reed et al. (2016), We extend the discriminator D to identify three types of paraphrases for each input sentence x: (1) Sx : the set of paraphrases produced by human corresponding to x, (2) SG the set of paraphrases produced by the generator G corresponding to x, and (3) Sx the set of paraphrases produced by human, but are randomly sampled from all paraphrases which may be irrelevant to the given sentence x. Then the training objective is given below: (2) where w and b are weight parameters, σ refers to the sigmoid activation, and q(x, y) ∈ [0, 1] is the quality of the paraphrase y given the sentence x. 3.1.3"
2020.findings-emnlp.218,C16-1275,0,0.308185,"Missing"
2020.findings-emnlp.218,P19-1605,0,0.0917963,"sistent or not. Yang et al. (2019) further introduce the CVAE-GAN framework for paraphrase generation. Some translation-based methods have also been proposed to generate paraphrases (Mallinson et al., 2017; Wieting et al., 2017; Guo et al., 2019). The main philosophy of these methods is to translate a text into another language (often referred to as “pivot language”), and translate it back to the original language. Then the original text and backtranslated text are considered as a pair of paraphrases. There are also some works trying to generate paraphrase in an unsupervised way. For example, Roy and Grangier (2019) adopt the vectorquantized VAE framework to discrete the latent space to generate paraphrases. Bao et al. (2019) decompose the latent space into syntactic and semantic space, and sample in the syntactic space while keeping semantics unchanged when generating paraphrases. 2.2 Generative Adversarial Nets Generative Adversarial Nets was proposed by Goodfellow et al. (2014). The main idea of GAN is to train the generator and discriminator via minimax optimization, where the generator tries to generate realistic samples that match the real distribution, and the discriminator tries to distinguish be"
2020.findings-emnlp.218,D18-1081,0,0.063919,"Missing"
2020.findings-emnlp.218,D19-1309,0,0.144354,"ieval, question answering, machine translation, and so on. Early works on paraphrase generation mainly focus on rule-based (McKeown, 1983; Meteer and Shaked, 1988), grammar-based (Narayan et al., 2016), lexicon-based (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and statistical machine translation (SMT)-based methods (Kauchak and Barzilay, 2006; Zhao et al., 2009). Recently, with the release of large-scale paraphrase datasets, sequence-to-sequence (seq2seq) models (Prakash et al., 2016; Li et al., 2019; Kajiwara, 2019; Li et al., 2018; Gupta et al., 2018; Shakeri and Sethy, 2019; Yang et al., 2019) have become the dominant technique in the field of paraphrase generation. Paraphrases should be diversified in nature, i.e., an input sentence can correspond to multiple plausible paraphrases. Traditional seq2seq-based methods tend to generate highly similar outputs since the maximum likelihood estimation (MLE)-based objective function mostly cares about the validity rather than the diversity of outputs. Some works introduce control mechanisms over seq2seq models to produce diverse outputs (Iyyer et al., 2018; Park et al., 2019; Chen et al., 2019). However, the templates or exemplars in contr"
2020.findings-emnlp.218,P09-1094,0,0.0222091,"m, but have the same expressed meaning. Paraphrase generation is a fundamental task of natural language processing (NLP). The technique of paraphrase generation has been widely used in many downstream applications, such as information retrieval, question answering, machine translation, and so on. Early works on paraphrase generation mainly focus on rule-based (McKeown, 1983; Meteer and Shaked, 1988), grammar-based (Narayan et al., 2016), lexicon-based (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), and statistical machine translation (SMT)-based methods (Kauchak and Barzilay, 2006; Zhao et al., 2009). Recently, with the release of large-scale paraphrase datasets, sequence-to-sequence (seq2seq) models (Prakash et al., 2016; Li et al., 2019; Kajiwara, 2019; Li et al., 2018; Gupta et al., 2018; Shakeri and Sethy, 2019; Yang et al., 2019) have become the dominant technique in the field of paraphrase generation. Paraphrases should be diversified in nature, i.e., an input sentence can correspond to multiple plausible paraphrases. Traditional seq2seq-based methods tend to generate highly similar outputs since the maximum likelihood estimation (MLE)-based objective function mostly cares about the"
2020.findings-emnlp.218,D17-1026,0,0.0204484,"et al. (2018) further adopt reinforcement learning with policy gradient technique to generate semantically consistent paraphrases. Gupta et al. (2018) propose a conditional VAE-based framework to generate paraphrases from the latent space. Shakeri and Sethy (2019) improve the VAE framework by conditioning the generator on a label which specifies whether the paraphrases are semantically consistent or not. Yang et al. (2019) further introduce the CVAE-GAN framework for paraphrase generation. Some translation-based methods have also been proposed to generate paraphrases (Mallinson et al., 2017; Wieting et al., 2017; Guo et al., 2019). The main philosophy of these methods is to translate a text into another language (often referred to as “pivot language”), and translate it back to the original language. Then the original text and backtranslated text are considered as a pair of paraphrases. There are also some works trying to generate paraphrase in an unsupervised way. For example, Roy and Grangier (2019) adopt the vectorquantized VAE framework to discrete the latent space to generate paraphrases. Bao et al. (2019) decompose the latent space into syntactic and semantic space, and sample in the syntactic s"
2020.findings-emnlp.218,D18-1428,0,0.144819,", i.e., an input sentence can correspond to multiple plausible paraphrases. Traditional seq2seq-based methods tend to generate highly similar outputs since the maximum likelihood estimation (MLE)-based objective function mostly cares about the validity rather than the diversity of outputs. Some works introduce control mechanisms over seq2seq models to produce diverse outputs (Iyyer et al., 2018; Park et al., 2019; Chen et al., 2019). However, the templates or exemplars in control mechanism cannot cover all the possibility of paraphrase, and the introduction of control mechanism is inflexible. Xu et al. (2018b) propose to use a shared decoder with different decoder embeddings to generate different outputs, but the decoder embeddings are not explicitly encouraged and learned to produce different outputs. Generative models, such as Variational Autoencoder (VAE) (Kingma and Welling, 2014) and Generative Adversarial Network (GAN) (Goodfellow et al., 2014), which learn distributions over the latent space, can generate diverse outputs. In this paper, we build a new framework on top of the conditional GAN (Mirza and Osindero, 2014) to generate diverse paraphrases. To get multiple outputs, the generative"
2020.findings-emnlp.231,N18-1150,0,0.114583,"inal documents, which are relatively simple but face the drawbacks of information redundancy and incoherence between sentences. Abstractive methods enable generating new words, phrases, and sentences, which are able to generate better summaries with higher readability and conciseness. In this paper, we focus on abstractive document summarization. Empowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017; Paulus et al., 2018; Tan et al., 2017; C¸elikyilmaz et al., 2018). Compared with single-document summarization, annotated multi-document summarization datasets are often scarce. Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to a"
2020.findings-emnlp.231,N13-1136,0,0.0749035,"Missing"
2020.findings-emnlp.231,P19-1102,0,0.514811,"and merging sentences from the input documents, while the abstractive methods generate a summary using arbitrary words and expressions based on the understanding of the documents. Due to the lack of available training data, most previous multi-document summarization methods were extractive (Erkan and Radev, 2004; Christensen et al., 2013; Yasunaga et al., 2017). Recently, two multi-document summarization datasets have been proposed, one for very long input, aimed at generating Wikipedia (Liu et al., 2018) and another dedicated to generating a comprehensive summary of multiple real-time news (Fabbri et al., 2019). Several works have begun to explore abstractive multi-document summarization. Liu et al. (2018) concatenated multiple source documents into a long flat text and modeled multidocument summarization as a long sequence-tosequence task. Liu and Lapata (2019) represented cross-document relationships via an attention mechanism that allows sharing information as opposed to simply concatenating text spans and processing them as a flat sequence. Fabbri et al. (2019) incorporated MMR into a hierarchical pointer-generator network to address the information redundancy in multi-document summarization. Th"
2020.findings-emnlp.231,D18-1443,0,0.0324393,"Missing"
2020.findings-emnlp.231,P16-1154,0,0.0374427,"= LayerNorm (d˜ + MHAtt(d, dl = LayerNorm (g + FFN(g)) (5) Let Uit denote the output of the L-th layer for document Di at position t. The output Uit is passed through a softmax layer to calculate the generation distribution of next word over the target vocabulary.  Pˆit = softmax Uit Wg + bg (6) where Wg ∈ Rdmodel ×dvocab , bg ∈ Rdvocab and dvocab is the size of target vocabulary. To tackle the problem of out-of-vocabulary (OOV) words, we compute the copy attention εti between Uit and the input representations Ci to allow copying words from the source text, and obtain the copy distribution (Gu et al., 2016). εti = softmax(Uit Ci> ) P˜it = Ni X εti,j oi,j (7) j=1 where oi,j is the one-hot indicator vector for wi,j . The generation probability ηit ∈ [0, 1] is calculated from the decoder output Uit .  ηit = σ Uit Wη + bη (8) 2548 where Wη ∈ Rdmodel ×1 , bη ∈ R1 . The overall distribution for document Di is given by combining the two distributions with ηit . Pit = ηit ∗ Pˆit + (1 − ηit ) ∗ P˜it 3.4 (9) Decoding Controller Multi-document summarization requires producing a summary for a cluster of thematically related documents. While the summary decoder has predicted the vocabulary distribution for"
2020.findings-emnlp.231,P82-1020,0,0.780017,"Missing"
2020.findings-emnlp.231,D18-1446,0,0.479606,"aper, we focus on abstractive document summarization. Empowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017; Paulus et al., 2018; Tan et al., 2017; C¸elikyilmaz et al., 2018). Compared with single-document summarization, annotated multi-document summarization datasets are often scarce. Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to address the information redundancy for multi-document summarization. Single-document and multi-document summarizations are very closely related in both task definition and solution method (Wan, 2010). Both tasks need to deal with document-level input, identify the important con"
2020.findings-emnlp.231,N03-1020,0,0.434827,".42 35.78 36.7 36.48 37.24 R-2 7.87 6.13 4.55 6.03 6.38 9.36 8.90 7.83 8.22 8.60 R-SU4 11.86 10.16 8.16 10.01 7.22 13.23 11.43 12.4 12.29 12.67 Table 2: ROUGE F1 evaluation results on the DUC-04 dataset. input documents into a long flat text, and treats multi-document summarization as a long singledocument summarization task. The best hyperparameter configuration is chosen for each model. 4.4 Automatic Evaluation Following previous work, we report ROUGE-1 (unigram), ROUGE-2 (bigram) and ROUGE-SU4 (skip bigrams with a maximum distance of 4 words) scores as the metrics for automatic evaluation (Lin and Hovy, 2003). In Table 1, we report the results on the Multi-News, and our proposed model outperforms various baseline models. CopyTransformer performs much better than PGN and achieves 1.72 points improvement on the ROUGE1 F1, which demonstrates the superiority of the Transformer architecture. The methods of leveraging single-document corpus (i.e., SDS-to-MDS, CopyTransformer? , and ours) perform much better than that of only training on multi-document corpus (i.e., PGN, CopyTransformer, and Hi-MAP). Our model gains an improvement of 1.52 points compared with SDS-to-MDS, 1.23 points compared with CopyTra"
2020.findings-emnlp.231,P19-1500,0,0.0464439,"ummarization methods were extractive (Erkan and Radev, 2004; Christensen et al., 2013; Yasunaga et al., 2017). Recently, two multi-document summarization datasets have been proposed, one for very long input, aimed at generating Wikipedia (Liu et al., 2018) and another dedicated to generating a comprehensive summary of multiple real-time news (Fabbri et al., 2019). Several works have begun to explore abstractive multi-document summarization. Liu et al. (2018) concatenated multiple source documents into a long flat text and modeled multidocument summarization as a long sequence-tosequence task. Liu and Lapata (2019) represented cross-document relationships via an attention mechanism that allows sharing information as opposed to simply concatenating text spans and processing them as a flat sequence. Fabbri et al. (2019) incorporated MMR into a hierarchical pointer-generator network to address the information redundancy in multi-document summarization. The above works were all trained and tested on multi-document summarization corpus. 2.2 Adaptation Method from Single to Multi-Document Summarization Since the neural abstractive models have achieved promising results on single-document summarization (See et"
2020.findings-emnlp.231,W04-3252,0,0.540699,"Missing"
2020.findings-emnlp.231,K16-1028,0,0.0709148,"Missing"
2020.findings-emnlp.231,C10-1128,1,0.597526,"ed for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural encoder-decoder model trained for singledocument summarization to address the information redundancy for multi-document summarization. Single-document and multi-document summarizations are very closely related in both task definition and solution method (Wan, 2010). Both tasks need to deal with document-level input, identify the important content of documents, and paraphrase the important information to generate the summary, while the main difference is that multi-document summarization involves summarizing multiple input documents. Since the two tasks are closely related, it is promising to learn for two summarization tasks jointly. Compared with single-document summarization, multi-document summarization needs to handle multiple input documents. A simple method is to concatenate multiple documents into a long flat text and treat it as a long sequence-"
2020.findings-emnlp.231,K17-1045,0,0.039281,"Missing"
2020.findings-emnlp.231,P17-1099,0,0.455605,"e methods directly select important sentences from the original documents, which are relatively simple but face the drawbacks of information redundancy and incoherence between sentences. Abstractive methods enable generating new words, phrases, and sentences, which are able to generate better summaries with higher readability and conciseness. In this paper, we focus on abstractive document summarization. Empowered by large parallel datasets automatically harvested from online news websites, sequence-to-sequence learning has shown promising results on abstractive single-document summarization (See et al., 2017; Paulus et al., 2018; Tan et al., 2017; C¸elikyilmaz et al., 2018). Compared with single-document summarization, annotated multi-document summarization datasets are often scarce. Several works have explored adapting the neural encoder-decoder model trained for single-document summarization to multi-document summarization. Zhang et al. (2018) add a document set encoder to extend the neural abstractive model trained on large scale single-document summarization corpus to the multi-document summarization task. Lebanoff et al. (2018) incorporate the maximal marginal relevance method into a neural"
2020.findings-emnlp.231,P17-1108,1,0.878498,"Missing"
2020.findings-emnlp.5,P82-1020,0,0.688049,"Missing"
2020.findings-emnlp.5,D19-1436,0,0.0196397,"University The MOE Key Laboratory of Computational Linguistics, Peking University {wangke17, wanxiaojun}@pku.edu.cn Abstract bias (Bengio et al., 2015) due to the dependence on the previous sampled output during the inferring phase. Third, they only consider the word-level objective and may fail to guarantee some sentencelevel goals, such as realism, preserving semantic consistencies, long-range semantic structure, and so on (Ranzato et al., 2016). Recently, lots of recent studies (Yu et al., 2017; Che et al., 2017; Lin et al., 2017; Zhang et al., 2017; Chen et al., 2018; Wang and Wan, 2018; Ke et al., 2019; Nie et al., 2019; Wang and Wan, 2019; Wang et al., 2019) try to apply generative adversarial networks (GAN) (Goodfellow et al., 2014) in text generation, which uses discriminator networks as loss functions to ensure these higher-level objectives. However, the discreteness of texts makes it difficult for the gradient to pass from the discriminator to the generator. The current solution is mainly based on reinforcement learning (Yu et al., 2017) or differentiable sampling functions (Jang et al., 2017). In addition, considering the complexity of the language, the generator is easily much weaker"
2020.findings-emnlp.5,J82-2003,0,0.668626,"Missing"
2020.findings-emnlp.5,P02-1040,0,0.111052,"θg ); θd ) − G(z; θg )||kl ]. (6) 49 Datasets Synthetic Real #Train 10,000 10,000 #Test 10,000 10,000 #Vocab 5,000 4,684 Max-Length 20 38 10 8 6 Table 1: Statistics for the synthetic and real dataset we use. 4 Learning Curve 13 NLL Loss 12 11 adv NLLoracle 2 MLE SeqGAN MaliGAN RankGAN GSGAN TextGAN LeakGAN SLGAN 14 adv rec mle 0 0 25 50 75 100 Epochs 125 150 175 200 Figure 4: Different loss curves during adversarial training process. 10 9 8 the results in Figure 3, we observe that our model has a better ability to generate new sequences. Generalization: Same as Texygen, we also evaluate BLEU (Papineni et al., 2002) between generated sentences and the test set to see the generalization capacity of different models. The BLEU scores are shown in Table 2, which show that our model has a rather good generalization capacity. Moreover, as the order (i.e., n) of n-gram rises, the corresponding BLEU performance of our model does not drop as fast as other models. Diversity: We use Self-BLEU to evaluate how one sentence resembles the rest in a generated collection. From Table 2, we see that the sentences generated by our model have the lowest Self-BLEU score, indicating the highest diversity. Human Evaluation: We"
2020.findings-emnlp.5,P13-2089,0,0.077882,"Missing"
2020.tacl-1.2,S17-2159,0,0.0174219,". AMR-to-text generation is the task of recovering a text representing the same definition as a given AMR graph. Because the function words and structures are abstracted away, the AMR graph can correspond to multiple realizations. Numerous important details are underspecified, including tense, number, and definiteness, which makes this task extremely challenging (Flanigan et al., 2016). Figure 1 shows an example AMR graph and its corresponding sentence. Early works relied on grammar-based or statistical approaches (Flanigan et al., 2016; Pourdamghani et al., 2016; Lampouras and Vlachos, 2017; Gruzitis et al., 2017). Such approaches generally require alignments between the graph nodes and surface tokens, which are automatically generated and can lead to error accumulation. In recent research, the graphs are first transformed into linear sequences, and then the text is generated from the inputs (Konstas et al., 2017). Such a method may lose information from the graph structure. The current state-of-theart neural methods are graph-to-sequence models and hybrid variants (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019). These methods use a graph state long short-term memory (LSTM) network, gat"
2020.tacl-1.2,P18-1026,0,0.137238,"mar-based or statistical approaches (Flanigan et al., 2016; Pourdamghani et al., 2016; Lampouras and Vlachos, 2017; Gruzitis et al., 2017). Such approaches generally require alignments between the graph nodes and surface tokens, which are automatically generated and can lead to error accumulation. In recent research, the graphs are first transformed into linear sequences, and then the text is generated from the inputs (Konstas et al., 2017). Such a method may lose information from the graph structure. The current state-of-theart neural methods are graph-to-sequence models and hybrid variants (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019). These methods use a graph state long short-term memory (LSTM) network, gated graph neural network (GGNN), or graph convolution network (GCN) to encode AMR graphs directly, and they can explicitly utilize the information provided by the graph structure. However, these graph encoders still cannot significantly outperform sequence encoders. The AMR-to-text generation task can be regarded as a distinct translation task, and basing it on the concepts of off-the-shelf methods Abstract meaning representation (AMR)-totext generation is the challenging tas"
2020.tacl-1.2,P16-1154,0,0.0403047,"l connection and layer normalization around each layer in the graph encoder is more effective than using them around each of the three sub-layers for our model. The final node representation is obtained by concatenating the forward and backward 23 where H is the package of final node representations hi encoded by the graph encoder. For convenience, we denote the final hidden ˆ i . Considering state of the decoder at position i as h that numerous low-frequency open-class tokens such as named entities and numbers in an AMR graph appear in the corresponding sentence, we adopt the copy mechanism (Gu et al., 2016) to solve the problem. A gate is used over the decoder stack for controlling the generation of words from the vocabulary or directly copying them from the graph, expressed as representations. A linear transformation layer is also used for compressing the dimension. For convenience, we denote hi as the final representation of node vi , − −L1  → 1 ← hi = h L h Wh (10) i i where Wh ∈ R2dmodel ×demb is a parameter and L1 is the number of layers of the encoder stack. 3.4 Sentence Decoder In our model, the decoder has an architecture similar to that in the original Transformer model, which is comp"
2020.tacl-1.2,P16-1025,0,0.067419,"Missing"
2020.tacl-1.2,N19-1366,0,0.331109,"lanigan et al., 2016; Pourdamghani et al., 2016; Lampouras and Vlachos, 2017; Gruzitis et al., 2017). Such approaches generally require alignments between the graph nodes and surface tokens, which are automatically generated and can lead to error accumulation. In recent research, the graphs are first transformed into linear sequences, and then the text is generated from the inputs (Konstas et al., 2017). Such a method may lose information from the graph structure. The current state-of-theart neural methods are graph-to-sequence models and hybrid variants (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019). These methods use a graph state long short-term memory (LSTM) network, gated graph neural network (GGNN), or graph convolution network (GCN) to encode AMR graphs directly, and they can explicitly utilize the information provided by the graph structure. However, these graph encoders still cannot significantly outperform sequence encoders. The AMR-to-text generation task can be regarded as a distinct translation task, and basing it on the concepts of off-the-shelf methods Abstract meaning representation (AMR)-totext generation is the challenging task of generating natural language texts from A"
2020.tacl-1.2,C12-1083,0,0.0750245,"Missing"
2020.tacl-1.2,N16-1087,0,0.453501,"benefit from using AMR, such as machine translation (Jones et al., 2012; Song et al., 2019), question answering (Mitra and Baral, 2016), summarization (Liu et al., 2015; Takase et al., 2016), and event extraction (Huang et al., 2016). AMR-to-text generation is the task of recovering a text representing the same definition as a given AMR graph. Because the function words and structures are abstracted away, the AMR graph can correspond to multiple realizations. Numerous important details are underspecified, including tense, number, and definiteness, which makes this task extremely challenging (Flanigan et al., 2016). Figure 1 shows an example AMR graph and its corresponding sentence. Early works relied on grammar-based or statistical approaches (Flanigan et al., 2016; Pourdamghani et al., 2016; Lampouras and Vlachos, 2017; Gruzitis et al., 2017). Such approaches generally require alignments between the graph nodes and surface tokens, which are automatically generated and can lead to error accumulation. In recent research, the graphs are first transformed into linear sequences, and then the text is generated from the inputs (Konstas et al., 2017). Such a method may lose information from the graph structur"
2020.tacl-1.2,P17-1014,0,0.597858,"definiteness, which makes this task extremely challenging (Flanigan et al., 2016). Figure 1 shows an example AMR graph and its corresponding sentence. Early works relied on grammar-based or statistical approaches (Flanigan et al., 2016; Pourdamghani et al., 2016; Lampouras and Vlachos, 2017; Gruzitis et al., 2017). Such approaches generally require alignments between the graph nodes and surface tokens, which are automatically generated and can lead to error accumulation. In recent research, the graphs are first transformed into linear sequences, and then the text is generated from the inputs (Konstas et al., 2017). Such a method may lose information from the graph structure. The current state-of-theart neural methods are graph-to-sequence models and hybrid variants (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019). These methods use a graph state long short-term memory (LSTM) network, gated graph neural network (GGNN), or graph convolution network (GCN) to encode AMR graphs directly, and they can explicitly utilize the information provided by the graph structure. However, these graph encoders still cannot significantly outperform sequence encoders. The AMR-to-text generation task can be r"
2020.tacl-1.2,S17-2096,0,0.0158407,"traction (Huang et al., 2016). AMR-to-text generation is the task of recovering a text representing the same definition as a given AMR graph. Because the function words and structures are abstracted away, the AMR graph can correspond to multiple realizations. Numerous important details are underspecified, including tense, number, and definiteness, which makes this task extremely challenging (Flanigan et al., 2016). Figure 1 shows an example AMR graph and its corresponding sentence. Early works relied on grammar-based or statistical approaches (Flanigan et al., 2016; Pourdamghani et al., 2016; Lampouras and Vlachos, 2017; Gruzitis et al., 2017). Such approaches generally require alignments between the graph nodes and surface tokens, which are automatically generated and can lead to error accumulation. In recent research, the graphs are first transformed into linear sequences, and then the text is generated from the inputs (Konstas et al., 2017). Such a method may lose information from the graph structure. The current state-of-theart neural methods are graph-to-sequence models and hybrid variants (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019). These methods use a graph state long short-term me"
2020.tacl-1.2,N15-1114,0,0.117653,"Missing"
2020.tacl-1.2,D16-1224,0,0.267137,"capturing of the semantic information provided in the graph. The code is available at https://github. com/sodawater/GraphTransformer. • The experimental results show that our model achieves a new state-of-the-art performance on benchmark datasets. 20 2 Related Work 2018); it incorporates the attention mechanism in the information aggregation. 2.1 AMR-to-Text Generation 2.3 Transformer Network Early work on AMR-to-text generation focused on statistical methods. Flanigan et al. (2016) transformed AMR graphs to appropriate spanning trees and applied tree-to-string transducers to generate texts. Song et al. (2016) partitioned an AMR graph into small fragments and generated the translations for all the fragments, whose order was finally decided by solving an asymmetric generalized traveling salesman problem. Song et al. (2017) used synchronous node replacement grammar to parse AMR graphs and generate output sentences. Pourdamghani et al. (2016) adopted a phrase-based machine translation model on the input of a linearized graph. Recent works propose using neural networks for generation. Konstas et al. (2017) used a sequence-to-sequence model to generate texts, leveraging an LSTM for encoding a linearized"
2020.tacl-1.2,P18-1150,0,0.638026,"tical approaches (Flanigan et al., 2016; Pourdamghani et al., 2016; Lampouras and Vlachos, 2017; Gruzitis et al., 2017). Such approaches generally require alignments between the graph nodes and surface tokens, which are automatically generated and can lead to error accumulation. In recent research, the graphs are first transformed into linear sequences, and then the text is generated from the inputs (Konstas et al., 2017). Such a method may lose information from the graph structure. The current state-of-theart neural methods are graph-to-sequence models and hybrid variants (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019). These methods use a graph state long short-term memory (LSTM) network, gated graph neural network (GGNN), or graph convolution network (GCN) to encode AMR graphs directly, and they can explicitly utilize the information provided by the graph structure. However, these graph encoders still cannot significantly outperform sequence encoders. The AMR-to-text generation task can be regarded as a distinct translation task, and basing it on the concepts of off-the-shelf methods Abstract meaning representation (AMR)-totext generation is the challenging task of generating nat"
2020.tacl-1.2,P02-1040,0,0.110757,"15E86 dataset contains 16,833 instances for the training, 1,368 for the development, and 1,371 for the test. The LDC2017T10 dataset is the latest AMR corpus release, which contains 36,521 instances for the training and the same instances for the development and test as in LDC2015E86. Most prior works evaluate their models on the former dataset. Because prior approaches during the same period achieve the state-of-the-art performances on LDC2015E86 and LDC2017T10, respectively, we performed experiments on both the datasets. Following existing works, we evaluate the results with the BLEU metric (Papineni et al., 2002). We also report the results using CHRF++ (Popovi´c, 2017), similar to Beck et al. (2018). Our direct baseline is the original Transformer, which takes a linearized graph as the input. We use the same linearization as that by Konstas et al. (2017). We also compare our model with prior statistical approaches (PBMT, Tree2Str, and TSP), sequence-to-sequence approaches (S2S+Anon and S2S+Copy), the current state-of-the-art 1 https://www.cs.rochester.edu/∼lsong10/ downloads/2m.json.gz. 25 Methods PBMT (Pourdamghani et al., 2016) Tree2Str (Flanigan et al., 2016) TSP (Song et al., 2016) S2S+Anon (Kons"
2020.tacl-1.2,D16-1112,0,0.028631,"Missing"
2020.tacl-1.2,D14-1162,0,0.0959096,"final layer. In our model, we use a scaled dot-product attention for all the attention layers. 5.2 Parameter Settings and Training Details We set our model parameters based on preliminary experiments on the development set. dmodel is set to 256 and demb is set to 300. The head number of attention is set to 2. The numbers (L1 and L2 ) of layers of the encoder and decoder are set to 8 and 6, respectively. The batch size is set to 64. We extract a vocabulary from the training set, which is shared by both the encoder and the decoder. The word embeddings are initialized from GloVe word embeddings (Pennington et al., 2014). We use the Adam optimizer (Kingma and Ba, 2015) with lr = 0.0002, β1 = 0.9, β2 = 0.98, and  = 10−9 . Learning rate is halved every time perplexity on the development set does not improve for two epochs. We apply dropout to the output of each attention sub-layer and the input embeddings, and use a rate of Pdrop = 0.3. Beam search with beam size to 6 is used for decoding. During training, we filter out instances with more than 100 nodes in graph or 100 words in sentence for speeding up. Note that dmodel is set to 512, the head number is set to 4, and the learning rate is set to 0.0001 when tr"
2020.tacl-1.2,W17-4770,0,0.109468,"Missing"
2020.tacl-1.2,W16-6603,0,0.438591,"et al., 2016), and event extraction (Huang et al., 2016). AMR-to-text generation is the task of recovering a text representing the same definition as a given AMR graph. Because the function words and structures are abstracted away, the AMR graph can correspond to multiple realizations. Numerous important details are underspecified, including tense, number, and definiteness, which makes this task extremely challenging (Flanigan et al., 2016). Figure 1 shows an example AMR graph and its corresponding sentence. Early works relied on grammar-based or statistical approaches (Flanigan et al., 2016; Pourdamghani et al., 2016; Lampouras and Vlachos, 2017; Gruzitis et al., 2017). Such approaches generally require alignments between the graph nodes and surface tokens, which are automatically generated and can lead to error accumulation. In recent research, the graphs are first transformed into linear sequences, and then the text is generated from the inputs (Konstas et al., 2017). Such a method may lose information from the graph structure. The current state-of-theart neural methods are graph-to-sequence models and hybrid variants (Beck et al., 2018; Song et al., 2018; Damonte and Cohen, 2019). These methods use a g"
2020.tacl-1.2,D18-1049,0,0.0190713,"on neural networks (CNNs) have been widely used in NLP tasks because of their advantages of capturing long-term and local dependencies, respectively. Compared with these networks, models based solely on the attention mechanism show superiority in terms of the parallelism and flexibility in the modeling dependencies. Recently, RNN/CNN-free networks have attracted increasing interests. Vaswani et al. (2017) proposed a stacked attention architecture, the Transformer model, for neural machine translation. Gu et al. (2018) introduced a non-autoregressive translation model based on the transformer. Zhang et al. (2018) integrated the paraphrase rules and the Transformer model, for sentence simplification. Devlin et al. (2018) proposed a language representation model called BERT, which achieved new state-of-the-art results on 11 NLP tasks. 3 Graph Transformer The overall architecture of Graph Transformer is shown in Figure 2, with an example AMR graph and its corresponding sentence. We begin by providing the formal definition of the AMRto-text generation and the notations we use, and then reviewing the Transformer model. Then we introduce the graph encoder and sentence decoder used in our model. Finally, we"
2020.tacl-1.2,Q19-1002,0,\N,Missing
2021.acl-short.9,P02-1040,0,0.137852,"II (Zhou et al., 2017). ActivityNet Captions contains 10,009 videos in train set, 4,917 videos in val set. Each video has 3.65 event segments on average. Following (Lei et al., 2020), the original val set is split into ae-val with 2,460 videos for validation and ae-test with 2,457 videos for test. YouCookII contains 1,333 videos in train set, 457 videos in val set. Each video has 7.70 event segments on average. 3.2 Evaluation Metrics Following (Lei et al., 2020; Park et al., 2019), we evaluate the captioning performance at paragraph level. We report standard caption metrics, including BLEU@4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), CIDEr (Vedantam et al., 2015). We also evaluate repetition using R@4 (Xiong et al., 2018). We use the scripts provided by (Lei et al., 2020) for evaluation1 . 3.3 Baselines and Results Models B@4 M C R@4↓ Soft-NMS ESGN V-Trans Trans-XL MART COOT VPCSum 10.33 10.38 9.89 10.36 10.13 9.85 10.89 14.93 15.74 15.11 14.89 14.94 14.67 15.84 22.58 21.85 20.95 20.73 20.16 21.83 24.33 10.17 6.51 7.04 7.45 6.09 7.15 1.54 V-trans* Trans-XL* MART* COOT* 9.31 10.25 9.78 10.85 15.54 14.91 15.57 15.99 21.33 21.71 22.16 28.19 7.45 8.79 5.44 6.64 Table 1: Comparison with bas"
2021.acl-short.9,W14-3348,0,0.0100008,"Net Captions contains 10,009 videos in train set, 4,917 videos in val set. Each video has 3.65 event segments on average. Following (Lei et al., 2020), the original val set is split into ae-val with 2,460 videos for validation and ae-test with 2,457 videos for test. YouCookII contains 1,333 videos in train set, 457 videos in val set. Each video has 7.70 event segments on average. 3.2 Evaluation Metrics Following (Lei et al., 2020; Park et al., 2019), we evaluate the captioning performance at paragraph level. We report standard caption metrics, including BLEU@4 (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014), CIDEr (Vedantam et al., 2015). We also evaluate repetition using R@4 (Xiong et al., 2018). We use the scripts provided by (Lei et al., 2020) for evaluation1 . 3.3 Baselines and Results Models B@4 M C R@4↓ Soft-NMS ESGN V-Trans Trans-XL MART COOT VPCSum 10.33 10.38 9.89 10.36 10.13 9.85 10.89 14.93 15.74 15.11 14.89 14.94 14.67 15.84 22.58 21.85 20.95 20.73 20.16 21.83 24.33 10.17 6.51 7.04 7.45 6.09 7.15 1.54 V-trans* Trans-XL* MART* COOT* 9.31 10.25 9.78 10.85 15.54 14.91 15.57 15.99 21.33 21.71 22.16 28.19 7.45 8.79 5.44 6.64 Table 1: Comparison with baselines on ActivityNet Captions ae-te"
2021.eacl-main.33,P09-1053,0,0.0517646,"s with an SVM classifier and human annotations. As is discovered, platforms such as Twitter also contain many paraphrase pairs. Twitter Paraphrase Corpus [PIT-2015] (Xu et al., 2015) contains 14,035 paraphrase pairs on more than 400 distinct topics. Two years later, Twitter Url Corpus [TUC] (Lan et al., 2017) was proposed as a development of PIT-2015. TUC contains 51,524 sentence pairs, collected from Twitter by linking tweets through shared URLs and do not leverage any classifier or human intervention. Datasets such as MSRP or PIT-2015 encourage a series of work in paraphrase identification (Das and Smith, 2009; Mallinson et al., 2017) but the size limitation hinders complex generation models. 2 Dataset Source Materials Our ParaSCI dataset is constructed based on the following source materials: ACL Anthology Sentence Corpus (AASC) AASC (Aizawa et al., 2018) is a corpus of natural language text extracted from scientific papers. It contains 2,339,195 sentences from 44,481 PDFformat papers from the ACL Anthology, a comprehensive scientific paper repository on computational linguistics and NLP. ArXiv Bulk Data ArXiv4 is an open-access repository of electronic preprints. It consists of scientific papers"
2021.eacl-main.33,I05-5002,0,0.478446,"Missing"
2021.eacl-main.33,D17-1091,0,0.0177125,"ntial question duplicate pairs. Wieting and Gimpel (2018) constructed ParaNMT-50M, a dataset of more than 50 million paraphrase pairs. The pairs were generated automatically by translating the non-English side of a large parallel corpus. Nowadays, MSCOCO and Quora are mainly used for paraphrase generation (Fu et al., 2019; Gupta et al., 2018). Nevertheless, their sentence lengths or related domains are restricted. 3 Related Work 3.1 Paraphrases capture the essence of language diversity (Pavlick et al., 2015) and play significant roles in many challenging NLP tasks, such as question answering (Dong et al., 2017), semantic parsing (Su and Yan, 2017) and machine translation (Cho et al., 2014). Development in paraphrases relies heavily on the construction of paraphrase datasets. Paraphrase Identification Datasets Dolan and Brockett (2005) proposed MSR Paraphrase Corpus [MSRP], a paraphrase dataset of 5,801 sentence pairs, by clustering news articles with an SVM classifier and human annotations. As is discovered, platforms such as Twitter also contain many paraphrase pairs. Twitter Paraphrase Corpus [PIT-2015] (Xu et al., 2015) contains 14,035 paraphrase pairs on more than 400 distinct topics. Two years"
2021.eacl-main.33,D17-1126,0,0.136666,"Missing"
2021.eacl-main.33,P18-1157,0,0.0185368,"and also much longer than that of TUC. Its average length is only a little shorter than that of MSRP. As MSRP only contains 3,900 gold-standard paraphrases, our ParaSCI is complementary to the vaSentence BERT for Paraphrase Extraction across Different Sections Noting that sentences with shared semantics appear in different parts of one paper. For instance, the following sentences are from different parts of a same paper, and they are paraphrases: S1 : we propose a simple yet robust stochastic answer network (SAN) that simulates multi-step reasoning in machine reading comprehension. (abstract, Liu et al. (2018)) S2 : we introduce Stochastic Answer Networks (SAN), a simple yet robust model for machine reading comprehension. (introduction, Liu et al. (2018)) However, sentences in Method, Data and Result sections are semantically different even when they only have minor changes. For example, the strings other than numbers may be very similar when presenting the two experimental results, but the semantics are completely different. Therefore, we mainly focus on six sections (Abstract, 426 Name Genre MSRP TUC ParaNMT-50M MSCOCO Quora ParaSCI-ACL ParaSCI-arXiv news Twitter Novels, laws Description Question"
2021.eacl-main.33,P04-2006,0,0.0279132,"sually change a lot lexically, because lexical variation is easier to realize. Although sentential variation scores are lower than lexical or phrasal scores, nearly one or two sentential variations for each pair are already rare and valuable for a paraphrase dataset. The long average length makes such sentential transformation possible, which is complementary to other datasets of short paraphrases. 6 Name Paraphrase Phenomenon Occurrence In order to show the differences across paraphrase datasets, we sample 100 sentential paraphrases from each dataset and count occurrences of each phenomenon. Boonthum (2004) grouped common paraphrase phenomenon into 6 categories : Synonym (substitute a word with its synonym), Voice (change the voice of sentence from active to passive or vice versa), Word-Form (change a word into a different form), Break (break a long sentence down into small sentences), Definition (substitute a word with its definition or meaning), Structure (use different sentence structures to express the same thing). We report the average number of occurrences of each paraphrase type per sentence pair for each corpus in Table 5 and visualize that in Figure 3. Figure 3: Visualization of Table 5"
2021.eacl-main.33,2020.acl-main.447,0,0.213315,"f natural language text extracted from scientific papers. It contains 2,339,195 sentences from 44,481 PDFformat papers from the ACL Anthology, a comprehensive scientific paper repository on computational linguistics and NLP. ArXiv Bulk Data ArXiv4 is an open-access repository of electronic preprints. It consists of scientific papers in the fields of mathematics, physics, astronomy, etc.. As the complete set is too large to process, we randomly select 202,125 PDF files as our original data and convert them to TXT files, arranged by sentence. Semantic Scholar Open Research Corpus (S2ORC) S2ORC (Lo et al., 2020) is a large contextual citation graph of scientific papers from multiple scientific domains, consisting of 81.1M papers, 380.5M citation edges. We select all the citation edges of ACL and arXiv from S2ORC for subsequent processing. https://github.com/dqxiu/ParaSCI 425 3 website:https://data.quora.com/First-Quora-DatasetRelease-Question-Pairs 4 website:https://arxiv.org/help/bulk data 3.2 Basic Information According to the source materials, ParaSCI includes two subsets, ParaSCI-ACL and ParaSCIarXiv. Paraphrase pairs in ParaSCI-ACL focus on the NLP field, while paraphrase pairs in ParaSCIarXiv a"
2021.eacl-main.33,E17-1083,0,0.0213655,"ier and human annotations. As is discovered, platforms such as Twitter also contain many paraphrase pairs. Twitter Paraphrase Corpus [PIT-2015] (Xu et al., 2015) contains 14,035 paraphrase pairs on more than 400 distinct topics. Two years later, Twitter Url Corpus [TUC] (Lan et al., 2017) was proposed as a development of PIT-2015. TUC contains 51,524 sentence pairs, collected from Twitter by linking tweets through shared URLs and do not leverage any classifier or human intervention. Datasets such as MSRP or PIT-2015 encourage a series of work in paraphrase identification (Das and Smith, 2009; Mallinson et al., 2017) but the size limitation hinders complex generation models. 2 Dataset Source Materials Our ParaSCI dataset is constructed based on the following source materials: ACL Anthology Sentence Corpus (AASC) AASC (Aizawa et al., 2018) is a corpus of natural language text extracted from scientific papers. It contains 2,339,195 sentences from 44,481 PDFformat papers from the ACL Anthology, a comprehensive scientific paper repository on computational linguistics and NLP. ArXiv Bulk Data ArXiv4 is an open-access repository of electronic preprints. It consists of scientific papers in the fields of mathemat"
2021.eacl-main.33,J08-4004,0,0.129609,"verall variation of ParaSCI from manual evaluation. Syn Voice Form Break Def Struct MSRP TUC ParaNMT-50M MSCOCO Quora ParaSCI-ACL ParaSCI-arXiv 0.80 0.50 0.87 0.72 1.02 0.97 1.04 0.19 0.10 0.15 0.05 0.16 0.14 0.11 0.15 0.10 0.20 0.12 0.22 0.12 0.15 0.15 0.09 0.13 0.10 0.23 0.28 0.32 0.31 0.53 0.40 0.36 0.74 0.57 0.68 0.28 0.29 0.25 0.26 0.46 0.45 0.41 Table 5: Example of extracting paraphrase pairs from papers sharing a same cited paper. three, 2 means it has only one change and 1 means no change. Quality Control We evaluate the annotation quality of each worker using Cohen’s kappa agreement (Artstein and Poesio, 2008) against the majority vote of other workers. We asked the best worker to label more data by republishing the questions done by workers with low reliability (Cohen’s kappa &lt;0.4). Finally, the average Cohen’s kappa of semantic consistency evaluation is 0.71 and that of literal variation is 0.62 (0.66 lexically, 0.59 phrasally and 0.61 sententially). Evaluation Results The average semantic consistency of ParaSCI-ACL is 4.17 and that of ParaSCI-arXiv is 3.94, both around 4, which means most paraphrase pairs are nearly equivalent, only some unimportant details may differ. Besides, the average seman"
2021.eacl-main.33,miltsakaki-etal-2004-penn,0,0.127733,"tence “we used moses as the phrase-based machine translation system.”. As we use domain-related terms and corresponding abbreviations in scientific texts frequently, this advantage can add conciseness, technicality and naturality to the generated sentences. Another thing to mention is that some common sense or scientific knowledge is also taken into paraphrase generation. For example, we input “the penn discourse treebank is the largest corpus richly annotated with explicit and implicit discourse relations and their senses.” as the original sentence. It introduces the Penn Discourse Treebank (Miltsakaki et al., 2004) without any information related to its size and source, but the information is added to the generated sentences: “the penn discourse treebank is the largest available annotated corpora of discourse relations over 2,312 wall street journal articles.” These cases reveal different aspects of ParaSCI’s advantages in paraphrase generation. In further work, we hope that ParaSCI will contribute more to scientific paraphrase generation and subsequently, be applied to more downstream tasks in the scientific field. 431 Name Original Paraphrase a group of people MSCOCO watch a dog ride a motorcycle. an"
2021.eacl-main.33,P02-1040,0,0.112619,"s Authors usually write down the same information with transformed expressions repeatedly in different parts of the paper to emphasize critical information or echo back and forth. This kind of feature is the premise of our intra-paper extraction methods. Table 1: Example paraphrase pairs in ParaSCI. Sentence A and corresponding Sentence B are paraphrase pairs. 3.3 cancy of large-scale long paraphrase pairs. The degree of alteration is another important aspect of paraphrases. To compare our ParaSCI with other existing paraphrase datasets in this aspect, we propose to calculate the BLEU4 score (Papineni et al., 2002) between the source and target sentences of each paraphrase pair and name it SelfBLEU. In Table 2, ParaSCI, especially ParaSCIACL, shows a relatively low Self-BLEU, which means sentences are significantly changed. Statistic Characteristic To assess the characteristics of ParaSCI, we compare its statistic characteristics with five main sentential paraphrase datasets in Table 2. The source genre of ParaSCI is scientific papers. Therefore, sentences are more formal and scholastic, and they differ from oral TUC or newsy MSRP. The average sentence length of ParaSCI is almost twice as long as that o"
2021.eacl-main.33,P15-2070,0,0.0473349,"Missing"
2021.eacl-main.33,D17-1127,0,0.0287247,"g and Gimpel (2018) constructed ParaNMT-50M, a dataset of more than 50 million paraphrase pairs. The pairs were generated automatically by translating the non-English side of a large parallel corpus. Nowadays, MSCOCO and Quora are mainly used for paraphrase generation (Fu et al., 2019; Gupta et al., 2018). Nevertheless, their sentence lengths or related domains are restricted. 3 Related Work 3.1 Paraphrases capture the essence of language diversity (Pavlick et al., 2015) and play significant roles in many challenging NLP tasks, such as question answering (Dong et al., 2017), semantic parsing (Su and Yan, 2017) and machine translation (Cho et al., 2014). Development in paraphrases relies heavily on the construction of paraphrase datasets. Paraphrase Identification Datasets Dolan and Brockett (2005) proposed MSR Paraphrase Corpus [MSRP], a paraphrase dataset of 5,801 sentence pairs, by clustering news articles with an SVM classifier and human annotations. As is discovered, platforms such as Twitter also contain many paraphrase pairs. Twitter Paraphrase Corpus [PIT-2015] (Xu et al., 2015) contains 14,035 paraphrase pairs on more than 400 distinct topics. Two years later, Twitter Url Corpus [TUC] (Lan"
2021.eacl-main.33,P18-1042,0,0.0156471,"Paraphrase Generation Datasets MSCOCO (Lin et al., 2014) was originally described as a large-scale object detection dataset. It contains human-annotated captions of over 120K images, and each image is associated with five captions from five different annotators. In most cases, annotators describe the most prominent object/action in an image, which makes this dataset suitable for paraphrase-related tasks. Consequently, MSCOCO makes great contribution to paraphrase generation. Quora released a new dataset3 in January 2017, which consists of over 400K lines of potential question duplicate pairs. Wieting and Gimpel (2018) constructed ParaNMT-50M, a dataset of more than 50 million paraphrase pairs. The pairs were generated automatically by translating the non-English side of a large parallel corpus. Nowadays, MSCOCO and Quora are mainly used for paraphrase generation (Fu et al., 2019; Gupta et al., 2018). Nevertheless, their sentence lengths or related domains are restricted. 3 Related Work 3.1 Paraphrases capture the essence of language diversity (Pavlick et al., 2015) and play significant roles in many challenging NLP tasks, such as question answering (Dong et al., 2017), semantic parsing (Su and Yan, 2017) a"
2021.eacl-main.33,S15-2001,0,0.148034,"Missing"
2021.emnlp-main.350,W18-3604,0,0.0224136,"2019 build a Transformerbased language model and pre-train the model on the concatenated bilingual parallel sentences. Text-to-UD and UD-to-Text In UD parsing, graph-based models are widely used (Dozat et al., 2017; Straka, 2018; Qi et al., 2018). Besides, many works (Smith et al., 2018; Kulmizev et al., 2019; Grünewald et al., 2020) attempt to make use of contextual embeddings such as ELMO (Peters et al., 2018), Bert (Devlin et al., 2019b), XLM-R (Conneau et al., 2020) and so on. UD-to-text task is introduced in Surface Realisation Shared Task (Mille et al., 2018, 2019, 2020). Several works (Ferreira et al., 2018; Castro Ferreira and Krahmer, 2019; Elder, 2020; Farahnak et al., 2020) first linearize UD trees without word reordering and then feed the linearized trees to the sequence-to-sequence models or statistical machine translation models to generate texts. Others (Cabezudo and Pardo, 2018; Yu et al., 2019; Recski et al., 2020; Yu et al., 2020) reorder the word in UD trees with neural models first, followed by word inflection. 9 Conclusions and Future Work In this work, we focus on pivot-based paraphrase generation. Previous works leverage language as As for AMR parsing, some previous works (Flani-"
2021.emnlp-main.350,P14-1134,0,0.031816,"Missing"
2021.emnlp-main.350,D18-1198,0,0.0197822,"UD generate the same sentence with the original sentence. Pipeline-language is the only model that fails to preserve semantics, due to the error propagation of machine translation systems. Table 5 is another example from Parabank. It reveals that Pipeline-AMR tends to paraphrase texts syntactically, while Pipeline-language tends to paraphrase texts by replacing words with their synonyms (e.g. tried and condemned). 2019a) first project words to AMR concepts and then identify the relations. Transition-based models are widely applied (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). Because of the rapid development of sequence-to-sequence model, many works leverage it to parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 20"
2021.emnlp-main.350,P19-1607,0,0.0255779,"(2020b) utilize pre-trained models and achieve better results. 8 8.3 • Compared to the Enc-dec method, PipelineAMR, E2E-language and E2E-AMR methods can generate paraphrases with similar fidelity, diversity and fluency scores, which indicates that parallel paraphrasing data may not be necessary for generating high-quality paraphrases. 7 8.1 Case Analysis Related Work Paraphrase Generation Recently, seq2seq-based methods have been widely used in the task of paraphrase generation and achieve state-of-the-art results. These models include Transformer-based (Prakash et al., 2016; Li et al., 2019; Kajiwara, 2019), Variational Autoencoder-based (Bowman et al., 2016; Shakeri and Sethy, 2019), Generative Adversarial Networks-based (Yang et al., 2019b; An and Liu, 2019; Cao and Wan, 2020), and Reinforcement Learning-based (Li et al., 2018) methods. Some translation-based models have also been proposed for paraphrase generation (Wieting and Gimpel, 2018; Mallinson et al., 2017; Wieting et al., 2017; Guo et al., 2019). Wieting et al. 2017 and Wieting and Gimpel 2018 select different languages as pivots to generate multiple and diverse paraphrase. Considering that two-step translation may incur semantic shif"
2021.emnlp-main.350,N06-1058,0,0.251644,"Missing"
2021.emnlp-main.350,W17-3204,0,0.0262932,"Missing"
2021.emnlp-main.350,P17-1014,0,0.0267162,"tems. Table 5 is another example from Parabank. It reveals that Pipeline-AMR tends to paraphrase texts syntactically, while Pipeline-language tends to paraphrase texts by replacing words with their synonyms (e.g. tried and condemned). 2019a) first project words to AMR concepts and then identify the relations. Transition-based models are widely applied (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). Because of the rapid development of sequence-to-sequence model, many works leverage it to parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019) employs GNNs to explicitly encode graph structures. Other approaches (Zhu et al., 2019; Cai and Lam, 2020b; Wang et al., 2020; Yao et al., 2020) encode AMR graph structures through se"
2021.emnlp-main.350,D19-1277,0,0.0235405,"Missing"
2021.emnlp-main.350,2020.findings-emnlp.288,0,0.0120387,"original sentence. Pipeline-language is the only model that fails to preserve semantics, due to the error propagation of machine translation systems. Table 5 is another example from Parabank. It reveals that Pipeline-AMR tends to paraphrase texts syntactically, while Pipeline-language tends to paraphrase texts by replacing words with their synonyms (e.g. tried and condemned). 2019a) first project words to AMR concepts and then identify the relations. Transition-based models are widely applied (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). Because of the rapid development of sequence-to-sequence model, many works leverage it to parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019) employs GNNs to explicitly encode gr"
2021.emnlp-main.350,D18-1421,0,0.0184524,"which indicates that parallel paraphrasing data may not be necessary for generating high-quality paraphrases. 7 8.1 Case Analysis Related Work Paraphrase Generation Recently, seq2seq-based methods have been widely used in the task of paraphrase generation and achieve state-of-the-art results. These models include Transformer-based (Prakash et al., 2016; Li et al., 2019; Kajiwara, 2019), Variational Autoencoder-based (Bowman et al., 2016; Shakeri and Sethy, 2019), Generative Adversarial Networks-based (Yang et al., 2019b; An and Liu, 2019; Cao and Wan, 2020), and Reinforcement Learning-based (Li et al., 2018) methods. Some translation-based models have also been proposed for paraphrase generation (Wieting and Gimpel, 2018; Mallinson et al., 2017; Wieting et al., 2017; Guo et al., 2019). Wieting et al. 2017 and Wieting and Gimpel 2018 select different languages as pivots to generate multiple and diverse paraphrase. Considering that two-step translation may incur semantic shift, Guo et al. 2019 build a Transformerbased language model and pre-train the model on the concatenated bilingual parallel sentences. Text-to-UD and UD-to-Text In UD parsing, graph-based models are widely used (Dozat et al., 201"
2021.emnlp-main.350,P19-1332,0,0.0200647,"and Mager et al. (2020b) utilize pre-trained models and achieve better results. 8 8.3 • Compared to the Enc-dec method, PipelineAMR, E2E-language and E2E-AMR methods can generate paraphrases with similar fidelity, diversity and fluency scores, which indicates that parallel paraphrasing data may not be necessary for generating high-quality paraphrases. 7 8.1 Case Analysis Related Work Paraphrase Generation Recently, seq2seq-based methods have been widely used in the task of paraphrase generation and achieve state-of-the-art results. These models include Transformer-based (Prakash et al., 2016; Li et al., 2019; Kajiwara, 2019), Variational Autoencoder-based (Bowman et al., 2016; Shakeri and Sethy, 2019), Generative Adversarial Networks-based (Yang et al., 2019b; An and Liu, 2019; Cao and Wan, 2020), and Reinforcement Learning-based (Li et al., 2018) methods. Some translation-based models have also been proposed for paraphrase generation (Wieting and Gimpel, 2018; Mallinson et al., 2017; Wieting et al., 2017; Guo et al., 2019). Wieting et al. 2017 and Wieting and Gimpel 2018 select different languages as pivots to generate multiple and diverse paraphrase. Considering that two-step translation may in"
2021.emnlp-main.350,D18-1264,0,0.0179862,"e-UD, LSR and E2E-UD generate the same sentence with the original sentence. Pipeline-language is the only model that fails to preserve semantics, due to the error propagation of machine translation systems. Table 5 is another example from Parabank. It reveals that Pipeline-AMR tends to paraphrase texts syntactically, while Pipeline-language tends to paraphrase texts by replacing words with their synonyms (e.g. tried and condemned). 2019a) first project words to AMR concepts and then identify the relations. Transition-based models are widely applied (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). Because of the rapid development of sequence-to-sequence model, many works leverage it to parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019;"
2021.emnlp-main.350,P18-1037,0,0.0124407,"20) first linearize UD trees without word reordering and then feed the linearized trees to the sequence-to-sequence models or statistical machine translation models to generate texts. Others (Cabezudo and Pardo, 2018; Yu et al., 2019; Recski et al., 2020; Yu et al., 2020) reorder the word in UD trees with neural models first, followed by word inflection. 9 Conclusions and Future Work In this work, we focus on pivot-based paraphrase generation. Previous works leverage language as As for AMR parsing, some previous works (Flani- the pivot, which may introduce semantic shift. In gan et al., 2014; Lyu and Titov, 2018; Zhang et al., this work, we explore whether we can use AMR 4262 8.2 Text-to-AMR and AMR-to-Text or UD as pivot. We also explore an end-to-end framework in a zero-shot way, using only parallel text-pivot data. Results of the automatic metrics and human evaluations show that AMR is a good choice of pivot, as AMR graphs preserve important words as concepts and thus preserve semantics. Moreover, replacing two-step pipeline process with the end-to-end framework is beneficial when language is the pivot, reducing the semantic change. Besides, some unsupervised pivot-based methods can perform as wel"
2021.emnlp-main.350,2020.acl-main.167,0,0.0293479,"Missing"
2021.emnlp-main.350,E17-1083,0,0.406529,"questions (Fader et al., 2013). Highkinds of pivots. Experimental results show that quality paraphrases for general domains are diffitaking AMR as pivot can obtain paraphrases cult to obtain in practice, which greatly restricts the with better quality than taking language as the pivot. The end-to-end framework can reapplication of these seq2seq models. duce semantic shift when language is used Benefiting from the rapid development of maas the pivot. Besides, several unsupervised chine translation technologies, pivot-based methpivot-based methods can generate paraphrases ods (Guo et al., 2019; Mallinson et al., 2017; Wiwith similar quality as the supervised encodereting et al., 2017) have been proposed for paradecoder model, which indicates that parallel phrase generation. Formally speaking, pivot-based data of paraphrases may not be necessary for paraphrase generation. methods generate the paraphrase by following P (Y |X) = P (Z|X)P (Y |Z), where Z denotes 1 Introduction the pivot of X. Existing pivot-based methods all choose Z as representations in a different language, Paraphrase generation is an important and challengtherefore the quality of the generated paraphrases ing task in the field of Natural"
2021.emnlp-main.350,P13-2017,0,0.0721532,"chine translation systems are sensitive to do1 The first two authors contributed equally to this pamain, and the quality of translating out-of-domain per. Codes are available at https://github.com/caoy1996/Pivotparaphrase. sentences can not be guaranteed. 4255 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4255–4268 c November 7–11, 2021. 2021 Association for Computational Linguistics In this paper, we explore the feasibility of using different pivots for pivot-based paraphrasing models, including syntactic representation (Universal Dependencies (McDonald et al., 2013), UD), semantic representation (Abstract Meaning Representation (Banarescu et al., 2013), AMR), and latent semantic representation (LSR). Compared with choosing other languages as pivot, choosing syntactic or semantic as pivot is a more direct way, and is less likely to incur semantic shift. Apart from pipeline pivot-based generation, we also investigate how much an end-to-end pivot-based model, which can produce paraphrases in a single step with the help of pivot, affects the quality of paraphrases. In the end-to-end framework, the model directly learns the paraphrasing probability P (Y |X) f"
2021.emnlp-main.350,2020.msr-1.1,0,0.0520669,"Missing"
2021.emnlp-main.350,2020.acl-demos.14,0,0.0124647,"end methods. Different from the supervised paraphrasing models, our model does not involve any explicit paraphrase sentences, so it needs to generate paraphrases in a ""zero-shot"" way. Inspired by recent work on cross-lingual transfer (Conneau and Lample, 2019), we propose a pretraining framework to endow the model with the ability of zero-shot paraphrasing. Besides, we also experiment using auto-encoder to generate paraphrases. In the auto-encoder model, the encoded latent semantic representation (LSR) can be considered as a kind of semantic pivot. 4.1 3.3 Pipeline-UD We apply Stanza toolkit (Qi et al., 2020) to obtain UD. Stanza is a pipeline system with tokenization, sentence and word segmentation, part-of-speech tagging, morphological features tagging, lemmatization and dependency parsing. We omit the model details here, which could be found in Qi et al. (2018). Towards End-to-End Paraphrase Generation LSR We train a Transformer-based auto-encoder model, and use the encoder to encode the input sentence. The dense representation, which is the output of the encoder and can be considered as the latent semantic representation, is then decoded back to a sentence by the decoder. 4.2 End-to-end Pivot-"
2021.emnlp-main.350,W18-3601,0,0.0235764,"ranslation may incur semantic shift, Guo et al. 2019 build a Transformerbased language model and pre-train the model on the concatenated bilingual parallel sentences. Text-to-UD and UD-to-Text In UD parsing, graph-based models are widely used (Dozat et al., 2017; Straka, 2018; Qi et al., 2018). Besides, many works (Smith et al., 2018; Kulmizev et al., 2019; Grünewald et al., 2020) attempt to make use of contextual embeddings such as ELMO (Peters et al., 2018), Bert (Devlin et al., 2019b), XLM-R (Conneau et al., 2020) and so on. UD-to-text task is introduced in Surface Realisation Shared Task (Mille et al., 2018, 2019, 2020). Several works (Ferreira et al., 2018; Castro Ferreira and Krahmer, 2019; Elder, 2020; Farahnak et al., 2020) first linearize UD trees without word reordering and then feed the linearized trees to the sequence-to-sequence models or statistical machine translation models to generate texts. Others (Cabezudo and Pardo, 2018; Yu et al., 2019; Recski et al., 2020; Yu et al., 2020) reorder the word in UD trees with neural models first, followed by word inflection. 9 Conclusions and Future Work In this work, we focus on pivot-based paraphrase generation. Previous works leverage language"
2021.emnlp-main.350,W04-3219,0,0.233056,"guistics, Peking University {caiyitao,yuecao,wanxiaojun}@pku.edu.cn Abstract Traditionally, paraphrase generation is usually implemented using ruled-based models (Fader Paraphrases refer to texts that convey the et al., 2014; Zhao et al., 2009), lexicon-based methsame meaning with different expression forms. ods (Bolshakov and Gelbukh, 2004; Kauchak and Pivot-based methods, also known as the roundBarzilay, 2006), grammar-based methods (Narayan trip translation, have shown promising results et al., 2016), statistical machine translation-based in generating high-quality paraphrases. Howmethods (Quirk et al., 2004; Zhao et al., 2008). ever, existing pivot-based methods all rely on language as the pivot, where large-scale, highWith the rapid development of deep learning techquality parallel bilingual texts are required. In niques, neural methods have shown great power in this paper, we explore the feasibility of using paraphrase generation and achieve state-of-the-art semantic and syntactic representations as the results (Gupta et al., 2018; Yang et al., 2019a). Neupivot for paraphrase generation. Concretely, ral paraphrase generation models usually follow the we transform a sentence into a variety of d"
2021.emnlp-main.350,D19-6301,0,0.0342507,"Missing"
2021.emnlp-main.350,W16-6625,0,0.0603966,"Missing"
2021.emnlp-main.350,P19-1451,0,0.01184,"me sentence with the original sentence. Pipeline-language is the only model that fails to preserve semantics, due to the error propagation of machine translation systems. Table 5 is another example from Parabank. It reveals that Pipeline-AMR tends to paraphrase texts syntactically, while Pipeline-language tends to paraphrase texts by replacing words with their synonyms (e.g. tried and condemned). 2019a) first project words to AMR concepts and then identify the relations. Transition-based models are widely applied (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). Because of the rapid development of sequence-to-sequence model, many works leverage it to parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019) employs GNNs to e"
2021.emnlp-main.350,N19-4009,0,0.0192052,"mework mentioned in Section 4. Besides, we also compare these unsupervised methods with a supervised encoder-decoder (Enc-dec) model based on Transformer, which is trained with parallel paraphrase pairs in the training set of ParaBank/Quora. By analyzing performance of these models, we want to examine (1) whether AMR or UD can serve as the pivot for paraphrase generation, (2) whether end-to-end framework can bring benefit to paraphrase generation, and (3) whether zero-shot methods can obtain paraphrase as good as the supervised model. 5.3 5.4 Implementation Details We use the fairseq toolkit (Ott et al., 2019) to implement Pipeline-language and all end-to-end models. We set the model hidden size, feed-forward hidden size to 512 and 2048 respectively, and set the number of heads, number of layers to 8 and 6 respectively. We use the Adam optimizer (Kingma and Ba, 2014) for training, and adopt the warm-up learning rate (Goyal et al., 2017) technique for the first 4,000 steps. 6 Results and Analysis The automatic evaluation results are shown in Table 1. The results of human evaluation on the Parabank and Quora test sets are shown in Table 2 and Table 3 respectively. We also calculate kappa coefficient"
2021.emnlp-main.350,N18-1202,0,0.0211055,"Wieting et al. 2017 and Wieting and Gimpel 2018 select different languages as pivots to generate multiple and diverse paraphrase. Considering that two-step translation may incur semantic shift, Guo et al. 2019 build a Transformerbased language model and pre-train the model on the concatenated bilingual parallel sentences. Text-to-UD and UD-to-Text In UD parsing, graph-based models are widely used (Dozat et al., 2017; Straka, 2018; Qi et al., 2018). Besides, many works (Smith et al., 2018; Kulmizev et al., 2019; Grünewald et al., 2020) attempt to make use of contextual embeddings such as ELMO (Peters et al., 2018), Bert (Devlin et al., 2019b), XLM-R (Conneau et al., 2020) and so on. UD-to-text task is introduced in Surface Realisation Shared Task (Mille et al., 2018, 2019, 2020). Several works (Ferreira et al., 2018; Castro Ferreira and Krahmer, 2019; Elder, 2020; Farahnak et al., 2020) first linearize UD trees without word reordering and then feed the linearized trees to the sequence-to-sequence models or statistical machine translation models to generate texts. Others (Cabezudo and Pardo, 2018; Yu et al., 2019; Recski et al., 2020; Yu et al., 2020) reorder the word in UD trees with neural models firs"
2021.emnlp-main.350,C16-1275,0,0.060431,"Missing"
2021.emnlp-main.350,K18-2016,0,0.0384102,", we propose a pretraining framework to endow the model with the ability of zero-shot paraphrasing. Besides, we also experiment using auto-encoder to generate paraphrases. In the auto-encoder model, the encoded latent semantic representation (LSR) can be considered as a kind of semantic pivot. 4.1 3.3 Pipeline-UD We apply Stanza toolkit (Qi et al., 2020) to obtain UD. Stanza is a pipeline system with tokenization, sentence and word segmentation, part-of-speech tagging, morphological features tagging, lemmatization and dependency parsing. We omit the model details here, which could be found in Qi et al. (2018). Towards End-to-End Paraphrase Generation LSR We train a Transformer-based auto-encoder model, and use the encoder to encode the input sentence. The dense representation, which is the output of the encoder and can be considered as the latent semantic representation, is then decoded back to a sentence by the decoder. 4.2 End-to-end Pivot-based Method (E2E-pivot) For E2E-pivot method, our framework contains We use the IMSurReal (Yu et al., 2019) to accom- only one encoder-decoder (transformer) model, plish the UD-to-text task. The model first linearizes which is learned from parallel text-to-pi"
2021.emnlp-main.350,2020.msr-1.2,0,0.0194405,"al., 2020) attempt to make use of contextual embeddings such as ELMO (Peters et al., 2018), Bert (Devlin et al., 2019b), XLM-R (Conneau et al., 2020) and so on. UD-to-text task is introduced in Surface Realisation Shared Task (Mille et al., 2018, 2019, 2020). Several works (Ferreira et al., 2018; Castro Ferreira and Krahmer, 2019; Elder, 2020; Farahnak et al., 2020) first linearize UD trees without word reordering and then feed the linearized trees to the sequence-to-sequence models or statistical machine translation models to generate texts. Others (Cabezudo and Pardo, 2018; Yu et al., 2019; Recski et al., 2020; Yu et al., 2020) reorder the word in UD trees with neural models first, followed by word inflection. 9 Conclusions and Future Work In this work, we focus on pivot-based paraphrase generation. Previous works leverage language as As for AMR parsing, some previous works (Flani- the pivot, which may introduce semantic shift. In gan et al., 2014; Lyu and Titov, 2018; Zhang et al., this work, we explore whether we can use AMR 4262 8.2 Text-to-AMR and AMR-to-Text or UD as pivot. We also explore an end-to-end framework in a zero-shot way, using only parallel text-pivot data. Results of the automatic"
2021.emnlp-main.350,D19-1314,0,0.0203426,"; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). Because of the rapid development of sequence-to-sequence model, many works leverage it to parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019) employs GNNs to explicitly encode graph structures. Other approaches (Zhu et al., 2019; Cai and Lam, 2020b; Wang et al., 2020; Yao et al., 2020) encode AMR graph structures through self-attention and Transformers. Ribeiro et al. (2020) and Mager et al. (2020b) utilize pre-trained models and achieve better results. 8 8.3 • Compared to the Enc-dec method, PipelineAMR, E2E-language and E2E-AMR methods can generate paraphrases with similar fidelity, diversity and fluency scores, which indicates that parallel paraphrasing data may not be necessary for generating high-quality paraphrases. 7 8.1 Cas"
2021.emnlp-main.350,D18-1291,0,0.022021,"Missing"
2021.emnlp-main.350,P18-1150,0,0.0212878,"d (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). Because of the rapid development of sequence-to-sequence model, many works leverage it to parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019) employs GNNs to explicitly encode graph structures. Other approaches (Zhu et al., 2019; Cai and Lam, 2020b; Wang et al., 2020; Yao et al., 2020) encode AMR graph structures through self-attention and Transformers. Ribeiro et al. (2020) and Mager et al. (2020b) utilize pre-trained models and achieve better results. 8 8.3 • Compared to the Enc-dec method, PipelineAMR, E2E-language and E2E-AMR methods can generate paraphrases with similar fidelity, diversity and fluency scores, which indicates that parallel paraphrasing data may"
2021.emnlp-main.350,P15-2141,0,0.0235674,"tioned in section 5.2. In this case, Pipeline-UD, LSR and E2E-UD generate the same sentence with the original sentence. Pipeline-language is the only model that fails to preserve semantics, due to the error propagation of machine translation systems. Table 5 is another example from Parabank. It reveals that Pipeline-AMR tends to paraphrase texts syntactically, while Pipeline-language tends to paraphrase texts by replacing words with their synonyms (e.g. tried and condemned). 2019a) first project words to AMR concepts and then identify the relations. Transition-based models are widely applied (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). Because of the rapid development of sequence-to-sequence model, many works leverage it to parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; B"
2021.emnlp-main.350,2020.emnlp-main.196,0,0.130922,"UDSK 8'7UHH 7UDQVIRUPHU VKDUHZHLJKWV /DQJXDJH $05*UDSK 8'7UHH 7UDQVIRUPHU 7H[W 7H[W 7UDQVIRUPHU 7H[W /DQJXDJH0RGHOLQJWDVN E D Figure 1: (a) Pipeline pivot-based paraphrase generation. (b) Left: training stage of end-to-end pivot-based paraphrase generation. Right: inference stage of end-to-end pivot-based paraphrase generation. (Vaswani et al., 2017). The English sentences are first translated into German and then translated back into English. The sentences in German are regarded as the pivot. 3.2 Pipeline-AMR When parsing texts to AMRs, we employ one of the state-of-the-art AMR parser (Xu et al., 2020). This is a sequence-to-sequence model, since AMR graphs are first linearized. Machine translation and constituent parsing are introduced as auxiliary tasks when training the model. Researchers first generate AMR graphs automatically with an existing AMR parser and construct a larger silver dataset. The seq2seq model is first trained on the silver dataset and fine-tuned on the gold dataset. As for generating texts from AMRs, we choose the graph-to-text model (Ribeiro et al., 2020). This model is based on T5 (Raffel et al., 2020). It is first trained on a larger task-specific silver dataset and"
2021.emnlp-main.350,P16-1049,0,0.0185635,"Missing"
2021.emnlp-main.350,D19-1309,0,0.333761,"ranslation, have shown promising results et al., 2016), statistical machine translation-based in generating high-quality paraphrases. Howmethods (Quirk et al., 2004; Zhao et al., 2008). ever, existing pivot-based methods all rely on language as the pivot, where large-scale, highWith the rapid development of deep learning techquality parallel bilingual texts are required. In niques, neural methods have shown great power in this paper, we explore the feasibility of using paraphrase generation and achieve state-of-the-art semantic and syntactic representations as the results (Gupta et al., 2018; Yang et al., 2019a). Neupivot for paraphrase generation. Concretely, ral paraphrase generation models usually follow the we transform a sentence into a variety of differencoder-decoder paradigm. Given a sentence X, ent semantic or syntactic representations (inthese models generate the paraphrase Y by directly cluding AMR, UD, and latent semantic repremodeling P (Y |X) through a deep neural network. sentation), and then decode the sentence back from the semantic representations. We furHowever, deep neural networks are sensitive to dother explore a pretraining-based approach to mains in general (Stahlberg, 2020)"
2021.emnlp-main.350,N15-1040,0,0.028495,"tioned in section 5.2. In this case, Pipeline-UD, LSR and E2E-UD generate the same sentence with the original sentence. Pipeline-language is the only model that fails to preserve semantics, due to the error propagation of machine translation systems. Table 5 is another example from Parabank. It reveals that Pipeline-AMR tends to paraphrase texts syntactically, while Pipeline-language tends to paraphrase texts by replacing words with their synonyms (e.g. tried and condemned). 2019a) first project words to AMR concepts and then identify the relations. Transition-based models are widely applied (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). Because of the rapid development of sequence-to-sequence model, many works leverage it to parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; B"
2021.emnlp-main.350,2020.acl-main.640,1,0.835197,"parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019) employs GNNs to explicitly encode graph structures. Other approaches (Zhu et al., 2019; Cai and Lam, 2020b; Wang et al., 2020; Yao et al., 2020) encode AMR graph structures through self-attention and Transformers. Ribeiro et al. (2020) and Mager et al. (2020b) utilize pre-trained models and achieve better results. 8 8.3 • Compared to the Enc-dec method, PipelineAMR, E2E-language and E2E-AMR methods can generate paraphrases with similar fidelity, diversity and fluency scores, which indicates that parallel paraphrasing data may not be necessary for generating high-quality paraphrases. 7 8.1 Case Analysis Related Work Paraphrase Generation Recently, seq2seq-based methods have been widely used in the task of paraphrase generation and achi"
2021.emnlp-main.350,2020.tacl-1.2,1,0.834235,"orks leverage it to parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019) employs GNNs to explicitly encode graph structures. Other approaches (Zhu et al., 2019; Cai and Lam, 2020b; Wang et al., 2020; Yao et al., 2020) encode AMR graph structures through self-attention and Transformers. Ribeiro et al. (2020) and Mager et al. (2020b) utilize pre-trained models and achieve better results. 8 8.3 • Compared to the Enc-dec method, PipelineAMR, E2E-language and E2E-AMR methods can generate paraphrases with similar fidelity, diversity and fluency scores, which indicates that parallel paraphrasing data may not be necessary for generating high-quality paraphrases. 7 8.1 Case Analysis Related Work Paraphrase Generation Recently, seq2seq-based methods have been widely used in the task of paraphrase"
2021.emnlp-main.350,P18-1042,0,0.137047,"of this paper are as follows: • We explore to use syntactic and semantic representations as pivots for pivot-based paraphrasing models, which is a more direct way and less likely to incur semantic shift. • We also investigate applying an end-to-end paraphrasing model instead of the pipeline framework. • We conduct experiments on two paraphrasing datasets to detailedly investigate the pros and cons of models using different pivots. not be essential to generate high-quality paraphrases. 2 2.1 Introduction of Pivots Language Using language as the pivot has been widely explored by previous works (Wieting and Gimpel, 2018; Mallinson et al., 2017; Wieting et al., 2017; Guo et al., 2019). There are hundreds of languages in the world, and a sentence has different expressions in different languages. Therefore, we can take the sentence representation in another language as the pivot. 2.2 Abstract Meaning Representation (AMR) Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, labeled, acyclic graph which abstracts away from syntax and preserves semantics. Nodes in AMR graph are concepts, which are highly related to English words. Edges represent semantic relations between concepts. Since AMR"
2021.emnlp-main.350,D17-1026,0,0.14674,"syntactic and semantic representations as pivots for pivot-based paraphrasing models, which is a more direct way and less likely to incur semantic shift. • We also investigate applying an end-to-end paraphrasing model instead of the pipeline framework. • We conduct experiments on two paraphrasing datasets to detailedly investigate the pros and cons of models using different pivots. not be essential to generate high-quality paraphrases. 2 2.1 Introduction of Pivots Language Using language as the pivot has been widely explored by previous works (Wieting and Gimpel, 2018; Mallinson et al., 2017; Wieting et al., 2017; Guo et al., 2019). There are hundreds of languages in the world, and a sentence has different expressions in different languages. Therefore, we can take the sentence representation in another language as the pivot. 2.2 Abstract Meaning Representation (AMR) Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, labeled, acyclic graph which abstracts away from syntax and preserves semantics. Nodes in AMR graph are concepts, which are highly related to English words. Edges represent semantic relations between concepts. Since AMR only keeps semantic information, paraphrases"
2021.emnlp-main.350,D19-6306,0,0.0836194,", part-of-speech tagging, morphological features tagging, lemmatization and dependency parsing. We omit the model details here, which could be found in Qi et al. (2018). Towards End-to-End Paraphrase Generation LSR We train a Transformer-based auto-encoder model, and use the encoder to encode the input sentence. The dense representation, which is the output of the encoder and can be considered as the latent semantic representation, is then decoded back to a sentence by the decoder. 4.2 End-to-end Pivot-based Method (E2E-pivot) For E2E-pivot method, our framework contains We use the IMSurReal (Yu et al., 2019) to accom- only one encoder-decoder (transformer) model, plish the UD-to-text task. The model first linearizes which is learned from parallel text-to-pivot distrithe UD trees and then inflects the lemmas into word bution P (Z|X), pivot-to-text distribution P (Y |Z), forms. At last, the model contracts the tokenized prior text distribution P (X), P (Y ), and prior pivot word into one token. distribution P (Z). At the inference time, given an 4257 input sentence, we guide the model to produce the output in text form again, which is then considered as the paraphrase of the input. The model archit"
2021.emnlp-main.350,2020.msr-1.4,0,0.0233218,"make use of contextual embeddings such as ELMO (Peters et al., 2018), Bert (Devlin et al., 2019b), XLM-R (Conneau et al., 2020) and so on. UD-to-text task is introduced in Surface Realisation Shared Task (Mille et al., 2018, 2019, 2020). Several works (Ferreira et al., 2018; Castro Ferreira and Krahmer, 2019; Elder, 2020; Farahnak et al., 2020) first linearize UD trees without word reordering and then feed the linearized trees to the sequence-to-sequence models or statistical machine translation models to generate texts. Others (Cabezudo and Pardo, 2018; Yu et al., 2019; Recski et al., 2020; Yu et al., 2020) reorder the word in UD trees with neural models first, followed by word inflection. 9 Conclusions and Future Work In this work, we focus on pivot-based paraphrase generation. Previous works leverage language as As for AMR parsing, some previous works (Flani- the pivot, which may introduce semantic shift. In gan et al., 2014; Lyu and Titov, 2018; Zhang et al., this work, we explore whether we can use AMR 4262 8.2 Text-to-AMR and AMR-to-Text or UD as pivot. We also explore an end-to-end framework in a zero-shot way, using only parallel text-pivot data. Results of the automatic metrics and human"
2021.emnlp-main.350,P19-1009,0,0.0403956,"Missing"
2021.emnlp-main.350,D19-1392,0,0.0344516,"Missing"
2021.emnlp-main.350,P09-1094,0,0.128117,"Missing"
2021.emnlp-main.350,P08-1116,0,0.0636296,"versity {caiyitao,yuecao,wanxiaojun}@pku.edu.cn Abstract Traditionally, paraphrase generation is usually implemented using ruled-based models (Fader Paraphrases refer to texts that convey the et al., 2014; Zhao et al., 2009), lexicon-based methsame meaning with different expression forms. ods (Bolshakov and Gelbukh, 2004; Kauchak and Pivot-based methods, also known as the roundBarzilay, 2006), grammar-based methods (Narayan trip translation, have shown promising results et al., 2016), statistical machine translation-based in generating high-quality paraphrases. Howmethods (Quirk et al., 2004; Zhao et al., 2008). ever, existing pivot-based methods all rely on language as the pivot, where large-scale, highWith the rapid development of deep learning techquality parallel bilingual texts are required. In niques, neural methods have shown great power in this paper, we explore the feasibility of using paraphrase generation and achieve state-of-the-art semantic and syntactic representations as the results (Gupta et al., 2018; Yang et al., 2019a). Neupivot for paraphrase generation. Concretely, ral paraphrase generation models usually follow the we transform a sentence into a variety of differencoder-decoder"
2021.emnlp-main.350,D19-1548,0,0.0234286,"of sequence-to-sequence model, many works leverage it to parse texts into AMRs. Some works (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020) linearize AMR graphs and directly use sequenceto-sequence models. Others (Zhang et al., 2019b; Cai and Lam, 2020a) use sequence-to-sequence model to predict concepts. They also jointly train the model to identify the relations. In AMR-to-text generation, recent methods (Song et al., 2018; Beck et al., 2018; Damonte and Cohen, 2019; Ribeiro et al., 2019) employs GNNs to explicitly encode graph structures. Other approaches (Zhu et al., 2019; Cai and Lam, 2020b; Wang et al., 2020; Yao et al., 2020) encode AMR graph structures through self-attention and Transformers. Ribeiro et al. (2020) and Mager et al. (2020b) utilize pre-trained models and achieve better results. 8 8.3 • Compared to the Enc-dec method, PipelineAMR, E2E-language and E2E-AMR methods can generate paraphrases with similar fidelity, diversity and fluency scores, which indicates that parallel paraphrasing data may not be necessary for generating high-quality paraphrases. 7 8.1 Case Analysis Related Work Paraphrase Generation Recently, seq2seq-based methods have been"
2021.emnlp-main.630,C18-1039,0,0.0224653,"sentences separately. Besides, sentences that are obscure and have little relation to the subject should be deleted instead of simplified. Therefore, studying documentlevel text simplification may be more meaningful than studying sentence-level text simplification alone. Unfortunately, the research on documentlevel text simplification is still scarce: there is no formal definition, no suitable dataset, and evaluation criteria. 1.2 Other tasks that may be related to document-level text simplification are text summarization(Dong et al., 2018; Cao et al., 2020), paraphrasing (Zhao et al., 2018; Guo et al., 2018), and split & rephrase (Narayan et al., 2017; Surya et al., 2019). Obviously, the paraphrasing and split & rephrase tasks are both sentence-level tasks. The most closely related task is text summarization, which is also a document-level task. We use an example to illustrate the difference between text summarization and our task, as shown in Table 1. We can see that text summarization does not involve rewriting text with simplified versions, though both the two tasks may filter or delete some unimportant text from the original document. 1.3 1.1 Why it is Valuable to Study Document-level Text Si"
2021.emnlp-main.630,P14-1041,0,0.0668551,"Missing"
2021.findings-acl.134,W13-2322,0,0.295503,"gual input, namely the translated texts as well as non-English texts, in order to enable the model to predict more accurate concepts. Besides, we also introduce an auxiliary task, requiring the decoder to predict the English sequences at the same time. The auxiliary task can help the decoder understand what exactly the corresponding English tokens are. Our proposed cross-lingual AMR parser surpasses previous state-of-the-art parser by 10.6 points on Smatch F1 score. The ablation study also demonstrates the efficacy of our proposed modules. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, labeled, acyclic graph representing sentence-level semantic of text. Nodes in the graph are concepts in the texts and edges in the graph are relations between concepts. Since AMR abstracts away from syntax and preserves only semantic information, it can be applied to many semantic related tasks such as summarization (Liu et al., 2015; Liao et al., 2018), paraphrase detection (Issa et al., 2018), machine translation (Song et al., 2019) and so on. Previous works on AMR parsing mainly focus on English, since AMR is designed for English texts and parallel corpus of non-English texts"
2021.findings-acl.134,2020.emnlp-main.195,0,0.0911022,"ummarization (Liu et al., 2015; Liao et al., 2018), paraphrase detection (Issa et al., 2018), machine translation (Song et al., 2019) and so on. Previous works on AMR parsing mainly focus on English, since AMR is designed for English texts and parallel corpus of non-English texts and AMRs are scarce. Early work of AMR announces that AMR is biased towards English and is not an interlingua (Banarescu et al., 2013). Besides, some studies show that aligning AMR with nonEnglish language is not always possible (Xue et al., 2014; Hajic et al., 2014). However, recent studies (Damonte and Cohen, 2018; Blloshmi et al., 2020) show that AMR parsers are able to recover AMR structures when there are structural differences between languages, which demonstrate that it is capable to overcome many translation divergences. Therefore, it is possible for us to parse texts in target (non-English) languages into AMRs. Another problem of cross-lingual AMR parsing is the scarcity of parallel corpus. Unlike machine translation or sentiment classification which have abundant resources on the Internet, we can only get non-English text and AMR pairs by human annotation. Damonte and Cohen (2018) align a non-English token with an AMR"
2021.findings-acl.134,2020.acl-main.119,0,0.0134723,"n sequence-to-sequence model, several works employ it to parse texts into AMRs (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020). They linearize AMR graphs and leverage character-level or word-level sequence-to-sequence model. Sequence-to-graph model is proposed to enable the decoder to better model the graph structure. Zhang et al. (2019a) first use a sequence-to-sequence model to predict concepts and use a biaffine classifier to predict edges. Zhang et al. (2019b) propose a one-stage sequence-to-graph model, predicting concepts and relations at the same time. Cai and Lam (2020) regard AMR parsing as dual decisions on input sequences and constructing graphs. They therefore propose a sequence-to-graph method by first mapping an input words to a concept and then linking an edge based on the generated concepts. Recently, pre-training models have been proved to perform well in AMR parsing (Xu et al., 2020). Lee et al. (2020) employ a self-training method to enhance a transition-based parser, which achieves the state-ofthe-art Smatch F1 score in English AMR parsing. Vanderwende et al. (2015) first carry out research of cross-lingual AMR parsing. They parse texts in target"
2021.findings-acl.134,P13-2131,0,0.192593,"concatenation of translated English texts and target language texts. We hope that our model can predict more accurate concepts with the help of the English tokens, while it can still preserve the meaning of the original texts if there are semantic shifts in the translated English texts. Besides, during training process, we also introduce an auxiliary task, requiring the decoder to restore English input tokens, which also aims at enhancing the ability of our parser to predict concepts. Our parser outperforms previous state-of-the-art parser XL-AMR (Blloshmi et al., 2020) on LDC2020T07 dataset (Cai and Knight, 2013) by about 10.6 points of Smatch F1 score on average, which demonstrates the efficacy of our proposed cross-lingual AMR parser. Our main contributions are summarized as follows: • We introduce bilingual inputs and an auxiliary task to a seq2seq cross-lingual AMR parser, aiming to enable the parser to make better use of bilingual information and predict more accurate concepts. • Our parser surpasses the best previously reported results of Smatch F1 score on LDC2020T07 by a large margin. The results demonstrate the effectiveness of our parser. Ablation studies show the usefulness of the model mod"
2021.findings-acl.134,2020.acl-main.747,0,0.0561361,"Missing"
2021.findings-acl.134,N18-1104,0,0.326289,"c related tasks such as summarization (Liu et al., 2015; Liao et al., 2018), paraphrase detection (Issa et al., 2018), machine translation (Song et al., 2019) and so on. Previous works on AMR parsing mainly focus on English, since AMR is designed for English texts and parallel corpus of non-English texts and AMRs are scarce. Early work of AMR announces that AMR is biased towards English and is not an interlingua (Banarescu et al., 2013). Besides, some studies show that aligning AMR with nonEnglish language is not always possible (Xue et al., 2014; Hajic et al., 2014). However, recent studies (Damonte and Cohen, 2018; Blloshmi et al., 2020) show that AMR parsers are able to recover AMR structures when there are structural differences between languages, which demonstrate that it is capable to overcome many translation divergences. Therefore, it is possible for us to parse texts in target (non-English) languages into AMRs. Another problem of cross-lingual AMR parsing is the scarcity of parallel corpus. Unlike machine translation or sentiment classification which have abundant resources on the Internet, we can only get non-English text and AMR pairs by human annotation. Damonte and Cohen (2018) align a non-E"
2021.findings-acl.134,E17-1051,0,0.200054,". • We further carry out experiments to investigate the influence of incorporating pretraining models into our cross-lingual AMR parser. 2 Related Work Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is becoming popular recently. Some of previous works (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) 1 https://github.com/headacheboy/cross-lingual-amrparsing solve this problem with a two-stage approach. They first project words in sentences to AMR concepts, followed by relation identification. Transitionbased parsing is applied by (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). They align words with AMR concepts and then take different actions based on different processed words to link edges or insert new nodes. Due to the recent development in sequence-to-sequence model, several works employ it to parse texts into AMRs (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020). They linearize AMR graphs and leverage character-level or word-level sequence-to-sequence model. Sequence-to-graph model is proposed to enable the decoder to better model the graph structure."
2021.findings-acl.134,N19-1423,0,0.00605204,"ce model, the bilingual input can improve the Smatch F1 score by 4.4 points on average. The introduction of auxiliary task brings 5.5 points improvement of Smatch on average. Our full model makes use of both bilingual input and auxiliary task at the same time, improving Smatch scores by 10.2 points, which indicates that each module is very beneficial to the performance of our model. Fine-grained results further demonstrate the efEffect of Pre-trained Models on Cross-Lingual AMR Parsing Recently, pre-training models on cross-lingual tasks have been proposed. Pre-training models, such as mBert (Devlin et al., 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al., 2020b) and mBart (Liu et al., 2020) achieve stateof-the-art results on many tasks such as machine translation, cross-lingual natural language inference and so on. In our experiment, we exploit XLM-R (Conneau et al., 2020b) as the input embeddings of the model. When training an English AMR parser, Xu et al. (2020) first pre-train the model on large scale synthetic data and fine-tune it on gold English-AMR data. Since cross-lingual AMR parsing shares the same output formats with AMR parsing, we can employ the decoder of (Xu et al., 2020) t"
2021.findings-acl.134,P14-1134,0,0.0239247,"use of bilingual information and predict more accurate concepts. • Our parser surpasses the best previously reported results of Smatch F1 score on LDC2020T07 by a large margin. The results demonstrate the effectiveness of our parser. Ablation studies show the usefulness of the model modules. Codes are public available 1 . • We further carry out experiments to investigate the influence of incorporating pretraining models into our cross-lingual AMR parser. 2 Related Work Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is becoming popular recently. Some of previous works (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) 1 https://github.com/headacheboy/cross-lingual-amrparsing solve this problem with a two-stage approach. They first project words in sentences to AMR concepts, followed by relation identification. Transitionbased parsing is applied by (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). They align words with AMR concepts and then take different actions based on different processed words to link edges or insert new nodes. Due to the recent development in sequence-to-sequence model, sev"
2021.findings-acl.134,D18-1198,0,0.0180283,"investigate the influence of incorporating pretraining models into our cross-lingual AMR parser. 2 Related Work Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is becoming popular recently. Some of previous works (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) 1 https://github.com/headacheboy/cross-lingual-amrparsing solve this problem with a two-stage approach. They first project words in sentences to AMR concepts, followed by relation identification. Transitionbased parsing is applied by (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). They align words with AMR concepts and then take different actions based on different processed words to link edges or insert new nodes. Due to the recent development in sequence-to-sequence model, several works employ it to parse texts into AMRs (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020). They linearize AMR graphs and leverage character-level or word-level sequence-to-sequence model. Sequence-to-graph model is proposed to enable the decoder to better model the graph structure. Zhang et al. (2019a) first use a seq"
2021.findings-acl.134,W14-5808,0,0.0227631,"information, it can be applied to many semantic related tasks such as summarization (Liu et al., 2015; Liao et al., 2018), paraphrase detection (Issa et al., 2018), machine translation (Song et al., 2019) and so on. Previous works on AMR parsing mainly focus on English, since AMR is designed for English texts and parallel corpus of non-English texts and AMRs are scarce. Early work of AMR announces that AMR is biased towards English and is not an interlingua (Banarescu et al., 2013). Besides, some studies show that aligning AMR with nonEnglish language is not always possible (Xue et al., 2014; Hajic et al., 2014). However, recent studies (Damonte and Cohen, 2018; Blloshmi et al., 2020) show that AMR parsers are able to recover AMR structures when there are structural differences between languages, which demonstrate that it is capable to overcome many translation divergences. Therefore, it is possible for us to parse texts in target (non-English) languages into AMRs. Another problem of cross-lingual AMR parsing is the scarcity of parallel corpus. Unlike machine translation or sentiment classification which have abundant resources on the Internet, we can only get non-English text and AMR pairs by human"
2021.findings-acl.134,N18-1041,0,0.0152177,"e-of-the-art parser by 10.6 points on Smatch F1 score. The ablation study also demonstrates the efficacy of our proposed modules. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, labeled, acyclic graph representing sentence-level semantic of text. Nodes in the graph are concepts in the texts and edges in the graph are relations between concepts. Since AMR abstracts away from syntax and preserves only semantic information, it can be applied to many semantic related tasks such as summarization (Liu et al., 2015; Liao et al., 2018), paraphrase detection (Issa et al., 2018), machine translation (Song et al., 2019) and so on. Previous works on AMR parsing mainly focus on English, since AMR is designed for English texts and parallel corpus of non-English texts and AMRs are scarce. Early work of AMR announces that AMR is biased towards English and is not an interlingua (Banarescu et al., 2013). Besides, some studies show that aligning AMR with nonEnglish language is not always possible (Xue et al., 2014; Hajic et al., 2014). However, recent studies (Damonte and Cohen, 2018; Blloshmi et al., 2020) show that AMR parsers are able to recover AMR structures when there a"
2021.findings-acl.134,P17-1014,0,0.0189473,"ttps://github.com/headacheboy/cross-lingual-amrparsing solve this problem with a two-stage approach. They first project words in sentences to AMR concepts, followed by relation identification. Transitionbased parsing is applied by (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). They align words with AMR concepts and then take different actions based on different processed words to link edges or insert new nodes. Due to the recent development in sequence-to-sequence model, several works employ it to parse texts into AMRs (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020). They linearize AMR graphs and leverage character-level or word-level sequence-to-sequence model. Sequence-to-graph model is proposed to enable the decoder to better model the graph structure. Zhang et al. (2019a) first use a sequence-to-sequence model to predict concepts and use a biaffine classifier to predict edges. Zhang et al. (2019b) propose a one-stage sequence-to-graph model, predicting concepts and relations at the same time. Cai and Lam (2020) regard AMR parsing as dual decisions on input sequences and constructing graphs."
2021.findings-acl.134,2020.findings-emnlp.288,0,0.0188362,"ting pretraining models into our cross-lingual AMR parser. 2 Related Work Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is becoming popular recently. Some of previous works (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) 1 https://github.com/headacheboy/cross-lingual-amrparsing solve this problem with a two-stage approach. They first project words in sentences to AMR concepts, followed by relation identification. Transitionbased parsing is applied by (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). They align words with AMR concepts and then take different actions based on different processed words to link edges or insert new nodes. Due to the recent development in sequence-to-sequence model, several works employ it to parse texts into AMRs (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020). They linearize AMR graphs and leverage character-level or word-level sequence-to-sequence model. Sequence-to-graph model is proposed to enable the decoder to better model the graph structure. Zhang et al. (2019a) first use a sequence-to-sequence model to predict conce"
2021.findings-acl.134,C18-1101,0,0.0117502,"lingual AMR parser surpasses previous state-of-the-art parser by 10.6 points on Smatch F1 score. The ablation study also demonstrates the efficacy of our proposed modules. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, labeled, acyclic graph representing sentence-level semantic of text. Nodes in the graph are concepts in the texts and edges in the graph are relations between concepts. Since AMR abstracts away from syntax and preserves only semantic information, it can be applied to many semantic related tasks such as summarization (Liu et al., 2015; Liao et al., 2018), paraphrase detection (Issa et al., 2018), machine translation (Song et al., 2019) and so on. Previous works on AMR parsing mainly focus on English, since AMR is designed for English texts and parallel corpus of non-English texts and AMRs are scarce. Early work of AMR announces that AMR is biased towards English and is not an interlingua (Banarescu et al., 2013). Besides, some studies show that aligning AMR with nonEnglish language is not always possible (Xue et al., 2014; Hajic et al., 2014). However, recent studies (Damonte and Cohen, 2018; Blloshmi et al., 2020) show that AMR parsers are a"
2021.findings-acl.134,N15-1114,0,0.0207141,"ur proposed cross-lingual AMR parser surpasses previous state-of-the-art parser by 10.6 points on Smatch F1 score. The ablation study also demonstrates the efficacy of our proposed modules. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, labeled, acyclic graph representing sentence-level semantic of text. Nodes in the graph are concepts in the texts and edges in the graph are relations between concepts. Since AMR abstracts away from syntax and preserves only semantic information, it can be applied to many semantic related tasks such as summarization (Liu et al., 2015; Liao et al., 2018), paraphrase detection (Issa et al., 2018), machine translation (Song et al., 2019) and so on. Previous works on AMR parsing mainly focus on English, since AMR is designed for English texts and parallel corpus of non-English texts and AMRs are scarce. Early work of AMR announces that AMR is biased towards English and is not an interlingua (Banarescu et al., 2013). Besides, some studies show that aligning AMR with nonEnglish language is not always possible (Xue et al., 2014; Hajic et al., 2014). However, recent studies (Damonte and Cohen, 2018; Blloshmi et al., 2020) show th"
2021.findings-acl.134,P16-1162,0,0.0148368,"Missing"
2021.findings-acl.134,Q19-1002,0,0.0151398,"tch F1 score. The ablation study also demonstrates the efficacy of our proposed modules. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rooted, labeled, acyclic graph representing sentence-level semantic of text. Nodes in the graph are concepts in the texts and edges in the graph are relations between concepts. Since AMR abstracts away from syntax and preserves only semantic information, it can be applied to many semantic related tasks such as summarization (Liu et al., 2015; Liao et al., 2018), paraphrase detection (Issa et al., 2018), machine translation (Song et al., 2019) and so on. Previous works on AMR parsing mainly focus on English, since AMR is designed for English texts and parallel corpus of non-English texts and AMRs are scarce. Early work of AMR announces that AMR is biased towards English and is not an interlingua (Banarescu et al., 2013). Besides, some studies show that aligning AMR with nonEnglish language is not always possible (Xue et al., 2014; Hajic et al., 2014). However, recent studies (Damonte and Cohen, 2018; Blloshmi et al., 2020) show that AMR parsers are able to recover AMR structures when there are structural differences between languag"
2021.findings-acl.134,N15-3006,0,0.0230707,"a one-stage sequence-to-graph model, predicting concepts and relations at the same time. Cai and Lam (2020) regard AMR parsing as dual decisions on input sequences and constructing graphs. They therefore propose a sequence-to-graph method by first mapping an input words to a concept and then linking an edge based on the generated concepts. Recently, pre-training models have been proved to perform well in AMR parsing (Xu et al., 2020). Lee et al. (2020) employ a self-training method to enhance a transition-based parser, which achieves the state-ofthe-art Smatch F1 score in English AMR parsing. Vanderwende et al. (2015) first carry out research of cross-lingual AMR parsing. They parse texts in target language into logical forms as a pivot, which are then parsed into AMR graphs. Damonte and Cohen (2018) attempt to project non-English words to AMR concepts and use a transition-based parser to parse texts to AMR graphs. They also attempt to automatically translate non-English texts to English and exploit an English AMR parser. Blloshmi et al. (2020) try to generate synthetic training data by a machine translation system or an English AMR parser. They conduct experiments with a sequenceto-graph model in differen"
2021.findings-acl.134,D18-1264,0,0.0154723,"out experiments to investigate the influence of incorporating pretraining models into our cross-lingual AMR parser. 2 Related Work Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is becoming popular recently. Some of previous works (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) 1 https://github.com/headacheboy/cross-lingual-amrparsing solve this problem with a two-stage approach. They first project words in sentences to AMR concepts, followed by relation identification. Transitionbased parsing is applied by (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). They align words with AMR concepts and then take different actions based on different processed words to link edges or insert new nodes. Due to the recent development in sequence-to-sequence model, several works employ it to parse texts into AMRs (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020). They linearize AMR graphs and leverage character-level or word-level sequence-to-sequence model. Sequence-to-graph model is proposed to enable the decoder to better model the graph structure. Zhang et al. (2019"
2021.findings-acl.134,P15-2141,0,0.0206045,"are public available 1 . • We further carry out experiments to investigate the influence of incorporating pretraining models into our cross-lingual AMR parser. 2 Related Work Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is becoming popular recently. Some of previous works (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) 1 https://github.com/headacheboy/cross-lingual-amrparsing solve this problem with a two-stage approach. They first project words in sentences to AMR concepts, followed by relation identification. Transitionbased parsing is applied by (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). They align words with AMR concepts and then take different actions based on different processed words to link edges or insert new nodes. Due to the recent development in sequence-to-sequence model, several works employ it to parse texts into AMRs (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020). They linearize AMR graphs and leverage character-level or word-level sequence-to-sequence model. Sequence-to-graph model is proposed to enable the decoder to better mo"
2021.findings-acl.134,2020.tacl-1.47,0,0.0186009,"roduction of auxiliary task brings 5.5 points improvement of Smatch on average. Our full model makes use of both bilingual input and auxiliary task at the same time, improving Smatch scores by 10.2 points, which indicates that each module is very beneficial to the performance of our model. Fine-grained results further demonstrate the efEffect of Pre-trained Models on Cross-Lingual AMR Parsing Recently, pre-training models on cross-lingual tasks have been proposed. Pre-training models, such as mBert (Devlin et al., 2019), XLM (Conneau and Lample, 2019), XLM-R (Conneau et al., 2020b) and mBart (Liu et al., 2020) achieve stateof-the-art results on many tasks such as machine translation, cross-lingual natural language inference and so on. In our experiment, we exploit XLM-R (Conneau et al., 2020b) as the input embeddings of the model. When training an English AMR parser, Xu et al. (2020) first pre-train the model on large scale synthetic data and fine-tune it on gold English-AMR data. Since cross-lingual AMR parsing shares the same output formats with AMR parsing, we can employ the decoder of (Xu et al., 2020) to initialize our decoder and further finetune the cross-lingual AMR parser. Results are list"
2021.findings-acl.134,N15-1040,0,0.0233958,"are public available 1 . • We further carry out experiments to investigate the influence of incorporating pretraining models into our cross-lingual AMR parser. 2 Related Work Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is becoming popular recently. Some of previous works (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) 1 https://github.com/headacheboy/cross-lingual-amrparsing solve this problem with a two-stage approach. They first project words in sentences to AMR concepts, followed by relation identification. Transitionbased parsing is applied by (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). They align words with AMR concepts and then take different actions based on different processed words to link edges or insert new nodes. Due to the recent development in sequence-to-sequence model, several works employ it to parse texts into AMRs (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020). They linearize AMR graphs and leverage character-level or word-level sequence-to-sequence model. Sequence-to-graph model is proposed to enable the decoder to better mo"
2021.findings-acl.134,P18-1037,0,0.0127533,"mation and predict more accurate concepts. • Our parser surpasses the best previously reported results of Smatch F1 score on LDC2020T07 by a large margin. The results demonstrate the effectiveness of our parser. Ablation studies show the usefulness of the model modules. Codes are public available 1 . • We further carry out experiments to investigate the influence of incorporating pretraining models into our cross-lingual AMR parser. 2 Related Work Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is becoming popular recently. Some of previous works (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) 1 https://github.com/headacheboy/cross-lingual-amrparsing solve this problem with a two-stage approach. They first project words in sentences to AMR concepts, followed by relation identification. Transitionbased parsing is applied by (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). They align words with AMR concepts and then take different actions based on different processed words to link edges or insert new nodes. Due to the recent development in sequence-to-sequence model, several works employ it"
2021.findings-acl.134,2020.emnlp-main.196,0,0.271119,"enable our parser to predict more accurate concepts. In particular, we first build our training data similar to (Blloshmi et al., 2020), translating English texts into target languages. Our basic model is a sequence-to-sequence 1537 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1537–1547 August 1–6, 2021. ©2021 Association for Computational Linguistics model, rather than the sequence-to-graph model used in (Blloshmi et al., 2020), since in English AMR parsing, sequence-to-sequence models can achieve state-of-the-art result with enough data for pre-training (Xu et al., 2020). While training, we introduce bilingual input by concatenating translated target language texts and English texts as inputs. As for inference stage, the bilingual input is the concatenation of translated English texts and target language texts. We hope that our model can predict more accurate concepts with the help of the English tokens, while it can still preserve the meaning of the original texts if there are semantic shifts in the translated English texts. Besides, during training process, we also introduce an auxiliary task, requiring the decoder to restore English input tokens, which als"
2021.findings-acl.134,P19-1451,0,0.0125844,"nfluence of incorporating pretraining models into our cross-lingual AMR parser. 2 Related Work Abstract Meaning Representation (AMR) (Banarescu et al., 2013) parsing is becoming popular recently. Some of previous works (Flanigan et al., 2014; Lyu and Titov, 2018; Zhang et al., 2019a) 1 https://github.com/headacheboy/cross-lingual-amrparsing solve this problem with a two-stage approach. They first project words in sentences to AMR concepts, followed by relation identification. Transitionbased parsing is applied by (Wang et al., 2015b,a; Damonte et al., 2017; Liu et al., 2018; Guo and Lu, 2018; Naseem et al., 2019; Lee et al., 2020). They align words with AMR concepts and then take different actions based on different processed words to link edges or insert new nodes. Due to the recent development in sequence-to-sequence model, several works employ it to parse texts into AMRs (Konstas et al., 2017; van Noord and Bos, 2017; Ge et al., 2019; Xu et al., 2020). They linearize AMR graphs and leverage character-level or word-level sequence-to-sequence model. Sequence-to-graph model is proposed to enable the decoder to better model the graph structure. Zhang et al. (2019a) first use a sequence-to-sequence mod"
2021.findings-acl.134,xue-etal-2014-interlingua,0,0.0266125,"ves only semantic information, it can be applied to many semantic related tasks such as summarization (Liu et al., 2015; Liao et al., 2018), paraphrase detection (Issa et al., 2018), machine translation (Song et al., 2019) and so on. Previous works on AMR parsing mainly focus on English, since AMR is designed for English texts and parallel corpus of non-English texts and AMRs are scarce. Early work of AMR announces that AMR is biased towards English and is not an interlingua (Banarescu et al., 2013). Besides, some studies show that aligning AMR with nonEnglish language is not always possible (Xue et al., 2014; Hajic et al., 2014). However, recent studies (Damonte and Cohen, 2018; Blloshmi et al., 2020) show that AMR parsers are able to recover AMR structures when there are structural differences between languages, which demonstrate that it is capable to overcome many translation divergences. Therefore, it is possible for us to parse texts in target (non-English) languages into AMRs. Another problem of cross-lingual AMR parsing is the scarcity of parallel corpus. Unlike machine translation or sentiment classification which have abundant resources on the Internet, we can only get non-English text an"
2021.findings-acl.134,P19-1009,0,0.0257041,"Missing"
2021.findings-acl.134,D19-1392,0,0.0223102,"Missing"
2021.findings-acl.135,J13-3001,0,0.0282104,"akash et al. (2016) leveraged stacked residual LSTM networks to generate paraphrase, and Gupta et al. (2018) proposed a deep generative framework based on variational auto-encoder for paraphrase generation. Though paraphrase generation models based on Seq2Seq have demonstrated advanced ability, the generated paraphrase still has the problem of lack of diversity, i.e., the output paraphrase only makes trivial changes to the original sentence. A good paraphrase of a sentence is one that is semantically similar to that sentence while being (very) syntactically and/or lexically different from it (Bhagat and Hovy, 2013). Paraphrase which is too similar to the original sentence is much less useful in many real applications. In this paper, we focus on improving the diversity of generated paraphrase, i.e., making generated paraphrase different from the original sentence as much as possible. An intuitive but uninvestigated idea is to adopt multi-round paraphrase generation. Concretely, we first send the original sentence into a paraphrase generation model to generate a paraphrase, and then we use the generated paraphrase as the input of the model to generate a new paraphrase. As long as we leverage a paraphrase"
2021.findings-acl.135,K16-1002,0,0.318872,"s during multi-round generation. In addition, we introduce gumble-softmax embedding to tackle the problem that the model with sampling operation between different rounds’ generation can not be optimized by SGD optimizer. 3.2 Paraphrase Model We require sufficient diversity of paraphrase model so that it is able to introduce enough changes in the paraphrase of each round. The VAE (Kingma and Welling, 2013; Rezende et al., 2014) is a deep generative model that allows learning rich, nonlinear representations for high dimensional inputs. It can improve the diversity by sampling from latent space. Bowman et al. (2016) proposed a new model to apply VAE to natural language generation for the first time. Our paraphrase model is based on conditional VAE with LSTM. Transformer (Vaswani et al., 2017) has achieved excellent performance in many tasks. But our experiments show that it may cause KL divergence to become 0, called posterior collapse, which means a decrease of diversity. So we do not employ Transformer as encoder and decoder. We define the embedding matrices of Si and i P as Esi = {e1s , e2s , · · · , eL = s } and Ep j 1 2 M i d {ep , ep , · · · , ep } respectively, where es , ep ∈ R e are the embeddin"
2021.findings-acl.135,2020.findings-emnlp.218,1,0.894888,"lti-round paraphrase model, we only train the first two rounds. Because we find that training too many rounds requires large computing resources, but can not improve the model performance significantly. During inference, we can generate paraphrase more than two rounds. Experiment 4.1 Datasets We evaluate our BTmPG model on two benchmark datasets: MSCOCO2 (Lin et al., 2014) dataset contains human annotated captions of over 120k images. Each image contains five captions from five different annotators. This dataset has been widely used in previous works (Prakash et al., 2016; Gupta et al., 2018; Cao and Wan, 2020). We sample the MSCOCO according to Prakash et al. (2016). 2 Valid Set 3,000 3,000 Test Set 3,000 3,000 (b) We train paraphrase model together with backtranslation model. The total loss of our model is as follow: 4 Train Set 206,852 129,263 Table 1: Statistic for datasets: the sizes of training, validation and test set. Figure 2: Figure (a) shows the decoder with teacher forcing in the first round generation. Figure (b) shows the decoder with autoregression in other-round generation. 3.5 Quora3 dataset is a question paraphrase dataset. It contains over 400k question pairs. Each pair marked wit"
2021.findings-acl.135,2020.acl-main.22,0,0.0825141,"ement learning framework. Liu et al. (2020) regarded paraphrase generation as an optimization problem and proposed a sophisticated objective function. All methods above focus on the generic quality of paraphrase and do not care about the diversity of paraphrase. There are also some methods focusing on improving the diversity of paraphrase. Gupta et al. (2018) leveraged VAE to generate several different paraphrases by sampling the latent space. (Kumar et al., 2019) provided a novel formulation of the problem in terms of monotone sub-modular function maximization to generate diverse paraphrase. Goyal and Durrett (2020) used syntactic transformations to softly “reorder” the source sentence and guide paraphrase model. Thompson and Post (2020) introduced a simple paraphrase generation algorithm which discourages the production of ngrams that are present in the input to prevent trivial copies or near copies. Note that the purpose of the work (Gupta et al., 2018; An and Liu, 2019) is different from ours, while Thompson and Post (2020) has the same purpose with our work, i.e., pushing the generated paraphrase away from the original sentence. 3 Model In this section, we introduce the components of our model in det"
2021.findings-acl.135,N19-1363,0,0.0406358,"Missing"
2021.findings-acl.135,D19-5543,0,0.0623488,"Missing"
2021.findings-acl.135,P19-1332,0,0.103114,"available at https://github.com/L-Zhe/BTmPG. 2) Automatic and human evaluation results demonstrate that our method can substantially improve the diversity of generated paraphrase, while preserving the semantics during multi-round paraphrase generation. 2 Related Work Paraphrase generation or sentence paraphrasing can been seen as a monolingual translation task. Prakash et al. (2016) leveraged stacked residual LSTM networks to generate paraphrase. Gupta et al. (2018) found deep generative model such as variational auto-encoder can be able to achieve better performance in paraphrase generation. Li et al. (2019) proposed DNPG to decompose a sentence into sentence-level pattern and phrase-level pattern to make neural paraphrase generation more interpretable and controllable, and they found DNPG can be adopted into unsupervised domain adaptation method for paraphrase generation. Fu et al. (2019) proposed a new paraphrase model with latent bag of words. Wang et al. (2019) found that adding semantics information into paraphrase model can significantly boost performance. Siddique et al. (2020) proposed an unsupervised paraphrase model with deep reinforcement learning framework. Liu et al. (2020) regarded"
2021.findings-acl.135,W04-1013,0,0.0572279,"ate of each other. So we select all such question pairs with binary value 1 as paraphrase dataset. There are about 150k question pairs in total. We randomly divide the training, validation and the test set. Table 1 provides statistics of these two benchmark datasets. https://cocodataset.org/ 4.2 Evaluation Metrics We use five widely-used metrics to evaluate paraphrases: BLEU4, self-BLEU, self-TER, BERTScore and p-BLEU. BLEU4 is widely used in generation tasks. It can measure how well the sentences generated by our model can match the references. Notice that some works also calculate the ROUGE(Lin, 2004) or METEOR, but we think the role of these two metrics overlaps with BLEU4, as they all calculate the overlap degree between outputs and references. Therefore we only calculate BLEU4 to evaluate the match degree between outputs and references. We evaluate the difference between the output sentence and the original sentence with two metrics. One of them is self-BLEU which is the BLEU4 score between the output sentence and the original sentence. The lower the value of self-BLEU, the more difference between output sentences and original sentences. Another is self-TER4 . TER(Zaidan and Callison-Bu"
2021.findings-acl.135,2020.acl-main.28,0,0.0235551,"eneration. Li et al. (2019) proposed DNPG to decompose a sentence into sentence-level pattern and phrase-level pattern to make neural paraphrase generation more interpretable and controllable, and they found DNPG can be adopted into unsupervised domain adaptation method for paraphrase generation. Fu et al. (2019) proposed a new paraphrase model with latent bag of words. Wang et al. (2019) found that adding semantics information into paraphrase model can significantly boost performance. Siddique et al. (2020) proposed an unsupervised paraphrase model with deep reinforcement learning framework. Liu et al. (2020) regarded paraphrase generation as an optimization problem and proposed a sophisticated objective function. All methods above focus on the generic quality of paraphrase and do not care about the diversity of paraphrase. There are also some methods focusing on improving the diversity of paraphrase. Gupta et al. (2018) leveraged VAE to generate several different paraphrases by sampling the latent space. (Kumar et al., 2019) provided a novel formulation of the problem in terms of monotone sub-modular function maximization to generate diverse paraphrase. Goyal and Durrett (2020) used syntactic tra"
2021.findings-acl.135,D15-1166,0,0.0500133,"z . hz is passed through two different feed-forward neural networks with parameter Φ to produce the mean µ and the variance σ 2 of the distribution of latent space. We can get the latent code z ∈ Rdz by sampling from latent space and reparameterization, where dz is the dimension of latent code. 3.2.2 Decoder We define the embedding matrix which be sent de ×N . into decoder as Ed = {e1d , e2d , · · · , eN d }∈R Then, we concatenate z with the embedding vector eid as the input of decoder. The decoder also takes his as input. The output of decoder is defined as Odi ∈ Rdh ×N . Then, an attention (Luong et al., 2015) and copy mechanism (See et al., 2017) are leveraged as follow. First, we get the attention weight pa and attention vector Va as follow. > pa = softmax(Odi Osi ) Va = pa Osi (1) Then, we leverage them to calculate the decoder probability pd and copy probability η. 1550 pd = softmax(Wo [Odi ||Va ] + bo ) η = σ(Wh [Odi ||Va ] + Ws {eid ||z}N i=1 + bη ) (2) where Wo , Wh , Wb , bo , bη are learnable parameters. ||is the concatenation operation. σ is the sigmoid activation function. The final output probability of decoder is as follow. p = ηpd + (1 − η)pa + EqΦ (z|P ) [log pΘ (P |z, Si )] Lis = Cr"
2021.findings-acl.135,C16-1275,0,0.35207,"nce. 1 Introduction Paraphrase generation or sentence paraphrasing is an important task in natural language processing, and it requires rewriting a sentence while preserving its semantics. Paraphrase generation has been widely used in many downstream tasks such as QA systems, semantic parsing, dialogue systems and so on. In recent years, deep learning techniques like sequence-to-sequence(Seq2Seq) have achieved superior performance on natural language generation tasks (Zhao et al., 2010; Wubben et al., 2010). Many paraphrase models based on Seq2Seq have achieved inspiring results. For example, Prakash et al. (2016) leveraged stacked residual LSTM networks to generate paraphrase, and Gupta et al. (2018) proposed a deep generative framework based on variational auto-encoder for paraphrase generation. Though paraphrase generation models based on Seq2Seq have demonstrated advanced ability, the generated paraphrase still has the problem of lack of diversity, i.e., the output paraphrase only makes trivial changes to the original sentence. A good paraphrase of a sentence is one that is semantically similar to that sentence while being (very) syntactically and/or lexically different from it (Bhagat and Hovy, 20"
2021.findings-acl.135,C10-1149,0,0.0147335,"ic and human evaluation show BTmPG can improve the diversity of paraphrase while preserving the semantics of the original sentence. 1 Introduction Paraphrase generation or sentence paraphrasing is an important task in natural language processing, and it requires rewriting a sentence while preserving its semantics. Paraphrase generation has been widely used in many downstream tasks such as QA systems, semantic parsing, dialogue systems and so on. In recent years, deep learning techniques like sequence-to-sequence(Seq2Seq) have achieved superior performance on natural language generation tasks (Zhao et al., 2010; Wubben et al., 2010). Many paraphrase models based on Seq2Seq have achieved inspiring results. For example, Prakash et al. (2016) leveraged stacked residual LSTM networks to generate paraphrase, and Gupta et al. (2018) proposed a deep generative framework based on variational auto-encoder for paraphrase generation. Though paraphrase generation models based on Seq2Seq have demonstrated advanced ability, the generated paraphrase still has the problem of lack of diversity, i.e., the output paraphrase only makes trivial changes to the original sentence. A good paraphrase of a sentence is one tha"
2021.findings-acl.135,P17-1099,0,0.0475662,"eed-forward neural networks with parameter Φ to produce the mean µ and the variance σ 2 of the distribution of latent space. We can get the latent code z ∈ Rdz by sampling from latent space and reparameterization, where dz is the dimension of latent code. 3.2.2 Decoder We define the embedding matrix which be sent de ×N . into decoder as Ed = {e1d , e2d , · · · , eN d }∈R Then, we concatenate z with the embedding vector eid as the input of decoder. The decoder also takes his as input. The output of decoder is defined as Odi ∈ Rdh ×N . Then, an attention (Luong et al., 2015) and copy mechanism (See et al., 2017) are leveraged as follow. First, we get the attention weight pa and attention vector Va as follow. > pa = softmax(Odi Osi ) Va = pa Osi (1) Then, we leverage them to calculate the decoder probability pd and copy probability η. 1550 pd = softmax(Wo [Odi ||Va ] + bo ) η = σ(Wh [Odi ||Va ] + Ws {eid ||z}N i=1 + bη ) (2) where Wo , Wh , Wb , bo , bη are learnable parameters. ||is the concatenation operation. σ is the sigmoid activation function. The final output probability of decoder is as follow. p = ηpd + (1 − η)pa + EqΦ (z|P ) [log pΘ (P |z, Si )] Lis = CrossEntropy(BTModel(Si ), S0 ) (3) 3.2."
2021.findings-acl.135,2020.wmt-1.67,0,0.207575,"icated objective function. All methods above focus on the generic quality of paraphrase and do not care about the diversity of paraphrase. There are also some methods focusing on improving the diversity of paraphrase. Gupta et al. (2018) leveraged VAE to generate several different paraphrases by sampling the latent space. (Kumar et al., 2019) provided a novel formulation of the problem in terms of monotone sub-modular function maximization to generate diverse paraphrase. Goyal and Durrett (2020) used syntactic transformations to softly “reorder” the source sentence and guide paraphrase model. Thompson and Post (2020) introduced a simple paraphrase generation algorithm which discourages the production of ngrams that are present in the input to prevent trivial copies or near copies. Note that the purpose of the work (Gupta et al., 2018; An and Liu, 2019) is different from ours, while Thompson and Post (2020) has the same purpose with our work, i.e., pushing the generated paraphrase away from the original sentence. 3 Model In this section, we introduce the components of our model in detail. First, we define the paraphrase generation task and give an overview of our model. Next, we describe the paraphrase mod"
2021.findings-acl.135,W10-4223,0,0.0713758,"Missing"
2021.findings-acl.135,N10-1057,0,0.046058,"e the ROUGE(Lin, 2004) or METEOR, but we think the role of these two metrics overlaps with BLEU4, as they all calculate the overlap degree between outputs and references. Therefore we only calculate BLEU4 to evaluate the match degree between outputs and references. We evaluate the difference between the output sentence and the original sentence with two metrics. One of them is self-BLEU which is the BLEU4 score between the output sentence and the original sentence. The lower the value of self-BLEU, the more difference between output sentences and original sentences. Another is self-TER4 . TER(Zaidan and Callison-Burch, 2010) is used to evaluate the edit distance between two sentences. Self-TER is calculated as the TER between the output sentence and the original sentence. BERTScore 5 is proposed by Zhang et al. (2020) to evaluate the semantic similarity between the output sentence and the original sentence. BERTScore 3 https://www.kaggle.com/c/ quora-question-pairs/data?select=train. csv.zip 4 We use the tool at https://github.com/ jhclark/multeval. 5 The tool of BERTScore is available at https:// github.com/Tiiiger/bert_score 1552 has been widely leveraged to measure semantic preserving in the paraphrase generat"
2021.findings-acl.200,2020.emnlp-main.697,0,0.422408,"signed to generate text based on text input, thus lacking the ability to understand structured inputs. Several pre-training methods designed for table-to-text task have been proposed. Deng et al. (2020) present a weakly supervised StructureGrounded pretraining framework (STRUG) for text-to-SQL that can effectively learn to capture text-table alignment. But their model is only for text-to-SQL task and need parallel text-table data. Yin et al. (2020) propose TABERT, a pretrained model which is trained with large amount of tables with their context. Their model is also used for text-to-SQL task. Chen et al. (2020a) propose a knowledge-grounded pre-trained (KGPT) model which is trained on a massive knowledge- grounded text corpus crawled from the web. Li et al. (2020) propose two self-supervised tasks, Number Ordering and Significance Ordering, to help to learn better table representation. 3 Approach We use the same model architecture as BART, and add several classification layers on top of the encoder for our new self-supervised tasks. We train our model based on the text-to-text pre-trained model BART instead of training from scratch because we hope our model can benefit from the knowledge and langua"
2021.findings-acl.200,W17-3518,0,0.157956,"input table and generate fluent text. Experiments on two datasets show the efficacy of our model. 1 Introduction Data-to-text generation (Reiter and Dale, 1997) is an important natural language generation task with many practical applications, and it refers to the task of generating textual output from nonlinguistic input data. The input data of the task can include tables of records, simulations of physical systems, spreadsheets, and so on. The output of the task is a natural language text. Datasets in common use include WEATHERGOV(Liang et al., 2009), ROTOWIRE(Wiseman et al., 2017), WebNLG(Gardent et al., 2017) and so on. Neural generation models with different improvements have achieved impressive results on data-to-text task. Table-to-text generation is a subtask of datato-text generation which takes tables as input. The pretrain-and-finetune framework, which refers to first pre-training a high capacity model on large corpora and then fine-tuning it on a downstream task, has outperformed prior state of the art on both natural language understanding task and natural language generation task. Inspired by the success of transfer learning, recently some works (Mager et al., 2020; Kale, 2020; Ribeiro e"
2021.findings-acl.200,2020.coling-main.218,0,0.0473343,"Missing"
2021.findings-acl.200,2020.inlg-1.14,0,0.29811,"Gardent et al., 2017) and so on. Neural generation models with different improvements have achieved impressive results on data-to-text task. Table-to-text generation is a subtask of datato-text generation which takes tables as input. The pretrain-and-finetune framework, which refers to first pre-training a high capacity model on large corpora and then fine-tuning it on a downstream task, has outperformed prior state of the art on both natural language understanding task and natural language generation task. Inspired by the success of transfer learning, recently some works (Mager et al., 2020; Kale, 2020; Ribeiro et al., 2020) try to apply the pretrain-and-finetune framework on data-to-text generation. They finetuned the pre-trained model such as BART(Lewis et al., 2019) or T5(Raffel et al., 2019) on several downstream data-to-text tasks and achieved stateof-the-art results. Although transfer learning has achieved great success on data-to-text generation, the pre-trained models used in previous works are typically trained on free-form natural language texts while the input of table-to-text task is structured table. The textto-text pre-trained models learn a lot of knowledge and good language"
2021.findings-acl.200,P09-1011,0,0.516676,"Missing"
2021.findings-acl.200,2020.acl-main.167,0,0.0235345,"al., 2017), WebNLG(Gardent et al., 2017) and so on. Neural generation models with different improvements have achieved impressive results on data-to-text task. Table-to-text generation is a subtask of datato-text generation which takes tables as input. The pretrain-and-finetune framework, which refers to first pre-training a high capacity model on large corpora and then fine-tuning it on a downstream task, has outperformed prior state of the art on both natural language understanding task and natural language generation task. Inspired by the success of transfer learning, recently some works (Mager et al., 2020; Kale, 2020; Ribeiro et al., 2020) try to apply the pretrain-and-finetune framework on data-to-text generation. They finetuned the pre-trained model such as BART(Lewis et al., 2019) or T5(Raffel et al., 2019) on several downstream data-to-text tasks and achieved stateof-the-art results. Although transfer learning has achieved great success on data-to-text generation, the pre-trained models used in previous works are typically trained on free-form natural language texts while the input of table-to-text task is structured table. The textto-text pre-trained models learn a lot of knowledge and go"
2021.findings-acl.200,E17-1035,0,0.0283598,"on WEATHERGOV dataset and WebNLG dataset show the efficacy of our model. Code will be released at https://github.com/XingXinyu96/STTP. 2 Related work Data-to-text generation task involves taking structured data as input and generating text that describes this data. Traditional approaches (Stent et al., 2004; Walker et al., 2007) deal with the task in two steps: the selection of a subset of the input data to discuss and the surface realization of a generation. More recent works combine both steps by learning content plan and surface realization jointly with end-to-end models (Wen et al., 2015; Peng et al., 2017). Although the end-to-end model has achieved good results, many models (PerezBeltrachini and Lapata, 2018; Sha et al., 2018; Puduppully et al., 2019) consider adding content selection and content planning modules to the endto-end framework to improve performance. A lot of other new models with different improvements (Wiseman et al., 2018; Li and Wan, 2018; Liu et al., 2018; Roberti et al., 2019; Rebuffel et al., 2020) are proposed to explore how to build an effective data-to-text generator. Inspired by the success of the pre-trained models in other natural language generation tasks, Harkous et"
2021.findings-acl.200,N18-1137,0,0.0321091,"Missing"
2021.findings-acl.200,P19-1195,0,0.0431356,"Missing"
2021.findings-acl.200,W18-6543,0,0.0197848,"model outperforms the BART-Retrain model, which proves the improvements of STTP model over BART model is from the proposed training objective instead of the additional training data. Model (Mei et al., 2015) BART-base BART-Retrain STTP BLEU 54.0 56.0 61.0 60.80 64.11 62.62 62.89 64.92 BLEU 61.01 81.54 81.63 82.63 METEOR n/a 54.81 55.50 56.35 Table 1: Results on WeatherGov Dataset. 4.2.2 Results on WebNLG Dataset We use a batch size of 4 and fine-tune for 16 epochs over the WebNLG Dataset. Results are presented in Table 2. The results of Seq2Seq, Seq2Seq+Delex and Seq2Seq+copy are copied from (Shimorina and Gardent, 2018). The results of GCN and KGPTSeq are copied from (Chen et al., 2020b). As is shown in this table, all pre-trained models outperform the models without pre-training even if some of them do not explicitly consider the structure of the input data. This is due to the pre-trained models learn a lot of external knowledge and a good language model from large corpora. The structure-aware model like GCN outperforms the We randomly sample 50 instances from the WebNLG dataset and perform human evaluation on them. Three graduate students are employed to rank the generated texts produced by each model in t"
2021.findings-acl.200,P04-1011,0,0.150616,"ics: ACL-IJCNLP 2021, pages 2273–2278 August 1–6, 2021. ©2021 Association for Computational Linguistics efficacy of our model. The main contributions of this work are: • We propose a structure-aware table-to-text pre-trained model STTP which is trained with three self-supervised tasks. • Experimental results on WEATHERGOV dataset and WebNLG dataset show the efficacy of our model. Code will be released at https://github.com/XingXinyu96/STTP. 2 Related work Data-to-text generation task involves taking structured data as input and generating text that describes this data. Traditional approaches (Stent et al., 2004; Walker et al., 2007) deal with the task in two steps: the selection of a subset of the input data to discuss and the surface realization of a generation. More recent works combine both steps by learning content plan and surface realization jointly with end-to-end models (Wen et al., 2015; Peng et al., 2017). Although the end-to-end model has achieved good results, many models (PerezBeltrachini and Lapata, 2018; Sha et al., 2018; Puduppully et al., 2019) consider adding content selection and content planning modules to the endto-end framework to improve performance. A lot of other new models"
2021.findings-acl.200,D15-1199,0,0.0250646,"erimental results on WEATHERGOV dataset and WebNLG dataset show the efficacy of our model. Code will be released at https://github.com/XingXinyu96/STTP. 2 Related work Data-to-text generation task involves taking structured data as input and generating text that describes this data. Traditional approaches (Stent et al., 2004; Walker et al., 2007) deal with the task in two steps: the selection of a subset of the input data to discuss and the surface realization of a generation. More recent works combine both steps by learning content plan and surface realization jointly with end-to-end models (Wen et al., 2015; Peng et al., 2017). Although the end-to-end model has achieved good results, many models (PerezBeltrachini and Lapata, 2018; Sha et al., 2018; Puduppully et al., 2019) consider adding content selection and content planning modules to the endto-end framework to improve performance. A lot of other new models with different improvements (Wiseman et al., 2018; Li and Wan, 2018; Liu et al., 2018; Roberti et al., 2019; Rebuffel et al., 2020) are proposed to explore how to build an effective data-to-text generator. Inspired by the success of the pre-trained models in other natural language generati"
2021.findings-acl.200,D17-1239,0,0.0257283,"can understand the structured input table and generate fluent text. Experiments on two datasets show the efficacy of our model. 1 Introduction Data-to-text generation (Reiter and Dale, 1997) is an important natural language generation task with many practical applications, and it refers to the task of generating textual output from nonlinguistic input data. The input data of the task can include tables of records, simulations of physical systems, spreadsheets, and so on. The output of the task is a natural language text. Datasets in common use include WEATHERGOV(Liang et al., 2009), ROTOWIRE(Wiseman et al., 2017), WebNLG(Gardent et al., 2017) and so on. Neural generation models with different improvements have achieved impressive results on data-to-text task. Table-to-text generation is a subtask of datato-text generation which takes tables as input. The pretrain-and-finetune framework, which refers to first pre-training a high capacity model on large corpora and then fine-tuning it on a downstream task, has outperformed prior state of the art on both natural language understanding task and natural language generation task. Inspired by the success of transfer learning, recently some works (Mager et al"
2021.findings-acl.200,D18-1356,0,0.0586878,"ith the task in two steps: the selection of a subset of the input data to discuss and the surface realization of a generation. More recent works combine both steps by learning content plan and surface realization jointly with end-to-end models (Wen et al., 2015; Peng et al., 2017). Although the end-to-end model has achieved good results, many models (PerezBeltrachini and Lapata, 2018; Sha et al., 2018; Puduppully et al., 2019) consider adding content selection and content planning modules to the endto-end framework to improve performance. A lot of other new models with different improvements (Wiseman et al., 2018; Li and Wan, 2018; Liu et al., 2018; Roberti et al., 2019; Rebuffel et al., 2020) are proposed to explore how to build an effective data-to-text generator. Inspired by the success of the pre-trained models in other natural language generation tasks, Harkous et al. (2020), Kale (2020) and Ribeiro et al. (2020) achieve state-of-the-art results on different data-totext benchmarks with different pre-trained models. However, the existing pre-trained models are usually designed to generate text based on text input, thus lacking the ability to understand structured inputs. Several pre-training metho"
2021.findings-acl.200,2020.acl-main.745,0,0.262504,"2020) achieve state-of-the-art results on different data-totext benchmarks with different pre-trained models. However, the existing pre-trained models are usually designed to generate text based on text input, thus lacking the ability to understand structured inputs. Several pre-training methods designed for table-to-text task have been proposed. Deng et al. (2020) present a weakly supervised StructureGrounded pretraining framework (STRUG) for text-to-SQL that can effectively learn to capture text-table alignment. But their model is only for text-to-SQL task and need parallel text-table data. Yin et al. (2020) propose TABERT, a pretrained model which is trained with large amount of tables with their context. Their model is also used for text-to-SQL task. Chen et al. (2020a) propose a knowledge-grounded pre-trained (KGPT) model which is trained on a massive knowledge- grounded text corpus crawled from the web. Li et al. (2020) propose two self-supervised tasks, Number Ordering and Significance Ordering, to help to learn better table representation. 3 Approach We use the same model architecture as BART, and add several classification layers on top of the encoder for our new self-supervised tasks. We"
2021.findings-acl.209,D11-1033,0,0.0602765,"ce the accuracy of classifying “This knife is solid.” (positive sentiment, kitchen domain), because “solid” has different meanings in these two domains. Any domain-specific expression like this would probably introduce some noise. So it is essential to find a suitable strategy to measure the importance of each training sample. There are many instance weighting (or instance selection) methods to tackle this problem. They assign a weight to each instance and transform the loss function to a weighted-sum formula. Most of the conventional methods (Jiang and Zhai, 2007; Gretton et al., 2006, 2009; Axelrod et al., 2011; Wang et al., 2017; Zhang and Xiong, 2018; Wang et al., 2019; Dou et al., 2020) propose different 2366 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2366–2376 August 1–6, 2021. ©2021 Association for Computational Linguistics kinds of manually designed metrics to calculate the weights of instances. The core idea of these methods is to weight the instances according to their importance and similarity to the target domain. However, in our domain adaptation setting, the size of out-of-domain corpus is much larger than that of in-domain corpus. The weights learn"
2021.findings-acl.209,P07-1056,0,0.256898,"traction, respectively. 3.1 is suitable for evaluating domain adaptation because it contains six different domains. It has been adopted by many previous works (Nguyen and Grishman, 2014; Gormley et al., 2015; Fu et al., 2017) for cross-domain relation extraction. In this work, we take broadcast news (bn) and newswire (nw) domain as out-of-domain, and split broadcast conversation (bc) domain into train/dev/test sets with the ratio of 1 : 1 : 4. Table 3 shows the detailed statistics. Datasets For sentiment classification task, we conduct the experiments on the widely-used Amazon Review Dataset (Blitzer et al., 2007). This dataset contains four domain: books (B), dvd (D), electronics (E) and kitchen (K). Each domain contains the reviews of a specific category of products. We use the data processed by He et al. (2018) and collect 6000 labeled samples for each domain. We split the data of each domain into training (Din ), development (Ddev ) and test (Dtest ) set. In each domain adaptation setting, we choose the training data of one domain as the in-domain data (Din ) and all data of other three domains as the out-of-domain data (Dout ). Table 1 shows an example with books domain as the in-domain. For machi"
2021.findings-acl.209,W06-1615,0,0.306766,"our method can still achieve consistent improvements in this three dataset size settings. 4 4.1 Related Work Cross-Domain Sentiment Classification Sentiment classification task aims to automatically classify the sentiment polarity of the given texts. Cross-domain sentiment classification aims to generalize the sentiment classifier from source domain to target domain. Besides the domain adaptation methods introduced in Section 4.1, there are some methods which are specific for cross-domain sentiment classification. An important line of works follow the Structural Correspondence Learning (SCL) (Blitzer et al., 2006), and they design an auxiliary task called pivot prediction to transfer domain-invariant knowledge (Pan et al., 2010; Yu and Jiang, 2016; Ziser and Reichart, 2016, 2018, 2019). But the pivot words need human knowledge to select, which may be not so accurate. Recently, the pretrained language models such as BERT (Devlin et al., 2018) have achieved state-of-the-art on many NLP tasks. DAAT (Du et al., 2020) performs a novel post-training procedure on BERT and uses adversarial training to transfer domain knowledge. But this method only works for classification task while our method is model-agnost"
2021.findings-acl.209,W17-4712,0,0.016258,"g the metrics manually may not be the best solution for all the tasks. Designing them requires many prior human expert knowledge which is hard to generalize well across tasks. By contrast, our method can learn instance weights with the help of meta-learning based algorithm to improve the models’ in-domain generalization capability. (2) Domain adversarial based method is a strong baseline which is surpassed only by our method in the sentiment classification task and the relation extraction task. However, it performs not so well for the machine translation task. The potential reason may be that Britz et al. (2017) introduces the domain classifier after the encoder to learn domain-invariant features of sentences from source language, but both domains share the same decoder which cannot discriminate the features encoded by the encoder. In other words, this type of method may only pay attention to the encoder and ignore the domain transfer of decoder. In contrast, our method overcomes this problem by considering weighting the loss of the whole model and thus achieves better performance. (3) For all three tasks, adding out-of-domain corpus to the training set will improve the overall performance. We believ"
2021.findings-acl.209,2020.emnlp-main.475,0,0.0221853,"domain), because “solid” has different meanings in these two domains. Any domain-specific expression like this would probably introduce some noise. So it is essential to find a suitable strategy to measure the importance of each training sample. There are many instance weighting (or instance selection) methods to tackle this problem. They assign a weight to each instance and transform the loss function to a weighted-sum formula. Most of the conventional methods (Jiang and Zhai, 2007; Gretton et al., 2006, 2009; Axelrod et al., 2011; Wang et al., 2017; Zhang and Xiong, 2018; Wang et al., 2019; Dou et al., 2020) propose different 2366 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2366–2376 August 1–6, 2021. ©2021 Association for Computational Linguistics kinds of manually designed metrics to calculate the weights of instances. The core idea of these methods is to weight the instances according to their importance and similarity to the target domain. However, in our domain adaptation setting, the size of out-of-domain corpus is much larger than that of in-domain corpus. The weights learned by the previous methods may be biased to the outof-domain data, which would u"
2021.findings-acl.209,2020.acl-main.370,0,0.0250257,"oduced in Section 4.1, there are some methods which are specific for cross-domain sentiment classification. An important line of works follow the Structural Correspondence Learning (SCL) (Blitzer et al., 2006), and they design an auxiliary task called pivot prediction to transfer domain-invariant knowledge (Pan et al., 2010; Yu and Jiang, 2016; Ziser and Reichart, 2016, 2018, 2019). But the pivot words need human knowledge to select, which may be not so accurate. Recently, the pretrained language models such as BERT (Devlin et al., 2018) have achieved state-of-the-art on many NLP tasks. DAAT (Du et al., 2020) performs a novel post-training procedure on BERT and uses adversarial training to transfer domain knowledge. But this method only works for classification task while our method is model-agnostic and does not need two-stage post-training and fine-tuning. Domain Adaptation Domain Adaptation is a fundamental problem in machine learning and NLP. We aim to train a wellperforming model on a source domain which can be generalized to a target domain. The basic idea for domain adaptation is to learn domain-invariant representations which generalize across the domains. To achieve this, the most prevail"
2021.findings-acl.209,I17-2072,0,0.224963,"est ) Sentences 202,356 500,000 1,700 993 1,305 3.2 Table 2: Statistics of machine translation setting. Dataset bc-train (Din ) nw & bn (Dout ) bc-dev (Ddev ) bc-test (Dtest ) Docs 10 332 10 40 Relations 222 4,695 347 1,036 Table 3: Statistics of relation extraction setting. three different dataset settings of three tasks: Sentiment Classification, Machine Translation (MT) and Relation Extraction, respectively. 3.1 is suitable for evaluating domain adaptation because it contains six different domains. It has been adopted by many previous works (Nguyen and Grishman, 2014; Gormley et al., 2015; Fu et al., 2017) for cross-domain relation extraction. In this work, we take broadcast news (bn) and newswire (nw) domain as out-of-domain, and split broadcast conversation (bc) domain into train/dev/test sets with the ratio of 1 : 1 : 4. Table 3 shows the detailed statistics. Datasets For sentiment classification task, we conduct the experiments on the widely-used Amazon Review Dataset (Blitzer et al., 2007). This dataset contains four domain: books (B), dvd (D), electronics (E) and kitchen (K). Each domain contains the reviews of a specific category of products. We use the data processed by He et al. (2018)"
2021.findings-acl.209,D15-1205,0,0.0269024,"test ) TED tst2014 (Dtest ) Sentences 202,356 500,000 1,700 993 1,305 3.2 Table 2: Statistics of machine translation setting. Dataset bc-train (Din ) nw & bn (Dout ) bc-dev (Ddev ) bc-test (Dtest ) Docs 10 332 10 40 Relations 222 4,695 347 1,036 Table 3: Statistics of relation extraction setting. three different dataset settings of three tasks: Sentiment Classification, Machine Translation (MT) and Relation Extraction, respectively. 3.1 is suitable for evaluating domain adaptation because it contains six different domains. It has been adopted by many previous works (Nguyen and Grishman, 2014; Gormley et al., 2015; Fu et al., 2017) for cross-domain relation extraction. In this work, we take broadcast news (bn) and newswire (nw) domain as out-of-domain, and split broadcast conversation (bc) domain into train/dev/test sets with the ratio of 1 : 1 : 4. Table 3 shows the detailed statistics. Datasets For sentiment classification task, we conduct the experiments on the widely-used Amazon Review Dataset (Blitzer et al., 2007). This dataset contains four domain: books (B), dvd (D), electronics (E) and kitchen (K). Each domain contains the reviews of a specific category of products. We use the data processed b"
2021.findings-acl.209,D18-1383,0,0.0174947,"Fu et al., 2017) for cross-domain relation extraction. In this work, we take broadcast news (bn) and newswire (nw) domain as out-of-domain, and split broadcast conversation (bc) domain into train/dev/test sets with the ratio of 1 : 1 : 4. Table 3 shows the detailed statistics. Datasets For sentiment classification task, we conduct the experiments on the widely-used Amazon Review Dataset (Blitzer et al., 2007). This dataset contains four domain: books (B), dvd (D), electronics (E) and kitchen (K). Each domain contains the reviews of a specific category of products. We use the data processed by He et al. (2018) and collect 6000 labeled samples for each domain. We split the data of each domain into training (Din ), development (Ddev ) and test (Dtest ) set. In each domain adaptation setting, we choose the training data of one domain as the in-domain data (Din ) and all data of other three domains as the out-of-domain data (Dout ). Table 1 shows an example with books domain as the in-domain. For machine translation task, similar to the settings of Luong and Manning (2015); Wang et al. (2017); Zeng et al. (2019), we use the IWSLT 2016 English (EN) to German (DE) corpus (Cettolo et al., 2016) as the in-"
2021.findings-acl.209,P07-1034,0,0.227291,".” (negative sentiment, furniture domain) may reduce the accuracy of classifying “This knife is solid.” (positive sentiment, kitchen domain), because “solid” has different meanings in these two domains. Any domain-specific expression like this would probably introduce some noise. So it is essential to find a suitable strategy to measure the importance of each training sample. There are many instance weighting (or instance selection) methods to tackle this problem. They assign a weight to each instance and transform the loss function to a weighted-sum formula. Most of the conventional methods (Jiang and Zhai, 2007; Gretton et al., 2006, 2009; Axelrod et al., 2011; Wang et al., 2017; Zhang and Xiong, 2018; Wang et al., 2019; Dou et al., 2020) propose different 2366 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2366–2376 August 1–6, 2021. ©2021 Association for Computational Linguistics kinds of manually designed metrics to calculate the weights of instances. The core idea of these methods is to weight the instances according to their importance and similarity to the target domain. However, in our domain adaptation setting, the size of out-of-domain corpus is much large"
2021.findings-acl.209,W17-3204,0,0.0152128,"available at https://github.com/ CasparSwift/WIND (out-of-domain). We aim to leverage the general knowledge from out-of-domain dataset to enhance the in-domain performance of our model. We consider a specific domain adaptation scenario in this work, where we have a few labeled in-domain training data and meanwhile we have sufficient labeled out-of-domain training data from other general domains. Training on these two datasets jointly is a straightforward solution for this scenario, but not all samples from out-of-domain dataset has equal effect during the training procedure. Several studies (Koehn and Knowles, 2017) on neural machine translation (NMT) task show that, out-of-domain instances relevant to the in-domain data are beneficial while the instances irrelevant to the in-domain data may be even harmful to the translation quality. Apart from that, for sentiment classification task, some general expressions such as “I’m truly impressed by the design.” may appear in all domains. Taking them as training samples can help the model to learn general syntactic and semantic knowledge, which improves the cross-domain sentiment classification performance. But using examples like “This chair is solid.” (negativ"
2021.findings-acl.209,2015.iwslt-evaluation.11,0,0.0167125,"dvd (D), electronics (E) and kitchen (K). Each domain contains the reviews of a specific category of products. We use the data processed by He et al. (2018) and collect 6000 labeled samples for each domain. We split the data of each domain into training (Din ), development (Ddev ) and test (Dtest ) set. In each domain adaptation setting, we choose the training data of one domain as the in-domain data (Din ) and all data of other three domains as the out-of-domain data (Dout ). Table 1 shows an example with books domain as the in-domain. For machine translation task, similar to the settings of Luong and Manning (2015); Wang et al. (2017); Zeng et al. (2019), we use the IWSLT 2016 English (EN) to German (DE) corpus (Cettolo et al., 2016) as the in-domain data. This corpus contains about 202K sentences from TED talks. For out-ofdomain data, we randomly sample a subset of 500K sentences from the WMT 2014 English-German corpus. Table 2 show the statistics of the datasets. For relation extraction task, we evaluate our method on the ACE 2005 dataset. This dataset Implementation Details For sentiment classification task, we use the pretrained BERT-base-uncased (Devlin et al., 2018) model provided by HuggingFace ("
2021.findings-acl.209,P14-2012,0,0.0263137,"2012 (Ddev ) TED tst2013 (Dtest ) TED tst2014 (Dtest ) Sentences 202,356 500,000 1,700 993 1,305 3.2 Table 2: Statistics of machine translation setting. Dataset bc-train (Din ) nw & bn (Dout ) bc-dev (Ddev ) bc-test (Dtest ) Docs 10 332 10 40 Relations 222 4,695 347 1,036 Table 3: Statistics of relation extraction setting. three different dataset settings of three tasks: Sentiment Classification, Machine Translation (MT) and Relation Extraction, respectively. 3.1 is suitable for evaluating domain adaptation because it contains six different domains. It has been adopted by many previous works (Nguyen and Grishman, 2014; Gormley et al., 2015; Fu et al., 2017) for cross-domain relation extraction. In this work, we take broadcast news (bn) and newswire (nw) domain as out-of-domain, and split broadcast conversation (bc) domain into train/dev/test sets with the ratio of 1 : 1 : 4. Table 3 shows the detailed statistics. Datasets For sentiment classification task, we conduct the experiments on the widely-used Amazon Review Dataset (Blitzer et al., 2007). This dataset contains four domain: books (B), dvd (D), electronics (E) and kitchen (K). Each domain contains the reviews of a specific category of products. We us"
2021.findings-acl.209,N19-4009,0,0.0254532,"ggingFace (Wolf et al., 2019) as our feature extractor. Our sentiment classifier is a one-hidden-layer MLP with ReLU as the activation function. For the optimization of model parameters θ, we use the AdamW (Loshchilov and Hutter, 2018) as the optimizer with a learning rate of 2e − 5, a warmup of 0.1 (of the total steps) and a linearly decayed learning rate scheduler. The computational cost is about 8-12 GPU hours on Tesla V100. For machine translation, we choose a vanilla Transformer (Vaswani et al., 2017) as our backbone. We implement some baseline methods and our method via fairseq toolkit (Ott et al., 2019). We use MOSES2 scripts to tokenize the English and German sentences, and then we apply Byte Pair Encoding (BPE) (Sennrich et al., 2015) algorithm to split the words into subwords. We limit the maximum length of the sentences to 250 subwords. We choose to share the embeddings of English and German with the vocabulary size of 32,000. We use Adam (Kingma and Ba, 2014) as the optimizer and a decayed learning rate of 7e − 4. For relation extraction, we only focus on relation classification when the entity pairs are given for simplicity. We use the RBERT (Wu and He, 2019) model as our backbone. The"
2021.findings-acl.209,P02-1040,0,0.115219,"on task. Our method achieves an absolute improvement of 0.45, 0.40, 0.52 and 0.88 points on four settings respectively in comparison to the In+Out baseline. Moreover, our method outperforms all the domain adaptation methods on the settings with B, D, K as the in-domain data except the E domain. Although our method does not beat 2371 all baselines on all settings, it achieves the best average performance across the four settings. On average, we achieve an improvement of 0.28 point over DANN and 0.54 point over In+Out. Table 5 shows the performance for the machine translation task. We use BLEU (Papineni et al., 2002) scores to measure the performance. Our method beats all baselines on all test sets (tst2013,tst2014) and the development set (tst2012). On these three datasets, we observe an improvement of 1.17, 1.08 and 0.54 BLEU points compared to In+Out. On tst2013 and tst2014 test sets, we also achieve an improvement of 0.65 and 0.38 BLEU points compared to IDDA (Zeng et al., 2019) method. Table 6 further shows our method’s effectiveness on the relation extraction task. Furthermore, from the results in Tables 4, 5 and 6, we can make the following observations: (1) In comparison to the method in (Wang et"
2021.findings-acl.209,N19-1258,0,0.0115362,"aining to transfer domain knowledge. But this method only works for classification task while our method is model-agnostic and does not need two-stage post-training and fine-tuning. Domain Adaptation Domain Adaptation is a fundamental problem in machine learning and NLP. We aim to train a wellperforming model on a source domain which can be generalized to a target domain. The basic idea for domain adaptation is to learn domain-invariant representations which generalize across the domains. To achieve this, the most prevailing method Domain Adversarial Neural Network (DANN) (Ganin et al., 2016; Qu et al., 2019; Xue et al., 2020) introduces a domain classifier and uses adversarial training to make the features unable to discriminate between source and target domains. This method has been applied to many NLP tasks. However, out-of-domain data is far more than in-domain data in our setting. DANN may cause some bias in this unbalanced dataset. Another type of methods (Fang and Xie, 2020; Li et al., 2020) propose to learn domain-general representations by contrastive learning (Chen et al., 4.3 Meta-Learning The goal of meta-learning is to train a model that can adapt to a new task quickly given a few ne"
2021.findings-acl.209,D17-1155,0,0.190082,"ssifying “This knife is solid.” (positive sentiment, kitchen domain), because “solid” has different meanings in these two domains. Any domain-specific expression like this would probably introduce some noise. So it is essential to find a suitable strategy to measure the importance of each training sample. There are many instance weighting (or instance selection) methods to tackle this problem. They assign a weight to each instance and transform the loss function to a weighted-sum formula. Most of the conventional methods (Jiang and Zhai, 2007; Gretton et al., 2006, 2009; Axelrod et al., 2011; Wang et al., 2017; Zhang and Xiong, 2018; Wang et al., 2019; Dou et al., 2020) propose different 2366 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2366–2376 August 1–6, 2021. ©2021 Association for Computational Linguistics kinds of manually designed metrics to calculate the weights of instances. The core idea of these methods is to weight the instances according to their importance and similarity to the target domain. However, in our domain adaptation setting, the size of out-of-domain corpus is much larger than that of in-domain corpus. The weights learned by the previous"
2021.findings-acl.209,D16-1023,0,0.0198529,"ication Sentiment classification task aims to automatically classify the sentiment polarity of the given texts. Cross-domain sentiment classification aims to generalize the sentiment classifier from source domain to target domain. Besides the domain adaptation methods introduced in Section 4.1, there are some methods which are specific for cross-domain sentiment classification. An important line of works follow the Structural Correspondence Learning (SCL) (Blitzer et al., 2006), and they design an auxiliary task called pivot prediction to transfer domain-invariant knowledge (Pan et al., 2010; Yu and Jiang, 2016; Ziser and Reichart, 2016, 2018, 2019). But the pivot words need human knowledge to select, which may be not so accurate. Recently, the pretrained language models such as BERT (Devlin et al., 2018) have achieved state-of-the-art on many NLP tasks. DAAT (Du et al., 2020) performs a novel post-training procedure on BERT and uses adversarial training to transfer domain knowledge. But this method only works for classification task while our method is model-agnostic and does not need two-stage post-training and fine-tuning. Domain Adaptation Domain Adaptation is a fundamental problem in machine le"
2021.findings-acl.209,D19-1078,0,0.164271,"domain contains the reviews of a specific category of products. We use the data processed by He et al. (2018) and collect 6000 labeled samples for each domain. We split the data of each domain into training (Din ), development (Ddev ) and test (Dtest ) set. In each domain adaptation setting, we choose the training data of one domain as the in-domain data (Din ) and all data of other three domains as the out-of-domain data (Dout ). Table 1 shows an example with books domain as the in-domain. For machine translation task, similar to the settings of Luong and Manning (2015); Wang et al. (2017); Zeng et al. (2019), we use the IWSLT 2016 English (EN) to German (DE) corpus (Cettolo et al., 2016) as the in-domain data. This corpus contains about 202K sentences from TED talks. For out-ofdomain data, we randomly sample a subset of 500K sentences from the WMT 2014 English-German corpus. Table 2 show the statistics of the datasets. For relation extraction task, we evaluate our method on the ACE 2005 dataset. This dataset Implementation Details For sentiment classification task, we use the pretrained BERT-base-uncased (Devlin et al., 2018) model provided by HuggingFace (Wolf et al., 2019) as our feature extrac"
2021.findings-acl.209,C18-1269,0,0.0135368,"e is solid.” (positive sentiment, kitchen domain), because “solid” has different meanings in these two domains. Any domain-specific expression like this would probably introduce some noise. So it is essential to find a suitable strategy to measure the importance of each training sample. There are many instance weighting (or instance selection) methods to tackle this problem. They assign a weight to each instance and transform the loss function to a weighted-sum formula. Most of the conventional methods (Jiang and Zhai, 2007; Gretton et al., 2006, 2009; Axelrod et al., 2011; Wang et al., 2017; Zhang and Xiong, 2018; Wang et al., 2019; Dou et al., 2020) propose different 2366 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2366–2376 August 1–6, 2021. ©2021 Association for Computational Linguistics kinds of manually designed metrics to calculate the weights of instances. The core idea of these methods is to weight the instances according to their importance and similarity to the target domain. However, in our domain adaptation setting, the size of out-of-domain corpus is much larger than that of in-domain corpus. The weights learned by the previous methods may be biased t"
2021.findings-acl.209,N18-1112,0,0.0337279,"Missing"
2021.findings-acl.209,P19-1591,0,0.0313328,"Missing"
2021.findings-acl.65,N04-4001,0,0.104929,"ally generate summaries for a group of opinions about a specific entity (e.g., user reviews of a product), and does not require any gold summaries (Ku et al., 2006; Kim et al., 2011; Chu and Liu, 2019). Most previous works focus on extractive approaches, which select a subset of salient sentences from the inputs based on topic-words (Paul et al., 2010; Fabbrizio et al., 2014), wordfrequencies (Erkan and Radev, 2004; Nenkova and Vanderwende, 2005), word embeddings (Rossiello et al., 2017) or textual graphs (Radev et al., 2004). However, due to their shortcomings of copying text from the input (Banko and Vanderwende, 2004), 736 5 Conclusion In this paper, we propose a novel self-supervised framework TransSum, to generate opinion summaries with only the aspect and sentiment embeddings, which are beneficial for maximizing informativeness, reducing redundancy of repeated opinions in reviews, and creating synthetic datasets of highly relevant reviews-summary pairs for training. Extensive evaluation and ablation studies show our model outperforms competitive systems in generating informative, high-relevant, low-redundant and fluent summaries. We believe that the viewpoint from modeling opinion summaries with only as"
2021.findings-acl.65,2020.emnlp-main.337,0,0.146567,"at it as a normal multi-document summarization task. They either struggle to reduce the opinion redundancy efficiently or output summaries lacking relevance to input reviews. Particularly, many previous studies focus on extractive approaches (Paul et al., 2010; Fabbrizio et al., 2014; Rossiello et al., 2017; Narayan et al., 2019), which copy texts from the input reviews but tend to be redundant and less informative (Chu and Liu, 2019). Some recently proposed abstractive methods are based on unsupervised representation learning, such as autoencoder (Chu and Liu, 2019; Amplayo and Lapata, 2019; Brazinskas et al., 2020a) or variational autoencoder (Brazinskas et al., 2020b; Angelidis et al., 2020), but mainly focus on the content transformation within each group of reviews. Other studies aim to create synthetic reviews-summary pairs to train a supervised multi-document summarization model (Amplayo and Lapata, 2019; Brazinskas et al., 2020b; Amplayo and Lapata, 2020; Amplayo 729 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 729–742 August 1–6, 2021. ©2021 Association for Computational Linguistics et al., 2021), such as sampling a review from a corpus of product reviews and"
2021.findings-acl.65,2020.acl-main.461,0,0.399181,"at it as a normal multi-document summarization task. They either struggle to reduce the opinion redundancy efficiently or output summaries lacking relevance to input reviews. Particularly, many previous studies focus on extractive approaches (Paul et al., 2010; Fabbrizio et al., 2014; Rossiello et al., 2017; Narayan et al., 2019), which copy texts from the input reviews but tend to be redundant and less informative (Chu and Liu, 2019). Some recently proposed abstractive methods are based on unsupervised representation learning, such as autoencoder (Chu and Liu, 2019; Amplayo and Lapata, 2019; Brazinskas et al., 2020a) or variational autoencoder (Brazinskas et al., 2020b; Angelidis et al., 2020), but mainly focus on the content transformation within each group of reviews. Other studies aim to create synthetic reviews-summary pairs to train a supervised multi-document summarization model (Amplayo and Lapata, 2019; Brazinskas et al., 2020b; Amplayo and Lapata, 2020; Amplayo 729 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 729–742 August 1–6, 2021. ©2021 Association for Computational Linguistics et al., 2021), such as sampling a review from a corpus of product reviews and"
2021.findings-acl.65,N19-1423,0,0.00551177,"r each group of reviews, as the human-annotated summaries do not exist in most domains. 2.3 Translation-Based Review Modeling The translation-based review modeling module aims to learn aspect and sentiment embeddings for reviews (the left block in Figure 2). For each review ri in the group G, we encode it using a Transformer (Vaswani et al., 2017) encoder E, and the output encoding hi ∈ R|ri |×k is: (1) (|ri |) hi = E(ri ) = (hi , · · · , hi ), (1) where k is the embedding dimension. Inspired by Zhong et al. (2019), we initialize the token embeddings of E with the ones of the BERT-base model (Devlin et al., 2019). Then we use projection matrices Aa ∈ Rk×k and As ∈ Rk×k to project hi to the aspect and sentiment spaces as ai and si (the blue and red squares in Figure 2), respectively. (|ri |) (1) ai = hi Aa = (ai , · · · , ai (|ri |) (1) si = hi As = (si , · · · , si ) (2) ) (3) ˆi = For use, we further denoteP a P|rlater (j) |ri |(j) i| 1 k ) and s ˆ a (ˆ a ∈ R = i i j=1 i j=1 si |ri | 1 |ri | (ˆ si ∈ Rk ) to represent the mean vectors of the embeddings, repectively. Translation-Based Reconstruction: We assume that each review is “an opinion summary” of the user’s intention and attitude, and model the"
2021.findings-acl.65,W14-4408,0,0.0634473,"Missing"
2021.findings-acl.65,C10-1039,0,0.343756,"ti-document summarization method that uses word embeddings (Mikolov et al., 2013) instead of TF-IDF to represent each sentence (Rossiello et al., 2017); and (3) Multi-Lead1 (See et al., 2017) which constructs the summary by selecting the leading sentences from each review of a group. Additionally, we also report the upper bound of extractive methods, i.e., the highest-scoring review in a group when computing ROUGE-L (Lin and Hovy, 2003) against reference summaries. We also compare with six state-of-the-art abstractive models where summaries are generated from scratch, including: (1) Opinosis (Ganesan et al., 2010), a graph-based method that uses token-level redundancy to generate summaries; (2) MeanSum (Chu and Liu, 2019), an auto-encoder that generates summaries by reconstructing the mean of review encodings, which is in fact special cases of our method without contrastive transformations of aspect and sentiment embeddings and high-relevance dataset creation; (3) OpinionDigest (Suhara et al., 2020), a combination of an aspect-based sentiment analysis model and a phrase-to-review seq2seq model, which can be seen as using opinion phrases to model summaries rather than using the aspect and sentiment embe"
2021.findings-acl.65,N16-1095,0,0.0426751,"Missing"
2021.findings-acl.65,N03-1020,0,0.799245,"1 https://github.com/sosuperic/MeanSum https://github.com/abrazinskas/Copycat-abstractiveopinion-summarizer 3 https://web.eecs.umich.edu/∼wangluxy/data.html 2 a centroid-based multi-document summarization method that uses word embeddings (Mikolov et al., 2013) instead of TF-IDF to represent each sentence (Rossiello et al., 2017); and (3) Multi-Lead1 (See et al., 2017) which constructs the summary by selecting the leading sentences from each review of a group. Additionally, we also report the upper bound of extractive methods, i.e., the highest-scoring review in a group when computing ROUGE-L (Lin and Hovy, 2003) against reference summaries. We also compare with six state-of-the-art abstractive models where summaries are generated from scratch, including: (1) Opinosis (Ganesan et al., 2010), a graph-based method that uses token-level redundancy to generate summaries; (2) MeanSum (Chu and Liu, 2019), an auto-encoder that generates summaries by reconstructing the mean of review encodings, which is in fact special cases of our method without contrastive transformations of aspect and sentiment embeddings and high-relevance dataset creation; (3) OpinionDigest (Suhara et al., 2020), a combination of an aspe"
2021.findings-acl.65,N15-1114,0,0.0271232,"ce of considering the relevance between reviews and pseudo-summaries. 4 studies of abstractive summarization methods have increased tremendously (Ganesan et al., 2010; Perez-Beltrachini et al., 2019; Zou et al., 2020; Mukherjee et al., 2020). Most of these abstractive works model the problem of opinion summarization as a normal multi-document summarization task, using an auto-encoder framework with attention (Chu and Liu, 2019; Amplayo and Lapata, 2019; Brazinskas et al., 2020a), variational distributions (Brazinskas et al., 2020b; Angelidis et al., 2020), or abstract meaning representations (Liu et al., 2015). Few of them pay attention to the opinion information, and model the opinion summary with opinion phrases (Suhara et al., 2020) or the aspect and sentiment distributions (Amplayo et al., 2021). To the best of our knowledge, we are the first to model opinion summaries with only aspect and sentiment embeddings, which are learned through two novel contrastive objectives based on the aspect and sentiment invariances. Our work is also related to contrastive learning, which a popular unsupervised learning paradigm in the field of computer vision and speech, aiming to enlarge the embedding disagreem"
2021.findings-acl.65,N15-1000,0,0.166364,"Missing"
2021.findings-acl.65,N19-4009,0,0.0212351,"Missing"
2021.findings-acl.65,D10-1007,0,0.127744,"ing, such as “ What is the biggest complaint on the iPod ‘screen’ ?”. However, compared with supervised summarization in the domain of news articles, the annotated training data for opinion summarization is expensive to acquire. Due to the lack of gold-standard summaries for training, most existing works focus on unsupervised opinion summarization and treat it as a normal multi-document summarization task. They either struggle to reduce the opinion redundancy efficiently or output summaries lacking relevance to input reviews. Particularly, many previous studies focus on extractive approaches (Paul et al., 2010; Fabbrizio et al., 2014; Rossiello et al., 2017; Narayan et al., 2019), which copy texts from the input reviews but tend to be redundant and less informative (Chu and Liu, 2019). Some recently proposed abstractive methods are based on unsupervised representation learning, such as autoencoder (Chu and Liu, 2019; Amplayo and Lapata, 2019; Brazinskas et al., 2020a) or variational autoencoder (Brazinskas et al., 2020b; Angelidis et al., 2020), but mainly focus on the content transformation within each group of reviews. Other studies aim to create synthetic reviews-summary pairs to train a supervi"
2021.findings-acl.65,P19-1504,0,0.0175158,"row#5) in ROUGE-L when the translation-based review modeling module is removed, demonstrating the great effectiveness of the aspect and sentiment embeddings learned through contrastive learning. Interestingly, even without learning aspect and sentiment embeddings, using high-relevant reviews-summary pairs created by only entangled representations (i.e., row#5) can also achieve competitive results. This further shows the importance of considering the relevance between reviews and pseudo-summaries. 4 studies of abstractive summarization methods have increased tremendously (Ganesan et al., 2010; Perez-Beltrachini et al., 2019; Zou et al., 2020; Mukherjee et al., 2020). Most of these abstractive works model the problem of opinion summarization as a normal multi-document summarization task, using an auto-encoder framework with attention (Chu and Liu, 2019; Amplayo and Lapata, 2019; Brazinskas et al., 2020a), variational distributions (Brazinskas et al., 2020b; Angelidis et al., 2020), or abstract meaning representations (Liu et al., 2015). Few of them pay attention to the opinion information, and model the opinion summary with opinion phrases (Suhara et al., 2020) or the aspect and sentiment distributions (Amplayo e"
2021.findings-acl.65,P19-1101,0,0.0156749,"ng baselines, especially in terms of relevance and non-redundancy. 2 2.1 TransSum Overview As aforementioned, a good opinion summary needs to cover major opinions/sentiments on different aspects of the entity (e.g., a movie, product, business) discussed in a group of reviews. Inspired by this observation, we propose a self-supervised framework (titled TransSum), aiming to generate opinion summaries without access to expensive annotations by interpreting them as translations operating on the aspect and sentiment embeddings. As noted in a recent theoretical model of importance in summarization (Peyrard, 2019), a good summary should meet three requirements: (i) minimum redundancy, (ii) maximum relevance with the input document(s), and (iii) maximum informativeness. Based on the observation that reviews are usually created to express users’ sentiments on certain aspects of a specific entity (e.g., the price and battery of a PC), we reasonably define informativeness, the amount of new information contained in the opinion summary relative to the background knowledge, as the aspect and sentiment information. The purpose is to reduce unnecessary information in the opinion summary, such as personal infor"
2021.findings-acl.65,W17-1003,0,0.409027,"t on the iPod ‘screen’ ?”. However, compared with supervised summarization in the domain of news articles, the annotated training data for opinion summarization is expensive to acquire. Due to the lack of gold-standard summaries for training, most existing works focus on unsupervised opinion summarization and treat it as a normal multi-document summarization task. They either struggle to reduce the opinion redundancy efficiently or output summaries lacking relevance to input reviews. Particularly, many previous studies focus on extractive approaches (Paul et al., 2010; Fabbrizio et al., 2014; Rossiello et al., 2017; Narayan et al., 2019), which copy texts from the input reviews but tend to be redundant and less informative (Chu and Liu, 2019). Some recently proposed abstractive methods are based on unsupervised representation learning, such as autoencoder (Chu and Liu, 2019; Amplayo and Lapata, 2019; Brazinskas et al., 2020a) or variational autoencoder (Brazinskas et al., 2020b; Angelidis et al., 2020), but mainly focus on the content transformation within each group of reviews. Other studies aim to create synthetic reviews-summary pairs to train a supervised multi-document summarization model (Amplayo"
2021.findings-acl.65,P17-1099,0,0.0118508,"selects the review closest to the centroid of a group as the summary; (2) W2VCent, We also explore non-equal weighting of the losses but do not find a meaningful difference in outcomes. We perform beam search decoding in the inference stage. 733 1 https://github.com/sosuperic/MeanSum https://github.com/abrazinskas/Copycat-abstractiveopinion-summarizer 3 https://web.eecs.umich.edu/∼wangluxy/data.html 2 a centroid-based multi-document summarization method that uses word embeddings (Mikolov et al., 2013) instead of TF-IDF to represent each sentence (Rossiello et al., 2017); and (3) Multi-Lead1 (See et al., 2017) which constructs the summary by selecting the leading sentences from each review of a group. Additionally, we also report the upper bound of extractive methods, i.e., the highest-scoring review in a group when computing ROUGE-L (Lin and Hovy, 2003) against reference summaries. We also compare with six state-of-the-art abstractive models where summaries are generated from scratch, including: (1) Opinosis (Ganesan et al., 2010), a graph-based method that uses token-level redundancy to generate summaries; (2) MeanSum (Chu and Liu, 2019), an auto-encoder that generates summaries by reconstructing"
2021.findings-acl.65,P19-1100,0,0.0268932,"· , rn } in terms of informativeness. Note that we cannot access gold-standard opinion summaries for each group of reviews, as the human-annotated summaries do not exist in most domains. 2.3 Translation-Based Review Modeling The translation-based review modeling module aims to learn aspect and sentiment embeddings for reviews (the left block in Figure 2). For each review ri in the group G, we encode it using a Transformer (Vaswani et al., 2017) encoder E, and the output encoding hi ∈ R|ri |×k is: (1) (|ri |) hi = E(ri ) = (hi , · · · , hi ), (1) where k is the embedding dimension. Inspired by Zhong et al. (2019), we initialize the token embeddings of E with the ones of the BERT-base model (Devlin et al., 2019). Then we use projection matrices Aa ∈ Rk×k and As ∈ Rk×k to project hi to the aspect and sentiment spaces as ai and si (the blue and red squares in Figure 2), respectively. (|ri |) (1) ai = hi Aa = (ai , · · · , ai (|ri |) (1) si = hi As = (si , · · · , si ) (2) ) (3) ˆi = For use, we further denoteP a P|rlater (j) |ri |(j) i| 1 k ) and s ˆ a (ˆ a ∈ R = i i j=1 i j=1 si |ri | 1 |ri | (ˆ si ∈ Rk ) to represent the mean vectors of the embeddings, repectively. Translation-Based Reconstruction: We"
2021.findings-acl.65,2020.acl-main.513,0,0.0807942,"up when computing ROUGE-L (Lin and Hovy, 2003) against reference summaries. We also compare with six state-of-the-art abstractive models where summaries are generated from scratch, including: (1) Opinosis (Ganesan et al., 2010), a graph-based method that uses token-level redundancy to generate summaries; (2) MeanSum (Chu and Liu, 2019), an auto-encoder that generates summaries by reconstructing the mean of review encodings, which is in fact special cases of our method without contrastive transformations of aspect and sentiment embeddings and high-relevance dataset creation; (3) OpinionDigest (Suhara et al., 2020), a combination of an aspect-based sentiment analysis model and a phrase-to-review seq2seq model, which can be seen as using opinion phrases to model summaries rather than using the aspect and sentiment embeddings as we do; (4) DenoiseSum (Amplayo and Lapata, 2020), which create a synthetic dataset by treating a review and its noisy versions as the summary and pseudo-review input, instead of using the aspect similarity of real-world reviews like ours; (5) CopyCat (Brazinskas et al., 2020b), a hierarchical variational auto-encoder which learns a latent code of the summary and uses a leave-one-o"
2021.findings-acl.65,2020.emnlp-main.297,0,0.0317025,"lation-based review modeling module is removed, demonstrating the great effectiveness of the aspect and sentiment embeddings learned through contrastive learning. Interestingly, even without learning aspect and sentiment embeddings, using high-relevant reviews-summary pairs created by only entangled representations (i.e., row#5) can also achieve competitive results. This further shows the importance of considering the relevance between reviews and pseudo-summaries. 4 studies of abstractive summarization methods have increased tremendously (Ganesan et al., 2010; Perez-Beltrachini et al., 2019; Zou et al., 2020; Mukherjee et al., 2020). Most of these abstractive works model the problem of opinion summarization as a normal multi-document summarization task, using an auto-encoder framework with attention (Chu and Liu, 2019; Amplayo and Lapata, 2019; Brazinskas et al., 2020a), variational distributions (Brazinskas et al., 2020b; Angelidis et al., 2020), or abstract meaning representations (Liu et al., 2015). Few of them pay attention to the opinion information, and model the opinion summary with opinion phrases (Suhara et al., 2020) or the aspect and sentiment distributions (Amplayo et al., 2021). To t"
2021.findings-acl.65,N16-1007,0,0.018082,"742 / 24.9 2,458 / 83.3 Training Finally, we optimize the sum of the above losses: Lf inal = Lrec + Lasp + Lsen + Lreg + Lsum . (12) 3 Experiments Datasets: We conduct experiments on three opinion summarization benchmarks in different domains, including: (1) Yelp (Chu and Liu, 2019) which contains business customer reviews from the Yelp Dataset Challenge1 ; (2) Amazon (Brazinskas et al., 2020b) which includes a large corpus of product reviews for four Amazon categories (i.g., Electronics, Clothing, Shoes and Jewelry, Home and Kitchen, and Health and Personal Care)2 ; (3) Rotten Tomatoes (RT) (Wang and Ling, 2016) which has a large set of reviews for various movies written by critics3 . The detailed statistics of the three datasets are shown in Table 1. For Yelp and Amazon, there are no gold standard summaries for large training corpora, but the small development and test sets have summaries written by Amazon Mechanical Turk (AMT) crowd-workers. In RT, each set of reviews has a gold-standard opinion summary written by an editor, but we do not use ground truth summaries for training due to the unsupervised setting. Note that all reviews have a binary sentiment label (e.g., positive or negative). For Yel"
2021.findings-acl.65,2020.emnlp-main.294,0,0.0299265,"Missing"
2021.findings-emnlp.223,D18-1258,0,0.0181433,"put is more succinct, constrained and learning and the availability of large-scale data, QA has received increasing attention from researchers. targeted (Kry´sci´nski et al., 2019). QA-based source code comprehension has direct In recent years, QA has been applied into broad use in education to facilitate programming learning, application domains, such as news (Hermann et al., 2015; Trischler et al., 2016), science (Khot et al., where a system automatically answers questions 2018; Hardalov et al., 2020), movies (Miller et al., about codes that someone has read. A more gen2016), medical field (Pampari et al., 2018), etc. eral use is to help improve software maintenance since it can advance the readability of code. MoreAmong QA’s wide applications, code QA is an over, it can provide diverse information that can be appealing application scenario on account of the leveraged to help perform a wide range of software distinctive nature of code differing from text. engineering tasks, such as bug detection, specificaIn this study, we focus on generating QA pairs for source code for the purpose of source code com- tion inference, testing and code synthesis. prehension. QA-based source code comprehension However,"
2021.findings-emnlp.223,P02-1040,0,0.110459,"Missing"
2021.findings-emnlp.223,D19-1051,0,0.0461508,"Missing"
2021.findings-emnlp.223,N18-1202,0,0.02226,"posed by LeClair et al. (2019). Structural information can be also encoded into tree structure encoders such as Tree-LSTM (Shido et al., 2019), Tree-Transformer (Harer et al., 2019), and Graph Neural Network (LeClair et al., 2020). Besides, other techniques like reinforcement learning (Wan et al., 2018), dual learning (Wei et al., 2019), retrieval-based techniques (Zhang et al., 2020), language-agnostic representation learning (Zügner et al., 2021) further enhance the code summarization models. Recently, neural architectures like Transformer (Vaswani et al., 2017) and large pretrained models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Liu et al., 2019b) have brought improvements on code summarization task. Representative works are transformer designed for code (Ahmad et al., 2020), CodeBERT(Feng et al., 2020), which is a model pre-trained on the CodeSearchNet (Husain et al., 2019) dataset for programming and natural languages. 3 The Code QA Task In this work, we focus on building a dataset and setting up baselines for the following code QA task: Given a source code c and a natural language question q about c, a free-form textual answer a is required to be generated. The textual a"
2021.findings-emnlp.223,2021.ccl-1.108,0,0.0580106,"Missing"
2021.findings-emnlp.223,D16-1264,0,0.0314912,"g Question answering has a long history and has attracted increasing attention in recent years. Question Answering tasks are usually divided into four categories (Chen, 2018; Qiu et al., 2019; Liu et al., 2019a): cloze tests, multiple-choice, span preDue to the varied nature of code comments, Cod- diction and free-form answering. A few exameQA covers a variety of information containing ples of QA datasets in each category are CNN & in codes, ranging from method to variable. We Daily Mail (Hermann et al., 2015), RACE (Lai analyze our dataset and classify all the generated et al., 2017), SQuAD (Rajpurkar et al., 2016), MS QA pairs into four categories: functionality, pur- MARCO (Nguyen et al., 2016). Compared with pose, property and workflow. Our experiments other categories, free-form answering tasks show with several baseline models demonstrate that neu- their superiority in the dimensions of understandral models struggle to generate correct answers. ing, flexibility, and application, which are the closThese results suggest that our dataset could serve est to practical application. However, the flexibilas a useful groundwork for QA-based source code ity of the answer form brings difficulty to build compr"
2021.findings-emnlp.223,P17-1099,0,0.0504294,"s models on Java dataset. ∗ means the result is obtained on only 100 questions sampled in the respective dataset. We evaluate the 100 answers generated by CodeBERT and that given by an experienced programmer respectively. The results are used only for comparing CodeBERT and Human. Seq2seq Dual Encoder Transformer CodeBERT CodeBERT∗ Human∗ BLEU 29.70 28.57 30.85 33.84 34.02 56.21 ROUGE 22.18 17.62 23.96 30.27 29.89 60.65 Dev METEOR 6.36 4.43 7.91 11.51 12.18 26.32 EM 2.95 2.22 3.37 5.50 8.00 34.00 F1 23.67 18.78 25.30 31.57 35.89 61.98 Table 8: Performance of various models on Python dataset. (See et al., 2017). While originally designed for text-to-text generation, it is commonly used in free-form question-answering as well (Nguyen et al., 2016). The input of the model is in the form of “[CLS] Question [SEP] Code”. Since models using original code tokens could perform better than models using abstract syntax tree (AST) sequences (Ahmad et al., 2020), we employ the code tokens as input for all baseline models. • Dual Encoder: A seq2seq model with two encoders. The model first builds a code representation and a question representation by its code-info encoder and question-info encoder respectively. A"
2021.findings-emnlp.223,N18-2074,0,0.0339673,"Missing"
2021.findings-emnlp.223,D16-1147,0,0.04526,"Missing"
2021.findings-emnlp.223,2020.emnlp-main.523,0,0.0244352,"source code ity of the answer form brings difficulty to build comprehension. datasets (Liu et al., 2019a). On these datasets, earPrior work (Bansal et al., 2021) built a dataset lier work in question answering employed rulefor code QA, but only a third of their questions are based and machine-learning-based methods. Refree-form. The main differences between our work cent deep-learning techniques leveraged neural netand prior work are: first, all of our questions are works with different attention mechanisms and prefree-form; second, our questions have diverse tex- trained text representation (Yamada et al., 2020; tual expressions and they are asking about informa- He et al., 2020), improving the ability of extracttion of various granularity in code, while questions ing contextual information and context-question in prior work are mostly fixed and on the single interaction. granularity. In code QA, Bansal et al. (2021) designed a Therefore, the contributions of this paper are as context-based QA system for basic questions about 2619 subroutines and evaluated the system by an RNNbased encoder-decoder network. They define the “basic” question as a question about a small detail of a method, such as “What"
2021.findings-emnlp.89,2020.acl-main.535,0,0.0200008,"tionship DNPG to decompose a sentence into sentence-level guided Paraphrase Generation), which is based pattern and phrase-level pattern to make neural on an automatically constructed pseudo document paraphrase generation more controllable. Goyal paraphrase dataset. Though the paraphrases in the and Durrett (2020) used syntactic transformations pseudo dataset do not involve inter-sentence diverto softly “reorder” the source sentence and guide sity, our model can learn the coherence relations beneural model to generate more diverse paraphrase. tween sentences via a coherence relationship graph Kazemnejad et al. (2020) explored to generate paragenerated by ALBERT (Lan et al., 2020), and make phrase by editing the original sentence. use of the learned coherence-aware representations Beside, there is another way to generate paraof sentences to reorder them, while keeping good phrase, called “pivoting"", which leverages backcoherence of the generated document. translation to introduce diversity. Recently, Our model consists of three parts: sentence en- Mallinson et al. (2017) revisited this method with coder, graph GRU and decoder. Sentence encoder neural machine translation to improve the paraonly encodes each"
2021.findings-emnlp.89,P19-1332,0,0.0181064,"velopment of deep learning, most paraand reordering the sentences in original docuphrasing models are based on seq2seq model. ment while still maintaining the original semantics Prakash et al. (2016) leveraged stacked residual and inter-sentence coherence. Due to the lack of LSTM networks to generate paraphrases. Gupta parallel document-level paraphrase pairs, it is not et al. (2018) found deep generative model such as possible to straightforwardly train a sequence-tovariational auto-encoder can improve the quality of sequence paraphrasing model to address this task. paraphrase significantly. Li et al. (2019) proposed We thus propose CoRPG (Coherence Relationship DNPG to decompose a sentence into sentence-level guided Paraphrase Generation), which is based pattern and phrase-level pattern to make neural on an automatically constructed pseudo document paraphrase generation more controllable. Goyal paraphrase dataset. Though the paraphrases in the and Durrett (2020) used syntactic transformations pseudo dataset do not involve inter-sentence diverto softly “reorder” the source sentence and guide sity, our model can learn the coherence relations beneural model to generate more diverse paraphrase. twee"
2021.findings-emnlp.89,E17-1083,0,0.0266663,"can learn the coherence relations beneural model to generate more diverse paraphrase. tween sentences via a coherence relationship graph Kazemnejad et al. (2020) explored to generate paragenerated by ALBERT (Lan et al., 2020), and make phrase by editing the original sentence. use of the learned coherence-aware representations Beside, there is another way to generate paraof sentences to reorder them, while keeping good phrase, called “pivoting"", which leverages backcoherence of the generated document. translation to introduce diversity. Recently, Our model consists of three parts: sentence en- Mallinson et al. (2017) revisited this method with coder, graph GRU and decoder. Sentence encoder neural machine translation to improve the paraonly encodes each sentence in the document indi- phrase quality. Wieting and Gimpel (2018) levervidually. We propose graph GRU, which combines aged bidirectional translation model to construct graph attention (Velickovic et al., 2018) and GRU, paraNMT, which is a very large sentence-level parato catch the coherence relationship information. phrase dataset. Finally, the outputs of graph GRU and sentence enAll works above focus on sentence-level paracoder are concatenated and"
2021.findings-emnlp.89,J83-1001,0,0.712814,"ssion. 3 We have on the edge of collapse, 5 but I worry that the worse things are yet to come. Table 1: An example for sentence reordering, splitting and merging in document paraphrase. The number before each sentence in the paraphrased document indicates the corresponding original sentence from the input document. changing its original meaning, is a more valuable and challenging task. However, because of the lack of parallel corpora, there is few research on document-level paraphrase generation. The differ1 Introduction ence between sentence-level paraphrase generation Paraphrase generation (McKeown, 1983; Barzilay and document-level paraphrase generation is that and Lee, 2003) is an important task in natural lan- the former task only focuses on the lexical and synguage processing, and it aims to rewrite a text tactic diversity of a sentence, while the latter task in other forms while preserving original seman- also needs to introduce the diversity across multitics. Paraphrase generation has many applications ple sentences (we call it inter-sentence diversity), in other down-stream tasks, such as text summa- such as sentence reordering, sentence merging and rization (Cao et al., 2017), dialogu"
2021.findings-emnlp.89,D19-1231,0,0.0272105,"iven in the next sections. {S 1 , S 2 , · · · , S N }, where N is the total number of sentences in the document and S i is the i-th sentence in the document. Because of lack of gold document paraphrase dataset, we leverage an off-the-shelf sentence paraphrasing model to generate a pseudo document paraphrase Dp = {Sp1 , Sp2 · · · , SpN } sentence by sentence, where Spi is obtained by paraphrasing S i with the sentence paraphrasing model. Then, we set Dp as input and D as target to train our model. 3.1.3 Coherence Relationship Graph There are many works focusing on text coherence, such as NCOH (Moon et al., 2019) and CohEval (Mohiuddin et al., 2020). Many pre-trained models also introduce text coherence as a subtask to improve the generalization ability. For example, Devlin et al. (2019) employed next sentence prediction (NSP) task to train BERT; Lan et al. (2020) proposed ALBERT, which leverages sentence order prediction (SOP) to catch the inter-sentence coherence better. We employ the SOP probability of ALBERT to measure the inter-sentence coherence. The coherence relationship graph G for document Dp = {Sp1 , Sp2 , · · · , SpN } takes sentences as nodes and the edge is defined as follows: 3.1.2 Pseu"
2021.findings-emnlp.89,Q17-1008,0,0.0236862,", 2018) is used to aggregate the information from neighbor nodes. We calculate the graph attention between sentence vectors Dr and the outputs of l-th layer Gl . For simplicity and clarity, we omit the layer index l for nodes. The aggregate operation is as follow: GAT(ri , G) = X αij gj WV (2) G(i,j)=1 gj ∈G Graph GRU Most of the previous graph models focus on encod- where WV ∈ Rdmodel ×du . αij is the attention ing the semantic information in the graph (Beck coefficient computed as follow: et al., 2018; Guo et al., 2019) or leveraging graph information to guide sequence encoding or decoding (Peng et al., 2017). However, few work focuses sij = (ri WQ ) (gj WK )> on the coherence relationship in the graph. In this (3) exp (sij ) section, we propose the graph GRU to explore the αij = P G(i,k)=1 exp (sik ) coherence relationship between sentences. We assume there exists coherence relationship between S i and S j if G(i, j) = 1. For a sentence in the where WQ , WK ∈ Rdmodel ×du are learnable pacoherence relationship graph, there may be more rameters. Notice that, there exists sink node in the than one precursor, and we leverage graph attention to aggregate the information from all its precursors. cohere"
2021.findings-emnlp.89,C16-1275,0,0.159184,"tion has been changed. These paraphrase with high diversity, semantic relevance operations can effectively improve the diversity and coherence. Our code is publicly available at of document paraphrase, but they are beyond the https://github.com/L-Zhe/CoRPG. ability of sentence-level paraphrasing model. 2 Related Work In this work, we conduct a pilot study of this challenging task and focus only on rewriting With the development of deep learning, most paraand reordering the sentences in original docuphrasing models are based on seq2seq model. ment while still maintaining the original semantics Prakash et al. (2016) leveraged stacked residual and inter-sentence coherence. Due to the lack of LSTM networks to generate paraphrases. Gupta parallel document-level paraphrase pairs, it is not et al. (2018) found deep generative model such as possible to straightforwardly train a sequence-tovariational auto-encoder can improve the quality of sequence paraphrasing model to address this task. paraphrase significantly. Li et al. (2019) proposed We thus propose CoRPG (Coherence Relationship DNPG to decompose a sentence into sentence-level guided Paraphrase Generation), which is based pattern and phrase-level pattern"
2021.findings-emnlp.89,P18-1042,0,0.149327,"an et al., 2020), and make phrase by editing the original sentence. use of the learned coherence-aware representations Beside, there is another way to generate paraof sentences to reorder them, while keeping good phrase, called “pivoting"", which leverages backcoherence of the generated document. translation to introduce diversity. Recently, Our model consists of three parts: sentence en- Mallinson et al. (2017) revisited this method with coder, graph GRU and decoder. Sentence encoder neural machine translation to improve the paraonly encodes each sentence in the document indi- phrase quality. Wieting and Gimpel (2018) levervidually. We propose graph GRU, which combines aged bidirectional translation model to construct graph attention (Velickovic et al., 2018) and GRU, paraNMT, which is a very large sentence-level parato catch the coherence relationship information. phrase dataset. Finally, the outputs of graph GRU and sentence enAll works above focus on sentence-level paracoder are concatenated and used as input to decoder phrase generation, and to the best of our knowledge, to generate the paraphrase. Extensive evaluations there is no research on document-level paraphrase are performed and our model gets"
2021.findings-emnlp.89,W16-0109,0,0.0288498,"ortant task in natural lan- the former task only focuses on the lexical and synguage processing, and it aims to rewrite a text tactic diversity of a sentence, while the latter task in other forms while preserving original seman- also needs to introduce the diversity across multitics. Paraphrase generation has many applications ple sentences (we call it inter-sentence diversity), in other down-stream tasks, such as text summa- such as sentence reordering, sentence merging and rization (Cao et al., 2017), dialogue system, ques- splitting. Sentence reordering is to reorder the sention answering (Xu et al., 2016), semantic parsing tences without significantly deteriorating the co(Berant and Liang, 2014) and so on. Inspired by herence of the document. Sentence merging and the success of deep learning, most paraphrase sys- splitting aim to merge two or more sentences into tems leverage existing paraphrase corpora to train one sentence, and vice versa. An example about a seq2seq model, such as variational auto-encoder document-level paraphrase is shown in Table 1. As (Gupta et al., 2018), syntactic pre-ordering (Goyal is shown in this example, there is inter-sentence and Durrett, 2020) and so on. All the"
2021.findings-emnlp.89,N10-1057,0,0.0444244,"ne translation. We sample 3000 documents (without references) from News Commentary for test. Appx.B shows more details about the data. 4.2 Evaluation We evaluate document paraphrases on three aspects: Diversity, Semantic Relevancy and Coherence. Diversity: Previous works use self-BLEU, which calculate BLEU score between original text and generated paraphrase, to measure the diversity of paraphrase. However, we find that BLEU score may not be suitable for document-level paraphrase generation task, as it only measures the diversity of N-gram phrase and ignores the inter-sentence diversity. TER (Zaidan and Callison-Burch, 2010) 4 and WER 5 are used to evaluate machine translation and automatic speech recognition based on edit distance. Previous works also employ self-TER and self-WER to evaluate the diversity of paraphrase (Gupta et al., 2018; Goyal and Durrett, 2020). So we add self-TER and self-WER to evaluate the document-level diversity. Semantic Relevancy: In addition to diversity, paraphrase also requires to preserve the semantic of the original input. We leverage BERTScore (Zhang et al., 2020) 6 to evaluate the semantic similarity between output and original document. {Sg1 , Sg2 , · · · , SgN } where Sgi is t"
2021.findings-emnlp.89,P17-1099,0,0.30089,"re as follow: We leverage Transformer decoder as our decoder. First, we combine the outputs of sentence encoder and graph GRU. ¯ l = MHGAT (Dr , Gl−1 ) G h n i  ¯ l Dr Wz zt = σ G h n i  ¯ l Dr Wr rt = σ G h  n i ˜l = tanh rt ⊗ G ¯ l D r Wm G where Wc ∈ R2dmodel ×dmodel , bc ∈ Rdmodel , gi ∈ Go . In order to avoid overfitting, we add dropout after ReLU function. The combination operation above can be regarded as introducing the inter-sentence coherence relationship information to each sentence embedding matrix. Then, we send dc into decoder to guide the generation. We add copy mechanism(See et al., 2017). We leverage the average attention weight over all heads in the last decoder layer as the copy probability to calculate the final output’s probability. f (5) ˆ l = (1 − zt ) ⊗ G ¯ l + zt ⊗ G ˜l G where σ is the sigmoid activation function, ⊗ is the element-wise product between matrices, and Wz , Wr , Wm ∈ R2dmodel ×dmodel . Finally, we leverage layer normalization (Ba ˆ l . The outputs of let al., 2016) to normalize G th graph GRU layer is as follow: ˆl) Gl = LayerNorm(G (6) h n in Sci = hij gi j=1  1 2  d˜c = Sc , Sc , · · · , ScN Wc + bc    dc = LayerNorm ReLU d˜c 3.5 (8) Diversity Co"
2021.naacl-main.310,2020.acl-main.692,0,0.0484041,"Missing"
2021.naacl-main.310,J90-2002,0,0.892835,"ed to the competitive models. In summary, the prime contributions of this paper are as follows: • We propose a novel continual learning framework for neural machine translation. Compared with existing works, we consider a more general scenario where the training is comprised of multiple stages. 2 2.1 Related Works Neural Machine Translation The task of machine translation is to automatically translate a written text from one natural language into another. Early machine translation systems are mostly built upon statistical learning techniques, which mainly rely on various count-based features (Brown et al., 1990; Och, 2003; Koehn et al., 2007). Recently, statistical machine translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-"
2021.naacl-main.310,W18-2705,0,0.273296,"llation-based methods transfer important knowledge from an old model to a new model • Experimental results in three different settings through a teacher-student framework. Usually, a all show that the proposed method obtains su- modified cross-entropy loss is adopted to preserve perior performance compared to competitive the knowledge of the old model. models.2 In the field of natural language processing, there 2 are some researches on solving catastrophic forCodes and data will be released once this paper gets accepted. getting problem in lifelong learning (Freitag and 3965 Al-Onaizan, 2016; Khayrallah et al., 2018; Saunders et al., 2019; Thompson et al., 2019). However, these works only consider the scenario of one-stage incremental training. To the best of our knowledge, there is no previous work that takes into account the scenario in which the training consists of multiple stages. Domain adaptation learning (or transfer learning) is a task similar to continual learning. The difference is that domain adaptation learning only cares about the performance of in-domain data, while continual learning cares about not only the performance on in-domain data, but also the performance on out-of-domain data. 3"
2021.naacl-main.310,P07-2045,0,0.0258947,"n summary, the prime contributions of this paper are as follows: • We propose a novel continual learning framework for neural machine translation. Compared with existing works, we consider a more general scenario where the training is comprised of multiple stages. 2 2.1 Related Works Neural Machine Translation The task of machine translation is to automatically translate a written text from one natural language into another. Early machine translation systems are mostly built upon statistical learning techniques, which mainly rely on various count-based features (Brown et al., 1990; Och, 2003; Koehn et al., 2007). Recently, statistical machine translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corpus via continual trai"
2021.naacl-main.310,2015.iwslt-evaluation.11,0,0.0278057,"ne translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corpus via continual training (Sennrich et al., 2016; Luong and Manning, 2015). However, this suffers from the problem of catastrophic forgetting (French, 1993) that the performance of the model on the general domain has decreased drastically. In this work, we aim to mitigate the catastrophic forgetting for NMT models. As for the bias in NMT systems, Michel and Neubig (2018) 2018 adapt the bias of the output softmax to build a personalized NMT model. Different from their work, we propose to elinamate the bias in the output layer. 2.2 Continual Learning Most of continual learning models are proposed for computer vision tasks. These models mainly fall into parameter-based"
2021.naacl-main.310,D15-1166,0,0.0618651,"general scenario where the training is comprised of multiple stages. 2 2.1 Related Works Neural Machine Translation The task of machine translation is to automatically translate a written text from one natural language into another. Early machine translation systems are mostly built upon statistical learning techniques, which mainly rely on various count-based features (Brown et al., 1990; Och, 2003; Koehn et al., 2007). Recently, statistical machine translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corpus via continual training (Sennrich et al., 2016; Luong and Manning, 2015). However, this suffers from the problem of catastrophic forgetting (French, 1993) that the performance of the model on the general domain ha"
2021.naacl-main.310,P18-2050,0,0.0174674,"systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corpus via continual training (Sennrich et al., 2016; Luong and Manning, 2015). However, this suffers from the problem of catastrophic forgetting (French, 1993) that the performance of the model on the general domain has decreased drastically. In this work, we aim to mitigate the catastrophic forgetting for NMT models. As for the bias in NMT systems, Michel and Neubig (2018) 2018 adapt the bias of the output softmax to build a personalized NMT model. Different from their work, we propose to elinamate the bias in the output layer. 2.2 Continual Learning Most of continual learning models are proposed for computer vision tasks. These models mainly fall into parameter-based methods (Aljundi et al., 2018; Kirkpatrick et al., 2016; Zenke et al., 2017) and distillation-based methods (Aljundi et al., 2017; • We propose a novel method to alleviate the Triki et al., 2017; Hou et al., 2018, 2019; Wu et al., problem of catastrophic forgetting in a system- 2019). The paramete"
2021.naacl-main.310,N18-1031,0,0.0177045,"inear Projection To reveal the bias weights phenomenon in the linear projection in continual training, we conduct a test that first trains an English-German NMT model on an IT-related corpus, and then fine-tunes it on law-related corpus.3 We find that after fine-tuning on law-related data, the model will no longer generate IT-specific words even we feed an IT-related 3 The number of training samples for IT and law corpus are 232K and 205K respectively. 3967 3.3.2 Weight Normalization for Bias Correction Based on the above observation, we propose to add a weight normalization module similar to Nguyen and Chiang (2018) in the linear projection. Concretly, we normalize the weights for all words by: θˆi = θi / kθi k (10) and compute the probability of generating each word as: Figure 1: The changes of η with the training of Model1 and Model-2. This figure shows that directly finetuning the model on new data will cause the biased weights problem. source sentence to the model. As a consequence, the model performs extremely poorly on the IT test set. We hypothesize that the model reduces the old words’ probability by shrinking their corresponding weights in the last linear projection Θ. To verify this, we train t"
2021.naacl-main.310,P03-1021,0,0.10873,"e models. In summary, the prime contributions of this paper are as follows: • We propose a novel continual learning framework for neural machine translation. Compared with existing works, we consider a more general scenario where the training is comprised of multiple stages. 2 2.1 Related Works Neural Machine Translation The task of machine translation is to automatically translate a written text from one natural language into another. Early machine translation systems are mostly built upon statistical learning techniques, which mainly rely on various count-based features (Brown et al., 1990; Och, 2003; Koehn et al., 2007). Recently, statistical machine translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corp"
2021.naacl-main.310,N19-4009,0,0.0203816,"or the EWC-based • w/o dynamic knowledge distillation It re- model. moves the dynamic knowledge distillation By comparing results of our model with module from the proposed model. the knowledge distillation-based and EWC regularization-based methods, we can see that our • w/o bias correction It removes the bias cormodel outperforms them in all cases. The proposed rection module from the proposed model. model achieves an average improvement of 0.3 and 0.8 BLEU scores compared to the knowledge 4.3 Implementation Details distillation-based and EWC regularization-based We use the Fairseq toolkit (Ott et al., 2019) to imple- methods, respectively. ment the proposed model. We process the text into The above results confirm the finding of prior subword units by using the subword-nmt toolkit10 . works that the learning-without-forgetting strateWe adopt the transformer (Vaswani et al., 2017) gies can benefit the continual training, and demonas the model architecture. We set the model’s hidstrate that the proposed method adds more gains. den size, feed-forward hidden size to 512, 2048, We also study the effect of α in Eq. 3. A small and set the number of layers and the number of value of α indicates that the"
2021.naacl-main.310,P19-1022,0,0.0346712,"Missing"
2021.naacl-main.310,P16-1009,0,0.0339867,"ntly, statistical machine translation (SMT) has largely been superseded by neural machine translation (NMT), which tackles machine translation with deep neural networks (Luong et al., 2015; Vaswani et al., 2017). Most NMT models either use LSTM (Luong et al., 2015) or Transformer (Vaswani et al., 2017) architectures. NMT systems are sensitive to the data distributions (Stahlberg, 2019). To improve the performance of NMT models in low-resource domains, a widely-used technique is to train the model on a general domain corpus, and then fine-tune it on the in-domain corpus via continual training (Sennrich et al., 2016; Luong and Manning, 2015). However, this suffers from the problem of catastrophic forgetting (French, 1993) that the performance of the model on the general domain has decreased drastically. In this work, we aim to mitigate the catastrophic forgetting for NMT models. As for the bias in NMT systems, Michel and Neubig (2018) 2018 adapt the bias of the output softmax to build a personalized NMT model. Different from their work, we propose to elinamate the bias in the output layer. 2.2 Continual Learning Most of continual learning models are proposed for computer vision tasks. These models mainly"
2021.naacl-main.310,N19-1209,0,0.0261324,"ge from an old model to a new model • Experimental results in three different settings through a teacher-student framework. Usually, a all show that the proposed method obtains su- modified cross-entropy loss is adopted to preserve perior performance compared to competitive the knowledge of the old model. models.2 In the field of natural language processing, there 2 are some researches on solving catastrophic forCodes and data will be released once this paper gets accepted. getting problem in lifelong learning (Freitag and 3965 Al-Onaizan, 2016; Khayrallah et al., 2018; Saunders et al., 2019; Thompson et al., 2019). However, these works only consider the scenario of one-stage incremental training. To the best of our knowledge, there is no previous work that takes into account the scenario in which the training consists of multiple stages. Domain adaptation learning (or transfer learning) is a task similar to continual learning. The difference is that domain adaptation learning only cares about the performance of in-domain data, while continual learning cares about not only the performance on in-domain data, but also the performance on out-of-domain data. 3 Methods 3.1 Overall Given a bilingual translati"
C08-1122,W03-1805,0,0.309031,"sed. Unsupervised methods usually involve assigning a saliency score to each candidate phrases by considering various features. Krulwich and Burkey (1996) use heuristics based on syntactic clues to extract keyphrases from a document. Barker and Cornacchia (2000) propose a simple system for choosing noun phrases from a document as keyphrases. Muñoz (1996) uses an unsupervised learning algorithm to discover two-word keyphrases. The algorithm is based on Adaptive Resonance Theory (ART) neural networks. Steier and Belew (1993) use the mutual information statistics to discover two-word keyphrases. Tomokiyo and Hurst (2003) use pointwise KLdivergence between multiple language models for scoring both phraseness and informativeness of phrases. More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank keywords based on the co-occurrence links between words. Such algorithms make use of “voting” or “recommendations” between words to extract keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea (Frank et al., 1999; Witten et al., 1999) are two typical systems, and the most important features for"
C08-1122,W00-1308,0,0.00434601,"the following sentence: “Mad/JJ cow/NN disease/NN has/VBZ killed/VBN 10,000/CD cattle/NNS”, the candidate phrases are “Mad cow disease” and “cattle”. The score of a candidate phrase pi is computed by summing the cluster-level saliency scores of the words contained in the phrase. (4) PhraseScore( pi ) = ∑ WordScore clus (v j ) (6) v j ∈ pi And the matrix form is: r ~ r (1 − µ ) r λ = µM T λ + e |V | r where λ = [WordScoreclus (vi )]|V |×1 is the vector of (5) The corresponding POS tags of the candidate words include “JJ”, “NN”, “NNS”, “NNP”, “NNPS”. We used the Stanford log-linear POS tagger (Toutanova and Manning, 2000) in this study. All the candidate phrases in the document are ranked in decreasing order of the phrase scores and the top n phrases are selected as the keyphrases of the document. n ranges from 1 to 20 in this study. Similarly for SingleRank, the phrase score is computed based on the document-level saliency scores of the words. 972 4 4.1 Empirical Evaluation Data Set To our knowledge, there is no gold standard news dataset with assigned keyphrases for evaluation. So we manually annotated the DUC2001 dataset (Over, 2001) and used the annotated dataset for evaluation in this study. The dataset w"
C08-1122,W03-1028,0,0.114506,"lcea and Tarau (2004) propose the TextRank model to rank keywords based on the co-occurrence links between words. Such algorithms make use of “voting” or “recommendations” between words to extract keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea (Frank et al., 1999; Witten et al., 1999) are two typical systems, and the most important features for classifying a candidate phrase are the frequency and location of the phrase in the document. More linguistic knowledge has been explored by Hulth (2003). Statistical associations between keyphrases have been used to enhance the coherence of the extracted keyphrases (Turney, 2003). Song et al. (2003) present an information gain-based keyphrase extraction system called KPSpotter. Medelyan and Witten (2006) propose KEA++ that enhances automatic keyphrase extraction by using semantic information on terms and phrases gleaned from a domainspecific thesaurus. Nguyen and Kan (2007) focus on keyphrase extraction in scientific publications by using new features that capture salient morphological phenomena found in scientific keyphrases. The tasks of ke"
C08-1122,P07-1070,1,0.812169,"EA++ that enhances automatic keyphrase extraction by using semantic information on terms and phrases gleaned from a domainspecific thesaurus. Nguyen and Kan (2007) focus on keyphrase extraction in scientific publications by using new features that capture salient morphological phenomena found in scientific keyphrases. The tasks of keyphrase extraction and document summarization are similar and thus they have been conducted in a uniform framework. Zha (2002) proposes a method for simultaneous keyphrase extraction and text summarization by using the heterogeneous sentence-to-word relationships. Wan et al. (2007a) propose an iterative reinforcement approach to simultaneous keyphrase extraction and text summarization. Other related works include web page keyword extraction (Kelleher and Luz, 2005; Zhang et al., 2005; Chen et al., 2005), advertising keywords finding (Yih et al., 2006). To the best of our knowledge, all previous work conducts the task of keyphrase extraction for each single document independently, without making use of the collaborative knowledge in multiple documents. We focus on unsupervised methods in this study. 970 3 3.1 The Proposed CollabRank Approach Framework Description Given"
C08-1122,W04-3252,0,\N,Missing
C10-1128,P06-1039,0,0.0160039,"ities, and then compute sentence scores based on graph-based learning algorithms. For example, Wan (2008) proposes to use only cross-document relationships for graph building and sentence ranking. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). For topic-focused multi-document summarization, many methods are extensions of generic summarization methods by incorporating the information of the given topic or query into generic summarizers. In recent years, a few novel methods have been proposed for topic-focused summarization (Daumé and Marcu, 2006; Wan et al., 2007c; Nastase 2008; Li et al., 2008; Schilder and Kondadadi, 2008; Wei et al., 2008). The above previous graph-based summarization methods aim to address either singledocument summarization or multi-document summarization, and the two summarization tasks have not yet been addressed in a unified graphbased framework. 3 3.1 The Unified proach Summarization ApOverview Given a document set, in which the whole document set and each single document in the set are required to be summarized, we use local saliency to indicate the importance of a sentence in a particular document, and use"
C10-1128,W04-3247,0,0.0192565,"sentence extraction (Kupiec et al., 1995; Conroy and O’Leary, 2001; Shen et al., 2007; Li et al., 2009). The mutual reinforcement principle has been exploited to iteratively extract key phrases and sentences from a document (Zha, 2002; Wan et al, 2007a). Wan et al. (2007b) propose the CollabSum algorithm to use additional knowledge in a cluster of documents to improve single document summarization in the cluster. In recent years, graph-based ranking methods have been investigated for document summarization, such as TextRank (Mihalcea and Tarau, 2004; Mihalcea and Tarau, 2005) and LexPageRank (ErKan and Radev, 2004). Similar to PageRank (Page et al., 1998), these methods first build a graph based on the similarity relationships between the sentences in a document and then the saliency of a sentence is determined by making use of the global information on the graph recursively. The basic idea underlying the graph-based ranking algorithm is that of “voting” or “recommendation” between sentences. Similar methods have been used for generic multi-document summarization. A typical method is the centroid-based method (Radev et al., 2004). For each sentence, the method computes a score based on each single featu"
C10-1128,D09-1044,0,0.0170726,"sed on each single feature (e.g. cluster centroids, position and TFIDF) and then linearly combines all the scores into an overall sentence score. Topic signature is used as a novel feature for selecting important content in NeATS (Lin and Hovy, 2002). Various sentence features have been combined by using machine learning techniques (Wong et al., 2008). A popular way for removing redundancy between summary sentences is the MMR algorithm (Carbonell and Goldstein, 1998). Themes (or topics, clusters) in documents have been discovered and used for sentence selection (Harabagiu and Lacatusu, 2005). Hachey (2009) investigates the effect of various source document representations on the accuracy of the sentence extraction phase of a multi-document summarization task. Graph-based methods have also been used to rank sentences in a document set. The methods first construct a graph to reflect sentence relationships at different granularities, and then compute sentence scores based on graph-based learning algorithms. For example, Wan (2008) proposes to use only cross-document relationships for graph building and sentence ranking. Cluster-level information has been incorporated in the graph model to better e"
C10-1128,W97-0704,0,0.0229132,"onference on Computational Linguistics (Coling 2010), pages 1137–1145, Beijing, August 2010 2 Related Work Document summarization methods can be either extraction-based or abstraction-based. In this section, we focus on extraction-based methods. Extraction-based methods for singledocument summarization usually assign a saliency score to each sentence in a document and then rank and select the sentences. The score is usually computed based on a combination of statistical and linguistic features, such as term frequency, sentence position, cue words and stigma words (Luhn, 1969; Edmundson, 1969; Hovy and Lin, 1997). Machine learning techniques have also been used for sentence extraction (Kupiec et al., 1995; Conroy and O’Leary, 2001; Shen et al., 2007; Li et al., 2009). The mutual reinforcement principle has been exploited to iteratively extract key phrases and sentences from a document (Zha, 2002; Wan et al, 2007a). Wan et al. (2007b) propose the CollabSum algorithm to use additional knowledge in a cluster of documents to improve single document summarization in the cluster. In recent years, graph-based ranking methods have been investigated for document summarization, such as TextRank (Mihalcea and Ta"
C10-1128,C08-1062,0,0.12637,"ased learning algorithms. For example, Wan (2008) proposes to use only cross-document relationships for graph building and sentence ranking. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). For topic-focused multi-document summarization, many methods are extensions of generic summarization methods by incorporating the information of the given topic or query into generic summarizers. In recent years, a few novel methods have been proposed for topic-focused summarization (Daumé and Marcu, 2006; Wan et al., 2007c; Nastase 2008; Li et al., 2008; Schilder and Kondadadi, 2008; Wei et al., 2008). The above previous graph-based summarization methods aim to address either singledocument summarization or multi-document summarization, and the two summarization tasks have not yet been addressed in a unified graphbased framework. 3 3.1 The Unified proach Summarization ApOverview Given a document set, in which the whole document set and each single document in the set are required to be summarized, we use local saliency to indicate the importance of a sentence in a particular document, and use global saliency to indicate the importance of a s"
C10-1128,N03-1020,0,0.017697,"near system, W is normalized by columns. If all the elements of a column are zero, we replace the elements with 1/(2n), where 2n equals to the element number of the column. We then multiply W by a decay factor θ (0<θ<1) to scale down each element in W, but remain the meaning of W. Here, θ is empirically set to 0.6 1 . Finally, Equation (21) is rewrittenr as rfollows: ( I − θ ⋅ W )r = p (22) 1 mathematics and the details of the method is omitted here. DUC 2001 Tasks 1, 2 309 30 TREC-9 100 words DUC 2002 Tasks 1, 2 567 59 TREC-9 100 words Table 1. Summary of datasets We used the ROUGE toolkit2 (Lin and Hovy, 2003) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation. It measured summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary. The ROUGE toolkit reported separate recalloriented scores for 1, 2, 3 and 4-gram, and also for longest common subsequence cooccurrences. We showed three of the ROUGE metrics in the experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigram-based), and ROUGE-W (based on weighted longest common subsequence, weight=1.2). In order to trunc"
C10-1128,I05-2004,0,0.0699352,"earning techniques have also been used for sentence extraction (Kupiec et al., 1995; Conroy and O’Leary, 2001; Shen et al., 2007; Li et al., 2009). The mutual reinforcement principle has been exploited to iteratively extract key phrases and sentences from a document (Zha, 2002; Wan et al, 2007a). Wan et al. (2007b) propose the CollabSum algorithm to use additional knowledge in a cluster of documents to improve single document summarization in the cluster. In recent years, graph-based ranking methods have been investigated for document summarization, such as TextRank (Mihalcea and Tarau, 2004; Mihalcea and Tarau, 2005) and LexPageRank (ErKan and Radev, 2004). Similar to PageRank (Page et al., 1998), these methods first build a graph based on the similarity relationships between the sentences in a document and then the saliency of a sentence is determined by making use of the global information on the graph recursively. The basic idea underlying the graph-based ranking algorithm is that of “voting” or “recommendation” between sentences. Similar methods have been used for generic multi-document summarization. A typical method is the centroid-based method (Radev et al., 2004). For each sentence, the method com"
C10-1128,D08-1080,0,0.0241762,"sed on graph-based learning algorithms. For example, Wan (2008) proposes to use only cross-document relationships for graph building and sentence ranking. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). For topic-focused multi-document summarization, many methods are extensions of generic summarization methods by incorporating the information of the given topic or query into generic summarizers. In recent years, a few novel methods have been proposed for topic-focused summarization (Daumé and Marcu, 2006; Wan et al., 2007c; Nastase 2008; Li et al., 2008; Schilder and Kondadadi, 2008; Wei et al., 2008). The above previous graph-based summarization methods aim to address either singledocument summarization or multi-document summarization, and the two summarization tasks have not yet been addressed in a unified graphbased framework. 3 3.1 The Unified proach Summarization ApOverview Given a document set, in which the whole document set and each single document in the set are required to be summarized, we use local saliency to indicate the importance of a sentence in a particular document, and use global saliency to indicate the"
C10-1128,P08-2052,0,0.0688356,"orithms. For example, Wan (2008) proposes to use only cross-document relationships for graph building and sentence ranking. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). For topic-focused multi-document summarization, many methods are extensions of generic summarization methods by incorporating the information of the given topic or query into generic summarizers. In recent years, a few novel methods have been proposed for topic-focused summarization (Daumé and Marcu, 2006; Wan et al., 2007c; Nastase 2008; Li et al., 2008; Schilder and Kondadadi, 2008; Wei et al., 2008). The above previous graph-based summarization methods aim to address either singledocument summarization or multi-document summarization, and the two summarization tasks have not yet been addressed in a unified graphbased framework. 3 3.1 The Unified proach Summarization ApOverview Given a document set, in which the whole document set and each single document in the set are required to be summarized, we use local saliency to indicate the importance of a sentence in a particular document, and use global saliency to indicate the importance of a sentence in the whole document"
C10-1128,P07-1070,1,0.664727,"ly assign a saliency score to each sentence in a document and then rank and select the sentences. The score is usually computed based on a combination of statistical and linguistic features, such as term frequency, sentence position, cue words and stigma words (Luhn, 1969; Edmundson, 1969; Hovy and Lin, 1997). Machine learning techniques have also been used for sentence extraction (Kupiec et al., 1995; Conroy and O’Leary, 2001; Shen et al., 2007; Li et al., 2009). The mutual reinforcement principle has been exploited to iteratively extract key phrases and sentences from a document (Zha, 2002; Wan et al, 2007a). Wan et al. (2007b) propose the CollabSum algorithm to use additional knowledge in a cluster of documents to improve single document summarization in the cluster. In recent years, graph-based ranking methods have been investigated for document summarization, such as TextRank (Mihalcea and Tarau, 2004; Mihalcea and Tarau, 2005) and LexPageRank (ErKan and Radev, 2004). Similar to PageRank (Page et al., 1998), these methods first build a graph based on the similarity relationships between the sentences in a document and then the saliency of a sentence is determined by making use of the global"
C10-1128,C08-1124,0,0.0141789,"nking algorithm is that of “voting” or “recommendation” between sentences. Similar methods have been used for generic multi-document summarization. A typical method is the centroid-based method (Radev et al., 2004). For each sentence, the method computes a score based on each single feature (e.g. cluster centroids, position and TFIDF) and then linearly combines all the scores into an overall sentence score. Topic signature is used as a novel feature for selecting important content in NeATS (Lin and Hovy, 2002). Various sentence features have been combined by using machine learning techniques (Wong et al., 2008). A popular way for removing redundancy between summary sentences is the MMR algorithm (Carbonell and Goldstein, 1998). Themes (or topics, clusters) in documents have been discovered and used for sentence selection (Harabagiu and Lacatusu, 2005). Hachey (2009) investigates the effect of various source document representations on the accuracy of the sentence extraction phase of a multi-document summarization task. Graph-based methods have also been used to rank sentences in a document set. The methods first construct a graph to reflect sentence relationships at different granularities, and then"
C10-1128,W04-3252,0,\N,Missing
C10-1128,P02-1058,0,\N,Missing
C10-1128,P08-1000,0,\N,Missing
C10-2090,N09-1055,0,0.0618976,"eviews and news articles, the * methods cannot perform well on news comments. Actually, target extraction in news comments significantly differs from that in product reviews and news articles in the following ways. 1) Products usually have a set of definite attributes (e.g. size) and related opinion words (e.g. large), and thus researchers can use a small fixed set of keywords to recognize frequent feature words (Zhuang et al., 2006), or leverage the associated rules between feature words and opinion words to improve the performance (Hu and Liu, 2004; Su et al., 2008; Jin and Ho, 2009; Du and Tan, 2009). But news comments are more complicated. There are much more potential opinion targets in news comments. In other words, the candidate targets are in a much more open domain. On the other hand, the opinion targets in news comments are not strongly associated with the opinion words. We cannot judge a target by a special opinion word as easily as in product reviews. 2) The opinionated sentences in news articles mostly contain opinion operators (e.g. believe, realize), which can be used to find the positions of opinion expressions. However, news comments have already been considered to be declar"
C10-2090,J95-2003,0,0.140338,"Missing"
C10-2090,W06-0301,0,0.038123,"on Centering Theory. In Section 5 we test the results and give a discussion on the errors. Finally Section 6 draws a conclusion. 2 Related Work The early research of opinion mining only focused on the sentiment classification (Turney et al., 2002; Pang et al., 2002). However, for many applications only judging the sentiment orientation is not sufficient (eg. Hu and Liu, 2004). Fine-grained opinion analysis has attracted more and more attention these years. It mainly includes these types: opinion holder extraction 783 (Kim and Hovy, 2005; Choi et al., 2005), opinion target extraction (Kim and Hovy, 2006; Ruppenhofer et al., 2008), and the identification of opinion proposals (Bethard et al., 2004) and some special opinion expressions (Bloom et al., 2007). Also, there are some other related tasks, such as detecting users’ needs and wants (Kanayama and Nasukawa, 2008). However, these general systems are different from ours because they do not have or use any contextual information, and implicit opinion targets are not recognized and handled there. A more special domain of feature extraction is product and movie reviews. Hu and Liu (2004) design a system to mine product features and generate opi"
C10-2090,W02-1011,0,0.0121893,"Missing"
C10-2090,H05-1043,0,0.862256,"it opinion targets are not recognized and handled there. A more special domain of feature extraction is product and movie reviews. Hu and Liu (2004) design a system to mine product features and generate opinion summaries of customer reviews. Frequent features are extracted by a statistical approach, and infrequent features are generated by the associated opinion words. The product features are limited in amount and they are strongly associated with specific opinion words, so researchers can use a fixed set of keywords or templates to extract frequent features (Zhuang et al., 2006; Popescu and Etzioni, 2005) or try various methods to augment the database of product features and improve the extraction accuracy by using the relations between attributes and opinions (Ghani et al., 2006; Su et al., 2008; Jin and Ho, 2009; Du and Tan, 2009). However, in news comments, the opinion targets are not strongly associated with specific opinion words and these techniques cannot be used. There are also some works focusing on the target extraction in news articles, such as NTCIR7-MOAT (Seki et al., 2008). Different from the news comments, there are opinion indicators in the subjective sentences. However, in our"
C10-2090,W03-1014,0,0.0890437,"Missing"
C10-2090,ruppenhofer-etal-2008-finding,0,0.0601808,"g Theory. In Section 5 we test the results and give a discussion on the errors. Finally Section 6 draws a conclusion. 2 Related Work The early research of opinion mining only focused on the sentiment classification (Turney et al., 2002; Pang et al., 2002). However, for many applications only judging the sentiment orientation is not sufficient (eg. Hu and Liu, 2004). Fine-grained opinion analysis has attracted more and more attention these years. It mainly includes these types: opinion holder extraction 783 (Kim and Hovy, 2005; Choi et al., 2005), opinion target extraction (Kim and Hovy, 2006; Ruppenhofer et al., 2008), and the identification of opinion proposals (Bethard et al., 2004) and some special opinion expressions (Bloom et al., 2007). Also, there are some other related tasks, such as detecting users’ needs and wants (Kanayama and Nasukawa, 2008). However, these general systems are different from ours because they do not have or use any contextual information, and implicit opinion targets are not recognized and handled there. A more special domain of feature extraction is product and movie reviews. Hu and Liu (2004) design a system to mine product features and generate opinion summaries of customer"
C10-2090,P02-1053,0,0.00271811,"Missing"
C10-2090,H05-2017,0,\N,Missing
C10-2090,H05-1045,0,\N,Missing
C12-2126,C08-1062,0,0.220105,"The task of update summarization was piloted in DUC2007, and it has been the fundamental task through TAC2008~TAC2011. The “update” characteristic makes the task more challenging than traditional document summarization tasks. Till now, most existing update summarization methods are adaptations of multi-document summarization methods by considering the redundancy information between the earlier and later document sets (Boudin et al. 2008; Fisher and Roark 2008; Nastase et al. 2008 ). In addition, several new methods have been proposed for addressing this task (Du et al. 2010; Wang and Li 2010; Li et al. 2008), and graph-based coranking is a typical one, where the sentences in the two document sets are ranked simultaneously by considering the sentence relationships across the document sets. Based on the co-ranking framework, Li et al. (2008) propose a graph-based sentence ranking algorithm named PNR2 for update summarization, and it models both the positive and negative mutual reinforcement between sentences in the ranking process. In addition, Wan et al. (2011) apply the co-ranking algorithm for multilingual news summarization. In this study, we propose a new co-ranking method, which is inspired b"
C16-2060,W04-3247,0,0.117587,"Missing"
C16-2060,W09-1802,0,0.42538,"Missing"
C16-2060,N03-1020,0,0.308641,"Missing"
C16-2060,N10-1134,0,0.076206,"Missing"
C16-2060,P16-4013,0,0.206472,"Missing"
C16-2060,radev-etal-2004-mead,0,0.0807698,"Missing"
C16-2060,W04-3252,0,\N,Missing
C18-1089,H05-1042,0,0.115524,"rating natural language descriptions for this subset. There is also an alternative decomposition in Reiter and Dale (1997), where they break the problem down to three modules: content selection, micro planning, and surface realization. There is also a corresponding work (Mellish et al., 2006) that challenges this decomposition. In early stages, surface realization is often realized using templates (van Deemter et al., 2005) or statistically learned models with hand-crafted features (Belz, 2008; Konstas and Lapata, 2012). Machine learning approaches have also been applied to content selection. Barzilay and Lapata (2005) models it as a classification problem, whereas Liang et al. (2009) uses a generative semi-Markov model. With the rise of neural networks, some recent work has focused on generating text from data using neural networks, for example, generating weather forecasts as well as game descriptions (Mei et al., 2016), generating short biographies from Wikipedia Tables (Lebret et al., 2016; Hachey et al., 2017; Sha et al., 2017; Liu et al., 2017), and generating market comments from stock prices (Murakami et al., 2017). With these models achieving excellent results on traditional evaluation metrics such"
C18-1089,P00-1037,0,0.755241,"Missing"
C18-1089,P15-1061,0,0.296156,"om the template. In contrast, the traditional copy network only applies attention mechanism to the input records. temp vcrecord , vc and the input vector hp are then combined to produce a score for a i-th input record ri : T Score(ri ) = [vcrecord ; vctemp ; hp ] · W1 · r˜i (3) where W1 is a parameter matrix in R3D×D . Position-aware Attention In the task of relationship classification, which involves determining the relationship of a pair of entities, it has been observed that the relative distances from other words in the sentence to the entities could be informative (Zeng et al., 2014; dos Santos et al., 2015). Inspired by Zhang et al. (2017), we employ a modified position-aware attention mechanism to the template text. When replacing some placeholder using the copy network, the distances between every word in the generated text and the placeholder are incorporated into the calculation of the template-text-based context temp temp temp vector vc . Compared to simply using Atten(hp , {ht }) to calculate vc , this method allows the network to pay attention to words according to their relative locations to the placeholder. Formally, let’s assume {y10 , y20 , ..., yn0 } is the generated template sentenc"
C18-1089,P16-1154,0,0.345995,"017) employ a “tag-then-replace” method, where they train their models to generate special tags which will be replaced with true entities or numeric values using handwritten rules. However, these approaches may fail under more complicated situations. For example, in the ROTOW IRE dataset, there are over 40 types of records and the entity names that appear in the entity portion of records change from game to game, making it hard, if not impossible, to devise hand-written rules for the “tag-then-replace” method. 1045 Copy Mechanism Copy mechanism in encoder-decoder models (Vinyals et al., 2015; Gu et al., 2016; G¨ulc¸ehre et al., 2016; Yang et al., 2017) provides a way to directly copy words from the input. At each time step of decoding, an attention-like function is used to produce an unnormalized score distribution ∗ . The decoder also produces an unnormalized distribution P ∗ over the inputs, denoted as Pcopy word over the words in the vocabulary. These two unnormalized score distributions are then merged into a normalized distribution Pjoint . ThePloss function is defined as the negative log likelihood function of vanilla encoderdecoder models: L = t − log Pjoint (wt∗ |w1:t−1 , S), where wt∗ is"
C18-1089,P16-1014,0,0.117664,"Missing"
C18-1089,E17-1060,0,0.0926037,"ter et al., 2005) or statistically learned models with hand-crafted features (Belz, 2008; Konstas and Lapata, 2012). Machine learning approaches have also been applied to content selection. Barzilay and Lapata (2005) models it as a classification problem, whereas Liang et al. (2009) uses a generative semi-Markov model. With the rise of neural networks, some recent work has focused on generating text from data using neural networks, for example, generating weather forecasts as well as game descriptions (Mei et al., 2016), generating short biographies from Wikipedia Tables (Lebret et al., 2016; Hachey et al., 2017; Sha et al., 2017; Liu et al., 2017), and generating market comments from stock prices (Murakami et al., 2017). With these models achieving excellent results on traditional evaluation metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004), Wiseman et al. (2017) proposed to evaluate data-to-text systems from three aspects using relationship classification systems: data fidelity, content selection and content ordering. While there has been some work (Liu et al., 2017; Sha et al., 2017) related to the last two aspects, we find little work explicitly tackling the data fidelity problem."
C18-1089,N12-1093,0,0.284174,"a records more precisely. 1 Introduction One important task in the natural language generation (NLG) area is to generate a natural language description for some structured data (i.e., a number of database records), a.k.a., the data-to-text generation task. Applicable to wide areas such as weather forecasting, financial reporting, game broadcasting, datato-text generation systems have been studied for decades (Gatt and Krahmer, 2018). Traditionally this task is addressed by using templates or statistically learned shallow models with hand-crafted features (van Deemter et al., 2005; Belz, 2008; Konstas and Lapata, 2012). Recently, neural generation systems have shown significant progress on this task. The attention based encoder-decoder models with the copy mechanism have achieved state-of-the-art results on a few data-to-text datasets (Wiseman et al., 2017; Sha et al., 2017). In this paper, we try to address the problem of faithfully describing the data in the generated texts, i.e., how to generate a descriptive text that contains correct entities and numeric values. Intuitively speaking, when a human writer writes weather forecasts or game summaries, he may not consider the exact entities or numbers right"
C18-1089,P83-1022,0,0.817099,"ion mechanism in our delayed copy network works somehow differently from the traditional attention mechanism. In machine translation (Bahdanau et al., 2014; Luong et al., 2015), the attention mechanism is applied between a language-model based decoder and an encoder. However, in our model, the template-text-based attention mechanism is applied within a bi-directional text encoder. So the bi-directional encoder in our model could have learned to condense the information into the hidden state of a single word. 5 Related Work Traditionally, data-to-text tasks are decomposed into two subproblems (Kukich, 1983; Goldberg et al., 1994): content selection, which involves choosing a subset of relevant records to talk about, and surface realization, which is concerned with generating natural language descriptions for this subset. There is also an alternative decomposition in Reiter and Dale (1997), where they break the problem down to three modules: content selection, micro planning, and surface realization. There is also a corresponding work (Mellish et al., 2006) that challenges this decomposition. In early stages, surface realization is often realized using templates (van Deemter et al., 2005) or sta"
C18-1089,D16-1128,0,0.243692,"ong all invalid input records by minimizing a pairwise ranking loss function, defined as: L = log(1 + exp(γ(m+ − X Score(ri ))) + log(1 + exp(γ(m− + ri ∈C max ri ∈S ∗ ;ri ∈C / Score(ri ))) (6) where m+ and m− are margins and γ is a scaling factor that magnifies the difference between the score and the margin, and helps to penalize more on the prediction errors. 4 4.1 Experiments Setup Dataset We experiment our method on the ROTOW IRE dataset (Wiseman et al., 2017). The average text length is 337.1, while the average text lengths of previous datasets (Chen and Mooney, 2008; Liang et al., 2009; Lebret et al., 2016) do not exceed 30. The average number of input records is 628, while those of other datasets do not exceed 200. The text in this dataset also contains a decent percentage of entities and numeric values. This is indeed a challenging enough dataset to test our model on. Implementation We perform an additional step in preprocessing called “relexicalization”4 . We used PyTorch (Paszke et al., 2017) for implementation. For encoders and decoders, we use two layers of LSTM. The hidden states and the embedding vectors are all in R600 . General-style attention and inputfeeding (Luong et al., 2015) are"
C18-1089,P15-1107,0,0.167594,"number&gt; - <number&gt; ) for a <number&gt; - <number&gt; win. <entity&gt; ’s three - point shooting was the key , as they went <number&gt; - for - <number&gt; from long distance in the victory . Table 1: An example of the original text and the corresponding target template. 3.1 Template Generation with Encoder-Decoder Model Overview The template generator generates the major part of the text while leaving the task of actually copying to the delayed copy network. It is essentially an encoder-decoder model and any well-performing model would suffice. In this work, we use a hierarchical model similar to the one in Li et al. (2015). While the template generator does not decide which input record to copy from, it is responsible for deciding whether to copy a token from the input records. At each time step t, if the template generator finds it necessary to copy a token, it outputs a special placeholder (a data slot), which will be replaced with actual entities and numeric values by the copy network. The special placeholder could be “<data&gt;”. However in this specific task, we make a distinction between entities and numeric values, and define two placeholders, “<entity&gt;” and “<number&gt;”. We denote the templates as y 0 . Now"
C18-1089,P09-1011,0,0.594768,"opy mechanism point more precisely. We evaluate our approach on the public ROTOW IRE dataset and evaluation results verify the efficacy of our approach, which makes a boost on the data precision of generated texts and brings a noticeable increase on BLEU score. We organize the paper as follows. We introduce some background knowledge in Section 2 and describe our approach in Section 3. In Section 4 we present the experiments and have discussion. In Section 5 we note some additional related works. In Section 6 we conclude this paper. 2 Background The Data-to-text Task Following the notations in Liang et al. (2009) and Wiseman et al. (2017), let S = {rj } be a set of records, where for each r ∈ S, r.type, r.entity and r.value are the record’s type, entity and value, respectively. For example, there could be a record r in a basketball dataset such that r.type = P OINTS F IRST Q UARTER, r.entity = L E B RON JAMES, and r.value = 10, which means LeBron James scored 10 points in the first quarter. From these records, we are interested in generating an descriptive summary yˆ = yˆ1 , . . . , yˆT of T words. A data-to-text dataset consists of pairs like (S, y), where y is a gold (human generated) summary for th"
C18-1089,D15-1166,0,0.697957,"additional records r∗ mentioned in Section 2. When we fill in the <number&gt; placeholders, we only consider the records whose entity portions have been copied into the sentence. In short, when we replace a certain placeholder, only a subset of S, which we denote as S ∗ , are considered. This small S ∗ trick ensures the copy network does not copy numeric values whose entity portion is not present in the same sentence. This trick is not applicable in traditional copy networks where the order in which we copy tokens can not be adjusted. Double Attention Attention mechanism (Bahdanau et al., 2014; Luong et al., 2015) can be described as a function mapping a query vector vq and a series of source vectors {vs } to a context vector vc , i.e. vc = Atten(vq , {vs }). More concretely, X vc = as vs (1) where exp(us ) as = P , 0 s0 exp(us ) us = vqT · W · vs (2) and W is a parameter matrix in RD×D . To point more precisely, attention mechanism is employed to the input records as well as to the generated template. Given the input vector hp , the input-record-based vectors {hrecord } and the templatej temp record text-based vectors {ht }, an input-record-based context vector vc = Atten(hp , {hrecord }) is comj pute"
C18-1089,N16-1086,0,0.378452,"onsidered unfaithful to the record. Instead, a sentence correctly reflecting the record should be “LeBron James scored 10 points in the first quarter.” Generating texts conditioned on the records is a non-trivial problem because a sentence could likely contain several records. However, there have been few works explicitly addressing this issue due to the lack of challenging datasets. Previous datasets (Liang et al., 2009; Chen and Mooney, 2008; Murakami et al., 2017) generally use relatively simple language and record structure, where some simple approaches suffice. For instance, some models (Mei et al., 2016) generate entities and numeric values as other normal words while some works (van Deemter et al., 2005; Liang et al., 2009; Murakami et al., 2017) employ a “tag-then-replace” method, where they train their models to generate special tags which will be replaced with true entities or numeric values using handwritten rules. However, these approaches may fail under more complicated situations. For example, in the ROTOW IRE dataset, there are over 40 types of records and the entity names that appear in the entity portion of records change from game to game, making it hard, if not impossible, to dev"
C18-1089,P02-1040,0,0.100925,"as a classification problem, whereas Liang et al. (2009) uses a generative semi-Markov model. With the rise of neural networks, some recent work has focused on generating text from data using neural networks, for example, generating weather forecasts as well as game descriptions (Mei et al., 2016), generating short biographies from Wikipedia Tables (Lebret et al., 2016; Hachey et al., 2017; Sha et al., 2017; Liu et al., 2017), and generating market comments from stock prices (Murakami et al., 2017). With these models achieving excellent results on traditional evaluation metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004), Wiseman et al. (2017) proposed to evaluate data-to-text systems from three aspects using relationship classification systems: data fidelity, content selection and content ordering. While there has been some work (Liu et al., 2017; Sha et al., 2017) related to the last two aspects, we find little work explicitly tackling the data fidelity problem. Here we point out a parallel and independent work (Lu et al., 2018) on image caption that also employs a similar framework. They also generate slotted templates first and then fill in those slots with a separate model, though th"
C18-1089,J05-1002,0,0.212481,"Missing"
C18-1089,D17-1197,0,0.0613951,"here they train their models to generate special tags which will be replaced with true entities or numeric values using handwritten rules. However, these approaches may fail under more complicated situations. For example, in the ROTOW IRE dataset, there are over 40 types of records and the entity names that appear in the entity portion of records change from game to game, making it hard, if not impossible, to devise hand-written rules for the “tag-then-replace” method. 1045 Copy Mechanism Copy mechanism in encoder-decoder models (Vinyals et al., 2015; Gu et al., 2016; G¨ulc¸ehre et al., 2016; Yang et al., 2017) provides a way to directly copy words from the input. At each time step of decoding, an attention-like function is used to produce an unnormalized score distribution ∗ . The decoder also produces an unnormalized distribution P ∗ over the inputs, denoted as Pcopy word over the words in the vocabulary. These two unnormalized score distributions are then merged into a normalized distribution Pjoint . ThePloss function is defined as the negative log likelihood function of vanilla encoderdecoder models: L = t − log Pjoint (wt∗ |w1:t−1 , S), where wt∗ is the target word at time step t, w1:t−1 are p"
C18-1089,C14-1220,0,0.0381283,"utilize information from the template. In contrast, the traditional copy network only applies attention mechanism to the input records. temp vcrecord , vc and the input vector hp are then combined to produce a score for a i-th input record ri : T Score(ri ) = [vcrecord ; vctemp ; hp ] · W1 · r˜i (3) where W1 is a parameter matrix in R3D×D . Position-aware Attention In the task of relationship classification, which involves determining the relationship of a pair of entities, it has been observed that the relative distances from other words in the sentence to the entities could be informative (Zeng et al., 2014; dos Santos et al., 2015). Inspired by Zhang et al. (2017), we employ a modified position-aware attention mechanism to the template text. When replacing some placeholder using the copy network, the distances between every word in the generated text and the placeholder are incorporated into the calculation of the template-text-based context temp temp temp vector vc . Compared to simply using Atten(hp , {ht }) to calculate vc , this method allows the network to pay attention to words according to their relative locations to the placeholder. Formally, let’s assume {y10 , y20 , ..., yn0 } is the"
C18-1089,D17-1004,0,0.0332246,"traditional copy network only applies attention mechanism to the input records. temp vcrecord , vc and the input vector hp are then combined to produce a score for a i-th input record ri : T Score(ri ) = [vcrecord ; vctemp ; hp ] · W1 · r˜i (3) where W1 is a parameter matrix in R3D×D . Position-aware Attention In the task of relationship classification, which involves determining the relationship of a pair of entities, it has been observed that the relative distances from other words in the sentence to the entities could be informative (Zeng et al., 2014; dos Santos et al., 2015). Inspired by Zhang et al. (2017), we employ a modified position-aware attention mechanism to the template text. When replacing some placeholder using the copy network, the distances between every word in the generated text and the placeholder are incorporated into the calculation of the template-text-based context temp temp temp vector vc . Compared to simply using Atten(hp , {ht }) to calculate vc , this method allows the network to pay attention to words according to their relative locations to the placeholder. Formally, let’s assume {y10 , y20 , ..., yn0 } is the generated template sentence and the placeholder we are repl"
D08-1058,P07-1056,0,0.033072,"plore a 554 computable metric of positive or negative polarity in financial news text. Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting. Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007; Wang et al., 2007). Such work represents heuristic extensions of the unsupervised or supervised methods for English sentiment analysis. To date, the most closely related work is Mihalcea et"
D08-1058,W06-1651,0,0.00719219,". They have investigated two approaches: a lexicon-based approach based on Romanian subjectivity lexicon translated from English lexicon, and a corpus-based approach based on Romanian subjectivity-annotated corpora obtained via cross-lingual projections. In this study, we focus on unsupervised sentiment polarity identification and we only investigate the lexicon-based approach in the experiments. Other related work includes subjective/objective analysis (Hatzivassiloglon and Wiebe, 2000; Riloff and Wiebe, 2003) and opinion mining and summarization (Liu et al., 2005; Popescu and Etzioni. 2005; Choi et al., 2006; Ku et al., 2006; Titov and McDonald, 2008). 3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of bilingual knowledge to improve sentiment analysis in a target language, where the resources for sentiment analysis are limited or unreliable. This study focuses on unsupervised polarity identification of Chinese product reviews by using both the rich English knowledge and the limited Chinese knowledge. The framework of our approach is illustrated in Figure 1. A Chinese review is translated into the corresponding English review using machine translation servic"
D08-1058,P07-1124,0,0.020862,"hrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentiment-bearing words. Hiroshi et al. (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a 554 computable metric of positive or negative polarity in financial news text. Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularit"
D08-1058,P97-1023,0,0.0606295,"by combining the individual analysis results in different languages. The results also demonstrate that our proposed approach is more effective than the approach that leverages generated Chinese resources. The rest of this paper is organized as follows: Section 2 introduces related work. The proposed approach is described in detail in Section 3. Section 4 shows the experimental results. Lastly we conclude this paper in Section 5. 2 Related Work Polarity identification can be performed on word level, sentence level or document level. Related work for word-level polarity identification includes (Hatzivassiloglou and McKeown, 1997; Kim and Hovy. 2004; Takamura et al., 2005; Yao et al. 2006; Kaji and Kitsuregawa, 2007), and related work for sentence-level polarity identification includes (Yu and Hatzivassiloglou, 2003; Kim and Hovy. 2004) Word-level or sentence-level sentiment analysis is not the focus of this paper. Generally speaking, document-level polarity identification methods can be categorized into unsupervised and supervised. Unsupervised methods involve deriving a sentiment metric for text without training corpus. Turney (2002) predicates the sentiment orientation of a review by the average semantic orientatio"
D08-1058,C00-1044,0,0.0141283,"oss-lingual projections to generate subjectivity analysis resources in Romanian by leveraging on the tools and resources available in English. They have investigated two approaches: a lexicon-based approach based on Romanian subjectivity lexicon translated from English lexicon, and a corpus-based approach based on Romanian subjectivity-annotated corpora obtained via cross-lingual projections. In this study, we focus on unsupervised sentiment polarity identification and we only investigate the lexicon-based approach in the experiments. Other related work includes subjective/objective analysis (Hatzivassiloglon and Wiebe, 2000; Riloff and Wiebe, 2003) and opinion mining and summarization (Liu et al., 2005; Popescu and Etzioni. 2005; Choi et al., 2006; Ku et al., 2006; Titov and McDonald, 2008). 3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of bilingual knowledge to improve sentiment analysis in a target language, where the resources for sentiment analysis are limited or unreliable. This study focuses on unsupervised polarity identification of Chinese product reviews by using both the rich English knowledge and the limited Chinese knowledge. The framework of our approach is"
D08-1058,C04-1071,0,0.107443,"us of this paper. Generally speaking, document-level polarity identification methods can be categorized into unsupervised and supervised. Unsupervised methods involve deriving a sentiment metric for text without training corpus. Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentiment-bearing words. Hiroshi et al. (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a 554 computable metric of positive or negative polarity in financial news text. Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier. Since the work of Pang et al. (2002), vario"
D08-1058,D07-1115,0,0.026668,"e that our proposed approach is more effective than the approach that leverages generated Chinese resources. The rest of this paper is organized as follows: Section 2 introduces related work. The proposed approach is described in detail in Section 3. Section 4 shows the experimental results. Lastly we conclude this paper in Section 5. 2 Related Work Polarity identification can be performed on word level, sentence level or document level. Related work for word-level polarity identification includes (Hatzivassiloglou and McKeown, 1997; Kim and Hovy. 2004; Takamura et al., 2005; Yao et al. 2006; Kaji and Kitsuregawa, 2007), and related work for sentence-level polarity identification includes (Yu and Hatzivassiloglou, 2003; Kim and Hovy. 2004) Word-level or sentence-level sentiment analysis is not the focus of this paper. Generally speaking, document-level polarity identification methods can be categorized into unsupervised and supervised. Unsupervised methods involve deriving a sentiment metric for text without training corpus. Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as th"
D08-1058,P08-1036,0,0.0244168,"hes: a lexicon-based approach based on Romanian subjectivity lexicon translated from English lexicon, and a corpus-based approach based on Romanian subjectivity-annotated corpora obtained via cross-lingual projections. In this study, we focus on unsupervised sentiment polarity identification and we only investigate the lexicon-based approach in the experiments. Other related work includes subjective/objective analysis (Hatzivassiloglon and Wiebe, 2000; Riloff and Wiebe, 2003) and opinion mining and summarization (Liu et al., 2005; Popescu and Etzioni. 2005; Choi et al., 2006; Ku et al., 2006; Titov and McDonald, 2008). 3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of bilingual knowledge to improve sentiment analysis in a target language, where the resources for sentiment analysis are limited or unreliable. This study focuses on unsupervised polarity identification of Chinese product reviews by using both the rich English knowledge and the limited Chinese knowledge. The framework of our approach is illustrated in Figure 1. A Chinese review is translated into the corresponding English review using machine translation services, and then the Chinese review and the Engl"
D08-1058,C04-1200,0,0.13054,"nce-level polarity identification includes (Yu and Hatzivassiloglou, 2003; Kim and Hovy. 2004) Word-level or sentence-level sentiment analysis is not the focus of this paper. Generally speaking, document-level polarity identification methods can be categorized into unsupervised and supervised. Unsupervised methods involve deriving a sentiment metric for text without training corpus. Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentiment-bearing words. Hiroshi et al. (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a 554 computable metric of positive or negative polarity in financial news text. Supervised me"
D08-1058,P02-1053,0,0.0297376,"Related work for word-level polarity identification includes (Hatzivassiloglou and McKeown, 1997; Kim and Hovy. 2004; Takamura et al., 2005; Yao et al. 2006; Kaji and Kitsuregawa, 2007), and related work for sentence-level polarity identification includes (Yu and Hatzivassiloglou, 2003; Kim and Hovy. 2004) Word-level or sentence-level sentiment analysis is not the focus of this paper. Generally speaking, document-level polarity identification methods can be categorized into unsupervised and supervised. Unsupervised methods involve deriving a sentiment metric for text without training corpus. Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentiment-bearing words. Hiroshi et al. (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative t"
D08-1058,H05-1044,0,0.333059,"ntiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a 554 computable metric of positive or negative polarity in financial news text. Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting. Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007; Wan"
D08-1058,P07-1055,0,0.0271843,"and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a 554 computable metric of positive or negative polarity in financial news text. Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting. Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007; Wang et al., 2007). Such work represents heuristic exten"
D08-1058,P07-1123,0,0.857223,"nly the limited Chinese knowledge, this study aims to improve Chinese sentiment analysis by making full use of bilingual knowledge in an unsupervised way, including both Chinese resources and English resources. Generally speaking, there are two unsupervised scenarios for “borrowing” English resources for sentiment analysis in other languages: one is to generate resources in a new language by leveraging on the resources available in English via cross-lingual projections, and then perform sentiment analysis in the English language based on the generated resources, which has been investigated by Mihalcea et al. (2007); the other is to translate the texts in a new language into English texts, and then perform sentiment analysis in the English language, which has not yet been investigated. In this study, we first translate Chinese reviews into English reviews by using machine translation services, and then identify the sentiment polarity of English reviews by directly leveraging English resources. Furthermore, ensemble methods are employed to combine the individual analysis results in each language (i.e. Chinese and English) in order to obtain improved results. Given machine translation services between the"
D08-1058,W04-3253,0,0.245199,"en (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a 554 computable metric of positive or negative polarity in financial news text. Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting. Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006;"
D08-1058,W02-1011,0,0.0175663,"ds. Hiroshi et al. (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a 554 computable metric of positive or negative polarity in financial news text. Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a"
D08-1058,P04-1035,0,0.00964945,"ts. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a 554 computable metric of positive or negative polarity in financial news text. Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting. Research work focusing on Chinese sentiment analysis includes (Tsou et al"
D08-1058,H05-1043,0,0.103558,"Missing"
D08-1058,P05-2008,0,0.0401539,"review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a 554 computable metric of positive or negative polarity in financial news text. Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting. Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007; Wang et al., 2007"
D08-1058,W03-1014,0,0.319112,"e subjectivity analysis resources in Romanian by leveraging on the tools and resources available in English. They have investigated two approaches: a lexicon-based approach based on Romanian subjectivity lexicon translated from English lexicon, and a corpus-based approach based on Romanian subjectivity-annotated corpora obtained via cross-lingual projections. In this study, we focus on unsupervised sentiment polarity identification and we only investigate the lexicon-based approach in the experiments. Other related work includes subjective/objective analysis (Hatzivassiloglon and Wiebe, 2000; Riloff and Wiebe, 2003) and opinion mining and summarization (Liu et al., 2005; Popescu and Etzioni. 2005; Choi et al., 2006; Ku et al., 2006; Titov and McDonald, 2008). 3 The Proposed Approach 3.1 Overview The motivation of our approach is to make full use of bilingual knowledge to improve sentiment analysis in a target language, where the resources for sentiment analysis are limited or unreliable. This study focuses on unsupervised polarity identification of Chinese product reviews by using both the rich English knowledge and the limited Chinese knowledge. The framework of our approach is illustrated in Figure 1."
D08-1058,P05-1017,0,0.0145154,"t languages. The results also demonstrate that our proposed approach is more effective than the approach that leverages generated Chinese resources. The rest of this paper is organized as follows: Section 2 introduces related work. The proposed approach is described in detail in Section 3. Section 4 shows the experimental results. Lastly we conclude this paper in Section 5. 2 Related Work Polarity identification can be performed on word level, sentence level or document level. Related work for word-level polarity identification includes (Hatzivassiloglou and McKeown, 1997; Kim and Hovy. 2004; Takamura et al., 2005; Yao et al. 2006; Kaji and Kitsuregawa, 2007), and related work for sentence-level polarity identification includes (Yu and Hatzivassiloglou, 2003; Kim and Hovy. 2004) Word-level or sentence-level sentiment analysis is not the focus of this paper. Generally speaking, document-level polarity identification methods can be categorized into unsupervised and supervised. Unsupervised methods involve deriving a sentiment metric for text without training corpus. Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain"
D08-1058,H05-2018,0,0.200816,"ntiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a 554 computable metric of positive or negative polarity in financial news text. Supervised methods consider the sentiment analysis task as a classification task and use labeled corpus to train the classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005a; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpusbased classifier and a lexicon-based classifier with precision-based vote weighting. Research work focusing on Chinese sentiment analysis includes (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007; Wan"
D08-1058,W03-1017,0,0.0213586,"esources. The rest of this paper is organized as follows: Section 2 introduces related work. The proposed approach is described in detail in Section 3. Section 4 shows the experimental results. Lastly we conclude this paper in Section 5. 2 Related Work Polarity identification can be performed on word level, sentence level or document level. Related work for word-level polarity identification includes (Hatzivassiloglou and McKeown, 1997; Kim and Hovy. 2004; Takamura et al., 2005; Yao et al. 2006; Kaji and Kitsuregawa, 2007), and related work for sentence-level polarity identification includes (Yu and Hatzivassiloglou, 2003; Kim and Hovy. 2004) Word-level or sentence-level sentiment analysis is not the focus of this paper. Generally speaking, document-level polarity identification methods can be categorized into unsupervised and supervised. Unsupervised methods involve deriving a sentiment metric for text without training corpus. Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. Kim and Hovy (2004) build three models to assign a sentiment category to"
D08-1058,H05-2017,0,\N,Missing
D08-1058,W06-1641,0,\N,Missing
D08-1058,P08-1034,0,\N,Missing
D08-1058,P08-1000,0,\N,Missing
D08-1079,P99-1071,0,0.012361,"proposed document-based graph model are described in detail in Sections 3 and 4, respectively. We show the experiments and results in Section 5 and finally we conclude this paper in Section 6. 2 Related Work Generally speaking, summarization methods can be abstractive summarization or extractive summarization. Extractive summarization is a simple but robust method for text summarization and it involves assigning saliency scores to some units (e.g. sentences, paragraphs) of the documents and extracting those with highest scores, while abstraction summarization usually needs information fusion (Barzilay et al., 1999), sentence compression (Knight and Marcu, 2002) and reformulation (McKeown et al., 1999). In this study, we focus on extractive summarization. The centroid-based method (Radev et al., 2004) is one of the most popular extractive summariza756 tion methods. MEAD2 is an implementation of the centroid-based method that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TFIDF, etc. NeATS (Lin and Hovy, 2002) is a project on multidocument summarization at ISI based on the single-document summarizer-SUMMARIST. Sentence position, term frequency,"
D08-1079,P06-1039,0,0.0480126,"Missing"
D08-1079,W04-3247,0,0.0512592,"nd hence we need effective summarization methods to analyze the information stored in different documents and extract the globally important information to reflect the main topic. In recent years, both unsupervised and supervised methods have been proposed to analyze the information contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features. Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the “voting” or “recommendations” between sentences in the documents (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006). The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences. The sentences with large rank scores are chosen into the summary. However, the model makes uniform use of the sentences in different documents, i.e. all the sentences are ranked without considering the document-level information and the sentence-todocument relationship. Actually, given a document set, different documents are not equally import"
D08-1079,P07-2049,0,0.011233,"Mihalcea and Tarau (2005) also propose a similar algorithm based on PageRank to compute sentence importance for document summarization. Wan and Yang (2006) improve the ranking algo2 http://www.summarization.com/mead/ rithm by differentiating intra-document links and inter-document links between sentences. All these methods make use of the relationships between sentences and select sentences according to the “votes” or “recommendations” from their neighboring sentences, which is similar to PageRank. Other related work includes topic-focused multidocument summarization (Daumé. and Marcu, 2006; Gupta et al., 2007; Wan et al., 2007), which aims to produce summary biased to a given topic or query. It is noteworthy that our proposed approach is inspired by (Liu and Ma, 2005), which proposes the Conditional Markov Random Walk Model based on two-layer web graph in the tasks of web page retrieval. 3 The Basic Graph-Based Model (GM) The basic graph-based model is essentially a way of deciding the importance of a vertex within a graph based on global information recursively drawn from the entire graph. The basic idea is that of “voting” or “recommendation” between the vertices. A link between two vertices is"
D08-1079,J97-1003,0,0.111714,"Missing"
D08-1079,P03-2021,0,0.0176288,"implementation of the centroid-based method that scores sentences based on sentence-level and inter-sentence features, including cluster centroids, position, TFIDF, etc. NeATS (Lin and Hovy, 2002) is a project on multidocument summarization at ISI based on the single-document summarizer-SUMMARIST. Sentence position, term frequency, topic signature and term clustering are used to select important content. MMR (Goldstein et al., 1999) is used to remove redundancy and stigma word filters and time stamps are used to improve cohesion and coherence. To further explore user interface issues, iNeATS (Leuski et al., 2003) is developed based on NeATS. XDoX (Hardy et al., 1998) is a cross document summarizer designed specifically to summarize large document sets. It identifies the most salient themes within the set by passage clustering and then composes an extraction summary, which reflects these main themes. Much other work also explores to find topic themes in the documents for summarization, e.g. Harabagiu and Lacatusu (2005) investigate five different topic representations and introduce a novel representation of topics based on topic themes. In addition, Marcu (2001) selects important sentences based on the"
D08-1079,N03-1020,0,0.0170946,"s (D088 is excluded from the original 60 document sets by NIST) and generic abstracts of each document set with lengths of approximately 100 words or less were required to be created. The documents were news articles collected from TREC-9. The sentences in each article have been separated and the sentence information has been stored into files. The summary of the two datasets are shown in Table 1. Task Number of documents Number of clusters Data source Summary length DUC 2001 Task 2 309 30 TREC-9 100 words DUC 2002 Task 2 567 59 TREC-9 100 words Table 1. Summary of datasets We used the ROUGE (Lin and Hovy, 2003) toolkit (i.e. ROUGEeval-1.4.2 in this study) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation. It measured summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary. ROUGE-N was an n-gram recall measure computed as follows: ROUGE − N = ∑ ∑ Count S∈{Ref Sum} n-gram∈S ∑ match (n − gram) ∑ Count(n − gram) (14) S ∈{Ref Sum} n-gram∈S 4 5 http://www-nlpir.nist.gov/projects/duc/guidelines/2001.html http://www-nlpir.nist.gov/projects/duc/guidelines/2002.html where n"
D08-1079,I05-2004,0,0.0859927,"ive summarization methods to analyze the information stored in different documents and extract the globally important information to reflect the main topic. In recent years, both unsupervised and supervised methods have been proposed to analyze the information contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features. Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the “voting” or “recommendations” between sentences in the documents (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006). The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences. The sentences with large rank scores are chosen into the summary. However, the model makes uniform use of the sentences in different documents, i.e. all the sentences are ranked without considering the document-level information and the sentence-todocument relationship. Actually, given a document set, different documents are not equally important. For example, the docu"
D08-1079,P08-1094,0,0.0100447,"the set by passage clustering and then composes an extraction summary, which reflects these main themes. Much other work also explores to find topic themes in the documents for summarization, e.g. Harabagiu and Lacatusu (2005) investigate five different topic representations and introduce a novel representation of topics based on topic themes. In addition, Marcu (2001) selects important sentences based on the discourse structure of the text. TNO’s system (Kraaij et al., 2001) scores sentences by combining a unigram language model approach with a Bayesian classifier based on surface features. Nenkova and Louis (2008) investigate how summary length and the characteristics of the input influence the summary quality in multi-document summarization. Graph-based models have been proposed to rank sentences or passages based on the PageRank algorithm (Page et al., 1998) or its variants. Websumm (Mani and Bloedorn, 2000) uses a graphconnectivity model and operates under the assumption that nodes which are connected to many other nodes are likely to carry salient information. LexPageRank (Erkan and Radev, 2004) is an approach for computing sentence importance based on the concept of eigenvector centrality. It cons"
D08-1079,N06-2046,1,0.863683,"to analyze the information stored in different documents and extract the globally important information to reflect the main topic. In recent years, both unsupervised and supervised methods have been proposed to analyze the information contained in a document set and extract highly salient sentences into the summary, based on syntactic or statistical features. Most recently, the graph-based models have been successfully applied for multi-document summarization by making use of the “voting” or “recommendations” between sentences in the documents (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006). The model first constructs a directed or undirected graph to reflect the relationships between the sentences and then applies the graph-based ranking algorithm to compute the rank scores for the sentences. The sentences with large rank scores are chosen into the summary. However, the model makes uniform use of the sentences in different documents, i.e. all the sentences are ranked without considering the document-level information and the sentence-todocument relationship. Actually, given a document set, different documents are not equally important. For example, the documents close to the ma"
D08-1079,P02-1058,0,\N,Missing
D08-1079,P08-1000,0,\N,Missing
D11-1040,W04-3247,0,0.0245181,"for generic multi-document summarization. The centroid-based method MEAD (Radev et al., 2004) is an implementation of the centroidbased method that scores sentences based on features such as cluster centroids, position, and TF.IDF, etc. NeATS (Lin and Hovy, 2002) adds new features such as topic signature and term clustering to select important content, and use MMR (Goldstein et al., 1999) to remove redundancy. Graph-based ranking methods have been proposed to rank sentences/passages based on “votes” or “recommendations” between each other. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. have improved the graph-ranking 2 http://www1.cs.columbia.edu/nlp/newsblaster/ algorithm by differentiating intra-document and inter-document links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relationships (Wan et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with a single update and we make a novel cont"
D11-1040,N03-1020,0,0.215307,"from its corresponding sub-collection. The sizes of component summaries are not necessarily equal, and moreover, not all dates may be represented, so date selection is also important. We apply a simple mechanism that users specify the overall compression rate φ, and we extract more sentences for important dates while fewer sentences for others. The importance of dates is measured by the burstiness, which indicates probable significant occurrences (Chieu and Lee, 2004). i| The compression rate on ti is set as φi = |C |C |. 4.3 Evaluation Metrics The ROUGE measure is widely used for evaluation (Lin and Hovy, 2003): the DUC contests usually officially employ ROUGE for automatic summarization evaluation. In ROUGE evaluation, the summarization quality is measured by counting the number of overlapping units, such as N-gram, word sequences, and word pairs between the candidate timelines CT and the reference timelines RT . There are several kinds of ROUGE metrics, of which the most important one is ROUGE-N with 3 sub-metrics: 1 ROUGE-N-R is an N-gram recall metric: ROUGE-N-R = P P I∈RT N-gram∈I P P Countmatch (N-gram) I∈RT N-gram∈I Count (N-gram) 2 ROUGE-N-P is an N-gram precision metric: ROUGE-N-P = P P I∈C"
D11-1040,I05-2004,0,0.0120654,"extraction-based methods have been proposed for generic multi-document summarization. The centroid-based method MEAD (Radev et al., 2004) is an implementation of the centroidbased method that scores sentences based on features such as cluster centroids, position, and TF.IDF, etc. NeATS (Lin and Hovy, 2002) adds new features such as topic signature and term clustering to select important content, and use MMR (Goldstein et al., 1999) to remove redundancy. Graph-based ranking methods have been proposed to rank sentences/passages based on “votes” or “recommendations” between each other. TextRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to compute sentence importance. Wan et al. have improved the graph-ranking 2 http://www1.cs.columbia.edu/nlp/newsblaster/ algorithm by differentiating intra-document and inter-document links between sentences (2007b), and have proposed a manifold-ranking method to utilize sentence-to-sentence and sentence-to-topic relationships (Wan et al., 2007a). ETTS seems to be related to a very recent task of “update summarization” started in DUC 2007 and continuing with TAC. However, update summarization only dealt with"
D11-1040,P02-1058,0,\N,Missing
D13-1189,C10-2005,0,0.0708952,"Missing"
D13-1189,P11-1016,0,0.0611836,"Missing"
D13-1189,D10-1101,0,0.558566,"el is a topic-dependent opinion target lexicon which is called object sheet. If a word or phrase in the object sheet appears in a sentence or a hashtag, it is extracted as opinion target. The object sheet is manually built for each topic, which means their method cannot be applied to new topics. The following models are also used for comparison. AssocMi: We implement the unsupervised method for opinion target extraction based on (Hu and Liu, 2004), which relies on association mining and a sentiment lexicon to extract frequent and infrequent product features. CRF: The CRF-based method used in (Jakob and Gurevych, 2010) is also used for comparison. We implement both the single-domain and crossdomain models. Both models are evaluated using 5-fold cross-validation. More specifically, the single-domain model, denoted as CRF-S, trains different models for different topics. In each crossvalidation round, 80 percent of each topic is used for training and the other 20 percent is used for test. The cross-domain model, denoted as CRF-C, uses 16 topics for training and the rest 4 topics for test in each round. 5.4 Comparison Results CMSAE requires all the teams to perform the subjectivity and polarity classification t"
D13-1189,W10-2910,0,0.0130811,"e. 1845 Evaluation Metric Precision, recall and F-measure are used in the evaluation. Since expression boundaries are hard to define exactly in annotation guidelines (Wiebe et al., 2005), both the strict evaluation metric and the soft evaluation metric are used in CMSAE. Strict Evaluation: For a proposed opinion target, it is regarded as correct only if it covers the same span with the annotation result. Note that, in CMSAE, an opinion target should be proposed along with its polarity. The correctness of the polarity is also necessary. Soft Evaluation: The soft evaluation metric presented in (Johansson and Moschitti, 2010) is adopted by CMSAE. The span coverage c between each pair of the proposed target span s and the gold standard span s’ is calculated as follows, s  s (12) c  s, s    s In Equation 12, the operator |· |counts Chinese characters, and the intersection ∩ gives the set of characters that two spans have in common. Using the span coverage, the span set coverage C of a set of spans S with respect to another set S’ C  S , S      c(s, s ) is (13) sS s S  The soft precision P and recall R of a proposed set of spans Sˆ with respect to a gold standard set S is defined as follows: C ( Sˆ"
D13-1189,W06-0301,0,0.0380739,"orithm works well in pseudo topics and the performance can be increased with better clustering results. Therefore, we can try to incorporate other social network information to improve the message clustering performance, which will be studied in our future work. 6 Related Work Sentiment analysis, a.k.a. opinion mining, is the field of studying and analyzing people’s opinions, sentiments, evaluations, appraisals, attitudes, and emotions (Liu, 2012). Most of the previous sentiment analysis researches focus on customer reviews (Pang et al., 2002; Hu and Liu, 2004) and some of them focus on news (Kim and Hovy, 2006) and blogs (Draya et al., 2009). However, sentiment analysis on microblogs has recently attracted much attention and has been proved to be very useful in many applications. Classification of opinion polarity is the most common task studied in microblogs. Go et.al (2009) follow the supervised machine learning approach of Pang et al. (2002) to classify the polarity of each tweet by distant supervision. The training dataset of their method is not manually labeled but automatically collected using the emoticons. Barbosa and Feng (2010) use the similar pseudo training data collected from three onli"
D13-1189,P12-1043,0,0.0217931,"gs that appear frequently in a topic as words. Formally, given a hashtag h that contains n Chinese characters c1c2...cn. We want to segment into several words w1w2...wm, where each word is formed by one of more characters. Firstly, we define the stickiness score for a Chinese string c1c2...cn based on the Symmetrical Conditional Probability (SCP) (Silva and Lopes, 1999): SCP(c1c2 ...cn )  Pr(c1c2 ...cn ) 2 n (1) 1  Pr(c1...ci ) Pr(ci 1...cn ) n  1 i 1 and SCP(c1) = Pr(c1)2 for string with only one character. Pr(c1c2...cn) is the occurrence frequency of the string in the topic. Following (Li et al., 2012b), we smooth the SCP value by taking logarithm calculation. Besides, the length of the string is taken into consideration, SCP(c1c2 ...cn )  n  log SCP(c1c2 ...cn ) (2) where n is the number of characters in the string. Then the stickiness score is defined by the sigmoid function as follows: Stickiness(c1c2 ...cn )  2 1 e  SCP ( c1c2 ...cn ) (3) For the hashtag h = c1c2...cn, we want to segment it into m words w1w2...wm which maximize the following equation, m max  Stickness( wi ) (4) i 1 The optimization of Equation (4) can be solved efficiently by dynamic programming which iterativ"
D13-1189,D12-1123,0,0.014442,"ask of sentiment analysis. Currently, this task has not been well studied in microblogs yet. It is mostly performed on product reviews where opinion targets are always described as product features or aspects. The pioneering research on this task is conducted by Hu and Liu (2004) who propose a method which extracts frequent nouns and noun phrases as the opinion targets. Jakob and Gurevych (2010) model the problem as a sequence labeling task based on Conditional Random Fields (CRF). Qiu et al. (2011) propose a double propagation method to extract opinion word and opinion target simultaneously. Liu et al. (2012) use the word translation model in a monolingual scenario to mine the associations between opinion targets and opinion words. 7 Conclusion and Future Work In this paper, we study the problem of opinion target extraction in Chinese microblogs which has not been well investigated yet. We propose an unsupervised label propagation algorithm to collectively rank the opinion target candidates of all sentences in a topic. We also propose a dynamic programming based algorithm for segmenting Chinese hashtags. Experimental results show the effectiveness of our method. In future work, we will try to coll"
D13-1189,W02-1011,0,0.0129318,"e results reveal that our proposed unsupervised label propagation algorithm works well in pseudo topics and the performance can be increased with better clustering results. Therefore, we can try to incorporate other social network information to improve the message clustering performance, which will be studied in our future work. 6 Related Work Sentiment analysis, a.k.a. opinion mining, is the field of studying and analyzing people’s opinions, sentiments, evaluations, appraisals, attitudes, and emotions (Liu, 2012). Most of the previous sentiment analysis researches focus on customer reviews (Pang et al., 2002; Hu and Liu, 2004) and some of them focus on news (Kim and Hovy, 2006) and blogs (Draya et al., 2009). However, sentiment analysis on microblogs has recently attracted much attention and has been proved to be very useful in many applications. Classification of opinion polarity is the most common task studied in microblogs. Go et.al (2009) follow the supervised machine learning approach of Pang et al. (2002) to classify the polarity of each tweet by distant supervision. The training dataset of their method is not manually labeled but automatically collected using the emoticons. Barbosa and Fen"
D13-1189,P06-1055,0,0.0240127,"Missing"
D13-1189,J11-1002,0,0.611576,"leverages the connection between two adjacent unlabeled nodes to find the correct labels for both of them. The proposed unsupervised method does not need any training corpus which will cost much human labor especially for fine-grained annotation. 4) To the best of our knowledge, the task of opinion target extraction in microblogs has not been well studied yet. It is more challenging than microblog sentiment classification and opinion target extraction in review texts. mentation tool performs poorly on microblogs because the microblog texts are much different from regular texts. 2) Wang et al. (2011) find that hashtags in English tweets are used to highlight the sentiment information such as “ #love”, “#sucks” or serve as user-annotated coarse topics such as “#news”, “#sports”. But in Chinese microblogs, most of the hashtags are used to indicate fine-grained topics such as #NBA 总决赛第七场# (#NBAFinalG7#). Besides, hashtags in Twitter always appear within a sentence such as “I love #BarackObama!” while hashtags in Chinese microblogs are always isolated and are surrounded by two # symbols such as “#巴拉克奥巴马# 我爱 他!” (“#BarackObama# I love him！”). It is noteworthy that topics aggregated by the same"
D13-1189,W11-2207,0,0.0456827,"Missing"
D14-1170,P11-1049,0,0.0246207,"rticle summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased methods (McDonald 2007; Gillick et al., 2009; Xie et al., 2009; Berg-Kirkpatrick et al., 2011; Lei Huang et al., 2011; Woodsend et al., 2012; Galanis 2012), etc. The most relevant work is (Hoang and Kan, 2010) as mentioned above. They also assumed the set of reference papers was given as part of the input. They also adopt the hierarchical topic tree that describes the topic structure in the target paper as an essential input for their system. However, it is non-trivial to build the hierarchical topic tree. Moreover, they do not consider the content of the target paper to construct the related work section, which is actually crucial in the related work section. To the best of our knowl"
D14-1170,W11-0502,0,0.0155514,"rating scores based on a user study. Therefore, our related work sections can be much more suitable for the authors to prepare their final related work sections. 2 Related Work There are few studies to directly address automatic related work generation. Hoang and Kan (2010) proposed a related work summarization system given the set of keywords arranged in a hierarchical fashion that describes the paper’s topic. They used two different rule-based strategies to extract sentences for general topics as well as detailed ones. A few studies focus on multi-document scientific article summarization. Agarwal et al., (2011) introduced an unsupervised approach to the problem of multi-document summarization. The input is a list of papers cited together within the same source article. The key point of this approach is a topic based clustering of fragments extracted from each co-cited article. They rank all the clusters using a query generated from the context surrounding the co-cited list of papers. Yeloglu et al., (2011) compared four different approaches for multi-document scientific articles summarization: MEAD, MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singl"
D14-1170,W04-3247,0,0.0396566,"has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased methods (McDonald 2007; Gillick et al., 2009; Xie et al., 2009; Berg-Kirkpatrick et al., 2011; Lei Huang et al., 2011; Woodsend et al., 2012; Galanis 2012), etc. The most relevant work is (Hoang and Kan, 2010) as mentioned above. They also assumed the set of reference papers was given as part of the input. They also adopt the hierarchical topic tree that describes the topic structure in the target paper as an essential input for their system"
D14-1170,C12-1056,0,0.0299963,"Missing"
D14-1170,W09-1802,0,0.079341,"Missing"
D14-1170,C10-2049,0,0.404416,"ly. Experimental results on a test set of 150 target papers show our method can generate related work sections with better quality than those of several baseline methods. With the ROUGE toolkit, the results indicate the related work sections generated by our system can get higher ROUGE scores. Moreover, our related work sections can get higher rating scores based on a user study. Therefore, our related work sections can be much more suitable for the authors to prepare their final related work sections. 2 Related Work There are few studies to directly address automatic related work generation. Hoang and Kan (2010) proposed a related work summarization system given the set of keywords arranged in a hierarchical fashion that describes the paper’s topic. They used two different rule-based strategies to extract sentences for general topics as well as detailed ones. A few studies focus on multi-document scientific article summarization. Agarwal et al., (2011) introduced an unsupervised approach to the problem of multi-document summarization. The input is a list of papers cited together within the same source article. The key point of this approach is a topic based clustering of fragments extracted from each"
D14-1170,P08-1093,0,0.0873537,"approaches for multi-document scientific articles summarization: MEAD, MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singledocument scientific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al."
D14-1170,I05-2004,0,0.0992742,"Missing"
D14-1170,N09-1066,0,0.0699449,"MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singledocument scientific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008), optimizationbased method"
D14-1170,C08-1087,0,0.577584,"i-document scientific articles summarization: MEAD, MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singledocument scientific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 200"
D14-1170,radev-etal-2004-mead,0,0.0245433,"set. We train two SVR regression models based on the own work part and the previous work part of the training data and apply the models to the test data. The global optimization framework is used to generate the related work sections. We set the maximum word count of the generated related work section to be equal to that of the gold related work section. The parameter values of ?1 , ?2 and ?3 are set to 0.3, 0.1 and 0.6, respectively. The parameter values are tuned on the validation data. We compare our system with five baseline systems: MEAD-WT, LexRank-WT, ARWG-WT, MEAD and LexRank. MEAD 6 (Radev et al., 2004) is an open-source extractive multidocument summarizer. LexRank 7 (Eran and Radev, 2004) is a multi-document summarization system which is based on a random walk on the similarity graph of sentences. We also implement the MEAD, LexRank baselines and our method 5 www-01.ibm.com/software/integration/optimization/cplexoptimizer/ 6 http://www.summarization.com/mead/ 7 In our experiments, LexRank performs much better than the more complex variant - C-LexRank (Qazvinian and Radev, 2008), and thus we choose LexRank, rather than CLexRank, to represent graph-based summarization methods for comparison i"
D14-1170,W06-3326,0,0.0304664,"cles summarization: MEAD, MEAD with corpus specific vocabulary, LexRank and W3SS. Other studies investigate mainly on the singledocument scientific article summarization. Early works including (Luhn 1958; Baxendale 1958; Edumundson 1969) tried to use various features specific to scientific text (e.g., sentence position, or rhetorical clues features). They have proved that these features are effective for the scientific article summarization. Citation information has been already shown effective in summarize the scientific articles. Works including (Mei and Zhai 2008; Qazvinian and Radev 2008; Schwartz and Hearst 2006; Mohammad et al., 2009) employed citation information for the single scientific article summarization. Earlier work (Nakov et al., 2004) indicated that citation sentences may contain important concepts that can give useful descriptions of a paper. Various methods have been proposed for news document summarization, including rule-based methods (Barzilay and Elhadad 1997; Marcu and Daniel 1997), graph-based methods (Mani and Bloedorn 2000; Erkan and Radev 2004; Michalcea and Tarau 2005), learning-based methods (Conroy et al., 2001; Shen et al., 2007; Ouyang et al., 2007; Galanis et al., 2008),"
D14-1170,J00-3003,0,0.0266592,"Missing"
D14-1170,D12-1022,0,0.0533226,"Missing"
D14-1170,P09-1031,0,0.0541856,"Missing"
D14-1170,D07-1040,0,\N,Missing
D14-1170,W04-1013,0,\N,Missing
D14-1195,D11-1003,0,0.0155079,"BV ) 2 . 1 S, R, L and V denote respectively for the number of source tree nodes, the number of rules, size of target lexicon and number of variables involved in each rule. 2 B denotes the beam width. 1829 In this work we utilize the efficiency of independent decoding from the two components respectively and then combine their solutions according to certain standards. This naturally results in a dual decomposition (Rush et al., 2010) solution. Dual decomposition has been applied in several natural language processing tasks, including dependency parsing (Koo et al., 2010), machine translation (Chang and Collins, 2011; Rush and Collins, 2011) and information extraction (Reichart and Barzilay, 2012). However, the strength of this inference strategy has seldom been noticed in researches on language generation tasks. We briefly describe the formulation here. 4.1 Description We denote the pure tree transduction part and the pure ngram part as g(y) and f (z) respectively. Then joint decoding is equivalent to solving: max g(y) + f (z) (3) y∈Y,z∈Z s.t. zkt = ykt , ∀k ∈ {1, ..., n}, ∀t ∈ {0, 1}, where y denotes a derivation which yields a final compression {y1 , ..., ym }. This derivation comes from a pure tree tr"
D14-1195,P06-1048,0,0.0158927,"am scores, we trained a trigram language model on the Reuters Corpus (Volume 1) 7 with modified Kneser-Ney smoothing, using the widely used tool SRILM 8 . 5.3 Model Training The training process of a tree transduction model followed similarly to Cohn and Lapata (2007) using structured SVMs (Tsochantaridis et al., 2005). The structured discriminative models were trained according to McDonald (2006). 5.4 Evaluation Metrics We assessed the compression results by the F1score of grammatical relations (provided by a dependency parser) of generated compressions against the gold-standard compression (Clarke and Lapata, 2006). All systems were controlled to produce similar compression ratios (CR) for fair comparison. We also reported manual evaluation on a sampled subset of 30 sentences from each dataset. Three unpaid volunteers with self-reported fluency in English were asked to rate every candidate. Ratings are in the form of 1-5 scores for each compression. 6 Table 2 shows the compression ratios and Fmeasure of grammatical relations in average for each dataset. Table 3 presents averaged human rating results for each dataset. We carried out pairwise t-test to examine the statistical significance of the differenc"
D14-1195,D07-1008,0,0.38685,"g noisy-channel based generative models and discriminative decision tree models. Structured discriminative compression models (McDonald, 2006) are capable of integrating rich features and have been proved effective for this task. Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including integer linear programming solutions (Clarke and Lapata, 2008) and first-order Markov logic networks (Huang et al., 2012; Yoshikawa et al., 2012). A notable class of methods that explicitly deal with syntactic structures are tree transduction models (Cohn and Lapata, 2007; Cohn and Lapata, 2009). In such models a synchronous grammar is extracted from a corpus of parallel syntax trees with leaves aligned. Compressions are generated from the grammar with learned weights. Previous works have noticed that local coherence is usually needed by introducing ngram language model scores, which will make accurate decoding intractable. Traditional approaches conduct beam search to find approximate solutions (Cohn and Lapata, 2009). In this paper we propose a joint decoding strategy to challenge this decoding task. We address the problem as jointly decoding a simple tree t"
D14-1195,C08-1018,0,0.0211665,"s, sentence compression is finding the best derivation from a syntax tree that produces a simpler target tree, under the current definition of grammar and learned parameters. Each derivation is attached with a score. For the sake of efficient decoding, the score often decom1828 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1828–1833, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics poses with rules involved in the derivation. A typical score definition for a derivation y of source tree x is in such form (Cohn and Lapata, 2008; Cohn and Lapata, 2009): X S(x, y) = wT φr (x)+log P (ngram(y)) (1) r∈y The first term is a weighted sum of features φr (x) defined on each rule r. It is plausible to introduce local scores from ngram models. The second term in the above score definition is added with such purpose. Cohn and Lapata (2009) explained that exact decoding of Equation 1 is intractable. They proposed a beam search decoding strategy coupled with cube-pruning heuristic (Chiang, 2007), which can further improve decoding efficiency at the cost of largely losing exactness in log probability calculations. For efficiency r"
D14-1195,D13-1155,0,0.0134659,"r experiments matched the aforementioned asymptotic analysis. The training process based on new decoding method consumes similar time as beam search with cube-pruning heuristic. 7 Conclusion and Future Work In this paper we propose a joint decoding scheme for tree transduction based sentence compression. Experimental results suggest that the proposed framework works well. The overall performance gets further improved under our framework by introducing the structured discriminative model. As several recent efforts have focused on extracting large-scale parallel corpus for sentence compression (Filippova and Altun, 2013), we would like to study how larger corpora can affect tree transduction and our joint decoding soOriginal: It was very high for people who took their full-time education beyond the age of 18 , and higher among women than men for all art forms except jazz and art galleries . Discr.: It was high for people took education higher among women . (Galanis and Androutsopoulos, 2010): It was high for people who took their education beyond the age of 18 , and higher among women . Pure T3: It was very high for people who took . T3+LM-BeamSearch: It was very high for people who took their education beyon"
D14-1195,N10-1131,0,0.0348863,"asonable time. 5 Experiments 5.1 With this factorization, Algorithm 1 tries to solve the dual problem minu L(u) by alternatively decoding each component. This framework is feasible and plausible in that the two subproblems (line 3 and line 4 in Algorithm 1) can be easily solved with slight modificaBaselines The pure tree transduction model and the discriminative model naturally become part of our baselines for comparison 3 . Besides comparing our methods against the tree-transduction model with ngram scores by beam search decoding, we also compare them against the available previous work from Galanis and Androutsopoulos (2010). This state-of-the-art work adopts a two-stage method to rerank results generated by a discriminative maximum entropy model. 5.2 k,t (i) 5: if ykt = zkt ∀k ∀t then 6: return (y(i) , z(i) ) 7: else (i) (i−1) (i) (i) 8: ukt ← ukt − δi (zkt − ykt ) 9: end if 10: end for Data Preparation We evaluated our methods on two standard corpora 4 , refer to as Written and Spoken respectively. 3 The pure ngram language model should not be considered here as it requires additional length constraints and in general does not produce competitive results at all merely by itself. 4 Available at http://jamesclark"
D14-1195,D10-1125,0,0.0265309,"reduce the time complexity down to O(SRBV ) 2 . 1 S, R, L and V denote respectively for the number of source tree nodes, the number of rules, size of target lexicon and number of variables involved in each rule. 2 B denotes the beam width. 1829 In this work we utilize the efficiency of independent decoding from the two components respectively and then combine their solutions according to certain standards. This naturally results in a dual decomposition (Rush et al., 2010) solution. Dual decomposition has been applied in several natural language processing tasks, including dependency parsing (Koo et al., 2010), machine translation (Chang and Collins, 2011; Rush and Collins, 2011) and information extraction (Reichart and Barzilay, 2012). However, the strength of this inference strategy has seldom been noticed in researches on language generation tasks. We briefly describe the formulation here. 4.1 Description We denote the pure tree transduction part and the pure ngram part as g(y) and f (z) respectively. Then joint decoding is equivalent to solving: max g(y) + f (z) (3) y∈Y,z∈Z s.t. zkt = ykt , ∀k ∈ {1, ..., n}, ∀t ∈ {0, 1}, where y denotes a derivation which yields a final compression {y1 , ..., y"
D14-1195,E06-1038,0,0.522043,"rmance. 1 Introduction Sentence compression is the task of generating a grammatical and shorter summary for a long sentence while preserving its most important information. One specific instantiation is deletion-based compression, namely generating a compression by dropping words. Various approaches have been proposed to challenge the task of deletion-based compression. Earlier pioneering works (Knight and Marcu, 2000) considered several insightful approaches, including noisy-channel based generative models and discriminative decision tree models. Structured discriminative compression models (McDonald, 2006) are capable of integrating rich features and have been proved effective for this task. Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including integer linear programming solutions (Clarke and Lapata, 2008) and first-order Markov logic networks (Huang et al., 2012; Yoshikawa et al., 2012). A notable class of methods that explicitly deal with syntactic structures are tree transduction models (Cohn and Lapata, 2007; Cohn and Lapata, 2009). In such models a synchronous grammar is extracted from a corpus of parallel syntax trees with l"
D14-1195,N12-1008,0,0.0216987,"des, the number of rules, size of target lexicon and number of variables involved in each rule. 2 B denotes the beam width. 1829 In this work we utilize the efficiency of independent decoding from the two components respectively and then combine their solutions according to certain standards. This naturally results in a dual decomposition (Rush et al., 2010) solution. Dual decomposition has been applied in several natural language processing tasks, including dependency parsing (Koo et al., 2010), machine translation (Chang and Collins, 2011; Rush and Collins, 2011) and information extraction (Reichart and Barzilay, 2012). However, the strength of this inference strategy has seldom been noticed in researches on language generation tasks. We briefly describe the formulation here. 4.1 Description We denote the pure tree transduction part and the pure ngram part as g(y) and f (z) respectively. Then joint decoding is equivalent to solving: max g(y) + f (z) (3) y∈Y,z∈Z s.t. zkt = ykt , ∀k ∈ {1, ..., n}, ∀t ∈ {0, 1}, where y denotes a derivation which yields a final compression {y1 , ..., ym }. This derivation comes from a pure tree transduction model. z denotes the compression composed of {z1 , ..., zm } from an ng"
D14-1195,P11-1008,0,0.0140195,"denote respectively for the number of source tree nodes, the number of rules, size of target lexicon and number of variables involved in each rule. 2 B denotes the beam width. 1829 In this work we utilize the efficiency of independent decoding from the two components respectively and then combine their solutions according to certain standards. This naturally results in a dual decomposition (Rush et al., 2010) solution. Dual decomposition has been applied in several natural language processing tasks, including dependency parsing (Koo et al., 2010), machine translation (Chang and Collins, 2011; Rush and Collins, 2011) and information extraction (Reichart and Barzilay, 2012). However, the strength of this inference strategy has seldom been noticed in researches on language generation tasks. We briefly describe the formulation here. 4.1 Description We denote the pure tree transduction part and the pure ngram part as g(y) and f (z) respectively. Then joint decoding is equivalent to solving: max g(y) + f (z) (3) y∈Y,z∈Z s.t. zkt = ykt , ∀k ∈ {1, ..., n}, ∀t ∈ {0, 1}, where y denotes a derivation which yields a final compression {y1 , ..., ym }. This derivation comes from a pure tree transduction model. z denot"
D14-1195,D10-1001,0,0.0280914,"building the chart (Cohn and Lapata, 2009). Cohn and Lapata (2009) proposed a beam search approximation along with cube-pruning heuristics to reduce the time complexity down to O(SRBV ) 2 . 1 S, R, L and V denote respectively for the number of source tree nodes, the number of rules, size of target lexicon and number of variables involved in each rule. 2 B denotes the beam width. 1829 In this work we utilize the efficiency of independent decoding from the two components respectively and then combine their solutions according to certain standards. This naturally results in a dual decomposition (Rush et al., 2010) solution. Dual decomposition has been applied in several natural language processing tasks, including dependency parsing (Koo et al., 2010), machine translation (Chang and Collins, 2011; Rush and Collins, 2011) and information extraction (Reichart and Barzilay, 2012). However, the strength of this inference strategy has seldom been noticed in researches on language generation tasks. We briefly describe the formulation here. 4.1 Description We denote the pure tree transduction part and the pure ngram part as g(y) and f (z) respectively. Then joint decoding is equivalent to solving: max g(y) +"
D14-1195,P12-2068,0,0.014933,"deletion-based compression. Earlier pioneering works (Knight and Marcu, 2000) considered several insightful approaches, including noisy-channel based generative models and discriminative decision tree models. Structured discriminative compression models (McDonald, 2006) are capable of integrating rich features and have been proved effective for this task. Another powerful paradigm for sentence compression should be mentioned here is constraints-based compression,including integer linear programming solutions (Clarke and Lapata, 2008) and first-order Markov logic networks (Huang et al., 2012; Yoshikawa et al., 2012). A notable class of methods that explicitly deal with syntactic structures are tree transduction models (Cohn and Lapata, 2007; Cohn and Lapata, 2009). In such models a synchronous grammar is extracted from a corpus of parallel syntax trees with leaves aligned. Compressions are generated from the grammar with learned weights. Previous works have noticed that local coherence is usually needed by introducing ngram language model scores, which will make accurate decoding intractable. Traditional approaches conduct beam search to find approximate solutions (Cohn and Lapata, 2009). In this paper w"
D14-1195,J07-2003,0,\N,Missing
D15-1012,P11-1049,0,0.210052,"ultaneously perform sentence scoring, extraction and compression. We design a greedy algorithm to approximately optimize the score function. Experimental results show that our methods outperform the state-of-theart extractive systems while maintaining similar grammatical quality. 1 Introduction The task of cross-language summarization is to produce a summary in a target language from documents written in a different source language. This task is particularly useful for readers to quickly get the main idea of documents written in a source language that they are not familiar with. Following Wan (2011), we focus on English-toChinese summarization in this work. The simplest and the most straightforward way to perform cross-language summarization is pipelining general summarization and machine translation. Such systems either translate all the documents before running generic summarization algorithms on the translated documents, or summarize from the original documents and then only translate the produced summary into the target language. Wan (2011) show that such pipelining approaches are inferior to methods that utilize information from both sides. In that work, the author proposes graph-ba"
D15-1012,P15-1153,0,0.0254008,"constituent parse trees for query-focused summarization. Li et al. (2013) propose a guided sentence compression model with ILP-based summary sentence selection. Their following work (Li et al., 2014) incorporate various constraints on constituent parse trees to improve the linguistic quality of the compressed sentences. In these studies, the bestperforming systems require supervised learning for different subtasks. More recent work tries to formulate document summarization tasks as optimization problems and use their solutions to guide sentence compression(Li et al., 2015; Yao et al., 2015). Bing et al. (2015) employ integer linear programming for conducting phrase selection and merging simultaneously to form compressed sentences after phrase extraction. 凯特 女士 硬朗 ， 紧急服务 在佛罗里达州 的 戴德 县， 承担了 风暴 的冲击 主 任 估计， 安德鲁 已经 造成 150亿 美元 到 200亿 美元 的损害 （ 75亿 英镑 ， 100亿 英镑 ） 。 Ms Kate Hale, director of emergency services in Florida's Dade County, which bore the brunt of the storm, estimated that Andrew had already caused Dollars 15bn to Dollars 20bn (Pounds 7.5bn-Pounds 10bn) of damage. 雨果飓风 ， 袭击 东海岸 在 1989年9月 ， 花费了 保险业 约 42亿 美元 。 Hurricane Hugo, which hit the east coast in September 1989, cost the insurance industry"
D15-1012,P13-1100,0,0.0180507,"Missing"
D15-1012,P13-1020,0,0.0114278,"ipeline strategy with heuristics to generate multiple candidate compressions and extract from this compressed sentences. Berg-Kirkpatrick et al. (2011) create linear models of weights learned by structural SVMs for different components and tried to jointly formulate sentence selection and syntax tree trimming in integer linear programs. Woodsend and Lapata (2012) propose quasi tree substitution grammars for multiple rewriting operations. All these methods involve integer linear programming solvers to generate compressed summaries, which is time-consuming for multidocument summarization tasks. Almeida and Martins (2013) form the compressive summarization problem in a more efficient dual decomposition framework. Models for sentence compression and extractive summarization are trained by multitask learning techniques. Wang et al. (2013) explore different types of compression on constituent parse trees for query-focused summarization. Li et al. (2013) propose a guided sentence compression model with ILP-based summary sentence selection. Their following work (Li et al., 2014) incorporate various constraints on constituent parse trees to improve the linguistic quality of the compressed sentences. In these studies"
D15-1012,P09-5002,0,0.0166282,"stant phrase translations. Background Document summarization can be treated as a special kind of translation process: translating from a bunch of related source documents to a short target summary. This analogy also holds for crosslanguage document summarization, with the only difference that the languages of source documents and the target summary are different. Our design of sentence scoring function for cross-language document summarization purpose is inspired by phrase-based machine translation models. Here we briefly describe the general idea of phrase-based translation. One may refer to Koehn (2009) for more detailed description. 2.1 g(pk ) + LM (e(y)) k=1 L−1 X • We achieve state-of-the-art results using the extractive counterpart of our compressive summarization framework. Performance in terms of ROUGE metrics can be significantly improved when simultaneously performing extraction and compression. 2 L X 3 Phrase-based Machine Translation Phrase-based machine translation models are currently giving state-of-the-art translations for many pairs of languages and dominating modern statistical machine translation. Classical word-based IBM models cannot capture local contextual information an"
D15-1012,P11-2000,0,0.229291,"Missing"
D15-1012,D13-1047,0,0.0352543,"Missing"
D15-1012,P13-1136,0,0.0169401,"mponents and tried to jointly formulate sentence selection and syntax tree trimming in integer linear programs. Woodsend and Lapata (2012) propose quasi tree substitution grammars for multiple rewriting operations. All these methods involve integer linear programming solvers to generate compressed summaries, which is time-consuming for multidocument summarization tasks. Almeida and Martins (2013) form the compressive summarization problem in a more efficient dual decomposition framework. Models for sentence compression and extractive summarization are trained by multitask learning techniques. Wang et al. (2013) explore different types of compression on constituent parse trees for query-focused summarization. Li et al. (2013) propose a guided sentence compression model with ILP-based summary sentence selection. Their following work (Li et al., 2014) incorporate various constraints on constituent parse trees to improve the linguistic quality of the compressed sentences. In these studies, the bestperforming systems require supervised learning for different subtasks. More recent work tries to formulate document summarization tasks as optimization problems and use their solutions to guide sentence compre"
D15-1012,D14-1076,0,0.0152152,"eger linear programming solvers to generate compressed summaries, which is time-consuming for multidocument summarization tasks. Almeida and Martins (2013) form the compressive summarization problem in a more efficient dual decomposition framework. Models for sentence compression and extractive summarization are trained by multitask learning techniques. Wang et al. (2013) explore different types of compression on constituent parse trees for query-focused summarization. Li et al. (2013) propose a guided sentence compression model with ILP-based summary sentence selection. Their following work (Li et al., 2014) incorporate various constraints on constituent parse trees to improve the linguistic quality of the compressed sentences. In these studies, the bestperforming systems require supervised learning for different subtasks. More recent work tries to formulate document summarization tasks as optimization problems and use their solutions to guide sentence compression(Li et al., 2015; Yao et al., 2015). Bing et al. (2015) employ integer linear programming for conducting phrase selection and merging simultaneously to form compressed sentences after phrase extraction. 凯特 女士 硬朗 ， 紧急服务 在佛罗里达州 的 戴德 县， 承担了"
D15-1012,D12-1022,0,0.0137514,"s on phrase/bigram selection consistency, we get an ILP with essentially the same objective function and a linear budget constraint. This is conceptually equivalent to solving 124 cially in the context of compressive summarization. Zajic et al. (2006) tries a pipeline strategy with heuristics to generate multiple candidate compressions and extract from this compressed sentences. Berg-Kirkpatrick et al. (2011) create linear models of weights learned by structural SVMs for different components and tried to jointly formulate sentence selection and syntax tree trimming in integer linear programs. Woodsend and Lapata (2012) propose quasi tree substitution grammars for multiple rewriting operations. All these methods involve integer linear programming solvers to generate compressed summaries, which is time-consuming for multidocument summarization tasks. Almeida and Martins (2013) form the compressive summarization problem in a more efficient dual decomposition framework. Models for sentence compression and extractive summarization are trained by multitask learning techniques. Wang et al. (2013) explore different types of compression on constituent parse trees for query-focused summarization. Li et al. (2013) pro"
D15-1012,N10-1134,0,0.0319367,"also naturally applicable to extractive summarization. For extractive summarization, Line 5 corresponds to direct calculations of sentence scores based on our proposed phrase-based function and U will denote all full sentences from the original translated documents. The outline of this algorithm is very similar to the greedy algorithm used by Morita et al. (2013) for subtree extraction, except that in our context the increase of cost function when adding a sentence is exactly the cost of that sentence. When the distortion term is ignored (η = 0), the scoring function is clearly submodular 1 (Lin and Bilmes, 2010) in terms of the set of compressed sentences, since the score now only consists of functional gains of phrases along with bigrams of a compressed sentence. Morita et al. (2013) have proved that when r = 1, this greedy algorithm will achieve a constant approximation factor 1 −1 2 (1 − e ) to the optimal solution. Note that this only gives us the worst case guarantee. What we can achieve in practice is usually far better. On the other hand, setting η < 0 will not affect bg(s) s∈S dist(y(s)) s∈S where d is a predefined constant damping factor to penalize repeated occurrences of the same phrases,"
D15-1012,P11-1052,0,0.0183498,"5 Related Work The task focused in this paper is cross-language document summarization. Several pilot studies have investigated this task. Before Wan (2011)’s work that explicitly utilizes bilingual information in a graph-based framework, earlier methods often use information only from one language (de Chalendar et al., 2005; Pingali et al., 2007; Orasan and Chiorean, 2008; Litvak et al., 2010). This work is closely related to greedy algorithms for budgeted submodular maximization. Many studies have formalized text summarization tasks as submodular maximization problems (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2013). A more recent work (Dasgupta et al., 2013) discussed the problem of maximizing a function with a submodular part and a nonsubmodular dispersion term, which may appear to be closer to our scoring functions. In recent years, some research has made progress beyond extractive summarization, espe6 Conclusion and Future Work In this paper we propose a phrase-based framework for the task of cross-language document summarization. The proposed scoring scheme can be naturally operated on compressive summarization. We use efficient greedy procedure to approximately optimize the sc"
D15-1012,P10-1095,0,0.0607813,"re 2c, we depict the objective value achieved by ILP as exact solution, comparing with results from sentences which are gradually selected and compressed by our greedy algorithm. We can see that the approximation is close. 5 Related Work The task focused in this paper is cross-language document summarization. Several pilot studies have investigated this task. Before Wan (2011)’s work that explicitly utilizes bilingual information in a graph-based framework, earlier methods often use information only from one language (de Chalendar et al., 2005; Pingali et al., 2007; Orasan and Chiorean, 2008; Litvak et al., 2010). This work is closely related to greedy algorithms for budgeted submodular maximization. Many studies have formalized text summarization tasks as submodular maximization problems (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2013). A more recent work (Dasgupta et al., 2013) discussed the problem of maximizing a function with a submodular part and a nonsubmodular dispersion term, which may appear to be closer to our scoring functions. In recent years, some research has made progress beyond extractive summarization, espe6 Conclusion and Future Work In this paper we propose a phras"
D15-1012,P13-1101,0,0.155272,"will follow previous work to set r = 1), and merge it to the summary set at the current iteration (denoted as Si ). The target is to find the compression with maximum gain-cost ratio. This will be discussed in the next section. Note that the algorithm is also naturally applicable to extractive summarization. For extractive summarization, Line 5 corresponds to direct calculations of sentence scores based on our proposed phrase-based function and U will denote all full sentences from the original translated documents. The outline of this algorithm is very similar to the greedy algorithm used by Morita et al. (2013) for subtree extraction, except that in our context the increase of cost function when adding a sentence is exactly the cost of that sentence. When the distortion term is ignored (η = 0), the scoring function is clearly submodular 1 (Lin and Bilmes, 2010) in terms of the set of compressed sentences, since the score now only consists of functional gains of phrases along with bigrams of a compressed sentence. Morita et al. (2013) have proved that when r = 1, this greedy algorithm will achieve a constant approximation factor 1 −1 2 (1 − e ) to the optimal solution. Note that this only gives us th"
D15-1012,orasan-chiorean-2008-evaluation,0,0.303961,"n DUC 2001 dataset. In Figure 2c, we depict the objective value achieved by ILP as exact solution, comparing with results from sentences which are gradually selected and compressed by our greedy algorithm. We can see that the approximation is close. 5 Related Work The task focused in this paper is cross-language document summarization. Several pilot studies have investigated this task. Before Wan (2011)’s work that explicitly utilizes bilingual information in a graph-based framework, earlier methods often use information only from one language (de Chalendar et al., 2005; Pingali et al., 2007; Orasan and Chiorean, 2008; Litvak et al., 2010). This work is closely related to greedy algorithms for budgeted submodular maximization. Many studies have formalized text summarization tasks as submodular maximization problems (Lin and Bilmes, 2010; Lin and Bilmes, 2011; Morita et al., 2013). A more recent work (Dasgupta et al., 2013) discussed the problem of maximizing a function with a submodular part and a nonsubmodular dispersion term, which may appear to be closer to our scoring functions. In recent years, some research has made progress beyond extractive summarization, espe6 Conclusion and Future Work In this pa"
D15-1012,P11-1155,1,0.679609,"simultaneously perform sentence scoring, extraction and compression. We design a greedy algorithm to approximately optimize the score function. Experimental results show that our methods outperform the state-of-theart extractive systems while maintaining similar grammatical quality. 1 Introduction The task of cross-language summarization is to produce a summary in a target language from documents written in a different source language. This task is particularly useful for readers to quickly get the main idea of documents written in a source language that they are not familiar with. Following Wan (2011), we focus on English-toChinese summarization in this work. The simplest and the most straightforward way to perform cross-language summarization is pipelining general summarization and machine translation. Such systems either translate all the documents before running generic summarization algorithms on the translated documents, or summarize from the original documents and then only translate the produced summary into the target language. Wan (2011) show that such pipelining approaches are inferior to methods that utilize information from both sides. In that work, the author proposes graph-ba"
D15-1012,N03-1017,0,\N,Missing
D16-1024,P14-1062,0,0.0416853,"bsequent research, more features and learning algorithms were tried for sentiment classification by a large number of researchers. Recently, the emerging of deep learning has also shed light on this area. Lots of representation learning methods has been proposed to address the sentiment classification task and many of them achieve the state-of-the-art performance on several benchmark datasets, such as the recursive neural tensor network (Socher et al., 2013), paragraph vector (Le and Mikolov, 2014), multi-channel convolutional neural networks (Kim, 2012), dynamic convolutional neural network (Blunsom et al., 2014) and tree structure LSTM (Tai et al., 2015). Very recently, Yang et al. (2016) proposed a similar hierarchical attention network based on GRU in the monolingual setting. Note that our work is independent with theirs and their study was released online after we submitted this study. Cross-lingual sentiment classification is also a popular research topic in the sentiment analysis community which aims to solve the sentiment classification task from a cross-language view. It is of great importance since it can exploit the existing labeled information in a source language to build a sentiment class"
D16-1024,P15-1041,0,0.0760293,"m a cross-language view. It is of great importance since it can exploit the existing labeled information in a source language to build a sentiment classification system in any other target language. Cross-lingual sentiment classification has been extensively studied in the very recent years. Mihalcea et al. (2007) translated English subjectivity words and phrases into the target language to build a lexicon-based classifier. Wan (2009) translated both the training data (English to Chinese) and the test data (Chinese to English) to train different models in both the source and target languages. Chen et al. (2015) proposed a knowledge validation method and incorporated it into a boosting model to transfer credible information between the two languages during training. There have also been several studies addressing the task via multi-lingual text representation learning. Xiao and Guo (2013) learned different representations for words in different languages. Part of the word vector is shared among different languages and the rest is language-dependent. Klementiev et al. (2012) treated the task as a multi-task learning problem where each task corresponds to a single word, and the task relatedness is deri"
D16-1024,P14-1006,0,0.0176442,"uments, which has been proved to be very effective for word sequences. Meanwhile, we propose a hierarchical attention mechanism for the bilingual LSTM network. The sentence-level attention model learns which sentences of a document are more important for determining the overall sentiment while the word-level attention model learns which words in each sentence are decisive. The proposed model achieves good results on a benchmark dataset using English as the source language and Chinese as the target language. 1 Recently, there have been several bilingual representation learning methods such as (Hermann and Blunsom, 2014; Gouws et al., 2014) for cross-lingual sentiment or text classification which achieve promising results. They try to learn a joint embedding space for different languages such that the training data in the source language can be directly applied to the test data in the target language. However, most of the studies only use simple functions, e.g. arithmetic average, to synthesize representations for larger text sequences. Some of them use more complicated compositional models such as the bi-gram non-linearity model in (Hermann and Blunsom, 2014) which also fail to capture the long distance dep"
D16-1024,C12-1089,0,0.0097608,"ng data (English to Chinese) and the test data (Chinese to English) to train different models in both the source and target languages. Chen et al. (2015) proposed a knowledge validation method and incorporated it into a boosting model to transfer credible information between the two languages during training. There have also been several studies addressing the task via multi-lingual text representation learning. Xiao and Guo (2013) learned different representations for words in different languages. Part of the word vector is shared among different languages and the rest is language-dependent. Klementiev et al. (2012) treated the task as a multi-task learning problem where each task corresponds to a single word, and the task relatedness is derived from cooccurrence statistics in bilingual parallel corpora. Chandar A P et al. (2014) and Zhou et al. (2015) used the autoencoders to model the connections between bilingual sentences. It aims to minimize the reconstruction error between the bag-of-words representations of two parallel sentences. Pham et al. (2015) extended the paragraph model into bilingual setting. Each pair of parallel sentences shares the same paragraph vector. Compared to the existing studie"
D16-1024,P07-1123,0,0.0344367,"in the monolingual setting. Note that our work is independent with theirs and their study was released online after we submitted this study. Cross-lingual sentiment classification is also a popular research topic in the sentiment analysis community which aims to solve the sentiment classification task from a cross-language view. It is of great importance since it can exploit the existing labeled information in a source language to build a sentiment classification system in any other target language. Cross-lingual sentiment classification has been extensively studied in the very recent years. Mihalcea et al. (2007) translated English subjectivity words and phrases into the target language to build a lexicon-based classifier. Wan (2009) translated both the training data (English to Chinese) and the test data (Chinese to English) to train different models in both the source and target languages. Chen et al. (2015) proposed a knowledge validation method and incorporated it into a boosting model to transfer credible information between the two languages during training. There have also been several studies addressing the task via multi-lingual text representation learning. Xiao and Guo (2013) learned differ"
D16-1024,W02-1011,0,0.0679136,"first attention-based model designed for cross-lingual sentiment analysis. 3) The proposed framework achieves good results on a benchmark dataset from a cross-language sentiment classification evaluation. It outperforms the best team in the evaluation as well as several strong baseline methods. 2 Related Work Sentiment analysis is the field of studying and analyzing peoples opinions, sentiments, evaluations, appraisals, attitudes, and emotions (Liu, 2012). The most common task of sentiment analysis is polarity classification which arises with the emergence of customer reviews on the Internet. Pang et al. (2002) used supervised learning methods and achieved promising results with simple unigram and bi-gram features. In subsequent research, more features and learning algorithms were tried for sentiment classification by a large number of researchers. Recently, the emerging of deep learning has also shed light on this area. Lots of representation learning methods has been proposed to address the sentiment classification task and many of them achieve the state-of-the-art performance on several benchmark datasets, such as the recursive neural tensor network (Socher et al., 2013), paragraph vector (Le and"
D16-1024,W15-1512,0,0.0422491,"epresentations for words in different languages. Part of the word vector is shared among different languages and the rest is language-dependent. Klementiev et al. (2012) treated the task as a multi-task learning problem where each task corresponds to a single word, and the task relatedness is derived from cooccurrence statistics in bilingual parallel corpora. Chandar A P et al. (2014) and Zhou et al. (2015) used the autoencoders to model the connections between bilingual sentences. It aims to minimize the reconstruction error between the bag-of-words representations of two parallel sentences. Pham et al. (2015) extended the paragraph model into bilingual setting. Each pair of parallel sentences shares the same paragraph vector. Compared to the existing studies, we propose to use the bilingual LSTM network to learn the document representations of reviews in each individual language. It has obvious advantage to model the compositional semantics and to capture the long distance dependencies between words. Besides, we propose a hierarchical neural attention mechanism to capture the sentiment attention in each document. The attention model helps to filter out the noise which is irrelevant to the overall"
D16-1024,J11-2001,0,0.0381771,"Missing"
D16-1024,P15-1150,0,0.0238456,"lgorithms were tried for sentiment classification by a large number of researchers. Recently, the emerging of deep learning has also shed light on this area. Lots of representation learning methods has been proposed to address the sentiment classification task and many of them achieve the state-of-the-art performance on several benchmark datasets, such as the recursive neural tensor network (Socher et al., 2013), paragraph vector (Le and Mikolov, 2014), multi-channel convolutional neural networks (Kim, 2012), dynamic convolutional neural network (Blunsom et al., 2014) and tree structure LSTM (Tai et al., 2015). Very recently, Yang et al. (2016) proposed a similar hierarchical attention network based on GRU in the monolingual setting. Note that our work is independent with theirs and their study was released online after we submitted this study. Cross-lingual sentiment classification is also a popular research topic in the sentiment analysis community which aims to solve the sentiment classification task from a cross-language view. It is of great importance since it can exploit the existing labeled information in a source language to build a sentiment classification system in any other target langua"
D16-1024,P09-1027,1,0.294376,"study. Cross-lingual sentiment classification is also a popular research topic in the sentiment analysis community which aims to solve the sentiment classification task from a cross-language view. It is of great importance since it can exploit the existing labeled information in a source language to build a sentiment classification system in any other target language. Cross-lingual sentiment classification has been extensively studied in the very recent years. Mihalcea et al. (2007) translated English subjectivity words and phrases into the target language to build a lexicon-based classifier. Wan (2009) translated both the training data (English to Chinese) and the test data (Chinese to English) to train different models in both the source and target languages. Chen et al. (2015) proposed a knowledge validation method and incorporated it into a boosting model to transfer credible information between the two languages during training. There have also been several studies addressing the task via multi-lingual text representation learning. Xiao and Guo (2013) learned different representations for words in different languages. Part of the word vector is shared among different languages and the r"
D16-1024,D13-1153,0,0.0160523,"cent years. Mihalcea et al. (2007) translated English subjectivity words and phrases into the target language to build a lexicon-based classifier. Wan (2009) translated both the training data (English to Chinese) and the test data (Chinese to English) to train different models in both the source and target languages. Chen et al. (2015) proposed a knowledge validation method and incorporated it into a boosting model to transfer credible information between the two languages during training. There have also been several studies addressing the task via multi-lingual text representation learning. Xiao and Guo (2013) learned different representations for words in different languages. Part of the word vector is shared among different languages and the rest is language-dependent. Klementiev et al. (2012) treated the task as a multi-task learning problem where each task corresponds to a single word, and the task relatedness is derived from cooccurrence statistics in bilingual parallel corpora. Chandar A P et al. (2014) and Zhou et al. (2015) used the autoencoders to model the connections between bilingual sentences. It aims to minimize the reconstruction error between the bag-of-words representations of two"
D16-1024,N16-1174,0,0.121332,"classification by a large number of researchers. Recently, the emerging of deep learning has also shed light on this area. Lots of representation learning methods has been proposed to address the sentiment classification task and many of them achieve the state-of-the-art performance on several benchmark datasets, such as the recursive neural tensor network (Socher et al., 2013), paragraph vector (Le and Mikolov, 2014), multi-channel convolutional neural networks (Kim, 2012), dynamic convolutional neural network (Blunsom et al., 2014) and tree structure LSTM (Tai et al., 2015). Very recently, Yang et al. (2016) proposed a similar hierarchical attention network based on GRU in the monolingual setting. Note that our work is independent with theirs and their study was released online after we submitted this study. Cross-lingual sentiment classification is also a popular research topic in the sentiment analysis community which aims to solve the sentiment classification task from a cross-language view. It is of great importance since it can exploit the existing labeled information in a source language to build a sentiment classification system in any other target language. Cross-lingual sentiment classif"
D16-1024,P15-1042,0,0.0497128,"credible information between the two languages during training. There have also been several studies addressing the task via multi-lingual text representation learning. Xiao and Guo (2013) learned different representations for words in different languages. Part of the word vector is shared among different languages and the rest is language-dependent. Klementiev et al. (2012) treated the task as a multi-task learning problem where each task corresponds to a single word, and the task relatedness is derived from cooccurrence statistics in bilingual parallel corpora. Chandar A P et al. (2014) and Zhou et al. (2015) used the autoencoders to model the connections between bilingual sentences. It aims to minimize the reconstruction error between the bag-of-words representations of two parallel sentences. Pham et al. (2015) extended the paragraph model into bilingual setting. Each pair of parallel sentences shares the same paragraph vector. Compared to the existing studies, we propose to use the bilingual LSTM network to learn the document representations of reviews in each individual language. It has obvious advantage to model the compositional semantics and to capture the long distance dependencies between"
D17-1003,C10-1011,0,0.0293179,"mposition for IntC [i, j, x]. 5 5.1 Data and Preprocessing • The DeepBank, Enju HPSGBank and Prague Dependency TreeBank are from SemEval 2014 Task 8 (Oepen et al., 2014), and the data splitting policy follows the shared task. Practical Parsing Derivation-Sensitive Training Experiments for CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined HMM tagger (Huang et al., 2010). Experiments for the other three data sets used POS-tags provided by the shared task. We also use features extracted from pseudo trees. We utilize the Mate parser (Bohnet, 2010) to generate pseudo trees. All experimental results consider directed dependencies in a standard way. We report Unlabeled Precision (UP), Recall (UR) and F-score (UF), which are calculated using the official evaluation tool provided by SDP2014 shared task. We extend our quartic-time parsing algorithm into a practical parser. In the context of data-driven parsing, this requires an extra disambiguation model. As with many other parsers, we employ a global linear model. Following Zhang et al. (2016)’s experience, we define rich features extracted from word, POS-tags and pseudo trees. To estimate"
D17-1003,Q15-1040,0,0.192719,"ruction for noncrossing edges and crossing edges; (2) in a single construction step, whether to create a new arc is deterministic. These two characteristics make our algorithm relatively easy to be extended to incorporiate crossing-sensitive second-order features. We then introduce a new algorithm for quasi-second-order parsing. Experiments demonstrate that second-order features are helpful for Maximum Subgraph parsing. 1 Introduction Previous work showed that treating semantic dependency parsing as the search for Maximum Subgraphs is not only elegant in theory but also effective in practice (Kuhlmann and Jonsson, 2015; Cao et al., 2017). In particular, our previous work showed that 1-endpoint-crossing, pagenumber-2 (1 EC / P 2) graphs are an appropriate graph class for modelling semantic dependency structures (Cao et al., 2017). On the one hand, it is highly expressive to cover a majority of semantic analysis. On the other hand, the corresponding Maximum Subgraph problem with an arc-factored disambiguation model can be solved in low-degree polynomial time. Defining disambiguation models on wider contexts than individual bi-lexical dependencies improves various syntactic parsers in different architectures."
D17-1003,P17-1193,1,0.858836,"s and crossing edges; (2) in a single construction step, whether to create a new arc is deterministic. These two characteristics make our algorithm relatively easy to be extended to incorporiate crossing-sensitive second-order features. We then introduce a new algorithm for quasi-second-order parsing. Experiments demonstrate that second-order features are helpful for Maximum Subgraph parsing. 1 Introduction Previous work showed that treating semantic dependency parsing as the search for Maximum Subgraphs is not only elegant in theory but also effective in practice (Kuhlmann and Jonsson, 2015; Cao et al., 2017). In particular, our previous work showed that 1-endpoint-crossing, pagenumber-2 (1 EC / P 2) graphs are an appropriate graph class for modelling semantic dependency structures (Cao et al., 2017). On the one hand, it is highly expressive to cover a majority of semantic analysis. On the other hand, the corresponding Maximum Subgraph problem with an arc-factored disambiguation model can be solved in low-degree polynomial time. Defining disambiguation models on wider contexts than individual bi-lexical dependencies improves various syntactic parsers in different architectures. This paper studies"
D17-1003,E06-1011,0,0.719571,"(GCHSW, hereafter), has two properties that make it hard to incorporate higher-order features in a principled way. First, GCHSW does not explicitly consider the construction of noncrossing arcs. We will show that incorporiating higher-order factors containing crossing arcs without increasing time and space complexity is extremely hard. An effective strategy is to only include higher-order factors containing only noncrossing arcs (Pitler, 2014). But this crossing-sensitive strategy is incompatible with GCHSW. Second, all existing higherorder parsing algorithms for projective trees, including (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), require that which arcs are created in a construction step be deterministic. This design is also incompatible with GCHSW . In summary, it is not convenient to extend GCHSW to incorporate higher-order features while keeping the same time complexity. In this paper, we introduce an alternative Maximum Subgraph algorithm for first-order parsing to 1 EC / P 2 graphs. while keeping the same time and space complexity to GCHSW, our new algorithm has two characteristics that make it relatively easy to be extended to incorporate crossingsensitive, second-order f"
D17-1003,D07-1101,0,0.167503,"properties that make it hard to incorporate higher-order features in a principled way. First, GCHSW does not explicitly consider the construction of noncrossing arcs. We will show that incorporiating higher-order factors containing crossing arcs without increasing time and space complexity is extremely hard. An effective strategy is to only include higher-order factors containing only noncrossing arcs (Pitler, 2014). But this crossing-sensitive strategy is incompatible with GCHSW. Second, all existing higherorder parsing algorithms for projective trees, including (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), require that which arcs are created in a construction step be deterministic. This design is also incompatible with GCHSW . In summary, it is not convenient to extend GCHSW to incorporate higher-order features while keeping the same time complexity. In this paper, we introduce an alternative Maximum Subgraph algorithm for first-order parsing to 1 EC / P 2 graphs. while keeping the same time and space complexity to GCHSW, our new algorithm has two characteristics that make it relatively easy to be extended to incorporate crossingsensitive, second-order features: (1) it"
D17-1003,W02-1001,0,0.091698,"er directed dependencies in a standard way. We report Unlabeled Precision (UP), Recall (UR) and F-score (UF), which are calculated using the official evaluation tool provided by SDP2014 shared task. We extend our quartic-time parsing algorithm into a practical parser. In the context of data-driven parsing, this requires an extra disambiguation model. As with many other parsers, we employ a global linear model. Following Zhang et al. (2016)’s experience, we define rich features extracted from word, POS-tags and pseudo trees. To estimate parameters, we utilize the averaged perceptron algorithm (Collins, 2002). Our training proceudre is sensitive to derivation rather then derived graphs. For each sentence, we first apply our algorithm to find the optimal prediction derivation. The we collect all first- and second-order factors from this derivation to update parameters. To train a first-order model, because our algorithm includes all factors, viz. depencies, there is no difference between our derivationbased method and a traditional derived structurebased method. For the second-order model, our method increases the second-order scores somehow. 5.3 Accuracy Table 1 lists the accuracy of our system. T"
D17-1003,S14-2008,0,0.280148,"acteristics that make it relatively easy to be extended to incorporate crossingsensitive, second-order features: (1) it separates the construction for noncrossing edges and possible crossing edges; (2) whether an edge is created is deterministic in each construction rule. We then introduce a new algorithm to perform secondorder parsing. When all second-order scores are greater than or equal to 0, it exactly solves the corresponding optimization problem. We implement a practical parser with a statistical disambiguation model and evaluate it on four data sets: those used in SemEval 2014 Task 8 (Oepen et al., 2014), and the dependency graphs extracted from CCGbank (Hockenmaier and Steedman, 2007). On all data sets, we find that our second-order parsing models are more acWe propose a new Maximum Subgraph algorithm for first-order parsing to 1endpoint-crossing, pagenumber-2 graphs. Our algorithm has two characteristics: (1) it separates the construction for noncrossing edges and crossing edges; (2) in a single construction step, whether to create a new arc is deterministic. These two characteristics make our algorithm relatively easy to be extended to incorporiate crossing-sensitive second-order features."
D17-1003,C96-1058,0,0.609042,"Maximum Subgraph algorithm, viz. GCHSW, for 1 EC / P 2 graphs by exploring the following property: Every subgraph of a 1 EC / P 2 graph is also a 1 EC / P 2 graph. GCHSW defines a number of prototype backbones for decomposing a 1 EC / P 2 graph in a principled way. In each decomposition step, GCHSW focuses on the edges that can be created without violating either the 1 EC nor P 2 restriction. Sometimes, multiple edges can be created simultaneously in one single step. Figure 4 is an example. There is an important difference between GCHSW and Eisner-style Maximum Spanning Tree algorithms (MST; Eisner, 1996; McDonald and Pereira, 2006; Koo and Collins, 2010). In each construction step, GCHSW allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. For the higher-order MST algorithms, in a single construction step, it is clear whether adding a new arc, and which one. There is no local search. This deterministic strategy is also followed by Kuhlmann and Jonsson’s Maximum Subgraph algorithm for n"
D17-1003,Q14-1004,0,0.220407,"ce and Technology, Peking University The MOE Key Laboratory of Computational Linguistics, Peking University {junjie.cao,huangsheng,ws,wanxiaojun}@pku.edu.cn Abstract (GCHSW, hereafter), has two properties that make it hard to incorporate higher-order features in a principled way. First, GCHSW does not explicitly consider the construction of noncrossing arcs. We will show that incorporiating higher-order factors containing crossing arcs without increasing time and space complexity is extremely hard. An effective strategy is to only include higher-order factors containing only noncrossing arcs (Pitler, 2014). But this crossing-sensitive strategy is incompatible with GCHSW. Second, all existing higherorder parsing algorithms for projective trees, including (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), require that which arcs are created in a construction step be deterministic. This design is also incompatible with GCHSW . In summary, it is not convenient to extend GCHSW to incorporate higher-order features while keeping the same time complexity. In this paper, we introduce an alternative Maximum Subgraph algorithm for first-order parsing to 1 EC / P 2 graphs. while keeping t"
D17-1003,Q13-1002,0,0.550744,"rds, are indexed with integers, an arc from wi to wj as a(i,j) , and the common endpoint, namely pencil point, of all edges crossed with a(i,j) or a(j,i) as pt(i, j). We denote an edge as e(i,j) , if we do not consider its direction. Figure 1 is an example. Definition 3. A pagenumber-k graph means it consists at most k half-planes, and arcs on each half-plane are noncrossing. These half-planes may be thought of as the pages of a book, with the vertex line corresponding to the books spine, and the embedding of a graph into such a structure is known as a book embedding. Figure 2 is an example. (Pitler et al., 2013) proved that 1-endpointcrossing trees are a subclass of graphs whose pagenumber is at most 2. In Cao et al. (2017), we studied graphs that are constrained to be both 1-endpoint-crossing and pagenumber-2. In this paper, we ignored a complex and linguistic-rare And the objective function turns to be: X d Definition 1. Edges e1 and e2 cross if e1 and e2 have distinct endpoints and exactly one of the endpoints of e1 lies between the endpoints of e2 . d∈A RC(G∗ ) sarc (d) + c The formal description of the 1-endpoint-crossing property is adopted from (Pitler et al., 2013). p in G∗ If G is the set of"
D17-1003,hajic-etal-2012-announcing,0,0.0570488,"Missing"
D17-1003,N12-1054,0,0.0445511,"Missing"
D17-1003,J07-3004,0,0.142743,"crossingsensitive, second-order features: (1) it separates the construction for noncrossing edges and possible crossing edges; (2) whether an edge is created is deterministic in each construction rule. We then introduce a new algorithm to perform secondorder parsing. When all second-order scores are greater than or equal to 0, it exactly solves the corresponding optimization problem. We implement a practical parser with a statistical disambiguation model and evaluate it on four data sets: those used in SemEval 2014 Task 8 (Oepen et al., 2014), and the dependency graphs extracted from CCGbank (Hockenmaier and Steedman, 2007). On all data sets, we find that our second-order parsing models are more acWe propose a new Maximum Subgraph algorithm for first-order parsing to 1endpoint-crossing, pagenumber-2 graphs. Our algorithm has two characteristics: (1) it separates the construction for noncrossing edges and crossing edges; (2) in a single construction step, whether to create a new arc is deterministic. These two characteristics make our algorithm relatively easy to be extended to incorporiate crossing-sensitive second-order features. We then introduce a new algorithm for quasi-second-order parsing. Experiments demo"
D17-1003,P17-1077,1,0.748186,"raph. The upper and the lower figures represent two half-planes respectively. G(s, G) denotes the set of all graphs that belong to G and are compatible with s and G. G is usually a complete digraph. spart (s, p) evaluates the event that part p (from a candidate graph G∗ ) is good. We define the order of p according to the number of arcs it contains, in analogy with tree parsing in terminology. Previous work only discussed the first-order case: X arg max sarc (d) X d Page 1 Preliminaries G∗ ∈G(G) c b Figure 1: e(a,c) ’s crossing edges e(b,d) and e(b,e) share an endpoint b. ssib (s) s∈S IB(G∗ ) Sun et al. (2017) introduced a dynamic programming algorithm for second-order planar parsing. Their empirical evaluation showed that secondorder features are effective to improve parsing accuracy. It is still unknown how to incorporate such features for 1 EC / P 2 parsing. x k i b Figure 3: C structure has two crossing chains. 25 k i l j x Figure 4: A prototype backbone of 1 EC / P 2 graphs. To decompose this structure, GCHSW focuses on e(i,j) and e(l,j) , because these two edges can be optionally created without violation of both 1 EC and P 2 restrictions. Our algorithm focuses on the existence of e(i,k) , an"
D17-1003,D10-1002,0,0.0137562,"• Following previous experimental setup for English CCG parsing, we use section 02-21 as training data, section 00 as the development data, and section 23 for testing. Figure 14: Decomposition for IntC [i, j, x]. 5 5.1 Data and Preprocessing • The DeepBank, Enju HPSGBank and Prague Dependency TreeBank are from SemEval 2014 Task 8 (Oepen et al., 2014), and the data splitting policy follows the shared task. Practical Parsing Derivation-Sensitive Training Experiments for CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined HMM tagger (Huang et al., 2010). Experiments for the other three data sets used POS-tags provided by the shared task. We also use features extracted from pseudo trees. We utilize the Mate parser (Bohnet, 2010) to generate pseudo trees. All experimental results consider directed dependencies in a standard way. We report Unlabeled Precision (UP), Recall (UR) and F-score (UF), which are calculated using the official evaluation tool provided by SDP2014 shared task. We extend our quartic-time parsing algorithm into a practical parser. In the context of data-driven parsing, this requires an extra disambiguation model. As with man"
D17-1003,D12-1030,0,0.0412381,"Missing"
D17-1003,P10-1001,0,0.444093,"make it hard to incorporate higher-order features in a principled way. First, GCHSW does not explicitly consider the construction of noncrossing arcs. We will show that incorporiating higher-order factors containing crossing arcs without increasing time and space complexity is extremely hard. An effective strategy is to only include higher-order factors containing only noncrossing arcs (Pitler, 2014). But this crossing-sensitive strategy is incompatible with GCHSW. Second, all existing higherorder parsing algorithms for projective trees, including (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), require that which arcs are created in a construction step be deterministic. This design is also incompatible with GCHSW . In summary, it is not convenient to extend GCHSW to incorporate higher-order features while keeping the same time complexity. In this paper, we introduce an alternative Maximum Subgraph algorithm for first-order parsing to 1 EC / P 2 graphs. while keeping the same time and space complexity to GCHSW, our new algorithm has two characteristics that make it relatively easy to be extended to incorporate crossingsensitive, second-order features: (1) it separates the constructi"
D17-1224,P11-1049,0,0.071474,"ocument summarization methods for addressing this challenging task. It is feasible to automatically construct news overview articles with news synthesis. 5 Related Work The most closely related work is multi-document summarization, which aims to produce a concise (or short) summary to deliver the major information for a given document set. Most summarization methods rank and select a few existing sentences in the documents or compose new sentences with phrases to form a summary. Typical summarization methods include graph-based ranking methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Berg-Kirkpatrick et al., 2011; Wan and Zhang, 2014; Wan and Yang, 2008), sentence classification or regression based methods (Conroy and O’leary, 2001; Shen et al., 2007; Ouyang et al., 2007), ILP-based methods (McDonald, 2007; Gillick and Favre, 2009; Xie et al., 2114 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Bing et al., 2015), submodular maximization based methods (Lin and Bilmes, 2010, 2011; Sipos et al., 2012), DPP (Determinantal Point Process) based methods (Kulesza et al., 2012), and neural model based methods (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), etc. Other related"
D17-1224,P15-1153,0,0.0159515,"nt set. Most summarization methods rank and select a few existing sentences in the documents or compose new sentences with phrases to form a summary. Typical summarization methods include graph-based ranking methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Berg-Kirkpatrick et al., 2011; Wan and Zhang, 2014; Wan and Yang, 2008), sentence classification or regression based methods (Conroy and O’leary, 2001; Shen et al., 2007; Ouyang et al., 2007), ILP-based methods (McDonald, 2007; Gillick and Favre, 2009; Xie et al., 2114 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Bing et al., 2015), submodular maximization based methods (Lin and Bilmes, 2010, 2011; Sipos et al., 2012), DPP (Determinantal Point Process) based methods (Kulesza et al., 2012), and neural model based methods (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), etc. Other related work includes automatic generation of well-structured Wikipedia articles (Sauper and Barzilay, 2009; Yao et al., 2011). Different from Wikinews, Wikipedia articles usually have domain-dependent templates for content filling and organization. 6 Conclusion In this pilot study we proposed a news synthesis approach to addres"
D17-1224,W09-1802,0,0.558309,"es for which the number of available source news articles are less than 5. Finally, we selected 100 longest Wikinews from the remaining set for testing 2 . The average number of words of Wikinews in the test set is 598 and the average number of total words of their source news articles is 2136. 2 Accordingly, the length limit of overview articles produced by different methods is 600 words. Our approach is compared with several typical multi-document summarization methods: Lead, Coverage, Centroid (Radev et al., 2004), TextRank (Mihalcea and Tarau, 2004), ClusterCMRW (Wan and Yang, 2008), ILP (Gillick and Favre, 2009) and Submodular (Li et al., 2012). We also implement SenDivRank that applies the DivRank algorithm on sentences. For our approach, τ is set to 0.4 and ξ is set to 0.5 based on an additional small development set chosen from the remaining Wikinews set. λ in the DivRank algorithm is set to 0.85 by default. Under the control of these thresholds, we only merge a very small number of passages and insert very few sentences from one passage to another passage, so the influence of passage merging on the coherence is very subtle. 4 Evaluation Results and Analysis Automatic Evaluation: Similar to tradit"
D17-1224,J97-1003,0,0.827113,"teps: passage segmentation, passage ranking, and passage selection and merging. The rationale of using passage rather than sentence lies in that 1) the sentences in a passage are more complete and coherent than multiple sentences selected from different places in different documents; 2) it is easier to arrange several passages than to arrange a large number of sentences. 2.1 Passage Segmentation In this step, we aim to segment each source news article into several passages, where each passage represents a subtopic of the event. In order to achieve this goal, we adopt the TextTiling algorithm (Hearst, 1997), which is a popular algorithm for discovering subtopic structure using term repetition. The original TextTiling algorithm usually splits a sentence into different passages, and in order to remedy this problem, we slightly modify the TextTiling algorithm and our new SenTiling algorithm consists of three steps: Tokenization refers to the division of the input text into individual lexical units, and the tokens are converted to lower-case characters and stemmed using the Porter stemmer. Lexical score determination refers to assigning a lexical score of each gap between text blocks. To avoid the i"
D17-1224,N03-1020,0,0.455161,"SenDivRank that applies the DivRank algorithm on sentences. For our approach, τ is set to 0.4 and ξ is set to 0.5 based on an additional small development set chosen from the remaining Wikinews set. λ in the DivRank algorithm is set to 0.85 by default. Under the control of these thresholds, we only merge a very small number of passages and insert very few sentences from one passage to another passage, so the influence of passage merging on the coherence is very subtle. 4 Evaluation Results and Analysis Automatic Evaluation: Similar to traditional summarization tasks, we use the ROUGE metrics (Lin and Hovy, 2003) to automatically evaluate the quality of peer overview articles against the gold-standard references. We use ROUGE1.5.5 and report the F-scores of ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4 (R-SU4). Firstly, we perform evaluation on the whole articles and Table 1 shows the comparison results. We can see that our approach outperforms all the baseline methods with respect to ROUGE-2 and ROUGE-SU4. The Submodular method achieves the highest ROUGE-1 score, but our approach also achieves very high ROUGE-1 score, which is very close to that of the Submodular method. Method Lead Coverage TextRank Ce"
D17-1224,N10-1134,0,0.0366848,"isting sentences in the documents or compose new sentences with phrases to form a summary. Typical summarization methods include graph-based ranking methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Berg-Kirkpatrick et al., 2011; Wan and Zhang, 2014; Wan and Yang, 2008), sentence classification or regression based methods (Conroy and O’leary, 2001; Shen et al., 2007; Ouyang et al., 2007), ILP-based methods (McDonald, 2007; Gillick and Favre, 2009; Xie et al., 2114 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Bing et al., 2015), submodular maximization based methods (Lin and Bilmes, 2010, 2011; Sipos et al., 2012), DPP (Determinantal Point Process) based methods (Kulesza et al., 2012), and neural model based methods (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), etc. Other related work includes automatic generation of well-structured Wikipedia articles (Sauper and Barzilay, 2009; Yao et al., 2011). Different from Wikinews, Wikipedia articles usually have domain-dependent templates for content filling and organization. 6 Conclusion In this pilot study we proposed a news synthesis approach to address the challenging task of automatic generation of news overvi"
D17-1224,P11-1052,0,0.0953868,"Missing"
D17-1224,N16-1012,0,0.0272435,"v, 2004; Mihalcea and Tarau, 2005; Berg-Kirkpatrick et al., 2011; Wan and Zhang, 2014; Wan and Yang, 2008), sentence classification or regression based methods (Conroy and O’leary, 2001; Shen et al., 2007; Ouyang et al., 2007), ILP-based methods (McDonald, 2007; Gillick and Favre, 2009; Xie et al., 2114 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Bing et al., 2015), submodular maximization based methods (Lin and Bilmes, 2010, 2011; Sipos et al., 2012), DPP (Determinantal Point Process) based methods (Kulesza et al., 2012), and neural model based methods (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), etc. Other related work includes automatic generation of well-structured Wikipedia articles (Sauper and Barzilay, 2009; Yao et al., 2011). Different from Wikinews, Wikipedia articles usually have domain-dependent templates for content filling and organization. 6 Conclusion In this pilot study we proposed a news synthesis approach to address the challenging task of automatic generation of news overview articles. Evaluation results on Wikinews verified the efficacy and feasibility of the proposed approach. In future work, we will investigate supervised learning methods"
D17-1224,I05-2004,0,0.038561,"ctive than typical multi-document summarization methods for addressing this challenging task. It is feasible to automatically construct news overview articles with news synthesis. 5 Related Work The most closely related work is multi-document summarization, which aims to produce a concise (or short) summary to deliver the major information for a given document set. Most summarization methods rank and select a few existing sentences in the documents or compose new sentences with phrases to form a summary. Typical summarization methods include graph-based ranking methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Berg-Kirkpatrick et al., 2011; Wan and Zhang, 2014; Wan and Yang, 2008), sentence classification or regression based methods (Conroy and O’leary, 2001; Shen et al., 2007; Ouyang et al., 2007), ILP-based methods (McDonald, 2007; Gillick and Favre, 2009; Xie et al., 2114 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Bing et al., 2015), submodular maximization based methods (Lin and Bilmes, 2010, 2011; Sipos et al., 2012), DPP (Determinantal Point Process) based methods (Kulesza et al., 2012), and neural model based methods (Rush et al., 2015; Chopra et al., 2016; Nallapati et"
D17-1224,K16-1028,0,0.0254109,"Tarau, 2005; Berg-Kirkpatrick et al., 2011; Wan and Zhang, 2014; Wan and Yang, 2008), sentence classification or regression based methods (Conroy and O’leary, 2001; Shen et al., 2007; Ouyang et al., 2007), ILP-based methods (McDonald, 2007; Gillick and Favre, 2009; Xie et al., 2114 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Bing et al., 2015), submodular maximization based methods (Lin and Bilmes, 2010, 2011; Sipos et al., 2012), DPP (Determinantal Point Process) based methods (Kulesza et al., 2012), and neural model based methods (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), etc. Other related work includes automatic generation of well-structured Wikipedia articles (Sauper and Barzilay, 2009; Yao et al., 2011). Different from Wikinews, Wikipedia articles usually have domain-dependent templates for content filling and organization. 6 Conclusion In this pilot study we proposed a news synthesis approach to address the challenging task of automatic generation of news overview articles. Evaluation results on Wikinews verified the efficacy and feasibility of the proposed approach. In future work, we will investigate supervised learning methods for passage ranking and"
D17-1224,D15-1044,0,0.0205439,"ods (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Berg-Kirkpatrick et al., 2011; Wan and Zhang, 2014; Wan and Yang, 2008), sentence classification or regression based methods (Conroy and O’leary, 2001; Shen et al., 2007; Ouyang et al., 2007), ILP-based methods (McDonald, 2007; Gillick and Favre, 2009; Xie et al., 2114 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Bing et al., 2015), submodular maximization based methods (Lin and Bilmes, 2010, 2011; Sipos et al., 2012), DPP (Determinantal Point Process) based methods (Kulesza et al., 2012), and neural model based methods (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), etc. Other related work includes automatic generation of well-structured Wikipedia articles (Sauper and Barzilay, 2009; Yao et al., 2011). Different from Wikinews, Wikipedia articles usually have domain-dependent templates for content filling and organization. 6 Conclusion In this pilot study we proposed a news synthesis approach to address the challenging task of automatic generation of news overview articles. Evaluation results on Wikinews verified the efficacy and feasibility of the proposed approach. In future work, we will investigate superv"
D17-1224,P09-1024,0,0.0387111,"ession based methods (Conroy and O’leary, 2001; Shen et al., 2007; Ouyang et al., 2007), ILP-based methods (McDonald, 2007; Gillick and Favre, 2009; Xie et al., 2114 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Bing et al., 2015), submodular maximization based methods (Lin and Bilmes, 2010, 2011; Sipos et al., 2012), DPP (Determinantal Point Process) based methods (Kulesza et al., 2012), and neural model based methods (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), etc. Other related work includes automatic generation of well-structured Wikipedia articles (Sauper and Barzilay, 2009; Yao et al., 2011). Different from Wikinews, Wikipedia articles usually have domain-dependent templates for content filling and organization. 6 Conclusion In this pilot study we proposed a news synthesis approach to address the challenging task of automatic generation of news overview articles. Evaluation results on Wikinews verified the efficacy and feasibility of the proposed approach. In future work, we will investigate supervised learning methods for passage ranking and selection, and try to paraphrase the selected passages. Acknowledgments This work was supported by NSFC (61331011), 863"
D17-1224,E12-1023,0,0.415518,"ments or compose new sentences with phrases to form a summary. Typical summarization methods include graph-based ranking methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Berg-Kirkpatrick et al., 2011; Wan and Zhang, 2014; Wan and Yang, 2008), sentence classification or regression based methods (Conroy and O’leary, 2001; Shen et al., 2007; Ouyang et al., 2007), ILP-based methods (McDonald, 2007; Gillick and Favre, 2009; Xie et al., 2114 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Bing et al., 2015), submodular maximization based methods (Lin and Bilmes, 2010, 2011; Sipos et al., 2012), DPP (Determinantal Point Process) based methods (Kulesza et al., 2012), and neural model based methods (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), etc. Other related work includes automatic generation of well-structured Wikipedia articles (Sauper and Barzilay, 2009; Yao et al., 2011). Different from Wikinews, Wikipedia articles usually have domain-dependent templates for content filling and organization. 6 Conclusion In this pilot study we proposed a news synthesis approach to address the challenging task of automatic generation of news overview articles. Evaluation res"
D17-1224,D12-1022,0,0.0831405,"ormation for a given document set. Most summarization methods rank and select a few existing sentences in the documents or compose new sentences with phrases to form a summary. Typical summarization methods include graph-based ranking methods (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Berg-Kirkpatrick et al., 2011; Wan and Zhang, 2014; Wan and Yang, 2008), sentence classification or regression based methods (Conroy and O’leary, 2001; Shen et al., 2007; Ouyang et al., 2007), ILP-based methods (McDonald, 2007; Gillick and Favre, 2009; Xie et al., 2114 2009; Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Bing et al., 2015), submodular maximization based methods (Lin and Bilmes, 2010, 2011; Sipos et al., 2012), DPP (Determinantal Point Process) based methods (Kulesza et al., 2012), and neural model based methods (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016), etc. Other related work includes automatic generation of well-structured Wikipedia articles (Sauper and Barzilay, 2009; Yao et al., 2011). Different from Wikinews, Wikipedia articles usually have domain-dependent templates for content filling and organization. 6 Conclusion In this pilot study we proposed a news synthesi"
D17-1224,W04-3252,0,\N,Missing
D18-1414,J08-2001,0,\N,Missing
D18-1414,P00-1065,0,\N,Missing
D18-1414,J08-2005,0,\N,Missing
D18-1414,J08-2004,0,\N,Missing
D18-1414,P06-1055,0,\N,Missing
D18-1414,I11-1017,0,\N,Missing
D18-1414,C12-1053,1,\N,Missing
D18-1414,P15-1109,0,\N,Missing
D18-1414,D15-1186,0,\N,Missing
D18-1414,P16-1070,0,\N,Missing
D18-1414,P16-1173,0,\N,Missing
D18-1414,K17-1041,0,\N,Missing
D18-1414,P17-1044,0,\N,Missing
D18-1414,W17-6306,0,\N,Missing
D18-1414,Q15-1003,0,\N,Missing
D18-1414,N06-1014,0,\N,Missing
I11-1096,W03-0420,0,0.0679046,"Missing"
I11-1096,W06-0130,0,0.137915,"(X) c∈C k Y is the label sequence; X is the observation sequence; Z(X) is a normalization term; fk is a feature function; λ k is the weight of feature function fk ; C is the set of cliques in the undirected graphic model. Given the training data with a set of sentences (characters with their corresponding tags), the parameters of the model are trained by maximizing the conditional loglikelihood. In the testing phase, given a test sentence x, the tagging sequence y is given by Argmaxy P(y|x). CRFs has been shown to perform well on the task of Chinese named entity recognition (Zhou et al, 2006, Chen et al, 2006, Yu et al, 2008). We treat the Chinese NER task as a character-based sequence labeling problem and use the CRFs model for learning and inference. In this study, we use the linear-chain CRFs model imple858 mented in the CRF++ toolkit1. We use four tags (B - beginning of an entity, I - inside of an entity, E – end of an entity, S – a single character entity) for tagging each type of named entity, and thus we have totally 13 tags (including the tag O- outside of any entity). Standard features used in our basic method are listed in Table 1. The segmentation features and POS features are given by"
I11-1096,D10-1098,0,0.0217768,"proach is effective for the task of NER in news comments. The use of focused entities and related entities in the referred news article is very beneficial for the task. The rest of this paper is organized as follows. Section 2 introduces related work. Section 3 presents our proposed approach. Section 4 shows the experimental setup and results. Finally, Section 5 summarizes the conclusions. 2 Related Work Traditional named entity recognition systems use linguistic grammar-based techniques as well as statistical models. The techniques can be categorized into rule-based (Sekine and Nobata, 2004; Chiticariu et al., 2010), machine learning-based (including unsupervised, supervised and semisupervised methods) (Bikel et al., 1999; Mayfield et al., 2003; McCallum and Li, 2003; Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Etzioni et al., 2005; Klementiev and Roth, 2006; Okanohara et al., 2006; Finkel and Manning, 2009; Singh et al., 2010) and hybrid models (Srihari et al., 2001). The most popular statistical models for named entity recognition include Support Vector Machine, Hidden Markov Model, Maximum Entropy Model, Conditional Random Fields, and so on. Background knowledge derived from Wik"
I11-1096,C02-1080,0,0.0665391,"ormation that plays an important role in signaling named entities, and moreover, the structures of Chinese named entities are more complicated, especially for entity abbreviations. Most Chinese NER systems adopt statistical models or hybrid solutions. Sun et al. (2002) consider the problem of Chinese named entity identification using statistical language model, and they integrate word 857 proposed approach. The three key components will be presented in next sections, respectively. segmentation and NE identification into a unified framework that consists of several class-based language models. Fang and Sheng (2002) present a hybrid approach, which combines a machine learning method and a rule based method, to improve the Chinese NE system’s efficiency. Zhu et al. (2003) adopt the source-channel model framework for the single character named entity recognition. Gao et al. (2005) propose a pragmatic mathematical framework in which segmenting known words and detecting unknown words of different types can be performed simultaneously in a unified way. Wu et al. (2005) present a statistical model with human knowledge which treats NER as a probabilistic tagging problem. Fu and Luke (2005) present a lexicalized"
I11-1096,D09-1015,0,0.0611025,"mental setup and results. Finally, Section 5 summarizes the conclusions. 2 Related Work Traditional named entity recognition systems use linguistic grammar-based techniques as well as statistical models. The techniques can be categorized into rule-based (Sekine and Nobata, 2004; Chiticariu et al., 2010), machine learning-based (including unsupervised, supervised and semisupervised methods) (Bikel et al., 1999; Mayfield et al., 2003; McCallum and Li, 2003; Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Etzioni et al., 2005; Klementiev and Roth, 2006; Okanohara et al., 2006; Finkel and Manning, 2009; Singh et al., 2010) and hybrid models (Srihari et al., 2001). The most popular statistical models for named entity recognition include Support Vector Machine, Hidden Markov Model, Maximum Entropy Model, Conditional Random Fields, and so on. Background knowledge derived from Wikipedia and WordNet has been used for improving the NER task (Kazama and Torisawa, 2007; Richman and Schone, 2008; Pennacchiotti and Pantel, 2009). Most previous works have investigated the NER task over formal text such as news articles. These kinds of texts are written by professional writers, and thus they are well o"
I11-1096,W03-0425,0,0.0170841,"ed as follows. Section 2 introduces related work. Section 3 presents our proposed approach. Section 4 shows the experimental setup and results. Finally, Section 5 summarizes the conclusions. 2 Related Work Traditional named entity recognition systems use linguistic grammar-based techniques as well as statistical models. The techniques can be categorized into rule-based (Sekine and Nobata, 2004; Chiticariu et al., 2010), machine learning-based (including unsupervised, supervised and semisupervised methods) (Bikel et al., 1999; Mayfield et al., 2003; McCallum and Li, 2003; Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Etzioni et al., 2005; Klementiev and Roth, 2006; Okanohara et al., 2006; Finkel and Manning, 2009; Singh et al., 2010) and hybrid models (Srihari et al., 2001). The most popular statistical models for named entity recognition include Support Vector Machine, Hidden Markov Model, Maximum Entropy Model, Conditional Random Fields, and so on. Background knowledge derived from Wikipedia and WordNet has been used for improving the NER task (Kazama and Torisawa, 2007; Richman and Schone, 2008; Pennacchiotti and Pantel, 2009). Most previous works have investigated the NER task"
I11-1096,J05-4005,0,0.0484637,"nsider the problem of Chinese named entity identification using statistical language model, and they integrate word 857 proposed approach. The three key components will be presented in next sections, respectively. segmentation and NE identification into a unified framework that consists of several class-based language models. Fang and Sheng (2002) present a hybrid approach, which combines a machine learning method and a rule based method, to improve the Chinese NE system’s efficiency. Zhu et al. (2003) adopt the source-channel model framework for the single character named entity recognition. Gao et al. (2005) propose a pragmatic mathematical framework in which segmenting known words and detecting unknown words of different types can be performed simultaneously in a unified way. Wu et al. (2005) present a statistical model with human knowledge which treats NER as a probabilistic tagging problem. Fu and Luke (2005) present a lexicalized HMM-based approach to Chinese NER. Yu et al. (2008) use a Markov Logic Network to combine various types of domain knowledge to correct the output of the Conditional Random Fields model. Zhao and Kit (2008) propose a supervised learning model which combines the unsupe"
I11-1096,P01-1039,0,0.0419543,"Model, Conditional Random Fields, and so on. Background knowledge derived from Wikipedia and WordNet has been used for improving the NER task (Kazama and Torisawa, 2007; Richman and Schone, 2008; Pennacchiotti and Pantel, 2009). Most previous works have investigated the NER task over formal text such as news articles. These kinds of texts are written by professional writers, and thus they are well organized and well-structured, and they have seldom grammatical and spelling errors and noises. Recently, a few works have investigated the task over informal English texts such as emails and blogs. Huang et al. (2001) address the problem of extracting identity and phone number of the caller from voicemail messages, and they present three typical information extraction methods: hand-crafted rule-based method, maximum entropy models, and probabilistic transducer induction. Jansche and Abney (2002) present a twophase procedure consisting of a hand-crafted component and a classifier for information extraction from voicemail. Minkov et al. (2005) propose to use email-specific structural features and a recall-enhancing method for improving person name recognition from email. Gruhl et al. (2009) explore the appli"
I11-1096,D07-1073,0,0.0347932,"and semisupervised methods) (Bikel et al., 1999; Mayfield et al., 2003; McCallum and Li, 2003; Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Etzioni et al., 2005; Klementiev and Roth, 2006; Okanohara et al., 2006; Finkel and Manning, 2009; Singh et al., 2010) and hybrid models (Srihari et al., 2001). The most popular statistical models for named entity recognition include Support Vector Machine, Hidden Markov Model, Maximum Entropy Model, Conditional Random Fields, and so on. Background knowledge derived from Wikipedia and WordNet has been used for improving the NER task (Kazama and Torisawa, 2007; Richman and Schone, 2008; Pennacchiotti and Pantel, 2009). Most previous works have investigated the NER task over formal text such as news articles. These kinds of texts are written by professional writers, and thus they are well organized and well-structured, and they have seldom grammatical and spelling errors and noises. Recently, a few works have investigated the task over informal English texts such as emails and blogs. Huang et al. (2001) address the problem of extracting identity and phone number of the caller from voicemail messages, and they present three typical information extrac"
I11-1096,P06-1103,0,0.0822583,"s our proposed approach. Section 4 shows the experimental setup and results. Finally, Section 5 summarizes the conclusions. 2 Related Work Traditional named entity recognition systems use linguistic grammar-based techniques as well as statistical models. The techniques can be categorized into rule-based (Sekine and Nobata, 2004; Chiticariu et al., 2010), machine learning-based (including unsupervised, supervised and semisupervised methods) (Bikel et al., 1999; Mayfield et al., 2003; McCallum and Li, 2003; Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Etzioni et al., 2005; Klementiev and Roth, 2006; Okanohara et al., 2006; Finkel and Manning, 2009; Singh et al., 2010) and hybrid models (Srihari et al., 2001). The most popular statistical models for named entity recognition include Support Vector Machine, Hidden Markov Model, Maximum Entropy Model, Conditional Random Fields, and so on. Background knowledge derived from Wikipedia and WordNet has been used for improving the NER task (Kazama and Torisawa, 2007; Richman and Schone, 2008; Pennacchiotti and Pantel, 2009). Most previous works have investigated the NER task over formal text such as news articles. These kinds of texts are written"
I11-1096,W03-0430,0,0.0549669,"r the task. The rest of this paper is organized as follows. Section 2 introduces related work. Section 3 presents our proposed approach. Section 4 shows the experimental setup and results. Finally, Section 5 summarizes the conclusions. 2 Related Work Traditional named entity recognition systems use linguistic grammar-based techniques as well as statistical models. The techniques can be categorized into rule-based (Sekine and Nobata, 2004; Chiticariu et al., 2010), machine learning-based (including unsupervised, supervised and semisupervised methods) (Bikel et al., 1999; Mayfield et al., 2003; McCallum and Li, 2003; Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Etzioni et al., 2005; Klementiev and Roth, 2006; Okanohara et al., 2006; Finkel and Manning, 2009; Singh et al., 2010) and hybrid models (Srihari et al., 2001). The most popular statistical models for named entity recognition include Support Vector Machine, Hidden Markov Model, Maximum Entropy Model, Conditional Random Fields, and so on. Background knowledge derived from Wikipedia and WordNet has been used for improving the NER task (Kazama and Torisawa, 2007; Richman and Schone, 2008; Pennacchiotti and Pantel, 2009). Most pr"
I11-1096,H05-1056,0,0.0297089,"and they have seldom grammatical and spelling errors and noises. Recently, a few works have investigated the task over informal English texts such as emails and blogs. Huang et al. (2001) address the problem of extracting identity and phone number of the caller from voicemail messages, and they present three typical information extraction methods: hand-crafted rule-based method, maximum entropy models, and probabilistic transducer induction. Jansche and Abney (2002) present a twophase procedure consisting of a hand-crafted component and a classifier for information extraction from voicemail. Minkov et al. (2005) propose to use email-specific structural features and a recall-enhancing method for improving person name recognition from email. Gruhl et al. (2009) explore the application of restricted relationship graphs and statistical techniques to improve named entity annotation in on-line forum texts discussing popular music. Generally speaking, the Chinese NER task is harder because Chinese texts have no explicit word segmentation information, and Chinese named entities lacks the capitalization information that plays an important role in signaling named entities, and moreover, the structures of Chine"
I11-1096,P06-1059,0,0.0249299,"ction 4 shows the experimental setup and results. Finally, Section 5 summarizes the conclusions. 2 Related Work Traditional named entity recognition systems use linguistic grammar-based techniques as well as statistical models. The techniques can be categorized into rule-based (Sekine and Nobata, 2004; Chiticariu et al., 2010), machine learning-based (including unsupervised, supervised and semisupervised methods) (Bikel et al., 1999; Mayfield et al., 2003; McCallum and Li, 2003; Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Etzioni et al., 2005; Klementiev and Roth, 2006; Okanohara et al., 2006; Finkel and Manning, 2009; Singh et al., 2010) and hybrid models (Srihari et al., 2001). The most popular statistical models for named entity recognition include Support Vector Machine, Hidden Markov Model, Maximum Entropy Model, Conditional Random Fields, and so on. Background knowledge derived from Wikipedia and WordNet has been used for improving the NER task (Kazama and Torisawa, 2007; Richman and Schone, 2008; Pennacchiotti and Pantel, 2009). Most previous works have investigated the NER task over formal text such as news articles. These kinds of texts are written by professional writers"
I11-1096,D09-1025,0,0.0264785,"ield et al., 2003; McCallum and Li, 2003; Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Etzioni et al., 2005; Klementiev and Roth, 2006; Okanohara et al., 2006; Finkel and Manning, 2009; Singh et al., 2010) and hybrid models (Srihari et al., 2001). The most popular statistical models for named entity recognition include Support Vector Machine, Hidden Markov Model, Maximum Entropy Model, Conditional Random Fields, and so on. Background knowledge derived from Wikipedia and WordNet has been used for improving the NER task (Kazama and Torisawa, 2007; Richman and Schone, 2008; Pennacchiotti and Pantel, 2009). Most previous works have investigated the NER task over formal text such as news articles. These kinds of texts are written by professional writers, and thus they are well organized and well-structured, and they have seldom grammatical and spelling errors and noises. Recently, a few works have investigated the task over informal English texts such as emails and blogs. Huang et al. (2001) address the problem of extracting identity and phone number of the caller from voicemail messages, and they present three typical information extraction methods: hand-crafted rule-based method, maximum entro"
I11-1096,P08-1001,0,0.0267794,"(Bikel et al., 1999; Mayfield et al., 2003; McCallum and Li, 2003; Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Etzioni et al., 2005; Klementiev and Roth, 2006; Okanohara et al., 2006; Finkel and Manning, 2009; Singh et al., 2010) and hybrid models (Srihari et al., 2001). The most popular statistical models for named entity recognition include Support Vector Machine, Hidden Markov Model, Maximum Entropy Model, Conditional Random Fields, and so on. Background knowledge derived from Wikipedia and WordNet has been used for improving the NER task (Kazama and Torisawa, 2007; Richman and Schone, 2008; Pennacchiotti and Pantel, 2009). Most previous works have investigated the NER task over formal text such as news articles. These kinds of texts are written by professional writers, and thus they are well organized and well-structured, and they have seldom grammatical and spelling errors and noises. Recently, a few works have investigated the task over informal English texts such as emails and blogs. Huang et al. (2001) address the problem of extracting identity and phone number of the caller from voicemail messages, and they present three typical information extraction methods: hand-crafted"
I11-1096,sekine-nobata-2004-definition,0,0.0417214,"show that our proposed approach is effective for the task of NER in news comments. The use of focused entities and related entities in the referred news article is very beneficial for the task. The rest of this paper is organized as follows. Section 2 introduces related work. Section 3 presents our proposed approach. Section 4 shows the experimental setup and results. Finally, Section 5 summarizes the conclusions. 2 Related Work Traditional named entity recognition systems use linguistic grammar-based techniques as well as statistical models. The techniques can be categorized into rule-based (Sekine and Nobata, 2004; Chiticariu et al., 2010), machine learning-based (including unsupervised, supervised and semisupervised methods) (Bikel et al., 1999; Mayfield et al., 2003; McCallum and Li, 2003; Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Etzioni et al., 2005; Klementiev and Roth, 2006; Okanohara et al., 2006; Finkel and Manning, 2009; Singh et al., 2010) and hybrid models (Srihari et al., 2001). The most popular statistical models for named entity recognition include Support Vector Machine, Hidden Markov Model, Maximum Entropy Model, Conditional Random Fields, and so on. Background"
I11-1096,N10-1009,0,0.0750002,"Finally, Section 5 summarizes the conclusions. 2 Related Work Traditional named entity recognition systems use linguistic grammar-based techniques as well as statistical models. The techniques can be categorized into rule-based (Sekine and Nobata, 2004; Chiticariu et al., 2010), machine learning-based (including unsupervised, supervised and semisupervised methods) (Bikel et al., 1999; Mayfield et al., 2003; McCallum and Li, 2003; Bender and Ney, 2003; Florian et al., 2003; McCallum and Li, 2003; Etzioni et al., 2005; Klementiev and Roth, 2006; Okanohara et al., 2006; Finkel and Manning, 2009; Singh et al., 2010) and hybrid models (Srihari et al., 2001). The most popular statistical models for named entity recognition include Support Vector Machine, Hidden Markov Model, Maximum Entropy Model, Conditional Random Fields, and so on. Background knowledge derived from Wikipedia and WordNet has been used for improving the NER task (Kazama and Torisawa, 2007; Richman and Schone, 2008; Pennacchiotti and Pantel, 2009). Most previous works have investigated the NER task over formal text such as news articles. These kinds of texts are written by professional writers, and thus they are well organized and well-str"
I11-1096,C02-1012,0,0.0364242,"9) explore the application of restricted relationship graphs and statistical techniques to improve named entity annotation in on-line forum texts discussing popular music. Generally speaking, the Chinese NER task is harder because Chinese texts have no explicit word segmentation information, and Chinese named entities lacks the capitalization information that plays an important role in signaling named entities, and moreover, the structures of Chinese named entities are more complicated, especially for entity abbreviations. Most Chinese NER systems adopt statistical models or hybrid solutions. Sun et al. (2002) consider the problem of Chinese named entity identification using statistical language model, and they integrate word 857 proposed approach. The three key components will be presented in next sections, respectively. segmentation and NE identification into a unified framework that consists of several class-based language models. Fang and Sheng (2002) present a hybrid approach, which combines a machine learning method and a rule based method, to improve the Chinese NE system’s efficiency. Zhu et al. (2003) adopt the source-channel model framework for the single character named entity recognitio"
I11-1096,N09-1033,0,0.0220859,"le”, “ 中 国 电 信”“China Telecom”, which do not appear in the news article, may be talked about in the associated comments. There related entities are also very useful clues and thus we develop a related NE expansion tool to discover the related entities by using web mining techniques. We use the focused name entities extracted from each news article in Section 3.3.2 as seeds and use our tool to discover related entities for each focused entity. Finally, we use these entities as useful entities for feature generation. There have been a few researches (Ohshima et al., 2006; Wang and Cohen, 2007, Vyas and Pantel, 2009) related to named entity expansion. One of the most famous online services is Google Sets2. Motivated by these related researches, our tool consists of the following two key steps for NE expansion of each single focused entity. 1) Given a focused entity e, we first submit four queries [“e 和”] / [“e and”], [“和 e”] / [“and e”], [“e 比”] / [“e than”] and [“比 e”] / [“than e”] to the Google web search engine and get the top 100 results for each query 3 . Then we split the snippets in the search result into sentences. The character sequences that occur both immediately before the two queries ([“和 e”]"
I11-1096,H05-1054,0,0.0272747,"sections, respectively. segmentation and NE identification into a unified framework that consists of several class-based language models. Fang and Sheng (2002) present a hybrid approach, which combines a machine learning method and a rule based method, to improve the Chinese NE system’s efficiency. Zhu et al. (2003) adopt the source-channel model framework for the single character named entity recognition. Gao et al. (2005) propose a pragmatic mathematical framework in which segmenting known words and detecting unknown words of different types can be performed simultaneously in a unified way. Wu et al. (2005) present a statistical model with human knowledge which treats NER as a probabilistic tagging problem. Fu and Luke (2005) present a lexicalized HMM-based approach to Chinese NER. Yu et al. (2008) use a Markov Logic Network to combine various types of domain knowledge to correct the output of the Conditional Random Fields model. Zhao and Kit (2008) propose a supervised learning model which combines the unsupervised segmentation results to improve performance. Most of the researches in this field are restricted to formal text corpus, such as newswire articles. To our knowledge, our work is the f"
I11-1096,I08-4017,0,0.0185532,"el framework for the single character named entity recognition. Gao et al. (2005) propose a pragmatic mathematical framework in which segmenting known words and detecting unknown words of different types can be performed simultaneously in a unified way. Wu et al. (2005) present a statistical model with human knowledge which treats NER as a probabilistic tagging problem. Fu and Luke (2005) present a lexicalized HMM-based approach to Chinese NER. Yu et al. (2008) use a Markov Logic Network to combine various types of domain knowledge to correct the output of the Conditional Random Fields model. Zhao and Kit (2008) propose a supervised learning model which combines the unsupervised segmentation results to improve performance. Most of the researches in this field are restricted to formal text corpus, such as newswire articles. To our knowledge, our work is the first attempt to investigate the named entity recognition task over the informal Chinese news comments. 3 3.1 News Article Useful Entity Identification News Comments Entity Information CRF-Based Entity Recognition Named Entities Dictionary-Based Correction Refined Named Entities Figure 1: The framework of our proposed approach 3.2 Our Proposed Appr"
I11-1096,W06-0140,0,0.0242314,"λ kfk(Yc, X,c) ) Z(X) c∈C k Y is the label sequence; X is the observation sequence; Z(X) is a normalization term; fk is a feature function; λ k is the weight of feature function fk ; C is the set of cliques in the undirected graphic model. Given the training data with a set of sentences (characters with their corresponding tags), the parameters of the model are trained by maximizing the conditional loglikelihood. In the testing phase, given a test sentence x, the tagging sequence y is given by Argmaxy P(y|x). CRFs has been shown to perform well on the task of Chinese named entity recognition (Zhou et al, 2006, Chen et al, 2006, Yu et al, 2008). We treat the Chinese NER task as a character-based sequence labeling problem and use the CRFs model for learning and inference. In this study, we use the linear-chain CRFs model imple858 mented in the CRF++ toolkit1. We use four tags (B - beginning of an entity, I - inside of an entity, E – end of an entity, S – a single character entity) for tagging each type of named entity, and thus we have totally 13 tags (including the tag O- outside of any entity). Standard features used in our basic method are listed in Table 1. The segmentation features and POS feat"
I11-1096,W03-1718,0,0.0290198,"ity abbreviations. Most Chinese NER systems adopt statistical models or hybrid solutions. Sun et al. (2002) consider the problem of Chinese named entity identification using statistical language model, and they integrate word 857 proposed approach. The three key components will be presented in next sections, respectively. segmentation and NE identification into a unified framework that consists of several class-based language models. Fang and Sheng (2002) present a hybrid approach, which combines a machine learning method and a rule based method, to improve the Chinese NE system’s efficiency. Zhu et al. (2003) adopt the source-channel model framework for the single character named entity recognition. Gao et al. (2005) propose a pragmatic mathematical framework in which segmenting known words and detecting unknown words of different types can be performed simultaneously in a unified way. Wu et al. (2005) present a statistical model with human knowledge which treats NER as a probabilistic tagging problem. Fu and Luke (2005) present a lexicalized HMM-based approach to Chinese NER. Yu et al. (2008) use a Markov Logic Network to combine various types of domain knowledge to correct the output of the Cond"
I13-1021,A00-2018,0,0.154822,"elman and Harper (2009) described and evaluated a bi-gram HMM tagger that utilizes latent annotations. The use of latent annotations substantially improves the performance of a simple generative bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. Syntax-free Syntax-based PCFG Parsing with latent variables (PCFGLA) POS tags can be taken as preterminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative PCFG learning, with lexicalized (Collins, 2003; Charniak, 2000) or latent annotation (Matsuzaki et al., 2005; Petrov et al., 2006) refinements. Compared to complex lexicalized parsers, the PCFGLA parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a PCFGLA parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark, 2009). Generative HMMLA PCFGLA Discriminative LLM, LGLM DEP Table 2: Two views of different tagging models. 2.3 Evaluation 2.3.1 Experimental Setting Penn Chinese Treebank (CTB) (Xue"
I13-1021,C04-1041,0,0.0359093,"undamental NLP tasks, including named entity recognition, POS tagging, text chunking, supertagging, etc., employ sequential classifiers for lexical and syntactic disambiguation. In addition to learning linear chain structures, sequence models can even be applied to acquire hierarchical syntactic structures (Tsuruoka et al., 2009). However, long-distance dependencies widely exist in linguistic structures, and many NLP systems suffer from the incapability of capturing these dependencies. For example, previous work has shown that sequence models alone cannot deal with syntactic ambiguities well (Clark and Curran, 2004; Tsuruoka et al., 2009). On the contrary, stateof-the-art systems usually utilize high complexity models, such as lexicalized PCFG models for syntactic parsing, to achieve high accuracy. Unfortunately, they are not suitable for many real world applications due to the sacrifice of efficiency. In this paper, we are concerned with capturing long-distance dependencies in sequence models. Our goal is to develop efficient models with linear time complexity that are also capable to capture non-local dependencies. Two techniques are studied to achieve this goal. First, stacked learning (Breiman, 1996"
I13-1021,I11-1136,0,0.0696737,"parser. Our linguistic analysis can also well explain the poor performance of Chinese CCG parsing when applying the C&C parser (Tse and Curran, 2012). We think the failure is mainly due to overplaying sequence models in both POS tagging and supertag• Character n-gram prefixes and suffixes for n up to 3. To train LLMs, we use the open source linear classifier – LIBLINEAR1 . To train LGLMs, we choose structured perceptron (SP) (Collins, 2002) and passive aggressive (PA) (Crammer et al., 2006) learning algorithms. For the LAHMM and DEP models, we use the systems discribed in (Huang et al., 2009; Hatori et al., 2011); for the PCFGLA models, we use the Berkeley parser2 . 2.3.2 Results Table 1 summarizes the performance in terms of per word classification of different supervised models on the development data. We present the results of both first order (on the left) and second order (on the right) LGLMs. We can see that the perceptron algorithm performs a little better than the PA algorithm for Chinese POS tagging. There is only a slight gap between the local classification model and various structured models. This is very different from English POS tagging. Although the local classifier achieves comparable"
I13-1021,P12-1046,0,0.0598969,"iscriminative models. Compared to generative models, discriminative models define expressive features to classify words. Note that the two generative models employ latent variables to refine the output spaces, which significantly boost the accuracy and increase the robustness of simple generative models. Hidden Markov model with latent variables (HMMLA) Generative models with latent annotations (LA) obtain state-of-the-art performance for a number of NLP tasks. For example, both PCFG and TSG with refined latent variables achieve excellent results for syntactic parsing (Matsuzaki et al., 2005; Shindo et al., 2012). For Chinese POS tagging, Huang, Eidelman and Harper (2009) described and evaluated a bi-gram HMM tagger that utilizes latent annotations. The use of latent annotations substantially improves the performance of a simple generative bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. Syntax-free Syntax-based PCFG Parsing with latent variables (PCFGLA) POS tags can be taken as preterminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative PCFG learni"
I13-1021,N09-2054,0,0.558188,"many errors to the parser. Our linguistic analysis can also well explain the poor performance of Chinese CCG parsing when applying the C&C parser (Tse and Curran, 2012). We think the failure is mainly due to overplaying sequence models in both POS tagging and supertag• Character n-gram prefixes and suffixes for n up to 3. To train LLMs, we use the open source linear classifier – LIBLINEAR1 . To train LGLMs, we choose structured perceptron (SP) (Collins, 2002) and passive aggressive (PA) (Crammer et al., 2006) learning algorithms. For the LAHMM and DEP models, we use the systems discribed in (Huang et al., 2009; Hatori et al., 2011); for the PCFGLA models, we use the Berkeley parser2 . 2.3.2 Results Table 1 summarizes the performance in terms of per word classification of different supervised models on the development data. We present the results of both first order (on the left) and second order (on the right) LGLMs. We can see that the perceptron algorithm performs a little better than the PA algorithm for Chinese POS tagging. There is only a slight gap between the local classification model and various structured models. This is very different from English POS tagging. Although the local classifi"
I13-1021,P12-1026,1,0.841523,"al linear model (LGLM) Sequence labeling models can capture output structures by exploiting local dependencies among words. A global linear model is flexible to in181 provements over the pipeline systems in both POS tagging and dependency parsing tasks. clude linguistic knowledge from multiple information sources, and thus suitable to recognize more new words. A majority of state-of-the-art English POS taggers are based on LGLMs, e.g. structured perceptron (Collins, 2002) and conditional random fields (Lafferty et al., 2001). Such models are also very popular for building Chinese POS taggers (Sun and Uszkoreit, 2012). 2.2 Comparison We can distinguish the five representative tagging models from two views (see Table 2). From a linguistic view, we can distinguish syntax-free and syntax-based models. In a syntex-based model, POS tagging is integrated into parsing, and thus (to some extent) is capable of capturing long range syntactic information. From a machine learning view, we can distinguish generative and discriminative models. Compared to generative models, discriminative models define expressive features to classify words. Note that the two generative models employ latent variables to refine the output"
I13-1021,P11-1139,1,0.863776,"3: Tagging accuracies of different stacking models on the development data. fine features for the LLM/LGLM. ging. Devel. Berkeley 1or LGLM 2or LGLM HMMLA 1or LGLM(HMMLA) 1or LGLM(PCFGLA) 1or LGLM(DEP) LP 80.44 80.38 80.98 80.65 81.55 82.84 82.69 LR 80.31 79.48 79.93 79.62 80.80 81.75 81.68 3.1 F1 81.36 79.93↓ 80.45↓ 80.13↓ 81.17↓ 82.29↑ 82.18↑ Stacked generalization is a meta-learning algorithm that has been first proposed in (Wolpert, 1992) and (Breiman, 1996). Stacked learning has been applied as a system ensemble method in several NLP tasks, such as joint word segmentation and POS tagging (Sun, 2011), and dependency parsing (Nivre and McDonald, 2008). The idea is to include two “levels” of predictors. The first level includes one or more predictors g1 , ..., gK : Rd → R; each receives input x ∈ Rd and outputs a prediction gk (x). The second level consists of a single function h : Rd+K → R that takes as input hx, g1 (x), ..., gK (x)i and outputs a final prediction yˆ = h(x, g1 (x), ..., gK (x)). The predictor, then, combines an ensemble (the gk ’s) with a meta-predictor (h). Table 4: Parsing accuracies on the development data. 1or and 2or respectively denote first order and second order. L"
I13-1021,N12-1030,0,0.238547,"Missing"
I13-1021,D11-1109,0,0.122737,"Missing"
I13-1021,E09-1090,0,0.0199536,"al Linguistics {ws,wanxiaojun}@pku.edu.cn; pxc.pku@gmail.com Abstract perform well for many applications, they are inadequate for tasks where many long-distance dependencies are involved. Sequential classification models play an important role in natural language processing (NLP). Several fundamental NLP tasks, including named entity recognition, POS tagging, text chunking, supertagging, etc., employ sequential classifiers for lexical and syntactic disambiguation. In addition to learning linear chain structures, sequence models can even be applied to acquire hierarchical syntactic structures (Tsuruoka et al., 2009). However, long-distance dependencies widely exist in linguistic structures, and many NLP systems suffer from the incapability of capturing these dependencies. For example, previous work has shown that sequence models alone cannot deal with syntactic ambiguities well (Clark and Curran, 2004; Tsuruoka et al., 2009). On the contrary, stateof-the-art systems usually utilize high complexity models, such as lexicalized PCFG models for syntactic parsing, to achieve high accuracy. Unfortunately, they are not suitable for many real world applications due to the sacrifice of efficiency. In this paper,"
I13-1021,P05-1010,0,0.0537042,"inguish generative and discriminative models. Compared to generative models, discriminative models define expressive features to classify words. Note that the two generative models employ latent variables to refine the output spaces, which significantly boost the accuracy and increase the robustness of simple generative models. Hidden Markov model with latent variables (HMMLA) Generative models with latent annotations (LA) obtain state-of-the-art performance for a number of NLP tasks. For example, both PCFG and TSG with refined latent variables achieve excellent results for syntactic parsing (Matsuzaki et al., 2005; Shindo et al., 2012). For Chinese POS tagging, Huang, Eidelman and Harper (2009) described and evaluated a bi-gram HMM tagger that utilizes latent annotations. The use of latent annotations substantially improves the performance of a simple generative bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. Syntax-free Syntax-based PCFG Parsing with latent variables (PCFGLA) POS tags can be taken as preterminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on"
I13-1021,P08-1108,0,0.0639317,"tacking models on the development data. fine features for the LLM/LGLM. ging. Devel. Berkeley 1or LGLM 2or LGLM HMMLA 1or LGLM(HMMLA) 1or LGLM(PCFGLA) 1or LGLM(DEP) LP 80.44 80.38 80.98 80.65 81.55 82.84 82.69 LR 80.31 79.48 79.93 79.62 80.80 81.75 81.68 3.1 F1 81.36 79.93↓ 80.45↓ 80.13↓ 81.17↓ 82.29↑ 82.18↑ Stacked generalization is a meta-learning algorithm that has been first proposed in (Wolpert, 1992) and (Breiman, 1996). Stacked learning has been applied as a system ensemble method in several NLP tasks, such as joint word segmentation and POS tagging (Sun, 2011), and dependency parsing (Nivre and McDonald, 2008). The idea is to include two “levels” of predictors. The first level includes one or more predictors g1 , ..., gK : Rd → R; each receives input x ∈ Rd and outputs a prediction gk (x). The second level consists of a single function h : Rd+K → R that takes as input hx, g1 (x), ..., gK (x)i and outputs a final prediction yˆ = h(x, g1 (x), ..., gK (x)). The predictor, then, combines an ensemble (the gk ’s) with a meta-predictor (h). Table 4: Parsing accuracies on the development data. 1or and 2or respectively denote first order and second order. LGLM(X) denotes a stacking model with X as the level"
I13-1021,W09-3825,0,0.0829788,"constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative PCFG learning, with lexicalized (Collins, 2003; Charniak, 2000) or latent annotation (Matsuzaki et al., 2005; Petrov et al., 2006) refinements. Compared to complex lexicalized parsers, the PCFGLA parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a PCFGLA parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark, 2009). Generative HMMLA PCFGLA Discriminative LLM, LGLM DEP Table 2: Two views of different tagging models. 2.3 Evaluation 2.3.1 Experimental Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation, POS tagging, syntactic parsing in both constituency and dependency formalisms. In this paper, we use CTB 6.0 as the labeled training data for the study. In order to obtain a representative split of data sets, we conduct experiments following the setting of the CoNLL 2009 shared task (Hajiˇc et al., 2009), which i"
I13-1021,P06-1055,0,0.0399384,"tagger that utilizes latent annotations. The use of latent annotations substantially improves the performance of a simple generative bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. Syntax-free Syntax-based PCFG Parsing with latent variables (PCFGLA) POS tags can be taken as preterminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative PCFG learning, with lexicalized (Collins, 2003; Charniak, 2000) or latent annotation (Matsuzaki et al., 2005; Petrov et al., 2006) refinements. Compared to complex lexicalized parsers, the PCFGLA parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a PCFGLA parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark, 2009). Generative HMMLA PCFGLA Discriminative LLM, LGLM DEP Table 2: Two views of different tagging models. 2.3 Evaluation 2.3.1 Experimental Setting Penn Chinese Treebank (CTB) (Xue et al., 2005) is a popular data set to evaluate a number of Chines"
I13-1021,D10-1001,0,0.0618766,"ve tagging accuracy beyond what is possible by either model in isolation. The method integrates the heterogeneous models by allowing the outputs of the HMMLA, PCFGLA and DEP to de184 added: w−1 , w, w+1 , w−1 w, w w+1 . The clusters are acquired based on the Chinese giga-word data with the MKCLS tool. The number of total clusters is set to 500, which is tuned by (Sun and Uszkoreit, 2012). 3.3 that it uses parser multiple times. We also implement their method and compare the results with our stacking model. We find the accuracy performance produced by the two different methods are comparable. (Rush et al., 2010) introduced dual decomposition as a framework for deriving inference algorithms for serious combinatorial problems in NLP. They successfully applied dual decomposition to the combination of a lexicalized parsing model and a trigram POS tagger. Despite the effectiveness, their method iteratively parses a sentence many times to achieve convergence, and thus is not as efficient as stacking. Evaluation Table 3 summarizes the tagging accuracy of different stacking models. From this table, we can clearly see that the new features derived from the outputs of other models lead to substantial improveme"
I13-1021,W02-1001,0,\N,Missing
I13-1021,J03-4003,0,\N,Missing
I13-1021,W09-1201,0,\N,Missing
I17-2060,I05-3027,0,0.0656516,"ata Since we study approaches based on sentence extraction in this preliminary work, We collected a source corpus that contains 800 articles in Chinese, with the overall size around 800,000 Chinese characters. The source corpus consists of essays written by various authors, discussing relatively diverse topics. 2 Since the similarity calculations in our framework involves vectorial representations for each word, we trained 300 dimensional GloVe vectors (Pennington et al., 2014) on the Chinese Gigaword corpus (Graff and Chen, 2005). We used the Stanford Chinese Segmenter for word segmentation (Tseng et al., 2005). For evaluation, 10 topics which have once appeared in previous Chinese college entrance exams will be provided for all the experimented essay construction systems. We have manually checked that there indeed exists several sentences in the source corpus that are relevant to the given topic. A good system should detect such sentences and use them to generate the essay that well responds to the specified topic. (2) We empirically set the ratio parameter ρ to be 0.8 and observe that the selected sentences can cover the subtopic well while being coherent with the topical word. An essay in Chinese"
I17-2060,E17-1011,0,0.0252914,"ess) 可怕(dreadful) - 悲剧(tragedy) - 灾难(disaster) - 灾害(calamity) - 严峻(rigorous) - 因素(factors) 特殊(extraordinary) - 困难(difficulty) - 困境(straits) - 低谷(trough) 意志(willpower) - 斗志(fighting will) - 顽强(indomitable) - 艰难(tough) - 困苦(tribulation) 痛苦(suffering) - 悲伤(sorrow) - 伤痛(pain) - 痛楚(agony) 悲哀(grieve) - 难过(sad) - 失望(disappointed) - 绝望(despair) - 无助(helpless) - 孤独(loneliness) 茫然(at a loss) - 迷茫(confused) - 困惑(puzzled) - 尴尬(embarrassed) (b) Figure 1: Lexical chains formulated by (a) DivRank and (b) PageRank 5 college entrance tests. One recent work focuses on multiple choice questions in that context (Guo et al., 2017). Conclusion and Future Work In this paper, we study the challenging task of essay construction for Chinese college entrance exams, propose a framework based on diversified lexical chains and show its effectiveness. Our framework is simple in nature and is by no means perfect. For example, structural coherence is not explicitly modeled in our method since lexical chains could only capture topical coherence. We leave more elaborated strategies for content planning as future work. Also, we would like to extend the framework for more difficult titles or topics by exploring proper vectorial repres"
I17-2060,hong-etal-2014-repository,0,0.0130639,"ike to extend the framework for more difficult titles or topics by exploring proper vectorial representations, and to collect manual data for supervised learning. Methods beyond sentence extraction should also be explored to utilize more elaborative syntactic and discursive structures. The approach of selecting sentence for constructing essays share similar methodological nature with extractive summarization, where classic graph-based ranking has been shown useful (Erkan and Radev, 2004). Diversified selection could further improve information coverage (Mei et al., 2010; Lin and Bilmes, 2011; Hong et al., 2014). Note that the goal of essay writing is different with summarization. The task in this study is to generate a rich but coherent article, and every student or system could write a very different essay, while the goal of summarization is to condense documents, in which case the output results should be similar in content, covering the same important facts. Acknowledgments This work was supported by 863 Program of China (2015AA015403), NSFC (61331011), and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology). We thank the ano"
I17-2060,P11-1052,0,0.0111581,"work. Also, we would like to extend the framework for more difficult titles or topics by exploring proper vectorial representations, and to collect manual data for supervised learning. Methods beyond sentence extraction should also be explored to utilize more elaborative syntactic and discursive structures. The approach of selecting sentence for constructing essays share similar methodological nature with extractive summarization, where classic graph-based ranking has been shown useful (Erkan and Radev, 2004). Diversified selection could further improve information coverage (Mei et al., 2010; Lin and Bilmes, 2011; Hong et al., 2014). Note that the goal of essay writing is different with summarization. The task in this study is to generate a rich but coherent article, and every student or system could write a very different essay, while the goal of summarization is to condense documents, in which case the output results should be similar in content, covering the same important facts. Acknowledgments This work was supported by 863 Program of China (2015AA015403), NSFC (61331011), and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technolog"
I17-2060,J91-1002,0,0.720873,"tween their average word vectors. In order to cover more aspects about the topic and to avoid redundancy, we would like to assign high important scores on more diverse chains. Therefore we utilize DivRank (Mei et al., 2010), a well-known diversified ranking algorithm, for calculating the ranking scores for different chains. Specifically, we utilize the pointwise variant of DivRank. At time T , the transition probability from node u to node v is defined as follows: Building Lexical Chains The main task studied in this paper is to construct an essay for a specified topic word. A lexical chain (Morris and Hirst, 1991) is a sequence of words that consists of a series of conceptually related words in a discourse. A lexical chain can be used to model topical contexts as well as text coherence. In our study, we adapt the calculation method used by Barzilay and Elhadad (1999) for our purpose. Due to the shortage of high-coverage thesaurus, we utilize vector semantics to capture lexical relationships, and find that pairwise similarities defined by word vectors can be used to build reliable lexical chains while being more flexible. We treat each retrieved article as a list of words. pT (u, v) = (1 − λ)p∗ (v) + λ"
J11-3005,P02-1046,0,0.00950876,"he co-training algorithm learns two separate classiﬁers: Cen and Ccn . Therefore, in the classiﬁcation phase, we can obtain two prediction values for a test review. We normalize the prediction values into [−1, 1] by dividing the maximum absolute value. Finally, the average of the normalized values is used as the overall prediction value of the review.4 Several theoretical studies have been performed on co-training in the machine learning ﬁeld. Blum and Mitchell (1998) prove that co-training can be successful if the two sufﬁcient and redundant views are conditionally independent of each other. Abney (2002) shows that weak dependence between the two views can also guarantee successful co-training. Balcan, Blum, and Yang (2005) prove that a weaker assumption called ε-expansion is sufﬁcient for iterative co-training to succeed. Wang and Zhou (2010) view the co-training process as a combinative label propagation over two views, and they 4 Though this method of combining scores is unprincipled due to the fact that the scores themselves are not calibrated, we found it worked well in practice. 596 Wan Bilingual Co-Training for Sentiment Classiﬁcation of Chinese Product Reviews provide the sufﬁcient an"
J11-3005,P08-1034,0,0.0128221,"d corpus to train a sentiment classiﬁer. Since the work of Pang, Lee, and Vaithyanathan (2002), various classiﬁcation models and linguistic features have been proposed to improve classiﬁcation performance (Mullen and Collier 2004; Pang and Lee 2004; Read 2005; Wilson, Wiebe, and Hoffmann 2005). More recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of a text at varying levels of granularity. Blitzer, Dredze, and Pereira (2007) investigate domain adaptation for sentiment classiﬁers, focusing on on-line reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classiﬁer and a lexicon-based classiﬁer with precision-based vote weighting. A non-negative matrix tri-factorization approach has been proposed for sentiment classiﬁcation, which learns from lexical prior knowledge in the form of domain-independent sentiment-laden terms in conjunction with domain-dependent unlabeled data and a few labeled data (Li, Zhang, and Sindhwani 2009). Dasgupta and Ng (2009) propose a semi-supervised approach to sentiment classiﬁcation where they ﬁrst use spectral techniques to mine the unambiguous revie"
J11-3005,D08-1014,0,0.0407369,"Missing"
J11-3005,P07-1056,0,0.0802407,"Missing"
J11-3005,W03-0407,0,0.0133212,"Missing"
J11-3005,P09-1079,0,0.0181151,"2007) investigate domain adaptation for sentiment classiﬁers, focusing on on-line reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classiﬁer and a lexicon-based classiﬁer with precision-based vote weighting. A non-negative matrix tri-factorization approach has been proposed for sentiment classiﬁcation, which learns from lexical prior knowledge in the form of domain-independent sentiment-laden terms in conjunction with domain-dependent unlabeled data and a few labeled data (Li, Zhang, and Sindhwani 2009). Dasgupta and Ng (2009) propose a semi-supervised approach to sentiment classiﬁcation where they ﬁrst use spectral techniques to mine the unambiguous reviews and then exploit them to classify the ambiguous reviews by a novel combination of active learning, transductive learning, and ensemble learning. Chinese sentiment analysis has also been studied (Li and Sun 2007) and most such work uses similar lexicon-based or corpus-based methods for Chinese sentiment classiﬁcation. To date, several pilot studies have been performed to leverage rich English resources for sentiment analysis in other languages. Standard naive Ba"
J11-3005,P07-1124,0,0.0169121,"hod. Kim and Hovy (2004) build 588 Wan Bilingual Co-Training for Sentiment Classiﬁcation of Chinese Product Reviews three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentiment-bearing words. Kanayama, Nasukawa, and Watanabe (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensiﬁers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in ﬁnancial news text. Corpus-based methods consider the sentiment analysis task as a classiﬁcation task and they use a labeled corpus to train a sentiment classiﬁer. Since the work of Pang, Lee, and Vaithyanathan (2002), various classiﬁcation models and linguistic features have been proposed to improve classiﬁcation performance (Mullen and Collier 2004; Pang and Lee 2004; Read 2005; Wilson, Wiebe, and Hoffmann 2005). More recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of a text"
J11-3005,W05-0802,0,0.00968973,"et al. 2010). Moreover, several previous studies focus on the problem of cross-lingual text classiﬁcation, which can be considered a special case of cross-domain text classiﬁcation. Bel, Koster, and Villegas (2003) empirically investigate three translation strategies for cross-lingual text categorization: document translation, terminology translation, and proﬁle-based translation. A few novel models have been proposed to address the problem—for example, the EM-based algorithm (Rigutini, Maggini, and Liu 2005), the information bottleneck approach (Ling et al. 2008), multilingual domain models (Gliozzo and Strapparava 2005), and the structural correspondence learning approach (Prettenhofer and Stein 2010; Wei and Pal 2010). Shi et al. (2010) introduce a method to transfer classiﬁcation knowledge across languages by translating the model features and using an EM algorithm. The most recent related work includes multilingual text categorization based on multi-view learning (Amini, Usunier, and Goutte 2009; Amini and Goutte 2010). To the best of our knowledge, co-training has not yet been investigated for cross-domain or cross-lingual text classiﬁcation. 3. The Basic Methods A straightforward method for cross-lingua"
J11-3005,C04-1071,0,0.01836,"Missing"
J11-3005,C04-1200,0,0.0473018,"ent Classiﬁcation Sentiment classiﬁcation can be performed on words, sentences, or documents. In this article we focus on document-level sentiment classiﬁcation, and research in this area has followed a lexicon-based (i.e., rule-based) or a corpus-based (i.e., classiﬁcation-based) approach. Lexicon-based methods involve deriving a sentiment measure for text based on sentiment lexica. Turney (2002) predicts the sentiment orientation of a review as the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is known as the semantic orientation method. Kim and Hovy (2004) build 588 Wan Bilingual Co-Training for Sentiment Classiﬁcation of Chinese Product Reviews three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentiment-bearing words. Kanayama, Nasukawa, and Watanabe (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensiﬁers. Devitt and Ahmad (2007)"
J11-3005,P07-1055,0,0.0112244,"into account contextual valence shifters, such as negations and intensiﬁers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in ﬁnancial news text. Corpus-based methods consider the sentiment analysis task as a classiﬁcation task and they use a labeled corpus to train a sentiment classiﬁer. Since the work of Pang, Lee, and Vaithyanathan (2002), various classiﬁcation models and linguistic features have been proposed to improve classiﬁcation performance (Mullen and Collier 2004; Pang and Lee 2004; Read 2005; Wilson, Wiebe, and Hoffmann 2005). More recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of a text at varying levels of granularity. Blitzer, Dredze, and Pereira (2007) investigate domain adaptation for sentiment classiﬁers, focusing on on-line reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classiﬁer and a lexicon-based classiﬁer with precision-based vote weighting. A non-negative matrix tri-factorization approach has been proposed for sentiment classiﬁcation, which learns from lexical prior knowledge in the form of domai"
J11-3005,W04-2405,0,0.0548991,"o-training algorithm (Blum and Mitchell 1998) is a bootstrapping method; it starts with a set of labeled data, and increases the amount of annotated data using some amount of unlabeled data in an incremental way. One important aspect of co-training is that two conditionally independent views are required for co-training to work, but the independence assumption can be relaxed. In the past, co-training has been successfully applied to statistical parsing (Sarkar 2001), reference resolution (Ng and Cardie 2003), part-of-speech tagging (Clark, Curran, and Osborne 2003), word sense disambiguation (Mihalcea 2004), and e-mail classiﬁcation (Kiritchenko and Matwin 2001). Cotraining has not yet been used for cross-domain or cross-lingual text categorization, however. The intuition behind the co-training algorithm is that if one classiﬁer can conﬁdently predict the class of an example, it can provide one more training example for the other classiﬁer. Of course, if this example happens to be easily classiﬁed by the ﬁrst classiﬁer, it does not mean that this example will be easily classiﬁed by the second classiﬁer, so the second classiﬁer will get useful information to improve itself, and vice versa (Kiritc"
J11-3005,P07-1123,0,0.143837,"Missing"
J11-3005,W04-3253,0,0.110516,"Missing"
J11-3005,N03-1023,0,0.0218368,"the polarity orientation of the review as either positive or negative. 4.2 The Co-Training Algorithm The co-training algorithm (Blum and Mitchell 1998) is a bootstrapping method; it starts with a set of labeled data, and increases the amount of annotated data using some amount of unlabeled data in an incremental way. One important aspect of co-training is that two conditionally independent views are required for co-training to work, but the independence assumption can be relaxed. In the past, co-training has been successfully applied to statistical parsing (Sarkar 2001), reference resolution (Ng and Cardie 2003), part-of-speech tagging (Clark, Curran, and Osborne 2003), word sense disambiguation (Mihalcea 2004), and e-mail classiﬁcation (Kiritchenko and Matwin 2001). Cotraining has not yet been used for cross-domain or cross-lingual text categorization, however. The intuition behind the co-training algorithm is that if one classiﬁer can conﬁdently predict the class of an example, it can provide one more training example for the other classiﬁer. Of course, if this example happens to be easily classiﬁed by the ﬁrst classiﬁer, it does not mean that this example will be easily classiﬁed by the second cla"
J11-3005,W02-1011,0,0.0178237,"Missing"
J11-3005,P04-1035,0,0.00417834,"entiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensiﬁers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in ﬁnancial news text. Corpus-based methods consider the sentiment analysis task as a classiﬁcation task and they use a labeled corpus to train a sentiment classiﬁer. Since the work of Pang, Lee, and Vaithyanathan (2002), various classiﬁcation models and linguistic features have been proposed to improve classiﬁcation performance (Mullen and Collier 2004; Pang and Lee 2004; Read 2005; Wilson, Wiebe, and Hoffmann 2005). More recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of a text at varying levels of granularity. Blitzer, Dredze, and Pereira (2007) investigate domain adaptation for sentiment classiﬁers, focusing on on-line reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classiﬁer and a lexicon-based classiﬁer with precision-based vote weighting. A non-negative matrix tri-factorization approach has been proposed for se"
J11-3005,P10-1114,0,0.0397658,"Missing"
J11-3005,P05-2008,0,0.0243792,"mer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensiﬁers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in ﬁnancial news text. Corpus-based methods consider the sentiment analysis task as a classiﬁcation task and they use a labeled corpus to train a sentiment classiﬁer. Since the work of Pang, Lee, and Vaithyanathan (2002), various classiﬁcation models and linguistic features have been proposed to improve classiﬁcation performance (Mullen and Collier 2004; Pang and Lee 2004; Read 2005; Wilson, Wiebe, and Hoffmann 2005). More recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of a text at varying levels of granularity. Blitzer, Dredze, and Pereira (2007) investigate domain adaptation for sentiment classiﬁers, focusing on on-line reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classiﬁer and a lexicon-based classiﬁer with precision-based vote weighting. A non-negative matrix tri-factorization approach has been proposed for sentiment cla"
J11-3005,W03-1014,0,0.0965563,"Missing"
J11-3005,N01-1023,0,0.0136519,"ned classiﬁer is applied to predict the polarity orientation of the review as either positive or negative. 4.2 The Co-Training Algorithm The co-training algorithm (Blum and Mitchell 1998) is a bootstrapping method; it starts with a set of labeled data, and increases the amount of annotated data using some amount of unlabeled data in an incremental way. One important aspect of co-training is that two conditionally independent views are required for co-training to work, but the independence assumption can be relaxed. In the past, co-training has been successfully applied to statistical parsing (Sarkar 2001), reference resolution (Ng and Cardie 2003), part-of-speech tagging (Clark, Curran, and Osborne 2003), word sense disambiguation (Mihalcea 2004), and e-mail classiﬁcation (Kiritchenko and Matwin 2001). Cotraining has not yet been used for cross-domain or cross-lingual text categorization, however. The intuition behind the co-training algorithm is that if one classiﬁer can conﬁdently predict the class of an example, it can provide one more training example for the other classiﬁer. Of course, if this example happens to be easily classiﬁed by the ﬁrst classiﬁer, it does not mean that this example"
J11-3005,D10-1103,0,0.0553577,"special case of cross-domain text classiﬁcation. Bel, Koster, and Villegas (2003) empirically investigate three translation strategies for cross-lingual text categorization: document translation, terminology translation, and proﬁle-based translation. A few novel models have been proposed to address the problem—for example, the EM-based algorithm (Rigutini, Maggini, and Liu 2005), the information bottleneck approach (Ling et al. 2008), multilingual domain models (Gliozzo and Strapparava 2005), and the structural correspondence learning approach (Prettenhofer and Stein 2010; Wei and Pal 2010). Shi et al. (2010) introduce a method to transfer classiﬁcation knowledge across languages by translating the model features and using an EM algorithm. The most recent related work includes multilingual text categorization based on multi-view learning (Amini, Usunier, and Goutte 2009; Amini and Goutte 2010). To the best of our knowledge, co-training has not yet been investigated for cross-domain or cross-lingual text classiﬁcation. 3. The Basic Methods A straightforward method for cross-lingual sentiment classiﬁcation is to use machine translation for transferring lexica or corpora of reviews between English an"
J11-3005,P08-1036,0,0.0175118,"additional unlabeled Chinese data. Experimental results on two test sets show the effectiveness of the proposed approach, which can outperform basic methods and transductive methods. 1. Introduction Sentiment classiﬁcation is the task of identifying the sentiment polarity of a given text, which is traditionally categorized as either positive or negative. In recent years, sentiment classiﬁcation has drawn much attention in the natural language processing (NLP) ﬁeld and it has many useful applications, such as opinion mining and summarization (Liu, Hu, and Cheng 2005; Ku, Liang, and Chen 2006; Titov and McDonald 2008). To date, a variety of lexicon-based and corpus-based methods have been developed for sentiment classiﬁcation. The lexicon-based methods rely heavily on a sentiment lexicon containing positive terms and negative terms. The corpus-based methods rely heavily on an annotated corpus for training a sentiment classiﬁer. The sentiment lexicon and corpus are considered the most valuable resources for the sentiment classiﬁcation task. However, such resources in different languages are rather unbalanced. Because most previous work focuses on English sentiment classiﬁcation, many annotated sentiment lex"
J11-3005,P02-1053,0,0.00592724,"oach is described in detail in Section 4. Sections 5 and 6 present the evaluation set-up and results, respectively. Lastly, we conclude this article and discuss future work in Section 7. 2. Related Work 2.1 Sentiment Classiﬁcation Sentiment classiﬁcation can be performed on words, sentences, or documents. In this article we focus on document-level sentiment classiﬁcation, and research in this area has followed a lexicon-based (i.e., rule-based) or a corpus-based (i.e., classiﬁcation-based) approach. Lexicon-based methods involve deriving a sentiment measure for text based on sentiment lexica. Turney (2002) predicts the sentiment orientation of a review as the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is known as the semantic orientation method. Kim and Hovy (2004) build 588 Wan Bilingual Co-Training for Sentiment Classiﬁcation of Chinese Product Reviews three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentiment-bearing words. Kanayama, Nasukawa, and Watanabe (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kenn"
J11-3005,D08-1058,1,0.87176,"studied (Li and Sun 2007) and most such work uses similar lexicon-based or corpus-based methods for Chinese sentiment classiﬁcation. To date, several pilot studies have been performed to leverage rich English resources for sentiment analysis in other languages. Standard naive Bayes and SVM classiﬁers have been applied for subjectivity classiﬁcation in Romanian and Spanish (Mihalcea, Banea, and Wiebe 2007; Banea et al. 2008), and the results show that automatic translation is a feasible alternative for the construction of resources and tools for subjectivity analysis in a new target language. Wan (2008) focuses on leveraging both Chinese and English lexica to improve Chinese sentiment analysis by using lexiconbased methods. Wei and Pal (2010) apply structural correspondence learning (SCL) to minimize the noise introduced by machine translations. In this study, we focus on developing novel approaches to improve the corpus-based method for cross-lingual sentiment classiﬁcation of Chinese product reviews. 2.2 Cross-Domain Text Classiﬁcation Cross-domain text classiﬁcation can be considered as a more general task than crosslingual sentiment classiﬁcation. In this task, the labeled and unlabeled"
J11-3005,P09-1027,1,0.169431,"a Chinese review.15 Therefore, each review has two views: the English view and the Chinese view. A review is represented by both its English view and its Chinese view. Fortunately, machine translation techniques have been well developed in the NLP ﬁeld (Lopez 2008), though the translation performance is far from satisfactory. A few commercial machine translation services can be publicly accessed, for example, Google Translate (GoogleTranslate),16 Yahoo Babel Fish (YahooTranslate),17 and 12 13 14 15 http://www.it168.com. http://www.360buy.com. Only 1,000 unlabeled Chinese reviews were used in Wan (2009). We used the recently updated MT services for machine translation; we believe that the translation results are better than those in Wan (2009). 16 http://translate.google.com/translate t. 17 http://babelfish.yahoo.com/translate txt. 598 Wan Bilingual Co-Training for Sentiment Classiﬁcation of Chinese Product Reviews Microsoft Bing Translate (MicrosoftTranslate).18 The three MT systems are considered to be state-of-the-art commercial machine translation systems, and all three MT systems provide Chinese-to-English and English-to-Chinese translation services. However, it is not easy to accuratel"
J11-3005,P10-2048,0,0.0574223,"To date, several pilot studies have been performed to leverage rich English resources for sentiment analysis in other languages. Standard naive Bayes and SVM classiﬁers have been applied for subjectivity classiﬁcation in Romanian and Spanish (Mihalcea, Banea, and Wiebe 2007; Banea et al. 2008), and the results show that automatic translation is a feasible alternative for the construction of resources and tools for subjectivity analysis in a new target language. Wan (2008) focuses on leveraging both Chinese and English lexica to improve Chinese sentiment analysis by using lexiconbased methods. Wei and Pal (2010) apply structural correspondence learning (SCL) to minimize the noise introduced by machine translations. In this study, we focus on developing novel approaches to improve the corpus-based method for cross-lingual sentiment classiﬁcation of Chinese product reviews. 2.2 Cross-Domain Text Classiﬁcation Cross-domain text classiﬁcation can be considered as a more general task than crosslingual sentiment classiﬁcation. In this task, the labeled and unlabeled data come from different domains and their underlying distributions are often different from each other, which violates the basic assumption o"
J11-3005,H05-1044,0,0.011344,"and the Chinese classiﬁer disagree on many unlabeled examples, which can also guarantee the success of the co-training approach. 5. Evaluation Set-up 5.1 English Sentiment Resources The basic LEX(EN) and LEX(CN) methods require English sentiment lexica. In this study, we collected and used the following popular and publicly available English sentiment lexica,5 without any further ﬁltering and labeling: Positive Dicen : 2,718 English positive terms (e.g., amazing, gorgeous) were collected from a feature ﬁle6 containing the subjectivity clues used in the work (Wilson, Wiebe, and Hoffmann 2005; Wilson et al. 2005). The clues in this ﬁle were collected from a number of sources. Some were culled from manually developed resources (e.g., General Inquirer7 [Stone et al. 1966]). Others were identiﬁed automatically using both annotated and unannotated data. A majority of the clues were collected as part of the work reported in Riloff and Wiebe (2003). Negative Dicen : 4,910 English negative terms (e.g., boring, idiot) were collected from the same ﬁle. Negation Dicen : 88 negation terms (e.g., never, lack) were collected from a feature ﬁle8 used in Wilson, Wiebe, and Hoffmann 2005; Wilson et al. 2005. Intensiﬁ"
J11-3005,H05-2018,0,0.0176434,"and the Chinese classiﬁer disagree on many unlabeled examples, which can also guarantee the success of the co-training approach. 5. Evaluation Set-up 5.1 English Sentiment Resources The basic LEX(EN) and LEX(CN) methods require English sentiment lexica. In this study, we collected and used the following popular and publicly available English sentiment lexica,5 without any further ﬁltering and labeling: Positive Dicen : 2,718 English positive terms (e.g., amazing, gorgeous) were collected from a feature ﬁle6 containing the subjectivity clues used in the work (Wilson, Wiebe, and Hoffmann 2005; Wilson et al. 2005). The clues in this ﬁle were collected from a number of sources. Some were culled from manually developed resources (e.g., General Inquirer7 [Stone et al. 1966]). Others were identiﬁed automatically using both annotated and unannotated data. A majority of the clues were collected as part of the work reported in Riloff and Wiebe (2003). Negative Dicen : 4,910 English negative terms (e.g., boring, idiot) were collected from the same ﬁle. Negation Dicen : 88 negation terms (e.g., never, lack) were collected from a feature ﬁle8 used in Wilson, Wiebe, and Hoffmann 2005; Wilson et al. 2005. Intensiﬁ"
J11-3005,P09-1028,0,\N,Missing
J16-3001,P11-1048,0,0.0161427,"gnificantly improved by system combination; compared to the best individual system, system combination gets an absolute labeled F-score improvement of 1.21 on average. 3. Transition combination significantly improves parsing accuracy on a wide range of conditions, resulting in an absolute labeled F-score improvement of 0.74 on average. 4. Pseudo trees contribute to semantic dependency parsing (SDP) equally well to syntactic trees, and result in an absolute labeled F-score improvement of 1.27 on average. We compare our parser with representative state-of-the-art parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b; Martins and Almeida 2014; Xu, Clark, and Zhang 2014; Du, Sun, and Wan 2015) with respect to different architectures. To evaluate the impact of grammatical knowledge, we compare our parser with parsers guided by treebank-induced HPSG and CCG grammars. Both of our individual and ensembled parsers achieve equivalent accuracy to HPSG and CCG chart parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b), and outperform a shift-reduce CCG parser (Xu, Clark, and Zhang 2014). It is worth noting that our parsers exclude all syntactic and grammatical information. In other words, strictly less informati"
J16-3001,D11-1031,0,0.059111,"Missing"
J16-3001,C10-1011,0,0.104622,"Missing"
J16-3001,P11-2121,0,0.013648,". Accordingly, their algorithm is specially designed to handle projective trees and two-planar trees, but not all graphs. Because many more crossing arcs exist in deep dependency structures and more sentences are assigned with neither planar nor two-planar graphs, their strategy of utilizing two stacks is not suitable for the deep dependency parsing problem. Different from their system, our new system maximizes the utility of two memory modules and is able to handle any directed graphs. The list-based systems, such as the basic one introduced by Nivre (2008) and the extended one introduced by Choi and Palmer (2011), also use two memory modules. The function of the secondary memory module of their systems and ours is very different. In our design, only nodes involved in a subgraph that contains crossing arcs may be put into the second stack. In the existing list-based systems, both lists are heavily used, and nodes may be transferred between them many times. The function of the two lists is to simulate one memory module that allows accessing any unit in it. 2.5 Extension 2.5.1 Graphs with Loops. It is easy to extend our system to generate arbitrary directed graphs by adding a new transition: r S ELF -A R"
J16-3001,J07-4004,0,0.188421,"Missing"
J16-3001,P02-1042,0,0.321502,"Missing"
J16-3001,W02-1001,0,0.800236,"k is about the model diversity obtained by the heterogeneous design of transition systems for general graph spanning. Empirical evaluation indicates that statistical parsers built upon our new transition systems as well as the existing best transition system—namely, Titov et al. (2009)’s system ( THMM, hereafter)—exhibit complementary parsing strengths, which benefit system combination. In order to take advantage of this model diversity, we propose a simple yet effective ensemble model to build a better hybrid system. We implement statistical parsers using the structured perceptron algorithm (Collins 2002) for transition classification and use a beam decoder for global inference. Concerning the disambiguation problem, we introduce two new techniques, namely, transition combination and tree approximation, to improve parsing quality. To increase system coverage, the A RC transitions designed by the THMM as well as our systems do not change the nodes in the stack nor buffer in a configuration: Only the nodes linked to the top of the stack or buffer are modified. Therefore, features derived from the configurations before and after an A RC transition are not distinct enough to train a good classifie"
J16-3001,P04-1015,0,0.0712844,"on of the maximization is extremely hard without any assumption of φ. Even with a proper φ for real-word parsing, exact decoding is still impractical for most practical feature designs. In this article, we follow the recent success of using beam search for approximate decoding. During parsing, the parser keeps track of multiple yet a fixed number of partial outputs to avoid making decisions too early. Training a parser in the discriminative setting corresponds to estimating θ associated with rich features. Previous research on dependency parsing shows that structured perceptron (Collins 2002; Collins and Roark 2004) is one of the strongest learning algorithms. In all experiments, we use the averaged perceptron algorithm with early update to estimate parameters. The whole parser is very similar to the transition-based system introduced in Zhang and Clark (2008, 2011b). 3.2 Transition Combination In either THMM, SS , or S2S , the L EFT /R IGHT-A RC transition does not modify either the stack or the buffer. Only new edges are added to the target graph. When automatic classifiers are utilized to approximate an oracle, a majority of features for predicting an A RC transition will be overlapped with the featur"
J16-3001,P15-1149,1,0.838524,"Missing"
J16-3001,S14-2080,1,0.843481,"ly less information is used. This result demonstrates the effectiveness of data-driven approaches to the deep linguistic processing problem. Compared to other types of data-driven parsers, our individual parser achieves equivalent performance to and our hybrid parser obtains slightly better results than factorization parsers based on dual decomposition (Martins and Almeida 2014; Du, Sun, and Wan 2015). This result highlights the effectiveness of the lightweight, transitionbased approach. Parsers based on the two new transition systems have been utilized as base components for parser ensemble (Du et al. 2014) for SemEval 2014 Task 8 (Oepen et al. 2014). Our hybrid system obtained the best overall performance of the closed track of this shared task. In this article, we re-implement all models, calibrate features more carefully, and thus obtain improved accuracy. The idea to extract tree-shaped backbone from a deep dependency graph has also been used to design other types of parsing models in our early work (Du et al. 2014, 2015; Du, Sun, and Wan 2015). Nevertheless, the idea to train a pseudo tree parser to serve a transition-based graph parser is new. The implementation of our parser is available"
J16-3001,S15-2154,1,0.797819,"Missing"
J16-3001,P10-1151,0,0.667255,"ows early encouraging research and studies transition-based approaches to construct deep dependency graphs. The computational challenge to incremental graph spanning is the existence of a large number of crossing arcs in deep Figure 1 An example from CCGBank. The upper curves represent a deep dependency graph and the bottom curves represent a traditional dependency tree. 354 Zhang, Du, Sun, and Wan Transition-Based Parsing for Deep Dependency Structures dependency analysis. To tackle this problem, we integrate insightful ideas, especially the ones illustrated in Nivre (2009) and Gomez-Rodr´ ´ ıguez and Nivre (2010), developed in the tree spanning scenario, and design two new transition systems, both of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been show"
J16-3001,J13-4002,0,0.0360302,"Missing"
J16-3001,W09-1201,0,0.0990053,"Missing"
J16-3001,J13-4006,0,0.155892,"ge is that a deep-grammar-guided parsing model usually cannot produce full coverage and the time complexity of the corresponding parsing algorithms is very high. Previous work on data-driven dependency parsing mainly focused on tree-shaped representations. Nevertheless, recent work has shown that a data-driven approach is also applicable to generate more general linguistic graphs. Sagae and Tsujii (2008) present an initial study on applying transition-based methods to generate HPSG-style predicate–argument structures, and have obtained competitive results. Furthermore, Titov et al. (2009) and Henderson et al. (2013) have shown that more general graphs rather than planars can be produced by augmenting existing transition systems. This work follows early encouraging research and studies transition-based approaches to construct deep dependency graphs. The computational challenge to incremental graph spanning is the existence of a large number of crossing arcs in deep Figure 1 An example from CCGBank. The upper curves represent a deep dependency graph and the bottom curves represent a traditional dependency tree. 354 Zhang, Du, Sun, and Wan Transition-Based Parsing for Deep Dependency Structures dependency a"
J16-3001,J07-3004,0,0.0745068,"y structures, for example, CCG-grounded functor–argument analysis and HPSG-grounded reduced minimal recursion semantics (MRS; Copestake et al. 2005) analysis, we find that syntactic trees can provide very helpful features. In case the syntactic information is not available, we introduce a tree approximation technique to induce tree backbones from deep dependency graphs. Such tree backbones can be utilized to train a tree parser which provides pseudo tree features. To evaluate transition-based models for deep dependency parsing, we conduct experiments on CCG-grounded functor–argument analysis (Hockenmaier and Steedman 2007; Tse and Curran 2010), LFG-grounded grammatical relation analysis (Sun et al. 2014), and HPSG-grounded semantic dependency analysis (Miyao, Ninomiya, and ichi Tsujii 2004; Ivanova et al. 2012) for English and Chinese. Empirical evaluation indicates some non-obvious facts: 1. Data-driven models with appropriate transition systems and disambiguation techniques can produce high-quality deep dependency analysis, comparable to more complex grammar-driven models. 355 Computational Linguistics Volume 42, Number 3 2. Parsers built upon heterogeneous transition systems and decoding orders have complem"
J16-3001,P10-1110,0,0.116738,"Missing"
J16-3001,D10-1002,0,0.200019,"Missing"
J16-3001,W12-3602,0,0.630056,"2000), lexical-functional grammar (LFG; Bresnan and Kaplan 1982) and head-driven phrase structure grammar (HPSG; Pollard and Sag 1994), are able to produce rich linguistic information encoded as bilexical dependencies. Under CCG, this is done by relating the lexical heads of functor categories and their arguments (Clark, Hockenmaier, and Steedman 2002). Under LFG, bilexical grammatical relations can be easily derived as the backbone of F-structures (Sun et al. 2014). Under HPSG, predicate–argument structures (Miyao, Ninomiya, and ichi Tsujii 2004) or reduction of minimal recursion semantics (Ivanova et al. 2012) can be extracted from typed feature structures corresponding to whole sentences. Dependency analysis grounded in deep grammar formalisms is usually beyond tree representations and well-suited for producing meaning representations. Figure 1 is an example from CCGBank. The deep dependency graph conveniently represents more semantically motivated information than the surface tree. For instance, it directly captures the Agent–Predicate relations between word “people” and conjuncts “fight,” “eat,” as well as “drink.” Automatically building deep dependency structures is desirable for many practical"
J16-3001,P10-1001,0,0.071173,"Missing"
J16-3001,P13-1008,0,0.0289887,"Missing"
J16-3001,S14-2082,0,0.178503,"bination; compared to the best individual system, system combination gets an absolute labeled F-score improvement of 1.21 on average. 3. Transition combination significantly improves parsing accuracy on a wide range of conditions, resulting in an absolute labeled F-score improvement of 0.74 on average. 4. Pseudo trees contribute to semantic dependency parsing (SDP) equally well to syntactic trees, and result in an absolute labeled F-score improvement of 1.27 on average. We compare our parser with representative state-of-the-art parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b; Martins and Almeida 2014; Xu, Clark, and Zhang 2014; Du, Sun, and Wan 2015) with respect to different architectures. To evaluate the impact of grammatical knowledge, we compare our parser with parsers guided by treebank-induced HPSG and CCG grammars. Both of our individual and ensembled parsers achieve equivalent accuracy to HPSG and CCG chart parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b), and outperform a shift-reduce CCG parser (Xu, Clark, and Zhang 2014). It is worth noting that our parsers exclude all syntactic and grammatical information. In other words, strictly less information is used. This result dem"
J16-3001,E06-1011,0,0.554556,"Missing"
J16-3001,J11-1007,0,0.0384152,"th of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been shown to be very helpful in boosting accuracy (Sagae and Lavie 2006; Zhang et al. 2009; McDonald and Nivre 2011). In particular, Surdeanu and Manning (2010) presented a nice comparative study on various ensemble models for dependency tree parsing. They found that the diversity of base parsers is more important than complex ensemble models for learning. Motivated by this observation, the authors proposed a hybrid transition-based parser that achieved state-of-the-art performance by combining complementary prediction powers of different transition systems. One advantage of their architecture is the linear-time decoding complexity, given that all base models run in linear-time. Another concern of our work"
J16-3001,P08-1006,0,0.0695221,"nding to whole sentences. Dependency analysis grounded in deep grammar formalisms is usually beyond tree representations and well-suited for producing meaning representations. Figure 1 is an example from CCGBank. The deep dependency graph conveniently represents more semantically motivated information than the surface tree. For instance, it directly captures the Agent–Predicate relations between word “people” and conjuncts “fight,” “eat,” as well as “drink.” Automatically building deep dependency structures is desirable for many practical NLP applications, for example, information extraction (Miyao et al. 2008) and question answering (Reddy, Lapata, and Steedman 2014). Traditionally, deep dependency graphs are generated as a by-product of grammar-guided parsers. The challenge is that a deep-grammar-guided parsing model usually cannot produce full coverage and the time complexity of the corresponding parsing algorithms is very high. Previous work on data-driven dependency parsing mainly focused on tree-shaped representations. Nevertheless, recent work has shown that a data-driven approach is also applicable to generate more general linguistic graphs. Sagae and Tsujii (2008) present an initial study o"
J16-3001,J08-1002,0,0.117885,"rsing quality can be significantly improved by system combination; compared to the best individual system, system combination gets an absolute labeled F-score improvement of 1.21 on average. 3. Transition combination significantly improves parsing accuracy on a wide range of conditions, resulting in an absolute labeled F-score improvement of 0.74 on average. 4. Pseudo trees contribute to semantic dependency parsing (SDP) equally well to syntactic trees, and result in an absolute labeled F-score improvement of 1.27 on average. We compare our parser with representative state-of-the-art parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b; Martins and Almeida 2014; Xu, Clark, and Zhang 2014; Du, Sun, and Wan 2015) with respect to different architectures. To evaluate the impact of grammatical knowledge, we compare our parser with parsers guided by treebank-induced HPSG and CCG grammars. Both of our individual and ensembled parsers achieve equivalent accuracy to HPSG and CCG chart parsers (Miyao and Tsujii 2008; Auli and Lopez 2011b), and outperform a shift-reduce CCG parser (Xu, Clark, and Zhang 2014). It is worth noting that our parsers exclude all syntactic and grammatical information. In other words, st"
J16-3001,J08-4003,0,0.710277,"lation r from head wi to dependent wj . A dependency graph G is thus a set of labeled dependency relations between the root and the words of x. To simplify the description in this section, we mainly consider unlabeled parsing and assume the relation set R is a singleton. Or, taking it another way, we assume A ⊆ V × V. It is straightforward to adapt the discussions in this article for labeled parsing. To do so, we can parameterize transitions with possible dependency relations. For empirical evaluation as discussed in Section 5, we will test both labeled and unlabeled parsing models. Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S = (C, T, cs, Ct ), where 1. C is a set of configurations, each of which contains a buffer β of (remaining) words and a set A of dependency arcs, 2. T is a set of transitions, each of which is a (partial) function t : C → C, 3. cs is an initialization function, mapping a sentence x to a configuration with β = [1, . . . , n], 4. Ct ⊆ C is a set of terminal configurations. Given a sentence x = w1 , . . . , wn and a graph G = (V, A) on it, if there is a sequence of transitions t1 , . . . , tm and a sequence of configurations"
J16-3001,P09-1040,0,0.1007,"nsition systems. This work follows early encouraging research and studies transition-based approaches to construct deep dependency graphs. The computational challenge to incremental graph spanning is the existence of a large number of crossing arcs in deep Figure 1 An example from CCGBank. The upper curves represent a deep dependency graph and the bottom curves represent a traditional dependency tree. 354 Zhang, Du, Sun, and Wan Transition-Based Parsing for Deep Dependency Structures dependency analysis. To tackle this problem, we integrate insightful ideas, especially the ones illustrated in Nivre (2009) and Gomez-Rodr´ ´ ıguez and Nivre (2010), developed in the tree spanning scenario, and design two new transition systems, both of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic"
J16-3001,P08-1108,0,0.0930353,"Missing"
J16-3001,P05-1013,0,0.0991563,"Missing"
J16-3001,S14-2008,0,0.214654,"demonstrates the effectiveness of data-driven approaches to the deep linguistic processing problem. Compared to other types of data-driven parsers, our individual parser achieves equivalent performance to and our hybrid parser obtains slightly better results than factorization parsers based on dual decomposition (Martins and Almeida 2014; Du, Sun, and Wan 2015). This result highlights the effectiveness of the lightweight, transitionbased approach. Parsers based on the two new transition systems have been utilized as base components for parser ensemble (Du et al. 2014) for SemEval 2014 Task 8 (Oepen et al. 2014). Our hybrid system obtained the best overall performance of the closed track of this shared task. In this article, we re-implement all models, calibrate features more carefully, and thus obtain improved accuracy. The idea to extract tree-shaped backbone from a deep dependency graph has also been used to design other types of parsing models in our early work (Du et al. 2014, 2015; Du, Sun, and Wan 2015). Nevertheless, the idea to train a pseudo tree parser to serve a transition-based graph parser is new. The implementation of our parser is available at http://www.icst.pku.edu.cn/ lcwm/grass. 2"
J16-3001,J05-1004,0,0.185242,"Missing"
J16-3001,J08-2005,0,0.11006,"Missing"
J16-3001,Q14-1030,0,0.061084,"Missing"
J16-3001,N06-2033,0,0.036496,"and design two new transition systems, both of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been shown to be very helpful in boosting accuracy (Sagae and Lavie 2006; Zhang et al. 2009; McDonald and Nivre 2011). In particular, Surdeanu and Manning (2010) presented a nice comparative study on various ensemble models for dependency tree parsing. They found that the diversity of base parsers is more important than complex ensemble models for learning. Motivated by this observation, the authors proposed a hybrid transition-based parser that achieved state-of-the-art performance by combining complementary prediction powers of different transition systems. One advantage of their architecture is the linear-time decoding complexity, given that all base models run"
J16-3001,C08-1095,0,0.146021,"xample, information extraction (Miyao et al. 2008) and question answering (Reddy, Lapata, and Steedman 2014). Traditionally, deep dependency graphs are generated as a by-product of grammar-guided parsers. The challenge is that a deep-grammar-guided parsing model usually cannot produce full coverage and the time complexity of the corresponding parsing algorithms is very high. Previous work on data-driven dependency parsing mainly focused on tree-shaped representations. Nevertheless, recent work has shown that a data-driven approach is also applicable to generate more general linguistic graphs. Sagae and Tsujii (2008) present an initial study on applying transition-based methods to generate HPSG-style predicate–argument structures, and have obtained competitive results. Furthermore, Titov et al. (2009) and Henderson et al. (2013) have shown that more general graphs rather than planars can be produced by augmenting existing transition systems. This work follows early encouraging research and studies transition-based approaches to construct deep dependency graphs. The computational challenge to incremental graph spanning is the existence of a large number of crossing arcs in deep Figure 1 An example from CCG"
J16-3001,P14-1042,1,0.903861,". 1. Introduction The derivations licensed by a grammar under deep grammar formalisms, for example, combinatory categorial grammar (CCG; Steedman 2000), lexical-functional grammar (LFG; Bresnan and Kaplan 1982) and head-driven phrase structure grammar (HPSG; Pollard and Sag 1994), are able to produce rich linguistic information encoded as bilexical dependencies. Under CCG, this is done by relating the lexical heads of functor categories and their arguments (Clark, Hockenmaier, and Steedman 2002). Under LFG, bilexical grammatical relations can be easily derived as the backbone of F-structures (Sun et al. 2014). Under HPSG, predicate–argument structures (Miyao, Ninomiya, and ichi Tsujii 2004) or reduction of minimal recursion semantics (Ivanova et al. 2012) can be extracted from typed feature structures corresponding to whole sentences. Dependency analysis grounded in deep grammar formalisms is usually beyond tree representations and well-suited for producing meaning representations. Figure 1 is an example from CCGBank. The deep dependency graph conveniently represents more semantically motivated information than the surface tree. For instance, it directly captures the Agent–Predicate relations betw"
J16-3001,Q13-1025,1,0.876415,"Missing"
J16-3001,W08-2121,0,0.158669,"Missing"
J16-3001,N10-1091,0,0.123101,"phs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been shown to be very helpful in boosting accuracy (Sagae and Lavie 2006; Zhang et al. 2009; McDonald and Nivre 2011). In particular, Surdeanu and Manning (2010) presented a nice comparative study on various ensemble models for dependency tree parsing. They found that the diversity of base parsers is more important than complex ensemble models for learning. Motivated by this observation, the authors proposed a hybrid transition-based parser that achieved state-of-the-art performance by combining complementary prediction powers of different transition systems. One advantage of their architecture is the linear-time decoding complexity, given that all base models run in linear-time. Another concern of our work is about the model diversity obtained by the"
J16-3001,D09-1058,0,0.0641133,"Missing"
J16-3001,P09-1039,0,0.451961,"Missing"
J16-3001,D08-1017,0,0.0819892,"Missing"
J16-3001,C10-1122,0,0.601432,"aging research and studies transition-based approaches to construct deep dependency graphs. The computational challenge to incremental graph spanning is the existence of a large number of crossing arcs in deep Figure 1 An example from CCGBank. The upper curves represent a deep dependency graph and the bottom curves represent a traditional dependency tree. 354 Zhang, Du, Sun, and Wan Transition-Based Parsing for Deep Dependency Structures dependency analysis. To tackle this problem, we integrate insightful ideas, especially the ones illustrated in Nivre (2009) and Gomez-Rodr´ ´ ıguez and Nivre (2010), developed in the tree spanning scenario, and design two new transition systems, both of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been show"
J16-3001,N12-1030,0,0.124466,"Missing"
J16-3001,P15-1032,0,0.0507278,"Missing"
J16-3001,P14-1021,0,0.0275392,"Missing"
J16-3001,W03-3023,0,0.606489,"Missing"
J16-3001,D09-1161,0,0.0305667,"nsition systems, both of which are able to produce arbitrary directed graphs. In particular, we explore two techniques to localize transition actions to maximize the effect of a greedy search procedure. In this way, the corresponding parsers for generating linguistically motivated bilexical graphs can process sentences in close to linear time with respect to the number of input words. This efficiency advantage allows deep linguistic processing for very-large-scale text data. For syntactic parsing, ensembled methods have been shown to be very helpful in boosting accuracy (Sagae and Lavie 2006; Zhang et al. 2009; McDonald and Nivre 2011). In particular, Surdeanu and Manning (2010) presented a nice comparative study on various ensemble models for dependency tree parsing. They found that the diversity of base parsers is more important than complex ensemble models for learning. Motivated by this observation, the authors proposed a hybrid transition-based parser that achieved state-of-the-art performance by combining complementary prediction powers of different transition systems. One advantage of their architecture is the linear-time decoding complexity, given that all base models run in linear-time. An"
J16-3001,D08-1059,0,0.0498215,"ch for approximate decoding. During parsing, the parser keeps track of multiple yet a fixed number of partial outputs to avoid making decisions too early. Training a parser in the discriminative setting corresponds to estimating θ associated with rich features. Previous research on dependency parsing shows that structured perceptron (Collins 2002; Collins and Roark 2004) is one of the strongest learning algorithms. In all experiments, we use the averaged perceptron algorithm with early update to estimate parameters. The whole parser is very similar to the transition-based system introduced in Zhang and Clark (2008, 2011b). 3.2 Transition Combination In either THMM, SS , or S2S , the L EFT /R IGHT-A RC transition does not modify either the stack or the buffer. Only new edges are added to the target graph. When automatic classifiers are utilized to approximate an oracle, a majority of features for predicting an A RC transition will be overlapped with the features for the successive transition. Empirically, this property significantly decreases the parsing accuracy. A key observation of a linguistically motivated bilexical graph is that there is usually at most one edge between any two words, therefore an"
J16-3001,P11-1069,0,0.0751119,"Missing"
J16-3001,J11-1005,0,0.0682425,"Missing"
J16-3001,P11-2033,0,0.0719523,"Missing"
J16-3001,C10-1153,0,0.0379597,"Missing"
J16-3001,P13-1007,0,\N,Missing
J16-3001,N15-1006,0,\N,Missing
J16-3001,oepen-lonning-2006-discriminant,0,\N,Missing
J16-3002,D15-1159,0,0.0224378,"and Harper (2009) mainly focused on the generative HMM models. To enhance a trigram HMM model, Huang, Harper, and Wang (2007) proposed a re-ranking procedure to include both morphology and syntactic structure features, which is difficult to capture for a generative model. Different from the discriminative re-ranking strategy, Huang, Eidelman, and Harper (2009) proposed a latent variable incorporated model to improve a bigram HMM model. Recently, researchers developed several models that integrate tagging into parsing (Hatori et al. 2011; Li et al. 2011; Bohnet and Nivre 2012; Ma et al. 2012; Alberti et al. 2015). The joint decoding architecture on one hand allows tagging to use rich syntactic features to improve accuracy, but on the other hand decreases the decoding efficiency. Different from the joint tagging and parsing approach, our method does not explicitly use syntactic features in the tagging phase. Only a simple sequence labeler with beam search is applied and therefore our tagger is much more efficient. Our work also borrows some ideas from investivations in Chinese word segmentation. Notably, the idea to harvest string knowledges from large-scale raw texts to define new features for disambi"
J16-3002,D12-1133,0,0.0206208,"r, and Wang (2007) and Huang, Eidelman, and Harper (2009) mainly focused on the generative HMM models. To enhance a trigram HMM model, Huang, Harper, and Wang (2007) proposed a re-ranking procedure to include both morphology and syntactic structure features, which is difficult to capture for a generative model. Different from the discriminative re-ranking strategy, Huang, Eidelman, and Harper (2009) proposed a latent variable incorporated model to improve a bigram HMM model. Recently, researchers developed several models that integrate tagging into parsing (Hatori et al. 2011; Li et al. 2011; Bohnet and Nivre 2012; Ma et al. 2012; Alberti et al. 2015). The joint decoding architecture on one hand allows tagging to use rich syntactic features to improve accuracy, but on the other hand decreases the decoding efficiency. Different from the joint tagging and parsing approach, our method does not explicitly use syntactic features in the tagging phase. Only a simple sequence labeler with beam search is applied and therefore our tagger is much more efficient. Our work also borrows some ideas from investivations in Chinese word segmentation. Notably, the idea to harvest string knowledges from large-scale raw te"
J16-3002,J92-4003,0,0.254186,"s partially resolved by exploring the ability of discriminative learning to automatically identify the correspondence between the two types of “word classes.” In the literature, contexts have been defined as subjective and objective relations involving the word, as the documents containing the word, or as search engine snippets for the word as a query. We derive new features for POS tagging by applying two distributional clustering methods, which both take into account surrounding words as contexts. Brown Clustering. Our first choice is the bottom–up agglomerative word clustering algorithm of Brown et al. (1992), which derives a hierarchical clustering of words from unlabeled data. This algorithm generates a hard clustering—each word belongs to exactly one cluster. The input to the algorithm is sequences of words w1 , ..., wn . Initially, the algorithm starts with each word in its own cluster. As long as there are at least two clusters left, the algorithm merges the two clusters that maximize the quality of the resulting clustering. The quality is defined based on a class-based bigram language model as follows. P(wi |w1 , ...wi−1 ) ≈ p(C(wi )|C(wi−1 ))p(wi |C(wi )) (7) where the function C maps a wor"
J16-3002,A00-2018,0,0.339526,"two syntax-free sequential taggers and a state-of-the-art syntax-based parser, aiming at illuminating more precisely the impact of information about phrase-structures as well as dependency structures on POS tagging. The analysis is helpful to understand the role of syntagmatic lexical relations in POS prediction. 4.1 CFG-Based Parsing POS tags can be taken as pre-terminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative probabilistic CGF (PCFG) learning, with lexicalized (Charniak 2000; Collins 2003) or latent annotation (Matsuzaki, Miyao, and Tsujii 2005; Petrov et al. 2006) refinements. Compared with complex lexicalized parsers, the symbolrefined PCFG (SR-PCFG) parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a SR-PCFG parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark 2009). In our work, the Berkeley parser,5 an open source implementation of the SR-PCFG model, is used for experiments. 4.2 Comparing"
J16-3002,D14-1082,0,0.0555118,"s state-of-the-art for both tagging and parsing. The major difference between these three taggers is the corresponding parsing approach: They apply transition-based, graph-based and easy-first methods, respectively. Table 21 presents the results. We can see our re-compiled tagger achieves significantly better results, though it utilizes a simpler technique (i.e., sequence labeling) and does not explicitly use syntactic information. Very recently, neural networks have been widely applied various NLP tasks, including word segmentation (Chen et al. 2015; Ma and Hinrichs 2015), syntactic parsing (Chen and Manning 2014; Weiss et al. 2015), and machine translation (Devlin et al. 2014). We also compare our tagger with a neural network–based tagger. Alberti et al. (2015) introduced a neural network–based joint tagging and parsing model that obtains state-of-the-art results on multiple languages. Table 22 shows the results. Because their experiments used the data from CoNLL 2009 shared task, their results are directly comparable to ours. We can see that our final tagger is significantly better than this currently developed neural network–based system. Table 21 Comparison with other taggers. Tagging accuracies a"
J16-3002,P15-1168,0,0.11677,"95.32 95.34 that such discriminative learning method achieves state-of-the-art for both tagging and parsing. The major difference between these three taggers is the corresponding parsing approach: They apply transition-based, graph-based and easy-first methods, respectively. Table 21 presents the results. We can see our re-compiled tagger achieves significantly better results, though it utilizes a simpler technique (i.e., sequence labeling) and does not explicitly use syntactic information. Very recently, neural networks have been widely applied various NLP tasks, including word segmentation (Chen et al. 2015; Ma and Hinrichs 2015), syntactic parsing (Chen and Manning 2014; Weiss et al. 2015), and machine translation (Devlin et al. 2014). We also compare our tagger with a neural network–based tagger. Alberti et al. (2015) introduced a neural network–based joint tagging and parsing model that obtains state-of-the-art results on multiple languages. Table 22 shows the results. Because their experiments used the data from CoNLL 2009 shared task, their results are directly comparable to ours. We can see that our final tagger is significantly better than this currently developed neural network–based sys"
J16-3002,W02-1001,0,0.845637,"Harper, and Wang 2007; 393 Computational Linguistics Volume 42, Number 3 Huang, Eidelman, and Harper 2009; Li et al. 2011). In this section, we give a brief introduction and a comparative analysis to several models that have been recently designed to resolve the Chinese POS tagging problem. 2.1 State-of-the-Art Tagging Models 2.1.1 Linear-Chain Global Linear Model (LGLM). All state-of-the-art English POS taggers are based on discriminative sequence labeling models—for example, maximum entropy (Toutanova et al. 2003), support vector machines (Gim´enez and M`arquez 2004), structure perceptron (Collins 2002; Shen, Satta, and Joshi 2007; Huang, Fayong, and Guo 2012), and conditional random fields (Sun 2014). A discriminative learner can be easily extended with arbitrary features and is therefore suitable to recognize more new words. Moreover, a majority of the POS tags are locally dependent on each other, so the Markov assumption can well capture the syntactic relations among words. A majority of discriminative POS taggers utilize Global Linear Models (GLMs) for learning and prediction. A GLM represents the sequence labeling task through a feature-vector representation of the whole observation an"
J16-3002,J03-4003,0,0.167651,"e sequential taggers and a state-of-the-art syntax-based parser, aiming at illuminating more precisely the impact of information about phrase-structures as well as dependency structures on POS tagging. The analysis is helpful to understand the role of syntagmatic lexical relations in POS prediction. 4.1 CFG-Based Parsing POS tags can be taken as pre-terminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative probabilistic CGF (PCFG) learning, with lexicalized (Charniak 2000; Collins 2003) or latent annotation (Matsuzaki, Miyao, and Tsujii 2005; Petrov et al. 2006) refinements. Compared with complex lexicalized parsers, the symbolrefined PCFG (SR-PCFG) parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a SR-PCFG parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark 2009). In our work, the Berkeley parser,5 an open source implementation of the SR-PCFG model, is used for experiments. 4.2 Comparing Tagging and Par"
J16-3002,P14-1129,0,0.0378166,"e between these three taggers is the corresponding parsing approach: They apply transition-based, graph-based and easy-first methods, respectively. Table 21 presents the results. We can see our re-compiled tagger achieves significantly better results, though it utilizes a simpler technique (i.e., sequence labeling) and does not explicitly use syntactic information. Very recently, neural networks have been widely applied various NLP tasks, including word segmentation (Chen et al. 2015; Ma and Hinrichs 2015), syntactic parsing (Chen and Manning 2014; Weiss et al. 2015), and machine translation (Devlin et al. 2014). We also compare our tagger with a neural network–based tagger. Alberti et al. (2015) introduced a neural network–based joint tagging and parsing model that obtains state-of-the-art results on multiple languages. Table 22 shows the results. Because their experiments used the data from CoNLL 2009 shared task, their results are directly comparable to ours. We can see that our final tagger is significantly better than this currently developed neural network–based system. Table 21 Comparison with other taggers. Tagging accuracies are all evaluated on CTB, but different training and test data sets"
J16-3002,gimenez-marquez-2004-svmtool,0,0.135284,"Missing"
J16-3002,I11-1136,0,0.0509337,"Missing"
J16-3002,N12-1015,0,0.0605964,"Missing"
J16-3002,P10-1110,0,0.202014,"Missing"
J16-3002,N09-2054,0,0.0530183,"Missing"
J16-3002,D07-1117,0,0.0830749,"Missing"
J16-3002,P08-1068,0,0.0942449,"Missing"
J16-3002,D11-1109,0,0.258362,"and thus does not perform as well as structured models. The local classification algorithm we adopt in this article is linear SVM.1 Because it is a local linear model, we denote it as LLM. 2.2 Evaluation 2.2.1 Setting. Penn Chinese Treebank (CTB) (Xue et al. 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Huang, Harper, and Wang 2007; Huang, Eidelman, and Harper 2009), constituency parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). We use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we conduct experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the principal organizer of the CTB project, and considers many annotation details. This setting is more robust for evaluating Chinese language processing algorithms. Table 1 shows the statistics of our experimental settings. To deeply analyze the POS tagging problem for Chinese, we"
J16-3002,C12-1106,0,0.0177921,"Huang, Eidelman, and Harper (2009) mainly focused on the generative HMM models. To enhance a trigram HMM model, Huang, Harper, and Wang (2007) proposed a re-ranking procedure to include both morphology and syntactic structure features, which is difficult to capture for a generative model. Different from the discriminative re-ranking strategy, Huang, Eidelman, and Harper (2009) proposed a latent variable incorporated model to improve a bigram HMM model. Recently, researchers developed several models that integrate tagging into parsing (Hatori et al. 2011; Li et al. 2011; Bohnet and Nivre 2012; Ma et al. 2012; Alberti et al. 2015). The joint decoding architecture on one hand allows tagging to use rich syntactic features to improve accuracy, but on the other hand decreases the decoding efficiency. Different from the joint tagging and parsing approach, our method does not explicitly use syntactic features in the tagging phase. Only a simple sequence labeler with beam search is applied and therefore our tagger is much more efficient. Our work also borrows some ideas from investivations in Chinese word segmentation. Notably, the idea to harvest string knowledges from large-scale raw texts to define ne"
J16-3002,P15-1167,0,0.0409363,"Missing"
J16-3002,P05-1010,0,0.067805,"Missing"
J16-3002,N04-1043,0,0.155293,"Missing"
J16-3002,W04-3236,0,0.0814478,"Missing"
J16-3002,P08-1108,0,0.0290476,"ely trained in their solution. In other words, one key difference is whether to allow integration of base models at learning time. Second, the application of the decomposition technique is dependent on the solvability of sub-problems. This technique, therefore, is not as flexible as stacking. 4.4.1 Stacked Learning. Stacked generalization is a meta-learning algorithm that was first proposed in Wolpert (1992) and Breiman (1996). Stacked learning has been applied as a system ensemble method in several NLP tasks, such as joint word segmentation and POS tagging (Sun 2011), and dependency parsing (Nivre and McDonald 2008). The idea is to include two “levels” of predictors. The first level includes one or more predictors g1 , ..., gK : Rd → R; each receives input x ∈ Rd and outputs a prediction gk (x). The second level consists of a single function h : Rd+K → R that takes as input hx, g1 (x), ..., gK (x)i and outputs a final prediction yˆ = h(x, g1 (x), ..., gK (x)). The predictor, then, combines an ensemble (the gk ’s) with a meta-predictor (h). Training is done as follows. The training data S = {(xt , yt ) : t ∈ [1, T]} are split into L equal-sized disjoint subsets S1 , ..., SL . Then functions g1 , ..., gL ("
J16-3002,E99-1010,0,0.056434,"...wi−1 ) ≈ p(C(wi )|C(wi−1 ))p(wi |C(wi )) (7) where the function C maps a word w to its class C(w). We use a publicly available package2 (Liang, Collins, and Liang 2005) to train this model. MKCLS Clustering. We also do experiments by using another popular clustering method based on the exchange algorithm (Kneser and Ney 1993). The objective function is Q maximizing the likelihood ni=1 P(wi |w1 , ..., wi−1 ) of the training data given a partially class-based bigram model of the form P(wi |w1 , ...wi−1 ) ≈ p(C(wi )|wi−1 )p(wi |C(wi )) (8) We use the publicly available implementation MKCLS3 (Och 1999) to train this model. One downside of both Brown and MKCLS clustering is that they are based solely on bigram statistics, and do not consider word usage in a wider context. We choose to work with these two algorithms considering their prior success in other NLP applications. However, we expect that our approach can function with other clustering algorithms. 2 http://cs.stanford.edu/~pliang/software/brown-cluster-1.2.zip. 3 http://code.google.com/p/giza-pp/. 400 Sun and Wan Towards Accurate and Efficient Chinese POS Tagging 3.1.2 Data. Chinese Gigaword is a comprehensive archive of newswire tex"
J16-3002,P06-1055,0,0.0968083,"at illuminating more precisely the impact of information about phrase-structures as well as dependency structures on POS tagging. The analysis is helpful to understand the role of syntagmatic lexical relations in POS prediction. 4.1 CFG-Based Parsing POS tags can be taken as pre-terminals of a constituency parse tree, so a constituency parser can also provide POS information. The majority of the state-of-the-art constituent parsers are based on generative probabilistic CGF (PCFG) learning, with lexicalized (Charniak 2000; Collins 2003) or latent annotation (Matsuzaki, Miyao, and Tsujii 2005; Petrov et al. 2006) refinements. Compared with complex lexicalized parsers, the symbolrefined PCFG (SR-PCFG) parsers leverage on an automatic procedure to learn refined grammars and are more robust to parse many non-English languages that are not well studied. For Chinese, a SR-PCFG parser achieves the state-of-the-art performance and outperforms many other types of parsers (Zhang and Clark 2009). In our work, the Berkeley parser,5 an open source implementation of the SR-PCFG model, is used for experiments. 4.2 Comparing Tagging and Parsing From a linguistic view, we can distinguish syntax-free and syntax-based"
J16-3002,D10-1069,0,0.0508594,"Missing"
J16-3002,N07-1051,0,0.0225375,"as “Mr./NR-2 Smith/NR-1 saw/VV-2 Ms./NR-2 Smith/NR-1.” The objective of training a symbol-refined bigram tagger is to solve the LA-involved emission and transition parameters by maximizing the likelihood of the training data. In contrast with a non-symbol-refined HMM tagger, where the POS tags are observed, the latent annotations are unseen variables. In order to learn these parameters, a variant of EM algorithm is used. The objective function used for decoding is: yˆ[1:n] = arg max y[1:n] nY −1 P(yi , yi+1 |x[1:n] ) (5) i=1 This goal function is a variant of the MAX-RULE-PRODUCT algorithm in Petrov and Klein (2007), which maximizes the product of rule posteriors. This algorithm is not probabilistically correct but follows the instinct of choosing the tree with the greatest chance of having all rules correct. Similarly, the goal function used in the SR-HMM POS tagger tries to find the tag sequence with the greatest chance of having all bigrams correct. The bigram tag posterior is calculated by marginalizing out the latent annotations in the bigram latent tag posterior. P(yi = t, yi+1 = s|x[1:n] ) = X X P(ta , sb |x[1:n] ) (6) ta ∈S(t) sb ∈S(s) 395 Computational Linguistics Volume 42, Number 3 Table 1 Tra"
J16-3002,D10-1001,0,0.0296712,"ing when applying the C&C parser (Tse and Curran 2012). We think the failure is mainly due to overplaying sequence models in both POS tagging and supertagging. 4.4 Enhancing Tagging via Stacking We study a simple way of integrating multiple heterogeneous models in order to exploit their complementary strengths and thereby improve tagging accuracy beyond what is possible by either model in isolation. The method integrates the heterogeneous models by allowing the outputs of SR-HMM and the parser to define features for the LLM/LGLM. Similar to our work on combining a sequence model and a parser, Rush et al. (2010) proposed a principled decoding technique based on dual decomposition to take advantages of heterogeneous models. There are two differences between their model and ours. First, the base models for combining are separately trained in their solution. In other words, one key difference is whether to allow integration of base models at learning time. Second, the application of the decomposition technique is dependent on the solvability of sub-problems. This technique, therefore, is not as flexible as stacking. 4.4.1 Stacked Learning. Stacked generalization is a meta-learning algorithm that was fir"
J16-3002,P07-1096,0,0.0928353,"Missing"
J16-3002,P12-1046,0,0.0220779,"ve method. However, they did not deeply analyze the problem from a linguistic view. The global linear algorithm we adopt in this article is averaged perceptron (Collins 2002). 394 Sun and Wan Towards Accurate and Efficient Chinese POS Tagging 2.1.2 Symbol-Refined Hidden Markov Model (SR-HMM). Generative models with latent annotations (LAs) obtain state-of-the-art performance for a number of NLP tasks. For example, both context-free Grammar (CFG) and tree-substitution grammar (TSG) with refined latent variables achieve excellent results for syntactic parsing (Matsuzaki, Miyao, and Tsujii 2005; Shindo et al. 2012). For Chinese POS tagging, Huang, Eidelman, and Harper (2009) described and evaluated a bigram HMM tagger that utilizes latent annotations. The use of latent annotations substantially improves the performance of a simple generative bigram tagger, outperforming a trigram HMM tagger with sophisticated smoothing. An HMM POS tagger models the joint distribution of the observation sequence x[1:n] and the tag sequence y[1:n] . Under the first-order Markov assumption, the inference problem can be computed as: yˆ[1:n] = arg max P(x[1:n] , y[1:n] ) y[1:n] = arg max y[1:n] n Y (4) P(yi |yi−1 )P(xi |yi )"
J16-3002,C10-2139,1,0.832536,"news text, that is, Xinhua newswire. These data cover all news published by Xinhua News Agency (the largest news agency in China) from 1991 to 2004, which contains over 473 million characters. 3.1.3 Pre-processing: Word Segmentation. Different from English and other Western languages, Chinese is written without explicit word delimiters such as space characters. To find the basic language units (i.e., words), segmentation is a necessary pre-processing step for word clustering. Our previous research showed that character-based segmentation models trained on labeled data are reasonably accurate (Sun 2010). In this work, we use a supervised segmenter introduced in Sun and Xu (2011) to process raw texts. 3.2 Improving Tagging with Cluster Features Our discriminative sequential tagger is easy to be extended with arbitrary features and therefore suitable to explore additional features derived from other sources. We propose using word clusters as substitutes for word forms to assist the POS tagger. We are relying on the ability of the discriminative learning method to explore informative features, which play central role to boost the tagging performance. Five clustering-based features are added: r"
J16-3002,P11-1139,1,0.9335,"] ) = n X φ(hi , yi ) (2) i=1 where hi is the history correlated with yi . Using this local representation, we can use the Viterbi algorithm for inference, which finds the optimal tag sequence yˆ[1:n] that maximizes the following score: yˆ[1:n] = arg max y[1:n] n X wφ(hi , yi ) (3) i=1 Discriminative learning is also an appropriate solution for Chinese POS tagging, because of its flexibility to include knowledge from multiple linguistic sources. Tseng, Jurafsky, and Manning (2005) introduced a maximum entropy–based model, which includes morphological features for unknown word recognition, and Sun (2011) studied the joint word segmentation and POS tagging problem and developed a fully discriminative method. However, they did not deeply analyze the problem from a linguistic view. The global linear algorithm we adopt in this article is averaged perceptron (Collins 2002). 394 Sun and Wan Towards Accurate and Efficient Chinese POS Tagging 2.1.2 Symbol-Refined Hidden Markov Model (SR-HMM). Generative models with latent annotations (LAs) obtain state-of-the-art performance for a number of NLP tasks. For example, both context-free Grammar (CFG) and tree-substitution grammar (TSG) with refined latent"
J16-3002,I13-1021,1,0.857765,"Missing"
J16-3002,C08-1105,1,0.860064,"Missing"
J16-3002,D11-1090,1,0.958956,"rious features can be drawn upon information sources such as word forms and characters that constitute words. Previous study on many languages shows that local classification is inadequate to capture structural information of output labels, and thus does not perform as well as structured models. The local classification algorithm we adopt in this article is linear SVM.1 Because it is a local linear model, we denote it as LLM. 2.2 Evaluation 2.2.1 Setting. Penn Chinese Treebank (CTB) (Xue et al. 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Huang, Harper, and Wang 2007; Huang, Eidelman, and Harper 2009), constituency parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). We use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we conduct experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the principal organizer of the CTB project,"
J16-3002,D08-1017,0,0.0796353,"Missing"
J16-3002,N03-1033,0,0.102768,"Missing"
J16-3002,N12-1030,0,0.291408,"gories. The numbers presented in the bottom block of Table 12 give a rough illustration. The results are obtained by providing the parser mixed POS tagging analysis: The tags of “的/得” predicted by the Berkeley parser and the tags of other words are utilized. We can see that though the overall parsing quality is still worse than Berkeley parser, it is better than sequence models. The performance change demonstrates the importance of prediction of these two particular words. Our linguistic analysis can also better explain the poor performance of Chinese CCG parsing when applying the C&C parser (Tse and Curran 2012). We think the failure is mainly due to overplaying sequence models in both POS tagging and supertagging. 4.4 Enhancing Tagging via Stacking We study a simple way of integrating multiple heterogeneous models in order to exploit their complementary strengths and thereby improve tagging accuracy beyond what is possible by either model in isolation. The method integrates the heterogeneous models by allowing the outputs of SR-HMM and the parser to define features for the LLM/LGLM. Similar to our work on combining a sequence model and a parser, Rush et al. (2010) proposed a principled decoding tech"
J16-3002,I05-3005,0,0.0447556,"s designed for English have been applied to many other languages as well. In some cases, the methods work well without large modifications, such as for German POS tagging. But a number of augmentations and changes became necessary when dealing with highly inflected or agglutinative languages, as well as analytic languages, of which Chinese is the focus of this article. Both discriminative and generative models are explored for accurate Chinese POS tagging (Ng and Low 2004; Tseng, Jurafsky, and Manning 2005; Huang, Harper, and Wang 2007; Huang, Eidelman, and Harper 2009). Ng and Low (2004) and Tseng et al. (2005) introduced a maximum entropy–based model, which includes morphological features for unknown word recognition. Huang, Harper, and Wang (2007) and Huang, Eidelman, and Harper (2009) mainly focused on the generative HMM models. To enhance a trigram HMM model, Huang, Harper, and Wang (2007) proposed a re-ranking procedure to include both morphology and syntactic structure features, which is difficult to capture for a generative model. Different from the discriminative re-ranking strategy, Huang, Eidelman, and Harper (2009) proposed a latent variable incorporated model to improve a bigram HMM mode"
J16-3002,P06-1054,0,0.324867,"Missing"
J16-3002,P15-1032,0,0.0289012,"both tagging and parsing. The major difference between these three taggers is the corresponding parsing approach: They apply transition-based, graph-based and easy-first methods, respectively. Table 21 presents the results. We can see our re-compiled tagger achieves significantly better results, though it utilizes a simpler technique (i.e., sequence labeling) and does not explicitly use syntactic information. Very recently, neural networks have been widely applied various NLP tasks, including word segmentation (Chen et al. 2015; Ma and Hinrichs 2015), syntactic parsing (Chen and Manning 2014; Weiss et al. 2015), and machine translation (Devlin et al. 2014). We also compare our tagger with a neural network–based tagger. Alberti et al. (2015) introduced a neural network–based joint tagging and parsing model that obtains state-of-the-art results on multiple languages. Table 22 shows the results. Because their experiments used the data from CoNLL 2009 shared task, their results are directly comparable to ours. We can see that our final tagger is significantly better than this currently developed neural network–based system. Table 21 Comparison with other taggers. Tagging accuracies are all evaluated on"
J16-3002,D08-1059,0,0.086811,"re structural information of output labels, and thus does not perform as well as structured models. The local classification algorithm we adopt in this article is linear SVM.1 Because it is a local linear model, we denote it as LLM. 2.2 Evaluation 2.2.1 Setting. Penn Chinese Treebank (CTB) (Xue et al. 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Huang, Harper, and Wang 2007; Huang, Eidelman, and Harper 2009), constituency parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). We use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we conduct experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the principal organizer of the CTB project, and considers many annotation details. This setting is more robust for evaluating Chinese language processing algorithms. Table 1 shows the statistics of our experimental settings. To deeply analyze t"
J16-3002,W09-3825,0,0.500508,"hat local classification is inadequate to capture structural information of output labels, and thus does not perform as well as structured models. The local classification algorithm we adopt in this article is linear SVM.1 Because it is a local linear model, we denote it as LLM. 2.2 Evaluation 2.2.1 Setting. Penn Chinese Treebank (CTB) (Xue et al. 2005) is a popular data set to evaluate a number of Chinese NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Huang, Harper, and Wang 2007; Huang, Eidelman, and Harper 2009), constituency parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). We use CTB 6.0 as the labeled data for the study. The corpus was collected during different time periods from different sources with a diversity of topics. In order to obtain a representative split of data sets, we conduct experiments following the setting of the CoNLL 2009 shared task. The setting is provided by the principal organizer of the CTB project, and considers many annotation details. This setting is more robust for evaluating Chinese language processing algorithms. Table 1 shows the statistics of"
J16-3002,J11-1005,0,0.0692174,". Such pseudo training data could be of very largescale theoretically and practically. Together with gold standard training data, large-scale pseudo training data can be used to train SR-HMMs and LGLMs. We expect that the SR-HMM tagger can be improved by exploring better latent variables, and that the discriminative taggers can be improved by using features in a larger context. 5.3 Beam Decoding for LGLM For a number of NLP tasks, including tagging and parsing, the generic beam-search algorithmic technique has been shown to be very powerful to build efficient systems with comparable accuracy (Zhang and Clark 2011). In our model re-compilation case, to train a second order LGLM on a very large data set is quite time-consuming. Rather than the Viterbi algorithm, we here use the beam search algorithm for decoding. Beyond simple beam decoding that essentially implements the greedy search strategy, Huang and Sagae (2010) discuss how the state-merging strategy that is used by dynamic programming methods can be applied to enhance a beam decoder. Considering that the total number of possible tags is much larger than conventional tagging, we implement a beam-search algorithm with state merging for our discrimin"
J16-3002,D13-1061,0,0.057421,"Missing"
J16-3002,P12-1026,1,\N,Missing
J16-3002,I05-3027,0,\N,Missing
J18-4005,C96-2183,0,0.631347,"Missing"
J19-1003,P16-1231,0,0.143503,"based (Yamada and Matsumoto 2003; Nivre 2008) and graph-based (McDonald et al. 2005; McDonald, Crammer, and Pereira 2005) models have attracted the most attention in dependency parsing in recent works. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by Yamada and Matsumoto (2003) and Nivre, Hall, and Nilsson (2004). Specifically, deep learning techniques have been successfully applied to enhance the parsing accuracy (Chen and Manning 2014; Weiss et al. 2015; Andor et al. 2016; Kiperwasser and Goldberg 2016; Dozat and Manning 2016). Chen and Manning (2014) made the first successful attempt at incorporating deep learning into a transition-based dependency parser. A number of other researchers have attempted to address some limitations of their parser by augmenting it with additional complexity: Later it was enhanced with a more principled decoding algorithm, namely beam search, as well as a conditional random field loss objective function (Weiss et al. 2015; Watanabe and Sumita 2015; Zhou et al. 2015; Andor et al. 2016). A graph-based system explicitly parameterizes"
J19-1003,W13-2322,0,0.014542,"stic graphs beyond trees, we propose a new approach, namely graph merging, by constructing GR graphs from subgraphs. There are two key issues involved in the approach, that is, graph decomposition and merging. To solve these two problems in a principled way, we treat both problems as optimization problems and employ combinatorial optimization techniques. Experiments demonstrate the effectiveness of the graph merging framework, which can be adopted to other types of flexible representations, for example, semantic dependency graphs (Oepen et al. 2014, 2015) and abstract meaning representations (Banarescu et al. 2013). The study is significant in demonstrating a linguistically motivated and computationally effective way of parsing Chinese texts. Appendix In annotations of dependency structures, typical grammatical relations are subject, object, etc., which imply the role the dependent plays with regard to its head. In addition to phrasal categories, CTB also has functional labels to represent relations. The CTB 131 Computational Linguistics Volume 45, Number 1 Table 16 Illustration of major dependency relations in our corpus. ADV AMOD AUX COMP DET LOC NMOD OBJ PRT QUANT RELATIVE SUBJ TMP adverbial adjunct"
J19-1003,D11-1037,0,0.156954,"an essential role in boosting the parsing performance. It is generally believed that the representation format for parser outputs may greatly affect its impact on applications. Predicate–argument structures extracted from deep parses have been shown very helpful for NLP tasks such as information extraction (Miyao et al. 2008; Yakushiji et al. 2005). Partly due to their importance in information processing, deep dependencies are the de facto standard to evaluate deep parsers (Kaplan et al. 99 Computational Linguistics Volume 45, Number 1 2004; Briscoe and Carroll 2006; Clark and Curran 2007a; Bender et al. 2011), including C&C2 and Enju.3 If the interpretable predicate–argument structures are so valuable, then the question arises: Why cannot we directly build deep dependency graphs? 2.2 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, for example, transition-based (Yamada and Matsumoto 2003; Nivre 2008) and graph-based (McDonald et al. 2005; McDonald, Crammer, and Pereira 2005) models have attracted the most attention in dependency parsing in recent works. Transition-based parsers utilize tr"
J19-1003,C10-1011,0,0.0122984,"ier u in our algorithm can be regarded as additional weights for the score function. The update of u is to increase weights to the edges that are not covered by any tree-like subgraph, so it is more likely for them to be selected in the next iteration. 4.3 Graph Merging As explained above, the extraction algorithm gives three classes of trees for each graph. The algorithm is applied to the graph training set to deliver three training tree sets. After that, three parsing models can be trained with the three tree sets. The parsers used in this study to train models and parse trees include Mate (Bohnet 2010), a second-order graph-based dependency parser, and our implementation of the first-order factorization model proposed in Kiperwasser and Goldberg (2016). If the scores used by the three models are f1 , f2 , f3 , respectively, then the parsers can find trees with the highest scores for a sentence. That solves the following optimization problems: arg maxg 1 f1 (gg1 ), arg maxg 2 f2 (gg2 ), and arg maxg 2 f3 (gg3 ). We can 115 Computational Linguistics Volume 45, Number 1 parse a given sentence with the three models, obtain three trees, and then transform them into subgraphs. We combine them tog"
J19-1003,P06-2006,0,0.0749123,"Missing"
J19-1003,D14-1082,0,0.0659172,"hich are exemplified in traditional grammars by the notions of subject, direct/indirect object, etc., and therefore encode rich syntactic information of natural language sentences in an explicit way. In recent years, parsing a sentence to a dependency representation has been well studied and widely applied to many Natural Language Processing (NLP) tasks, for example, Information Extraction and Machine Translation. In particular, the ¨ data-driven approaches have made great progress during the past two decades (Kubler, McDonald, and Nivre 2009; Koo et al. 2010; Pitler, Kannan, and Marcus 2013; Chen and Manning 2014; Weiss et al. 2015; Dozat and Manning 2016). Various practical parsing systems have been built, not only for English but also for a large number of typologically different languages, for example, Arabic, Basque, Catalan, Chinese, Czech, Greek, Hungarian, Italian, and Turkish (Nivre et al. 2007). Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed bilexical dependency trees. Although tree-shaped graphs have several desirable properties from the computational perspective, they do not work well for coordinations, long-range dependencies i"
J19-1003,P07-1032,0,0.423293,"syntactic and semantic dependencies in more explicit details. Deep parses arguably contain the most linguistically satisfactory account of the deep dependencies inherent in many complicated linguistic phenomena, for example, coordination and extraction. Among the grammatical models, LFG and HPSG encode grammatical functions directly, and they are adequate for generating predicate–argument analyses (King et al. 2003; Flickinger, Zhang, and Kordoni 2012). In terms of CCG, Hockenmaier and Steedman (2007) introduced a co-indexing method to extract (predicate, argument) pairs from CCG derivations; Clark and Curran (2007a) also utilized a similar method to derive LFG-style grammatical relations. These bilexical dependencies can be used to approximate the corresponding semantic structures, such as logic forms in Minimal Recursion Semantics (Copestake et al. 2005). In recent years, considerable progress has been made in deep linguistic processing for realistic texts. Traditionally, deep linguistic processing has been concerned with grammar development for parsing and generation. During the last decade, techniques for treebank-oriented grammar development (Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2005; Ho"
J19-1003,J07-4004,0,0.602185,"syntactic and semantic dependencies in more explicit details. Deep parses arguably contain the most linguistically satisfactory account of the deep dependencies inherent in many complicated linguistic phenomena, for example, coordination and extraction. Among the grammatical models, LFG and HPSG encode grammatical functions directly, and they are adequate for generating predicate–argument analyses (King et al. 2003; Flickinger, Zhang, and Kordoni 2012). In terms of CCG, Hockenmaier and Steedman (2007) introduced a co-indexing method to extract (predicate, argument) pairs from CCG derivations; Clark and Curran (2007a) also utilized a similar method to derive LFG-style grammatical relations. These bilexical dependencies can be used to approximate the corresponding semantic structures, such as logic forms in Minimal Recursion Semantics (Copestake et al. 2005). In recent years, considerable progress has been made in deep linguistic processing for realistic texts. Traditionally, deep linguistic processing has been concerned with grammar development for parsing and generation. During the last decade, techniques for treebank-oriented grammar development (Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2005; Ho"
J19-1003,C96-1058,0,0.897898,"raph as well as cover all edges in y . The optimization problem can be written as max. s1 (gg1 ) + s2 (gg2 ) + s3 (gg3 ) s.t. g 1 , g 2 , g 3 are tree-like g 1 (i, j) + g 2 (i, j) + g 3 (i, j) ≥ y (i, j), ∀i, j Scoring a Subgraph. We score a subgraph in a first order arc-factored way, which first scores theP edges separately and then adds up the scores. Formally, the score function is sk (gg ) = ωk (i, j)ggk (i, j) (k = 1, 2, 3) where ωk (i, j) is the score of the edge from i to j. Under this score function, we can use the Maximum Spanning Tree (MST) algorithm (Chu and Liu 1965; Edmonds 1967; Eisner 1996) to decode the tree-like subgraph with the highest score. After the score function is defined, extracting a subgraph from a GR graph works in the following way: We first assign heuristic weights ωk (i, j) (1 ≤ i, j ≤ n) to the potential edges between all the pairs of words, then compute a best projective tree g k using the Eisner’s Algorithm (Eisner 1996): g k = arg max sk (gg ) = arg max g g X ωk (i, j)gg (i, j). g k is not exactly a subgraph of y , because there may be some edges in the tree but not in the graph. To guarantee a meaningful subgraph of the original graph, we add labels to the"
J19-1003,C12-1059,0,0.183579,"nodes to the secondary memory module, namely λ0 . Another key property of the oracle is building arcs as soon as possible to avoid further complication. 5.3 Bi-LSTM Based Scorer The neural model, which acts as a classifier of actions in this transition system, is similar to previous neural models. The Bi-LSTMs play the same role, but the feature vectors of the front of the buffer and the top of list λ are used to assign scores for actions. The structure is shown in Figure 16. W 1 · (rrlistλtop ⊕ r bufferfront ) + b 1 ) + b 2 Scores = W 2 · ReLU (W Two search methods, that is, dynamic oracle (Goldberg and Nivre 2012) and beam search, can be used with this transition system. 122 Sun et al. Parsing Chinese Sentences with Grammatical Relations Scores of actions MLP Bi-LSTMs ... Bi-LSTMs Bi-LSTMs ... Embedding ... Embedding Embedding ... list top 浦东 NR buffer front 实行 VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate"
J19-1003,J13-4006,0,0.0728661,"Missing"
J19-1003,J07-3004,0,0.26688,"and text mining (Miyao et al. 2008). In general, derivations based on deep grammar formalisms may provide a wider variety of syntactic and semantic dependencies in more explicit details. Deep parses arguably contain the most linguistically satisfactory account of the deep dependencies inherent in many complicated linguistic phenomena, for example, coordination and extraction. Among the grammatical models, LFG and HPSG encode grammatical functions directly, and they are adequate for generating predicate–argument analyses (King et al. 2003; Flickinger, Zhang, and Kordoni 2012). In terms of CCG, Hockenmaier and Steedman (2007) introduced a co-indexing method to extract (predicate, argument) pairs from CCG derivations; Clark and Curran (2007a) also utilized a similar method to derive LFG-style grammatical relations. These bilexical dependencies can be used to approximate the corresponding semantic structures, such as logic forms in Minimal Recursion Semantics (Copestake et al. 2005). In recent years, considerable progress has been made in deep linguistic processing for realistic texts. Traditionally, deep linguistic processing has been concerned with grammar development for parsing and generation. During the last de"
J19-1003,P10-1110,0,0.0606884,"Missing"
J19-1003,N04-1013,0,0.147685,"Missing"
J19-1003,W03-2401,0,0.435763,"machine translation (Oepen et al. 2007; Wu, Matsuzaki, and Tsujii 2010) and text mining (Miyao et al. 2008). In general, derivations based on deep grammar formalisms may provide a wider variety of syntactic and semantic dependencies in more explicit details. Deep parses arguably contain the most linguistically satisfactory account of the deep dependencies inherent in many complicated linguistic phenomena, for example, coordination and extraction. Among the grammatical models, LFG and HPSG encode grammatical functions directly, and they are adequate for generating predicate–argument analyses (King et al. 2003; Flickinger, Zhang, and Kordoni 2012). In terms of CCG, Hockenmaier and Steedman (2007) introduced a co-indexing method to extract (predicate, argument) pairs from CCG derivations; Clark and Curran (2007a) also utilized a similar method to derive LFG-style grammatical relations. These bilexical dependencies can be used to approximate the corresponding semantic structures, such as logic forms in Minimal Recursion Semantics (Copestake et al. 2005). In recent years, considerable progress has been made in deep linguistic processing for realistic texts. Traditionally, deep linguistic processing ha"
J19-1003,Q16-1023,0,0.109372,"es that are not covered by any tree-like subgraph, so it is more likely for them to be selected in the next iteration. 4.3 Graph Merging As explained above, the extraction algorithm gives three classes of trees for each graph. The algorithm is applied to the graph training set to deliver three training tree sets. After that, three parsing models can be trained with the three tree sets. The parsers used in this study to train models and parse trees include Mate (Bohnet 2010), a second-order graph-based dependency parser, and our implementation of the first-order factorization model proposed in Kiperwasser and Goldberg (2016). If the scores used by the three models are f1 , f2 , f3 , respectively, then the parsers can find trees with the highest scores for a sentence. That solves the following optimization problems: arg maxg 1 f1 (gg1 ), arg maxg 2 f2 (gg2 ), and arg maxg 2 f3 (gg3 ). We can 115 Computational Linguistics Volume 45, Number 1 parse a given sentence with the three models, obtain three trees, and then transform them into subgraphs. We combine them together to obtain the graph parse of the sentence by putting all the edges in the three subgraphs together. That is to say, graph y = max{t2g(gg1 ), t2g(gg"
J19-1003,P10-1001,0,0.0323173,"Missing"
J19-1003,D10-1125,0,0.458865,"s represent various grammatical relations (GRs), which are exemplified in traditional grammars by the notions of subject, direct/indirect object, etc., and therefore encode rich syntactic information of natural language sentences in an explicit way. In recent years, parsing a sentence to a dependency representation has been well studied and widely applied to many Natural Language Processing (NLP) tasks, for example, Information Extraction and Machine Translation. In particular, the ¨ data-driven approaches have made great progress during the past two decades (Kubler, McDonald, and Nivre 2009; Koo et al. 2010; Pitler, Kannan, and Marcus 2013; Chen and Manning 2014; Weiss et al. 2015; Dozat and Manning 2016). Various practical parsing systems have been built, not only for English but also for a large number of typologically different languages, for example, Arabic, Basque, Catalan, Chinese, Czech, Greek, Hungarian, Italian, and Turkish (Nivre et al. 2007). Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed bilexical dependency trees. Although tree-shaped graphs have several desirable properties from the computational perspective, they do no"
J19-1003,D11-1109,0,0.0273462,"VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Sun and Uszkoreit 2012), constituent parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). This corpus was collected during different time periods from different sources with a diverse range of topics. We used CTB 6.0 and defined the training, development, and test sets according to the CoNLL 2009 shared task. Table 3 gives a summary of the data sets for experiments. Evaluation on this benchmark data allows us to directly compare our parsers and other parsers in the literature, according to numeric performance. The measure for comparing two dependency graphs is precision/recall of bilexical dependencies, which are defined as hwh , wd , li tuples, where wh is the head, wd is the de"
J19-1003,P05-1012,0,0.801363,"standard to evaluate deep parsers (Kaplan et al. 99 Computational Linguistics Volume 45, Number 1 2004; Briscoe and Carroll 2006; Clark and Curran 2007a; Bender et al. 2011), including C&C2 and Enju.3 If the interpretable predicate–argument structures are so valuable, then the question arises: Why cannot we directly build deep dependency graphs? 2.2 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, for example, transition-based (Yamada and Matsumoto 2003; Nivre 2008) and graph-based (McDonald et al. 2005; McDonald, Crammer, and Pereira 2005) models have attracted the most attention in dependency parsing in recent works. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by Yamada and Matsumoto (2003) and Nivre, Hall, and Nilsson (2004). Specifically, deep learning techniques have been successfully applied to enhance the parsing accuracy (Chen and Manning 2014; Weiss et al. 2015; Andor et al. 2016; Kiperwasser and Goldberg 2016; Dozat and Manning 2016). Chen and"
J19-1003,E06-1011,0,0.413161,"Missing"
J19-1003,H05-1066,0,0.706592,"standard to evaluate deep parsers (Kaplan et al. 99 Computational Linguistics Volume 45, Number 1 2004; Briscoe and Carroll 2006; Clark and Curran 2007a; Bender et al. 2011), including C&C2 and Enju.3 If the interpretable predicate–argument structures are so valuable, then the question arises: Why cannot we directly build deep dependency graphs? 2.2 Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, for example, transition-based (Yamada and Matsumoto 2003; Nivre 2008) and graph-based (McDonald et al. 2005; McDonald, Crammer, and Pereira 2005) models have attracted the most attention in dependency parsing in recent works. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by Yamada and Matsumoto (2003) and Nivre, Hall, and Nilsson (2004). Specifically, deep learning techniques have been successfully applied to enhance the parsing accuracy (Chen and Manning 2014; Weiss et al. 2015; Andor et al. 2016; Kiperwasser and Goldberg 2016; Dozat and Manning 2016). Chen and"
J19-1003,P08-1006,0,0.219536,"Structure Grammar (HPSG) (Pollard and Sag [1994]), and Tree-adjoining Grammar (TAG) (Joshi and Schabes [1997]). These theories provide not only the description of the syntactic structures, but also the ways in which meanings are composed, and thus parsing in these formalisms provides an elegant way to simultaneously obtain both syntactic and semantic analyses, generating valuable and richer linguistic information. Deep linguistic annotators and processors are strongly demanded in NLP applications, such as machine translation (Oepen et al. 2007; Wu, Matsuzaki, and Tsujii 2010) and text mining (Miyao et al. 2008). In general, derivations based on deep grammar formalisms may provide a wider variety of syntactic and semantic dependencies in more explicit details. Deep parses arguably contain the most linguistically satisfactory account of the deep dependencies inherent in many complicated linguistic phenomena, for example, coordination and extraction. Among the grammatical models, LFG and HPSG encode grammatical functions directly, and they are adequate for generating predicate–argument analyses (King et al. 2003; Flickinger, Zhang, and Kordoni 2012). In terms of CCG, Hockenmaier and Steedman (2007) int"
J19-1003,J08-1002,0,0.539472,"bilexical dependencies can be used to approximate the corresponding semantic structures, such as logic forms in Minimal Recursion Semantics (Copestake et al. 2005). In recent years, considerable progress has been made in deep linguistic processing for realistic texts. Traditionally, deep linguistic processing has been concerned with grammar development for parsing and generation. During the last decade, techniques for treebank-oriented grammar development (Cahill et al. 2002; Miyao, Ninomiya, and Tsujii 2005; Hockenmaier and Steedman 2007) and statistical deep parsing (Clark and Curran 2007b; Miyao and Tsujii 2008) have been well studied for English. Statistical models that are estimated on large-scale treebanks play an essential role in boosting the parsing performance. It is generally believed that the representation format for parser outputs may greatly affect its impact on applications. Predicate–argument structures extracted from deep parses have been shown very helpful for NLP tasks such as information extraction (Miyao et al. 2008; Yakushiji et al. 2005). Partly due to their importance in information processing, deep dependencies are the de facto standard to evaluate deep parsers (Kaplan et al. 9"
J19-1003,J08-4003,0,0.620576,"Volume 45, Number 1 labeled directed graph in the standard graph-theoretic sense and consists of nodes, V, and arcs, A, such that for sentence x = w0 w1 . . . wn and label set R the following holds: • V = {0, 1, . . . , n}, • A ⊆ V × V × R. The vertex −1 denotes a virtual root. The arc set A represents the labeled dependency relations of the particular analysis G. Specifically, an arc (wi , wj , r) ∈ A represents a dependency relation from head wi to dependent wj labeled with relation type r. A dependency graph G is thus a set of labeled dependency relations between the words of x. Following Nivre (2008), we define a transition system for dependency parsing as a quadruple S = (C, T, cs , Ct ), where 1. C is a set of configurations, each of which contains a buffer β of (remaining) words and a set A of arcs, 2. T is a set of transitions, each of which is a (partial) function t : C 7→ C, 3. cs is an initialization function, mapping a sentence x to a configuration, with β = [0, . . . , n], and 4. Ct ⊆ C is a set of terminal configurations. Given a sentence x = w1 , . . . , wn and a graph G = (V, A) on it, if there is a sequence of transitions t1 , . . . , tm and a sequence of configurations c0 ,"
J19-1003,W04-2407,0,0.247099,"Missing"
J19-1003,S15-2153,0,0.050761,"Missing"
J19-1003,S14-2008,0,0.0140807,"tituency and dependency formalisms. To construct complex linguistic graphs beyond trees, we propose a new approach, namely graph merging, by constructing GR graphs from subgraphs. There are two key issues involved in the approach, that is, graph decomposition and merging. To solve these two problems in a principled way, we treat both problems as optimization problems and employ combinatorial optimization techniques. Experiments demonstrate the effectiveness of the graph merging framework, which can be adopted to other types of flexible representations, for example, semantic dependency graphs (Oepen et al. 2014, 2015) and abstract meaning representations (Banarescu et al. 2013). The study is significant in demonstrating a linguistically motivated and computationally effective way of parsing Chinese texts. Appendix In annotations of dependency structures, typical grammatical relations are subject, object, etc., which imply the role the dependent plays with regard to its head. In addition to phrasal categories, CTB also has functional labels to represent relations. The CTB 131 Computational Linguistics Volume 45, Number 1 Table 16 Illustration of major dependency relations in our corpus. ADV AMOD AUX"
J19-1003,2007.tmi-papers.18,0,0.0195122,"Missing"
J19-1003,N18-1202,0,0.289677,"e proposed graph decomposition and merging methods. 3. The transition-based parser can be trained with either the dynamic oracle or the beam search method. According to our implementations, the dynamic oracle method performs better. Coupled with dynamic oracle, the transition-based parser reaches a labeled f-score of 85.51 when gold POS tags are utilized. 4. Gold-standard POS tags play a very important role in achieving the above accuracies. However, the automatic tagging quality of state-of-the-art Chinese POS taggers are still far from satisfactory. To deal with this problem, we apply ELMo (Peters et al. 2018), a contextualized word embedding producer, to obtain adequate lexical information. Experiments show that ELMo is extremely effective in harvesting lexical knowledge from raw texts. With the help of the ELMo vectors but not any POS tagging results, the graph merging parser achieves a labeled f-score of 84.90. This is a realistic set-up to evaluate how accurate the GR parsers could be for real-world Chinese Language Processing applications. The current study expands on the earlier and preliminary results, which have been published in Sun et al. (2014) and Sun, Du, and Wan (2017). The new findin"
J19-1003,Q13-1002,0,0.0519761,"Missing"
J19-1003,D09-1085,0,0.0857498,"Missing"
J19-1003,C08-1095,0,0.308052,"Missing"
J19-1003,P14-1042,1,0.643032,"To deal with this problem, we apply ELMo (Peters et al. 2018), a contextualized word embedding producer, to obtain adequate lexical information. Experiments show that ELMo is extremely effective in harvesting lexical knowledge from raw texts. With the help of the ELMo vectors but not any POS tagging results, the graph merging parser achieves a labeled f-score of 84.90. This is a realistic set-up to evaluate how accurate the GR parsers could be for real-world Chinese Language Processing applications. The current study expands on the earlier and preliminary results, which have been published in Sun et al. (2014) and Sun, Du, and Wan (2017). The new findings and contributions in this submission include: (1) a detailed comparison between our 98 Sun et al. Parsing Chinese Sentences with Grammatical Relations GR analysis and other syntactic/semantic analyses, including Universal Dependency, Semantic Role Labeling, and LFG’s f-structure, (2) the design of a new neural graph merging parser, (3) the design of a new neural transition-based parser, and (4) more comprehensive evaluations and analyses of the neural parsing models. The implementation of the corpus conversion algorithm, a corpus visualization too"
J19-1003,K17-1005,1,0.860347,"Missing"
J19-1003,P12-1026,1,0.783037,"inese Sentences with Grammatical Relations Scores of actions MLP Bi-LSTMs ... Bi-LSTMs Bi-LSTMs ... Embedding ... Embedding Embedding ... list top 浦东 NR buffer front 实行 VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Sun and Uszkoreit 2012), constituent parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). This corpus was collected during different time periods from different sources with a diverse range of topics. We used CTB 6.0 and defined the training, development, and test sets according to the CoNLL 2009 shared task. Table 3 gives a summary of the data sets for experiments. Evaluation on this benchmark data allows us to directly compare our parsers and other parsers in the literature, according to numeric performance. The measure"
J19-1003,J16-3002,1,0.860014,"07b; Miyao and Tsujii 2008). Previous work on Chinese CCG and HPSG parsing unanimously agree that obtaining the deep analysis of Chinese is more challenging (Yu et al. 2011; Tse and Curran 2012). The successful C&C and Enju parsers provide very inaccurate results for Chinese texts. Though the numbers profiling the qualities of deep dependency structures under different formalisms are not directly comparable, all empirical evaluation indicates that the state of the art of deep linguistic processing for Chinese lags very much behind. 6.4.3 Impact of POS Tagging. According to our previous study (Sun and Wan 2016), the use of different POS taggers has a great impact on syntactic analysis. This is highly related to a prominent language-specific property of Mandarin Chinese: as an analytic language, Mandarin Chinese lacks morphosyntactic features to explicitly indicate lexical categories. To evaluate the parsing performance in a more realistic setup, we report parsing results based on two different POS taggers introduced in Sun and Wan (2016). Table 9 presents the results. We can see that automatic POS tagging has a great impact on deep dependency parsing. Even when assisted with a state-of-the-art tagge"
J19-1003,D11-1090,1,0.896865,"stem. 122 Sun et al. Parsing Chinese Sentences with Grammatical Relations Scores of actions MLP Bi-LSTMs ... Bi-LSTMs Bi-LSTMs ... Embedding ... Embedding Embedding ... list top 浦东 NR buffer front 实行 VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Sun and Uszkoreit 2012), constituent parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). This corpus was collected during different time periods from different sources with a diverse range of topics. We used CTB 6.0 and defined the training, development, and test sets according to the CoNLL 2009 shared task. Table 3 gives a summary of the data sets for experiments. Evaluation on this benchmark data allows us to directly compare our parsers and other parsers in the literature, accordi"
J19-1003,P09-1039,0,0.0970611,"Missing"
J19-1003,C10-1122,0,0.0298864,"rin Chinese. 2.3 Initial Research on Chinese Deep Linguistic Processing In the last few years, study on deep linguistic processing for Chinese has been initialized. Treebank annotation for individual formalisms is prohibitively expensive. To quickly construct deep annotations, corpus-driven grammar engineering has been developed. Phrase structure trees in CTB have been semi-automatically converted to deep derivations in the CCG (Tse and Curran 2010), LFG (Guo, van Genabith, and Wang 2007), and HPSG (Yu et al. 2010) formalisms. To semi-automatically extract a largescale HPSG grammar, Yu et al. (2010) defined a skeleton, including the structure of sign, grammatical principles, and schemata, based on which, the CTB trees are converted into HPSG-style trees. The treebank conversion under the CCG formalism is relatively easier. Tse and Curran (2010) followed the method proposed for English Penn Treebank (Hockenmaier and Steedman 2007) to process CTB. The main steps include (1) distinguishing head, argument, and adjunct, (2) binarizing CTB trees, and (3) redefining syntactic categories. To more robustly construct f-structure for CTB trees, Guo, van Genabith, and Wang (2007) proposed a dependen"
J19-1003,N12-1030,0,0.127842,"ockenmaier and Steedman 2007) to process CTB. The main steps include (1) distinguishing head, argument, and adjunct, (2) binarizing CTB trees, and (3) redefining syntactic categories. To more robustly construct f-structure for CTB trees, Guo, van Genabith, and Wang (2007) proposed a dependency-based model, which extracts functional information from dependency trees. With the use of converted fine-grained linguistic annotations, successful English deep parsers, such as C&C (Clark and Curran 2007b) and Enju (Miyao and Tsujii 2008), have been evaluated on the Chinese annotations (Yu et al. 2011; Tse and Curran 2012). Although the discriminative learning architecture of both C&C and Enju parsers makes them relatively easy to be adapted to solve multilingual parsing, their performance on Chinese sentences is far from satisfactory. Yu et al. (2011) and Tse and Curran (2012) analyze the challenges and difficulties in Chinese deep parsing. In particular, some language-specific properties account for a large number of errors. 3. Representing Deep Linguistic Information Using Dependency Graphs In this section, we discuss the construction of the GR annotations. Basically, the annotations are automatically conver"
J19-1003,P06-1054,0,0.119024,"Missing"
J19-1003,P15-1113,0,0.0641625,"Missing"
J19-1003,P15-1032,0,0.333835,"traditional grammars by the notions of subject, direct/indirect object, etc., and therefore encode rich syntactic information of natural language sentences in an explicit way. In recent years, parsing a sentence to a dependency representation has been well studied and widely applied to many Natural Language Processing (NLP) tasks, for example, Information Extraction and Machine Translation. In particular, the ¨ data-driven approaches have made great progress during the past two decades (Kubler, McDonald, and Nivre 2009; Koo et al. 2010; Pitler, Kannan, and Marcus 2013; Chen and Manning 2014; Weiss et al. 2015; Dozat and Manning 2016). Various practical parsing systems have been built, not only for English but also for a large number of typologically different languages, for example, Arabic, Basque, Catalan, Chinese, Czech, Greek, Hungarian, Italian, and Turkish (Nivre et al. 2007). Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed bilexical dependency trees. Although tree-shaped graphs have several desirable properties from the computational perspective, they do not work well for coordinations, long-range dependencies involved in raising,"
J19-1003,P10-1034,0,0.0673123,"Missing"
J19-1003,W03-3023,0,0.471219,"Missing"
J19-1003,W11-2907,0,0.176594,"Penn Treebank (Hockenmaier and Steedman 2007) to process CTB. The main steps include (1) distinguishing head, argument, and adjunct, (2) binarizing CTB trees, and (3) redefining syntactic categories. To more robustly construct f-structure for CTB trees, Guo, van Genabith, and Wang (2007) proposed a dependency-based model, which extracts functional information from dependency trees. With the use of converted fine-grained linguistic annotations, successful English deep parsers, such as C&C (Clark and Curran 2007b) and Enju (Miyao and Tsujii 2008), have been evaluated on the Chinese annotations (Yu et al. 2011; Tse and Curran 2012). Although the discriminative learning architecture of both C&C and Enju parsers makes them relatively easy to be adapted to solve multilingual parsing, their performance on Chinese sentences is far from satisfactory. Yu et al. (2011) and Tse and Curran (2012) analyze the challenges and difficulties in Chinese deep parsing. In particular, some language-specific properties account for a large number of errors. 3. Representing Deep Linguistic Information Using Dependency Graphs In this section, we discuss the construction of the GR annotations. Basically, the annotations ar"
J19-1003,C10-2162,0,0.0658995,"Missing"
J19-1003,J16-3001,1,0.948956,"architecture employed here, beam search performs significantly worse than the dynamic oracle strategy. However, do note that beam search and structured learning may be very helpful for neural parsing models of other architectures (Weiss et al. 2015; Andor et al. 2016). Here, the beam size is set to 16. 6.4 Analysis 6.4.1 Precision vs. Recall. A noteworthy fact about the overall performance of the neural transition-based system is that the precision is promising but the recall is low. This difference is consistent with the result obtained by transition-based parsers with linear scoring models (Zhang et al. 2016), and the result obtained by a shift-reduce CCG parser (Zhang and Clark 2011a). The functor-argument dependencies generated by the CCG parser also has a relatively high precision but considerably low recall. To build NLP application, for example, information extraction, and systems upon GR parsing, such property merits attention. A good trade-off between the precision and the recall may have a great impact on final results. The graph merging system coupled with neural tree parser performs quite differently. The precision and recall are quite harmonious. Figure 17 and Table 8 present detailed a"
J19-1003,D08-1059,0,0.0400365,"mbedding ... list top 浦东 NR buffer front 实行 VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Sun and Uszkoreit 2012), constituent parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). This corpus was collected during different time periods from different sources with a diverse range of topics. We used CTB 6.0 and defined the training, development, and test sets according to the CoNLL 2009 shared task. Table 3 gives a summary of the data sets for experiments. Evaluation on this benchmark data allows us to directly compare our parsers and other parsers in the literature, according to numeric performance. The measure for comparing two dependency graphs is precision/recall of bilexical dependencies, which are defined as hwh , wd , li tup"
J19-1003,W09-3825,0,0.0245686,"Bi-LSTMs Bi-LSTMs ... Embedding ... Embedding Embedding ... list top 浦东 NR buffer front 实行 VV 了 AS Figure 16 The neural network structure for parsing the running sentence. We select the top element of list λ and top front of the buffer as features. 6. Empirical Evaluation 6.1 Experimental Setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popularly used to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu 2011), POS tagging (Sun and Uszkoreit 2012), constituent parsing (Wang, Sagae, and Mitamura 2006; Zhang and Clark 2009), and dependency parsing (Zhang and Clark 2008; Huang and Sagae 2010; Li et al. 2011). This corpus was collected during different time periods from different sources with a diverse range of topics. We used CTB 6.0 and defined the training, development, and test sets according to the CoNLL 2009 shared task. Table 3 gives a summary of the data sets for experiments. Evaluation on this benchmark data allows us to directly compare our parsers and other parsers in the literature, according to numeric performance. The measure for comparing two dependency graphs is precision/recall of bilexical depend"
J19-1003,P11-1069,0,0.0243552,"5 88.36 20.72 20.72 20.66 29.34 88.80 88.16 89.11 86.43 77.82 80.20 79.95 85.66 82.95 83.99 84.28 86.05 18.39 18.16 17.93 25.09 125 Computational Linguistics Volume 45, Number 1 Table 7 Accuracies of the transition-based parser on development set. Dynamic oracle Beam search UP UR UF UCM LP LR LF LCM 89.59 84.58 85.50 87.38 87.49 85.96 24.74 20.89 87.49 82.17 83.49 84.89 85.44 83.51 21.45 17.99 achieves excellent results for various NLP tasks, for example, Machine Translation. When coupled with linear models, beam search has shown to be a useful technique to improve both training and decoding (Zhang and Clark 2011b). However, in the particular neural parsing architecture employed here, beam search performs significantly worse than the dynamic oracle strategy. However, do note that beam search and structured learning may be very helpful for neural parsing models of other architectures (Weiss et al. 2015; Andor et al. 2016). Here, the beam size is set to 16. 6.4 Analysis 6.4.1 Precision vs. Recall. A noteworthy fact about the overall performance of the neural transition-based system is that the precision is promising but the recall is low. This difference is consistent with the result obtained by transit"
J19-1003,J11-1005,0,0.0312884,"5 88.36 20.72 20.72 20.66 29.34 88.80 88.16 89.11 86.43 77.82 80.20 79.95 85.66 82.95 83.99 84.28 86.05 18.39 18.16 17.93 25.09 125 Computational Linguistics Volume 45, Number 1 Table 7 Accuracies of the transition-based parser on development set. Dynamic oracle Beam search UP UR UF UCM LP LR LF LCM 89.59 84.58 85.50 87.38 87.49 85.96 24.74 20.89 87.49 82.17 83.49 84.89 85.44 83.51 21.45 17.99 achieves excellent results for various NLP tasks, for example, Machine Translation. When coupled with linear models, beam search has shown to be a useful technique to improve both training and decoding (Zhang and Clark 2011b). However, in the particular neural parsing architecture employed here, beam search performs significantly worse than the dynamic oracle strategy. However, do note that beam search and structured learning may be very helpful for neural parsing models of other architectures (Weiss et al. 2015; Andor et al. 2016). Here, the beam size is set to 16. 6.4 Analysis 6.4.1 Precision vs. Recall. A noteworthy fact about the overall performance of the neural transition-based system is that the precision is promising but the recall is low. This difference is consistent with the result obtained by transit"
J19-1003,P15-1117,0,0.0403883,"Missing"
K17-1005,W13-2322,0,0.0379397,"Missing"
K17-1005,C10-1011,0,0.100823,"consistency tags emerge, for convenience we index the graph and tree vector representation using three indices. g(i, j, t) denotes whether there is an edge from word wi to word wj with tag t in graph g. The joint decoding problem can be written as a constrained optimization problem as Graph Merging The extraction algorithm gives three classes of trees for each graph. We apply the algorithm to the graph training set, and get three training tree sets. After that, we can train three parsing models with the three tree sets. In this work, the parser we use to train models and parse trees is Mate (Bohnet, 2010), a second-order graph-based dependency parser. Let the scores the three models use be f1 , f2 , f3 respectively. Then the parsers can find trees with highest scores for a sentence. That is solving the following optimization problems: arg maxg1 f1 (g1 ), arg maxg2 f2 (g2 ) and arg maxg2 f3 (g3 ). We can parse a given sentence with the three models, obtain three trees, and then transform them into subgraphs, and combine them together to obtain the graph parse of the sentence by putting all the edges in the three subgraphs together. That is to say, we obtain the graph y = max{t2g(g1 ), t2g(g2 ),"
K17-1005,P17-1193,1,0.746711,"cy graphs (Oepen et al., 2014, 2015) and abstract meaning representations (Banarescu et al., 2013). 2 3 The Idea The key idea of this work is constructing a complex structure via constructing simple partial structures. Each partial structure is simple in the sense that it allows efficient construction. For instance, projective trees, 1-endpoint-corssing trees, non-crossing dependency graphs and 1-endpointcrossing, pagenumber-2 graphs can be taken as simple structures, given that low-degree polynomial time parsing algorithms exist (Eisner, 1996; Pitler et al., 2013; Kuhlmann and Jonsson, 2015; Cao et al., 2017; Sun et al., 2017). To construct each partial structure, we can employ mature parsing techniques. To get the final target output, we also require the total of all partial structures enables whole target structure to be produced. In this paper, we exemplify the above idea by designing a new parser for obtaining GR graphs. Take the GR graph in Figure 1 for example. It can be decomposed into two tree-like subgraphs, shown in Figure 2. If we can parse the sentence into subgraphs and combine them in a principled way, we get the original GR graph. Under this perspective, we need to develop a princi"
K17-1005,C02-1013,0,0.0856166,"nto simple subgraphs, and (2) how to combine subgraphs into a coherent complex graph. Experiments demonstrate the effectiveness of graph merging. Our parser reaches state-of-the-art performance and is significantly better than two transition-based parsers. 1 Introduction Grammatical relations (GRs) represent functional relationships between language units in a sentence. Marking not only local but also a wide variety of long distance dependencies, GRs encode in-depth information of natural language sentences. Traditionally, GRs are generated as a byproduct by grammar-guided parsers, e.g. RASP (Carroll and Briscoe, 2002), C&C (Clark and Curran, 2007b) and Enju (Miyao et al., 2007). Very recently, by representing GR analysis using general directed dependency graphs, Sun et al. (2014) and Zhang et al. (2016) showed that considerably good GR structures can be directly obtained using data-driven, transition-based parsing techniques. We follow their encouraging work and study the data-driven approach for producing GR analyses. The key challenge of building GR graphs is due to their flexibility. Different from surface syntax, the GR graphs are not constrained to trees, which is a fundamental consideration in design"
K17-1005,J16-3001,1,0.910283,"d is significantly better than two transition-based parsers. 1 Introduction Grammatical relations (GRs) represent functional relationships between language units in a sentence. Marking not only local but also a wide variety of long distance dependencies, GRs encode in-depth information of natural language sentences. Traditionally, GRs are generated as a byproduct by grammar-guided parsers, e.g. RASP (Carroll and Briscoe, 2002), C&C (Clark and Curran, 2007b) and Enju (Miyao et al., 2007). Very recently, by representing GR analysis using general directed dependency graphs, Sun et al. (2014) and Zhang et al. (2016) showed that considerably good GR structures can be directly obtained using data-driven, transition-based parsing techniques. We follow their encouraging work and study the data-driven approach for producing GR analyses. The key challenge of building GR graphs is due to their flexibility. Different from surface syntax, the GR graphs are not constrained to trees, which is a fundamental consideration in design26 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 26–35, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computationa"
K17-1005,N04-1013,0,0.654687,"Missing"
K17-1005,Q15-1040,0,0.013714,"ions, e.g. semantic dependency graphs (Oepen et al., 2014, 2015) and abstract meaning representations (Banarescu et al., 2013). 2 3 The Idea The key idea of this work is constructing a complex structure via constructing simple partial structures. Each partial structure is simple in the sense that it allows efficient construction. For instance, projective trees, 1-endpoint-corssing trees, non-crossing dependency graphs and 1-endpointcrossing, pagenumber-2 graphs can be taken as simple structures, given that low-degree polynomial time parsing algorithms exist (Eisner, 1996; Pitler et al., 2013; Kuhlmann and Jonsson, 2015; Cao et al., 2017; Sun et al., 2017). To construct each partial structure, we can employ mature parsing techniques. To get the final target output, we also require the total of all partial structures enables whole target structure to be produced. In this paper, we exemplify the above idea by designing a new parser for obtaining GR graphs. Take the GR graph in Figure 1 for example. It can be decomposed into two tree-like subgraphs, shown in Figure 2. If we can parse the sentence into subgraphs and combine them in a principled way, we get the original GR graph. Under this perspective, we need t"
K17-1005,S15-2153,0,0.287007,"Missing"
K17-1005,S14-2008,0,0.136054,"Missing"
K17-1005,Q13-1002,0,0.22183,"flexible representations, e.g. semantic dependency graphs (Oepen et al., 2014, 2015) and abstract meaning representations (Banarescu et al., 2013). 2 3 The Idea The key idea of this work is constructing a complex structure via constructing simple partial structures. Each partial structure is simple in the sense that it allows efficient construction. For instance, projective trees, 1-endpoint-corssing trees, non-crossing dependency graphs and 1-endpointcrossing, pagenumber-2 graphs can be taken as simple structures, given that low-degree polynomial time parsing algorithms exist (Eisner, 1996; Pitler et al., 2013; Kuhlmann and Jonsson, 2015; Cao et al., 2017; Sun et al., 2017). To construct each partial structure, we can employ mature parsing techniques. To get the final target output, we also require the total of all partial structures enables whole target structure to be produced. In this paper, we exemplify the above idea by designing a new parser for obtaining GR graphs. Take the GR graph in Figure 1 for example. It can be decomposed into two tree-like subgraphs, shown in Figure 2. If we can parse the sentence into subgraphs and combine them in a principled way, we get the original GR graph. Under"
K17-1005,P17-1077,1,0.80564,"t al., 2014, 2015) and abstract meaning representations (Banarescu et al., 2013). 2 3 The Idea The key idea of this work is constructing a complex structure via constructing simple partial structures. Each partial structure is simple in the sense that it allows efficient construction. For instance, projective trees, 1-endpoint-corssing trees, non-crossing dependency graphs and 1-endpointcrossing, pagenumber-2 graphs can be taken as simple structures, given that low-degree polynomial time parsing algorithms exist (Eisner, 1996; Pitler et al., 2013; Kuhlmann and Jonsson, 2015; Cao et al., 2017; Sun et al., 2017). To construct each partial structure, we can employ mature parsing techniques. To get the final target output, we also require the total of all partial structures enables whole target structure to be produced. In this paper, we exemplify the above idea by designing a new parser for obtaining GR graphs. Take the GR graph in Figure 1 for example. It can be decomposed into two tree-like subgraphs, shown in Figure 2. If we can parse the sentence into subgraphs and combine them in a principled way, we get the original GR graph. Under this perspective, we need to develop a principled method to deco"
K17-1005,P14-1042,1,0.758478,"the-art performance and is significantly better than two transition-based parsers. 1 Introduction Grammatical relations (GRs) represent functional relationships between language units in a sentence. Marking not only local but also a wide variety of long distance dependencies, GRs encode in-depth information of natural language sentences. Traditionally, GRs are generated as a byproduct by grammar-guided parsers, e.g. RASP (Carroll and Briscoe, 2002), C&C (Clark and Curran, 2007b) and Enju (Miyao et al., 2007). Very recently, by representing GR analysis using general directed dependency graphs, Sun et al. (2014) and Zhang et al. (2016) showed that considerably good GR structures can be directly obtained using data-driven, transition-based parsing techniques. We follow their encouraging work and study the data-driven approach for producing GR analyses. The key challenge of building GR graphs is due to their flexibility. Different from surface syntax, the GR graphs are not constrained to trees, which is a fundamental consideration in design26 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 26–35, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Asso"
K17-1005,D10-1001,0,\N,Missing
K17-1005,W03-3023,0,\N,Missing
K17-1005,E06-1011,0,\N,Missing
K17-1005,C96-1058,0,\N,Missing
K17-1005,J08-4003,0,\N,Missing
K17-1005,C08-1095,0,\N,Missing
K17-1005,S14-2082,0,\N,Missing
K17-1005,J13-4006,0,\N,Missing
K17-1005,P09-1039,0,\N,Missing
K17-1005,J05-1004,0,\N,Missing
K17-1005,P15-1149,1,\N,Missing
K17-1005,D10-1125,0,\N,Missing
K17-1005,P11-1008,0,\N,Missing
K17-1035,P16-1231,0,0.017546,"Complete spans are depicted as triangles, incomplete spans as trapezoids, and sibling spans as rectangles. A new dependency is created by applying the last rule. Especially, the score associated with the last rule is determined by a sibling part. For brevity, we elide the right-headed versions. = l The Existing Parsing Algorithms m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l second-order sibling parts, McDonald an"
K17-1035,D12-1091,0,0.0131874,"r ones. When we take into account empty categories, more information is available. The empirical results suggest that deep linguistic information does not necessarily help surface analysis. 6.4 Results of Joint Decoding Table 5 lists the accuracy of different joint decoding models on the test sets. We can see that the joint decoding framework is effective to deal with structure-based overfitting. This time, the accuracy of analysis for overt words is consistently improved across a wide range of conditions. Especially, the third-order model is improved more. We use the Hypothesis Tests method (Berg-Kirkpatrick et al., 2012) to evaluate the improvements. When the p-value is set to 0.05, all improvements in Figure 5 is statistically significant. We separate all sentences in test data set into two subsets: One contains sentences that have no • arc features: funi (h), funi (c), fbi (h, c), fcontext (h, c). • sibling features: fsib (c, m0 ), fsib (h, c, mk ). • tri-sibling features: fsib (h, c, m1 ), ftsib (h, c, m, m1 ). 6.3 Chinese 89.16 89.20 (+0.04) 89.28 (+0.12) 90.00 89.82 (−0.18) Table 4: UASo of different individual models on test data. The upper and bottom blocks present results obtained by sibling and tri-s"
K17-1035,C96-1058,0,0.0354057,"ure 2. 3.2 Algorithm 2: Tri-sibling Factorization It is easy to extend the second-order sibling factorization to parts containing multiple siblings. For example, Koo and Collins (2010) introduced trisibling factorization in which a triple of three successive edges on the same side. Here, we consider parsing for tri-sibling factorization only. To this end, we augment the incomplete span structure with an internal index. The modified construction rule is specified graphically in Figure 3. Note that the presentation is slightly different from Koo and Collins’s. Algorithm 1: Sibling Factorization Eisner (1996) introduced a widely-used DP algorithm for first-order parsing. Their algorithm includes two interrelated types of DP structures: (1) complete spans, which consist of a head-word and its descendents on one side, and (2) incomplete spans, which consist of a dependency and the region between the head and modifier. To include 2 Comparing their numeric results with other papers’, we find that their model does not result in improved parsing. 345 ∅1 h a ... h1 ... ∅1 ... n ∅2 ... ∅3 Length 1 2 3 Total ∅2 ∅4 ... h2 defined in Section 6). We show the statistics in Table 1. The length indicates the num"
K17-1035,P02-1018,0,0.0662743,"an example. Detecting empty elements is important to the interpretation of the syntactic structure of a sentence. For example, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic empty element detection on statistical machine translation. There are three strategies to find empty categories. Dienes and Dubey (2003) introduced a model that utilizes clues from word forms and POS tags to predict the existence of empty categories. In their method, syntactic parsing was treated as a next-step task and therefore had no influence on finding empty elements. Johnson (2002) and Xue and Yang To ensure that predicting the empty elements helps parse the overt, we need to reduce the new estimation error. To this end, we propose to integrate scores from parsing models with and without empty elements and perform joint decoding. The intuition is to leverage parameters estimated without empty elements as a backoff, which exhibit better generalization ability. We evaluate two joint decoders: One is based on chart merging and the other is based on dual decomposition. Experiments demonstrate that information about the covert improves surface analysis in this way. Accuracy"
K17-1035,P11-2037,0,0.025696,"fact that empty category can help reduce the approximation error for surface analysis. The remaining part of the paper is organized as follows. Section 2 is a brief introduction to the problem. Section 3 describes existing algorithms for parsing for overt words only, while Section 4 gives the details of our new algorithms for parsing with empty elements. Section 5 describes the de344 (2013) proposed to identify empty categories after syntactic parsing. Different from the above preprocessing strategy, their post-processing models can not use information about empty category to improve parsing. Cai et al. (2011) introduced an integrated model, where empty category detection and phrase-structure parsing are combined in a single model. They, however, did not report any improvement for parsing.2 Seeker et al. (2012) evaluted all above strategies to include empty nodes in dependency parsing for German and Hungarian. To predict both empty nodes and dependency relations, they enriched the information encoded in dependency labels. They showed that both pre-processing and integrated strategies failed to leverage empty categories to improve parsing. Especially, their preprocessing method significantly descrea"
K17-1035,N04-1013,0,0.0650461,"ses the search space for inference and accordingly the estimation error. To deal with structure-based overfitting, we propose to integrate disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing. 1 Introduction In the last two decades, there was an increasing interest in producing rich syntactic annotations that are not limited to surface analysis. See, among others, (Callmeier, 2000; Kaplan et al., 2004; Clark and Curran, 2007; Miyao and Tsujii, 2008; Zhang et al., 2016). Such analysis, e.g. deep dependency structures (King et al., 2003), is usually coupled with grammars under deep formalisms, e.g. Combinatory Categorial Grammar (CCG; Steedman, 2000), Headdriven Phrase-Structure Grammar (HPSG; Pollard and Sag, 1994) and Lexical-Functional Grammar (LFG; Bresnan and Kaplan, 1982). Although deep grammar formalisms allow information beyond local construction to be constructed, it is still not 1 In this paper, we arguably call dependencies among overt words only surface analysis. 343 Proceedings"
K17-1035,W03-2401,0,0.049369,"disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing. 1 Introduction In the last two decades, there was an increasing interest in producing rich syntactic annotations that are not limited to surface analysis. See, among others, (Callmeier, 2000; Kaplan et al., 2004; Clark and Curran, 2007; Miyao and Tsujii, 2008; Zhang et al., 2016). Such analysis, e.g. deep dependency structures (King et al., 2003), is usually coupled with grammars under deep formalisms, e.g. Combinatory Categorial Grammar (CCG; Steedman, 2000), Headdriven Phrase-Structure Grammar (HPSG; Pollard and Sag, 1994) and Lexical-Functional Grammar (LFG; Bresnan and Kaplan, 1982). Although deep grammar formalisms allow information beyond local construction to be constructed, it is still not 1 In this paper, we arguably call dependencies among overt words only surface analysis. 343 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 343–353, c Vancouver, Canada, August 3 - August 4,"
K17-1035,D10-1062,0,0.0259654,"function tags, they make up the full syntactic representation of a sentence. Empty category is one key concept bridging SStructure and D-Structure, given that they contain essential information to trace movements, i.e. the transformation procedure to convert a D-Structure to an S-Structure. When representing empty categories in dependency trees, we can use a null symbol to depict the idea that there is a mental category at the level being represented. See Figure 1 for an example. Detecting empty elements is important to the interpretation of the syntactic structure of a sentence. For example, Chung and Gildea (2010) reported preliminary work that has shown a positive impact of automatic empty element detection on statistical machine translation. There are three strategies to find empty categories. Dienes and Dubey (2003) introduced a model that utilizes clues from word forms and POS tags to predict the existence of empty categories. In their method, syntactic parsing was treated as a next-step task and therefore had no influence on finding empty elements. Johnson (2002) and Xue and Yang To ensure that predicting the empty elements helps parse the overt, we need to reduce the new estimation error. To this"
K17-1035,P10-1001,0,0.0166792,"ended Eisner’s algorithm with a third structure, viz. (3) sibling spans, which represent the region between successive modifiers of same head. The second-order algorithm visits all the spans from bottom to top, finding the best combination of smaller structures to form a new one. Each type of span is created by recursively combining two smaller, adjacent spans. The DP structures and their constructions are specified graphically in Figure 2. 3.2 Algorithm 2: Tri-sibling Factorization It is easy to extend the second-order sibling factorization to parts containing multiple siblings. For example, Koo and Collins (2010) introduced trisibling factorization in which a triple of three successive edges on the same side. Here, we consider parsing for tri-sibling factorization only. To this end, we augment the incomplete span structure with an internal index. The modified construction rule is specified graphically in Figure 3. Note that the presentation is slightly different from Koo and Collins’s. Algorithm 1: Sibling Factorization Eisner (1996) introduced a widely-used DP algorithm for first-order parsing. Their algorithm includes two interrelated types of DP structures: (1) complete spans, which consist of a he"
K17-1035,D10-1125,0,0.0223262,"91.72 (−0.01) 92.23 92.41 (+0.18) Results of Individual Models Table 4 lists the accuracy of individual models coupled with different decoding algorithms on the test sets. We focus on the prediction for overt 350 English −EC +EC 92.50 91.53 92.83 91.77 92.82 91.48 92.84 91.74 93.68 92.13 93.99 92.43 Algo 3 1+3 4 1+4 5 2+5 Chinese −EC +EC 90.92 88.60 91.12 88.97 91.29 88.58 91.10 88.98 92.00 89.06 92.10 89.77 composition practically gives the exact solutions in a few iterations. One advantage relevant is that such a decoder can integrate parsing models that are somehow heterogeneous. Refer to (Koo et al., 2010) for example. 7 Can deep syntactic information help surface parsing, which is the mainstream focus of NLP research. In this paper, we investigate this topic under the umbrella of Transformational Grammar, GB in particular. We focused on empty category augmented dependency analysis. We demonstrate that on the one hand deep information helps reduce the approximation error for traditional (surface) parsing, while on the other hand traditional parsing helps reduce the estimation error for deep parsing. Coupling surface and deep information in an appropriate way is able to produce better syntactic"
K17-1035,W02-1001,0,0.0799215,"38 (+0.38) Table 5: UASo of different joint decoding models on test data. “CM” and “DD” are short for joint decoders based on chart merging and dual decomposition respectively. The upper and bottom blocks present results obtained by sibling and tri-sibling models respectively. All improvements are statistically significant. Statistical Disambiguation In the context of data-driven parsing, we still need an extra disambiguation model for building a practical parser. As with many other parsers, we employ a global linear model. To estimate parameters, we utilize the averaged perceptron algorithm (Collins, 2002). Developing features has been shown crucial to advancing the state-of-the-art in dependency parsing. We adopt features from previous work. We refer to the head/child of the arc as h/c, the k-th inner split point as mk , and the grand point as g. We list features selected by different algorithm as follows, and all following features should be concatenated with direction and distance of the arc. words only. Models coupled with Algorithm 1, 3 and 4 are second-order models, while with 2 and 5 third-order ones. When we take into account empty categories, more information is available. The empirica"
K17-1035,P14-1130,0,0.0190081,"ling spans as rectangles. A new dependency is created by applying the last rule. Especially, the score associated with the last rule is determined by a sibling part. For brevity, we elide the right-headed versions. = l The Existing Parsing Algorithms m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l second-order sibling parts, McDonald and Pereira (2006) extended Eisner’s algorithm with a third structure, viz. (3) si"
K17-1035,de-marneffe-etal-2006-generating,0,0.200503,"Missing"
K17-1035,E06-1011,0,0.168251,"Missing"
K17-1035,N13-1125,0,0.0665693,"cs, Peking University {zhangxunah,ws,wanxiaojun}@pku.edu.cn Abstract clear whether such additional information is helpful for surface syntactic analysis. This is partly because analysis grounded on different grammar formalisms, e.g. HPSG and CFG, are not directly comparable. In the Government and Binding (GB; Chomsky, 1981) theory, empty category is a key concept bridging S-Structure and D-Structure, due to its possible contribution to trace movements. Following the linguistic insights underlying GB, a traditional dependency analysis can be augmented with empty elements, viz. covert elements (Xue and Yang, 2013). See Figure 1 for an example. The new representation provides a considerable amount of deep syntactic information, while keeping intact all dependencies of overt words. Integrating both overt and covert elements in one unified representation provides an effective yet lightweight way to achieve deeper language understanding beyond surface syntax1 . Even more important, this modest way to modify tree analysis makes possible fair evaluation of the influence of deep syntactic elements on surface parsing. We study graph-based parsing models for this new representation with a particular focus on th"
K17-1035,H05-1066,0,0.137515,"m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l second-order sibling parts, McDonald and Pereira (2006) extended Eisner’s algorithm with a third structure, viz. (3) sibling spans, which represent the region between successive modifiers of same head. The second-order algorithm visits all the spans from bottom to top, finding the best combination of smaller structures to form a new one. Each type of span is created by rec"
K17-1035,W03-3023,0,0.136041,"ations of the standard sibling algorithm. Complete spans are depicted as triangles, incomplete spans as trapezoids, and sibling spans as rectangles. A new dependency is created by applying the last rule. Especially, the score associated with the last rule is determined by a sibling part. For brevity, we elide the right-headed versions. = l The Existing Parsing Algorithms m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l secon"
K17-1035,J16-3001,1,0.854813,"or. To deal with structure-based overfitting, we propose to integrate disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing. 1 Introduction In the last two decades, there was an increasing interest in producing rich syntactic annotations that are not limited to surface analysis. See, among others, (Callmeier, 2000; Kaplan et al., 2004; Clark and Curran, 2007; Miyao and Tsujii, 2008; Zhang et al., 2016). Such analysis, e.g. deep dependency structures (King et al., 2003), is usually coupled with grammars under deep formalisms, e.g. Combinatory Categorial Grammar (CCG; Steedman, 2000), Headdriven Phrase-Structure Grammar (HPSG; Pollard and Sag, 1994) and Lexical-Functional Grammar (LFG; Bresnan and Kaplan, 1982). Although deep grammar formalisms allow information beyond local construction to be constructed, it is still not 1 In this paper, we arguably call dependencies among overt words only surface analysis. 343 Proceedings of the 21st Conference on Computational Natural Language Learning (Co"
K17-1035,J08-1002,0,0.0436971,"ingly the estimation error. To deal with structure-based overfitting, we propose to integrate disambiguation models with and without empty elements, and perform structure regularization via joint decoding. Experiments on English and Chinese TreeBanks with different parsing models indicate that incorporating empty elements consistently improves surface parsing. 1 Introduction In the last two decades, there was an increasing interest in producing rich syntactic annotations that are not limited to surface analysis. See, among others, (Callmeier, 2000; Kaplan et al., 2004; Clark and Curran, 2007; Miyao and Tsujii, 2008; Zhang et al., 2016). Such analysis, e.g. deep dependency structures (King et al., 2003), is usually coupled with grammars under deep formalisms, e.g. Combinatory Categorial Grammar (CCG; Steedman, 2000), Headdriven Phrase-Structure Grammar (HPSG; Pollard and Sag, 1994) and Lexical-Functional Grammar (LFG; Bresnan and Kaplan, 1982). Although deep grammar formalisms allow information beyond local construction to be constructed, it is still not 1 In this paper, we arguably call dependencies among overt words only surface analysis. 343 Proceedings of the 21st Conference on Computational Natural"
K17-1035,J08-4003,0,0.032539,"ng algorithm. Complete spans are depicted as triangles, incomplete spans as trapezoids, and sibling spans as rectangles. A new dependency is created by applying the last rule. Especially, the score associated with the last rule is determined by a sibling part. For brevity, we elide the right-headed versions. = l The Existing Parsing Algorithms m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l second-order sibli"
K17-1035,C12-2105,0,0.0220798,"3 describes existing algorithms for parsing for overt words only, while Section 4 gives the details of our new algorithms for parsing with empty elements. Section 5 describes the de344 (2013) proposed to identify empty categories after syntactic parsing. Different from the above preprocessing strategy, their post-processing models can not use information about empty category to improve parsing. Cai et al. (2011) introduced an integrated model, where empty category detection and phrase-structure parsing are combined in a single model. They, however, did not report any improvement for parsing.2 Seeker et al. (2012) evaluted all above strategies to include empty nodes in dependency parsing for German and Hungarian. To predict both empty nodes and dependency relations, they enriched the information encoded in dependency labels. They showed that both pre-processing and integrated strategies failed to leverage empty categories to improve parsing. Especially, their preprocessing method significantly descreased parsing accuracy. Although empty categories are very important in theory, it is still unclear that they can help parsing in practice. 3 = l m = r l r l + m r m+1 r m r + l m l m = + Figure 2: The DP st"
K17-1035,P09-1039,0,0.0276317,"as trapezoids, and sibling spans as rectangles. A new dependency is created by applying the last rule. Especially, the score associated with the last rule is determined by a sibling part. For brevity, we elide the right-headed versions. = l The Existing Parsing Algorithms m r l n m + m r Figure 3: The modified construction rule for the tri-sibling algorithm. Data-driven dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transition-based (Yamada and Matsumoto, 2003; Nivre, 2008; Andor et al., 2016) and graph-based (McDonald, 2006; Torres Martins et al., 2009; Lei et al., 2014) models have attracted the most attention of dependency parsing in recent years. A graph-based system explicitly parameterizes models over substructures of a dependency tree, and formulates parsing as a Maximum Spanning Tree problem (McDonald et al., 2005). A number of dynamic programming (DP) algorithms have been designed. Here we summarize the design of two widely used algorithms for second- and third-order factorization, since it is the basis of our new algorithms. 3.1 r l second-order sibling parts, McDonald and Pereira (2006) extended Eisner’s algorithm with a third str"
K17-1035,P03-1055,0,\N,Missing
K18-1054,W13-2322,0,0.0628345,"Missing"
K18-1054,P17-1193,1,0.269538,"ience and Technology, Peking University ♠ The MOE Key Laboratory of Computational Linguistics, Peking University ♥ Center for Chinese Linguistics, Peking University {yufei.chen,huangsheng,foundwang,ws,wanxiaojun}@pku.edu.cn Abstract There are two key dimensions of the data-driven dependency parsing approach: decoding and disambiguation. Existing decoding approaches to syntactic or semantic analysis into bilexical dependencies can be categorized into two dominant types: transition-based (Zhang et al., 2016; Wang et al., 2018) and graph-based, i.e., Maximum Subgraph (Kuhlmann and Jonsson, 2015; Cao et al., 2017a) approaches. For disambiguation, while early work on dependency parsing focused on global linear models, e.g., structured perceptron (Collins, 2002), recent work shows that deep learning techniques, e.g., LSTM (Hochreiter and Schmidhuber, 1997), is able to significantly advance the state-of-the-art of the parsing accuracy. From the above two perspectives, i.e., the decoding and disambiguation frameworks, we find that what is still underexploited is neural Maximum Subgraph parsing for highly constrained graph classes, e.g., noncrossing graphs. In this paper, we fill this gap in the literature"
K18-1054,D17-1003,1,0.59271,"ience and Technology, Peking University ♠ The MOE Key Laboratory of Computational Linguistics, Peking University ♥ Center for Chinese Linguistics, Peking University {yufei.chen,huangsheng,foundwang,ws,wanxiaojun}@pku.edu.cn Abstract There are two key dimensions of the data-driven dependency parsing approach: decoding and disambiguation. Existing decoding approaches to syntactic or semantic analysis into bilexical dependencies can be categorized into two dominant types: transition-based (Zhang et al., 2016; Wang et al., 2018) and graph-based, i.e., Maximum Subgraph (Kuhlmann and Jonsson, 2015; Cao et al., 2017a) approaches. For disambiguation, while early work on dependency parsing focused on global linear models, e.g., structured perceptron (Collins, 2002), recent work shows that deep learning techniques, e.g., LSTM (Hochreiter and Schmidhuber, 1997), is able to significantly advance the state-of-the-art of the parsing accuracy. From the above two perspectives, i.e., the decoding and disambiguation frameworks, we find that what is still underexploited is neural Maximum Subgraph parsing for highly constrained graph classes, e.g., noncrossing graphs. In this paper, we fill this gap in the literature"
K18-1054,W02-1001,0,0.179089,"Missing"
K18-1054,P16-2058,0,0.060807,"Missing"
K18-1054,P15-1149,1,0.908825,"Missing"
K18-1054,S14-2082,0,0.0313727,"ture of the network when processing He wants to go. The upper-left nonlinear transform is used for edge scoring while the upper right one is used for label scoring. L ABEL(i, j) = arg max W2 · ReLU(W1,1 · ri + W1,2 · rj + b) + b2 candidate dependencies as well as their relation types. Figure 4 shows the architecture of our system. We can see here the two local score functions explicitly utilize the positions of a semantic head and a semantic dependent. It is similar to the firstorder factorization as defined in a number of linear parsing models, e.g., the models defined by Martins and Almeida (2014) and Cao et al. (2017a). 3.3.2 Dense Representations We use words as well as POS tags as clues for scoring an individual arc. In particular, we transform all of them into continuous and dense vectors. Inspired by Costa-juss`a and Fonollosa (2016)’s work, we utilize character-based embedding for low-frequency words, i.e., words that appear more than k times in the training data, and word-based embeddings for other words. The word-based embedding module applies the common lookup-table mechanism, while the character-based word embedding wi is implemented by extracting the features (denoted as c1"
K18-1054,flickinger-etal-2010-wikiwoods,0,0.122158,"uracy, it usually performs rather poorly on the out-of-domain data (Oepen et al., 2015). How to build robust semantic dependency parsers that can learn across domains remains an under-addressed problem. To improve the cross-domain parsing performance, we propose a data-oriented model to explore the linguistic generality encoded in a hand-crafted, domainindependent, linguistically-precise English grammar, namely English Resource Grammar (ERG; Flickinger, 2000). In particular, we introduce a cost-sensitive training model to learn crossdomain semantic information implicitly encoded in WikiWoods (Flickinger et al., 2010), i.e., a corpus that collects the wikipedia1 texts as well as their automatic syntactico-semantic annotations produced by ERG. Evaluation demonstrates the usefulness of the imperfect annotations automatically created by ERG. Our parser is available at https://github. com/draplater/msg-parser. 2 ARG2 ARG1 ARG2 BV The company that Mark wants to buy is broken Figure 1: A fragment of a semantic dependency graph. of more than one predicate; (2) cycles are allowed if the direction of arcs are not taken into account. 2.2 Semantic Dependency Analysis SDP is the task of mapping a natural language sent"
K18-1054,S15-2153,0,0.159321,"Missing"
K18-1054,N09-2054,0,0.0141939,"select 480,564 sentences (5,346,703 tokens) from WikiWoods to train another model, and leave out other parts of Redwoods. The performance improvement is more remarkable when providing more data, even though such data contains annotation errors. For the second group of experiments, we use the RedwoodsWOD sentences for training and the DeepBank WSJ sentences for evaluation. For this set-up, consistent improvements of the parser quality are observed. 5.6 Chinese POS tagging has a great impact on parsing. In this paper, we consider two POS taggers: a symbol-refined generative HMM tagger (SR-HMM) (Huang et al., 2009) and a BiLSTMCRF model when assisting Chinese SDG. For the neural tagging model, in addition to a BiLSTM layer for encoding words, we set a BiLSTM layer for encoding characters, which supports us to derive character-level representations for all words. In particular, vectors from the characterlevel LSTM is concatenated with the pre-trained word embedding before feeding into the other word-level BiLSTM network to capture contextual information. The final module of our CRF tagger is a linear chain CRF which scores the output sequence by factoring it in local tag bi-grams. From Table 5, we can se"
K18-1054,S14-2008,0,0.0373675,"ment relationships for all content words. Such sentence-level semantic analysis of text is concerned with the characterization of events and is therefore important to understand the essential meaning of a natural language sentence. With the advent of many supporting resources, SDP has become a well-defined task with a substantial body of work and comparative evaluation. (Almeida and Martins, 2015; Du et al., 2015a; Zhang et al., 2016; Peng et al., 2017; Wang et al., 2018). Two SDP shared tasks have been run as part of the 2014 and 2015 International Workshops on Semantic Evaluation (SemEval) (Oepen et al., 2014, 2015). 562 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 562–572 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics ARG2 published result reported in Zhang et al. (2016) and Du et al. (2015b). Most studies on semantic parsing focused on the in-domain setting, meaning that both training and testing data are drawn from the same domain. Even a data-driven parsing system achieves a high in-domain accuracy, it usually performs rather poorly on the out-of-domain data (Oepen et al., 2015). How to bui"
K18-1054,C02-2025,0,0.0168485,"” annotations (RedwoodsWOD for short) as well as the official WikiWoods annotations. Note that the MaxEnt model used to obtain the official WikiWoods annotations are compatible with RedwoodswWOD. Due to the diversity of the RedwoodsWOD and DeepBank sentences, this set-up can also be viewed as an outof-domain evaluation. Data for Cross-Domain Experiments Since around 2001, the ERG has been accompanied by syntactico-semantic annotations, where for each sentence an annotator has selected the intended analysis among all alternatives licensed by the grammar. This derived resource, namly Redwoods6 (Oepen et al., 2002; Flickinger et al., 2017), is a collection of hand-annotated corpora and consists of data sets from several distinct domains. Redwoods also includes (re)treebanking results of the first 22 sections of the venerable Wall Street Journal (WSJ) text and the section of Brown Corpus in the Penn Treebank (Marcus et al., 1993). The WSJ part is also known as Deep6 5.5 Results of Cross-Domain Parsing Table 3 summarizes experimental results for different cross-domain evaluation set-ups. For the 7 8 http://moin.delph-in.net/RedwoodsTop 568 http://moin.delph-in.net/WikiWoods https://github.com/delph-in/py"
K18-1054,W13-5707,0,0.0606194,"Missing"
K18-1054,P17-1186,0,0.0639981,"Missing"
K18-1054,W12-3602,0,0.0252342,"ions automatically created by ERG. Our parser is available at https://github. com/draplater/msg-parser. 2 ARG2 ARG1 ARG2 BV The company that Mark wants to buy is broken Figure 1: A fragment of a semantic dependency graph. of more than one predicate; (2) cycles are allowed if the direction of arcs are not taken into account. 2.2 Semantic Dependency Analysis SDP is the task of mapping a natural language sentence into a formal meaning representation in the form of a dependency graph. Figure 1 shows an Minimal Recursion Semantics (MRS; Copestake et al., 2005) reduced semantic dependency analysis (Ivanova et al., 2012). In this example, the semantic analysis is represented as a labeled directed graph in which the vertices are tokens in the sentence. The graph abstracts away from syntactic analysis (e.g., the complementizer—that— and passive construction are excluded) and includes most semantically relevant non-anaphoric local (e.g., from “wants” to “Mark”) and longdistance (e.g., from “buy” to “company”) dependencies. The arc labels encode linguisticallymotivated, broadly-applicable semantic relations that are grounded under the type-driven semantics. It is worth noting that semantic dependency graphs are n"
K18-1054,Q16-1023,0,0.159873,"he Architecture A semantic graph mainly consists of two parts: the structural part and the label part. The former describes the predicate–argument relation in the sentence, and the latter describes the type of this relation. In our model, the structural part and the label part are regarded as independent of each other. We use a coarse-to-fine strategy: finding the maximum unlabeled subgraph first and assigning a label for every edge in this subgraph then. The motivation is to avoid the calculation of a number of unnecessary label scores in order to improve the processing efficiency. Following Kiperwasser and Goldberg (2016)’s successful experience on syntactic tree parsing and Peng et al. (2017)’s experience on semantic graph parsing, we employ a stacked bidirectional-LSTM (BiLSTM) based model to assign scores. In our system, the BiLSTM vectors associated with the input words are utilized to calculate scores for the Parsing to 1 EC / P 2 Graphs Previous work showed that the Maximum Subgraph framework is not only elegant in theory but also effective in practice (Kuhlmann and Jonsson, 2015; Cao et al., 2017a,b). In particular, 1 EC / P 2 graphs are an appropriate graph class for modeling semantic dependency struct"
K18-1054,Q13-1002,0,0.0223189,"ted with the input words are utilized to calculate scores for the Parsing to 1 EC / P 2 Graphs Previous work showed that the Maximum Subgraph framework is not only elegant in theory but also effective in practice (Kuhlmann and Jonsson, 2015; Cao et al., 2017a,b). In particular, 1 EC / P 2 graphs are an appropriate graph class for modeling semantic dependency structures (Cao et al., 2017a). Figure 2 presents an example to illustrate the 1-endpoint-crossing property, while Figure 3 shows a case for pagenumber-2. Below we present the formal description of the two properties that are adopted from Pitler et al. (2013) and Kuhlmann and Jonsson (2015) respectively. 564 LSTM LSTM ... LSTM ... 3.3.4 Factorized Scoring In our first order model, the S CORE function evaluates the preference of a semantic dependency graph by considering every bilexical relation in this graph one by one. In particular, the corresponding S CORE PART function assigns a score to a candidate arc between word i and word j using a non-linear transform from the two feature vectors, viz. ri and rj , associated to the two words: S CORE PART(i, j) = W2 · ReLU(W1,1 · ri + W1,2 · rj + b) He PRP wants VBZ go VB The assignment task for dependenc"
K18-1054,C10-1122,0,0.0859428,"he BiLSTM based disambiguation model. The preciExperiments Set-up for the Baseline System To evaluate neural Maximum Subgraph parsing in practice, we first conduct experiments on the three English data sets, namely DM, PAS and PSD4 , which are from the SemEval 2015 Task18 (Oepen et al., 2015). We use the “standard” training, validation, and test splits to facilitate comparisons. In other words, the data splitting policy follows the shared task. In addition to English parsing, we consider Chinese SDP and use two data sets: (1) Chinese PAS data provided by SemEval 2015, and (2) Chinese CCGBank (Tse and Curran, 2010) to evaluate the cross-lingual ability of our model. All the SemEval data sets are publicly available from 4 DM, PAS and PSD are short for DeepBank, Enju HPSGBank and Prague Dependency Treebank. 5 567 https://github.com/clab/dynet 94 90 Labeled F-score Bank (Flickinger et al., 2012). The Brown corpus part is used as the out-of-domain test data by SemEval 2015. The DM data sets for both SemEval 2014 and 2015 SDP shared tasks are based on the RedWoods corpus. Besides gold standard annoations, Flickinger et al. (2010) built the WikiWoods corpus7 , which provides automatically created annotations"
K18-1054,C10-2162,0,0.0319857,"eepBank S 85.70 85.02 Redwoods S 86.28 84.85 DeepBank+WikiWoods-ACE S 88.30 86.42 DeepBank+WikiWoods-ACE E[3] 89.53 87.57 O UT- OF -D OMAIN (R EDWOODS WOD) DeepBank S 90.74 90.40 RedwoodsWOD S 81.40 78.99 RedwoodsWOD+WikiWoods S 84.05 79.86 RedwoodsWOD+WikiWoods E[3] 84.84 81.02 LF 90.57 91.03 91.32 92.11 85.37 85.56 87.35 88.54 90.57 80.18 81.90 82.88 Table 3: Labeled F1 on the DM test sets. “S” denotes single model, while “E[3]” denotes ensemble model with 3 sub-models. set-up as Zhang et al. (2016). Both data sets are transformed from Chinese TreeBank with two rich sets of heuristic rules (Yu et al., 2010; Tse and Curran, 2010). Table 4 and 5 presents all results. Our parser significantly outperforms Zhang et al. (2016)’s Zhang et al. (2016) system on Chinese CCGBank, which achieved best reported performance. first group of experiments, we test the parser using different training data sets. The baseline utilizes the WSJ portion only. While more reliable training data is added, the performances increase consistently. We notice that the improvement extending the training data from DeepBank to Redwoods is quite limited for the out-of-domain evaluation. One reason is that the amount of enlarged go"
K18-1054,J16-3001,1,0.852396,"Analysis Yufei Chen♠ , Sheng Huang♠ , Fang Wang♠ , Weiwei Sun♠♥ and Xiaojun Wan♠ ♠ Institute of Computer Science and Technology, Peking University ♠ The MOE Key Laboratory of Computational Linguistics, Peking University ♥ Center for Chinese Linguistics, Peking University {yufei.chen,huangsheng,foundwang,ws,wanxiaojun}@pku.edu.cn Abstract There are two key dimensions of the data-driven dependency parsing approach: decoding and disambiguation. Existing decoding approaches to syntactic or semantic analysis into bilexical dependencies can be categorized into two dominant types: transition-based (Zhang et al., 2016; Wang et al., 2018) and graph-based, i.e., Maximum Subgraph (Kuhlmann and Jonsson, 2015; Cao et al., 2017a) approaches. For disambiguation, while early work on dependency parsing focused on global linear models, e.g., structured perceptron (Collins, 2002), recent work shows that deep learning techniques, e.g., LSTM (Hochreiter and Schmidhuber, 1997), is able to significantly advance the state-of-the-art of the parsing accuracy. From the above two perspectives, i.e., the decoding and disambiguation frameworks, we find that what is still underexploited is neural Maximum Subgraph parsing for hig"
K18-1054,P09-1043,0,0.0903088,"Missing"
K19-1097,J82-2005,0,0.707381,"Missing"
K19-1097,P18-1073,0,0.265156,"Fully Unsupervised Cross-lingual Sentiment Analysis Yanlin Feng and Xiaojun Wan Wangxuan Institute of Computer Science and Technology, Peking University The MOE Key Laboratory of Computational Linguistics, Peking University {fengyanlin,wanxiaojun}@pku.edu.cn Abstract the source language to the target language. CLWE encodes words from multiple languages in a common space, thus making it possible to share a classifier across languages. Recent studies have shown that CLWE can be obtained in an unsupervised way, i.e., without any cross-lingual resources (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). This motivates fully unsupervised CLSA approaches (Chen et al., 2018a) that do not rely on either target language supervision or cross-lingual supervision. These methods generally involve the following steps: Sentiment analysis in low-resource languages suffers from the lack of training data. Crosslingual sentiment analysis (CLSA) aims to improve the performance on these languages by leveraging annotated data from other languages. Recent studies have shown that CLSA can be performed in a fully unsupervised manner, without exploiting either target language supervision or cross-lingual supervi"
K19-1097,P10-1114,0,0.589448,") to compute the adversarial loss. The LSTM layers are trained to fool the language discriminator so that it cannot distinguish the examples in (ls , ds ) from those in (lt , dt ). We jointly optimize three language modeling objectives for each language-domain pair, the adversarial objective, and a sentiment classification objective for (ls , ds ). Books DVD Music EN 50000 30000 25220 DE 165470 91516 60392 FR 32870 9358 15940 JA 169780 68326 55892 Table 1: Number of unlabeled examples in the Amazon dataset. Datasets We evaluate our model on the multilingual multidomain Amazon review dataset (Prettenhofer and Stein, 2010) which contains product reviews in four languages (English, French, German, Japanese) and three domains (Books, DVD, Music). For each language-domain pair, there are 2000 examples for training and 2000 examples for testing. The statistics of unlabeled data is summarized in Table 1. For cross-lingual in-domain sentiment analysis, we use English as the source language and the others as target languages, resulting in nine tasks in total. For cross-lingual crossdomain sentiment analysis, we follow the setting in (Ziser and Reichart, 2018) and use English as the source language, French and German a"
K19-1097,P18-1231,0,0.0738313,"o and Guo, 2012; Zhou et al., 2016a) to provide cross-lingual supervision, making themselves implicitly dependant on largescale parallel corpus which may not be available for low-resource languages. Wan (2009) apply the co-training algorithm to translated data while other researchers have proposed multi-view learning (Xiao and Guo, 2012). Another line of CLSA research bridges the language gap using CLWE, which saves the efforts of training a machine translation system thus requires less cross-lingual resources. Some work has proposed to map pretrained monolingual embeddings to a shared space (Barnes et al., 2018) to obtain CLWE while others proposed jointly learning CLWE and a sentiment classifier, allowing the embeddings to encode sentiment information (Zhou et al., 2016b; Xu and Wan, 2017). Very recently, unsupervised CLSA methods that do not require either cross-lingual supervision or target language supervision have been proposed (Chen et al., 2018b,a). Chen et al. (2018a) transfer sentiment information from multiple source languages by jointly learning language invariant and language specific features. Yet, these unsupervised CLSA methods rely on unsupervised CLWE which builds on the assumption t"
K19-1097,P18-1072,0,0.0454442,"Missing"
K19-1097,Q18-1039,0,0.433134,"un Wan Wangxuan Institute of Computer Science and Technology, Peking University The MOE Key Laboratory of Computational Linguistics, Peking University {fengyanlin,wanxiaojun}@pku.edu.cn Abstract the source language to the target language. CLWE encodes words from multiple languages in a common space, thus making it possible to share a classifier across languages. Recent studies have shown that CLWE can be obtained in an unsupervised way, i.e., without any cross-lingual resources (Zhang et al., 2017; Conneau et al., 2017; Artetxe et al., 2018). This motivates fully unsupervised CLSA approaches (Chen et al., 2018a) that do not rely on either target language supervision or cross-lingual supervision. These methods generally involve the following steps: Sentiment analysis in low-resource languages suffers from the lack of training data. Crosslingual sentiment analysis (CLSA) aims to improve the performance on these languages by leveraging annotated data from other languages. Recent studies have shown that CLSA can be performed in a fully unsupervised manner, without exploiting either target language supervision or cross-lingual supervision. However, these methods rely heavily on unsupervised cross-lingua"
K19-1097,P09-1027,1,0.723708,"oss-lingual language modeling based methods are able to outperform CLWE based methods in the unsupervised setting. 3. Our model can be easily generalized to different CLSA settings. Experiments on the multilingual multi-domain Amazon review dataset show that our method achieves state of the art in both the cross-lingual in-domain setting and the cross-lingual cross-domain setting despite its minimal resource requirement. 2 Related Work Cross-lingual Sentiment Analysis The most related topic to our work is cross-lingual sentiment analysis. Some CLSA methods rely on machine translation systems (Wan, 2009; Demirtas and Pechenizkiy, 2013; Xiao and Guo, 2012; Zhou et al., 2016a) to provide cross-lingual supervision, making themselves implicitly dependant on largescale parallel corpus which may not be available for low-resource languages. Wan (2009) apply the co-training algorithm to translated data while other researchers have proposed multi-view learning (Xiao and Guo, 2012). Another line of CLSA research bridges the language gap using CLWE, which saves the efforts of training a machine translation system thus requires less cross-lingual resources. Some work has proposed to map pretrained monol"
K19-1097,C12-1174,0,0.332085,"are able to outperform CLWE based methods in the unsupervised setting. 3. Our model can be easily generalized to different CLSA settings. Experiments on the multilingual multi-domain Amazon review dataset show that our method achieves state of the art in both the cross-lingual in-domain setting and the cross-lingual cross-domain setting despite its minimal resource requirement. 2 Related Work Cross-lingual Sentiment Analysis The most related topic to our work is cross-lingual sentiment analysis. Some CLSA methods rely on machine translation systems (Wan, 2009; Demirtas and Pechenizkiy, 2013; Xiao and Guo, 2012; Zhou et al., 2016a) to provide cross-lingual supervision, making themselves implicitly dependant on largescale parallel corpus which may not be available for low-resource languages. Wan (2009) apply the co-training algorithm to translated data while other researchers have proposed multi-view learning (Xiao and Guo, 2012). Another line of CLSA research bridges the language gap using CLWE, which saves the efforts of training a machine translation system thus requires less cross-lingual resources. Some work has proposed to map pretrained monolingual embeddings to a shared space (Barnes et al.,"
K19-1097,D17-1053,1,0.742995,"ce languages. Wan (2009) apply the co-training algorithm to translated data while other researchers have proposed multi-view learning (Xiao and Guo, 2012). Another line of CLSA research bridges the language gap using CLWE, which saves the efforts of training a machine translation system thus requires less cross-lingual resources. Some work has proposed to map pretrained monolingual embeddings to a shared space (Barnes et al., 2018) to obtain CLWE while others proposed jointly learning CLWE and a sentiment classifier, allowing the embeddings to encode sentiment information (Zhou et al., 2016b; Xu and Wan, 2017). Very recently, unsupervised CLSA methods that do not require either cross-lingual supervision or target language supervision have been proposed (Chen et al., 2018b,a). Chen et al. (2018a) transfer sentiment information from multiple source languages by jointly learning language invariant and language specific features. Yet, these unsupervised CLSA methods rely on unsupervised CLWE which builds on the assumption that pretrained monolingual embeddings can be properly aligned. This assumption, however, is not true in low-resource scenarios (Søgaard et al., 2018). It is worth pointing out that t"
K19-1097,P17-1130,0,0.281389,"-SCL Prettenhofer and Stein (2010) map the bag-of-word representations to a cross-lingual space via structural correspondence learning. BiDRL Zhou et al. (2016b) learn bilingual document representation for CLSA. The authors translate each document into both languages and enforce a bilingual constraint between the original document and the translated version. UMM Xu and Wan (2017) jointly learn multilingual word embeddings and a sentiment classifier using parallel corpora of multiple language pairs. Languages that do not have direct parallel corpus are bridged via a third pivot language. CLDFA Xu and Yang (2017) propose crosslingual distillation using translated reviews. MAN-MoE Chen et al. (2018a) propose the state-of-the-art unsupervised CLSA model that learns language invariant features and language specific features. It relies on unsupervised CLWE for cross-lingual transfer. Unlike other CLSA approaches, it transfers the sentiment information from multiple source languages. MWE This is a variant of our proposed model that relies on unsupervised CLWE instead of language modeling. We map all target language embeddings to the English space using the MUSE library (Conneau et al., 2017) and use them t"
K19-1097,D18-1268,0,0.0527014,"ual in-domain setting and the more challenging cross-lingual cross-domain setting. We empirically evaluate our approach on the multilingual multi-domain Amazon review dataset. Experimental results show that our model outperforms the baselines by a large margin despite its minimal resource requirement. 1 1 1. Train monolingual embeddings separately on multiple languages using monolingual unlabeled data. 2. Map the monolingual embeddings to a shared space using unsupervised CLWE methods, either adversarial training methods (Conneau et al., 2017) or non-adversarial methods (Artetxe et al., 2018; Xu et al., 2018). 3. Train a sentiment classifier using the annotated corpus in the source language. Introduction While English sentiment analysis has achieved great success with the help of large-scale annotated corpus, this is not the case for most of languages where only limited data is available. Cross-lingual sentiment analysis (CLSA) tackles this problem by adapting the sentiment resource in a source language to a poor-resource language (the target language). Current state-of-the-art CLSA methods rely heavily on cross-lingual word embeddings (CLWE) to transfer sentiment information from 1 The source cod"
K19-1097,P17-1179,0,0.0532997,"Missing"
K19-1097,D16-1024,1,0.755649,"Missing"
K19-1097,P16-1133,1,0.87631,"Missing"
K19-1097,D18-1022,0,0.401359,"ture extractor, a sentiment classifier and a language discriminator. The feature extractor is trained to fool the discriminator so that the extracted features are language invariant. However, its performance is significantly lower than the variant that uses pretrained CLWE. While traditional CLSA methods assume that data in both languages is within the same domain (e.g. English hotel reviews for training and Chinese hotel review for testing, we refer to this setting as “cross-lingual in-domain sentiment analysis”), the more challenging cross-lingual crossdomain setting has also been explored. Ziser and Reichart (2018) extend pivot-based monolingual domain adaption methods to the cross-lingual setting. However, their method is not unsupervised and requires expensive cross-lingual resources. 1036 Cross-lingual Language Modeling Our work is also related to cross-lingual language modeling, which is a topic that has been explored by researchers very recently. Lample and Conneau (2019), pretrain a language model with a joint vocabulary on the concatenation of multiple largescale monolingual corpora and finetune it on labeled data. However, this approach exploits crosslingual supervision provided by shared sub-wo"
N06-2046,N03-1020,0,0.174387,"Missing"
N06-2046,P02-1058,0,\N,Missing
N06-2046,I05-2004,0,\N,Missing
N06-2046,W04-3247,0,\N,Missing
N19-1040,P15-1119,0,0.0324348,"method in Section 4. Xu and Wan (2017) learned multilingual sentimental embeddings by extending the BiSkip model (Luong et al., 2015). However, their method does not apply to pretrained embeddings and requires large-scale parallel corpora thus is not Bilingual Word Embeddings Word embeddings trained separately on two languages can be aligned in a shared space to produce Bilingual Word Embeddings (BWE), which support many NLP tasks including machine translation (Lample et al., 2017), cross-lingual sentiment analysis (Barnes et al., 2018; Zhou et al., 2015) and crosslingual dependency parsing (Guo et al., 2015). BWE can be obtained in a supervised way using a seed dictionary (Joulin et al., 2018; Artetxe et al., 421 included in our experiments. 3 word pairs in the dictionary to have similar representations in the bilingual space. In the unsupervised case, such a dictionary can be induced from the monolingual embeddings S and T (Artetxe et al., 2018). However, the quality of this dictionary is usually not good, which in turn degrades the quality of the projection matrices learned from this dictionary. Previous work (Artetxe et al., 2017, 2018) showed that an iterative self-learning procedure can indu"
N19-1040,D18-1024,0,0.0423277,"t results. 2. We introduce a novel sentiment-specific objective without having to explicitly build a classifier. Our approach is more explainable and better balances sentimental similarity and semantic similarity compared to previous approaches. Multilingual Word Embeddings BWE methods can be extended to the case of multiple languages by simply mapping all the languages to the vector space of a selected language. However, directly learning multilingual word embeddings (M WE) in a shared space has been shown to improve performance (Ammar et al., 2016; Duong et al., 2017; Chen and Cardie, 2018; Alaux et al., 2018). Yet, all these approaches are mainly evaluated on word translation and their effectiveness on cross-lingual sentiment analysis have not been empirically compared. 3. We introduce an alignment-specific objective and a simple re-normalization trick. Unlike previous BWE methods that learn orthogonal mappings, we introduce non-orthogonal mappings which enable the transfer of sentiment information from the source language to the target language. 2 Related Work Cross-Lingual Sentiment Analysis Existing approaches for cross-lingual sentiment analysis can be mainly divided into two categories: (i) a"
N19-1040,P15-1162,0,0.0555856,"Missing"
N19-1040,D16-1250,0,0.0253382,"Normalize the rows of S0 , T0 to unit length return S0 , T0 , Ws , Wt 4.3.1 Unsupervised BWE Methods ADVERSARIAL Conneau et al. (2017) proposed an unsupervised BWE method based on adversarial training. After a near-orthogonal projection matrix is learned through adversarial training, a refinement procedure is applied to improve the quality of the alignment. VECMAP Artetxe et al. (2018) proposed an unsupervised BWE learning framework. It consists of an unsupervised dictionary initialization step and the self-learning procedure mentioned in Section 3.1.2. 4.3.2 Supervised BWE Methods PROCRUSTES Artetxe et al. (2016) proposed a simple and effective supervised BWE method that requires a seed dictionary. It computes the optimal projection matrix by taking singular value decomposition (SVD). The normalized 300-dimension fastText vectors (Bojanowski et al., 2017) are used by all methods. The MUSE dataset(Conneau et al., 2017) is used by approaches that require bilingual supervision 4 . Each dictionary contains 5000 unique source words. 4.2 RCSLS Joulin et al. (2018) proposed an supervised BWE method that also requires a seed dictionary. They proposed a training objective that is consistent with the retieval c"
N19-1040,D18-1330,0,0.0263465,"Missing"
N19-1040,P17-1042,0,0.0994954,"e a novel approach to learn bilingual sentiment-specific word embeddings without any cross-lingual supervision and perform cross-lingual sentiment analysis with minimum resource requirement. We propose an iterative constraint relaxation pro420 Proceedings of NAACL-HLT 2019, pages 420–429 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2016), or in an unsupervised way without any bilingual data. Adversarial training was the first successful attempt to learn unsupervised BWE (Zhang et al., 2017; Conneau et al., 2017). Selflearning was proposed by (Artetxe et al., 2017)to learn BWE with minimum bilingual resources, which was later extended into a fully unsupervised framework by adding an unsupervised dictionary initialization step (Artetxe et al., 2018). cedure that gradually incorporates the sentiment information into the BWE. Our proposed approach achieves state-of-the-art results. 2. We introduce a novel sentiment-specific objective without having to explicitly build a classifier. Our approach is more explainable and better balances sentimental similarity and semantic similarity compared to previous approaches. Multilingual Word Embeddings BWE methods can"
N19-1040,P18-1073,0,0.298696,"nt. We propose an iterative constraint relaxation pro420 Proceedings of NAACL-HLT 2019, pages 420–429 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2016), or in an unsupervised way without any bilingual data. Adversarial training was the first successful attempt to learn unsupervised BWE (Zhang et al., 2017; Conneau et al., 2017). Selflearning was proposed by (Artetxe et al., 2017)to learn BWE with minimum bilingual resources, which was later extended into a fully unsupervised framework by adding an unsupervised dictionary initialization step (Artetxe et al., 2018). cedure that gradually incorporates the sentiment information into the BWE. Our proposed approach achieves state-of-the-art results. 2. We introduce a novel sentiment-specific objective without having to explicitly build a classifier. Our approach is more explainable and better balances sentimental similarity and semantic similarity compared to previous approaches. Multilingual Word Embeddings BWE methods can be extended to the case of multiple languages by simply mapping all the languages to the vector space of a selected language. However, directly learning multilingual word embeddings (M W"
N19-1040,W15-1521,0,0.0273764,"h languages then aligning them in a common space. However, it requires sentiment resources in the target language thus is impractical for low-resource languages. There are also approaches to learn sentimental embeddings in the bilingual space without any sentiment resources in the target language. Barnes et al. (2018) jointly minimized an alignment objective based on a seed dictionary, and a classification objective based on the sentiment corpus. Its performance is compared to our method in Section 4. Xu and Wan (2017) learned multilingual sentimental embeddings by extending the BiSkip model (Luong et al., 2015). However, their method does not apply to pretrained embeddings and requires large-scale parallel corpora thus is not Bilingual Word Embeddings Word embeddings trained separately on two languages can be aligned in a shared space to produce Bilingual Word Embeddings (BWE), which support many NLP tasks including machine translation (Lample et al., 2017), cross-lingual sentiment analysis (Barnes et al., 2018; Zhou et al., 2015) and crosslingual dependency parsing (Guo et al., 2015). BWE can be obtained in a supervised way using a seed dictionary (Joulin et al., 2018; Artetxe et al., 421 included"
N19-1040,P18-1231,0,0.667037,"sentiment analysis. Tang et al. (2014) learned word representations that encode both syntactic context and sentiment polarity by adding an objective to classify the polarity of an n-gram. This method can be generalized to the cross-lingual setting by training monolingual sentimental embeddings on both languages then aligning them in a common space. However, it requires sentiment resources in the target language thus is impractical for low-resource languages. There are also approaches to learn sentimental embeddings in the bilingual space without any sentiment resources in the target language. Barnes et al. (2018) jointly minimized an alignment objective based on a seed dictionary, and a classification objective based on the sentiment corpus. Its performance is compared to our method in Section 4. Xu and Wan (2017) learned multilingual sentimental embeddings by extending the BiSkip model (Luong et al., 2015). However, their method does not apply to pretrained embeddings and requires large-scale parallel corpora thus is not Bilingual Word Embeddings Word embeddings trained separately on two languages can be aligned in a shared space to produce Bilingual Word Embeddings (BWE), which support many NLP task"
N19-1040,Q17-1010,0,0.0597561,"is learned through adversarial training, a refinement procedure is applied to improve the quality of the alignment. VECMAP Artetxe et al. (2018) proposed an unsupervised BWE learning framework. It consists of an unsupervised dictionary initialization step and the self-learning procedure mentioned in Section 3.1.2. 4.3.2 Supervised BWE Methods PROCRUSTES Artetxe et al. (2016) proposed a simple and effective supervised BWE method that requires a seed dictionary. It computes the optimal projection matrix by taking singular value decomposition (SVD). The normalized 300-dimension fastText vectors (Bojanowski et al., 2017) are used by all methods. The MUSE dataset(Conneau et al., 2017) is used by approaches that require bilingual supervision 4 . Each dictionary contains 5000 unique source words. 4.2 RCSLS Joulin et al. (2018) proposed an supervised BWE method that also requires a seed dictionary. They proposed a training objective that is consistent with the retieval criterion that can be minimized by using gradient descent. It achieves state-of-the-art results on the word translation task. Implementation details We empirically set ∆r = 0.01 and v = 10000. The vocabulary of each language is limited to the v mos"
N19-1040,P18-1072,0,0.0415802,"Missing"
N19-1040,P14-1146,0,0.0163167,"16b,a) performed crosslingual sentiment analysis by learning bilingual document representations. These methods translate each document into the other language and enforce a bilingual constraint between the original document and the translated version. Sentimental Embeddings Continuous word representations encode the syntactic context of a word but often ignore the information of sentiment polarity. This drawback makes them hard to distinguish words with similar syntactic context but opposite sentiment polarity (e.g. good and bad), resulting in unsatisfactory performance on sentiment analysis. Tang et al. (2014) learned word representations that encode both syntactic context and sentiment polarity by adding an objective to classify the polarity of an n-gram. This method can be generalized to the cross-lingual setting by training monolingual sentimental embeddings on both languages then aligning them in a common space. However, it requires sentiment resources in the target language thus is impractical for low-resource languages. There are also approaches to learn sentimental embeddings in the bilingual space without any sentiment resources in the target language. Barnes et al. (2018) jointly minimized"
N19-1040,P09-1027,1,0.713599,"ntroduce non-orthogonal mappings which enable the transfer of sentiment information from the source language to the target language. 2 Related Work Cross-Lingual Sentiment Analysis Existing approaches for cross-lingual sentiment analysis can be mainly divided into two categories: (i) approaches that rely on machines translation (MT) systems (ii) approaches that rely on cross-lingual word embeddings. Standard MT-based approaches perform crosslingual sentiment analysis by translating the sentiment data into a selected language (e.g. English). More sophisticated algorithms including co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) and multi-view learning (Xiao and Guo, 2012) have been shown to improve performance. Zhou et al. (2015, 2016b,a) performed crosslingual sentiment analysis by learning bilingual document representations. These methods translate each document into the other language and enforce a bilingual constraint between the original document and the translated version. Sentimental Embeddings Continuous word representations encode the syntactic context of a word but often ignore the information of sentiment polarity. This drawback makes them hard to distinguish words with si"
N19-1040,E17-1084,0,0.0223476,"proposed approach achieves state-of-the-art results. 2. We introduce a novel sentiment-specific objective without having to explicitly build a classifier. Our approach is more explainable and better balances sentimental similarity and semantic similarity compared to previous approaches. Multilingual Word Embeddings BWE methods can be extended to the case of multiple languages by simply mapping all the languages to the vector space of a selected language. However, directly learning multilingual word embeddings (M WE) in a shared space has been shown to improve performance (Ammar et al., 2016; Duong et al., 2017; Chen and Cardie, 2018; Alaux et al., 2018). Yet, all these approaches are mainly evaluated on word translation and their effectiveness on cross-lingual sentiment analysis have not been empirically compared. 3. We introduce an alignment-specific objective and a simple re-normalization trick. Unlike previous BWE methods that learn orthogonal mappings, we introduce non-orthogonal mappings which enable the transfer of sentiment information from the source language to the target language. 2 Related Work Cross-Lingual Sentiment Analysis Existing approaches for cross-lingual sentiment analysis can"
N19-1040,C12-1174,0,0.0257502,"ent information from the source language to the target language. 2 Related Work Cross-Lingual Sentiment Analysis Existing approaches for cross-lingual sentiment analysis can be mainly divided into two categories: (i) approaches that rely on machines translation (MT) systems (ii) approaches that rely on cross-lingual word embeddings. Standard MT-based approaches perform crosslingual sentiment analysis by translating the sentiment data into a selected language (e.g. English). More sophisticated algorithms including co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) and multi-view learning (Xiao and Guo, 2012) have been shown to improve performance. Zhou et al. (2015, 2016b,a) performed crosslingual sentiment analysis by learning bilingual document representations. These methods translate each document into the other language and enforce a bilingual constraint between the original document and the translated version. Sentimental Embeddings Continuous word representations encode the syntactic context of a word but often ignore the information of sentiment polarity. This drawback makes them hard to distinguish words with similar syntactic context but opposite sentiment polarity (e.g. good and bad), r"
N19-1040,D17-1053,1,0.613119,"eneralized to the cross-lingual setting by training monolingual sentimental embeddings on both languages then aligning them in a common space. However, it requires sentiment resources in the target language thus is impractical for low-resource languages. There are also approaches to learn sentimental embeddings in the bilingual space without any sentiment resources in the target language. Barnes et al. (2018) jointly minimized an alignment objective based on a seed dictionary, and a classification objective based on the sentiment corpus. Its performance is compared to our method in Section 4. Xu and Wan (2017) learned multilingual sentimental embeddings by extending the BiSkip model (Luong et al., 2015). However, their method does not apply to pretrained embeddings and requires large-scale parallel corpora thus is not Bilingual Word Embeddings Word embeddings trained separately on two languages can be aligned in a shared space to produce Bilingual Word Embeddings (BWE), which support many NLP tasks including machine translation (Lample et al., 2017), cross-lingual sentiment analysis (Barnes et al., 2018; Zhou et al., 2015) and crosslingual dependency parsing (Guo et al., 2015). BWE can be obtained"
N19-1040,P17-1179,0,0.0248408,"languages in a common space by exploiting either a bilingual 1. We propose a novel approach to learn bilingual sentiment-specific word embeddings without any cross-lingual supervision and perform cross-lingual sentiment analysis with minimum resource requirement. We propose an iterative constraint relaxation pro420 Proceedings of NAACL-HLT 2019, pages 420–429 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2016), or in an unsupervised way without any bilingual data. Adversarial training was the first successful attempt to learn unsupervised BWE (Zhang et al., 2017; Conneau et al., 2017). Selflearning was proposed by (Artetxe et al., 2017)to learn BWE with minimum bilingual resources, which was later extended into a fully unsupervised framework by adding an unsupervised dictionary initialization step (Artetxe et al., 2018). cedure that gradually incorporates the sentiment information into the BWE. Our proposed approach achieves state-of-the-art results. 2. We introduce a novel sentiment-specific objective without having to explicitly build a classifier. Our approach is more explainable and better balances sentimental similarity and semantic similarity c"
N19-1040,P15-1042,0,0.0513498,"age. 2 Related Work Cross-Lingual Sentiment Analysis Existing approaches for cross-lingual sentiment analysis can be mainly divided into two categories: (i) approaches that rely on machines translation (MT) systems (ii) approaches that rely on cross-lingual word embeddings. Standard MT-based approaches perform crosslingual sentiment analysis by translating the sentiment data into a selected language (e.g. English). More sophisticated algorithms including co-training (Wan, 2009; Demirtas and Pechenizkiy, 2013) and multi-view learning (Xiao and Guo, 2012) have been shown to improve performance. Zhou et al. (2015, 2016b,a) performed crosslingual sentiment analysis by learning bilingual document representations. These methods translate each document into the other language and enforce a bilingual constraint between the original document and the translated version. Sentimental Embeddings Continuous word representations encode the syntactic context of a word but often ignore the information of sentiment polarity. This drawback makes them hard to distinguish words with similar syntactic context but opposite sentiment polarity (e.g. good and bad), resulting in unsatisfactory performance on sentiment analys"
N19-1040,D16-1024,1,0.906234,"Missing"
N19-1040,P16-1133,1,0.736434,"Missing"
N19-1092,P92-1051,0,0.68216,"n the corpus. To summarise, the contributions of our work are as follows1 : del. Our model outperforms the baseline models on 3 aspects significantly2 . 2 Related Work Metaphor is highly frequent in language and its computational processing is indispensable for real-world NLP applications addressing semantic tasks. Automatic processing of metaphor can be clearly divided into two subtasks: metaphor identification and metaphor interpretation (Shutova, 2010), little research has been devoted to the metaphor generation. In this subsection, we briefly review some prior work on metaphor generation. Jones (1992) aims to generate a specific class of metaphors: Transparently-Motivated Metaphor, which is based on universal groundings that are often linked to bodily experience. Abe et al. (2006), Terai and Nakagawa (2010) generate metaphors in the form of “A (target) like B (vehicle)”. They firstly compute the probabilistic relationship between concepts and words with a statistical analysis of language data and then select candidates to fill in the template. From a given mapping between the concepts of two domains, Herv´as et al. (2007) present an approach to the application of metaphors for referring to"
N19-1092,W16-1101,0,0.0306634,"application of metaphors for referring to concepts in an automatically generated text. Mason (2004) obtains domainspecific selectional preferences of verbs from large corpora and maps their common nominal arguments in other domains. The corresponding metaphorical mappings are achieved by such systematic variations and can generate simple conceptual metaphors in the form of: “A is (a) B”. Ovchinnikova et al. (2014) also rely on characteristic predicate but use general propositions instead of the verb and adjective phrases. As some metaphors’ target domain is lexically divorced from the source, Lederer (2016) identifies constellations of source-domain triggers in limited source domains. To make the metaphor generators more comprehensible and forceful, Veale (2016) presents a knowledge-base to generate XYZ metaphors such as “Bruce Wayne is the Donald Trump of Gotham City”. Previous methods make groundbreaking explorations on metaphor generation. However, these approaches mainly focus on modeling the phraselevel metaphor expressions and the generation process depends on the templates, which causes the lack of linguistic subtlety to some extent and • As far as we know, our work is the first endeavor"
N19-1092,N10-1039,0,0.217843,"d data available now is still far from training a good language model. To the best of our knowledge, there has been no work combining metaphor generation with the end-toend approach. In this paper, we propose a neural approach for metaphor generation trained with Wiki corpus rather than the limited annotated metaphor corpus, which assures the quality of the language model. The approach is shown in Figure 1. Relevant staIntroduction Metaphor is a kind of language filled with vitality and elasticity. It employs words in a way that deviates from their normal meaning to represent another concept (Li and Sporleder, 2010). Using metaphor allows us to express not just information but also real feelings and complex attitudes. There is a clear need in computational metaphor generation whose insightful results can be used in many applications from entertainment to education (Veale, 2016). Besides, a unified metaphor annotation procedure and creation of a large publicly available metaphor corpus are in great demands. Such resources make it possible to interpretate the identified metaphorical expressions and enhance the performance on other Natural Language Processing (NLP) applications (Shutova, 2010). Although met"
N19-1092,D18-1170,0,0.0256283,"entences which are suitable for the fit word but inappropriate for the target word. For example, “they crowed for the first time” is fluent while “they poured for the first time” is strange. Other models decoding with a sense constraint generate sentences conveying the assigned senses to different degree. However, the uncommon sense model 868 is fine-grained and the training corpus is annotated with WSD tools which results in not only the lack of corpora for some senses but also a senselabel error, as there are no WSD tools that could tag the senses of the verbs with a high precision (&gt; 0.6) (Luo et al., 2018b,a). To solve these problems, we use the extracted metaphorical pairs to depict a metaphorical sense and the generated metaphors are readable. We also explore the shared inferential structure of a metaphorical usage and a literal usage of a verb by changing the adjustable factors. Table 2 demonstrates the semantic shift of the verb. The POS constrained model can be seen as a special adjustable joint model whose adjustable factors are α = 1, β = 0 and it generates sentences only considering the target words. In this way the target words are literally used. For contrast, the fit word model with"
N19-1092,T87-1040,0,0.374185,"et word (e.g. “devoured”) and its fit word (e.g. “enjoyed”) automatically. Then we adopt an end-to-end neural framework to train a POS constrained language model which can generate a sentence containing the assigned verb. For metaphor inference, we apply an adjustable joint beam search algorithm to the decoding phase. In this way, the target verb is metaphorically used in the generated sentence. The proposed model is named Adjustable Joint Model and is shown in Figure 2. 3.1 As metaphors begin their lives with marked rhetorical effects, whose comprehension requires a special imaginative leap (Nunberg, 1987), it is intuitive to assume that a metaphorical word can be distinguished from the literal one in the corpus with the violation of semantic constraints within a context. It has been proved that the dissimilarity between neural embeddings of the two words in a phrase is indicative of identifying the metaphoricity of the phrase (Shutova et al., 2016; Rei et al., 2017). Thus we find the semantic violation in the corpus based on word embeddings. The word embeddings are obtained by using Continuous Bagof-Words Model(CBOW) (Mikolov et al., 2013). Automatic Extraction of Metaphor Pairs Our automatic"
N19-1092,P18-1230,0,0.066339,"Missing"
N19-1092,N18-1119,0,0.0260003,"in parallel and the corresponding two sentences are the same except for the input words. We find an intersection between a metaphorical usage and a literal usage of a verb by this means. To avoid that the generated sentence is semantically inclined to the word in a metaphorical pair whose frequency is relatively higher and take the second hypothesis (H2) into consideration, we calculate the adjustment factors as follows: the beam size. When “&lt;/s&gt;” is selected among the highest scoring candidates the beam is reduced by one. When the beam is zero, the beam search algorithm stops (Lowerre, 1976; Post and Vilar, 2018). As the metaphorical usage of the verb is represented by the metaphorical pair. We need to generate a sentence for the target verb where the contextual sense of the target word equals to the literal sense of its fit word, which means both target word and its fit word should be suitable in the generated sentence. Given two different verbs in a metaphorical pair as inputs (e.g. “devoured.v”, “enjoyed.v”), we hope to generate a same context for them. However, the original beam search algorithm can hardly choose the same candidates at each time step for them. Yu et al. (2018) propose the joint be"
N19-1092,P18-1113,0,0.240307,"etween neural embeddings of the two words in a phrase is indicative of identifying the metaphoricity of the phrase (Shutova et al., 2016; Rei et al., 2017). Thus we find the semantic violation in the corpus based on word embeddings. The word embeddings are obtained by using Continuous Bagof-Words Model(CBOW) (Mikolov et al., 2013). Automatic Extraction of Metaphor Pairs Our automatic extraction method is based on the hypotheses as follows: H1. A metaphorical word is employed in the sentence to represent another concept and deviates from its normal meaning (Wilks, 1978; Li and Sporleder, 2010; Mao et al., 2018). H2. The metaphorical senses of words occur with relatively lower frequency in the corpus than their literal senses do (Cameron, 2003; Martin, 2006; Mao et al., 2018). Inspired by Mao et al. (2018), we use a fit word to model the contextual sense of the target word. To find a fit word for the target word t, we construct a candidate word set candidates which consists of the target word as well as its synonyms and direct hypernyms extracted from WordNet(Miller, 1998). The target word may have several senses. Each of these senses has a set of corresponding synonyms and hypernyms. We extract the"
N19-1092,D17-1162,0,0.0129365,"y used in the generated sentence. The proposed model is named Adjustable Joint Model and is shown in Figure 2. 3.1 As metaphors begin their lives with marked rhetorical effects, whose comprehension requires a special imaginative leap (Nunberg, 1987), it is intuitive to assume that a metaphorical word can be distinguished from the literal one in the corpus with the violation of semantic constraints within a context. It has been proved that the dissimilarity between neural embeddings of the two words in a phrase is indicative of identifying the metaphoricity of the phrase (Shutova et al., 2016; Rei et al., 2017). Thus we find the semantic violation in the corpus based on word embeddings. The word embeddings are obtained by using Continuous Bagof-Words Model(CBOW) (Mikolov et al., 2013). Automatic Extraction of Metaphor Pairs Our automatic extraction method is based on the hypotheses as follows: H1. A metaphorical word is employed in the sentence to represent another concept and deviates from its normal meaning (Wilks, 1978; Li and Sporleder, 2010; Mao et al., 2018). H2. The metaphorical senses of words occur with relatively lower frequency in the corpus than their literal senses do (Cameron, 2003; Ma"
N19-1092,N16-1020,0,0.0313599,"verb is metaphorically used in the generated sentence. The proposed model is named Adjustable Joint Model and is shown in Figure 2. 3.1 As metaphors begin their lives with marked rhetorical effects, whose comprehension requires a special imaginative leap (Nunberg, 1987), it is intuitive to assume that a metaphorical word can be distinguished from the literal one in the corpus with the violation of semantic constraints within a context. It has been proved that the dissimilarity between neural embeddings of the two words in a phrase is indicative of identifying the metaphoricity of the phrase (Shutova et al., 2016; Rei et al., 2017). Thus we find the semantic violation in the corpus based on word embeddings. The word embeddings are obtained by using Continuous Bagof-Words Model(CBOW) (Mikolov et al., 2013). Automatic Extraction of Metaphor Pairs Our automatic extraction method is based on the hypotheses as follows: H1. A metaphorical word is employed in the sentence to represent another concept and deviates from its normal meaning (Wilks, 1978; Li and Sporleder, 2010; Mao et al., 2018). H2. The metaphorical senses of words occur with relatively lower frequency in the corpus than their literal senses do"
N19-1092,J04-1002,0,0.543814,"s: Transparently-Motivated Metaphor, which is based on universal groundings that are often linked to bodily experience. Abe et al. (2006), Terai and Nakagawa (2010) generate metaphors in the form of “A (target) like B (vehicle)”. They firstly compute the probabilistic relationship between concepts and words with a statistical analysis of language data and then select candidates to fill in the template. From a given mapping between the concepts of two domains, Herv´as et al. (2007) present an approach to the application of metaphors for referring to concepts in an automatically generated text. Mason (2004) obtains domainspecific selectional preferences of verbs from large corpora and maps their common nominal arguments in other domains. The corresponding metaphorical mappings are achieved by such systematic variations and can generate simple conceptual metaphors in the form of: “A is (a) B”. Ovchinnikova et al. (2014) also rely on characteristic predicate but use general propositions instead of the verb and adjective phrases. As some metaphors’ target domain is lexically divorced from the source, Lederer (2016) identifies constellations of source-domain triggers in limited source domains. To ma"
N19-1092,S16-2003,0,0.12281,"more fluent sentences without considering the constraint that a given word must appear in the outputs, however the ppl. is calculated as Eq 5 shows: ppl = 10∧ ( −logP (T ) ), s.num + w.num − OOV s to the fit words and then the fit words in the sentences are replaced by their target words directly, which results in a higher ppl. Since the amount of the training data corresponding to the inputs of the uncommon sense model is the least, the generated sentences are not so fluent. 4.4 For a thorough comparison, we select 80 gold metaphors with high confidence (&gt;0.6) from the data set proposed by (Mohammad et al., 2016). Each verb in the data set was annotated for metaphoricity by 10 annotators and we use the verbs in the selected metaphors as the target words. As metaphor is such a creative and delicate language that automatic evaluation is not adequate. We ask native English speakers on Amazon Mechanical Turk to evaluate all the sentences generated by the neural models and corresponding gold metaphors in four aspects. Each sentence is scored from 1 to 5 by 5 judges. Readability(read.) indicates whether the sentence is fluent and consistent with the rules of grammar. Creativity(crea.) indicates whether the"
N19-1092,P17-1108,1,0.855395,"Missing"
N19-1092,C16-1316,0,0.0328123,"er than “he whispered spells as he moved his hands.” when given “spells.v” as input. POS Constrained Language Model Our goal is to generate a sentence containing a metaphorically used verb. However, the vanilla endto-end model cannot guarantee the target word appearing in the generated sequence all the time, letting alone the appearance of a word with a specific part-of-speech. To solve this problem, we present a neural language model which can ensure an assigned verb to appear in the generated sentence. Our design is inspired by the asynchronous forward/backward generation model proposed by (Mou et al., 2016). The POS constrained language model is trained end-to-end. Given a target verb as input, the model first generates the backward sequence starting from the target word wt at position t of the sentence and ending up with “&lt;/s&gt;” at position 0 of the sentence. n is the position of the last word in the sentence. p(wt1 ) denotes the probability of the backward sequence. Then we reverse the output of the backward sequence as the input to the forward model. And it generates the rest part of the sentence accordingly. p(wtn ) denotes the probability 3.3 Adjustable Joint Beam Search In the end-to-end mo"
N19-1092,W16-1105,0,0.69432,"ther than the limited annotated metaphor corpus, which assures the quality of the language model. The approach is shown in Figure 1. Relevant staIntroduction Metaphor is a kind of language filled with vitality and elasticity. It employs words in a way that deviates from their normal meaning to represent another concept (Li and Sporleder, 2010). Using metaphor allows us to express not just information but also real feelings and complex attitudes. There is a clear need in computational metaphor generation whose insightful results can be used in many applications from entertainment to education (Veale, 2016). Besides, a unified metaphor annotation procedure and creation of a large publicly available metaphor corpus are in great demands. Such resources make it possible to interpretate the identified metaphorical expressions and enhance the performance on other Natural Language Processing (NLP) applications (Shutova, 2010). Although metaphor has a long history of academic studies in both philosophy and linguistics 861 Proceedings of NAACL-HLT 2019, pages 861–871 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics tistics demonstrate that the most frequent"
N19-1092,C18-1330,0,0.0163337,"m for the NLP community. As the metaphor is hardly to be well-defined and modeled, little work focuses on the metaphor generation. Most of the previous efforts rely on hand-coded knowledge (Martin, 1990; Feldman and Narayanan, 2004; Agerri et al., 2007) , which heavily constrains the diversity of generated metaphors. The end-to-end approach presented to sequence learning has been proved effective on the generation tasks like machine translation (Sutskever et al., 2014), abstractive summarization (Tan et al., 2017), product review generation (Zang and Wan, 2017) and multi-label classification (Yang et al., 2018). The approach is able to train a language model which can generate fluent and creative sentences with sufficient corpus. Unfortunately, in spite of the industrious exploration of the metaphor corpus, the annotated data available now is still far from training a good language model. To the best of our knowledge, there has been no work combining metaphor generation with the end-toend approach. In this paper, we propose a neural approach for metaphor generation trained with Wiki corpus rather than the limited annotated metaphor corpus, which assures the quality of the language model. The approac"
N19-1092,P18-1153,1,0.647652,"(Lowerre, 1976; Post and Vilar, 2018). As the metaphorical usage of the verb is represented by the metaphorical pair. We need to generate a sentence for the target verb where the contextual sense of the target word equals to the literal sense of its fit word, which means both target word and its fit word should be suitable in the generated sentence. Given two different verbs in a metaphorical pair as inputs (e.g. “devoured.v”, “enjoyed.v”), we hope to generate a same context for them. However, the original beam search algorithm can hardly choose the same candidates at each time step for them. Yu et al. (2018) propose the joint beam search algorithm for pun generation. The algorithm selects candidates for the two inputs according to the joint score distribution on all beam while decoding. Nevertheless, the semanα = σ(1 − wf (word1) ), wf (word1) + wf (word2) (3) β = σ(1 − wf (word2) ), wf (word1) + wf (word2) (4) where wf (a) denotes the frequency of the word a in the corpus. As metaphorical senses of words occur with relatively lower frequency in the corpus, we adjust the weights negatively correlated to the word frequency. And we use σ, the sigmoid function which is differentiable and widely used"
N19-1092,W17-3526,1,0.845217,"0; G. and M., 1985), it still remains a tough problem for the NLP community. As the metaphor is hardly to be well-defined and modeled, little work focuses on the metaphor generation. Most of the previous efforts rely on hand-coded knowledge (Martin, 1990; Feldman and Narayanan, 2004; Agerri et al., 2007) , which heavily constrains the diversity of generated metaphors. The end-to-end approach presented to sequence learning has been proved effective on the generation tasks like machine translation (Sutskever et al., 2014), abstractive summarization (Tan et al., 2017), product review generation (Zang and Wan, 2017) and multi-label classification (Yang et al., 2018). The approach is able to train a language model which can generate fluent and creative sentences with sufficient corpus. Unfortunately, in spite of the industrious exploration of the metaphor corpus, the annotated data available now is still far from training a good language model. To the best of our knowledge, there has been no work combining metaphor generation with the end-toend approach. In this paper, we propose a neural approach for metaphor generation trained with Wiki corpus rather than the limited annotated metaphor corpus, which ass"
N19-4004,W09-1802,0,0.0305852,"ge of a news event, we have to look through all of the related news articles, which is very time-consuming and inefficient. With a large amount of time spent, what we get is just fragmented information scattered in different news articles. Ideally, if there exists an overview article about an event, we will fully and efficiently understand the event by reading it. However, such overview articles are not easy to write, even for professional editors, and the writing of them is 2 Related Work One of the related fields is document summarization. The methods can be divided into extractive methods (Gillick and Favre, 2009; Lin and Bilmes, 2010; Berg-Kirkpatrick et al., 2011; Sipos et al., 2012; Woodsend and Lapata, 2012; Wan and Zhang, 2014; Nallapati et al., 2017; Ren et al., 2017) and abstractive methods (Rush et al., 2015; Nallapati et al., 2016; Tan et al., 2017). There are several pilot studies on producing 18 Proceedings of NAACL-HLT 2019: Demonstrations, pages 18–23 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 杯/2018 FIFA World Cup Russia’. INS can interact with users in different stages: Users can choose and re-order some of the labels according to the"
N19-4004,J97-1003,0,0.529992,"labels are shown to users and each label stands for a subtopic. 5 Text Segmentation: Most of the methods for document summarization use sentences as the basic unit, which is not appropriate for long article generation. Synthesis articles using the sentence as the basic unit tend to be fragmented and hard to read. As a result, we use the text block as our basic unit. A text block is a set of several continuous sentences and it can cover a relative complete idea. We use our proposed SenTiling algorithm (Zhang and Wan, 2017) to segment texts, which is a variant of Hearst’s TextTiling algorithm (Hearst, 1997). After text segmentation, news articles are divided into text blocks, which include 2.3 sentences on average. Text Block Ranking and Selection: For each subtopic, INS system first selects candidate text blocks using exact match. A text block that contains a subtopic label is assigned to the subtopic. Then INS uses a topic sensitive TextRank algorithm to rank candidate text blocks for each subtopic where TextRank((Mihalcea and Tarau, 2004)) is a typical graph-based ranking algorithm applied in document summarization. We build a graph with the candidate text blocks as vertexes and the similarit"
N19-4004,W04-3252,0,0.0270636,"Missing"
N19-4004,K16-1028,0,0.0133798,"ticles. Ideally, if there exists an overview article about an event, we will fully and efficiently understand the event by reading it. However, such overview articles are not easy to write, even for professional editors, and the writing of them is 2 Related Work One of the related fields is document summarization. The methods can be divided into extractive methods (Gillick and Favre, 2009; Lin and Bilmes, 2010; Berg-Kirkpatrick et al., 2011; Sipos et al., 2012; Woodsend and Lapata, 2012; Wan and Zhang, 2014; Nallapati et al., 2017; Ren et al., 2017) and abstractive methods (Rush et al., 2015; Nallapati et al., 2016; Tan et al., 2017). There are several pilot studies on producing 18 Proceedings of NAACL-HLT 2019: Demonstrations, pages 18–23 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 杯/2018 FIFA World Cup Russia’. INS can interact with users in different stages: Users can choose and re-order some of the labels according to their preferences; Users can choose and edit the text blocks for each chosen subtopic; Users can edit on the final news synthesis result to get a more excellent article. Note that the interactions are optional. For ‘lazy’ users, INS c"
N19-4004,P11-1049,0,0.0256163,"of the related news articles, which is very time-consuming and inefficient. With a large amount of time spent, what we get is just fragmented information scattered in different news articles. Ideally, if there exists an overview article about an event, we will fully and efficiently understand the event by reading it. However, such overview articles are not easy to write, even for professional editors, and the writing of them is 2 Related Work One of the related fields is document summarization. The methods can be divided into extractive methods (Gillick and Favre, 2009; Lin and Bilmes, 2010; Berg-Kirkpatrick et al., 2011; Sipos et al., 2012; Woodsend and Lapata, 2012; Wan and Zhang, 2014; Nallapati et al., 2017; Ren et al., 2017) and abstractive methods (Rush et al., 2015; Nallapati et al., 2016; Tan et al., 2017). There are several pilot studies on producing 18 Proceedings of NAACL-HLT 2019: Demonstrations, pages 18–23 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 杯/2018 FIFA World Cup Russia’. INS can interact with users in different stages: Users can choose and re-order some of the labels according to their preferences; Users can choose and edit the text bl"
N19-4004,E12-1023,0,0.0184893,"which is very time-consuming and inefficient. With a large amount of time spent, what we get is just fragmented information scattered in different news articles. Ideally, if there exists an overview article about an event, we will fully and efficiently understand the event by reading it. However, such overview articles are not easy to write, even for professional editors, and the writing of them is 2 Related Work One of the related fields is document summarization. The methods can be divided into extractive methods (Gillick and Favre, 2009; Lin and Bilmes, 2010; Berg-Kirkpatrick et al., 2011; Sipos et al., 2012; Woodsend and Lapata, 2012; Wan and Zhang, 2014; Nallapati et al., 2017; Ren et al., 2017) and abstractive methods (Rush et al., 2015; Nallapati et al., 2016; Tan et al., 2017). There are several pilot studies on producing 18 Proceedings of NAACL-HLT 2019: Demonstrations, pages 18–23 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 杯/2018 FIFA World Cup Russia’. INS can interact with users in different stages: Users can choose and re-order some of the labels according to their preferences; Users can choose and edit the text blocks for each chosen"
N19-4004,P17-1108,1,0.900012,"Missing"
N19-4004,D12-1022,0,0.0223094,"onsuming and inefficient. With a large amount of time spent, what we get is just fragmented information scattered in different news articles. Ideally, if there exists an overview article about an event, we will fully and efficiently understand the event by reading it. However, such overview articles are not easy to write, even for professional editors, and the writing of them is 2 Related Work One of the related fields is document summarization. The methods can be divided into extractive methods (Gillick and Favre, 2009; Lin and Bilmes, 2010; Berg-Kirkpatrick et al., 2011; Sipos et al., 2012; Woodsend and Lapata, 2012; Wan and Zhang, 2014; Nallapati et al., 2017; Ren et al., 2017) and abstractive methods (Rush et al., 2015; Nallapati et al., 2016; Tan et al., 2017). There are several pilot studies on producing 18 Proceedings of NAACL-HLT 2019: Demonstrations, pages 18–23 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 杯/2018 FIFA World Cup Russia’. INS can interact with users in different stages: Users can choose and re-order some of the labels according to their preferences; Users can choose and edit the text blocks for each chosen subtopic; Users can edit o"
N19-4004,D17-1224,1,0.790896,"polis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 杯/2018 FIFA World Cup Russia’. INS can interact with users in different stages: Users can choose and re-order some of the labels according to their preferences; Users can choose and edit the text blocks for each chosen subtopic; Users can edit on the final news synthesis result to get a more excellent article. Note that the interactions are optional. For ‘lazy’ users, INS can generate the final synthesis article with just one click. long articles from a batch of news articles or web pages(Yao et al., 2011; Zhang and Wan, 2017; Liu et al., 2018). However, the generated overview articles do not have good structures and there are no interaction functions. There are some attempts of adding interaction functions into the traditional document summarization tasks (Jones et al., 2002; Leuski et al., 2003). However, the above work focuses on producing short summaries and the generation of long news overview articles is more challenging. Moreover, in the above work, the keyphrases to represent salient information are extracted based on some heuristic rules or simple clues, and they are usually not good subtopic representati"
N19-4004,C16-2060,1,0.858767,"the rest one for validation in turn. The values of P@5, P@10 and P@20 for SVR are 0.722, 0.693 and 0.619, respectively. The results indicate the majority of top 20 labels can represent subtopics. Other inappropriate labels can be filtered out by interaction with users. Evaluation on News Synthesis Articles: There are no gold reference synthesis articles for each news topic, and it is also hard to manually write a few reference articles. Thus we choose to conduct a manual evaluation of the final news synthesis articles. We use the multi-document summarization methods implemented in PKUSUMSUM (Zhang et al., 2016) as baselines. The summarization methods we choose include Lead, Coverage, Centroid, and TextRank. Centroid and TextRank can work on either the sentence unit (i.e., Centroid-sen, TextRank-sen) or the text block unit (i.e., Centroid-blk, TextRank-blk). Thus we have six baselines. The baselines are compared with INS that does not involve user interaction. Each baseline and INS generate a synthesis article with 1000 words for each news event and 10 news topics will be manually evaluated. We employ 16 Chinese college students as judges. Each student evaluates one or two news topics. We make sure e"
P07-1070,W04-3247,0,0.529774,"ional clues and prior knowledge. In this paper, we focus on generic document summarization and keyword extraction for single documents. Document summarization and keyword extraction have been widely explored in the natural language processing and information retrieval communities. A series of workshops and conferences on automatic text summarization (e.g. SUMMAC, DUC and NTCIR) have advanced the technology and produced a couple of experimental online systems. In recent years, graph-based ranking algorithms have been successfully used for document summarization (Mihalcea and Tarau, 2004, 2005; ErKan and Radev, 2004) and keyword extraction (Mihalcea and Tarau, 2004). Such algorithms make use of “voting” or “recommendations” between sentences (or words) to extract sentences (or keywords). Though the two tasks essentially share much in common, most algorithms have been developed particularly for either document summarization or keyword extraction. Zha (2002) proposes a method for simultaneous keyphrase extraction and text summarization by using only the heterogeneous sentence-to-word relationships. Inspired by this, we aim to take into account all the three kinds of relationships among sentences and words ("
P07-1070,W97-0704,0,0.0285246,"presents and discusses the evaluation results. Lastly we conclude our paper in Section 5. 2 2.1 Related Works Document Summarization Generally speaking, single document summarization methods can be either extraction-based or abstraction-based and we focus on extraction-based methods in this study. Extraction-based methods usually assign a saliency score to each sentence and then rank the sentences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and O’Leary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including"
P07-1070,W03-1028,0,0.265421,"yphrases. The algorithm is based on Adaptive Resonance Theory (ART) neural networks. Steier and Belew (1993) use the mutual information statistics to discover two-word keyphrases. Supervised machine learning algorithms have been proposed to classify a candidate phrase into either keyphrase or not. GenEx (Turney, 2000) and Kea (Frank et al., 1999; Witten et al., 1999) are two typical systems, and the most important features for classifying a candidate phrase are the frequency and location of the phrase in the document. More linguistic knowledge (such as syntactic features) has been explored by Hulth (2003). More recently, Mihalcea and Tarau (2004) propose the TextRank model to rank keywords based on the co-occurrence links between words. 3 3.1 Iterative Reinforcement Approach Overview The proposed approach is intuitively based on the following assumptions: Assumption 1: A sentence should be salient if it is heavily linked with other salient sentences, and a word should be salient if it is heavily linked with other salient words. Assumption 2: A sentence should be salient if it contains many salient words, and a word should be salient if it appears in many salient sentences. The first assumption"
P07-1070,C00-1072,0,0.0168281,"es the evaluation results. Lastly we conclude our paper in Section 5. 2 2.1 Related Works Document Summarization Generally speaking, single document summarization methods can be either extraction-based or abstraction-based and we focus on extraction-based methods in this study. Extraction-based methods usually assign a saliency score to each sentence and then rank the sentences in the document. The scores are usually computed based on a combination of statistical and linguistic features, including term frequency, sentence position, cue words, stigma words, topic signature (Hovy and Lin, 1997; Lin and Hovy, 2000), etc. Machine learning methods have also been employed to extract sentences, including unsupervised methods (Nomoto and Matsumoto, 2001) and supervised methods (Kupiec et al., 1995; Conroy and O’Leary, 2001; Amini and Gallinari, 2002; Shen et al., 2007). Other methods include maximal marginal relevance (MMR) (Carbonell and Goldstein, 1998), latent semantic analysis (LSA) (Gong and Liu, 2001). In Zha (2002), the mutual reinforcement principle is employed to iteratively extract key phrases and sentences from a document. Most recently, graph-based ranking methods, including TextRank ((Mihalcea a"
P07-1070,N03-1020,0,0.153136,"ews articles collected from TREC-9 for single556 document summarization task. The sentences in each article have been separated and the sentence information was stored into files. In the experiments, the background corpus for using the mutual information measure to compute word semantics simply consisted of all the documents from DUC2001 to DUC2005, which could be easily expanded by adding more documents. The stopwords were removed and the remaining words were converted to the basic forms based on WordNet. Then the semantic similarity values between the words were computed. We used the ROUGE (Lin and Hovy, 2003) toolkit (i.e.ROUGEeval-1.4.2 in this study) for evaluation, which has been widely adopted by DUC for automatic summarization evaluation. It measured summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary. ROUGE toolkit reported separate scores for 1, 2, 3 and 4-gram, and also for longest common subsequence co-occurrences. Among these different scores, unigram-based ROUGE score (ROUGE-1) has been shown to agree with human judgment most (Lin and Hovy, 2003). We showed three of the ROUGE metrics in t"
P07-1070,I05-2004,0,0.1802,"Missing"
P07-1070,N04-3012,0,0.0113148,"art system have been removed from the collection. unique meaning of a word is represented by a synonym set or synset. Each synset has a gloss that defines the concept that it represents. Synsets are connected to each other through explicit semantic relations that are defined in WordNet. Many approaches have been proposed to measure semantic relatedness based on WordNet. The measures vary from simple edge-counting to attempt to factor in peculiarities of the network structure by considering link direction, relative path, and density, such as vector, lesk, hso, lch, wup, path, res, lin and jcn (Pedersen et al., 2004). For example, “cat” and “dog” has higher semantic similarity than “cat” and “computer”. In this study, we implement the vector measure to efficiently evaluate the similarities of a large number of word pairs. The vector measure (Patwardhan, 2003) creates a co– occurrence matrix from a corpus made up of the WordNet glosses. Each content word used in a WordNet gloss has an associated context vector. Each gloss is represented by a gloss vector that is the average of all the context vectors of the words found in the gloss. Relatedness between concepts is measured by finding the cosine between a p"
P07-1070,W04-3252,0,\N,Missing
P09-1027,D08-1014,0,0.832419,"e rich English corpora for Chinese sentiment classification. In this study, we focus on the problem of cross-lingual sentiment classification, which leverages only English training data for supervised sentiment classification of Chinese product reviews, without using any Chinese resources. Note that the above problem is not only defined for Chinese sentiment classification, but also for various sentiment analysis tasks in other different languages. Though pilot studies have been performed to make use of English corpora for subjectivity classification in other languages (Mihalcea et al., 2007; Banea et al., 2008), the methods are very straightforward by directly employing an inductive classifier (e.g. SVM, NB), and the classification performance is far from satisfactory because of the language gap between the original language and the translated language. In this study, we propose a co-training approach to improving the classification accuracy of polarity identification of Chinese product reviews. Unlabeled Chinese reviews can be fully leveraged in the proposed approach. First, machine translation services are used to translate English training reviews into Chinese reviews and also translate Chinese t"
P09-1027,P07-1056,0,0.817992,"etric of positive or negative polarity in financial news text. Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting. Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses similar lexiconbased or corpus-based methods for Chinese sentiment classification. To date, several pilot studies have been performed to leverage rich English resources for sent"
P09-1027,P06-1013,0,0.00634741,"Missing"
P09-1027,W03-0407,0,0.00717777,"en the languages. 3.3 The Co-Training Algorithm The co-training algorithm (Blum and Mitchell, 1998) is a typical bootstrapping method, which starts with a set of labeled data, and increase the amount of annotated data using some amounts of unlabeled data in an incremental way. One important aspect of co-training is that two conditional independent views are required for cotraining to work, but the independence assumption can be relaxed. Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). In the context of cross-lingual sentiment classification, each labeled English review or unlabeled Chinese review has two views of features: English features and Chinese features. Here, a review is used to indicate both its Chinese version and its English version, until stated otherwise. The co-training algorithm is illustrated in Figure 2. In the algorithm, the class distribution in the labeled data is maintained by balancing the parameter values of p and n at each iteration. The intuition of"
P09-1027,P07-1124,0,0.0152021,"phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentimentbearing words. Hiroshi et al. (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text. Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying"
P09-1027,W05-0802,0,0.0579493,"roach (Jiang and Zhai, 2007). DauméIII and Marcu (2006) introduce a statistical formulation of this problem in terms of a simple mixture model. In particular, several previous studies focus on the problem of cross-lingual text classification, which can be considered as a special case of general cross-domain text classification. Bel et al. (2003) present practical and cost-effective solutions. A few novel models have been proposed to address the problem, e.g. the EM-based algorithm (Rigutini et al., 2005), the information bottleneck approach (Ling et al., 2008), the multilingual domain models (Gliozzo and Strapparava, 2005), etc. To the best of our knowledge, cotraining has not yet been investigated for crossdomain or cross-lingual text classification. 236 3 3.1 The Co-Training Approach Overview The purpose of our approach is to make use of the annotated English corpus for sentiment polarity identification of Chinese reviews in a supervised framework, without using any Chinese resources. Given the labeled English reviews and unlabeled Chinese reviews, two straightforward methods for addressing the problem are as follows: 1) We first learn a classifier based on the labeled English reviews, and then translate Chin"
P09-1027,C04-1071,0,0.358587,"ication. The methods for document sentiment classification can be generally categorized into lexicon-based and corpus-based. Lexicon-based methods usually involve deriving a sentiment measure for text based on sentiment lexicons. Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentimentbearing words. Hiroshi et al. (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text. Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier. Since the work of Pang"
P09-1027,P05-2008,0,0.134933,"ositive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text. Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting. Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses simil"
P09-1027,N01-1023,0,0.00585667,"target language and aligned text consisting of examples of human translations between the languages. 3.3 The Co-Training Algorithm The co-training algorithm (Blum and Mitchell, 1998) is a typical bootstrapping method, which starts with a set of labeled data, and increase the amount of annotated data using some amounts of unlabeled data in an incremental way. One important aspect of co-training is that two conditional independent views are required for cotraining to work, but the independence assumption can be relaxed. Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). In the context of cross-lingual sentiment classification, each labeled English review or unlabeled Chinese review has two views of features: English features and Chinese features. Here, a review is used to indicate both its Chinese version and its English version, until stated otherwise. The co-training algorithm is illustrated in Figure 2. In the algorithm, the class distribution in the labeled data is ma"
P09-1027,C04-1200,0,0.391718,"rk Sentiment Classification Sentiment classification can be performed on words, sentences or documents. In this paper we focus on document sentiment classification. The methods for document sentiment classification can be generally categorized into lexicon-based and corpus-based. Lexicon-based methods usually involve deriving a sentiment measure for text based on sentiment lexicons. Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentimentbearing words. Hiroshi et al. (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text. Corpus-based metho"
P09-1027,P08-1036,0,0.147506,"ng use of unlabeled Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers. 1 Introduction Sentiment classification is the task of identifying the sentiment polarity of a given text. The sentiment polarity is usually positive or negative and the text genre is usually product review. In recent years, sentiment classification has drawn much attention in the NLP field and it has many useful applications, such as opinion mining and summarization (Liu et al., 2005; Ku et al., 2006; Titov and McDonald, 2008). To date, a variety of corpus-based methods have been developed for sentiment classification. The methods usually rely heavily on an annotated corpus for training the sentiment classifier. The sentiment corpora are considered as the most valuable resources for the sentiment classification task. However, such resources in different languages are very imbalanced. Because most previous work focuses on English sentiment classification, many annotated corpora for English sentiment classification are freely available on the Web. However, the annotated corpora for Chinese sentiment classification ar"
P09-1027,P02-1053,0,0.0360014,"untec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP co-training approach is described in detail in Section 3. Section 4 shows the experimental results. Lastly we conclude this paper in Section 5. 2 2.1 Related Work Sentiment Classification Sentiment classification can be performed on words, sentences or documents. In this paper we focus on document sentiment classification. The methods for document sentiment classification can be generally categorized into lexicon-based and corpus-based. Lexicon-based methods usually involve deriving a sentiment measure for text based on sentiment lexicons. Turney (2002) predicates the sentiment orientation of a review by the average semantic orientation of the phrases in the review that contain adjectives or adverbs, which is denoted as the semantic oriented method. Kim and Hovy (2004) build three models to assign a sentiment category to a given sentence by combining the individual sentiments of sentimentbearing words. Hiroshi et al. (2004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative te"
P09-1027,P07-1055,0,0.0630085,"nd taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text. Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting. Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses similar lexiconbased or corpus-based methods"
P09-1027,W04-2405,0,0.0384321,"The co-training algorithm (Blum and Mitchell, 1998) is a typical bootstrapping method, which starts with a set of labeled data, and increase the amount of annotated data using some amounts of unlabeled data in an incremental way. One important aspect of co-training is that two conditional independent views are required for cotraining to work, but the independence assumption can be relaxed. Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). In the context of cross-lingual sentiment classification, each labeled English review or unlabeled Chinese review has two views of features: English features and Chinese features. Here, a review is used to indicate both its Chinese version and its English version, until stated otherwise. The co-training algorithm is illustrated in Figure 2. In the algorithm, the class distribution in the labeled data is maintained by balancing the parameter values of p and n at each iteration. The intuition of the co-training algorithm is that if one cl"
P09-1027,D08-1058,1,0.658444,"t al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such work uses similar lexiconbased or corpus-based methods for Chinese sentiment classification. To date, several pilot studies have been performed to leverage rich English resources for sentiment analysis in other languages. Standard Naïve Bayes and SVM classifiers have been applied for subjectivity classification in Romanian (Mihalcea et al., 2007; Banea et al., 2008), and the results show that automatic translation is a viable alternative for the construction of resources and tools for subjectivity analysis in a new target language. Wan (2008) focuses on leveraging both Chinese and English lexicons to improve Chinese sentiment analysis by using lexicon-based methods. In this study, we focus on improving the corpus-based method for crosslingual sentiment classification of Chinese product reviews by developing novel approaches. 2.2 Cross-Domain Text Classification Cross-domain text classification can be considered as a more general task than cross-lingual sentiment classification. In the problem of crossdomain text classification, the labeled and unlabeled data come from different domains, and their underlying distributions are often"
P09-1027,H05-1044,0,0.0946784,"review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text. Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting. Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2007) and most such wo"
P09-1027,P07-1123,0,0.893915,"re us is how to leverage rich English corpora for Chinese sentiment classification. In this study, we focus on the problem of cross-lingual sentiment classification, which leverages only English training data for supervised sentiment classification of Chinese product reviews, without using any Chinese resources. Note that the above problem is not only defined for Chinese sentiment classification, but also for various sentiment analysis tasks in other different languages. Though pilot studies have been performed to make use of English corpora for subjectivity classification in other languages (Mihalcea et al., 2007; Banea et al., 2008), the methods are very straightforward by directly employing an inductive classifier (e.g. SVM, NB), and the classification performance is far from satisfactory because of the language gap between the original language and the translated language. In this study, we propose a co-training approach to improving the classification accuracy of polarity identification of Chinese product reviews. Unlabeled Chinese reviews can be fully leveraged in the proposed approach. First, machine translation services are used to translate English training reviews into Chinese reviews and als"
P09-1027,W04-3253,0,0.689711,"he sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text. Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting. Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye et al., 2006; Li and Sun, 2"
P09-1027,N03-1023,0,0.00741784,"isting of examples of human translations between the languages. 3.3 The Co-Training Algorithm The co-training algorithm (Blum and Mitchell, 1998) is a typical bootstrapping method, which starts with a set of labeled data, and increase the amount of annotated data using some amounts of unlabeled data in an incremental way. One important aspect of co-training is that two conditional independent views are required for cotraining to work, but the independence assumption can be relaxed. Till now, co-training has been successfully applied to statistical parsing (Sarkar, 2001), reference resolution (Ng and Cardie, 2003), part of speech tagging (Clark et al., 2003), word sense disambiguation (Mihalcea, 2004) and email classification (Kiritchenko and Matwin, 2001). In the context of cross-lingual sentiment classification, each labeled English review or unlabeled Chinese review has two views of features: English features and Chinese features. Here, a review is used to indicate both its Chinese version and its English version, until stated otherwise. The co-training algorithm is illustrated in Figure 2. In the algorithm, the class distribution in the labeled data is maintained by balancing the parameter values o"
P09-1027,W02-1011,0,0.0574949,"004) use the technique of deep language analysis for machine translation to extract sentiment units in text documents. Kennedy and Inkpen (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text. Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a"
P09-1027,P04-1035,0,0.0342919,"n (2006) determine the sentiment of a customer review by counting positive and negative terms and taking into account contextual valence shifters, such as negations and intensifiers. Devitt and Ahmad (2007) explore a computable metric of positive or negative polarity in financial news text. Corpus-based methods usually consider the sentiment analysis task as a classification task and they use a labeled corpus to train a sentiment classifier. Since the work of Pang et al. (2002), various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005). Most recently, McDonald et al. (2007) investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products. Andreevskaia and Bergler (2008) present a new system consisting of the ensemble of a corpus-based classifier and a lexicon-based classifier with precision-based vote weighting. Chinese sentiment analysis has also been studied (Tsou et al., 2005; Ye e"
P09-1027,P08-1034,0,\N,Missing
P09-1027,P08-1000,0,\N,Missing
P10-1094,P07-1111,0,0.00970836,"anslation evaluation aims to assess the correctness and quality of the translation. Usually, the human reference translation is provided, and various methods and metrics have been developed for comparing the system-translated text and the human reference text. For example, the BLEU metric, the NIST metric and their relatives are all based on the idea that the more shared substrings the system-translated text has with the human reference translation, the better the translation is. Blatz et al. (2003) investigate training sentence-level confidence measures using a variety of fuzzy match scores. Albrecht and Hwa (2007) rely on regression algorithms and reference-based features to measure the quality of sentences. Transition evaluation without using reference translations has also been investigated. Quirk (2004) presents a supervised method for training a sentence level confidence measure on translation output using a human-annotated corpus. Features derived from the source sentence and the target sentence (e.g. sentence length, perplexity, etc.) and features about the translation process are leveraged. Gamon et al. (2005) investigate the possibility of evaluating MT quality and fluency at the sentence level"
P10-1094,E09-1017,0,0.012986,"d the target sentence (e.g. sentence length, perplexity, etc.) and features about the translation process are leveraged. Gamon et al. (2005) investigate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations, and they can improve on the correlation between language model perplexity scores and human judgment by combing these perplexity scores with class probabilities from a machine-learned classifier. Specia et al. (2009) use the ICM theory to identify the threshold to map a continuous predicted score into “good” or “bad” categories. Chae and Nenkova (2009) use surface syntactic features to assess the fluency of machine translation results. In this study, we further predict the translation quality of an English sentence before the machine translation process, i.e., we do not leverage reference translation and the target sentence. 2.2 Document Summarization Document summarization methods can be generally categorized into extraction-based methods and abstraction-based methods. In this paper, we focus on extraction-based methods. Extractionbased summarization methods usually assign each sentence a saliency score and then rank the sentences in a doc"
P10-1094,2005.eamt-1.15,0,0.0125844,"training sentence-level confidence measures using a variety of fuzzy match scores. Albrecht and Hwa (2007) rely on regression algorithms and reference-based features to measure the quality of sentences. Transition evaluation without using reference translations has also been investigated. Quirk (2004) presents a supervised method for training a sentence level confidence measure on translation output using a human-annotated corpus. Features derived from the source sentence and the target sentence (e.g. sentence length, perplexity, etc.) and features about the translation process are leveraged. Gamon et al. (2005) investigate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations, and they can improve on the correlation between language model perplexity scores and human judgment by combing these perplexity scores with class probabilities from a machine-learned classifier. Specia et al. (2009) use the ICM theory to identify the threshold to map a continuous predicted score into “good” or “bad” categories. Chae and Nenkova (2009) use surface syntactic features to assess the fluency of machine translation results. In this study, we further predi"
P10-1094,C00-1072,0,0.0408297,"e. 2.2 Document Summarization Document summarization methods can be generally categorized into extraction-based methods and abstraction-based methods. In this paper, we focus on extraction-based methods. Extractionbased summarization methods usually assign each sentence a saliency score and then rank the sentences in a document or document set. For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based a"
P10-1094,N03-1020,0,0.0696603,"Missing"
P10-1094,I05-2004,0,0.060761,"t summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extracting easy-to-understand English summaries for non-native readers. Several pilot studies have been performed for the cross-language summarization task by simply using document translation or summary translation. Leuski et al. (2003) use machin"
P10-1094,P08-1094,0,0.0210602,"d Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extracting easy-to-understand English summaries for non-native readers. Several pilot studies have been performed for the cross-lan"
P10-1094,orasan-chiorean-2008-evaluation,0,0.123765,"Missing"
P10-1094,quirk-2004-training,0,0.0192244,"system-translated text and the human reference text. For example, the BLEU metric, the NIST metric and their relatives are all based on the idea that the more shared substrings the system-translated text has with the human reference translation, the better the translation is. Blatz et al. (2003) investigate training sentence-level confidence measures using a variety of fuzzy match scores. Albrecht and Hwa (2007) rely on regression algorithms and reference-based features to measure the quality of sentences. Transition evaluation without using reference translations has also been investigated. Quirk (2004) presents a supervised method for training a sentence level confidence measure on translation output using a human-annotated corpus. Features derived from the source sentence and the target sentence (e.g. sentence length, perplexity, etc.) and features about the translation process are leveraged. Gamon et al. (2005) investigate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations, and they can improve on the correlation between language model perplexity scores and human judgment by combing these perplexity scores with class probabi"
P10-1094,H05-1005,0,0.110107,"nces. Chalendar et al. (2005) focuses on semantic analysis and sentence generation techniques for cross-language summarization. Orasan 918 and Chiorean (2008) propose to produce summaries with the MMR method from Romanian news articles and then automatically translate the summaries into English. Cross language query based summarization has been investigated in (Pingali et al., 2007), where the query and the documents are in different languages. Other related work includes multilingual summarization (Lin et al., 2005), which aims to create summaries from multiple sources in multiple languages. Siddharthan and McKeown (2005) use the information redundancy in multilingual input to correct errors in machine translation and thus improve the quality of multilingual summaries. 3 The Proposed Approach Previous methods for cross-language summarization usually consist of two steps: one step for summarization and one step for translation. Different order of the two steps can lead to the following two basic English-to-Chinese summarization methods: Late Translation (LateTrans): Firstly, an English summary is produced for the English document set by using existing summarization methods. Then, the English summary is automati"
P10-1094,2009.mtsummit-papers.16,0,0.0123491,"a sentence level confidence measure on translation output using a human-annotated corpus. Features derived from the source sentence and the target sentence (e.g. sentence length, perplexity, etc.) and features about the translation process are leveraged. Gamon et al. (2005) investigate the possibility of evaluating MT quality and fluency at the sentence level in the absence of reference translations, and they can improve on the correlation between language model perplexity scores and human judgment by combing these perplexity scores with class probabilities from a machine-learned classifier. Specia et al. (2009) use the ICM theory to identify the threshold to map a continuous predicted score into “good” or “bad” categories. Chae and Nenkova (2009) use surface syntactic features to assess the fluency of machine translation results. In this study, we further predict the translation quality of an English sentence before the machine translation process, i.e., we do not leverage reference translation and the target sentence. 2.2 Document Summarization Document summarization methods can be generally categorized into extraction-based methods and abstraction-based methods. In this paper, we focus on extracti"
P10-1094,P07-1070,1,0.829651,"re and then rank the sentences in a document or document set. For single document summarization, the sentence score is usually computed by empirical combination of a number of statistical and linguistic feature values, such as term frequency, sentence position, cue words, stigma words, topic signature (Luhn 1969; Lin and Hovy, 2000). The summary sentences can also be selected by using machine learning methods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document"
P10-1094,C08-1124,0,0.0290453,"ods (Kupiec et al., 1995; Amini and Gallinari, 2002) or graph-based methods (ErKan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). For multi-document summarization, the centroid-based method (Radev et al., 2004) is a typical method, and it scores sentences based on cluster centroids, position and TFIDF features. NeATS (Lin and Hovy, 2002) makes use of new features such as topic signature to select important sentences. Machine Learning based approaches have also been proposed for combining various sentence features (Wong et al., 2008). The influences of input difficulty on summarization performance have been investigated in (Nenkova and Louis, 2008). Graph-based methods have also been used to rank sentences in a document set. For example, Mihalcea and Tarau (2005) extend the TextRank algorithm to compute sentence importance in a document set. Cluster-level information has been incorporated in the graph model to better evaluate sentences (Wan and Yang, 2008). Topic-focused or query biased multi-document summarization has also been investigated (Wan et al., 2006). Wan et al. (2010) propose the EUSUM system for extracting eas"
P10-1094,W04-3252,0,\N,Missing
P10-1094,P02-1058,0,\N,Missing
P10-1094,W04-3247,0,\N,Missing
P11-1155,D10-1047,0,0.0744825,"cluster centroids, position and TFIDF. Machine Learning techniques have also been used for feature combining (Wong et al., 2008). Nenkova and Louis (2008) investigate the influences of input difficulty on summarization performance. Pitler et al. (2010) present a systematic assessment of several diverse classes of metrics designed for automatic evaluation of linguistic quality of multi-document summaries. Celikyilmaz and Hakkani-Tur (2010) formulate extractive summarization as a two-step learning problem by building a generative model for pattern discovery and a regression model for inference. Aker et al. (2010) propose an A* search algorithm to find the best extractive summary up to a given length, and they propose a discriminative training algorithm for directly maximizing the quality of the best summary. Graph-based methods have also been used to rank sentences for multi-document summarization (Mihalcea and Tarau, 2005; Wan and Yang, 2008). 2.2 Cross-Lingual Document Summarization 3.1 SimFusion This method uses the English-side information for Chinese sentence ranking in the graph-based framework. The sentence similarities in the two languages are fused in the method. In other words, when we compu"
P11-1155,P10-1084,0,0.0240214,"ciple (Zha 2002; Wan et al., 2007). In the task of multi-document summarization, the centroid-based method (Radev et al., 2004) ranks the sentences in a document set based on such features as cluster centroids, position and TFIDF. Machine Learning techniques have also been used for feature combining (Wong et al., 2008). Nenkova and Louis (2008) investigate the influences of input difficulty on summarization performance. Pitler et al. (2010) present a systematic assessment of several diverse classes of metrics designed for automatic evaluation of linguistic quality of multi-document summaries. Celikyilmaz and Hakkani-Tur (2010) formulate extractive summarization as a two-step learning problem by building a generative model for pattern discovery and a regression model for inference. Aker et al. (2010) propose an A* search algorithm to find the best extractive summary up to a given length, and they propose a discriminative training algorithm for directly maximizing the quality of the best summary. Graph-based methods have also been used to rank sentences for multi-document summarization (Mihalcea and Tarau, 2005; Wan and Yang, 2008). 2.2 Cross-Lingual Document Summarization 3.1 SimFusion This method uses the English-s"
P11-1155,W04-3247,0,0.0413514,"Missing"
P11-1155,C00-1072,0,0.0245245,"conclude this paper in Section 5. 1547 2 2.1 Related Work General Document Summarization Document summarization methods can be extraction-based, abstraction-based or hybrid methods. We focus on extraction-based methods in this study, and the methods directly extract summary sentences from a document or document set by ranking the sentences in the document or document set. In the task of single document summarization, various features have been investigated for ranking sentences in a document, including term frequency, sentence position, cue words, stigma words, and topic signature (Luhn 1969; Lin and Hovy, 2000). Machine learning techniques have been used for sentence ranking (Kupiec et al., 1995; Amini and Gallinari, 2002). Litvak et al. (2010) present a language-independent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. In recent years, graph-based methods have been proposed for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). In the task of multi-document summarization, the centroid-based method (Radev et al.,"
P11-1155,N03-1020,0,0.0164349,"E-W Average_F ROUGE-L Average_F ROUGE-SU4 Average_F 0.03723 0.03805 0.04017 0.04282 0.05566 0.05886 0.06117 0.06158 0.13259 0.13871 0.14362 0.14521 0.07177 0.07474 0.07645 0.07805 Table 1: Comparison Results All the English sentences in the document set were automatically translated into Chinese sentences by using Google Translate, and the Stanford Chinese Word Segmenter2 was used for segmenting the Chinese documents and summaries into words. For comparative study, the summary length was limited to five sentences, i.e. each Chinese summary consisted of five sentences. We used the ROUGE-1.5.5 (Lin and Hovy, 2003) toolkit for evaluation, which has been widely adopted by DUC and TAC for automatic summarization evaluation. It measured summary quality by counting overlapping units such as the n-gram, word sequences and word pairs between the candidate summary and the reference summary. We showed three of the ROUGE F-measure scores in the experimental results: ROUGE-2 (bigrambased), ROUGE-W (based on weighted longest common subsequence, weight=1.2), ROUGE-L (based on longest common subsequences), and ROUGE-SU4 (based on skip bigram with a maximum skip distance of 4). Note that the ROUGE toolkit was perform"
P11-1155,P10-1095,0,0.0486694,"on-based, abstraction-based or hybrid methods. We focus on extraction-based methods in this study, and the methods directly extract summary sentences from a document or document set by ranking the sentences in the document or document set. In the task of single document summarization, various features have been investigated for ranking sentences in a document, including term frequency, sentence position, cue words, stigma words, and topic signature (Luhn 1969; Lin and Hovy, 2000). Machine learning techniques have been used for sentence ranking (Kupiec et al., 1995; Amini and Gallinari, 2002). Litvak et al. (2010) present a language-independent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. In recent years, graph-based methods have been proposed for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). In the task of multi-document summarization, the centroid-based method (Radev et al., 2004) ranks the sentences in a document set based on such features as cluster centroids, position and TFIDF. Machine Learning techniques"
P11-1155,I05-2004,0,0.0498181,"s of metrics designed for automatic evaluation of linguistic quality of multi-document summaries. Celikyilmaz and Hakkani-Tur (2010) formulate extractive summarization as a two-step learning problem by building a generative model for pattern discovery and a regression model for inference. Aker et al. (2010) propose an A* search algorithm to find the best extractive summary up to a given length, and they propose a discriminative training algorithm for directly maximizing the quality of the best summary. Graph-based methods have also been used to rank sentences for multi-document summarization (Mihalcea and Tarau, 2005; Wan and Yang, 2008). 2.2 Cross-Lingual Document Summarization 3.1 SimFusion This method uses the English-side information for Chinese sentence ranking in the graph-based framework. The sentence similarities in the two languages are fused in the method. In other words, when we compute the similarity value between two Chinese sentences, the similarity value between the corresponding two English sentences is used by linear fusion. Since sentence similarity evaluation plays a very important role in the graph-based ranking algorithm, this method can leverage bothside information through similarit"
P11-1155,P08-1094,0,0.0119402,"ation based on the linear optimization of several sentence ranking measures using a genetic algorithm. In recent years, graph-based methods have been proposed for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). In the task of multi-document summarization, the centroid-based method (Radev et al., 2004) ranks the sentences in a document set based on such features as cluster centroids, position and TFIDF. Machine Learning techniques have also been used for feature combining (Wong et al., 2008). Nenkova and Louis (2008) investigate the influences of input difficulty on summarization performance. Pitler et al. (2010) present a systematic assessment of several diverse classes of metrics designed for automatic evaluation of linguistic quality of multi-document summaries. Celikyilmaz and Hakkani-Tur (2010) formulate extractive summarization as a two-step learning problem by building a generative model for pattern discovery and a regression model for inference. Aker et al. (2010) propose an A* search algorithm to find the best extractive summary up to a given length, and they propose a discriminative training alg"
P11-1155,orasan-chiorean-2008-evaluation,0,0.0926557,"n the information on the target side. The summary translation scheme first extracts summary sentences from the source documents based only on the information on the source side, and then translates the summary sentences into the corresponding summary sentences in the target language. For example Leuski et al. (2003) use machine translation for English headline generation for Hindi documents. Lim et al. (2004) propose to generate a Japanese summary by using Korean summarizer. Chalendar et al. (2005) focus on semantic analysis and sentence generation techniques for cross-language summarization. Orasan and Chiorean (2008) propose to produce summaries with the MMR method from Romanian news articn cn cn cn en en cles and then automatically translate the summaries f ( si , s j ) = λ ⋅ simcos ine ( si , s j ) + (1 − λ ) ⋅ simcos ine ( si , s j ) into English. Cross language query based summarization has been investigated in (Pingali et al., where senj and seni are the source English sentences 2007), where the query and the documents are in for scnj and scni. λ∈[0, 1] is a parameter to control different languages. Wan et al. (2010) adopt the the relative contributions of the two similarity valsummary translation sc"
P11-1155,P10-1056,0,0.0539184,"In recent years, graph-based methods have been proposed for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). In the task of multi-document summarization, the centroid-based method (Radev et al., 2004) ranks the sentences in a document set based on such features as cluster centroids, position and TFIDF. Machine Learning techniques have also been used for feature combining (Wong et al., 2008). Nenkova and Louis (2008) investigate the influences of input difficulty on summarization performance. Pitler et al. (2010) present a systematic assessment of several diverse classes of metrics designed for automatic evaluation of linguistic quality of multi-document summaries. Celikyilmaz and Hakkani-Tur (2010) formulate extractive summarization as a two-step learning problem by building a generative model for pattern discovery and a regression model for inference. Aker et al. (2010) propose an A* search algorithm to find the best extractive summary up to a given length, and they propose a discriminative training algorithm for directly maximizing the quality of the best summary. Graph-based methods have also been"
P11-1155,H05-1005,0,0.0607828,"Missing"
P11-1155,P10-1094,1,0.761266,"e propose to use the bilingual information from both the source and translated documents for this task. Two summarization methods (SimFusion and CoRank) are proposed to leverage the bilingual information in the graph-based ranking framework for cross-language summary extraction. Experimental results on the DUC2001 dataset with manually translated reference Chinese summaries show the effectiveness of the proposed methods. 1 Introduction Cross-language document summarization is defined as the task of producing a summary in a different target language for a set of documents in a source language (Wan et al., 2010). In this study, we focus on English-to-Chinese cross-language summarization, which aims to produce Chinese summaries for English document sets. The task is very useful in the field of multilingual information access. For example, it is beneficial for most Chinese readers to quickly browse and understand English news documents or document sets by reading the corresponding Chinese summaries. A few pilot studies have investigated the task in recent years and exiting methods make use of either the information in the source language or the information in the target language after using machine tra"
P11-1155,P07-1070,1,0.328886,"requency, sentence position, cue words, stigma words, and topic signature (Luhn 1969; Lin and Hovy, 2000). Machine learning techniques have been used for sentence ranking (Kupiec et al., 1995; Amini and Gallinari, 2002). Litvak et al. (2010) present a language-independent approach for extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. In recent years, graph-based methods have been proposed for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). In the task of multi-document summarization, the centroid-based method (Radev et al., 2004) ranks the sentences in a document set based on such features as cluster centroids, position and TFIDF. Machine Learning techniques have also been used for feature combining (Wong et al., 2008). Nenkova and Louis (2008) investigate the influences of input difficulty on summarization performance. Pitler et al. (2010) present a systematic assessment of several diverse classes of metrics designed for automatic evaluation of linguistic quality of multi-document summaries. Celikyilmaz and Hakkani-Tur (2010)"
P11-1155,C08-1124,0,0.0112372,"extractive summarization based on the linear optimization of several sentence ranking measures using a genetic algorithm. In recent years, graph-based methods have been proposed for sentence ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Other methods include mutual reinforcement principle (Zha 2002; Wan et al., 2007). In the task of multi-document summarization, the centroid-based method (Radev et al., 2004) ranks the sentences in a document set based on such features as cluster centroids, position and TFIDF. Machine Learning techniques have also been used for feature combining (Wong et al., 2008). Nenkova and Louis (2008) investigate the influences of input difficulty on summarization performance. Pitler et al. (2010) present a systematic assessment of several diverse classes of metrics designed for automatic evaluation of linguistic quality of multi-document summaries. Celikyilmaz and Hakkani-Tur (2010) formulate extractive summarization as a two-step learning problem by building a generative model for pattern discovery and a regression model for inference. Aker et al. (2010) propose an A* search algorithm to find the best extractive summary up to a given length, and they propose a d"
P11-1155,W04-3252,0,\N,Missing
P11-1155,P08-1000,0,\N,Missing
P11-2114,N09-2029,0,0.148665,"o analyze trends, draw lessons from the past, and gain insights about similar situations. For example, by comparing the information about mining accidents in Chile and China, we can discover what leads to the different endings and how to avoid those tragedies. Comparative text mining has drawn much attention in recent years. The proposed works differ in the domain of corpus, the source of comparison and the representing form of results. So far, most researches focus on comparing review opinions of products (Liu et al., 2005; Jindal and Liu, 2006a; ∗ Corresponding author Jindal and Liu, 2006b; Lerman and McDonald, 2009; Kim and Zhai, 2009). A reason is that the aspects in reviews are easy to be extracted and the comparisons have simple patterns, e.g. positive vs. negative. A few other works have also tried to compare facts and views in news article (Zhai et al., 2004) and Blogs (Wang et al., 2009). The comparative information can be extracted from explicit comparative sentences (Jindal and Liu, 2006a; Jindal and Liu, 2006b; Huang et al., 2008), or mined implicitly by matching up features of objects in the same aspects (Zhai et al., 2004; Liu et al., 2005; Kim and Zhai, 2009; Sun et al., 2006). The compariso"
P11-2114,N03-1020,0,0.0758466,"ined objective function under above constraints is an integer linear programming (ILP) problem. Though the ILP problems are generally NP-hard, considerable works have been done and several software solutions have been released to solve them efficiently.1 1 Topic 2 (10) i=1 k=1 4 Topic 1 We use IBM ILOG CPLEX optimizer to solve the problem. http://news.google.com 651 ca ; aa CR = ca ; am CF = 2 · CP · CR CP + CR ROUGE: the ROUGE is a widely used metric in summarization evaluation. It measures summary quality by counting overlapping units between the candidate summary and the reference summary (Lin and Hovy, 2003). In the experiment, we report the f-measure values of ROUGE-1, ROUGE-2 and ROUGE-SU4, which count overlapping unigrams, bigrams and skip-4-grams respectively. To evaluate whether the summary is related to both topics, we also split each comparative summary into two topic-related parts, evaluate them respectively, and report the mean of the two ROUGE values (denoted as MROUGE). 4.3 Baseline Systems Non-Comparative Model (NCM): The non-comparative model treats the task as a traditional summarization problem and selects the important sentences from each document collection. The model is adapted"
P11-2114,P07-1070,1,0.812673,"ic-related parts, evaluate them respectively, and report the mean of the two ROUGE values (denoted as MROUGE). 4.3 Baseline Systems Non-Comparative Model (NCM): The non-comparative model treats the task as a traditional summarization problem and selects the important sentences from each document collection. The model is adapted from our approach by setting λ = 0 in the objection function 1. Co-Ranking Model (CRM): The co-ranking model makes use of the relations within each topic and relations across the topics to reinforce scores of the comparison related sentences. The model is adapted from (Wan et al., 2007). The SS, W W and SW relationships are replaced by relationships between two sentences within each topic and relationships between two sentences from different topics. using the CLPM model. The summary describes several comparisons between two FIFA World Cups in 2006 and 2010. Most of the comparisons are clear and representative. 4.4 Experiment Results 5 Conclusion We apply all the systems to generate comparative summaries with a length limit of 200 words. The evaluation results are shown in table 2. Compared with baseline models, our linear programming based comparative model (denoted as LPCM"
P11-2114,N04-3012,0,\N,Missing
P11-2114,W01-0100,0,\N,Missing
P12-1025,J07-3004,0,0.00703274,"acquire 00 = D ST agctb . We can a new labeled data set Dctb ppd→ctb 00 . If re-train the STagctb model with Dctb ∪ Dctb 00 we use the gold PPD-style labels of Dctb to extract sub-words, the new model will overfit to the gold PPD-style labels, which are unavailable at test time. To avoid this training/test mismatch problem, we also employ a 10-fold cross validation procedure to add noise. It is not a new topic to convert corpus from one formalism to another. A well known work is transforming Penn Treebank into resources for various deep linguistic processing, including LTAG (Xia, 1999), CCG (Hockenmaier and Steedman, 2007), HPSG (Miyao et al., 2004) and LFG (Cahill et al., 2002). Such work for corpus conversion mainly leverages rich sets of hand-crafted rules to convert corpora. The construction of linguistic rules is usually time-consuming and the rules are not full coverage. Compared to rule-based conversion, our statistical converters are much easier to built and empirically perform well. 6 Experiments 6.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. We follow this setting in this paper. We use CTB 5.0 as our main corpus and define the training"
P12-1025,P08-1102,0,0.324178,"Missing"
P12-1025,C08-1049,0,0.107848,"itted as a supplemental material for research purposes. 233 basic language units, i.e. words, word segmentation and POS tagging are important initial steps for Chinese language processing. Supervised learning with specifically defined training data has become a dominant paradigm. Joint approaches that resolve the two tasks simultaneously have received much attention in recent research. Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008; Sun, 2011). Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the “word-based” method"
P12-1025,P09-1059,0,0.816229,"acked sub-word tagging model, which combines strengths of both character-based and word-based approaches (Sun, 2011). First, different characterbased and word-based models are trained to produce multiple segmentation and tagging results. Second, the outputs of these coarse-grained models are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a finegrained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their method"
P12-1025,P09-1058,0,0.0932823,"ersion mainly leverages rich sets of hand-crafted rules to convert corpora. The construction of linguistic rules is usually time-consuming and the rules are not full coverage. Compared to rule-based conversion, our statistical converters are much easier to built and empirically perform well. 6 Experiments 6.1 Setting Previous studies on joint Chinese word segmentation and POS tagging have used the CTB in experiments. We follow this setting in this paper. We use CTB 5.0 as our main corpus and define the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b; Kruengkrai et al., 2009; Zhang and Clark, 2010; Sun, 2011). Jiang et al. (2009) present a preliminary study for the annotation adaptation topic, and conduct experiments with the extra PPD data4 . In other words, the CTB-sytle annotation is the target analysis while the PPD-style annotation is the complementary/auxiliary analysis. Our experiments for annotation ensemble follows their setting to lead to a fair comparison of our system and theirs. A CRF learning toolkit, wapiti5 (Lavergne et al., 2010), is used to resolve sequence labeling problems. Among several parameter estimation methods provided by wapiti, our aux"
P12-1025,P10-1052,0,0.0183469,"Missing"
P12-1025,W04-3236,0,0.08008,"s data set is submitted as a supplemental material for research purposes. 233 basic language units, i.e. words, word segmentation and POS tagging are important initial steps for Chinese language processing. Supervised learning with specifically defined training data has become a dominant paradigm. Joint approaches that resolve the two tasks simultaneously have received much attention in recent research. Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008; Sun, 2011). Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the"
P12-1025,P09-1006,0,0.053446,"re trained to produce multiple segmentation and tagging results. Second, the outputs of these coarse-grained models are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a finegrained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heter"
P12-1025,P08-1108,0,0.101577,"erent training data, we can construct multiple heterogeneous systems. These systems produce similar linguistic analysis which holds the same high level linguistic principles but differ in details. A very simple idea to take advantage of heterogeneous structures is to design a predictor which can predict a more accurate target structure based on the input, the less accurate target structure and complementary structures. This idea is very close to stacked learning (Wolpert, 1992), which is well developed for ensemble learning, and successfully applied to some NLP tasks, e.g. dependency parsing (Nivre and McDonald, 2008; Torres Martins et al., 2008). Formally speaking, our idea is to include two “levels” of processing. The first level includes one 235 AS ⇒ u:44; DEC ⇒ u:83; DEG ⇒ u:123; LB ⇒ p:1; OD ⇒ m:41; SP ⇒ u:1; VE ⇒ v:13; CS ⇒ c:3; d:1; MSP ⇒ c:2; u:1; CC ⇒ c:73; p:5; v:2; LC ⇒ f:51; Ng:3; v:1; u:1; VA ⇒ a:57; i:4; z:2; ad:1; b:1; VV ⇒ v:382; i:5; a:3; Vg:2; vn:2; n:2; p:2; w:1; AD ⇒ d:149; c:11; ad:6; z:4; a:3; v:2; n:1; r:1; m:1; f:1; t:1; CD ⇒ m:134; DEV ⇒ u:7; ETC ⇒ u:9; NT ⇒ t:98; PU ⇒ w:552; VC ⇒ v:32; BA ⇒ p:2; d:1; DT ⇒ r:15; b:1; PN ⇒ r:53; n:2; M ⇒ q:101; n:11; v:1; P ⇒ p:133; v:4; c:2; Vg:1;"
P12-1025,W95-0107,0,0.0773981,"independently built on different training data. The second level processing consists of an inference function h that takes as input hx, f1 (x), ..., fK (x)i3 and outputs a final prediction h(x, f1 (x), ..., fK (x)). The only difference between model ensemble and annotation ensemble is that the output spaces of model ensemble are the same while the output spaces of annotation ensemble are different. This framework is general and flexible, in the sense that it assumes almost nothing about the individual systems and take them as black boxes. 4.2 A Character-based Tagger With IOB2 representation (Ramshaw and Marcus, 1995), the problem of joint segmentation and tagging can be regarded as a character classification task. Previous work shows that the character-based approach is an effective method for Chinese lexical processing. Both of our feature- and structure-based stacking models employ base character-based taggers to generate multiple segmentation and tagging results. Our base tagger use a discriminative sequential classifier to predict the POS tag with positional information for each character. Each character can be assigned one of two possible boundary tags: “B” for a character that begins a word and “I”"
P12-1025,D11-1090,1,0.0791492,"finegrained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heterogeneous Annotations For Chinese word segmentation and POS tagging, supervised learning has become a dominant paradigm. Much of the progress is due to the development of both corpora and machine learning techniqu"
P12-1025,W10-4144,1,0.594589,"uce multiple segmentation and tagging results. Second, the outputs of these coarse-grained models are merged into sub-word sequences, which are further bracketed and labeled with POS tags by a finegrained sub-word tagger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heterogeneous Annotation"
P12-1025,C10-2139,1,0.656916,"tence of multiple resources, such data cannot be simply put together for training systems, because almost all of statistical NLP systems assume homogeneous annotation. Therefore, it is not only interesting but also important to study how to fully utilize heterogeneous resources to improve Chinese lexical processing. There are two main types of errors in statistical NLP: (1) the approximation error that is due to the intrinsic suboptimality of a model and (2) the estimation error that is due to having only finite training data. Take Chinese word segmentation for example. Our previous analysis (Sun, 2010) shows that one main intrinsic disadvantage of characterbased model is the difficulty in incorporating the whole word information, while one main disadvantage of word-based model is the weak ability to express word formation. In both models, the significant decrease of the prediction accuracy of out-ofvocabulary (OOV) words indicates the impact of the estimation error. The two essential characteristics about systematic diversity of heterogeneous annota234 tions can be utilized to reduce both approximation and estimation errors. 3.1 Analysis of the CTB and PPD Standards This paper focuses on tw"
P12-1025,P11-1139,1,0.549812,"processing tasks. We empirically analyze the diversity between two representative popular heterogeneous corpora, i.e. 232 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 232–241, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Penn Chinese Treebank (CTB) and PKU’s People’s Daily (PPD). To that end, we manually label 200 sentences from CTB with PPD-style annotations.1 Our analysis confirms the aforementioned two properties of heterogeneous annotations. Inspired by the sub-word tagging method introduced in (Sun, 2011), we propose a structure-based stacking model to fully utilize heterogeneous word structures to reduce the approximation error. In particular, joint word segmentation and POS tagging is addressed as a two step process. First, character-based taggers are respectively trained on heterogeneous annotations to produce multiple analysis. The outputs of these taggers are then merged into sub-word sequences, which are further re-segmented and tagged by a sub-word tagger. The sub-word tagger is designed to refine the tagging result with the help of heterogeneous annotations. To reduce the estimation er"
P12-1025,D08-1017,0,0.0484194,"Missing"
P12-1025,I11-1035,0,0.00729695,"agger. Their solution can be viewed as utilizing stacked learning to integrate heterogeneous models. Supervised segmentation and tagging can be improved by exploiting rich linguistic resources. Jiang et al. (2009) presented a preliminary study for annotation ensemble, which motivates our research as well as similar investigations for other NLP tasks, e.g. parsing (Niu et al., 2009; Sun et al., 2010). In their solution, heterogeneous data is used to train an auxiliary segmentation and tagging system to produce informative features for target prediction. Our previous work (Sun and Xu, 2011) and Wang et al. (2011) explored unlabeled data to enhance strong supervised segmenters and taggers. Both of their work fall into the category of feature induction based semi-supervised learning. In brief, their methods harvest useful string knowledge from unlabeled or automatically analyzed data, and apply the knowledge to design new features for discriminative learning. 3 About Heterogeneous Annotations For Chinese word segmentation and POS tagging, supervised learning has become a dominant paradigm. Much of the progress is due to the development of both corpora and machine learning techniques. Although several in"
P12-1025,P08-1101,0,0.0237511,"al material for research purposes. 233 basic language units, i.e. words, word segmentation and POS tagging are important initial steps for Chinese language processing. Supervised learning with specifically defined training data has become a dominant paradigm. Joint approaches that resolve the two tasks simultaneously have received much attention in recent research. Previous work has shown that joint solutions led to accuracy improvements over pipelined systems by avoiding segmentation error propagation and exploiting POS information to help segmentation (Ng and Low, 2004; Jiang et al., 2008a; Zhang and Clark, 2008; Sun, 2011). Two kinds of approaches are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the “word-based” method, also known as semi-Mar"
P12-1025,D10-1082,0,0.264343,"s are popular for joint word segmentation and POS tagging. The first is the “character-based” approach, where basic processing units are characters which compose words (Jiang et al., 2008a). In this kind of approach, the task is formulated as the classification of characters into POS tags with boundary information. For example, the label B-NN indicates that a character is located at the begging of a noun. Using this method, POS information is allowed to interact with segmentation. The second kind of solution is the “word-based” method, also known as semi-Markov tagging (Zhang and Clark, 2008; Zhang and Clark, 2010), where the basic predicting units are words themselves. This kind of solver sequentially decides whether the local sequence of characters makes up a word as well as its possible POS tag. Solvers may use previously predicted words and their POS information as clues to process a new word. In addition, we proposed an effective and efficient stacked sub-word tagging model, which combines strengths of both character-based and word-based approaches (Sun, 2011). First, different characterbased and word-based models are trained to produce multiple segmentation and tagging results. Second, the outputs"
P13-2016,I05-1055,0,0.0673805,"ant task in many natural language processing (NLP) applications. It is typically applicable in the text generation field, both for concept-to-text generation and text-totext generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. However, ordering a set of sentences into a coherent text is still a hard and challenging problem for computers. Previous works on sentence ordering mainly focus on the MDS task (Barzilay et al., 2002; Okazaki et al., 2004; Nie et al., 2006; Ji and Pulman, 2006; Madnani et al., 2007; Zhang et al., 2010; He et al., 2006; Bollegala et al., 2005; Bollegala et al., 2010). In this task, each summary sentence is extracted from a source document. The timestamp of the source documents and the adjacent sentences in the source documents can be used as important clues for ordering summary sentences. In this study, we investigate a more challenging and more general task of ordering a set of unordered sentences (e.g. randomly shuffle the * Related Work 3 3.1 Our Proposed Method Overview The task of text ordering can be modeled like (Cohen et al., 1998), as measuring the coherence of a text by summing the association strength of any sentence pa"
P13-2016,C10-2170,0,0.016843,"roduction Ordering texts is an important task in many natural language processing (NLP) applications. It is typically applicable in the text generation field, both for concept-to-text generation and text-totext generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. However, ordering a set of sentences into a coherent text is still a hard and challenging problem for computers. Previous works on sentence ordering mainly focus on the MDS task (Barzilay et al., 2002; Okazaki et al., 2004; Nie et al., 2006; Ji and Pulman, 2006; Madnani et al., 2007; Zhang et al., 2010; He et al., 2006; Bollegala et al., 2005; Bollegala et al., 2010). In this task, each summary sentence is extracted from a source document. The timestamp of the source documents and the adjacent sentences in the source documents can be used as important clues for ordering summary sentences. In this study, we investigate a more challenging and more general task of ordering a set of unordered sentences (e.g. randomly shuffle the * Related Work 3 3.1 Our Proposed Method Overview The task of text ordering can be modeled like (Cohen et al., 1998), as measuring the coherence of a text by summing th"
P13-2016,N09-3014,0,0.0147459,"our proposed method. 1 2 For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. Experimental evaluation indicated the importance of several learned lexical and syntactic features. However, the model only works well when using single feature, but unfortunately, it becomes worse when multiple features are combined. Barzilay and Lee (2004) investigated the utility of domain-specific content model for representing topic and topic shifts and the model performed well on the five selected domains. Nahnsen (2009) employed features which were based on discourse entities, shallow syntactic analysis, and temporal precedence relations retrieved from VerbOcean. However, the model does not perform well on datasets describing the consequences of events. Introduction Ordering texts is an important task in many natural language processing (NLP) applications. It is typically applicable in the text generation field, both for concept-to-text generation and text-totext generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. However, ordering a set of sentences into a"
P13-2016,P03-1069,0,0.161704,"g. adjacent sentences) of each sentence in the source document. In this paper, we investigate a more challenging task of ordering a set of unordered sentences without any contextual information. We introduce a set of features to characterize the order and coherence of natural language texts, and use the learning to rank technique to determine the order of any two sentences. We also propose to use the genetic algorithm to determine the total order of all sentences. Evaluation results on a news corpus show the effectiveness of our proposed method. 1 2 For works taking no use of source document, Lapata (2003) proposed a probabilistic model which learns constraints on sentence ordering from a corpus of texts. Experimental evaluation indicated the importance of several learned lexical and syntactic features. However, the model only works well when using single feature, but unfortunately, it becomes worse when multiple features are combined. Barzilay and Lee (2004) investigated the utility of domain-specific content model for representing topic and topic shifts and the model performed well on the five selected domains. Nahnsen (2009) employed features which were based on discourse entities, shallow s"
P13-2016,C04-1108,0,0.0300938,"model does not perform well on datasets describing the consequences of events. Introduction Ordering texts is an important task in many natural language processing (NLP) applications. It is typically applicable in the text generation field, both for concept-to-text generation and text-totext generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. However, ordering a set of sentences into a coherent text is still a hard and challenging problem for computers. Previous works on sentence ordering mainly focus on the MDS task (Barzilay et al., 2002; Okazaki et al., 2004; Nie et al., 2006; Ji and Pulman, 2006; Madnani et al., 2007; Zhang et al., 2010; He et al., 2006; Bollegala et al., 2005; Bollegala et al., 2010). In this task, each summary sentence is extracted from a source document. The timestamp of the source documents and the adjacent sentences in the source documents can be used as important clues for ordering summary sentences. In this study, we investigate a more challenging and more general task of ordering a set of unordered sentences (e.g. randomly shuffle the * Related Work 3 3.1 Our Proposed Method Overview The task of text ordering can be mode"
P13-2016,W07-2312,0,0.0384739,"Missing"
P13-2016,W06-1662,0,0.0210296,"describing the consequences of events. Introduction Ordering texts is an important task in many natural language processing (NLP) applications. It is typically applicable in the text generation field, both for concept-to-text generation and text-totext generation (Lapata, 2003), such as multiple document summarization (MDS), question answering and so on. However, ordering a set of sentences into a coherent text is still a hard and challenging problem for computers. Previous works on sentence ordering mainly focus on the MDS task (Barzilay et al., 2002; Okazaki et al., 2004; Nie et al., 2006; Ji and Pulman, 2006; Madnani et al., 2007; Zhang et al., 2010; He et al., 2006; Bollegala et al., 2005; Bollegala et al., 2010). In this task, each summary sentence is extracted from a source document. The timestamp of the source documents and the adjacent sentences in the source documents can be used as important clues for ordering summary sentences. In this study, we investigate a more challenging and more general task of ordering a set of unordered sentences (e.g. randomly shuffle the * Related Work 3 3.1 Our Proposed Method Overview The task of text ordering can be modeled like (Cohen et al., 1998), as measu"
P13-2016,N04-1015,0,\N,Missing
P13-2094,D08-1014,0,0.0770692,"has also been investigated (Lu et al. 2011b; Snyder and Barzilay, 2007; Zhu et al. 2009; Wang et al. 2010; Lu et al. 2009; Titov and McDonald, 2008). All the above previous works are working under a monolingual setting, and to the best of our knowledge, there exists no previous work on cross-language review rating prediction. It is noteworthy that a few studies have been conducted for the task of cross-lingual sentiment classification or text classification, which aims to make use of labeled data in a language for the binary classification task in a different language (Mihalcea et al., 2007; Banea et al., 2008; Wan 2009; Lu et al. 2011a; Meng et al. 2012; Shi et al., 2010; Prettenhofer and Stein 2010). However, the binary classification task is very different from the regression task studied in this paper, and the proposed methods in the above previous works cannot be directly applied. 3 Problem Definition and Baseline Approaches language, and any regression algorithm (e.g. logistic regression, least squares regression, KNN regressor) can be applied for learning and prediction. In this study, without loss of generality, we adopt the widely used regression SVM (Vapnik 1995; Joachims 1999) implemente"
P13-2094,P07-1056,0,0.11826,"Missing"
P13-2094,P11-1033,0,0.0337296,"on review rating prediction model this problem as a multi-class classification task or a regression task. Various features have been exploited from the review text, including words, patterns, syntactic structure, and semantic topic (Qu et al. 2010; Pang and Lee, 2005; Leung et al. 2006; Ganu et al. 2009). Traditional learn526 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 526–531, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ing models, such as SVM, are adopted for rating prediction. Most recently, Li et al. (2011) propose a novel tensor-based learning framework to incorporate reviewer and product information into the text based learner for rating prediction. Saggion et al. (2012) study the use of automatic text summaries instead of the full reviews for movie review rating prediction. In addition to predicting the overall rating of a full review, multi-aspect rating prediction has also been investigated (Lu et al. 2011b; Snyder and Barzilay, 2007; Zhu et al. 2009; Wang et al. 2010; Lu et al. 2009; Titov and McDonald, 2008). All the above previous works are working under a monolingual setting, and to the"
P13-2094,P12-1060,0,0.0334994,"nyder and Barzilay, 2007; Zhu et al. 2009; Wang et al. 2010; Lu et al. 2009; Titov and McDonald, 2008). All the above previous works are working under a monolingual setting, and to the best of our knowledge, there exists no previous work on cross-language review rating prediction. It is noteworthy that a few studies have been conducted for the task of cross-lingual sentiment classification or text classification, which aims to make use of labeled data in a language for the binary classification task in a different language (Mihalcea et al., 2007; Banea et al., 2008; Wan 2009; Lu et al. 2011a; Meng et al. 2012; Shi et al., 2010; Prettenhofer and Stein 2010). However, the binary classification task is very different from the regression task studied in this paper, and the proposed methods in the above previous works cannot be directly applied. 3 Problem Definition and Baseline Approaches language, and any regression algorithm (e.g. logistic regression, least squares regression, KNN regressor) can be applied for learning and prediction. In this study, without loss of generality, we adopt the widely used regression SVM (Vapnik 1995; Joachims 1999) implemented in the SVMLight toolkit 2 as the basic regr"
P13-2094,P07-1123,0,0.0880788,"spect rating prediction has also been investigated (Lu et al. 2011b; Snyder and Barzilay, 2007; Zhu et al. 2009; Wang et al. 2010; Lu et al. 2009; Titov and McDonald, 2008). All the above previous works are working under a monolingual setting, and to the best of our knowledge, there exists no previous work on cross-language review rating prediction. It is noteworthy that a few studies have been conducted for the task of cross-lingual sentiment classification or text classification, which aims to make use of labeled data in a language for the binary classification task in a different language (Mihalcea et al., 2007; Banea et al., 2008; Wan 2009; Lu et al. 2011a; Meng et al. 2012; Shi et al., 2010; Prettenhofer and Stein 2010). However, the binary classification task is very different from the regression task studied in this paper, and the proposed methods in the above previous works cannot be directly applied. 3 Problem Definition and Baseline Approaches language, and any regression algorithm (e.g. logistic regression, least squares regression, KNN regressor) can be applied for learning and prediction. In this study, without loss of generality, we adopt the widely used regression SVM (Vapnik 1995; Joach"
P13-2094,W02-1011,0,0.0174208,"Missing"
P13-2094,P05-1015,0,0.669852,"age and the target language to collaboratively determine the confidently predicted ones out of the unlabeled reviews, and then use the selected examples to enlarge the training set. Evaluation results on several datasets show that our proposed coregression algorithm can consistently improve the prediction results. 2 Related Work Most previous works on review rating prediction model this problem as a multi-class classification task or a regression task. Various features have been exploited from the review text, including words, patterns, syntactic structure, and semantic topic (Qu et al. 2010; Pang and Lee, 2005; Leung et al. 2006; Ganu et al. 2009). Traditional learn526 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 526–531, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ing models, such as SVM, are adopted for rating prediction. Most recently, Li et al. (2011) propose a novel tensor-based learning framework to incorporate reviewer and product information into the text based learner for rating prediction. Saggion et al. (2012) study the use of automatic text summaries instead of the full reviews for movie review rati"
P13-2094,P10-1114,0,0.0390569,"2009; Wang et al. 2010; Lu et al. 2009; Titov and McDonald, 2008). All the above previous works are working under a monolingual setting, and to the best of our knowledge, there exists no previous work on cross-language review rating prediction. It is noteworthy that a few studies have been conducted for the task of cross-lingual sentiment classification or text classification, which aims to make use of labeled data in a language for the binary classification task in a different language (Mihalcea et al., 2007; Banea et al., 2008; Wan 2009; Lu et al. 2011a; Meng et al. 2012; Shi et al., 2010; Prettenhofer and Stein 2010). However, the binary classification task is very different from the regression task studied in this paper, and the proposed methods in the above previous works cannot be directly applied. 3 Problem Definition and Baseline Approaches language, and any regression algorithm (e.g. logistic regression, least squares regression, KNN regressor) can be applied for learning and prediction. In this study, without loss of generality, we adopt the widely used regression SVM (Vapnik 1995; Joachims 1999) implemented in the SVMLight toolkit 2 as the basic regressor. For comparative analysis, we simply use t"
P13-2094,C10-1103,0,0.528769,"the source language and the target language to collaboratively determine the confidently predicted ones out of the unlabeled reviews, and then use the selected examples to enlarge the training set. Evaluation results on several datasets show that our proposed coregression algorithm can consistently improve the prediction results. 2 Related Work Most previous works on review rating prediction model this problem as a multi-class classification task or a regression task. Various features have been exploited from the review text, including words, patterns, syntactic structure, and semantic topic (Qu et al. 2010; Pang and Lee, 2005; Leung et al. 2006; Ganu et al. 2009). Traditional learn526 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 526–531, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ing models, such as SVM, are adopted for rating prediction. Most recently, Li et al. (2011) propose a novel tensor-based learning framework to incorporate reviewer and product information into the text based learner for rating prediction. Saggion et al. (2012) study the use of automatic text summaries instead of the full reviews f"
P13-2094,D10-1103,0,0.0372344,"Missing"
P13-2094,N07-1038,0,0.0495562,"s 526–531, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics ing models, such as SVM, are adopted for rating prediction. Most recently, Li et al. (2011) propose a novel tensor-based learning framework to incorporate reviewer and product information into the text based learner for rating prediction. Saggion et al. (2012) study the use of automatic text summaries instead of the full reviews for movie review rating prediction. In addition to predicting the overall rating of a full review, multi-aspect rating prediction has also been investigated (Lu et al. 2011b; Snyder and Barzilay, 2007; Zhu et al. 2009; Wang et al. 2010; Lu et al. 2009; Titov and McDonald, 2008). All the above previous works are working under a monolingual setting, and to the best of our knowledge, there exists no previous work on cross-language review rating prediction. It is noteworthy that a few studies have been conducted for the task of cross-lingual sentiment classification or text classification, which aims to make use of labeled data in a language for the binary classification task in a different language (Mihalcea et al., 2007; Banea et al., 2008; Wan 2009; Lu et al. 2011a; Meng et al. 2012; Shi et"
P13-2094,P08-1036,0,0.0827156,"nal Linguistics ing models, such as SVM, are adopted for rating prediction. Most recently, Li et al. (2011) propose a novel tensor-based learning framework to incorporate reviewer and product information into the text based learner for rating prediction. Saggion et al. (2012) study the use of automatic text summaries instead of the full reviews for movie review rating prediction. In addition to predicting the overall rating of a full review, multi-aspect rating prediction has also been investigated (Lu et al. 2011b; Snyder and Barzilay, 2007; Zhu et al. 2009; Wang et al. 2010; Lu et al. 2009; Titov and McDonald, 2008). All the above previous works are working under a monolingual setting, and to the best of our knowledge, there exists no previous work on cross-language review rating prediction. It is noteworthy that a few studies have been conducted for the task of cross-lingual sentiment classification or text classification, which aims to make use of labeled data in a language for the binary classification task in a different language (Mihalcea et al., 2007; Banea et al., 2008; Wan 2009; Lu et al. 2011a; Meng et al. 2012; Shi et al., 2010; Prettenhofer and Stein 2010). However, the binary classification t"
P13-2094,P02-1053,0,0.0185029,"Missing"
P13-2094,P09-1027,1,0.870827,"tigated (Lu et al. 2011b; Snyder and Barzilay, 2007; Zhu et al. 2009; Wang et al. 2010; Lu et al. 2009; Titov and McDonald, 2008). All the above previous works are working under a monolingual setting, and to the best of our knowledge, there exists no previous work on cross-language review rating prediction. It is noteworthy that a few studies have been conducted for the task of cross-lingual sentiment classification or text classification, which aims to make use of labeled data in a language for the binary classification task in a different language (Mihalcea et al., 2007; Banea et al., 2008; Wan 2009; Lu et al. 2011a; Meng et al. 2012; Shi et al., 2010; Prettenhofer and Stein 2010). However, the binary classification task is very different from the regression task studied in this paper, and the proposed methods in the above previous works cannot be directly applied. 3 Problem Definition and Baseline Approaches language, and any regression algorithm (e.g. logistic regression, least squares regression, KNN regressor) can be applied for learning and prediction. In this study, without loss of generality, we adopt the widely used regression SVM (Vapnik 1995; Joachims 1999) implemented in the S"
P14-1042,D11-1037,0,0.150172,"Missing"
P14-1042,P08-1006,0,0.451573,"Missing"
P14-1042,P06-2006,0,0.170001,"Missing"
P14-1042,J08-1002,0,0.0834445,"latively high precision but considerably low recall. There are two similarities between our parser and theirs: 1) both parsers produce dependency graphs rather trees; 2) both parser employ a beam decoder that does not guarantee global optimality. To build NLP application, e.g. information extraction, systems upon GR parsing, such property merits attention. A good trade-off between the precision and the recall may have a great impact on final results. 453 4.4 Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our work very much. However, simple adoption of their systems does not resolve Chinese GR parsing well because the GR graphs are much more complicated. Our investigation on the K-permutation transition system advances the capacity of existing methods. Local vs. Non-local Although the micro accuracy of all dep"
P14-1042,P07-1032,0,0.219132,"Missing"
P14-1042,J08-4003,0,0.138881,"say the sequence of transitions is an oracle sequence. And we define A¯ci = A − Aci for the arcs to be built in ci . In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack. A set of S HIFT/R EDUCE actions are performed sequentially to consume words from the queue and update the partial parsing results. Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) ind"
P14-1042,P09-1040,0,0.0718205,"Missing"
P14-1042,J07-4004,0,0.510125,"ortant role in linguistic theorizing, within a variety of approaches ranging from generative grammar to functional theories. For example, several computational grammar formalisms, such as Lexical Function Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001) and Head-driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994) encode grammatical functions directly. In particular, GRs can be viewed as the dependency backbone of an LFG analysis that provide general linguistic insights, and have great potential advantages for NLP applications, (Kaplan et al., 2004; Briscoe and Carroll, 2006; Clark and Curran, 2007a; Miyao et al., 2007). ∗ Recent years have seen the introduction of a number of treebank-guided statistical parsers capable of generating considerably accurate parses for Chinese. With the high-quality GR resource at hand, we study data-driven GR parsing. Previous work on dependency parsing mainly focused on structures that can be represented in terms of directed trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually studied two transition systems that can generate more general graphs rather than trees. Inspired by their work, we study transition-based m"
P14-1042,W04-2407,0,0.0954544,"ing results. Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually introduced two transition systems that can generate specific graphs rather than trees. Inspired by their work, we study transition-based approach to build GR graphs. 3.2 (σ, j|β, A) ⇒ (σ|j, β, A) (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(j, l, i)}) (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(i, l, j)}) (σ|i, β, A) ⇒ (σ, β, A) (σ|ik |. . . |i2 |i1 , β, A) ⇒ (σ|i1 |ik |. . . |i2 , β, A) 3.3 On"
P14-1042,D09-1085,0,0.138373,"Missing"
P14-1042,J07-3004,0,0.342597,"Missing"
P14-1042,P80-1024,0,0.634562,"Missing"
P14-1042,C08-1095,0,0.778542,"tionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually introduced two transition systems that can generate specific graphs rather than trees. Inspired by their work, we study transition-based approach to build GR graphs. 3.2 (σ, j|β, A) ⇒ (σ|j, β, A) (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(j, l, i)}) (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(i, l, j)}) (σ|i, β, A) ⇒ (σ, β, A) (σ|ik |. . . |i2 |i1 , β, A) ⇒ (σ|i1 |ik |. . . |i2 , β, A) 3.3 Online Reordering Among existing systems, Sagae and Tsujii’s is designed for projective graphs (denoted by G1 in Definition 1), and Titov et al.’s handles only a specific subset of non-proje"
P14-1042,W03-2401,0,0.0722133,"ction Algorithm Our treebank conversion algorithm borrows key insights from Lexical Functional Grammar (LFG; Bresnan and Kaplan, 1982; Dalrymple, 2001). LFG posits two levels of representation: c(onstituent)-structure and f(unctional)-structure minimally. C-structure is represented by phrasestructure trees, and captures surface syntactic configurations such as word order, while f-structure encodes grammatical functions. It is easy to extract a dependency backbone which approximates basic predicate-argument-adjunct structures from f-structures. The construction of the widely used PARC DepBank (King et al., 2003) is a good example. Beyond CTB annotations: tracing more. Natural languages do not always interpret linguistic material locally. In order to obtain accurate and complete GR, predicate-argument, or logical form representations, a hallmark of deep grammars is that they usually involve a non-local dependency resolution mechanism. CTB trees utilize ECs and coindexed materials to represent long-distance dependencies. An EC is a nominal element that does not have any phonological content and is therefore unpronounced. Two kinds of anaphoric ECs, i.e. big PRO and trace, are annotated in CTB. Theoreti"
P14-1042,P12-1026,1,0.830759,"0 ) strictly precedes any L(j), j ∈ σ). If k > 0, we set ti to ROTATEk ; else we set ti to S HIFT. The approximation assumes L(σ) is completely ordered except the first element, and insert the first element to its proper place each time. Definition 2. We define GˆK as the graphs the oracle of which can be extracted by SK with the approximation procedure. 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. It can be inferred similarly that Theorem 1 and ˆ However, the GˆK is Theorem 2 also"
P14-1042,P10-1001,0,0.0800084,"is the set of all graphs without self-loop. Proof. It follows immediately from the fact that G ∈ G|V |, ∀G = hV, Ei. The transition systems introduced in (Sagae and Tsujii, 2008) and (Titov et al., 2009) can be viewed as S1 1 and S2 . 1 Though Sagae and Tsujii (2008) introduced additional constraints to exclude cyclic path, the fundamental transition mechanism of their system is the same to S1 . 451 weight vector. We also use parameter averaging and early update to achieve better training. Developing features has been shown crucial to advancing the state-of-the-art in dependency tree parsing (Koo and Collins, 2010; Zhang and Nivre, 2011). To build accurate deep dependency parsers, we utilize a large set of features for disambiguation. See the notes included in the supplymentary material for details. To improve the performance, we also apply the technique of beam search, which keep a beam of transition sequences with highest scores when parsing. w1 w2 w3 w4 w5 w6 w7 w8 w9 Figure 5: A graph that can be parsed with S3 with a transition sequence SSSSR3 SR3 APAPR2 R3 SR3 SR3 APAPAPAPAP, where S stands for S HIFT, R for ROTATE, A for L EFT-A RC, and P for P OP. But the approximate procedure fails to find the"
P14-1042,Q13-1025,1,0.829862,"else we set ti to S HIFT. The approximation assumes L(σ) is completely ordered except the first element, and insert the first element to its proper place each time. Definition 2. We define GˆK as the graphs the oracle of which can be extracted by SK with the approximation procedure. 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. It can be inferred similarly that Theorem 1 and ˆ However, the GˆK is Theorem 2 also hold for G’s. not equal to GK in non-trivial cases. Theorem 4. Gˆ"
P14-1042,D11-1090,1,0.801829,"phical order (here we assume L(j0 ) strictly precedes any L(j), j ∈ σ). If k > 0, we set ti to ROTATEk ; else we set ti to S HIFT. The approximation assumes L(σ) is completely ordered except the first element, and insert the first element to its proper place each time. Definition 2. We define GˆK as the graphs the oracle of which can be extracted by SK with the approximation procedure. 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. It can be inferred similarly that Theorem 1 a"
P14-1042,C10-2162,0,0.0740776,"igure 1: An example: Pudong recently enacted regulatory documents involving the economic field. The symbol “*ldd” indicates long-distance dependencies; “subj*ldd” between the word “涉及/involve” and the word “文件/documents” represents a long-range subject-predicate relation. The arguments and adjuncts of the coordinated verbs, namely “颁布/issue” and “实行/practice,” are separately yet distributively linked the two heads. structure treebank, namely CTB. Conceptually, this conversion is similar to the conversions from CTB structures to representations in deep grammar formalisms (Tse and Curran, 2010; Yu et al., 2010; Guo et al., 2007; Xia, 2001). However, our work is grounded in GB, which is the linguistic basis of the construction of CTB. We argue that this theoretical choice makes the conversion process more compatible with the original annotations and therefore more accurate. We use directed graphs to explicitly encode bi-lexical dependencies involved in coordination, raising/control constructions, extraction, topicalization, and many other complicated phenomena. Fig. 1 shows an example of such a GR graph and its original CTB annotation. our problem, we extend Titov et al.’s work and study what we cal"
P14-1042,W09-3825,0,0.0567214,"we set ti to ROTATEk ; else we set ti to S HIFT. The approximation assumes L(σ) is completely ordered except the first element, and insert the first element to its proper place each time. Definition 2. We define GˆK as the graphs the oracle of which can be extracted by SK with the approximation procedure. 4.1 Experimental setup CTB is a segmented, part-of-speech (POS) tagged, and fully bracketed corpus in the constituency formalism, and very popular to evaluate fundamental NLP tasks, including word segmentation (Sun and Xu, 2011), POS tagging (Sun and Uszkoreit, 2012), and syntactic parsing (Zhang and Clark, 2009; Sun and Wan, 2013). We use CTB 6.0 and define the training, development and test sets according to the CoNLL 2009 shared task. We use gold-standard word segmentation and POS taging results as inputs. All transition-based parsing models are trained with beam 16 and iteration 30. Overall precision/recall/f-score with respect to dependency tokens is reported. To evaluate the ability to recover non-local dependencies, the recall of such dependencies are reported too. It can be inferred similarly that Theorem 1 and ˆ However, the GˆK is Theorem 2 also hold for G’s. not equal to GK in non-trivial"
P14-1042,P09-1039,0,0.103764,"e. And we define A¯ci = A − Aci for the arcs to be built in ci . In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack. A set of S HIFT/R EDUCE actions are performed sequentially to consume words from the queue and update the partial parsing results. Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et al. (2009) individually introduced two transition systems that can generate"
P14-1042,P11-1069,0,0.149804,"le the improved numeric accuracies practically certify the benefits. The two points merit further exploration to more expressive transition systems for deep dependency parsing, at least for Chinese. The labeled evaluation scores on the final test data are presented in Tab. 4. Test S5 UP 83.93 UR 79.82 UF 81.82 LRL 80.94 LRNL 54.38 Table 4: Performance on the test data. 4.3 Precision vs. Recall A noteworthy thing about the overall performance is that the precision is promising but the recall is too low behind. This difference is consistent with the result obtained by a shift-reduce CCG parser (Zhang and Clark, 2011). The functor-argument dependencies generated by that parser also has a relatively high precision but considerably low recall. There are two similarities between our parser and theirs: 1) both parsers produce dependency graphs rather trees; 2) both parser employ a beam decoder that does not guarantee global optimality. To build NLP application, e.g. information extraction, systems upon GR parsing, such property merits attention. A good trade-off between the precision and the recall may have a great impact on final results. 453 4.4 Based on converted fine-grained linguistic annotations, success"
P14-1042,C10-1122,0,0.261191,"gulatory 文件 document Figure 1: An example: Pudong recently enacted regulatory documents involving the economic field. The symbol “*ldd” indicates long-distance dependencies; “subj*ldd” between the word “涉及/involve” and the word “文件/documents” represents a long-range subject-predicate relation. The arguments and adjuncts of the coordinated verbs, namely “颁布/issue” and “实行/practice,” are separately yet distributively linked the two heads. structure treebank, namely CTB. Conceptually, this conversion is similar to the conversions from CTB structures to representations in deep grammar formalisms (Tse and Curran, 2010; Yu et al., 2010; Guo et al., 2007; Xia, 2001). However, our work is grounded in GB, which is the linguistic basis of the construction of CTB. We argue that this theoretical choice makes the conversion process more compatible with the original annotations and therefore more accurate. We use directed graphs to explicitly encode bi-lexical dependencies involved in coordination, raising/control constructions, extraction, topicalization, and many other complicated phenomena. Fig. 1 shows an example of such a GR graph and its original CTB annotation. our problem, we extend Titov et al.’s work and"
P14-1042,P11-2033,0,0.0356007,"s without self-loop. Proof. It follows immediately from the fact that G ∈ G|V |, ∀G = hV, Ei. The transition systems introduced in (Sagae and Tsujii, 2008) and (Titov et al., 2009) can be viewed as S1 1 and S2 . 1 Though Sagae and Tsujii (2008) introduced additional constraints to exclude cyclic path, the fundamental transition mechanism of their system is the same to S1 . 451 weight vector. We also use parameter averaging and early update to achieve better training. Developing features has been shown crucial to advancing the state-of-the-art in dependency tree parsing (Koo and Collins, 2010; Zhang and Nivre, 2011). To build accurate deep dependency parsers, we utilize a large set of features for disambiguation. See the notes included in the supplymentary material for details. To improve the performance, we also apply the technique of beam search, which keep a beam of transition sequences with highest scores when parsing. w1 w2 w3 w4 w5 w6 w7 w8 w9 Figure 5: A graph that can be parsed with S3 with a transition sequence SSSSR3 SR3 APAPR2 R3 SR3 SR3 APAPAPAPAP, where S stands for S HIFT, R for ROTATE, A for L EFT-A RC, and P for P OP. But the approximate procedure fails to find the oracle, since R2 R3 in"
P14-1042,N12-1030,0,0.379065,"two similarities between our parser and theirs: 1) both parsers produce dependency graphs rather trees; 2) both parser employ a beam decoder that does not guarantee global optimality. To build NLP application, e.g. information extraction, systems upon GR parsing, such property merits attention. A good trade-off between the precision and the recall may have a great impact on final results. 453 4.4 Based on converted fine-grained linguistic annotations, successful English deep parsers, such as C&C (Clark and Curran, 2007b) and Enju (Miyao and Tsujii, 2008), have been evaluated (Yu et al., 2011; Tse and Curran, 2012). We also borrow many ideas from recent advances in deep syntactic or semantic parsing for English. In particular, Sagae and Tsujii (2008)’s and Titov et al. (2009)’s studies on transition-based deep dependency parsing motivated our work very much. However, simple adoption of their systems does not resolve Chinese GR parsing well because the GR graphs are much more complicated. Our investigation on the K-permutation transition system advances the capacity of existing methods. Local vs. Non-local Although the micro accuracy of all dependencies are considerably good, the ability of current state"
P14-1042,P10-1034,0,0.268424,"Missing"
P14-1042,W03-3023,0,0.200374,"), cm ∈ Ct , and Acm = A, we say the sequence of transitions is an oracle sequence. And we define A¯ci = A − Aci for the arcs to be built in ci . In a typical transition-based parsing process, the input words are put into a queue and partially built structures are organized by a stack. A set of S HIFT/R EDUCE actions are performed sequentially to consume words from the queue and update the partial parsing results. Data-Driven Dependency Parsing Data-driven, grammar-free dependency parsing has received an increasing amount of attention in the past decade. Such approaches, e.g. transitionbased (Yamada and Matsumoto, 2003; Nivre, 2008) and graph-based (McDonald, 2006; Torres Martins et al., 2009) models have attracted the most attention of dependency parsing in recent years. Transition-based parsers utilize transition systems to derive dependency trees together with treebank-induced statistical models for predicting transitions. This approach was pioneered by (Yamada and Matsumoto, 2003) and (Nivre et al., 2004). Most research concentrated on surface syntactic structures, and the majority of existing approaches are limited to producing only trees. We notice two exceptions. Sagae and Tsujii (2008) and Titov et"
P14-1042,W11-2907,0,0.194823,"Missing"
P14-1042,N04-1013,0,\N,Missing
P15-1149,D11-1031,0,0.388421,"ic features are able to improve the f-score achieved by the “PC+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improve the complete match with respect to the whole sentence. 5.5 Comparison to the State-of-the-art We compare our results with the best published CCG parsing performance obtained by the models presented in (Auli and Lopez, 2011) and (Xu et al., 2014)3 . Auli and Lopez (2011) reported best numeric performance. The performance is evaluated on sentences that can be parsed by their model. Xu et al. (2014) reported the best published results for sentences with full coverage. All results on the test set is shown in Table 3. Even without any syntactic features, our parser achieves accuracies that are superior to Xu et al.’s parser and comparable to Auli and Lopez’s system. When unlabeled syntactic trees are provided, our parser outperform the state-of-the-art. 3 The unlabeled parsing results are not reported in the original"
P15-1149,W13-2322,0,0.182655,"Missing"
P15-1149,D11-1037,0,0.0404513,"Missing"
P15-1149,C10-1011,0,0.146901,"ncy trees provided by the CCGBank to obtain necessary information for graph parsing. However, different from experiments in the CCG parsing literature, we use no grammar information. Neither lexical categories nor CCG derivations are utilized. All experiments were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger1 (Huang et al., 2010), and automatically parsed dependency trees that are generated by our in-house implementation of the transition-based model presented in (Zhang and Nivre, 2011) as well as a 2nd-order graph-based parser2 (Bohnet, 2010). The accuracy of these preprocessors is shown in Table 1. We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times. For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion. Previous research on dependency parsing shows that structured perceptron (Collins, 2002) is one of the strongest discriminative learning algorithms. To estimate θ’s of different models, we utilize the averaged perceptron algorithm. W"
P15-1149,P07-1032,0,0.195199,"natory Categorial Grammar (CCG; Steedman, 2000) is a linguistically expressive grammar formalism which has a transparent yet elegant interface between syntax and semantics. By assigning each lexical category a dependency interpretation, we can derive typed dependency structures from CCG derivations (Clark et al., 2002), providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). Such semantic-oriented dependency structures have been shown very helpful for NLP ap∗ Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicab"
P15-1149,C04-1041,0,0.100725,"Missing"
P15-1149,J07-4004,0,0.619934,"natory Categorial Grammar (CCG; Steedman, 2000) is a linguistically expressive grammar formalism which has a transparent yet elegant interface between syntax and semantics. By assigning each lexical category a dependency interpretation, we can derive typed dependency structures from CCG derivations (Clark et al., 2002), providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). Such semantic-oriented dependency structures have been shown very helpful for NLP ap∗ Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicab"
P15-1149,P02-1042,0,0.0866988,"nk, from the Penn Treebank (PTB; Marcus et al., 1993). In CCGBank, PTB phrase-structure trees have been transformed into normal-form CCG derivations, and deep bi-lexical dependency graphs that encode functor-argument strcutures have been extracted from these derivations using coindexation information. The typed dependency analysis provides a useful approximation to the underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler and Penn, 2010). On the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to"
P15-1149,W02-1001,0,0.704001,"ted by our in-house implementation of the transition-based model presented in (Zhang and Nivre, 2011) as well as a 2nd-order graph-based parser2 (Bohnet, 2010). The accuracy of these preprocessors is shown in Table 1. We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times. For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion. Previous research on dependency parsing shows that structured perceptron (Collins, 2002) is one of the strongest discriminative learning algorithms. To estimate θ’s of different models, we utilize the averaged perceptron algorithm. We implement our own the predicate- and argument-centric models. To perform tree parsing, we re-use the open-source implementation provided by the mate-tool. See the source code attached for details. We set iteration 5 to train predicate- and argument-centric models and 10 for the tree approximation model. To perform dual decomposition, we set the maximum iteration 200. Experimental Setup 1 CCGbank is a translation of the Penn Treebank into a corpus of"
P15-1149,C96-1058,0,0.436886,"Missing"
P15-1149,P10-1035,0,0.517446,", providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). Such semantic-oriented dependency structures have been shown very helpful for NLP ap∗ Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres M"
P15-1149,J07-3004,0,0.682088,"Missing"
P15-1149,D10-1002,0,0.192969,"nt structure. Our experiments were performed using CCGBank which was split into three subsets for training (Sections 02-21), development testing (Section 00) and the final test (Section 23). We also use the syntactic dependency trees provided by the CCGBank to obtain necessary information for graph parsing. However, different from experiments in the CCG parsing literature, we use no grammar information. Neither lexical categories nor CCG derivations are utilized. All experiments were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger1 (Huang et al., 2010), and automatically parsed dependency trees that are generated by our in-house implementation of the transition-based model presented in (Zhang and Nivre, 2011) as well as a 2nd-order graph-based parser2 (Bohnet, 2010). The accuracy of these preprocessors is shown in Table 1. We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times. For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion. Previous resear"
P15-1149,W03-2401,0,0.0496616,"underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler and Penn, 2010). On the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the"
P15-1149,P10-1001,0,0.24472,"Missing"
P15-1149,D10-1125,0,0.608752,"is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency parser that efficiently produces globally optimal CCG dependency graphs acc"
P15-1149,Q13-1018,0,0.0403024,"Missing"
P15-1149,C12-2077,0,0.0413808,"Missing"
P15-1149,J93-2004,0,0.0536153,"general dependency graphs, rather than trees. Nevertheless, empirical evaluation indicates that explicitly or implicitly using tree-structured information plays an essential role. The result also suggests that a wider range of complicated linguistic phenomena beyond surface syntax can be well modeled even without explicitly using grammars. Our algorithm is also applicable to other graphstructured representations, e.g. HPSG predicateargument analysis (Miyao et al., 2004). 2 Related Work Hockenmaier and Steedman (2007) developed linguistic resources, namely CCGBank, from the Penn Treebank (PTB; Marcus et al., 1993). In CCGBank, PTB phrase-structure trees have been transformed into normal-form CCG derivations, and deep bi-lexical dependency graphs that encode functor-argument strcutures have been extracted from these derivations using coindexation information. The typed dependency analysis provides a useful approximation to the underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler a"
P15-1149,S14-2082,0,0.247471,"ls as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been successfully applied to several NLP tasks, including parsing (Koo et al., 2010; Rush et al., 2010) and machine translation (Rush and Collins, 2011). To provide principled decoding for our factorization parser, we employ the dual decomposition technique. Our work directly follows (Koo et al., 2010). The two basic factorizations are similar to the model introduced in (Martins and Almeida, 2014). Llu´ıs et al. (2013) introduced a dual decomposition based joint model for joint syntactic and semantic parsing. They are concerned with shallow semantic representation, i.e. Semantic Role Labeling, whose graphs are sparse. Different from their concern on integrating syntactic parsing and semantic role labeling under 1storder factorization, we are interested in designing higher-order factorization models for more dense and general linguistic graphs. 3 3.1 Graph Factorization Background Notations Consider a sentence s = hw, pi with words w = w1 w2 · · · wn and POS-tags p = p1 p2 · · · pn . Fi"
P15-1149,P05-1012,0,0.61802,"rsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency pars"
P15-1149,E06-1011,0,0.927776,"014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the Danish Treebank. Encouraged by their work, we study factorization models as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been successfully applied to several NLP tasks, including parsing (Koo et al., 2010; Rush et al., 2010) and machine translation (Rush and Collins, 2011). To provide principled d"
P15-1149,H05-1066,0,0.802292,"rsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade, the techniques for datadriven dependency parsing has made a great progress (McDonald et al., 2005a,b; Nivre et al., 2004; Torres Martins et al., 2009; Koo et al., 2010). The major advantage of the data-driven architecture is complementary to the grammardriven one. On one hand, data-driven approaches make essential uses of machine learning from linguistic annotations and are flexible to produce analysis for arbitrary sentences. On the other hand, without hard constraints, parsing algorithms for spanning specific types of graphs, e.g. projective (Eisner, 1996) and 1-endpoint-crossing trees (Pitler et al., 2013), can be of low complexity. This paper proposes a new data-driven dependency pars"
P15-1149,W04-2407,0,0.109566,"Missing"
P15-1149,P08-1108,0,0.0413116,"PC+AC+TA Gr PC+AC+TA Tr PC+AC+TA Auli and Lopez Xu et al. UP 93.03 93.71 93.63 93.08 93.15 UR 92.03 92.72 92.83 92.44 91.06 UF 92.53 93.21 93.23 92.76 92.09 UEM 32.61 38.14 37.47 37.56 Table 3: Comparing the state-of-art with our models on test set. based. The architecture of the syntactic tree parser does not affect the results much. The two tree parsers give identical attachment scores, and lead to similar graph parsing accuracy. This result is somehow non-obvious given that the combination of a graph-based and transition-based parser usually gives significantly better parsing performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Although the target representation of our parser is general graphs rather trees, implicitly or explicitly using tree-structured information plays an essential role. Syntactic features are able to improve the f-score achieved by the “PC+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improv"
P15-1149,Q13-1002,0,0.360478,"Missing"
P15-1149,Q14-1030,0,0.15802,"lexical category a dependency interpretation, we can derive typed dependency structures from CCG derivations (Clark et al., 2002), providing a useful approximation to the underlying meaning representations. To date, CCG parsers are among the most competitive systems for generating such deep bi-lexical dependencies that appropriately encode a wide range of local and non-local syntacto-semantic information (Clark and Curran, 2007a; Bender et al., 2011). Such semantic-oriented dependency structures have been shown very helpful for NLP ap∗ Email correspondence. plications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by grammar-guided parsers (Clark and Curran, 2007b; Fowler and Penn, 2010). The main challenge is that a deep-grammar-guided model usually can only produce limited coverage and corresponding parsing algorithms is of relatively high complexity. Robustness and efficiency, thus, are two major problems for handling practical tasks. To increase the applicability of such parsers, lexical or syntactic pruning has been shown necessary (Clark and Curran, 2004; Matsuzaki et al., 2007; Sagae et al., 2007; Zhang and Clark, 2011). In the past decade"
P15-1149,P11-1008,0,0.0345487,"C+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improve the complete match with respect to the whole sentence. 5.5 Comparison to the State-of-the-art We compare our results with the best published CCG parsing performance obtained by the models presented in (Auli and Lopez, 2011) and (Xu et al., 2014)3 . Auli and Lopez (2011) reported best numeric performance. The performance is evaluated on sentences that can be parsed by their model. Xu et al. (2014) reported the best published results for sentences with full coverage. All results on the test set is shown in Table 3. Even without any syntactic features, our parser achieves accuracies that are superior to Xu et al.’s parser and comparable to Auli and Lopez’s system. When unlabeled syntactic trees are provided, our parser outperform the state-of-the-art. 3 The unlabeled parsing results are not reported in the original paper. The figures presented in are provided b"
P15-1149,D10-1001,0,0.256493,"dencies. First, all arguments associated with the same predicate are highly correlated due to the nature that they approximates type-logical semantics. Second, all predicates govern the same argument exhibit the hybrid syntactic/semantic, i.e. head-complementadjunct, relationships. Finally, the CCG dependency graphs are not but look very much like trees, which have many good computational properties. Simultaneously modeling the three properties yields intrinsically heterogeneous factorizations over the same graph, and hence results in intractability in decoding. Inspired by (Koo et al., 2010; Rush et al., 2010), we employ dual decomposition to perform principled decoding. Though not always, we can obtain the optimal solution most of time. The time complexity of our parser is O(n3 ) when various 1st- and 2nd-order features are incorporated. We conduct experiments on English CCGBank (Hockenmaier and Steedman, 2007). Though our parser does not use any grammar information, including both lexical categories and syntactic derivations, it produces very accurate CCG dependency graphs with respect to both token and complete matching. Our parser obtains an unlabeled f-score of 93.23, resulting in, perhaps sur"
P15-1149,P07-1079,0,0.0702333,"Missing"
P15-1149,C08-1095,0,0.420854,"the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDonald and Pereira (2006) presented a graph-based parser that can generate graphs in which a word may depend on multiple heads, and evaluated it on the Danish Treebank. Encouraged by their work, we study factorization models as well as principled decoding for CCG-grounded, graph-structured representations. Dual decomposition, and more generally Lagrangian relaxation, is a classical method for solving combinatorial optimization problems. It has been suc"
P15-1149,P09-1039,0,0.331719,"Missing"
P15-1149,D08-1017,0,0.0403206,"Auli and Lopez Xu et al. UP 93.03 93.71 93.63 93.08 93.15 UR 92.03 92.72 92.83 92.44 91.06 UF 92.53 93.21 93.23 92.76 92.09 UEM 32.61 38.14 37.47 37.56 Table 3: Comparing the state-of-art with our models on test set. based. The architecture of the syntactic tree parser does not affect the results much. The two tree parsers give identical attachment scores, and lead to similar graph parsing accuracy. This result is somehow non-obvious given that the combination of a graph-based and transition-based parser usually gives significantly better parsing performance (Nivre and McDonald, 2008; Torres Martins et al., 2008). Although the target representation of our parser is general graphs rather trees, implicitly or explicitly using tree-structured information plays an essential role. Syntactic features are able to improve the f-score achieved by the “PC+AC” model from 90.9 to 92.8, while the “TA” model can bring in an absolute gain of 1.6. Note that the “TA” model does not utilize any syntactic tree information. The converted trees are automatically induced from the CCG graphs. Even when syntactic trees are available, the automatically induced trees can still significantly improve the complete match with resp"
P15-1149,P14-1021,0,0.166683,"res have been extracted from these derivations using coindexation information. The typed dependency analysis provides a useful approximation to the underlying meaning representations, and has been shown very helpful for NLP applications e.g. Question Answering (Reddy et al., 2014). Traditionally, CCG graphs are generated as a by-product by deep parsers with a core grammar (Clark et al., 2002; Clark and Curran, 2007b; Fowler and Penn, 2010). On the other hand, modeling these dependencies within a CCG parser has been shown very effective to improve the parsing accuracy (Clark and Curran, 2007b; Xu et al., 2014). Besides CCG, similar deep dependency structures can be also extracted from parsers under other deep grammar formalisms, e.g. LFG (King et al., 2003) and HPSG (Miyao et al., 2004). In recent years, data-driven dependency parsing has been well studied and widely applied to many NLP tasks. Research on data-driven approach to producing dependency graphs that are not limited to tree or forest structures has also been initialized. Sagae and Tsujii (2008) introduced a transition-based parser that is able to handle projective directed dependency graphs for HPSGstyle predicate-argument analysis. McDo"
P15-1149,P11-1069,0,0.104639,"Missing"
P15-1149,P11-2033,0,0.036617,"and the final test (Section 23). We also use the syntactic dependency trees provided by the CCGBank to obtain necessary information for graph parsing. However, different from experiments in the CCG parsing literature, we use no grammar information. Neither lexical categories nor CCG derivations are utilized. All experiments were performed using automatically assigned POS-tags that are generated by a symbol-refined generative HMM tagger1 (Huang et al., 2010), and automatically parsed dependency trees that are generated by our in-house implementation of the transition-based model presented in (Zhang and Nivre, 2011) as well as a 2nd-order graph-based parser2 (Bohnet, 2010). The accuracy of these preprocessors is shown in Table 1. We ran 5-fold jack-knifing on the gold-standard training data to obtain imperfect dependency trees, splitting off 4 of 5 sentences for training and the other 1/5 for testing, 5 times. For each split, we re-trained the tree parsers on the training portion and applied the resulting model to the test portion. Previous research on dependency parsing shows that structured perceptron (Collins, 2002) is one of the strongest discriminative learning algorithms. To estimate θ’s of differe"
P15-2095,W04-3247,0,0.0613104,"e either the first or last letter of the word while truncating the rest of the word, using a double-letter contraction such as &quot;bb&quot; or &quot;cc&quot;, or removing most or all of the vowels in a word in order to shorten it. A complex system of styles, rules, and usage has been developed for this grade of braille. ing sentences in a document set to form a summary. Typical methods include the centroid-based method (Radev et al., 2004), NeATS (Lin and Hovy, 2002), supervised learning based methods (Ouyang et al., 2007; Shen et al., 2007; Schilder and Kondadadi, 2008; Wong et al., 2008), graphbased ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2005), Integer Linear Programming (Gillick et al., 2008; Gillick and Favre, 2009; Li et al., 2013), and submodular function (Lin and Bilmes, 2010). Moreover, cross-language document summarization has been investigated (Wan et al., 2010), but the task focuses on how to select the translated sentences with good content quality. We can see that all existing summarization systems were proposed for sighted people, but not for the blind and visually impaired people. Document summarization for the blind and visually impaired people has its specialty and is worth exploring. It ha"
P15-2095,W09-1802,0,0.247823,"letter contraction such as &quot;bb&quot; or &quot;cc&quot;, or removing most or all of the vowels in a word in order to shorten it. A complex system of styles, rules, and usage has been developed for this grade of braille. ing sentences in a document set to form a summary. Typical methods include the centroid-based method (Radev et al., 2004), NeATS (Lin and Hovy, 2002), supervised learning based methods (Ouyang et al., 2007; Shen et al., 2007; Schilder and Kondadadi, 2008; Wong et al., 2008), graphbased ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2005), Integer Linear Programming (Gillick et al., 2008; Gillick and Favre, 2009; Li et al., 2013), and submodular function (Lin and Bilmes, 2010). Moreover, cross-language document summarization has been investigated (Wan et al., 2010), but the task focuses on how to select the translated sentences with good content quality. We can see that all existing summarization systems were proposed for sighted people, but not for the blind and visually impaired people. Document summarization for the blind and visually impaired people has its specialty and is worth exploring. It has been a long way to help the blind and visually impaired people to browse information as conveniently"
P15-2095,P13-1099,0,0.0149953,"s &quot;bb&quot; or &quot;cc&quot;, or removing most or all of the vowels in a word in order to shorten it. A complex system of styles, rules, and usage has been developed for this grade of braille. ing sentences in a document set to form a summary. Typical methods include the centroid-based method (Radev et al., 2004), NeATS (Lin and Hovy, 2002), supervised learning based methods (Ouyang et al., 2007; Shen et al., 2007; Schilder and Kondadadi, 2008; Wong et al., 2008), graphbased ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2005), Integer Linear Programming (Gillick et al., 2008; Gillick and Favre, 2009; Li et al., 2013), and submodular function (Lin and Bilmes, 2010). Moreover, cross-language document summarization has been investigated (Wan et al., 2010), but the task focuses on how to select the translated sentences with good content quality. We can see that all existing summarization systems were proposed for sighted people, but not for the blind and visually impaired people. Document summarization for the blind and visually impaired people has its specialty and is worth exploring. It has been a long way to help the blind and visually impaired people to browse information as conveniently as ordinary peopl"
P15-2095,N10-1134,0,0.0360082,"the vowels in a word in order to shorten it. A complex system of styles, rules, and usage has been developed for this grade of braille. ing sentences in a document set to form a summary. Typical methods include the centroid-based method (Radev et al., 2004), NeATS (Lin and Hovy, 2002), supervised learning based methods (Ouyang et al., 2007; Shen et al., 2007; Schilder and Kondadadi, 2008; Wong et al., 2008), graphbased ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2005), Integer Linear Programming (Gillick et al., 2008; Gillick and Favre, 2009; Li et al., 2013), and submodular function (Lin and Bilmes, 2010). Moreover, cross-language document summarization has been investigated (Wan et al., 2010), but the task focuses on how to select the translated sentences with good content quality. We can see that all existing summarization systems were proposed for sighted people, but not for the blind and visually impaired people. Document summarization for the blind and visually impaired people has its specialty and is worth exploring. It has been a long way to help the blind and visually impaired people to browse information as conveniently as ordinary people. Special devices have been developed for achie"
P15-2095,N03-1020,0,0.296033,"Missing"
P15-2095,1983.tc-1.13,0,0.523442,"document summarization has been investigated (Wan et al., 2010), but the task focuses on how to select the translated sentences with good content quality. We can see that all existing summarization systems were proposed for sighted people, but not for the blind and visually impaired people. Document summarization for the blind and visually impaired people has its specialty and is worth exploring. It has been a long way to help the blind and visually impaired people to browse information as conveniently as ordinary people. Special devices have been developed for achieving this long-term goal (Linvill and Bliss, 1966; Shinohara et al., 1998). After the popularity of Braille, many kinds of braille display devices have been developed for braille reading (Rantala et al., 2009). In addition, most research in this area focused on how to improve accessibility of web information for the blind people (Salampasis et al., 2005; Mahmud et al., 2007; Hadjadj and Burger, 1999). 3 4 Preliminaries of Braille Grades Braille is a system of raised dots arranged in cells and it was developed by Louis Braille in the beginning of the 19th century. Braille letters, common punctuation marks, and a few symbols are displayed as r"
P15-2095,I05-2004,0,0.0328617,"ast letter of the word while truncating the rest of the word, using a double-letter contraction such as &quot;bb&quot; or &quot;cc&quot;, or removing most or all of the vowels in a word in order to shorten it. A complex system of styles, rules, and usage has been developed for this grade of braille. ing sentences in a document set to form a summary. Typical methods include the centroid-based method (Radev et al., 2004), NeATS (Lin and Hovy, 2002), supervised learning based methods (Ouyang et al., 2007; Shen et al., 2007; Schilder and Kondadadi, 2008; Wong et al., 2008), graphbased ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2005), Integer Linear Programming (Gillick et al., 2008; Gillick and Favre, 2009; Li et al., 2013), and submodular function (Lin and Bilmes, 2010). Moreover, cross-language document summarization has been investigated (Wan et al., 2010), but the task focuses on how to select the translated sentences with good content quality. We can see that all existing summarization systems were proposed for sighted people, but not for the blind and visually impaired people. Document summarization for the blind and visually impaired people has its specialty and is worth exploring. It has been a long way to help t"
P15-2095,P08-2052,0,0.0308083,"letter to represent the entire word, using a special symbol to precede either the first or last letter of the word while truncating the rest of the word, using a double-letter contraction such as &quot;bb&quot; or &quot;cc&quot;, or removing most or all of the vowels in a word in order to shorten it. A complex system of styles, rules, and usage has been developed for this grade of braille. ing sentences in a document set to form a summary. Typical methods include the centroid-based method (Radev et al., 2004), NeATS (Lin and Hovy, 2002), supervised learning based methods (Ouyang et al., 2007; Shen et al., 2007; Schilder and Kondadadi, 2008; Wong et al., 2008), graphbased ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2005), Integer Linear Programming (Gillick et al., 2008; Gillick and Favre, 2009; Li et al., 2013), and submodular function (Lin and Bilmes, 2010). Moreover, cross-language document summarization has been investigated (Wan et al., 2010), but the task focuses on how to select the translated sentences with good content quality. We can see that all existing summarization systems were proposed for sighted people, but not for the blind and visually impaired people. Document summarization for the blind and visually"
P15-2095,C10-1111,0,0.0530312,"Missing"
P15-2095,P10-1094,1,0.819245,"been developed for this grade of braille. ing sentences in a document set to form a summary. Typical methods include the centroid-based method (Radev et al., 2004), NeATS (Lin and Hovy, 2002), supervised learning based methods (Ouyang et al., 2007; Shen et al., 2007; Schilder and Kondadadi, 2008; Wong et al., 2008), graphbased ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2005), Integer Linear Programming (Gillick et al., 2008; Gillick and Favre, 2009; Li et al., 2013), and submodular function (Lin and Bilmes, 2010). Moreover, cross-language document summarization has been investigated (Wan et al., 2010), but the task focuses on how to select the translated sentences with good content quality. We can see that all existing summarization systems were proposed for sighted people, but not for the blind and visually impaired people. Document summarization for the blind and visually impaired people has its specialty and is worth exploring. It has been a long way to help the blind and visually impaired people to browse information as conveniently as ordinary people. Special devices have been developed for achieving this long-term goal (Linvill and Bliss, 1966; Shinohara et al., 1998). After the popu"
P15-2095,C08-1124,0,0.0269844,"e word, using a special symbol to precede either the first or last letter of the word while truncating the rest of the word, using a double-letter contraction such as &quot;bb&quot; or &quot;cc&quot;, or removing most or all of the vowels in a word in order to shorten it. A complex system of styles, rules, and usage has been developed for this grade of braille. ing sentences in a document set to form a summary. Typical methods include the centroid-based method (Radev et al., 2004), NeATS (Lin and Hovy, 2002), supervised learning based methods (Ouyang et al., 2007; Shen et al., 2007; Schilder and Kondadadi, 2008; Wong et al., 2008), graphbased ranking (Erkan and Radev, 2004; Mihalcea and Tarau, 2005), Integer Linear Programming (Gillick et al., 2008; Gillick and Favre, 2009; Li et al., 2013), and submodular function (Lin and Bilmes, 2010). Moreover, cross-language document summarization has been investigated (Wan et al., 2010), but the task focuses on how to select the translated sentences with good content quality. We can see that all existing summarization systems were proposed for sighted people, but not for the blind and visually impaired people. Document summarization for the blind and visually impaired people has"
P15-2095,P02-1058,0,\N,Missing
P16-1129,W11-2810,0,0.102338,"ay not be that indicative enough. Therefore, it is interesting to find alternative automatic metrics in order to better reflect the general quality for this task. 7 Related Work To the best of our knowledge, generation of sports news from live text commentary is not a wellstudied task in related fields. One related study focused on generating textual summaries for sports events from status updates in Twitter (Nichols et al., 2012). There also exists earlier work on generation of sports highlight frames from sports videos, focusing on a very different type of data (Tjondronegoro et al., 2004). Bouayad-Agha et al. (2011) and Bouayad-Agha et al. (2012) constructed an ontology-based knowledge base for the generation of football summaries, using predefined extraction templates. Our task is closely related to document summarization, which has been studied quite intensively. Various approaches exist to challenge the document summarization task, including centroidbased methods, link analysis and graph-based algorithms (Erkan and Radev, 2004; Wan et al., 2007), combinatorial optimization techniques such as integer linear programming (Gillick et al., 2008) and submodular optimization (Lin and Bilmes, 2010). Supervise"
P16-1129,E14-1075,0,0.010698,"related to document summarization, which has been studied quite intensively. Various approaches exist to challenge the document summarization task, including centroidbased methods, link analysis and graph-based algorithms (Erkan and Radev, 2004; Wan et al., 2007), combinatorial optimization techniques such as integer linear programming (Gillick et al., 2008) and submodular optimization (Lin and Bilmes, 2010). Supervised models including learning to rank models (Metzler and Kanungo, 2008; Shen and Li, 2011; Wang et al., 2013) and regression (Ouyang et al., 2007; Galanis and Malakasiotis, 2008; Hong and Nenkova, 2014) have also been adapted in the scenario of document summarization. Since sports live texts contain timeline information, summarization paradigms that utilize timeline and temporal information (Yan et al., 2011; Ng et al., 2014; Li et al., 2015) are also conceptually related. Supervised approaches related to this work have also been applied for timeline summarization, including linear regression for important scores (Tran et al., 2013a) and learning to rank models (Tran et al., 2013b). In this preliminary work we only use the timestamps in the definition of similarity for sentence selection. Mo"
P16-1129,N15-1145,0,0.0235644,"t al., 2007), combinatorial optimization techniques such as integer linear programming (Gillick et al., 2008) and submodular optimization (Lin and Bilmes, 2010). Supervised models including learning to rank models (Metzler and Kanungo, 2008; Shen and Li, 2011; Wang et al., 2013) and regression (Ouyang et al., 2007; Galanis and Malakasiotis, 2008; Hong and Nenkova, 2014) have also been adapted in the scenario of document summarization. Since sports live texts contain timeline information, summarization paradigms that utilize timeline and temporal information (Yan et al., 2011; Ng et al., 2014; Li et al., 2015) are also conceptually related. Supervised approaches related to this work have also been applied for timeline summarization, including linear regression for important scores (Tran et al., 2013a) and learning to rank models (Tran et al., 2013b). In this preliminary work we only use the timestamps in the definition of similarity for sentence selection. More crafted usages will be explored in the future. 8 Conclusion and Future Work In this paper we study a challenging task to automatically construct sports news from live text commentary. Using football live texts as an instance, we collect trai"
P16-1129,N10-1134,0,0.286292,"tant which has a succinct closed-form and easy to compute. We can define the entries of L as follows: Lij = qi φ&gt; i φj qj = qi · sim(i, j) · qj (1) where we can think of qi ∈ R+ as the quality of an item i and φi ∈ Rn with kφi k2 = 1 denotes a normalised feature vector such that sim(i, j) ∈ [−1, 1] measures similarity between item i and item j. This simple definition gives rise to a distribution that places most of its mass on sets that are both high quality and diverse. This is intuitive in a 7 Many other approaches can also be used to achieve similar effect, such as submodular maximization (Lin and Bilmes, 2010). We leave the comparison with these alternatives for future work study. 1364 geometric sense since determinants are closely related to volumes; in particular, det(LY ) is proportional to the volume spanned by the vectors qi φi for i ∈ Y . Thus, item sets with both high-quality and diverse items will have the highest probability (Figure 3). (a) (b) (c) Figure 3: (a) The DPP probability of a set Y depends on the volume spanned by vectors qi φi for i ∈ Y (b) As length increases, so does volume. (c) As similarity increases, volume decreases. 3.3.2 Sentence Selection In this work we formulate the"
P16-1129,N03-1020,0,0.0599452,"the number of highlight markers it includes. For fair comparisons the length of each constructed news report is limited to be no more than 1,000 Chinese characters, roughly the same with the average length of the gold-standard news. Note that we do not use the traditional MMR redundancy removal algorithm based on literal similarity (Carbonell and Goldstein, 1998) since we find only ignorable differences between using MMR or not for all systems. 4.4 4.4.1 Evaluation Methods and Metrics Automatic Evaluation Similar to the evaluation for traditional summarization tasks, we use the ROUGE metrics (Lin and Hovy, 2003) to automatically evaluate the quality of produced summaries given the goldstandard reference news. The ROUGE metrics measure summary quality by counting the precision, recall and F-score of overlapping units, such as n-grams and skip grams, between a candidate summary and the reference summaries. We use the ROUGE-1.5.5 toolkit to perform the 10 We also tried words rather than bigrams but found slightly worse performance. evaluation. In this paper we report the F-scores of the following metrics in the experimental results: ROUGE-1 (unigram-based), ROUGE-2 (bigrambased) and ROUGE-SU4 (based on"
P16-1129,N04-1019,0,0.115864,"skip distance of 4). 4.4.2 Pyramid Evaluation We also conduct manual pyramid evaluation in this study. Specifically, we use the modified pyramid scores as described in (Passonneau et al., 2005) to manually evaluate the summaries generated by different methods. We randomly sample 20 games from the data set and manually annotate facts on the gold-standard news. The annotated facts are mostly describing specific events happened during the game, e.g. “伊万被黄牌警告” (Ivanovic is shown the yellow card) and “内马尔 开出角球” (Neymar takes the corner). Each fact is treated as a Summarization Content Unit, (SCU) (Nenkova and Passonneau, 2004). The number of occurrences for each SCU in the gold-standard news is regarded as the weight of this SCU. 5 Results and Analysis 5.1 Comparison with Baseline Methods The average performance on all three folds of different methods are displayed in Table 1. Method R-1 R-2 R-SU4 HeadTail 0.30147 0.07779 0.10336 Centroid 0.32508 0.08113 0.11245 LexRank 0.31284 0.06159 0.09376 ILP 0.32552 0.07285 0.10378 Highlight 0.34687 0.08748 0.11924 RF 0.38559 0.11887 0.14907 RF+DPP 0.39391 0.11986 0.15097 Table 1: Comparison results of different methods As we can see from the results, our learning to rank app"
P16-1129,P14-1087,0,0.013122,"adev, 2004; Wan et al., 2007), combinatorial optimization techniques such as integer linear programming (Gillick et al., 2008) and submodular optimization (Lin and Bilmes, 2010). Supervised models including learning to rank models (Metzler and Kanungo, 2008; Shen and Li, 2011; Wang et al., 2013) and regression (Ouyang et al., 2007; Galanis and Malakasiotis, 2008; Hong and Nenkova, 2014) have also been adapted in the scenario of document summarization. Since sports live texts contain timeline information, summarization paradigms that utilize timeline and temporal information (Yan et al., 2011; Ng et al., 2014; Li et al., 2015) are also conceptually related. Supervised approaches related to this work have also been applied for timeline summarization, including linear regression for important scores (Tran et al., 2013a) and learning to rank models (Tran et al., 2013b). In this preliminary work we only use the timestamps in the definition of similarity for sentence selection. More crafted usages will be explored in the future. 8 Conclusion and Future Work In this paper we study a challenging task to automatically construct sports news from live text commentary. Using football live texts as an instanc"
P16-1129,W00-0403,0,0.466047,"ditional summarization approaches: HeadTail: Using head and tail sentences only. Commentators usually describe some basic information of the two sides at the beginning and summarize the scoring events in the end of commentary. This baseline resembles the baseline of leading sentences for traditional summarization. 9 http://sourceforge.net/p/lemur/wiki/RankLib/; In preliminary experiments, we contrasted RF with support vector regression predictor as well as other pairwise and listwise LTR models. We found that RF consistently outperformed others. 1365 Centroid: In centroid-based summarization (Radev et al., 2000), a pseudo-sentence of the document called centroid is calculated. The centroid consists of words with TFIDF scores above a predefined threshold. The score of each sentence is defined by summing the scores based on different features including cosine similarity of sentences with the centroid, position weight and cosine similarity with the first sentence. LexRank: LexRank (Erkan and Radev, 2004) computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used"
P16-1129,P13-1136,0,0.144945,"word sequences. For each sentence, we compute its TFIDF vector for calculating literal cosine similarity when used. 3 Constructing Sports News via Sentence Extraction We build a system to automatically construct match reports from live text commentary. Since we have described the new challenges for this task, we may design a number of relevant features to address them. In this work, we cast the problem into supervised sentence extraction. Supervised approaches, especially those based on learning to rank (LTR), can better utilize the power of various task-dependent features (Shen and Li, 2011; Wang et al., 2013). For a given specific sports game, we extract features from all candidate sentences in the corresponding live texts and score the sentences using a learning to rank (LTR) model learned from the training data (Section 3.1). Then we select a few of them according to the ranking scores to form the constructed news (Section 3.3). 3.2.1 Basic Features Position: The position of each candidate sentence. Suppose there are n sentences in a document. For the i-th sentence, its position feature is computed as 1 − i−1 n . Length: The number of words contained in the sentence after stopwords removal. Numb"
P16-1129,D11-1040,1,0.300449,"ithms (Erkan and Radev, 2004; Wan et al., 2007), combinatorial optimization techniques such as integer linear programming (Gillick et al., 2008) and submodular optimization (Lin and Bilmes, 2010). Supervised models including learning to rank models (Metzler and Kanungo, 2008; Shen and Li, 2011; Wang et al., 2013) and regression (Ouyang et al., 2007; Galanis and Malakasiotis, 2008; Hong and Nenkova, 2014) have also been adapted in the scenario of document summarization. Since sports live texts contain timeline information, summarization paradigms that utilize timeline and temporal information (Yan et al., 2011; Ng et al., 2014; Li et al., 2015) are also conceptually related. Supervised approaches related to this work have also been applied for timeline summarization, including linear regression for important scores (Tran et al., 2013a) and learning to rank models (Tran et al., 2013b). In this preliminary work we only use the timestamps in the definition of similarity for sentence selection. More crafted usages will be explored in the future. 8 Conclusion and Future Work In this paper we study a challenging task to automatically construct sports news from live text commentary. Using football live te"
P16-1133,C12-2008,0,0.0246841,"vestigates several questions based on the parallel corpus including both the monolingual sentiment classification and cross-lingual sentiment classification. Wan (2009) translates both the training data (English to Chinese) and the test data (Chinese to English) to train different models in both the source and target languages. The co-training algorithm (Blum and Mitchell, 1998) is used to combine the bilingual models together and improve the performance. In addition to the translation-based methods, several studies utilize parallel corpus or existing resources to bridge the language barrier. Balamurali (2012) use WordNet senses as features for supervised sentiment classification. They use the linked WordNets of two languages to bridge the language gap. Lu et al. (2011) consider the multilingual scenario where small amount of labeled data is available in the target language. They attempted to jointly classify the sentiment for both source language and target language. Meng et al. (2012) propose a generative cross-lingual mixture model to leverage unlabeled bilingual parallel data. Prettenhofer and Stein (2010) use the structural correspondence learning algorithm to learn a map between the source la"
P16-1133,C10-1004,0,0.0289594,"to solve the sentiment classification task from a cross-language view. It is of great importance for the area since it can exploit the existing labeled information in a source language to build a sentiment classification system in any other target language. It saves us from manually labeling data for all the languages in the world which is expensive and time-consuming. Cross-lingual sentiment classification has been extensively studied in the very recent years. Mihalcea et al. (2007) translate English subjectivity words and phrases into the target language to build a lexicon-based classifier. Banea et al. (2010) also use the machine translation service to obtain parallel corpus. It investigates several questions based on the parallel corpus including both the monolingual sentiment classification and cross-lingual sentiment classification. Wan (2009) translates both the training data (English to Chinese) and the test data (Chinese to English) to train different models in both the source and target languages. The co-training algorithm (Blum and Mitchell, 1998) is used to combine the bilingual models together and improve the performance. In addition to the translation-based methods, several studies util"
P16-1133,C04-1121,0,0.0157673,"lti-domain Amazon review dataset. We use English as the source language and use Japanese, German and French as the target languages. The experimental results show that BiDRL outperforms the state-of-the-art methods for all the target languages. 1 Introduction Sentiment analysis for online user-generated contents has become a hot research topic during the last decades. Among all the sentiment analysis tasks, polarity classification is the most widely studied topic. It has been proved to be invaluable in many applications, such as opinion polling (Tang et al., 2012), customer feedback tracking (Gamon, 2004), election prediction (Tumasjan et al., 2010), stock market prediction (Bollen et al., 2011) and so on. Most of the current sentiment classification systems are built on supervised machine learning algorithms which require manually labelled data. However, sentiment resources are usually unbalanced in different languages. Cross-lingual sentiment classification aims to leverage the resources in a resource-rich language (such as English) to classify the sentiment polarity of texts in a resource-poor language (such as Japanese). The biggest challenge for cross-lingual sentiment classification is t"
P16-1133,P11-1033,0,0.11719,"e usually unbalanced in different languages. Cross-lingual sentiment classification aims to leverage the resources in a resource-rich language (such as English) to classify the sentiment polarity of texts in a resource-poor language (such as Japanese). The biggest challenge for cross-lingual sentiment classification is the vocabulary gap between the source language and the target language. This problem is addressed with different strategies in different approaches. Wan (2009) use machine translation tools to translate the training data directly into the target language. Meng et al. (2012) and Lu et al. (2011) exploit parallel unlabeled data to bridge the language barrier. Prettenhofer and Stein (2010) use correspondence learning algorithm to learn a map between the source language and the target language. Recently, representation learning methods has been proposed to solve the cross-lingual classification problem (Xiao and Guo, 2013; Zhou et al., 2015). These methods aim to learn common feature representations for different languages. However, most of the current researches only focus on bilingual word embedding. In addition, these models only use the semantic correlations between aligned words or"
P16-1133,W15-1521,0,0.0769384,"oblem where each task corresponds to a single word, and task relatedness is derived from co-occurrence statistics in bilingual parallel data. Hermann and Blunsom (2015) propose the bilingual CVM model which directly minimizes the representation of a pair of parallel documents. The document representation is calculated with a composition function based on words. Chandar A P et al. (2014) and Zhou et al. (2015) use the autoencoder to model the connections between bilingual sentences. It aims to minimize the reconstruction error between the bag-of-words representations of two parallel sentences. Luong et al. (2015) propose the bilingual skip-gram model which leverages the word alignment between parallel sentences. Pham et al. (2015) extend the paragraph vector model to force bilingual sentences to share the same sentence vector. This study differs with the existing works in the following three aspects, 1) we exploit both the semantic and sentiment correlations of the bilingual texts. Existing bilingual embedding algorithms only use the semantic connection between parallel sentences or documents. 2) Our algorithm learns both the word and document representations. Most of the previous studies simply compu"
P16-1133,P12-1060,0,0.0980119,"sentiment resources are usually unbalanced in different languages. Cross-lingual sentiment classification aims to leverage the resources in a resource-rich language (such as English) to classify the sentiment polarity of texts in a resource-poor language (such as Japanese). The biggest challenge for cross-lingual sentiment classification is the vocabulary gap between the source language and the target language. This problem is addressed with different strategies in different approaches. Wan (2009) use machine translation tools to translate the training data directly into the target language. Meng et al. (2012) and Lu et al. (2011) exploit parallel unlabeled data to bridge the language barrier. Prettenhofer and Stein (2010) use correspondence learning algorithm to learn a map between the source language and the target language. Recently, representation learning methods has been proposed to solve the cross-lingual classification problem (Xiao and Guo, 2013; Zhou et al., 2015). These methods aim to learn common feature representations for different languages. However, most of the current researches only focus on bilingual word embedding. In addition, these models only use the semantic correlations bet"
P16-1133,P07-1123,0,0.029492,"ed task (Pang et al., 2002). Cross-lingual sentiment classification is a popular topic in the sentiment analysis community which aims to solve the sentiment classification task from a cross-language view. It is of great importance for the area since it can exploit the existing labeled information in a source language to build a sentiment classification system in any other target language. It saves us from manually labeling data for all the languages in the world which is expensive and time-consuming. Cross-lingual sentiment classification has been extensively studied in the very recent years. Mihalcea et al. (2007) translate English subjectivity words and phrases into the target language to build a lexicon-based classifier. Banea et al. (2010) also use the machine translation service to obtain parallel corpus. It investigates several questions based on the parallel corpus including both the monolingual sentiment classification and cross-lingual sentiment classification. Wan (2009) translates both the training data (English to Chinese) and the test data (Chinese to English) to train different models in both the source and target languages. The co-training algorithm (Blum and Mitchell, 1998) is used to co"
P16-1133,W02-1011,0,0.0408039,"s but also the documents with the same sentiment are required to get similar representations. 3) Our model achieves the state-of-the-art performances on nine benchmark cross-lingual sentiment classification tasks and it consistently outperforms the existing methods by a large margin. 2 Related Work Sentiment analysis is the field of studying and analyzing people’s opinions, sentiments, evaluations, appraisals, attitudes, and emotions (Liu, 2012). Most of the previous sentiment analysis researches focus on customer reviews and classifying the sentiment polarity is the most widely studied task (Pang et al., 2002). Cross-lingual sentiment classification is a popular topic in the sentiment analysis community which aims to solve the sentiment classification task from a cross-language view. It is of great importance for the area since it can exploit the existing labeled information in a source language to build a sentiment classification system in any other target language. It saves us from manually labeling data for all the languages in the world which is expensive and time-consuming. Cross-lingual sentiment classification has been extensively studied in the very recent years. Mihalcea et al. (2007) tran"
P16-1133,W15-1512,0,0.295367,"Missing"
P16-1133,P10-1114,0,0.183963,"on aims to leverage the resources in a resource-rich language (such as English) to classify the sentiment polarity of texts in a resource-poor language (such as Japanese). The biggest challenge for cross-lingual sentiment classification is the vocabulary gap between the source language and the target language. This problem is addressed with different strategies in different approaches. Wan (2009) use machine translation tools to translate the training data directly into the target language. Meng et al. (2012) and Lu et al. (2011) exploit parallel unlabeled data to bridge the language barrier. Prettenhofer and Stein (2010) use correspondence learning algorithm to learn a map between the source language and the target language. Recently, representation learning methods has been proposed to solve the cross-lingual classification problem (Xiao and Guo, 2013; Zhou et al., 2015). These methods aim to learn common feature representations for different languages. However, most of the current researches only focus on bilingual word embedding. In addition, these models only use the semantic correlations between aligned words or sentences in different languages while the sentiment correlations are ignored. In this study,"
P16-1133,C12-1089,0,0.0266319,"to bilingual representation learning. Zou et al. (2013) propose to use word alignment as the constraints in bilingual word embedding. Each word in one language should be similar to the aligned words in another language. Gouws et al. (2015) propose a similar algorithm but only use sentence-level alignment. It tries to minimize a sampled L2-loss between the bag-of-words sentence vectors of the parallel cor1404 pus. Xiao and Guo (2013) learn different representations for words in different languages. Part of the word vector is shared among different languages and the rest is language-dependent. Klementiev et al. (2012) treat the task as a multi-task learning problem where each task corresponds to a single word, and task relatedness is derived from co-occurrence statistics in bilingual parallel data. Hermann and Blunsom (2015) propose the bilingual CVM model which directly minimizes the representation of a pair of parallel documents. The document representation is calculated with a composition function based on words. Chandar A P et al. (2014) and Zhou et al. (2015) use the autoencoder to model the connections between bilingual sentences. It aims to minimize the reconstruction error between the bag-of-words"
P16-1133,P09-1027,1,0.875262,"systems are built on supervised machine learning algorithms which require manually labelled data. However, sentiment resources are usually unbalanced in different languages. Cross-lingual sentiment classification aims to leverage the resources in a resource-rich language (such as English) to classify the sentiment polarity of texts in a resource-poor language (such as Japanese). The biggest challenge for cross-lingual sentiment classification is the vocabulary gap between the source language and the target language. This problem is addressed with different strategies in different approaches. Wan (2009) use machine translation tools to translate the training data directly into the target language. Meng et al. (2012) and Lu et al. (2011) exploit parallel unlabeled data to bridge the language barrier. Prettenhofer and Stein (2010) use correspondence learning algorithm to learn a map between the source language and the target language. Recently, representation learning methods has been proposed to solve the cross-lingual classification problem (Xiao and Guo, 2013; Zhou et al., 2015). These methods aim to learn common feature representations for different languages. However, most of the current"
P16-1133,D13-1153,0,0.579112,"abulary gap between the source language and the target language. This problem is addressed with different strategies in different approaches. Wan (2009) use machine translation tools to translate the training data directly into the target language. Meng et al. (2012) and Lu et al. (2011) exploit parallel unlabeled data to bridge the language barrier. Prettenhofer and Stein (2010) use correspondence learning algorithm to learn a map between the source language and the target language. Recently, representation learning methods has been proposed to solve the cross-lingual classification problem (Xiao and Guo, 2013; Zhou et al., 2015). These methods aim to learn common feature representations for different languages. However, most of the current researches only focus on bilingual word embedding. In addition, these models only use the semantic correlations between aligned words or sentences in different languages while the sentiment correlations are ignored. In this study, we propose a cross-lingual representation learning model BiDRL which simultaneously learns both the word and document representations in both languages. We propose a joint learning algorithm which exploits both monolingual and bilingua"
P16-1133,P15-1042,0,0.464557,"the source language and the target language. This problem is addressed with different strategies in different approaches. Wan (2009) use machine translation tools to translate the training data directly into the target language. Meng et al. (2012) and Lu et al. (2011) exploit parallel unlabeled data to bridge the language barrier. Prettenhofer and Stein (2010) use correspondence learning algorithm to learn a map between the source language and the target language. Recently, representation learning methods has been proposed to solve the cross-lingual classification problem (Xiao and Guo, 2013; Zhou et al., 2015). These methods aim to learn common feature representations for different languages. However, most of the current researches only focus on bilingual word embedding. In addition, these models only use the semantic correlations between aligned words or sentences in different languages while the sentiment correlations are ignored. In this study, we propose a cross-lingual representation learning model BiDRL which simultaneously learns both the word and document representations in both languages. We propose a joint learning algorithm which exploits both monolingual and bilingual constraints. The m"
P16-1133,D13-1141,0,0.0669894,"scenario where small amount of labeled data is available in the target language. They attempted to jointly classify the sentiment for both source language and target language. Meng et al. (2012) propose a generative cross-lingual mixture model to leverage unlabeled bilingual parallel data. Prettenhofer and Stein (2010) use the structural correspondence learning algorithm to learn a map between the source language and the target language. Xiao and Guo (2014) treat the bilingual feature learning problem as a matrix completion task. This work is also related to bilingual representation learning. Zou et al. (2013) propose to use word alignment as the constraints in bilingual word embedding. Each word in one language should be similar to the aligned words in another language. Gouws et al. (2015) propose a similar algorithm but only use sentence-level alignment. It tries to minimize a sampled L2-loss between the bag-of-words sentence vectors of the parallel cor1404 pus. Xiao and Guo (2013) learn different representations for words in different languages. Part of the word vector is shared among different languages and the rest is language-dependent. Klementiev et al. (2012) treat the task as a multi-task"
P16-1217,N13-1016,0,0.35474,"river state friday central water rain high california weather}. It is not easy for a user to fully understand this topic if the user is not very familiar with the document collection. The situation may become worse when the user faces with a number of discovered topics and the sets of top terms of the topics are often overlapping with each other on many practical document collections. In order to address the above challenge, a few previous studies have proposed to use phrases, concepts and even images for labeling the discovered topics (Mei et al., 2007; Lau et al., 2011; Hulpus et al., 2013; Aletras and Stevenson, 2013). For example, we may automatically extract the phrase “southern california” to represent the example topic mentioned earlier. These topic labels can help the user to understand the topics to some extent. However, the use of phrases or concepts as topic labels are not very satisfactory in practice, because the phrases or concepts are still very short, and the information expressed in these short labels is not adequate for user’s understanding. The case will become worse when some ambiguous phrase is used or multiple discrete phrases with poor coherence are used for a topic. To address the draw"
P16-1217,W04-3247,0,0.366738,"ed in the natural language processing and information retrieval fields, and most previous works focus on directly extracting sentences from a news document or collection to form the summary. The summary can be used for helping users quickly browse and understand a document or document collection. Typical multi-document summarization methods include the centroid-based method (Radev et al., 2004), integer linear programming (ILP) (Gillick et al., 2008), sentence-based LDA (Chang and Chien, 2009), submodular function maximization (Lin and Bilmes, 2010; Lin and Bilmes, 2011), graph based methods (Erkan and Radev, 2004; Wan et al., 2007; Wan and Yang, 2008), and supervised learning based methods (Ouyang et al., 2007; Shen et al., 2007). Though different summarization methods have been proposed in recent years, the submodular function maximization method is still one of the state-of-the-art summarization methods. Moreover, the method is easy to follow and its framework is very flexible. One can design specific submodular functions for addressing special summarization tasks, without altering the overall greedy selection framework. 2298 Though various summarization methods have been proposed, none of existing"
P16-1217,P11-1154,0,0.149973,"ople coast homes south damage northern river state friday central water rain high california weather}. It is not easy for a user to fully understand this topic if the user is not very familiar with the document collection. The situation may become worse when the user faces with a number of discovered topics and the sets of top terms of the topics are often overlapping with each other on many practical document collections. In order to address the above challenge, a few previous studies have proposed to use phrases, concepts and even images for labeling the discovered topics (Mei et al., 2007; Lau et al., 2011; Hulpus et al., 2013; Aletras and Stevenson, 2013). For example, we may automatically extract the phrase “southern california” to represent the example topic mentioned earlier. These topic labels can help the user to understand the topics to some extent. However, the use of phrases or concepts as topic labels are not very satisfactory in practice, because the phrases or concepts are still very short, and the information expressed in these short labels is not adequate for user’s understanding. The case will become worse when some ambiguous phrase is used or multiple discrete phrases with poor"
P16-1217,N10-1134,0,0.126988,"document or document set. The task has been extensively investigated in the natural language processing and information retrieval fields, and most previous works focus on directly extracting sentences from a news document or collection to form the summary. The summary can be used for helping users quickly browse and understand a document or document collection. Typical multi-document summarization methods include the centroid-based method (Radev et al., 2004), integer linear programming (ILP) (Gillick et al., 2008), sentence-based LDA (Chang and Chien, 2009), submodular function maximization (Lin and Bilmes, 2010; Lin and Bilmes, 2011), graph based methods (Erkan and Radev, 2004; Wan et al., 2007; Wan and Yang, 2008), and supervised learning based methods (Ouyang et al., 2007; Shen et al., 2007). Though different summarization methods have been proposed in recent years, the submodular function maximization method is still one of the state-of-the-art summarization methods. Moreover, the method is easy to follow and its framework is very flexible. One can design specific submodular functions for addressing special summarization tasks, without altering the overall greedy selection framework. 2298 Though"
P16-1217,P11-1052,0,0.0300636,"et. The task has been extensively investigated in the natural language processing and information retrieval fields, and most previous works focus on directly extracting sentences from a news document or collection to form the summary. The summary can be used for helping users quickly browse and understand a document or document collection. Typical multi-document summarization methods include the centroid-based method (Radev et al., 2004), integer linear programming (ILP) (Gillick et al., 2008), sentence-based LDA (Chang and Chien, 2009), submodular function maximization (Lin and Bilmes, 2010; Lin and Bilmes, 2011), graph based methods (Erkan and Radev, 2004; Wan et al., 2007; Wan and Yang, 2008), and supervised learning based methods (Ouyang et al., 2007; Shen et al., 2007). Though different summarization methods have been proposed in recent years, the submodular function maximization method is still one of the state-of-the-art summarization methods. Moreover, the method is easy to follow and its framework is very flexible. One can design specific submodular functions for addressing special summarization tasks, without altering the overall greedy selection framework. 2298 Though various summarization m"
P16-1217,W03-0502,0,0.142466,"Missing"
P17-1077,C10-1011,0,0.0580287,"B 833 B IND T WO PAGES(gA , gB ) 1 u(0) ← 0 2 for k ← 0..T do 3 gA ← arg maxg fA (g) − u(k)⊤ Ag 4 gB ← arg maxg fB (g) − u(k)⊤ Bg 5 if AgA + BgB ≤ 0 then 6 return gA , gB 7 else 8 u(k+1) ← u(k) + α(k) (AgA + BgB ) 9 return gA , gB 2010). For the other three data sets we use POStags provided by the shared task. We also use features extracted from trees. We consider two types of trees: (1) syntactic trees provided as a companion analysis by the shared task and CCGBank, (2) pseudo trees (Zhang et al., 2016) automatically extracted from semantic dependency annotations. We utilize the Mate parser (Bohnet, 2010) to generate pseudo trees for all data sets and also syntactic trees for CCG analysis, and use the companion syntactic analysis provided by the shared task for the other three data sets. Figure 7: The page binding algorithm. 5.2 Statistical Disambiguation We instead try to find the solution for maxu L(u). By using a subgradient method to calculate maxu L(u), we have an algorithm for joint decoding (see Figure 7). L(u) is divided into two optimization problems which can be decoded easily. Each sub-problem is still a parsing problem for noncrossing graphs. Only the scores of factors are modified"
P17-1077,P17-1193,1,0.833873,"onditions. Our parser obtains comparable results with a state-of-the-art transition-based parser. 1 Introduction Dependency analysis provides a lightweight and effective way to encode syntactic and semantic information of natural language sentences. One of its branches, syntactic dependency parsing (K¨ubler et al., 2009) has been an extremely active research area, with high-performance parsers being built and applied for practical use of NLP. Semantic dependency parsing, however, has only been addressed in the literature recently (Oepen et al., 2014, 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Semantic dependency parsing employs a graphstructured semantic representation. On the one hand, it is flexible enough to provide analysis for various semantic phenomena (Ivanova et al., 2012). This very flexibility, on the other hand, brings along new challenges for designing parsing algorithms. For graph-based parsing, no previously defined Maximum Subgraph algorithm has simultaneously a high coverage and a polynomial 828 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 828–838 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for"
P17-1077,W02-1001,0,0.0661282,"is “yes,” we stop and return the merged graph. Otherwise, we update u in a way to increase L(u) (see Line 8). 5 Our parsing algorithms can be applied to scores originated from any source, but in our experiments we chose to use the framework of global linear models, deriving our scores as: S CORE PART(s, p) = w⊤ ϕ(s, p) ϕ is a feature-vector mapping and w is a parameter vector. p may refer to a single arc, a pair of neighboring arcs, or a general tuple of arcs, according to the definition of a parsing model. For details we refer to the source code. We chose the averaged structured perceptron (Collins, 2002) for parameter estimation. 5.3 Results of Practical Parsing We evaluate five decoding algorithms: Experiments M1 first-order exact algorithm, 5.1 Data Sets M2 second-order exact algorithm with singleside factorization, To evaluate the effectiveness of book embedding in practice, we conduct experiments on unlabeled parsing using four corpora: CCGBank (Hockenmaier and Steedman, 2007), DeepBank (Flickinger et al., 2012), Enju HPSGBank (EnjuBank; Miyao et al., 2004) and Prague Dependency TreeBank (PCEDT; Hajic et al., 2012), We use “standard” training, validation, and test splits to facilitate com"
P17-1077,P15-1149,1,0.796967,"Missing"
P17-1077,S14-2080,1,0.893308,"92.83 92.83 92.87 91.61 93.02 UP 93.45 93.66 92.49 -- CCGBank UR UF 92.51 92.98 92.06 92.85 92.30 92.40 --- UP 89.58 89.27 -91.79 PCEDT UR UF 87.73 88.65 87.37 88.31 --86.02 88.81 Table 3: Parsing accuracy evaluated on the test sets. 5.4 Comparison with Other Parsers Acknowledgments We show the parsing results on the test data together with some relevant results from related work. We compare our parser with two other systems: (1) ZDSW (Zhang et al., 2016) is a transition-based system that obtains state-of-theart accuracy; we present the results of their best single parsing model; (2) Peking (Du et al., 2014) is the best-performing system in the shared task; it is a hybrid system that integrate more than ten submodels to achieve high accuracy. Our parser can be taken as a graph-based parser. It reaches stateof-the-art performance produced by the transitionbased system. On DeepBank and EnjuBank, the accuracy of our parser is equivalent to ZDSW, while on CCGBank, our parser is significantly better. There is still a gap between our single parsing model and Peking hybrid model. For a majority of NLP tasks, e.g. parsing (Surdeanu and Manning, 2010), semantic role labeling (Koomen et al., 2005), hybrid"
P17-1077,E06-1011,0,0.613142,"Missing"
P17-1077,J11-1007,0,0.0121486,"different from the intuition of the design of the open structure when we consider first-order factorization. For the last combination, we need to search for two best separating words, namely sr and le , and two best labels, namely l′ and l′ , so the time complexity of this second-order algorithm is O(n4 |L|2 ). 3.3 Generalized Higher-Order Parsing Both of the above two algorithms are exact decoding algorithms. Solutions allow for exact decoding with higher-order features typically at a high cost in terms of efficiency. A trade-off between rich features and exact decoding benefit tree parsing (McDonald and Nivre, 2011). In particular, Zhang and McDonald (2012) proposed a generalized higher-order model that abandons exact search in graph-based parsing in favor of freedom in feature scope. They kept intact Eisner’s algorithm for first-order parsing problems, while enhanced the scoring function in an approximate way by introducing higher-order features. We borrow Zhang and McDonald’s idea and develop a generalized parsing model for noncrossing dependency representations. The sub-problems and their decomposition are much like the firstorder algorithm. The difference is that we expand 4 Finding and Binding Pages"
P17-1077,P10-1151,0,0.49585,"Missing"
P17-1077,hajic-etal-2012-announcing,0,0.118887,"Missing"
P17-1077,S15-2153,0,0.138924,"Missing"
P17-1077,S14-2008,0,0.52016,"new exact second- and approximate higher-order algorithms. Our algorithms facilitate building with high accuracy the partial semantic dependency graphs on each page. To produce a full semantic analysis, we also need to integrate partial graphs on all pages into one coherent book. To this end, we formulate the problem as a combinatorial optimization problem, and propose a Lagrangian Relaxation-based algorithm for solutions. We implement a practical parser in the new framework with a statistical disambiguation model. We evaluate this parser on four data sets: those used in SemEval 2014 Task 8 (Oepen et al., 2014), and the dependency graphs extracted from We model a dependency graph as a book, a particular kind of topological space, for semantic dependency parsing. The spine of the book is made up of a sequence of words, and each page contains a subset of noncrossing arcs. To build a semantic graph for a given sentence, we design new Maximum Subgraph algorithms to generate noncrossing graphs on each page, and a Lagrangian Relaxation-based algorithm to combine pages into a book. Experiments demonstrate the effectiveness of the book embedding framework across a wide range of conditions. Our parser obtain"
P17-1077,J07-3004,0,0.304208,"i.org/10.18653/v1/P17-1077 Kuhlmann and Jonsson (2015) proposed to generalize the MST model to other types of subgraphs. In general, dependency parsing is formulated as the search for Maximum Subgraph for graph class G: Given a graph G = (V, A), find a subset A′ ⊆ A with maximum total weight such that the induced subgraph G′ = (V, A′ ) belongs to G. Formally, we have the following optimization problem: ∑ G′ (s) = arg max S CORE PART(s, p) arg2 arg1 arg1 arg1 arg2 arg1 arg2 arg1 arg2 . . The . company . that . Mark wants . to. buy . Figure 1: A fragment of a semantic dependency graph. CCGbank (Hockenmaier and Steedman, 2007). On all data sets, we find that our higher-order parsing models are more accurate than the first-order baseline. Experiments also demonstrate the effectiveness of our page binding algorithm. Our new parser can be taken as a graph-based parser extended for more general dependency graphs. It parallels the state-of-the-art transition-based system of Zhang et al. (2016) in performance. The implementation of our parser is available at http://www.icst.pku.edu.cn/ lcwm/grass. 2 H∈G(s,G) p∈H Here, G(s, G) is the set of all graphs that belong to G and are compatible with s and G. For parsing, G is usu"
P17-1077,D10-1002,0,0.341865,"Missing"
P17-1077,W12-3602,0,0.0240379,"ctic and semantic information of natural language sentences. One of its branches, syntactic dependency parsing (K¨ubler et al., 2009) has been an extremely active research area, with high-performance parsers being built and applied for practical use of NLP. Semantic dependency parsing, however, has only been addressed in the literature recently (Oepen et al., 2014, 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Semantic dependency parsing employs a graphstructured semantic representation. On the one hand, it is flexible enough to provide analysis for various semantic phenomena (Ivanova et al., 2012). This very flexibility, on the other hand, brings along new challenges for designing parsing algorithms. For graph-based parsing, no previously defined Maximum Subgraph algorithm has simultaneously a high coverage and a polynomial 828 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 828–838 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1077 Kuhlmann and Jonsson (2015) proposed to generalize the MST model to other types of subgraphs. In general, dependency parsing is fo"
P17-1077,W05-0625,0,0.0449306,"Missing"
P17-1077,D12-1030,0,0.053895,"of the open structure when we consider first-order factorization. For the last combination, we need to search for two best separating words, namely sr and le , and two best labels, namely l′ and l′ , so the time complexity of this second-order algorithm is O(n4 |L|2 ). 3.3 Generalized Higher-Order Parsing Both of the above two algorithms are exact decoding algorithms. Solutions allow for exact decoding with higher-order features typically at a high cost in terms of efficiency. A trade-off between rich features and exact decoding benefit tree parsing (McDonald and Nivre, 2011). In particular, Zhang and McDonald (2012) proposed a generalized higher-order model that abandons exact search in graph-based parsing in favor of freedom in feature scope. They kept intact Eisner’s algorithm for first-order parsing problems, while enhanced the scoring function in an approximate way by introducing higher-order features. We borrow Zhang and McDonald’s idea and develop a generalized parsing model for noncrossing dependency representations. The sub-problems and their decomposition are much like the firstorder algorithm. The difference is that we expand 4 Finding and Binding Pages Statistics presented in Table 1 indicate"
P17-1077,J16-3001,1,0.676361,"ss a wide range of conditions. Our parser obtains comparable results with a state-of-the-art transition-based parser. 1 Introduction Dependency analysis provides a lightweight and effective way to encode syntactic and semantic information of natural language sentences. One of its branches, syntactic dependency parsing (K¨ubler et al., 2009) has been an extremely active research area, with high-performance parsers being built and applied for practical use of NLP. Semantic dependency parsing, however, has only been addressed in the literature recently (Oepen et al., 2014, 2015; Du et al., 2015; Zhang et al., 2016; Cao et al., 2017). Semantic dependency parsing employs a graphstructured semantic representation. On the one hand, it is flexible enough to provide analysis for various semantic phenomena (Ivanova et al., 2012). This very flexibility, on the other hand, brings along new challenges for designing parsing algorithms. For graph-based parsing, no previously defined Maximum Subgraph algorithm has simultaneously a high coverage and a polynomial 828 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 828–838 c Vancouver, Canada, July 30 - August 4, 2017. 20"
P17-1077,N10-1091,0,\N,Missing
P17-1108,P15-2136,0,0.0341234,"ed to extractive methods and abstractive methods. Extractive methods extract sentences from the original document to form the summary. Notable early works include (Edmundson, 1969; Carbonell and Goldstein, 1998; McDonald, 2007). In recent years much progress has also been made under traditional extractive frameworks (Li et al., 2013; Dasgupta et al., 2013; Nishikawa et al., 2014). Neural networks have also been widely investigated on the extractive summarization task. Earlier works explore to use deep learning techniques in the traditional framework (Kobayashi et al., 2015; Yin and Pei, 2015; Cao et al., 2015a,b). More recent works predict the extraction of sentences in a more data-driven way. Cheng and Lapata (2016) propose an encoder-decoder approach where the encoder learns the representation of sentences and documents while the decoder classifies each sentence using an attention mechanism. Nallapati et al. (2017) propose a recurrent neural network (RNN)-based sequence model for extractive summarization of documents. Neural sentence extractive models are able to leverage large-scale training data and achieve performance better than traditional extractive summarization methods. 2.2 Abstractive S"
P17-1108,P82-1020,0,0.86775,"Missing"
P17-1108,P16-1046,0,0.737394,"l document to form the summary. Notable early works include (Edmundson, 1969; Carbonell and Goldstein, 1998; McDonald, 2007). In recent years much progress has also been made under traditional extractive frameworks (Li et al., 2013; Dasgupta et al., 2013; Nishikawa et al., 2014). Neural networks have also been widely investigated on the extractive summarization task. Earlier works explore to use deep learning techniques in the traditional framework (Kobayashi et al., 2015; Yin and Pei, 2015; Cao et al., 2015a,b). More recent works predict the extraction of sentences in a more data-driven way. Cheng and Lapata (2016) propose an encoder-decoder approach where the encoder learns the representation of sentences and documents while the decoder classifies each sentence using an attention mechanism. Nallapati et al. (2017) propose a recurrent neural network (RNN)-based sequence model for extractive summarization of documents. Neural sentence extractive models are able to leverage large-scale training data and achieve performance better than traditional extractive summarization methods. 2.2 Abstractive Summarization Methods Abstractive summarization aims at generating the summary based on understanding the input"
P17-1108,D14-1085,0,0.0131586,"ork (RNN)-based sequence model for extractive summarization of documents. Neural sentence extractive models are able to leverage large-scale training data and achieve performance better than traditional extractive summarization methods. 2.2 Abstractive Summarization Methods Abstractive summarization aims at generating the summary based on understanding the input text. It involves multiple subproblems like simplification, paraphrasing, and fusion. Previous research is mostly restricted in one or a few of the subproblems or specific domains (Woodsend and Lapata, 2012; Thadani and McKeown, 2013; Cheung and Penn, 2014; Pighin et al., 2014; Sun et al., 2015). As for neural network models, success is achieved on sentence abstractive summarization. Rush et al. (2015) train a neural attention model on a large corpus of news documents and their headlines, and later Chopra et al. (2016) extend their work with an attentive recurrent neural network framework. Nallapati et al. (2016) introduce various effective techniques in the RNN seq2seq framework. These neural sentence abstraction models are able to achieve state-of-the-art results on the DUC competition of generating headlinelevel summaries for news documents."
P17-1108,N16-1012,0,0.320635,"thods Abstractive summarization aims at generating the summary based on understanding the input text. It involves multiple subproblems like simplification, paraphrasing, and fusion. Previous research is mostly restricted in one or a few of the subproblems or specific domains (Woodsend and Lapata, 2012; Thadani and McKeown, 2013; Cheung and Penn, 2014; Pighin et al., 2014; Sun et al., 2015). As for neural network models, success is achieved on sentence abstractive summarization. Rush et al. (2015) train a neural attention model on a large corpus of news documents and their headlines, and later Chopra et al. (2016) extend their work with an attentive recurrent neural network framework. Nallapati et al. (2016) introduce various effective techniques in the RNN seq2seq framework. These neural sentence abstraction models are able to achieve state-of-the-art results on the DUC competition of generating headlinelevel summaries for news documents. Some recent works investigate neural abstractive models on the document summarization task. Cheng and Lapata (2016) also adopt a word extraction model, which is restricted to use the words of the source document to generate a summary, although the performance is much"
P17-1108,P13-1100,0,0.00653985,"Section 2 introduces related work. Section 3 describes our method. In Section 4 we present the experiments and have discussion. Finally in Section 5 we conclude this paper. 2 2.1 Related Work Extractive Summarization Methods Document summarization can be categorized to extractive methods and abstractive methods. Extractive methods extract sentences from the original document to form the summary. Notable early works include (Edmundson, 1969; Carbonell and Goldstein, 1998; McDonald, 2007). In recent years much progress has also been made under traditional extractive frameworks (Li et al., 2013; Dasgupta et al., 2013; Nishikawa et al., 2014). Neural networks have also been widely investigated on the extractive summarization task. Earlier works explore to use deep learning techniques in the traditional framework (Kobayashi et al., 2015; Yin and Pei, 2015; Cao et al., 2015a,b). More recent works predict the extraction of sentences in a more data-driven way. Cheng and Lapata (2016) propose an encoder-decoder approach where the encoder learns the representation of sentences and documents while the decoder classifies each sentence using an attention mechanism. Nallapati et al. (2017) propose a recurrent neural"
P17-1108,D15-1232,0,0.00968904,"ods Document summarization can be categorized to extractive methods and abstractive methods. Extractive methods extract sentences from the original document to form the summary. Notable early works include (Edmundson, 1969; Carbonell and Goldstein, 1998; McDonald, 2007). In recent years much progress has also been made under traditional extractive frameworks (Li et al., 2013; Dasgupta et al., 2013; Nishikawa et al., 2014). Neural networks have also been widely investigated on the extractive summarization task. Earlier works explore to use deep learning techniques in the traditional framework (Kobayashi et al., 2015; Yin and Pei, 2015; Cao et al., 2015a,b). More recent works predict the extraction of sentences in a more data-driven way. Cheng and Lapata (2016) propose an encoder-decoder approach where the encoder learns the representation of sentences and documents while the decoder classifies each sentence using an attention mechanism. Nallapati et al. (2017) propose a recurrent neural network (RNN)-based sequence model for extractive summarization of documents. Neural sentence extractive models are able to leverage large-scale training data and achieve performance better than traditional extractive sum"
P17-1108,P13-1099,0,0.0144325,"aper as follows. Section 2 introduces related work. Section 3 describes our method. In Section 4 we present the experiments and have discussion. Finally in Section 5 we conclude this paper. 2 2.1 Related Work Extractive Summarization Methods Document summarization can be categorized to extractive methods and abstractive methods. Extractive methods extract sentences from the original document to form the summary. Notable early works include (Edmundson, 1969; Carbonell and Goldstein, 1998; McDonald, 2007). In recent years much progress has also been made under traditional extractive frameworks (Li et al., 2013; Dasgupta et al., 2013; Nishikawa et al., 2014). Neural networks have also been widely investigated on the extractive summarization task. Earlier works explore to use deep learning techniques in the traditional framework (Kobayashi et al., 2015; Yin and Pei, 2015; Cao et al., 2015a,b). More recent works predict the extraction of sentences in a more data-driven way. Cheng and Lapata (2016) propose an encoder-decoder approach where the encoder learns the representation of sentences and documents while the decoder classifies each sentence using an attention mechanism. Nallapati et al. (2017) pro"
P17-1108,P15-1107,0,0.0489576,"t al. (2016) first attempt to explore the novelty factor of summarization, and propose a distraction-based attentional model. Unfortunately these state-ofthe-art neural abstractive summarization models are still not competitive to extractive methods, and there are several problems remain to be solved. 3 3.1 Our Method Overview In this section we introduce our method. We adopt an encoder-decoder framework, which is 1172 widely used in machine translation (Bahdanau et al., 2014) and dialog systems (Mou et al., 2016), etc. In particular, we use a hierarchical encoderdecoder framework similar to (Li et al., 2015), as shown in Figure 1. The main distinction of this work is that we introduce a graph-based attention mechanism which is illustrated in Figure 1b, and we propose a hierarchical decoding algorithm with a reference mechanism to tackle the difficulty of abstractive summary generation. In the following parts, we will first introduce the encoder-decoder framework, and then describe the graph-based attention and the hierarchical decoding algorithm. 3.2 Encoder The goal of the encoder is to map the input document to a vector representation. A document d is a sequence of sentences d = {si }, and a se"
P17-1108,W04-1013,0,0.0507513,"model. The dimension of word vectors is 100. λ is set to 0.9. The parameters of Adamax are set to those provided in (Kingma and Ba, 2014). The batch size is set to 8 documents, and an epoch is set containing 10,000 randomly sampled documents. Convergence is reached within 200 epochs on the DailyMail dataset and 120 epochs on the CNN dataset. It takes about one day for every 30 epochs on a GTX-1080 GPU card. γ is tuned on the validation set and the best choice is 300. The beam sizes for word decoder and sentence decoder are 15 and 2, respectively. 4.3 Evaluation We adopt the widely used ROUGE (Lin, 2004) toolkit for evaluation. We first compare with the reported results in (Chen et al., 2016) including various traditional extractive methods and a state-of-the-art abstractive model (DistractionM3) on the CNN dataset, as shown in Table 2. Uni-GRU is a non-hierarchical seq2seq baseline model. In Table 3 we compare our method with the results of state-of-the-art neural summarization methods reported in recent papers. Extractive models include NN-SE (Cheng and Lapata, 2016) and SummaRuNNer (Nallapati et al., 2017), while SummaRuNNer-abs is also an extractive model similar to SummaRuNNer but is tra"
P17-1108,W04-3252,0,0.0639939,"h1,3 (b) Graph-based attention. Figure 1: Hierarchical encoder-decoder framework and comparison of the attention mechanisms. in Figure 1a. This attention mechanism is useful in scenarios like machine translation and image captioning, because the model is able to learn a relevance mapping between the input and output. However, for document summarization, it is not easy for the model to learn how to summarize the salient information of a document, i.e., which sentences are more important to a document. To tackle this challenge, we learn from graphbased extractive summarization models TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), which are based on the PageRank (Page et al., 1999) algorithm. These unsupervised graph-based models show good ability to identify important sentences in a document. The underlying idea is that a sentence is important in a document if it is heavily linked with many important sentences (Wan, 2010). In graph-based extractive summarization, a graph G is constructed to rank the original sentences. The vertices V are the set of n sentences to be considered, and the edges E are the relations between the sentences, which are typically modeled by the similarity of"
P17-1108,C16-1316,0,0.0228898,". However these models still investigate few properties of the document summarization task. Chen et al. (2016) first attempt to explore the novelty factor of summarization, and propose a distraction-based attentional model. Unfortunately these state-ofthe-art neural abstractive summarization models are still not competitive to extractive methods, and there are several problems remain to be solved. 3 3.1 Our Method Overview In this section we introduce our method. We adopt an encoder-decoder framework, which is 1172 widely used in machine translation (Bahdanau et al., 2014) and dialog systems (Mou et al., 2016), etc. In particular, we use a hierarchical encoderdecoder framework similar to (Li et al., 2015), as shown in Figure 1. The main distinction of this work is that we introduce a graph-based attention mechanism which is illustrated in Figure 1b, and we propose a hierarchical decoding algorithm with a reference mechanism to tackle the difficulty of abstractive summary generation. In the following parts, we will first introduce the encoder-decoder framework, and then describe the graph-based attention and the hierarchical decoding algorithm. 3.2 Encoder The goal of the encoder is to map the input"
P17-1108,K16-1028,0,0.758335,"ut text. It involves multiple subproblems like simplification, paraphrasing, and fusion. Previous research is mostly restricted in one or a few of the subproblems or specific domains (Woodsend and Lapata, 2012; Thadani and McKeown, 2013; Cheung and Penn, 2014; Pighin et al., 2014; Sun et al., 2015). As for neural network models, success is achieved on sentence abstractive summarization. Rush et al. (2015) train a neural attention model on a large corpus of news documents and their headlines, and later Chopra et al. (2016) extend their work with an attentive recurrent neural network framework. Nallapati et al. (2016) introduce various effective techniques in the RNN seq2seq framework. These neural sentence abstraction models are able to achieve state-of-the-art results on the DUC competition of generating headlinelevel summaries for news documents. Some recent works investigate neural abstractive models on the document summarization task. Cheng and Lapata (2016) also adopt a word extraction model, which is restricted to use the words of the source document to generate a summary, although the performance is much worse than the sentence extractive model. Nallapati et al. (2016) extend the sentence summariza"
P17-1108,C14-1156,0,0.0803598,"Missing"
P17-1108,D14-1162,0,0.110927,"Missing"
P17-1108,P14-1084,0,0.0170945,"e model for extractive summarization of documents. Neural sentence extractive models are able to leverage large-scale training data and achieve performance better than traditional extractive summarization methods. 2.2 Abstractive Summarization Methods Abstractive summarization aims at generating the summary based on understanding the input text. It involves multiple subproblems like simplification, paraphrasing, and fusion. Previous research is mostly restricted in one or a few of the subproblems or specific domains (Woodsend and Lapata, 2012; Thadani and McKeown, 2013; Cheung and Penn, 2014; Pighin et al., 2014; Sun et al., 2015). As for neural network models, success is achieved on sentence abstractive summarization. Rush et al. (2015) train a neural attention model on a large corpus of news documents and their headlines, and later Chopra et al. (2016) extend their work with an attentive recurrent neural network framework. Nallapati et al. (2016) introduce various effective techniques in the RNN seq2seq framework. These neural sentence abstraction models are able to achieve state-of-the-art results on the DUC competition of generating headlinelevel summaries for news documents. Some recent works in"
P17-1108,D15-1044,0,0.544526,"e summarization involves sophisticated techniques including meaning representation, content organization, and surface realization. Each of these techniques has large space to be improved (Yao et al., 2017). Due to the immaturity of natural language generation techniques, fully abstractive approaches are still at the beginning and cannot always ensure grammatical abstracts. Recent neural networks enable an end-to-end framework for natural language generation. Success has been witnessed on tasks like machine translation and image captioning, together with the abstractive sentence summarization (Rush et al., 2015). Unfortunately, the extension of sentence abstractive methods to the document summarization task is not straightforward. Encoding and decoding for a long sequence of multiple sentences, currently still lack satisfactory solutions (Yao et al., 2017). Recent abstractive document summarization models are yet not able to achieve convincing performance, with a considerable gap from extractive methods. In this paper, we review the key factors of document summarization, i.e., the saliency, fluency, coherence, and novelty requirements of the generated summary. Fluency is what neural generation models"
P17-1108,P15-1045,0,0.017087,"e summarization of documents. Neural sentence extractive models are able to leverage large-scale training data and achieve performance better than traditional extractive summarization methods. 2.2 Abstractive Summarization Methods Abstractive summarization aims at generating the summary based on understanding the input text. It involves multiple subproblems like simplification, paraphrasing, and fusion. Previous research is mostly restricted in one or a few of the subproblems or specific domains (Woodsend and Lapata, 2012; Thadani and McKeown, 2013; Cheung and Penn, 2014; Pighin et al., 2014; Sun et al., 2015). As for neural network models, success is achieved on sentence abstractive summarization. Rush et al. (2015) train a neural attention model on a large corpus of news documents and their headlines, and later Chopra et al. (2016) extend their work with an attentive recurrent neural network framework. Nallapati et al. (2016) introduce various effective techniques in the RNN seq2seq framework. These neural sentence abstraction models are able to achieve state-of-the-art results on the DUC competition of generating headlinelevel summaries for news documents. Some recent works investigate neural ab"
P17-1108,I13-1198,0,0.00736217,"ose a recurrent neural network (RNN)-based sequence model for extractive summarization of documents. Neural sentence extractive models are able to leverage large-scale training data and achieve performance better than traditional extractive summarization methods. 2.2 Abstractive Summarization Methods Abstractive summarization aims at generating the summary based on understanding the input text. It involves multiple subproblems like simplification, paraphrasing, and fusion. Previous research is mostly restricted in one or a few of the subproblems or specific domains (Woodsend and Lapata, 2012; Thadani and McKeown, 2013; Cheung and Penn, 2014; Pighin et al., 2014; Sun et al., 2015). As for neural network models, success is achieved on sentence abstractive summarization. Rush et al. (2015) train a neural attention model on a large corpus of news documents and their headlines, and later Chopra et al. (2016) extend their work with an attentive recurrent neural network framework. Nallapati et al. (2016) introduce various effective techniques in the RNN seq2seq framework. These neural sentence abstraction models are able to achieve state-of-the-art results on the DUC competition of generating headlinelevel summar"
P17-1108,C10-1128,1,0.8171,"ation, it is not easy for the model to learn how to summarize the salient information of a document, i.e., which sentences are more important to a document. To tackle this challenge, we learn from graphbased extractive summarization models TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004), which are based on the PageRank (Page et al., 1999) algorithm. These unsupervised graph-based models show good ability to identify important sentences in a document. The underlying idea is that a sentence is important in a document if it is heavily linked with many important sentences (Wan, 2010). In graph-based extractive summarization, a graph G is constructed to rank the original sentences. The vertices V are the set of n sentences to be considered, and the edges E are the relations between the sentences, which are typically modeled by the similarity of sentences. Let W ∈ Rn×n be the adjacent matrix. Then the saliency scores of the sentences are determined by making use of the global information on the graph recursively, as: f (t + 1) = λW D−1 f (t) + (1 − λ)y (2) where f = [f1 , . . . , fn ] ∈ Rn denotes the rank scores of the n sentences. f (t) denotes the rank scores at the t-th"
P17-1108,D12-1022,0,0.0166415,"allapati et al. (2017) propose a recurrent neural network (RNN)-based sequence model for extractive summarization of documents. Neural sentence extractive models are able to leverage large-scale training data and achieve performance better than traditional extractive summarization methods. 2.2 Abstractive Summarization Methods Abstractive summarization aims at generating the summary based on understanding the input text. It involves multiple subproblems like simplification, paraphrasing, and fusion. Previous research is mostly restricted in one or a few of the subproblems or specific domains (Woodsend and Lapata, 2012; Thadani and McKeown, 2013; Cheung and Penn, 2014; Pighin et al., 2014; Sun et al., 2015). As for neural network models, success is achieved on sentence abstractive summarization. Rush et al. (2015) train a neural attention model on a large corpus of news documents and their headlines, and later Chopra et al. (2016) extend their work with an attentive recurrent neural network framework. Nallapati et al. (2016) introduce various effective techniques in the RNN seq2seq framework. These neural sentence abstraction models are able to achieve state-of-the-art results on the DUC competition of gene"
P17-1193,C10-1011,0,0.0780292,"92.40 --92.03 92.53 UP 90.09 -90.21 -- PCEDT UR UF 85.90 87.95 --85.51 87.80 --- Table 3: Parsing accuracy evaluated on the test sets. 2014), and the data splitting policy follows the shared task. All the four data sets are publicly available from LDC (Oepen et al., 2016). Experiments for CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined HMM tagger (Huang et al., 2010). Experiments for the other three data sets used POS-tags provided by the shared task. We also use features extracted from pseudo trees. We utilize the Mate parser (Bohnet, 2010) to generate pseudo trees. The pre-processing for CCGBank, DeepBank and EnjuBank are exactly the same as in experiments reported in (Zhang et al., 2016). 5.3 Accuracy We evaluate two parsing algorithms, the algorithm for noncrossing dependency graphs (Kuhlmann and Jonsson, 2015), i.e. pagenumber-1 (denoted as P1) graphs, and our quartic-time algorithm (denoted as 1ECP2d ). Table 2 summerizes the accuracy obtained our parser. Same feature templates are applied for disambiguation. We can see that our new algorithm yields significant improvements on all data sets, as expected. Especially, due to"
P17-1193,D07-1101,0,0.0702152,"processes. There is no need to distinguish one special derivation here. . i l k Int[i, . j] LR[i,. k, j] L[i, .k, j] . . j 4.8.4 Complexity Int[i, . j] . k] L[k,.j, l] R[i, .l, k] Int[l, Int[k, . j] . Int[i, . l] Int[k, . j] . l] L[l, .k, i] Int[i, Int[l, . k] Figure 8: A maximal 1 EC / P 2 graph and its two derivations. For brevity, we elide the edges created in each derivation step. restrictions. So our algorithm is sound. 4.8.2 Greedy Search during Construction There is an important difference between our algorithm and Eisner-style MST algorithms (Eisner, 1996b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) for trees as well as Kuhlmann and Jonsson’s Maximum Subgraph algorithm for noncrossing graphs. In each construction step, our algorithm allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. In each step, we do greedy search and decide if adding an related arc according to local scores. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. That means adding one more arc voilates the 1 EC or P 2 restrictions. For all o"
P17-1193,W02-1001,0,0.220605,"expressiveness in terms of linguistic data. This degenerated version algorithm requires O(n4 ) time and O(n3 ) space. 5 Practical Parsing 5.1 Disambiguation We extend our quartic-time parsing algorithm into a practical parser. In the context of data-driven parsing, this requires an extra disambiguation model. As with many other parsers, we employ a global linear model. Following Zhang et al. (2016)’s experience, we define rich features extracted from word, POS-tags and pseudo trees. For details we refer to the source code. To estimate parameters, we utilize the averaged perceptron algorithm (Collins, 2002). 4.8.3 Spurious Ambiguity 5.2 Data To generate the same graph, even a maximal 1 EC / P 2 graph, we may have different derivations. Figure 8 is an example. This is similar to syntactic analysis licensed by Combinatory Categorial Grammar (CCG; Steedman, 1996, 2000). To derive one surface string, there usually exists multiple CCG derivations. A practice of CCG parsing is defining one particular derivation as the standard one, namely normal form (Eisner, 1996a). The spurious ambiguity in our algorithm does not affect the correctness of first-order parsing, because scores are assigned to individua"
P17-1193,P15-1149,1,0.904732,"Missing"
P17-1193,S14-2080,1,0.878685,"w1 · · · wn−1 of length n, the vertices, i.e. words, are indexed with integers, an arc from wi to wj as a(i,j) , and the common endpoint, namely pencil point, of all edges crossed with a(i,j) or a(j,i) as pt(i, j). We denote an edge as e(i,j) , if we do not consider its direction. Background Dependency parsing is the task of mapping a natural language sentence into a dependency graph. Previous work on dependency parsing mainly focused on tree-shaped representations. Recently, it is shown that data-driven parsing techniques are also applicable to generate more flexible deep dependency graphs (Du et al., 2014; Martins and Almeida, 2014; Du et al., 2015b,a; Zhang et al., 2016; Sun et al., 2017). Parsing for deep dependency representations can be viewed as the search for Maximum Subgraphs for a certain graph class G (Kuhlmann and Jonsson, 2015), a generalization of the MST perspective for tree parsing. In particular, we have the following optimization problem: Given an arc-weighted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projec"
P17-1193,S15-2154,1,0.948367,"e. words, are indexed with integers, an arc from wi to wj as a(i,j) , and the common endpoint, namely pencil point, of all edges crossed with a(i,j) or a(j,i) as pt(i, j). We denote an edge as e(i,j) , if we do not consider its direction. Background Dependency parsing is the task of mapping a natural language sentence into a dependency graph. Previous work on dependency parsing mainly focused on tree-shaped representations. Recently, it is shown that data-driven parsing techniques are also applicable to generate more flexible deep dependency graphs (Du et al., 2014; Martins and Almeida, 2014; Du et al., 2015b,a; Zhang et al., 2016; Sun et al., 2017). Parsing for deep dependency representations can be viewed as the search for Maximum Subgraphs for a certain graph class G (Kuhlmann and Jonsson, 2015), a generalization of the MST perspective for tree parsing. In particular, we have the following optimization problem: Given an arc-weighted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projective trees, the problem can be solved in tim"
P17-1193,P96-1011,0,0.657272,"nor P 2 2116 cies, rather than derivation processes. There is no need to distinguish one special derivation here. . i l k Int[i, . j] LR[i,. k, j] L[i, .k, j] . . j 4.8.4 Complexity Int[i, . j] . k] L[k,.j, l] R[i, .l, k] Int[l, Int[k, . j] . Int[i, . l] Int[k, . j] . l] L[l, .k, i] Int[i, Int[l, . k] Figure 8: A maximal 1 EC / P 2 graph and its two derivations. For brevity, we elide the edges created in each derivation step. restrictions. So our algorithm is sound. 4.8.2 Greedy Search during Construction There is an important difference between our algorithm and Eisner-style MST algorithms (Eisner, 1996b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) for trees as well as Kuhlmann and Jonsson’s Maximum Subgraph algorithm for noncrossing graphs. In each construction step, our algorithm allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. In each step, we do greedy search and decide if adding an related arc according to local scores. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. That means adding one more arc voi"
P17-1193,C96-1058,0,0.644214,"nor P 2 2116 cies, rather than derivation processes. There is no need to distinguish one special derivation here. . i l k Int[i, . j] LR[i,. k, j] L[i, .k, j] . . j 4.8.4 Complexity Int[i, . j] . k] L[k,.j, l] R[i, .l, k] Int[l, Int[k, . j] . Int[i, . l] Int[k, . j] . l] L[l, .k, i] Int[i, Int[l, . k] Figure 8: A maximal 1 EC / P 2 graph and its two derivations. For brevity, we elide the edges created in each derivation step. restrictions. So our algorithm is sound. 4.8.2 Greedy Search during Construction There is an important difference between our algorithm and Eisner-style MST algorithms (Eisner, 1996b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) for trees as well as Kuhlmann and Jonsson’s Maximum Subgraph algorithm for noncrossing graphs. In each construction step, our algorithm allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. In each step, we do greedy search and decide if adding an related arc according to local scores. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. That means adding one more arc voi"
P17-1193,P10-1151,0,0.277089,"Missing"
P17-1193,hajic-etal-2012-announcing,0,0.15452,"Missing"
P17-1193,J13-4006,0,0.113669,"Missing"
P17-1193,J07-3004,0,0.105502,"one linguistically-rare structure descreases the complexity to O(n4 ). We also extend our quartic-time algorithm into a practical parser with a discriminative disambiguation model and evaluate its performance on four linguistic data sets used in semantic dependency parsing. 1 Introduction Dependency parsing has long been studied as a central issue in developing syntactic or semantic analysis. Recently, some linguistic projects grounded on deep grammar formalisms, including CCG, LFG, and HPSG, draw attentions to rich syntactic and semantic dependency annotations that are not limited to trees (Hockenmaier and Steedman, 2007; Sun et al., 2014; Ivanova et al., 2012). Parsing for these deep dependency representations can be viewed as the search for Maximum Subgraphs (Kuhlmann and Jonsson, 2015). This is a natural extension of the Maximum Spanning Tree (MST) perspective (McDonald et al., 2005) for dependency tree parisng. One main challenge of the Maximum Subgraph perspective is to design tracTable algorithms for certain graph classes that have good empirical coverage for linguistic annotations. Unfortunately, no previously defined class simultaneously has high ∗ The first two authors contribute equally. coverage an"
P17-1193,D10-1002,0,0.0926707,"Bank UR UF 86.98 88.90 88.85 88.95 88.65 89.39 --- UP 93.83 92.92 93.18 -- EnjuBank UR UF 91.49 92.64 92.83 92.87 91.12 92.14 --- UP 94.23 92.49 -93.03 CCGBank UR UF 91.13 92.66 92.30 92.40 --92.03 92.53 UP 90.09 -90.21 -- PCEDT UR UF 85.90 87.95 --85.51 87.80 --- Table 3: Parsing accuracy evaluated on the test sets. 2014), and the data splitting policy follows the shared task. All the four data sets are publicly available from LDC (Oepen et al., 2016). Experiments for CCG-grounded analysis were performed using automatically assigned POS-tags that are generated by a symbol-refined HMM tagger (Huang et al., 2010). Experiments for the other three data sets used POS-tags provided by the shared task. We also use features extracted from pseudo trees. We utilize the Mate parser (Bohnet, 2010) to generate pseudo trees. The pre-processing for CCGBank, DeepBank and EnjuBank are exactly the same as in experiments reported in (Zhang et al., 2016). 5.3 Accuracy We evaluate two parsing algorithms, the algorithm for noncrossing dependency graphs (Kuhlmann and Jonsson, 2015), i.e. pagenumber-1 (denoted as P1) graphs, and our quartic-time algorithm (denoted as 1ECP2d ). Table 2 summerizes the accuracy obtained our p"
P17-1193,W12-3602,0,0.345787,"Missing"
P17-1193,P10-1001,0,0.0363537,"is no need to distinguish one special derivation here. . i l k Int[i, . j] LR[i,. k, j] L[i, .k, j] . . j 4.8.4 Complexity Int[i, . j] . k] L[k,.j, l] R[i, .l, k] Int[l, Int[k, . j] . Int[i, . l] Int[k, . j] . l] L[l, .k, i] Int[i, Int[l, . k] Figure 8: A maximal 1 EC / P 2 graph and its two derivations. For brevity, we elide the edges created in each derivation step. restrictions. So our algorithm is sound. 4.8.2 Greedy Search during Construction There is an important difference between our algorithm and Eisner-style MST algorithms (Eisner, 1996b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) for trees as well as Kuhlmann and Jonsson’s Maximum Subgraph algorithm for noncrossing graphs. In each construction step, our algorithm allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. In each step, we do greedy search and decide if adding an related arc according to local scores. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. That means adding one more arc voilates the 1 EC or P 2 restrictions. For all other aforementioned algo"
P17-1193,Q15-1040,0,0.537517,"an edge as e(i,j) , if we do not consider its direction. Background Dependency parsing is the task of mapping a natural language sentence into a dependency graph. Previous work on dependency parsing mainly focused on tree-shaped representations. Recently, it is shown that data-driven parsing techniques are also applicable to generate more flexible deep dependency graphs (Du et al., 2014; Martins and Almeida, 2014; Du et al., 2015b,a; Zhang et al., 2016; Sun et al., 2017). Parsing for deep dependency representations can be viewed as the search for Maximum Subgraphs for a certain graph class G (Kuhlmann and Jonsson, 2015), a generalization of the MST perspective for tree parsing. In particular, we have the following optimization problem: Given an arc-weighted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projective trees, the problem can be solved in time O(|V |3 ), and if G is the set of noncrossing dependency graphs, the complexity is O(|V |3 ). Unfortunately, no previously defined class simultaneously has high coverage on deep dependency ann"
P17-1193,S14-2082,0,0.238393,"Missing"
P17-1193,E06-1011,0,0.181954,"ies, rather than derivation processes. There is no need to distinguish one special derivation here. . i l k Int[i, . j] LR[i,. k, j] L[i, .k, j] . . j 4.8.4 Complexity Int[i, . j] . k] L[k,.j, l] R[i, .l, k] Int[l, Int[k, . j] . Int[i, . l] Int[k, . j] . l] L[l, .k, i] Int[i, Int[l, . k] Figure 8: A maximal 1 EC / P 2 graph and its two derivations. For brevity, we elide the edges created in each derivation step. restrictions. So our algorithm is sound. 4.8.2 Greedy Search during Construction There is an important difference between our algorithm and Eisner-style MST algorithms (Eisner, 1996b; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) for trees as well as Kuhlmann and Jonsson’s Maximum Subgraph algorithm for noncrossing graphs. In each construction step, our algorithm allows multiple arcs to be constructed, but whether or not such arcs are added to the target graph depends on their arc-weights. In each step, we do greedy search and decide if adding an related arc according to local scores. If all arcs are assigned scores that are greater than 0, the output of our algorithm includes the most complicated 1 EC / P 2 graphs. That means adding one more arc voilates the 1 EC or P 2 restric"
P17-1193,H05-1066,0,0.41364,"Missing"
P17-1193,J16-3001,1,0.885044,"d with integers, an arc from wi to wj as a(i,j) , and the common endpoint, namely pencil point, of all edges crossed with a(i,j) or a(j,i) as pt(i, j). We denote an edge as e(i,j) , if we do not consider its direction. Background Dependency parsing is the task of mapping a natural language sentence into a dependency graph. Previous work on dependency parsing mainly focused on tree-shaped representations. Recently, it is shown that data-driven parsing techniques are also applicable to generate more flexible deep dependency graphs (Du et al., 2014; Martins and Almeida, 2014; Du et al., 2015b,a; Zhang et al., 2016; Sun et al., 2017). Parsing for deep dependency representations can be viewed as the search for Maximum Subgraphs for a certain graph class G (Kuhlmann and Jonsson, 2015), a generalization of the MST perspective for tree parsing. In particular, we have the following optimization problem: Given an arc-weighted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projective trees, the problem can be solved in time O(|V |3 ), and if G i"
P17-1193,S14-2008,0,0.219665,"Missing"
P17-1193,Q13-1002,0,0.711438,"ted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projective trees, the problem can be solved in time O(|V |3 ), and if G is the set of noncrossing dependency graphs, the complexity is O(|V |3 ). Unfortunately, no previously defined class simultaneously has high coverage on deep dependency annotations and low-degree polynomial decoding algorithms for practical parsing. In this paper, we study well-motivated restrictions: 1 EC (Pitler et al., 2013) and P 2 (Kuhlmann and Jonsson, 2015). We will show that relatively satisfactory coverage and parsing complexity can be obtained for graphs that satisfy both restrictions. 3 The 1 EC, P 2 Graphs 3.1 The 1 EC Restriction Pitler et al. (2013) introduced a very nice property for modelling non-projective dependency trees, i.e. 1 EC. This property not only covers a large amount of tree annotations in natural language treebanks, but also allows the corresponding MST problem to bo solved in time of O(n4 ). The formal description of the 1 EC property is adopted from (Pitler et al., 2013). Definition 1"
P17-1193,P17-1077,1,0.833825,"arc from wi to wj as a(i,j) , and the common endpoint, namely pencil point, of all edges crossed with a(i,j) or a(j,i) as pt(i, j). We denote an edge as e(i,j) , if we do not consider its direction. Background Dependency parsing is the task of mapping a natural language sentence into a dependency graph. Previous work on dependency parsing mainly focused on tree-shaped representations. Recently, it is shown that data-driven parsing techniques are also applicable to generate more flexible deep dependency graphs (Du et al., 2014; Martins and Almeida, 2014; Du et al., 2015b,a; Zhang et al., 2016; Sun et al., 2017). Parsing for deep dependency representations can be viewed as the search for Maximum Subgraphs for a certain graph class G (Kuhlmann and Jonsson, 2015), a generalization of the MST perspective for tree parsing. In particular, we have the following optimization problem: Given an arc-weighted graph G = (V, A), find a subgraph G′ = (V, A′ ⊆ A) with maximum total weight such that G′ belongs to G. The choice of G determines the computational complexity of dependency parsing. For example, if G is the set of projective trees, the problem can be solved in time O(|V |3 ), and if G is the set of noncro"
P17-1193,P14-1042,1,0.920004,"Missing"
P18-1038,P13-1023,0,0.0624778,"arsing. This accuracy is equivalent to that of English Resource Grammar guided models, suggesting that (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations. 1 Introduction Graph-structured semantic representations, e.g. Semantic Dependency Graphs (SDG; Clark et al., 2002; Ivanova et al., 2012), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006), Abstract Meaning Representation (AMR; Banarescu et al., 2013), Dependency-based Minimal Recursion Semantics (DMRS; Copestake, 2009), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013), provide a lightweight yet effective way to encode rich semantic information of natural language sentences (Kuhlmann and Oepen, 2016). Parsing to semantic graphs has been extensively studied recently. At the risk of oversimplifying, work in this area can be divided into three types, according to how much structural information of a target graph is explicitly modeled. Parsers of the first type throw an 1 http://moin.delph-in.net/ErgSemantics 408 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 408–418 c Melbourne, Australia, July 15 -"
P18-1038,W13-2322,0,0.101456,"t over the best existing data-driven model, indicating, in our view, the importance of linguistically-informed derivation for data-driven semantic parsing. This accuracy is equivalent to that of English Resource Grammar guided models, suggesting that (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations. 1 Introduction Graph-structured semantic representations, e.g. Semantic Dependency Graphs (SDG; Clark et al., 2002; Ivanova et al., 2012), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006), Abstract Meaning Representation (AMR; Banarescu et al., 2013), Dependency-based Minimal Recursion Semantics (DMRS; Copestake, 2009), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013), provide a lightweight yet effective way to encode rich semantic information of natural language sentences (Kuhlmann and Oepen, 2016). Parsing to semantic graphs has been extensively studied recently. At the risk of oversimplifying, work in this area can be divided into three types, according to how much structural information of a target graph is explicitly modeled. Parsers of the first type throw an 1 http://moin.delph-in.net/ErgSemantics 408"
P18-1038,P02-1042,0,0.24527,"Missing"
P18-1038,E09-1001,0,0.35333,"rtance of linguistically-informed derivation for data-driven semantic parsing. This accuracy is equivalent to that of English Resource Grammar guided models, suggesting that (recurrent) neural network models are able to effectively learn deep linguistic knowledge from annotations. 1 Introduction Graph-structured semantic representations, e.g. Semantic Dependency Graphs (SDG; Clark et al., 2002; Ivanova et al., 2012), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006), Abstract Meaning Representation (AMR; Banarescu et al., 2013), Dependency-based Minimal Recursion Semantics (DMRS; Copestake, 2009), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013), provide a lightweight yet effective way to encode rich semantic information of natural language sentences (Kuhlmann and Oepen, 2016). Parsing to semantic graphs has been extensively studied recently. At the risk of oversimplifying, work in this area can be divided into three types, according to how much structural information of a target graph is explicitly modeled. Parsers of the first type throw an 1 http://moin.delph-in.net/ErgSemantics 408 Proceedings of the 56th Annual Meeting of the Association for Computa"
P18-1038,W15-0128,0,0.177066,"HS (syntax) D+N V + PUNCT CM + HD↓V V + HD-CMP SP-HD + HD-CMP HD-CMP arg2 V HD-CMP arg1 arg1 SP-HD N bv D V HD↓V V(2,3) arg1 HD-CMP(3,6) ¬ RHS (semantics)  Figure 2: The grammar extraction process of the running example. Conceptual edges which are directly aligned with the syntactic rules are painted in green. The span-based alignment is shown in the parentheses. Structural edges that connect conceptual edges are painted in brown. Green edges and brown edges together form the subgraph, which acts as RHS in the HRG rule. External nodes are represented as solid dots. ciple of compositionality (Bender et al., 2015). A precise syntax-semantics interface is introduced to guarantee compositionality and therefore all meaning units can be traced back to linguistic signals, including both lexical and constructional ones. Take Figure 2 for example. Every concept, e.g. the existence quantifier some q, is associated with a surface string. We favor such correspondence not because it eases extraction of SHRGs, but because we emphasize sentence meanings that are from forms. The connection between syntax (sentence form) and semantics (word and sentence meaning) is fundamental to the study of language. 3.2 results ba"
P18-1038,D16-1001,0,0.017707,"divided into two steps: syntactic parsing and semantic interpretation. Syntactic parsing utilizes the CFG part to get a derivation that is shared by the HRG part. At one derivation step, there may be more than one HRG rule applicable. In this case, we need a semantic disambiguation model to choose a good one. 4.1 Semantic Interpretation ˆ = argmaxR∈R(T ) S CORE(R|T ) R (1) To solve this optimization problem, we implement a greedy search decoder and a bottom-up beam search decoder. The final semantic graph G is read ˆ off from R. Syntactic Parsing Following the LSTM-Minus approach proposed by Cross and Huang (2016), we build a constituent parser with a CKY decoder. We denote the output vectors of forward and backward LSTM as fi and bi . The feature si,j of a span (i, j) can be calculated from the differences of LSTM encodings: 4.2.1 Greedy Search Model In this model, we assume that each HRG rule is selected independently of the others. The score of G is defined as the sum of all rule scores: si,j = (fj − fi ) ⊕ (bi − bj ) S CORE(R = {r1 , r2 , ...}|T ) = The operator ⊕ indicates the concatenation of two vectors. Constituency parsing can be regarded as predicting scores for spans and labels, and getting"
P18-1038,P17-1112,0,0.733501,"oin.delph-in.net/ErgSemantics 408 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 408–418 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Model Grammar SDG EDS DMRS Data-driven ERG-based SHRG-based NO Unification Rewriting 89.4 92.80 -- 85.48 89.58 90.39 84.16 89.64 89.51 HD-CMP HD-CMP arg1 S arg1 arg1 bv arg1 D SP-HD Table 1: Parsing accuracy of the best existing grammar-free and -based models as well as our SHRG-based model. Results are copied from (Oepen et al., 2015; Peng et al., 2017a; Buys and Blunsom, 2017). arg2 _some_q ing techniques, we build a robust SHRG parser that is able to produce semantic analysis for all sentences. Our parser achieves an accuracy of 90.35 for EDS and 89.51 for DMRS in terms of EL EMENTARY DEPENDENCY MATCH (EDM) which outperforms the best existing grammar-free model (Buys and Blunsom, 2017) by a significant margin (see Table 1). This marked result affirms the value of modeling the syntacto-semantic composition process for semantic parsing. On sentences that can be parsed by ERG-guided parsers, e.g. PET2 or ACE3 , significant accuracy gaps between ERG-guided parsers and"
P18-1038,W11-2927,0,0.45783,"are the original labels, namely fine-grained construction types. We use the part before the first underscore of each label, e.g. SP-HD, as a coarse-grained label. The coarse-grained labels are more like the highly generalized rule schemata proposed by Pollard and Sag (1994). Some statistics are shown in Table 3. Instead of using gold-standard trees to extract a synchronous grammar, we also tried randomlygenerated alignment-compatible trees. The result is shown in Table 4. Gold standard trees exhibit a low entropy, indicating a high regularity. 5.3 Condensed Results of Semantic Interpretation Dridan and Oepen (2011) proposed the EDM metric to evaluate the performance the ERS-based graphs. EDM uses the alignment between the nodes in a graph and the spans in a string to detect the common parts between two graphs. It converts the predicate and predicate–argument relationship to comparable triples and calculates the correctness in these triples. A predicate of label L and span S is denoted as triple (S, NAME, L) and a relationship R between the predicate labelled P and argument labelled A is denoted as triple (P, R, A). We calculate the F1 value of the total triples as EDM score. Similarity, we compute the F"
P18-1038,P17-1193,1,0.828312,"to a sequence-to-sequence model and leverage the power of deep learning technologies to obtain auxiliary symbols to transform the output sequence into a graph (Peng et al., 2017b; Konstas et al., 2017). The strategy of the second type is to gradually generate a graph in a greedy search fashion (Zhang et al., 2016; Buys and Blunsom, 2017). Usually, a transition system is defined to handle graph construction. The last solution explicitly associates each basic part with a target graph score, and casts parsing as the search for the graphs with highest sum of partial scores (Flanigan et al., 2014; Cao et al., 2017). Although many parsers achieve encouraging results, they are very hard for linguists to interpret and understand, partially because they do not explicitly model the syntacto-semantic composition process which is a significant characteristic of natural languages. In theory, Synchronous Hyperedge Replacement Grammar (SHRG; Drewes et al., 1997) provides a mathematically sound framework to construct semantic graphs. In practice, however, initial results on the utility of SHRG for semantic parsing were somewhat disappointing (Peng et al., 2015; Peng and Gildea, 2016). In this paper, we show that t"
P18-1038,P14-1134,0,0.04233,"tract input sentence into a sequence-to-sequence model and leverage the power of deep learning technologies to obtain auxiliary symbols to transform the output sequence into a graph (Peng et al., 2017b; Konstas et al., 2017). The strategy of the second type is to gradually generate a graph in a greedy search fashion (Zhang et al., 2016; Buys and Blunsom, 2017). Usually, a transition system is defined to handle graph construction. The last solution explicitly associates each basic part with a target graph score, and casts parsing as the search for the graphs with highest sum of partial scores (Flanigan et al., 2014; Cao et al., 2017). Although many parsers achieve encouraging results, they are very hard for linguists to interpret and understand, partially because they do not explicitly model the syntacto-semantic composition process which is a significant characteristic of natural languages. In theory, Synchronous Hyperedge Replacement Grammar (SHRG; Drewes et al., 1997) provides a mathematically sound framework to construct semantic graphs. In practice, however, initial results on the utility of SHRG for semantic parsing were somewhat disappointing (Peng et al., 2015; Peng and Gildea, 2016). In this pa"
P18-1038,P13-1091,0,0.100031,"when replacing a hyperedge. An HRG G = hN, T, P, Si is a graph rewriting system, where N and T are two disjoint finite sets of nonterminal and terminal symbols respectively. S ∈ N is the start symbol. P is a finite set of productions of the form A → R, where the left hand side (LHS) A ∈ N , and the right hand side (RHS) R is a hypergraph with edge labels over N ∪ T . The rewriting process replaces a nonterminal hyperedge with the graph fragment specified by a production’s RHS, attaching each external node to the matched node of the corresponding LHS. An example is shown in Figure 1. Following Chiang et al. (2013), we make the nodes only describe connections between edges and store no other information. A synchronous grammar defines mappings between different grammars. Here we focus on relating a string grammar, CFG in our case, to a graph grammar, i.e., HRG. SHRG can be represented as tuple G = hN, T, T 0 , P, Si. N is a finite set of nonterminal symbols in both CFG and HRG. T 0 and T are finite sets of terminal symbols in CFG and HRG, respectively. S ∈ N is the start symbol. P is a finite set of productions of the form A → hR, R0 , ∼i, where A ∈ N , R is a hypergraph 3 3.1 Grammar Extraction Graph Re"
P18-1038,W12-3602,0,0.177345,"Missing"
P18-1038,Q16-1023,0,0.0692475,"pa ⊕ pb )[L] 5 We assign a beam to each node in the syntactic tree. To ensure that we always get a subgraph which does not contain any nonterminal edges during the search process, we perform the beam search in the bottom-up direction. We only reserve top k subgraphs in each beam. Figure 3 illustrates the process. 4.3 5.1 Training The objective of training is to make the score of the correct graph higher than incorrect graphs. We use the score difference between the correct graph Rg and the highest scoring incorrect graph as the loss: ˆ )−S CORE(Rg |T ) loss = maxR6ˆ =Rg S CORE(R|T Following (Kiperwasser and Goldberg, 2016)’s experience of loss augmented inference, in order to update graphs which have high model scores but are very wrong, we augment each factor belonging to the gold graph by adding a penalty term c to its score. Finally the loss term is: loss = S CORE(Rg |T ) − X Results of Grammar Extraction DeepBank provides fine-grained syntactic trees with rich information. For example, the label SP-HD HC C denotes that this is a “head+specifier” construction, where the semantic head is also the syntactic head. But there c− X Set-up DeepBank is an annotation of the Penn TreeBank Wall Street Journal which is"
P18-1038,K15-1004,0,0.259315,"highest sum of partial scores (Flanigan et al., 2014; Cao et al., 2017). Although many parsers achieve encouraging results, they are very hard for linguists to interpret and understand, partially because they do not explicitly model the syntacto-semantic composition process which is a significant characteristic of natural languages. In theory, Synchronous Hyperedge Replacement Grammar (SHRG; Drewes et al., 1997) provides a mathematically sound framework to construct semantic graphs. In practice, however, initial results on the utility of SHRG for semantic parsing were somewhat disappointing (Peng et al., 2015; Peng and Gildea, 2016). In this paper, we show that the performance that can be achieved by an SHRG-based parser is far higher than what has previously been demonstrated. We focus here on relating SHRG rules to the syntactosemantic composition process because we feel that information about syntax-semantics interface has been underexploited in the data-driven parsing architecture. We demonstrate the feasibility of inducing a high-quality, linguistically-informed SHRG from compositional semantic annotations licensed by English Resource Grammar (ERG; Flickinger, 2000), dubbed English Resource S"
P18-1038,P17-1014,0,0.097929,"Missing"
P18-1038,E17-1035,0,0.0249248,"throw an 1 http://moin.delph-in.net/ErgSemantics 408 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 408–418 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Model Grammar SDG EDS DMRS Data-driven ERG-based SHRG-based NO Unification Rewriting 89.4 92.80 -- 85.48 89.58 90.39 84.16 89.64 89.51 HD-CMP HD-CMP arg1 S arg1 arg1 bv arg1 D SP-HD Table 1: Parsing accuracy of the best existing grammar-free and -based models as well as our SHRG-based model. Results are copied from (Oepen et al., 2015; Peng et al., 2017a; Buys and Blunsom, 2017). arg2 _some_q ing techniques, we build a robust SHRG parser that is able to produce semantic analysis for all sentences. Our parser achieves an accuracy of 90.35 for EDS and 89.51 for DMRS in terms of EL EMENTARY DEPENDENCY MATCH (EDM) which outperforms the best existing grammar-free model (Buys and Blunsom, 2017) by a significant margin (see Table 1). This marked result affirms the value of modeling the syntacto-semantic composition process for semantic parsing. On sentences that can be parsed by ERG-guided parsers, e.g. PET2 or ACE3 , significant accuracy gaps betw"
P18-1038,J16-4009,0,0.144393,"s are able to effectively learn deep linguistic knowledge from annotations. 1 Introduction Graph-structured semantic representations, e.g. Semantic Dependency Graphs (SDG; Clark et al., 2002; Ivanova et al., 2012), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006), Abstract Meaning Representation (AMR; Banarescu et al., 2013), Dependency-based Minimal Recursion Semantics (DMRS; Copestake, 2009), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013), provide a lightweight yet effective way to encode rich semantic information of natural language sentences (Kuhlmann and Oepen, 2016). Parsing to semantic graphs has been extensively studied recently. At the risk of oversimplifying, work in this area can be divided into three types, according to how much structural information of a target graph is explicitly modeled. Parsers of the first type throw an 1 http://moin.delph-in.net/ErgSemantics 408 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 408–418 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Model Grammar SDG EDS DMRS Data-driven ERG-based SHRG-based NO Unification R"
P18-1038,J93-2004,0,0.0641098,"Missing"
P18-1038,P17-1076,0,0.0224556,"e the output vectors of forward and backward LSTM as fi and bi . The feature si,j of a span (i, j) can be calculated from the differences of LSTM encodings: 4.2.1 Greedy Search Model In this model, we assume that each HRG rule is selected independently of the others. The score of G is defined as the sum of all rule scores: si,j = (fj − fi ) ⊕ (bi − bj ) S CORE(R = {r1 , r2 , ...}|T ) = The operator ⊕ indicates the concatenation of two vectors. Constituency parsing can be regarded as predicting scores for spans and labels, and getting the best syntactic tree with dynamic programming. Following Stern et al. (2017)’s approach, We calculate the span scores S COREspan (i, j) and labels scores S CORElabel (i, j, l) from si,j with multilayer perceptrons (MLPs): X S CORE(r|T ) r∈R The maximization of the graph score can be decomposed into the maximization of each rule score. S CORE(r|T ) can be calculated in many ways. Count-based approach is the simplest one, where the rule score is estimated by its frequency in the training data. We also evaluate a sophisticated scoring method, i.e., training a classifier based on rule embedding: S COREspan (i, j) = MLPspan (si,j ) S CORElabel (i, j, l) = MLPlabel (si,j )["
P18-1038,J16-3001,1,0.913456,"Missing"
P18-1038,S15-2153,0,0.474919,"Missing"
P18-1038,oepen-lonning-2006-discriminant,0,0.705721,"Missing"
P18-1038,P17-1186,0,0.0801732,"Missing"
P18-1038,S16-1183,0,0.0152412,"tial scores (Flanigan et al., 2014; Cao et al., 2017). Although many parsers achieve encouraging results, they are very hard for linguists to interpret and understand, partially because they do not explicitly model the syntacto-semantic composition process which is a significant characteristic of natural languages. In theory, Synchronous Hyperedge Replacement Grammar (SHRG; Drewes et al., 1997) provides a mathematically sound framework to construct semantic graphs. In practice, however, initial results on the utility of SHRG for semantic parsing were somewhat disappointing (Peng et al., 2015; Peng and Gildea, 2016). In this paper, we show that the performance that can be achieved by an SHRG-based parser is far higher than what has previously been demonstrated. We focus here on relating SHRG rules to the syntactosemantic composition process because we feel that information about syntax-semantics interface has been underexploited in the data-driven parsing architecture. We demonstrate the feasibility of inducing a high-quality, linguistically-informed SHRG from compositional semantic annotations licensed by English Resource Grammar (ERG; Flickinger, 2000), dubbed English Resource Semantics1 (ERS). Coupled"
P18-1153,islam-inkpen-2006-second,0,0.0198284,"l is named as Highlight Model. To extract associative words of each sense of the target word, we ﬁrst build word association norms in our corpus by using pointwise mutual information (PMI). As mutual information compares the probability of observing w1 and w2 together (the joint probability) with the probabilities of observing w1 and w2 independently (chance) (Church and Hanks, 1990), positive PMI scores indicate that the words occur together more than would be expected under an independence assumption, and negative scores indicate that one word tends to appear solely when the other does not (Islam and Inkpen, 2006). In this case we take top k associative words for each sense with relatively high positive PMI scores, which are calculated as follows: P M I(w1 , w2 ) = log2 p(w1 , w2 ) . p(w1 ) · p(w2 ) (5) During decoding we increase the probability of the associative words to be chosen according to their PMI scores. For each sense of the target word, we normalize the PMI scores of the associative words as follows: (s) Asso(wt , cp ) = σ( (s) P M I(wt , cp ) (s) maxcj P M I(wt , cj ) ), (6) (s) where wt represents the s-th sense of the target word wt , and cp is the p-th associative word (s) for wt . To s"
P18-1153,D14-1179,0,0.0423769,"Missing"
P18-1153,J90-1003,0,0.435912,"tly. Based on such observations, we improve the pun generation model by adding some keywords to the sentence which could remind people some special sense of the target word. We call those keywords associative words, and the improved model is named as Highlight Model. To extract associative words of each sense of the target word, we ﬁrst build word association norms in our corpus by using pointwise mutual information (PMI). As mutual information compares the probability of observing w1 and w2 together (the joint probability) with the probabilities of observing w1 and w2 independently (chance) (Church and Hanks, 1990), positive PMI scores indicate that the words occur together more than would be expected under an independence assumption, and negative scores indicate that one word tends to appear solely when the other does not (Islam and Inkpen, 2006). In this case we take top k associative words for each sense with relatively high positive PMI scores, which are calculated as follows: P M I(w1 , w2 ) = log2 p(w1 , w2 ) . p(w1 ) · p(w2 ) (5) During decoding we increase the probability of the associative words to be chosen according to their PMI scores. For each sense of the target word, we normalize the PMI"
P18-1153,S17-2011,0,0.478531,"act word relationships in puns automatically. The system stores the extracted knowledge in template form and results in computer-generated puns. Most previous research on pun generation is based on templates which is convenient but lacks linguistic subtlety and can be inﬂexible. None of the systems aimed to be creative as the skeletons of the sentences are ﬁxed and the generation process based on lexical information rarely needs world knowledge or reasoning (Ritchie, 2004). Recently more and more work focuses on pun detection and interpretation (Miller et al., 2017; Miller and Gurevych, 2015; Doogan et al., 2017), rather than pun generation. 1651 2.2 Natural Language Generation Natural language generation is an important area of NLP and it is an essential foundation for the tasks like machine translation, dialogue response generation, summarization and of course pun generation. In the past, text generation is usually based on the techniques like templates or rules, probabilistic models like n-gram or log-linear models. Those models are fairly interpretable and wellbehaved but require infeasible amounts of handengineering to scale with the increasing training data (Xie, 2017). In most cases larger corp"
P18-1153,W17-3207,0,0.0309869,"ns “have faith or conﬁdence in”. 3.1.2 Decoding with Joint Beam Search Algorithm Beam search is a frequently-used algorithm in the decoding stage of seq2seq models to generate the output sequence. It can be viewed as an adaptation of branch-and-bound search that uses an inadmissible pruning rule. In the beam search algorithm, only the most promising nodes at each level of the search graph are selected and the rest nodes are permanently removed. This strategy makes beam search able to ﬁnd a solution within practical time or memory limits and work well in practical tasks (Zhou and Hansen, 2005; Freitag and Al-Onaizan, 2017). We also use beam search in our pun generation model. According to the deﬁnition of homographic puns, at least two senses of the target word 1653 should be interpreted in one sentence. We hope to generate a same sentence for distinct senses of the same word, and in this way the target word in the sentence can be interpreted as various senses. Provided with two senses of a target word as inputs to the encoder in the backward generation process, e.g. “countv01” as input1 and “countv08” as input2 , we decode two output sentences in parallel, and the two sentences should be the same (s) except fo"
P18-1153,W09-2004,0,0.376587,"et al., 1997) and STANDUP (Ritchie et al., 2007) introduced constructing descriptions. The Homonym Common Phrase Pun generator (Venour, 1999) could create two-utterance texts: a one-sentence set-up and a punch-line. Venour (1999) used schemata to specify the required lexical items and their intern relations, and used templates to indicate where to ﬁt the lexical items in a skeleton text (Ritchie, 2004). McKay (2002) proposed WISCRAIC program which can produce puns in three forms: question-answer form, single sentence and a two-sentence sequence. The Template-Based Pun Extractor and Generator (Hong and Ong, 2009) utilized phonetic and semantic linguistic resources to extract word relationships in puns automatically. The system stores the extracted knowledge in template form and results in computer-generated puns. Most previous research on pun generation is based on templates which is convenient but lacks linguistic subtlety and can be inﬂexible. None of the systems aimed to be creative as the skeletons of the sentences are ﬁxed and the generation process based on lexical information rarely needs world knowledge or reasoning (Ritchie, 2004). Recently more and more work focuses on pun detection and inte"
P18-1153,D17-1067,0,0.0290679,"ating puns based on the seq2seq framework as far as we know. The inherent property of humor makes the pun generation task more challenging. Despite decades devoted to theories and algorithms for humor, computerized humor still lacks of creativity, sophistication of language, world knowledge, 1650 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1650–1660 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics empathy and cognitive mechanisms compared to humans, which are extremely difﬁcult to model (Hossain et al., 2017). In this paper, we study the challenging task of generating puns with seq2seq models without using a pun corpus for training. We propose a brandnew method to generate homographic puns using normal text corpus which can result in good quality of language model and avoid considerable expense of human annotators on the limited pun resources. Our proposed method can generate puns according to the given two senses of a target word. We achieve this by ﬁrst proposing an improved language model that is able to generate a sentence containing a given word with a speciﬁc sense. Based on the improved lan"
P18-1153,S17-2072,0,0.229925,"training. We ﬁrst train a conditional neural language model from a general text corpus, and then generate puns from the language model with an elaborately designed decoding algorithm. Automatic and human evaluations show that our models are able to generate homographic puns of good readability and quality. 1 Introduction Punning is an ingenious way to make conversation enjoyable and plays important role in entertainment, advertising and literature. A pun is a means of expression, the essence of which is in the given context the word or phrase can be understood in two meanings simultaneously (Mikhalkova and Karyakin, 2017). Puns can be classiﬁed according to various standards, and the most essential distinction for our research is between homographic and homophonic puns. A homographic pun exploits distinct meanings of the same written word while a homophonic pun exploits distinct meanings of the same spoken word. Puns can be homographic, homophonic, both, or neither (Miller and Gurevych, 2015). Puns have the potential to combine novelty and familiarity appropriately, which can induce pleasing effect to advertisement (Valitutti et al., 2008). Using puns also contributes to elegancy in literary writing, as labori"
P18-1153,P15-1070,0,0.434752,"and plays important role in entertainment, advertising and literature. A pun is a means of expression, the essence of which is in the given context the word or phrase can be understood in two meanings simultaneously (Mikhalkova and Karyakin, 2017). Puns can be classiﬁed according to various standards, and the most essential distinction for our research is between homographic and homophonic puns. A homographic pun exploits distinct meanings of the same written word while a homophonic pun exploits distinct meanings of the same spoken word. Puns can be homographic, homophonic, both, or neither (Miller and Gurevych, 2015). Puns have the potential to combine novelty and familiarity appropriately, which can induce pleasing effect to advertisement (Valitutti et al., 2008). Using puns also contributes to elegancy in literary writing, as laborious manual counts revealed that puns are one of the most commonly used rhetoric of Shakespeare, with the frequency in certain of his plays ranging from 17 to 85 instances per thousand lines (Miller and Gurevych, 2015). It is not an overstatement to say that pun generation has signiﬁcance in human society. However, as a special branch of humor, generating puns is not easy for"
P18-1153,S17-2005,0,0.540805,"onetic and semantic linguistic resources to extract word relationships in puns automatically. The system stores the extracted knowledge in template form and results in computer-generated puns. Most previous research on pun generation is based on templates which is convenient but lacks linguistic subtlety and can be inﬂexible. None of the systems aimed to be creative as the skeletons of the sentences are ﬁxed and the generation process based on lexical information rarely needs world knowledge or reasoning (Ritchie, 2004). Recently more and more work focuses on pun detection and interpretation (Miller et al., 2017; Miller and Gurevych, 2015; Doogan et al., 2017), rather than pun generation. 1651 2.2 Natural Language Generation Natural language generation is an important area of NLP and it is an essential foundation for the tasks like machine translation, dialogue response generation, summarization and of course pun generation. In the past, text generation is usually based on the techniques like templates or rules, probabilistic models like n-gram or log-linear models. Those models are fairly interpretable and wellbehaved but require infeasible amounts of handengineering to scale with the increasing tra"
P18-1153,P17-1108,1,0.781095,"Missing"
P18-1179,W13-2322,0,0.0288581,"igned especially for natural language generation (NLG) from type-logical semantic graphs. Taking Elementary Dependency Structures, a format of English Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design. 1 Introduction The recent years have seen an increased interest as well as rapid progress in semantic parsing and surface realization based on graph-structured semantic representations, e.g. Abstract Meaning Representation (AMR; Banarescu et al., 2013), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006) and Depedendency-based Minimal Recursion Semantics (DMRS; Copestake, 2009). Still underexploited is a formal framework for manipulating graphs that parallels automata, tranducers or formal grammars for strings and trees. Two such formalisms have recently been proposed and applied for NLP. One is graph grammar, e.g. Hyperedge Replacement Grammar (HRG; Ehrig et al., 1999). The other is DAG automata, originally studied by Kamimura and Slutzki (1982) and extended by Chiang et al. (2018). In this paper, we study DAG transducers in dep"
P18-1179,bohnet-wanner-2010-open,0,0.0149683,"edges. In this paper, we only consider DAGs that are unordered, node-labeled, multi-rooted1 and connected. Conceptual graphs, including AMR and EDS, are both node-labeled and edge-labeled. It seems that without edge labels, a DAG is inadequate, but this problem can be solved easily by using the strategies introduced in (Chiang et al., 2018). Take BV a labeled edge proper q −→ named for example2 . We can represent the same information by replacing it with two unlabeled edges and a new labeled node: proper q → BV → named. 2.2 Previous Work DAG automata are the core engines of graph transducers (Bohnet and Wanner, 2010; Quernheim and Knight, 2012). In this work, we adopt Chiang et al. (2018)’s design and define a weighted DAG automaton as a tuple M = ⟨Σ, Q, δ, K⟩: • Σ is an alphabet of node labels. • Q is a finite set of states. • (K, ⊕, ⊗, 0, 1) is a semiring of weights. • δ : Θ → K{0} is a weight function that assigns nonzero weights to a finite transition set Θ. Every transition t ∈ Θ is of the form σ {q1 , · · · , qm } − → {r1 , · · · , rn } where qi and rj are states in Q. A transition t gets m states on the incoming edges of a node and puts n states on the outgoing edges. A transition that does not b"
P18-1179,J18-1005,0,0.362951,", e.g. Abstract Meaning Representation (AMR; Banarescu et al., 2013), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006) and Depedendency-based Minimal Recursion Semantics (DMRS; Copestake, 2009). Still underexploited is a formal framework for manipulating graphs that parallels automata, tranducers or formal grammars for strings and trees. Two such formalisms have recently been proposed and applied for NLP. One is graph grammar, e.g. Hyperedge Replacement Grammar (HRG; Ehrig et al., 1999). The other is DAG automata, originally studied by Kamimura and Slutzki (1982) and extended by Chiang et al. (2018). In this paper, we study DAG transducers in depth, with the goal of building accurate, efficient yet robust natural language generation (NLG) systems. The meaning representation studied in this work is what we call type-logical semantic graphs, i.e. semantic graphs grounded under type-logical semantics (Carpenter, 1997), one dominant theoretical framework for modeling natural language semantics. In this framework, adjuncts, such as adjective and adverbal phrases, are analyzed as (higher-order) functors, the function of which is to consume complex arguments (Kratzer and Heim, 1998). In the sam"
P18-1179,P97-1003,0,0.0227035,"namic rule on-the-fly. Our rule creator builds a new rule following the Markov assumption: P (O|C) = P (q1 |C) n ∏ P (qi |C)P (qi |qi−1 , C) i=2 C = ⟨I, D⟩ represents the context. O = {q1 , · · · , qn } denotes the outgoing states and I, D have the same meaning as before. Though they are unordered multisets, we can give them an explicit alphabet order by their edge labels. There is also a group of hard constraints to make sure that the predicted rules are well-formed as the definition in §5 requires. This Markovization strategy is widely utilized by lexicalized and unlexicalized PCFG parsers (Collins, 1997; Klein and Manning, 2003). For a dynamic rule, all variables in this rule will appear in the statement. We use a simple perceptron-based scorer to assign every variable a score and arrange them in an decreasing order. 6 Evaluation and Analysis 5.4 Extended Rules 6.1 Set-up Extended rules are used when no induced rules can cover a given node. In theory, there can be unlimited modifier nodes pointing to a given node, such as PP and ADJ. We use some manually written rules to slightly change an induced rule (prototype) by addition or deletion to generate a group of extended rules. The motivation"
P18-1179,W02-1001,0,0.0482386,"1,758 and 1,444 sentences (all disconnected graphs as well as their associated sentences are removed) in the training, development and test data sets. We use a small portion of wikiwoods data, c.a. 300K sentences, for experiments. 37,537 induced rules are directly extracted from the training data set, and 447,602 extended rules are obtained. For DAG recognition, at one particular position, there may be more than one rule applicable. In this case, we need a disambiguation model as well as a decoder to search for a globally optimal solution. In this work, we train a structured perceptron model (Collins, 2002) for disambiguation and employ a beam decoder. The perceptron model used by our dynamic rule generator are trained with the induced rules. To get a sequence-to-sequence model, we use the open source tool—OpenNMT4 . 6.2 The Decoder We implement a fine-to-coarse beam search decoder. Given a DAG D, our goal is to find the highest scored labeling function ρ: ρ = arg max ρ n ∑ ∏ wj · fj (rule(vi ), D) i=1 j ℓ(vi ) s.t. rule(vi ) = ρ(in(vi )) −−−→ ⟨ρ(out(vi )), Ei ⟩ where n is the node count and fj (·, ·) and wj represent a feature and the corresponding weight, respectively. The features are chosen"
P18-1179,P03-1054,0,0.00869505,"he-fly. Our rule creator builds a new rule following the Markov assumption: P (O|C) = P (q1 |C) n ∏ P (qi |C)P (qi |qi−1 , C) i=2 C = ⟨I, D⟩ represents the context. O = {q1 , · · · , qn } denotes the outgoing states and I, D have the same meaning as before. Though they are unordered multisets, we can give them an explicit alphabet order by their edge labels. There is also a group of hard constraints to make sure that the predicted rules are well-formed as the definition in §5 requires. This Markovization strategy is widely utilized by lexicalized and unlexicalized PCFG parsers (Collins, 1997; Klein and Manning, 2003). For a dynamic rule, all variables in this rule will appear in the statement. We use a simple perceptron-based scorer to assign every variable a score and arrange them in an decreasing order. 6 Evaluation and Analysis 5.4 Extended Rules 6.1 Set-up Extended rules are used when no induced rules can cover a given node. In theory, there can be unlimited modifier nodes pointing to a given node, such as PP and ADJ. We use some manually written rules to slightly change an induced rule (prototype) by addition or deletion to generate a group of extended rules. The motivation here is to deal with the d"
P18-1179,P17-1014,0,0.112577,"Missing"
P18-1179,J93-2004,0,0.0652256,"ons licensed by English Resource Grammar (ERG; Flickinger, 2000). We introduce a principled method to derive transduction rules from DeepBank (Flickinger et al., 2012). Furthermore, we introduce a fine-to-coarse strategy to ensure that at least one sentence is generated for any input graph. Taking EDS graphs, a variable-free ERS format, as input, our NLG system achieves a BLEU-4 score of 68.07. On average, it produces more than 5 sentences in a second on an x86 64 GNU/Linux platform with two Intel Xeon E5-2620 CPUs. Since the data for experiments is newswire data, i.e. WSJ sentences from PTB (Marcus et al., 1993), the input graphs are quite large on average. The remarkable accuracy, efficiency and robustness demonstrate the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our transducer design. 2 Previous Work and Challenges 2.1 Preliminaries A node-labeled simple graph over alphabet Σ is a triple G = (V, E, ℓ), where V is a finite set of nodes, E ⊆ V × V is an finite set of edges and ℓ : V → Σ is a labeling function. For a node v ∈ V , sets of its incoming and outgoing edges are denoted by in(v) and out(v) respectively. For an edge e ∈ E, its source node and ta"
P18-1179,E09-1001,0,0.401001,"lish Resource Semantics, as input, our NLG system achieves a BLEU-4 score of 68.07. This remarkable result demonstrates the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our design. 1 Introduction The recent years have seen an increased interest as well as rapid progress in semantic parsing and surface realization based on graph-structured semantic representations, e.g. Abstract Meaning Representation (AMR; Banarescu et al., 2013), Elementary Dependency Structure (EDS; Oepen and Lønning, 2006) and Depedendency-based Minimal Recursion Semantics (DMRS; Copestake, 2009). Still underexploited is a formal framework for manipulating graphs that parallels automata, tranducers or formal grammars for strings and trees. Two such formalisms have recently been proposed and applied for NLP. One is graph grammar, e.g. Hyperedge Replacement Grammar (HRG; Ehrig et al., 1999). The other is DAG automata, originally studied by Kamimura and Slutzki (1982) and extended by Chiang et al. (2018). In this paper, we study DAG transducers in depth, with the goal of building accurate, efficient yet robust natural language generation (NLG) systems. The meaning representation studied"
P18-1179,W12-4209,0,0.435466,"function of which is to consume complex arguments (Kratzer and Heim, 1998). In the same spirit, generalized quantifiers, prepositions and function words in many languages other than English are also analyzed as higher-order functions. Accordingly, all the linguistic elements are treated as roots in type-logical semantic graphs, such as EDS and DMRS. This makes the typological structure quite flat rather than hierachical, which is an essential distinction between natural language semantics and syntax. To the best of our knowledge, the only existing DAG transducer for NLG is the one proposed by Quernheim and Knight (2012). Quernheim and Knight introduced a DAG-to-tree transducer that can be applied to AMR-to-text generation. This transducer is designed to handle hierarchical structures with limited reentrencies, and it is unsuitable for meaning graphs transformed from type-logical semantics. Furthermore, Quernheim and Knight did not describe how to acquire graph recognition and transduction rules from linguistic data, and reported no result of practical generation. It is still unknown to what extent a DAG transducer suits realistic NLG. The design for string and tree transducers 1928 Proceedings of the 56th An"
P18-1179,P17-2002,0,0.0540524,"Missing"
P18-1179,flickinger-etal-2010-wikiwoods,0,0.0158063,"ed Rules 6.1 Set-up Extended rules are used when no induced rules can cover a given node. In theory, there can be unlimited modifier nodes pointing to a given node, such as PP and ADJ. We use some manually written rules to slightly change an induced rule (prototype) by addition or deletion to generate a group of extended rules. The motivation here is to deal with the data sparseness problem. We use DeepBank 1.1 (Flickinger et al., 2012), i.e. gold-standard ERS annotations, as our main experimental data set to train a DAG transducer as well as a sequence-to-sequence morpholyzer, and wikiwoods (Flickinger et al., 2010), i.e. automatically-generated ERS annotations by ERG, as additional data set to enhance the sequence-to-sequence morpholyzer. The training, 1934 development and test data sets are from DeepBank and split according to DeepBank’s recommendation. There are 34,505, 1,758 and 1,444 sentences (all disconnected graphs as well as their associated sentences are removed) in the training, development and test data sets. We use a small portion of wikiwoods data, c.a. 300K sentences, for experiments. 37,537 induced rules are directly extracted from the training data set, and 447,602 extended rules are obt"
P18-1250,P11-2037,0,0.163599,"terest in automatic empty category detection (ECD; Johnson, 2002; Seeker et al., 2012; Xue and Yang, 2013; Wang et al., 2015). And it has been shown that ECD is able to improve the linear model-based dependency parsing (Zhang et al., 2017b). There are two key dimensions of approaches for ECD: the relationship with parsing and statistical disambiguation. Considering the relationship with parsing, we can divide ECD models into three types: (1) Pre-parsing approach (e.g. Dienes and Dubey (2003)) where empty categories are identified without using syntactic analysis, (2) In-parsing approach (e.g. Cai et al. (2011); Zhang et al. (2017b)) where detection is integrated into a parsing model, and (3) Post-parsing approach (e.g. Johnson (2002); Wang et al. (2015)) where parser outputs are utilized as clues to determine the existence of empty categories. For disambiguation, while early work on dependency parsing focused on linear models, recent work started exploring deep learning techniques for the post-parsing approach (Wang et al., 2015). From the above two dimensions, we show all existing systems for ECD in Table 1. Neural models for pre- and in-parsing ECD have not been studied yet. In this paper, we fil"
P18-1250,P03-1055,0,0.345223,"s constituents, and certain dropped elements (Marcus et al., 1993; Xue et al., 2005). Recently, there has been an increasing interest in automatic empty category detection (ECD; Johnson, 2002; Seeker et al., 2012; Xue and Yang, 2013; Wang et al., 2015). And it has been shown that ECD is able to improve the linear model-based dependency parsing (Zhang et al., 2017b). There are two key dimensions of approaches for ECD: the relationship with parsing and statistical disambiguation. Considering the relationship with parsing, we can divide ECD models into three types: (1) Pre-parsing approach (e.g. Dienes and Dubey (2003)) where empty categories are identified without using syntactic analysis, (2) In-parsing approach (e.g. Cai et al. (2011); Zhang et al. (2017b)) where detection is integrated into a parsing model, and (3) Post-parsing approach (e.g. Johnson (2002); Wang et al. (2015)) where parser outputs are utilized as clues to determine the existence of empty categories. For disambiguation, while early work on dependency parsing focused on linear models, recent work started exploring deep learning techniques for the post-parsing approach (Wang et al., 2015). From the above two dimensions, we show all existi"
P18-1250,P02-1018,0,0.234613,"of machinery in representing the (deep) syntactic structure of a sentence (Carnie, 2012). Figure 1 shows an example. In linguistic theory, e.g. Government and Binding (GB; Chomsky, 1981), empty category is a key concept bridging S-Structure and D-Structure, due to its possible contribution to trace movements. In practical treebanking, empty categories have been used to indicate long-distance dependencies, discontinuous constituents, and certain dropped elements (Marcus et al., 1993; Xue et al., 2005). Recently, there has been an increasing interest in automatic empty category detection (ECD; Johnson, 2002; Seeker et al., 2012; Xue and Yang, 2013; Wang et al., 2015). And it has been shown that ECD is able to improve the linear model-based dependency parsing (Zhang et al., 2017b). There are two key dimensions of approaches for ECD: the relationship with parsing and statistical disambiguation. Considering the relationship with parsing, we can divide ECD models into three types: (1) Pre-parsing approach (e.g. Dienes and Dubey (2003)) where empty categories are identified without using syntactic analysis, (2) In-parsing approach (e.g. Cai et al. (2011); Zhang et al. (2017b)) where detection is inte"
P18-1250,N13-1125,0,0.0398982,"is 3. We calculate labeled recall scores for enumerated Dependency Distance. A higher score means greater capability to catch and to represent long-distance details. 4 4.2 Results of Pre-Parsing Models Experiments 4.1 Experimental Setup 4.1.1 Data We conduct experiments on a subset of Penn Chinese Treebank (CTB; Xue et al., 2005) 9.0. As a pro-drop language, the empty category is a very useful method for representing the (deep) syntactic analysis in Chinese language. Empty categories in CTB is divided into six classes: pro, PRO, OP, T, RNR and *, which were described in detail in Xue and Yang (2013); Wang et al. (2015). For comparability with the state-of-the-art, the division of training, development and testing data is coincident with the previous work (Xue and Yang, 2013). Our experiments can be divided into two groups. The first group is conducted on the linear conditional random field (Linear-CRF) model and LSTM-CRF tagging model to evaluate gains from the introduction of neural structures. The second group is designed for the dependency-based inparsing models. 4.1.2 Evaluation Metrics We adopt two kinds of metrics for the evaluation of our experiments. The first one focuses on EC’s"
P18-1250,E17-1063,0,0.0594007,"Missing"
P18-1250,C12-2105,0,0.0136955,"n representing the (deep) syntactic structure of a sentence (Carnie, 2012). Figure 1 shows an example. In linguistic theory, e.g. Government and Binding (GB; Chomsky, 1981), empty category is a key concept bridging S-Structure and D-Structure, due to its possible contribution to trace movements. In practical treebanking, empty categories have been used to indicate long-distance dependencies, discontinuous constituents, and certain dropped elements (Marcus et al., 1993; Xue et al., 2005). Recently, there has been an increasing interest in automatic empty category detection (ECD; Johnson, 2002; Seeker et al., 2012; Xue and Yang, 2013; Wang et al., 2015). And it has been shown that ECD is able to improve the linear model-based dependency parsing (Zhang et al., 2017b). There are two key dimensions of approaches for ECD: the relationship with parsing and statistical disambiguation. Considering the relationship with parsing, we can divide ECD models into three types: (1) Pre-parsing approach (e.g. Dienes and Dubey (2003)) where empty categories are identified without using syntactic analysis, (2) In-parsing approach (e.g. Cai et al. (2011); Zhang et al. (2017b)) where detection is integrated into a parsing"
P18-1250,N15-1030,0,0.138237,"ture of a sentence (Carnie, 2012). Figure 1 shows an example. In linguistic theory, e.g. Government and Binding (GB; Chomsky, 1981), empty category is a key concept bridging S-Structure and D-Structure, due to its possible contribution to trace movements. In practical treebanking, empty categories have been used to indicate long-distance dependencies, discontinuous constituents, and certain dropped elements (Marcus et al., 1993; Xue et al., 2005). Recently, there has been an increasing interest in automatic empty category detection (ECD; Johnson, 2002; Seeker et al., 2012; Xue and Yang, 2013; Wang et al., 2015). And it has been shown that ECD is able to improve the linear model-based dependency parsing (Zhang et al., 2017b). There are two key dimensions of approaches for ECD: the relationship with parsing and statistical disambiguation. Considering the relationship with parsing, we can divide ECD models into three types: (1) Pre-parsing approach (e.g. Dienes and Dubey (2003)) where empty categories are identified without using syntactic analysis, (2) In-parsing approach (e.g. Cai et al. (2011); Zhang et al. (2017b)) where detection is integrated into a parsing model, and (3) Post-parsing approach (e"
P18-2087,S17-2005,0,0.449106,"e exists a class of language constructs known as puns in natural language utterances and texts, and the speaker or writer intends for a certain word or other lexical item to be interpreted as simultaneously carrying two or more separate meanings. Though puns are an important feature in many discourse types, they have attracted relatively little attention in the area of natural language processing. A pun is a form of wordplay in which a word suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another word, for an intended humorous or rhetorical effect (Miller et al., 2017). Puns where the two meanings share the same pronunciation are known as homographic puns, which are the focus of this study. For example, the following punning joke exploits contrasting meanings of the word “interest” and it is a homographic pun. • We propose a novel sense-aware neural model to address the pun location task. • Our proposed model outperforms several baseline neural models and achieves the state-of-theart performance. 1 http://alt.qcri.org/semeval2017/task7/ 546 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 546–551"
P18-2087,S17-2070,0,0.212251,"l baseline neural models and achieves the state-of-theart performance. 1 http://alt.qcri.org/semeval2017/task7/ 546 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 546–551 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work been successfully applied to WSD (Yuan et al., 2016; Raganato et al., 2017). Pun detection aims to determine whether a given short text contains a pun (Miller et al., 2017). Various methods have been proposed to address this task, including WSD based methods (Pedersen, 2017), PMI-based methods (Sevgili et al., 2017) supervised methods (Xiu et al., 2017; Indurthi and Oota, 2017; Pramanick and Das, 2017; Mikhalkova and Karyakin, 2017; Vadehra, 2017). More specifically, the bi-directional RNN has been used in (Indurthi and Oota, 2017), and vote-based ensemble classifier is used by (Vadehra, 2017). Pun location is a more challenging task than pun detection, because it aims to find the actual pun word in the given text. Previous works find some clues about puns in the texts. For example, pun is more likely appeared towards the end of sentences (Pedersen, 2017; Miller"
P18-2087,S17-2011,0,0.20258,"a given text. The task of identifying the pun word is known as pun location, which is defined in SemEval 2017 Task 71 . In order to address this special task, various approaches have been attempted, including rule based approach (Vechtomova, 2017), knowledgebased approach (Indurthi and Oota, 2017; Xiu et al., 2017) and supervised approach (Pramanick and Das, 2017; Mikhalkova and Karyakin, 2017). However, these approaches do not achieve good results, and the best F1 score for homographic pun location is just 0.6631, which is achieved by the Idiom Savant system with a knowledge based approach (Doogan et al., 2017). The results demonstrate that pun location is a very challenging task. In order to address this challenging task and improve the state-of-the-art results, we propose a sense-aware neural model in this study. Our model first obtains several WSD (Word Sense Disambiguation) results for the text, and leverages a bidirectional LSTM network to model each sequence of word senses. The outputs at each time step for different LSTM networks are then concatenated for pun word prediction. Evaluation results of cross-validation on the benchmark SemEval 2017 dataset demonstrate the efficacy of our proposed"
P18-2087,W16-1620,0,0.0319296,"(j = 1, ..., K) by K different bi-directional LSTM networks for the same i-th word (i.e., the i-th time step) are then concatenated into one vector, and the vector is sent to a two-layer feed-forward neural network and a sigmoid function for prediction. The loss function is the same as that of the baseline model. Our sense-aware model can be considered as applying the baseline model on different WSD results and then combining the outputs for prediction. Word Sense Disambiguation and Sense Embedding In order to obtain sense inventory and the sense embeddings for each word, we choose SenseGram (Pelevina et al., 2016). The SenseGram toolkit is available online2 , and it can take as an input the word embeddings and split different senses of the input words. For instance, the vector for the word “table” will be split into “table (data)” and “table (furniture)”. SenseGram induces sense inventory from existing word embeddings via clustering of ego-networks of related words. In our work, the Wikipedia corpus is used to train word embeddings (together with contextual embeddings of words) by using word2vec and then the word embeddings are used by SenseGram for inducing sense inventory and sense embeddings. The wo"
P18-2087,S17-2073,0,0.237206,"putational Linguistics, Peking University {caiyitao,xyz1305121,wanxiaojun}@pku.edu.cn Abstract I used to be a banker but I lost interest. Since the pun word plays the key role in forming a pun, it is very important and meaningful to identify the pun word in a given text. The task of identifying the pun word is known as pun location, which is defined in SemEval 2017 Task 71 . In order to address this special task, various approaches have been attempted, including rule based approach (Vechtomova, 2017), knowledgebased approach (Indurthi and Oota, 2017; Xiu et al., 2017) and supervised approach (Pramanick and Das, 2017; Mikhalkova and Karyakin, 2017). However, these approaches do not achieve good results, and the best F1 score for homographic pun location is just 0.6631, which is achieved by the Idiom Savant system with a knowledge based approach (Doogan et al., 2017). The results demonstrate that pun location is a very challenging task. In order to address this challenging task and improve the state-of-the-art results, we propose a sense-aware neural model in this study. Our model first obtains several WSD (Word Sense Disambiguation) results for the text, and leverages a bidirectional LSTM network to model"
P18-2087,S17-2079,0,0.0975154,"Missing"
P18-2087,D17-1120,0,0.0257534,"joke exploits contrasting meanings of the word “interest” and it is a homographic pun. • We propose a novel sense-aware neural model to address the pun location task. • Our proposed model outperforms several baseline neural models and achieves the state-of-theart performance. 1 http://alt.qcri.org/semeval2017/task7/ 546 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 546–551 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work been successfully applied to WSD (Yuan et al., 2016; Raganato et al., 2017). Pun detection aims to determine whether a given short text contains a pun (Miller et al., 2017). Various methods have been proposed to address this task, including WSD based methods (Pedersen, 2017), PMI-based methods (Sevgili et al., 2017) supervised methods (Xiu et al., 2017; Indurthi and Oota, 2017; Pramanick and Das, 2017; Mikhalkova and Karyakin, 2017; Vadehra, 2017). More specifically, the bi-directional RNN has been used in (Indurthi and Oota, 2017), and vote-based ensemble classifier is used by (Vadehra, 2017). Pun location is a more challenging task than pun detection, because it ai"
P18-2087,S17-2074,0,0.193469,"s the state-of-theart performance. 1 http://alt.qcri.org/semeval2017/task7/ 546 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 546–551 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work been successfully applied to WSD (Yuan et al., 2016; Raganato et al., 2017). Pun detection aims to determine whether a given short text contains a pun (Miller et al., 2017). Various methods have been proposed to address this task, including WSD based methods (Pedersen, 2017), PMI-based methods (Sevgili et al., 2017) supervised methods (Xiu et al., 2017; Indurthi and Oota, 2017; Pramanick and Das, 2017; Mikhalkova and Karyakin, 2017; Vadehra, 2017). More specifically, the bi-directional RNN has been used in (Indurthi and Oota, 2017), and vote-based ensemble classifier is used by (Vadehra, 2017). Pun location is a more challenging task than pun detection, because it aims to find the actual pun word in the given text. Previous works find some clues about puns in the texts. For example, pun is more likely appeared towards the end of sentences (Pedersen, 2017; Miller and Turkovi´c, 2016). Many puns have a par"
P18-2087,S17-2072,0,0.270008,"eking University {caiyitao,xyz1305121,wanxiaojun}@pku.edu.cn Abstract I used to be a banker but I lost interest. Since the pun word plays the key role in forming a pun, it is very important and meaningful to identify the pun word in a given text. The task of identifying the pun word is known as pun location, which is defined in SemEval 2017 Task 71 . In order to address this special task, various approaches have been attempted, including rule based approach (Vechtomova, 2017), knowledgebased approach (Indurthi and Oota, 2017; Xiu et al., 2017) and supervised approach (Pramanick and Das, 2017; Mikhalkova and Karyakin, 2017). However, these approaches do not achieve good results, and the best F1 score for homographic pun location is just 0.6631, which is achieved by the Idiom Savant system with a knowledge based approach (Doogan et al., 2017). The results demonstrate that pun location is a very challenging task. In order to address this challenging task and improve the state-of-the-art results, we propose a sense-aware neural model in this study. Our model first obtains several WSD (Word Sense Disambiguation) results for the text, and leverages a bidirectional LSTM network to model each sequence of word senses. T"
P18-2087,S17-2077,0,0.239911,"Computational Linguistics (Short Papers), pages 546–551 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 2 Related Work been successfully applied to WSD (Yuan et al., 2016; Raganato et al., 2017). Pun detection aims to determine whether a given short text contains a pun (Miller et al., 2017). Various methods have been proposed to address this task, including WSD based methods (Pedersen, 2017), PMI-based methods (Sevgili et al., 2017) supervised methods (Xiu et al., 2017; Indurthi and Oota, 2017; Pramanick and Das, 2017; Mikhalkova and Karyakin, 2017; Vadehra, 2017). More specifically, the bi-directional RNN has been used in (Indurthi and Oota, 2017), and vote-based ensemble classifier is used by (Vadehra, 2017). Pun location is a more challenging task than pun detection, because it aims to find the actual pun word in the given text. Previous works find some clues about puns in the texts. For example, pun is more likely appeared towards the end of sentences (Pedersen, 2017; Miller and Turkovi´c, 2016). Many puns have a particularly strong associations with other words in the contexts (Sevgili et al., 2017). A variety of methods have been proposed to loca"
P18-2087,S17-2071,0,0.243803,"Li and Xiaojun Wan Institute of Computer Science and Technology, Peking University The MOE Key Laboratory of Computational Linguistics, Peking University {caiyitao,xyz1305121,wanxiaojun}@pku.edu.cn Abstract I used to be a banker but I lost interest. Since the pun word plays the key role in forming a pun, it is very important and meaningful to identify the pun word in a given text. The task of identifying the pun word is known as pun location, which is defined in SemEval 2017 Task 71 . In order to address this special task, various approaches have been attempted, including rule based approach (Vechtomova, 2017), knowledgebased approach (Indurthi and Oota, 2017; Xiu et al., 2017) and supervised approach (Pramanick and Das, 2017; Mikhalkova and Karyakin, 2017). However, these approaches do not achieve good results, and the best F1 score for homographic pun location is just 0.6631, which is achieved by the Idiom Savant system with a knowledge based approach (Doogan et al., 2017). The results demonstrate that pun location is a very challenging task. In order to address this challenging task and improve the state-of-the-art results, we propose a sense-aware neural model in this study. Our model first obt"
P18-2087,S17-2078,0,0.407045,"ng University The MOE Key Laboratory of Computational Linguistics, Peking University {caiyitao,xyz1305121,wanxiaojun}@pku.edu.cn Abstract I used to be a banker but I lost interest. Since the pun word plays the key role in forming a pun, it is very important and meaningful to identify the pun word in a given text. The task of identifying the pun word is known as pun location, which is defined in SemEval 2017 Task 71 . In order to address this special task, various approaches have been attempted, including rule based approach (Vechtomova, 2017), knowledgebased approach (Indurthi and Oota, 2017; Xiu et al., 2017) and supervised approach (Pramanick and Das, 2017; Mikhalkova and Karyakin, 2017). However, these approaches do not achieve good results, and the best F1 score for homographic pun location is just 0.6631, which is achieved by the Idiom Savant system with a knowledge based approach (Doogan et al., 2017). The results demonstrate that pun location is a very challenging task. In order to address this challenging task and improve the state-of-the-art results, we propose a sense-aware neural model in this study. Our model first obtains several WSD (Word Sense Disambiguation) results for the text, an"
P19-1239,K16-1017,0,0.142622,") propose a model with CNN and RNN layers. Besides the tweet content in question, contextual features such as historical behaviors of the author and the audience serve as a good indicator for 1 https://github.com/headacheboy/data-of-multimodalsarcasm-detection sarcasm. Bamman and Smith (2015) make use of human-engineered author, audience and response features to promote sarcasm detection. Zhang, Zhang and Fu (2016) concatenate target tweet embeddings(obtained by a Bi-GRU model) with manually engineered contextual features, and show fair improvement compared to completely featurebased systems. Amir et al. (2016) exploit trainable user embeddings to enhance the performance of a CNN classification model. Poria et al. (2016) use the concatenated output of CNNs trained on tweets and pre-trained on emotion, sentiment, personality as the inputs for the final SVM classifier. Y. Tay et al. (2018) come up with a novel multi-dimensional intra-attention mechanism to explicitly model contrast and incongruity. Wu et al. (2018) construct a multi-task model with densely connected LSTM based on embeddings, sentiment features and syntactic features. Baziotis et al. (2018) ensemble a word based bidirectional LSTM and"
P19-1239,W10-2914,0,0.340174,"than na¨ıve concatenation, for Twitter sarcasm detection. • We create a new dataset for multi-modal Twitter sarcasm detection and release it1 . • We quantitatively show the significance of each modality in Twitter sarcasm detection. We further show that to fully unleash the potential of images, we would need to consider image attributes - a high-level abstract information bridging the gap between texts and images. 2 2.1 Related Works Sarcasm Detection Various methods have been proposed for sarcasm detection from texts. Earlier methods extract carefully engineered discrete features from texts (Davidov et al., 2010; Riloff et al., 2013; Pt´acˇ ek et al., 2014; Bouazizi and Ohtsuki, 2015), including n-grams, word’s sentiment, punctuations, emoticons, part-of-speech tags, etc. More recently, researchers leverage the powerful techniques of deep learning to get more precise semantic representations of tweet texts. Ghosh and Veale (2016) propose a model with CNN and RNN layers. Besides the tweet content in question, contextual features such as historical behaviors of the author and the audience serve as a good indicator for 1 https://github.com/headacheboy/data-of-multimodalsarcasm-detection sarcasm. Bamman"
P19-1239,W16-0425,0,0.0511708,"ibutes - a high-level abstract information bridging the gap between texts and images. 2 2.1 Related Works Sarcasm Detection Various methods have been proposed for sarcasm detection from texts. Earlier methods extract carefully engineered discrete features from texts (Davidov et al., 2010; Riloff et al., 2013; Pt´acˇ ek et al., 2014; Bouazizi and Ohtsuki, 2015), including n-grams, word’s sentiment, punctuations, emoticons, part-of-speech tags, etc. More recently, researchers leverage the powerful techniques of deep learning to get more precise semantic representations of tweet texts. Ghosh and Veale (2016) propose a model with CNN and RNN layers. Besides the tweet content in question, contextual features such as historical behaviors of the author and the audience serve as a good indicator for 1 https://github.com/headacheboy/data-of-multimodalsarcasm-detection sarcasm. Bamman and Smith (2015) make use of human-engineered author, audience and response features to promote sarcasm detection. Zhang, Zhang and Fu (2016) concatenate target tweet embeddings(obtained by a Bi-GRU model) with manually engineered contextual features, and show fair improvement compared to completely featurebased systems. A"
P19-1239,C18-1201,0,0.09776,"modalities. 2.2 Other Multi-Modal Tasks Sentiment analysis is a related task with sarcasm detection. Many researches on multi-modal sen2507 timent analysis deal with video data (Wang et al., 2016; Zadeh et al., 2017), where text, image and audio data can usually be aligned and support each other. Though inputs are different, their fusion mechanisms can be inspiring to our task. Poria, Cambria, and Gelbukh (2015) use multiple kernel learning to fuse different modalities. Zadeh et al. (2017) build their fusion layer by outer product instead of simple concatenation in order to get more features. Gu et al. (2018b) align text and audio at word level and apply several attention mechanisms. Gu et al. (2018a) first introduce modality fusion structure attempting to reveal the actual importance of multiple modalities, but their methods are quite different from our hierarchical fusion techniques. Inspiration can also be drawn from other multimodal tasks, such as visual question answering (VQA) tasks where a frame of image and a query sentence are provided as model inputs. A question-guided attention mechanism is proposed in VQA tasks (Chen et al., 2015) and can boost model performance compared to those usin"
P19-1239,D14-1181,0,0.0052893,"proposed model. We implement models with one or multiple modalities as baseline models. We also present the results of na¨ıve solution (all negative, random) of this task. Random. It randomly predicts whether a tweet is sarcastic or not. Text(Bi-LSTM). Bi-LSTM is one of the most popular method for addressing many text classification problems. It leverages a bidirectional LSTM network for learning text representations and then uses a classification layer to make prediction. Text(CNN). CNN is also one of the state-of-theart methods to address text classification problems. We implement text CNN (Kim, 2014) as a baseline model. 2511 Model F-score All negative Random Text(Bi-LSTM) Text(CNN) Image Attr Concat(2) Concat(3) Our model 0.4470 0.7753 0.7532 0.6153 0.6334 0.7799 0.7874 0.8018 Pre 0.4005 0.7666 0.7429 0.5441 0.5606 0.7388 0.7336 0.7657 Rec 0.5057 0.7842 0.7639 0.7080 0.7278 0.8259 0.8498 0.8415 Acc + 0.6019 0.5027 0.8190 0.8003 0.6476 0.6646 0.8103 0.8174 0.8344 t t− p Concat(3) Concat(2) Text(Bi-LSTM) 106 65 0.0011 149 91 0.0001 120 83 0.0057 Table 4: Statistics of sign tests. (t+ is the number of tweets that our proposed model predicts them right but baseline models do not. t− is the n"
P19-1239,D14-1162,0,0.0807245,"Missing"
P19-1239,D15-1303,0,0.0843423,"Missing"
P19-1239,C16-1151,0,0.0847188,"istorical behaviors of the author and the audience serve as a good indicator for 1 https://github.com/headacheboy/data-of-multimodalsarcasm-detection sarcasm. Bamman and Smith (2015) make use of human-engineered author, audience and response features to promote sarcasm detection. Zhang, Zhang and Fu (2016) concatenate target tweet embeddings(obtained by a Bi-GRU model) with manually engineered contextual features, and show fair improvement compared to completely featurebased systems. Amir et al. (2016) exploit trainable user embeddings to enhance the performance of a CNN classification model. Poria et al. (2016) use the concatenated output of CNNs trained on tweets and pre-trained on emotion, sentiment, personality as the inputs for the final SVM classifier. Y. Tay et al. (2018) come up with a novel multi-dimensional intra-attention mechanism to explicitly model contrast and incongruity. Wu et al. (2018) construct a multi-task model with densely connected LSTM based on embeddings, sentiment features and syntactic features. Baziotis et al. (2018) ensemble a word based bidirectional LSTM and a character based bidirectional LSTM to capture both semantic and syntactic features. However, little has been r"
P19-1239,D13-1066,0,0.433983,"Missing"
P19-1239,S18-1006,0,0.0451254,"Missing"
P19-1239,C16-1231,0,0.282584,"Missing"
P19-1497,P15-1086,0,0.0151753,"ing domain-specific. To this end, we first analyze how language use affects the number of answers, and then build evaluation models based on these conclusions. There are some researches (Guerini et al., 2011; Danescu-Niculescu-Mizil et al., 2012; Guerini et al., 2012; Tan et al., 2014) about how language use affects the reaction that a piece of text generates, but we are the first to focus on questions as far as we know. 2.2 Question Generation QG was traditionally tackled by rule-based approaches (Heilman and Smith, 2010; Lindberg et al., 2013; Mazidi and Nielsen, 2014; Hussein et al., 2014; Labutov et al., 2015). In recent years, neural network (NN) approaches have taken the mainstream. Du et al. (2017) pioneered NN-based QG by using Seq2seq models (Sutskever et al., 2014). Many researches have tried to make it more suitable for QG tasks since then, including using answer position features (Zhou et al., 5033 2017), pointer mechanism (Kumar et al., 2018a; Zhao et al., 2018), etc. Adding more constraints, e.g. controlling the topic (Hu et al., 2018) and difficulty (Gao et al., 2018) of QG, or combining it with QA (Duan et al., 2017; Wang et al., 2017; Tang et al., 2017) have also been studied. Recently"
P19-1497,W07-0734,0,0.010009,"use affects the number of answers a question receives, and draw some interesting conclusions for linguisticbased question evaluation. • We propose a model based on CGAN and our question evaluation model, which outperforms commonly-used text generation models in the quality of generated questions. In this paper, the two datasets OQRanD and OQGend are available at https://github. com/ChaiZ-pku/OQRanD-and-OQGenD. 2 2.1 Related Work Question Evaluation Question evaluation is a rather challenging task. Automatic evaluation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) were widely used to measure n-gram overlaps between generated questions and ground truth questions, however, they are far from enough since we cannot list all possible ground truth questions in openQG. To this end, we need to develop specific evaluation metrics for questions. Some researches (Heilman and Smith, 2010; Figueroa and Neumann, 2013) directly trained question ranking (QR) models via supervised learning, and used it to perform evaluation. However, these models are always domainspecific and not interpretable since we cannot tell what makes a question get a high (low) score. Rao and D"
P19-1497,D17-1230,0,0.0382854,"rs (X, Y ) from OQGenD. (2) News and questions generated by Gθ , i.e. (X, Yˆ ). (3) Unmatched NQ-pairs created from OQGenD. We label “real data” (1) as “1”; and regard both (2), (3) as “fake data” with label “0”. It is worth mentioning that the unmatched NQ-pairs are used to keep Dφ from only focusing on the questions. To train Dφ , we minimize the objective function: φ) = −E(X,Y )∼Preal data log Dφ (X, Y ) JD (φ −E(X,Y )∼Pfake data log(1 − Dφ (X, Y )) (6) Since text-generation is a discrete process, we cannot directly use Dφ (X, Yˆ ) to update θ in Gθ . A commonly-used idea (Yu et al., 2017; Li et al., 2017) is to train Gθ based on policy gradient (Sutton et al., 2000). In this case, Gθ is regarded as a policy network. At time-step t, state st is the generated text Yˆ[1:t] , and action at is generating the next word yˆt+1 with a probability πG (at |st ) = pG (ˆ yt+1 |Yˆ[1:t] , X). To get reward rt , we perform Monte-Carlo search, i.e. sample Yˆ[1:t] into a complete sentence YˆM C for k times, and perform: rt = k 1X (i) Dφ (YˆM C , X) k (7) i=1 After getting rt , θ is updated by minimizing X rt · log π(at |st )] (8) JG (θθ ) = −E[ t We can also change Eq 8 into a penalty-based version: X 0 JG (θθ"
P19-1497,W13-2114,0,0.0336457,"estion evaluation metric should be interpretable and keeps away from being domain-specific. To this end, we first analyze how language use affects the number of answers, and then build evaluation models based on these conclusions. There are some researches (Guerini et al., 2011; Danescu-Niculescu-Mizil et al., 2012; Guerini et al., 2012; Tan et al., 2014) about how language use affects the reaction that a piece of text generates, but we are the first to focus on questions as far as we know. 2.2 Question Generation QG was traditionally tackled by rule-based approaches (Heilman and Smith, 2010; Lindberg et al., 2013; Mazidi and Nielsen, 2014; Hussein et al., 2014; Labutov et al., 2015). In recent years, neural network (NN) approaches have taken the mainstream. Du et al. (2017) pioneered NN-based QG by using Seq2seq models (Sutskever et al., 2014). Many researches have tried to make it more suitable for QG tasks since then, including using answer position features (Zhou et al., 5033 2017), pointer mechanism (Kumar et al., 2018a; Zhao et al., 2018), etc. Adding more constraints, e.g. controlling the topic (Hu et al., 2018) and difficulty (Gao et al., 2018) of QG, or combining it with QA (Duan et al., 2017;"
P19-1497,D15-1166,0,0.00870569,"replace the original question. We ensured that each NQ-pair was labeled by three people, and it was preserved in OQGenD only if all of them agreed. In this way, we got 20K NQ-pairs. Among these pairs, there were 9K news, each corresponding with more than one questions. The average word numbers in each piece of news, question were 508, 12, respectively. 4.2 Model As shown in Figure 3, our model is composed by a generator Gθ and a discriminator Dφ . Gθ outputs a question Yˆ = {ˆ y1 , yˆ2 , ..., yˆn } from given news X = {x1 , x2 , ..., xm }. It is a Seq2seq network with the attention mechanism (Luong et al., 2015). Both encoder and decoder are GRU (Chung et al., 2014) networks. Dφ takes an NQ-pair (X, YD ) as input, and predicts how likely it comes from real-world dataset. First, it embeds the X, YD into vnews , vques respectively by two CNNs similar to Zhang and Wallace (2015). Based on the two representations, it computes vmatch = Wm [vnews ; vques ] + bm vf luent = Wf vques + bf (4) where [vnews ; vques ] is the concatenation of the two vectors vnews , vques , and Wm , Wf , bm , bf are parameters of our model. We expect vmatch to measure if the question matches the news, and vf luent to measure if t"
P19-1497,P14-1017,0,0.270259,"ntly differs from it in two aspects: first, there is no correct answer for open-answered questions thus it is hard to tell which answer is “useful”. Second, the goal of openQG is to arouse open discussions instead of “solving a problem”. Intuitively, a good question evaluation metric should be interpretable and keeps away from being domain-specific. To this end, we first analyze how language use affects the number of answers, and then build evaluation models based on these conclusions. There are some researches (Guerini et al., 2011; Danescu-Niculescu-Mizil et al., 2012; Guerini et al., 2012; Tan et al., 2014) about how language use affects the reaction that a piece of text generates, but we are the first to focus on questions as far as we know. 2.2 Question Generation QG was traditionally tackled by rule-based approaches (Heilman and Smith, 2010; Lindberg et al., 2013; Mazidi and Nielsen, 2014; Hussein et al., 2014; Labutov et al., 2015). In recent years, neural network (NN) approaches have taken the mainstream. Du et al. (2017) pioneered NN-based QG by using Seq2seq models (Sutskever et al., 2014). Many researches have tried to make it more suitable for QG tasks since then, including using answer"
P19-1497,P14-2053,0,0.0235099,"c should be interpretable and keeps away from being domain-specific. To this end, we first analyze how language use affects the number of answers, and then build evaluation models based on these conclusions. There are some researches (Guerini et al., 2011; Danescu-Niculescu-Mizil et al., 2012; Guerini et al., 2012; Tan et al., 2014) about how language use affects the reaction that a piece of text generates, but we are the first to focus on questions as far as we know. 2.2 Question Generation QG was traditionally tackled by rule-based approaches (Heilman and Smith, 2010; Lindberg et al., 2013; Mazidi and Nielsen, 2014; Hussein et al., 2014; Labutov et al., 2015). In recent years, neural network (NN) approaches have taken the mainstream. Du et al. (2017) pioneered NN-based QG by using Seq2seq models (Sutskever et al., 2014). Many researches have tried to make it more suitable for QG tasks since then, including using answer position features (Zhou et al., 5033 2017), pointer mechanism (Kumar et al., 2018a; Zhao et al., 2018), etc. Adding more constraints, e.g. controlling the topic (Hu et al., 2018) and difficulty (Gao et al., 2018) of QG, or combining it with QA (Duan et al., 2017; Wang et al., 2017; Tang e"
P19-1497,D17-1090,0,0.016581,", 2014; Hussein et al., 2014; Labutov et al., 2015). In recent years, neural network (NN) approaches have taken the mainstream. Du et al. (2017) pioneered NN-based QG by using Seq2seq models (Sutskever et al., 2014). Many researches have tried to make it more suitable for QG tasks since then, including using answer position features (Zhou et al., 5033 2017), pointer mechanism (Kumar et al., 2018a; Zhao et al., 2018), etc. Adding more constraints, e.g. controlling the topic (Hu et al., 2018) and difficulty (Gao et al., 2018) of QG, or combining it with QA (Duan et al., 2017; Wang et al., 2017; Tang et al., 2017) have also been studied. Recently, using adversarial training and reinforcement learning (Yuan et al., 2017; Kumar et al., 2018b; Yao et al., 2018) have become a new trend. As far as we know, the CGAN model we proposed has not used before. Besides, most prior researches aimed to generate fixed-answered questions, and we are the first to propose openQG task to the best of our knowledge. It is worth mentioning that though we only focus on text-based QG, we can also generate questions from images, i.e. visual question generation (Ren et al., 2015; Fan et al., 2018) and knowledge graphs (Serban et"
P19-1497,P02-1040,0,0.105013,"ing and evaluating questions. • We study how language use affects the number of answers a question receives, and draw some interesting conclusions for linguisticbased question evaluation. • We propose a model based on CGAN and our question evaluation model, which outperforms commonly-used text generation models in the quality of generated questions. In this paper, the two datasets OQRanD and OQGend are available at https://github. com/ChaiZ-pku/OQRanD-and-OQGenD. 2 2.1 Related Work Question Evaluation Question evaluation is a rather challenging task. Automatic evaluation metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and METEOR (Lavie and Agarwal, 2007) were widely used to measure n-gram overlaps between generated questions and ground truth questions, however, they are far from enough since we cannot list all possible ground truth questions in openQG. To this end, we need to develop specific evaluation metrics for questions. Some researches (Heilman and Smith, 2010; Figueroa and Neumann, 2013) directly trained question ranking (QR) models via supervised learning, and used it to perform evaluation. However, these models are always domainspecific and not interpretable since we cannot tell"
P19-1497,D16-1264,0,0.0506671,"perform question evaluation, which is rather challenging. Introduction Teaching machines to ask questions from given corpus, i.e. question generation (QG), is an important yet challenging task in natural language processing. In recent years, QG has received increasing attention from both the industrial and academic communities due to its wide applications. Dialog systems can be proactive by asking users questions (Wang et al., 2018), question answering (QA) systems can benefit from the corpus produced by a QG model (Duan et al., 2017), • Questions in most existed QG (QA) datasets, e.g. SQuAD (Rajpurkar et al., 2016), are fixed-answered thus not suitable for openQG. It is worth mentioning that a good question evaluation metric is not only a necessity to compare 1 Quora and Zhihu are large-scale online English, Chinese QA forums, respectively (https://www.quora.com/, https://www.zhihu.com/). 5032 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5032–5046 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics different models, but can also throw light on the text generation process, e.g. acting as the reward function through"
P19-1497,W17-2603,0,0.0272501,"Missing"
P19-1497,D18-1424,0,0.0203879,"re the first to focus on questions as far as we know. 2.2 Question Generation QG was traditionally tackled by rule-based approaches (Heilman and Smith, 2010; Lindberg et al., 2013; Mazidi and Nielsen, 2014; Hussein et al., 2014; Labutov et al., 2015). In recent years, neural network (NN) approaches have taken the mainstream. Du et al. (2017) pioneered NN-based QG by using Seq2seq models (Sutskever et al., 2014). Many researches have tried to make it more suitable for QG tasks since then, including using answer position features (Zhou et al., 5033 2017), pointer mechanism (Kumar et al., 2018a; Zhao et al., 2018), etc. Adding more constraints, e.g. controlling the topic (Hu et al., 2018) and difficulty (Gao et al., 2018) of QG, or combining it with QA (Duan et al., 2017; Wang et al., 2017; Tang et al., 2017) have also been studied. Recently, using adversarial training and reinforcement learning (Yuan et al., 2017; Kumar et al., 2018b; Yao et al., 2018) have become a new trend. As far as we know, the CGAN model we proposed has not used before. Besides, most prior researches aimed to generate fixed-answered questions, and we are the first to propose openQG task to the best of our knowledge. It is worth"
P19-1597,P18-1154,0,0.218678,"oards, predicting moves, and analyzing situations. By jointly training the neural chess engine and the generation models for different categories, the models become more effective. We conduct experiments on 5 categories in a benchmark Chess Commentary dataset and achieve inspiring results in both automatic and human evaluations. 1 Figure 1: Chess Commentary Examples. Introduction With games exploding in popularity, the demand for Natural Language Generation (NLG) applications for games is growing rapidly. Related researches about generating real-time game reports (Yao et al., 2017), comments (Jhamtani et al., 2018; Kameko et al., 2015), and tutorials (Green et al., 2018a,b) benefit people with entertainments and learning materials. Among these, chess commentary is a typical task. As illustrated in Figure 1, the commentators need to understand the current board and move. And then they comment about the current move (Description), their judgment about the move (Quality), the game situation for both sides (Contexts), their analysis (Comparison) and guesses about player’s strategy (Planning). The comments provide valuable information about what is going on and what will happen. Such information not only ma"
P19-1597,W14-3348,0,0.00981513,"d above, and they are trained independently with respect to the 5 categories in Chess Commentary dataset. • SCC-strong: The model is similar to SCCweak, but integrated with the strong engine. • SCC-mult: This is a multi-task learning • Re: This is a retrieval-based baseline method. For each input in the test set, we find the most matched datum in the training set by numbers of matched input board and move features. 4.3 Evaluation Metrics We develop both automatic evaluations and human evaluations to compare the models. For automatic evaluations, we use BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) to evaluate the generated comments with ground-truth outputs. BLEU evaluates the modified precision between the predicted texts and gold-standard references on corpus level. Evaluating with 4-grams (BLEU-4 5 ) is the most popular way in NLG researches. However, for tasks like dialogue system (Li et al., 2016), story telling generation (Jain et al., 2017), and chess commentary (Jhamtani et al., 2018), the outputs can be rather short and free expressions. Under such circumstances, brevity penalty for 4-grams can be too strict and makes the results unbalanced. We use BLEU-2 6 to show more steady"
P19-1597,N16-1014,0,0.0843054,"Missing"
P19-1597,C18-1089,1,0.855214,". We follow the advanced techniques and design our neural chess engine. Apart from learning to play the game, our engine is designed to make game states compatible with semantic representations, which bridges the game state space and human language space. And to realize this, we deploy multi-task learning (Collobert and Weston, 2008; Sanh et al., 2018) in our proposed models. Data-to-text generation is a popular track in NLG researches. Recent researches are mainly about generating from structured data to biography (Sha et al., 2018), market comments (Murakami et al., 2017), and game reports (Li and Wan, 2018). Here we manage to ground the commentary to the game data (boards and moves). Addressing content selection (Wiseman et al., 2017) is one of the top considerations in our designs. 3 Our Approach The overview of our approach is shown in Figure 2. Apart from the text generation models, there are three crucial modules in our approach: the internal chess engine, the move encoder, and the multichoices encoder. We will first introduce our solution to all the sub-tasks of chess commentary generation with the modules as black boxes. And then 5953 Figure 2: Overview of our chess commentary model. we de"
P19-1597,D15-1166,0,0.0239101,"STM decoders (Sundermeyer et al., 2012). And we use cross entropy loss function for training. The function is formalized as Eq.7, where Y is the gold standard outputs. LossGen = −logp(Y |b(0) ; m(0) ) We denote E ∈ IRn×d as a bunch of raw context vectors, where n is the number of such context vectors and d is the dimension of the vectors. Although the semantic contexts E for different generation models are different as described before, we regard all of the board states, wining rates, and move representations as general semantic contexts. And we use attention mechanism (Bahdanau et al., 2015; Luong et al., 2015) to gather information from the contexts. For example, assuming that we have a hidden vector h drawing from LSTM units, to decode with the semantic contexts, we use the score function f of Luong attention (Luong et al., 2015) as f (X, y) = XW y, (b (3) ,m a = sof tmax(f (E, h)). Contexts Model: To analyze the situation of the whole game, the model should know about not only the current, but also the future. And similar to the planning model, contexts model takes a series of long-term moves and boards produced by self-play predictions as inputs. In this way, the model comments the game in a god"
P19-1597,P02-1040,0,0.104366,"rated with the weak engine mentioned above, and they are trained independently with respect to the 5 categories in Chess Commentary dataset. • SCC-strong: The model is similar to SCCweak, but integrated with the strong engine. • SCC-mult: This is a multi-task learning • Re: This is a retrieval-based baseline method. For each input in the test set, we find the most matched datum in the training set by numbers of matched input board and move features. 4.3 Evaluation Metrics We develop both automatic evaluations and human evaluations to compare the models. For automatic evaluations, we use BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) to evaluate the generated comments with ground-truth outputs. BLEU evaluates the modified precision between the predicted texts and gold-standard references on corpus level. Evaluating with 4-grams (BLEU-4 5 ) is the most popular way in NLG researches. However, for tasks like dialogue system (Li et al., 2016), story telling generation (Jain et al., 2017), and chess commentary (Jhamtani et al., 2018), the outputs can be rather short and free expressions. Under such circumstances, brevity penalty for 4-grams can be too strict and makes the results unbalanc"
P19-1597,D17-1239,0,0.0556668,"Missing"
P19-1597,W17-3504,1,0.841224,"odels to help with encoding boards, predicting moves, and analyzing situations. By jointly training the neural chess engine and the generation models for different categories, the models become more effective. We conduct experiments on 5 categories in a benchmark Chess Commentary dataset and achieve inspiring results in both automatic and human evaluations. 1 Figure 1: Chess Commentary Examples. Introduction With games exploding in popularity, the demand for Natural Language Generation (NLG) applications for games is growing rapidly. Related researches about generating real-time game reports (Yao et al., 2017), comments (Jhamtani et al., 2018; Kameko et al., 2015), and tutorials (Green et al., 2018a,b) benefit people with entertainments and learning materials. Among these, chess commentary is a typical task. As illustrated in Figure 1, the commentators need to understand the current board and move. And then they comment about the current move (Description), their judgment about the move (Quality), the game situation for both sides (Contexts), their analysis (Comparison) and guesses about player’s strategy (Planning). The comments provide valuable information about what is going on and what will hap"
Q13-1025,C10-1011,0,0.0273249,"otations also includes functional information and empty categories. Modern parsers, e.g. Collins and Berkeley parsers, ignore these types of linguistic knowledge. To train a constituent parser, we perform a heuristic procedure on the treebank data to delete function tags and empty categories as well as its associated redundant ancestors. Many papers reported parsing results of an older version CTB (namely CTB 5). To compare with systems introduced in these papers, we evaluate our final ensemble model on CTB5 in Section 5.4. For dependency parsing, we choose a second order graph-based parser2 (Bohnet, 2010) and a transition-based parser (Hatori et al., 2011), for experiments. For constituent parsing, we choose Berkeley parser,3 a well known implementation of the unlexicalized PCFGLA model and Bikel parser,4 2 code.google.com/p/mate-tools/ code.google.com/p/berkeleyparser/ 4 cis.upenn.edu/˜dbikel/software.html 3 304 a well known implementation of Collins’ lexicalized model, for experiments. In data-driven parsing, features consisting of POS tags are very effective, so typically POS tagging is performed as a preprocessing. We use the baseline sequential tagger described in (Sun and Uszkoreit, 2012"
Q13-1025,P04-1041,0,0.087956,"Missing"
Q13-1025,A00-2018,0,0.0793168,"ions normally contain richer information and thus are reliable for tree conversion. 2.2.1 Constituency parsing Compared to many other languages, statistical constituent parsing for Chinese has reached early success, due to the fact that the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from da"
Q13-1025,P05-1022,0,0.0129527,"ntain richer information and thus are reliable for tree conversion. 2.2.1 Constituency parsing Compared to many other languages, statistical constituent parsing for Chinese has reached early success, due to the fact that the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCF"
Q13-1025,P12-2003,0,0.221451,"model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCFGLA parser leverages on an automatic procedure to learn refined grammars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-the-art performance and defeats many other types of parsers, including Collins as well as Charniak parser (Che et al., 2012) and discriminative transition-based models (Zhang and Clark, 2009). 2.2.2 CS to DS conversion In the absence of dependency and constituency structures for a particular treebank, treebank-guided parser developers normally apply rich linguistic rules to convert one representation formalism to another to get necessary data to train parsers. Xue (2007) examines the linguistic adequacy of dependency structure annotation automatically converted from phrase structure treebanks with rule-based approaches. A structural approach is introduced for the constituency structure (CS) to dependency structure"
Q13-1025,J03-4003,0,0.160915,"ructure annotations normally contain richer information and thus are reliable for tree conversion. 2.2.1 Constituency parsing Compared to many other languages, statistical constituent parsing for Chinese has reached early success, due to the fact that the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-ca"
Q13-1025,P99-1065,0,0.199812,"es are two automatically converted pseudo constituency trees. By applying DS to CS rules, we can acquire pseudo constituency treebanks and then learn pseudo grammars from them. (1) Dependency tree (2) Linguistic constituency tree (3) Flat constituency tree (4) Binarized constituency tree Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure. The basic idea of our method is to use parsing models in one formalism for parsing in another formalism. In previous work, PCFGs are used to solve parsing problems in many other formalisms, including dependency (Collins et al., 1999), CCG (Fowler and Penn, 2010), LFG (Cahill et al., 2004) and HPSG (Zhang and Krieger, 2011) parsing. boundary information. Nevertheless, a complete subtree in a projective dependency tree should be considered as a constituent. We can construct a very flat constituent tree, of which nodes are associated with complete subtrees of a dependency parse. The third tree in Figure 3 is an example of such conversion. 5.1 Strategies for DS to CS conversion Right-to-left binarization According to the study in (Sun, 2010a), head words of most phrases in Chinese are located at the first or the last position"
Q13-1025,C96-1058,0,0.159863,"re superior to each of the individual parsers. We implement their method to aggregate models. Once we have obtained multiple dependency trees respectively from base parsers, we can build a graph where each word in the sentence is a node. We then create weighted directed edges between the nodes corresponding to words for which dependencies are obtained from each of the initial structures. The weights are the word-by-word voting results of sub-models. Based on this graph, the sentence can be reparsed by a graph-based algorithm. Taking Chinese as a projective language, we use Eisner’s algorithm (Eisner, 1996) to combine multiple dependency parses. Surdeanu and Manning (2010) indicates that reparsing performs essentially as well as other simpler or more complex models. 86.5 G raph[-lab] 85.5 Tran U nlex 84.5 G raph[-lab]+U nlex 83.5 Tran+U nlex 82 .5 G raph[-lab]+Tran 81.5 3 4 5 6 7 8 9 4.2 Parameter tuning We evaluate our combination model on the same data set used in the last section. The two hyperparameters (λ and m) of our Bagging model are tuned on the development (validation) set. On one hand, with the increase of the size of sub-samples, i.e. λ, the performance of sub-models is improved. How"
Q13-1025,P10-1035,0,0.0195225,"verted pseudo constituency trees. By applying DS to CS rules, we can acquire pseudo constituency treebanks and then learn pseudo grammars from them. (1) Dependency tree (2) Linguistic constituency tree (3) Flat constituency tree (4) Binarized constituency tree Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure. The basic idea of our method is to use parsing models in one formalism for parsing in another formalism. In previous work, PCFGs are used to solve parsing problems in many other formalisms, including dependency (Collins et al., 1999), CCG (Fowler and Penn, 2010), LFG (Cahill et al., 2004) and HPSG (Zhang and Krieger, 2011) parsing. boundary information. Nevertheless, a complete subtree in a projective dependency tree should be considered as a constituent. We can construct a very flat constituent tree, of which nodes are associated with complete subtrees of a dependency parse. The third tree in Figure 3 is an example of such conversion. 5.1 Strategies for DS to CS conversion Right-to-left binarization According to the study in (Sun, 2010a), head words of most phrases in Chinese are located at the first or the last position. That means for binarizing m"
Q13-1025,I11-1136,0,0.0454682,"t dependency parsing focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by Zhang and Nivre (2011). Second, beyond deterministic greedy search, principled dynam"
Q13-1025,W99-0623,0,0.0760128,"is of the CoNLL 2009 shared task data. By applying this conversion procedure on the outputs of an automatic phrase structure parser, we can build a PCFG-based dependency parser. 2.3 Parser ensemble NLP systems built on particular single views normally capture different properties of an original problem, and therefore differ in predictive powers. As a result, NLP systems can take advantage of complementary strengths of multiple views. Combining the outputs of several systems has been shown in the past to improve parsing performance significantly, including integrating phrase-structure parsers (Henderson and Brill, 1999), dependency parsers (Nivre and McDonald, 2008), or both (McDonald, 2006). Several ensemble models have been proposed for the parsing of syntactic constituents and dependencies, including learning-based stacking (Nivre and McDonald, 2008; Torres Martins et al., 2008) and learning-free post-inference (Henderson and Brill, 1999; Sagae and Lavie, 2006). Surdeanu and Manning (2010) present a systematic analysis of these ensemble methods and find several non-obvious facts: • the diversity of base parsers is more important than complex models for learning, and 1 For example, as two popular dependenc"
Q13-1025,A00-2005,0,0.0431167,"ines (http://www.cis.upenn. edu/˜chinese/posguide.3rd.ch.pdf). 306 4.1 Applying Bagging to dependency parsing Bagging is a machine learning ensemble metaalgorithm to improve classification and regression models in terms of stability and classification accuracy (Breiman, 1996). It also reduces variance and helps to avoid overfitting. Given a training set D of size n, Bagging generates m new training sets Di of size n′ ≤ n, by sampling examples from D. m models are separately learned on the m new training sets and combined by voting (for classification) or averaging the output (for regression). Henderson and Brill (2000) successfully applied Bagging to enhance a constituent parser. Moreover, Bagging has been applied to combine multiple solutions for Chinese lexical processing (Sun, 2010b; Sun and Uszkoreit, 2012). In this paper, we apply Bagging to dependency parsing. Since training even one single parser takes hours (if not days), experiments on Bagging is time-consuming. To save time, we conduct data-driven parsing experiments based on simple configuration. More specifically, the beam size of the transition-based parser is set to 16, and the simple feature set is utilized; dependency relations are not incor"
Q13-1025,P10-1110,0,0.103321,"ainstream work on recent dependency parsing focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by Zhang and Nivre (2011). Second, beyond deterministic greedy sea"
Q13-1025,N09-2054,0,0.0160455,"discriminative Markov and semi-Markov tagging models are reported. Their experiments showed that Bagging can consistently enhance a semi-Markov model but not the Markov one. Experiments on POS tagging indicated that Bagging Markov models hurts tagging performance. It seems that the relationships among basic processing units affect Bagging. PCFGLA parsers are built upon generative models with latent annotations. The use of automatically induced latent variables may also affect Bagging. Generative sequence models with latent annotations can also achieve good performance for Chinese POS tagging. Huang et al. (2009) described and evaluated a bi-gram HMM tagger that utilizes latent annotations. Different from negative results of Bagging discriminative models, our auxiliary experiment shows that Bagging Huang et al.’s tagger can help Chinese POS tagging. In other words, Bagging substantially improves both HMMLA and PCFGLA models, at least for Chinese POS tagging and constituency parsing. It seems that Bagging favors the use of latent variables. denote graph-based, transition-based and PCFGbased parsers as g, t and c; denote the reparsing procedure as reparse and the Bagging procedure as bagging. The two ex"
Q13-1025,P12-1071,0,0.0291299,"Missing"
Q13-1025,D11-1109,0,0.0457477,"Missing"
Q13-1025,P05-1010,0,0.0380651,"hat the language has relatively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCFGLA parser leverages on an automatic procedure to learn refined grammars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-th"
Q13-1025,D07-1013,0,0.0875169,"Missing"
Q13-1025,J08-4003,0,0.362412,"Missing"
Q13-1025,P08-1108,0,0.246077,"mpact on dependency parsing and PCFGbased models have complementary predictive powers to data-driven models. System ensemble is an effective and important technique to build more accurate parsers based on multiple, diverse, weaker models. Exploiting differ301 Transactions of the Association for Computational Linguistics, 1 (2013) 301–314. Action Editor: Jason Eisner. c Submitted 6/2012; Revised 10/2012; Published 7/2013. 2013 Association for Computational Linguistics. ent data-driven models, e.g. transition- and graphbased models, has received the most attention in dependency parser ensemble (Nivre and McDonald, 2008; Torres Martins et al., 2008; Sagae and Lavie, 2006). Only a few works investigate integrating data-driven and PCFG-based models (McDonald, 2006). We argue that grammars can significantly increase the diversity of base models, which plays a central role in parser ensemble, and therefore lead to better and more promising hybrid systems. We introduce a general classifier enhancing technique, i.e. bootstrap aggregating (Bagging), to improve dependency parsing accuracy. This technique can be applied to enhance a single-view parser, or to combine multiple heterogeneous parsers. Experiments on the"
Q13-1025,P06-1055,0,0.0554414,"atively fixed word order and extremely poor inflectional morphology. Both facts allow PCFG-based statistical modeling to perform well. For the constituent parsing, the majority of the state-of-theart parsers are based on generative PCFG learning. For example, the well-known and successful Collins and Charniak&Johnson parsers (Collins, 2003; Charniak, 2000; Charniak and Johnson, 2005) implement generative lexicalized statistical models. Apart from lexicalized PCFG parsing, unlexicalized parsing with latent variable grammars (PCFGLA) can also produce comparable accuracy (Matsuzaki et al., 2005; Petrov et al., 2006). Latent variable grammars model an observed treebank of coarse parse trees with a model over more refined, but unobserved, derivation trees that represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCFGLA parser leverages on an automatic procedure to learn refined grammars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-the-art performance and"
Q13-1025,N06-2033,0,0.583141,"omplementary predictive powers to data-driven models. System ensemble is an effective and important technique to build more accurate parsers based on multiple, diverse, weaker models. Exploiting differ301 Transactions of the Association for Computational Linguistics, 1 (2013) 301–314. Action Editor: Jason Eisner. c Submitted 6/2012; Revised 10/2012; Published 7/2013. 2013 Association for Computational Linguistics. ent data-driven models, e.g. transition- and graphbased models, has received the most attention in dependency parser ensemble (Nivre and McDonald, 2008; Torres Martins et al., 2008; Sagae and Lavie, 2006). Only a few works investigate integrating data-driven and PCFG-based models (McDonald, 2006). We argue that grammars can significantly increase the diversity of base models, which plays a central role in parser ensemble, and therefore lead to better and more promising hybrid systems. We introduce a general classifier enhancing technique, i.e. bootstrap aggregating (Bagging), to improve dependency parsing accuracy. This technique can be applied to enhance a single-view parser, or to combine multiple heterogeneous parsers. Experiments on the CoNLL 09 shared task data demonstrate its effectivene"
Q13-1025,P10-2031,1,0.90609,"Missing"
Q13-1025,C10-2139,1,0.914673,"labels to a model. • For every word h that governs at least two children (d1 , ..., dn ), we consider every word triple hh, di , di+1 i, among h and its sibling dependents di as well as di+1 (0 ≤ i &lt; n). Similar to the grandparent dependencies, we can define evaluation metrics for sibling dependencies. structures, and an extra statistical classifier can be employed to label automatically recognized dependencies with a high accuracy. Although this issue is not well studied for Chinese dependency parsing, previous research on function tag labeling (Sun and Sui, 2009) and semantic role labeling (Sun, 2010a) gives us some clues. Their research shows that both functional and predicate-argument structural information is relatively easy to predict if high-quality syntactic parses are available. We mainly focus on the UAS metric in the following experiments. From Table 1, we can see that the grammar-based model parses relatively better for slightly larger fragments. For example, the UAS of the graph-based model is significantly higher than the grammarbased one, but their sibling and grandparent scores are similar. In the next section, we will introduce a general parser enhancement technique and pre"
Q13-1025,Y09-2011,1,0.791843,". +/-lab means whether to incorporate relation labels to a model. • For every word h that governs at least two children (d1 , ..., dn ), we consider every word triple hh, di , di+1 i, among h and its sibling dependents di as well as di+1 (0 ≤ i &lt; n). Similar to the grandparent dependencies, we can define evaluation metrics for sibling dependencies. structures, and an extra statistical classifier can be employed to label automatically recognized dependencies with a high accuracy. Although this issue is not well studied for Chinese dependency parsing, previous research on function tag labeling (Sun and Sui, 2009) and semantic role labeling (Sun, 2010a) gives us some clues. Their research shows that both functional and predicate-argument structural information is relatively easy to predict if high-quality syntactic parses are available. We mainly focus on the UAS metric in the following experiments. From Table 1, we can see that the grammar-based model parses relatively better for slightly larger fragments. For example, the UAS of the graph-based model is significantly higher than the grammarbased one, but their sibling and grandparent scores are similar. In the next section, we will introduce a genera"
Q13-1025,P12-1026,1,0.925684,"parser2 (Bohnet, 2010) and a transition-based parser (Hatori et al., 2011), for experiments. For constituent parsing, we choose Berkeley parser,3 a well known implementation of the unlexicalized PCFGLA model and Bikel parser,4 2 code.google.com/p/mate-tools/ code.google.com/p/berkeleyparser/ 4 cis.upenn.edu/˜dbikel/software.html 3 304 a well known implementation of Collins’ lexicalized model, for experiments. In data-driven parsing, features consisting of POS tags are very effective, so typically POS tagging is performed as a preprocessing. We use the baseline sequential tagger described in (Sun and Uszkoreit, 2012) to provide such lexical information to the graph-based parser. Note that the transition-based parser performs a joint inference to acquire POS and dependency information simultaneously, so there is no need to offer extra tagging results to it. 3.2 Overall performance Table 1 (Column 2-6) summarizes the overall accuracy of different parsers. Two transition-based parsing results are presented: The first one employ a simple feature set (Zhang and Clark, 2008) and a small beam (16); the second one employ rich features (Zhang and Nivre, 2011) and a larger beam (32). Two graph-based parsing results"
Q13-1025,P09-1039,0,0.18528,"Missing"
Q13-1025,D08-1017,0,0.120811,"Missing"
Q13-1025,W11-2923,0,0.0238692,", we can acquire pseudo constituency treebanks and then learn pseudo grammars from them. (1) Dependency tree (2) Linguistic constituency tree (3) Flat constituency tree (4) Binarized constituency tree Figure 3: An example: China encourages private entrepreneurs to invest in national infrastructure. The basic idea of our method is to use parsing models in one formalism for parsing in another formalism. In previous work, PCFGs are used to solve parsing problems in many other formalisms, including dependency (Collins et al., 1999), CCG (Fowler and Penn, 2010), LFG (Cahill et al., 2004) and HPSG (Zhang and Krieger, 2011) parsing. boundary information. Nevertheless, a complete subtree in a projective dependency tree should be considered as a constituent. We can construct a very flat constituent tree, of which nodes are associated with complete subtrees of a dependency parse. The third tree in Figure 3 is an example of such conversion. 5.1 Strategies for DS to CS conversion Right-to-left binarization According to the study in (Sun, 2010a), head words of most phrases in Chinese are located at the first or the last position. That means for binarizing most phrases, we only need sequentially combine the right or le"
Q13-1025,D08-1059,0,0.112678,"S tags are very effective, so typically POS tagging is performed as a preprocessing. We use the baseline sequential tagger described in (Sun and Uszkoreit, 2012) to provide such lexical information to the graph-based parser. Note that the transition-based parser performs a joint inference to acquire POS and dependency information simultaneously, so there is no need to offer extra tagging results to it. 3.2 Overall performance Table 1 (Column 2-6) summarizes the overall accuracy of different parsers. Two transition-based parsing results are presented: The first one employ a simple feature set (Zhang and Clark, 2008) and a small beam (16); the second one employ rich features (Zhang and Nivre, 2011) and a larger beam (32). Two graph-based parsing results are reported; the difference between them is whether integrate relation labels into the parsing procedure. Roughly speaking, currently state-of-the-art data-driven models achieves slightly better precision than unlexicalized PCFG-based models with regard to unlabeled dependency prediction. There is a big gap between lexicalized and unlexicalized parsing. The same phenomenon has been observed by (Che et al., 2012) and (Zhuang and Zong, 2010). In addition to"
Q13-1025,W09-3825,0,0.08432,"represent much more complex syntactic processes. Rather than attempting to manually specify fine-grained categories, previous work shows that automatically inducing the sub-categories from data can work quite well. A PCFGLA parser leverages on an automatic procedure to learn refined grammars and are therefore more robust to parse non-English languages that are not well studied. For Chinese, such a parser achieves the state-of-the-art performance and defeats many other types of parsers, including Collins as well as Charniak parser (Che et al., 2012) and discriminative transition-based models (Zhang and Clark, 2009). 2.2.2 CS to DS conversion In the absence of dependency and constituency structures for a particular treebank, treebank-guided parser developers normally apply rich linguistic rules to convert one representation formalism to another to get necessary data to train parsers. Xue (2007) examines the linguistic adequacy of dependency structure annotation automatically converted from phrase structure treebanks with rule-based approaches. A structural approach is introduced for the constituency structure (CS) to dependency structure (DS) conversion for the Chinese Treebank data, which is the basis o"
Q13-1025,J11-1005,0,0.0444095,"work 2.1 Data-driven dependency parsing The mainstream work on recent dependency parsing focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by Zhang and Nivre ("
Q13-1025,P11-2033,0,0.431697,"ependency parsing The mainstream work on recent dependency parsing focuses on data-driven approaches that automatically learn to produce dependency graphs for sentences solely from a hand-crafted dependency treebank. The advantage of such models is that they are easily ported to any language in which labeled linguistic resources exist. Practically all statistical models that have been proposed in recent years can be mainly described as either graph-based or transition-based (McDonald and Nivre, 2007). Both models have been adopted to learn Chinese dependency structures (Zhang and Clark, 2011; Zhang and Nivre, 2011; Huang and Sagae, 2010; Hatori et al., 2011; Li et al., 2011, 2012). According to published results, graph-based and transition-based parsers achieve similar accuracy. In the graph-based framework, informative evaluation results have been presented in (Li et al., 2011). First, second and third order projective parsing models are well evaluated. In the transition-based framework, two advanced techniques have been studied. First, developing features has been shown crucial to advancing parsing accuracy and a very rich feature set is carefully evaluated by Zhang and Nivre (2011). Second, beyond d"
Q13-1025,C10-1153,0,0.0914276,"le feature set (Zhang and Clark, 2008) and a small beam (16); the second one employ rich features (Zhang and Nivre, 2011) and a larger beam (32). Two graph-based parsing results are reported; the difference between them is whether integrate relation labels into the parsing procedure. Roughly speaking, currently state-of-the-art data-driven models achieves slightly better precision than unlexicalized PCFG-based models with regard to unlabeled dependency prediction. There is a big gap between lexicalized and unlexicalized parsing. The same phenomenon has been observed by (Che et al., 2012) and (Zhuang and Zong, 2010). In addition to dependency parsing, Zhuang and Zong (2010) found that Berkeley parser produce much more accurate syntactic analyses to assist a Chinese semantic role labeler than Bikel parser. Charniak and Stanford parsers are two other wellknown and frequently used tools that can provide lexicalized parsing results. According to (Che et al., 2012), they perform even worse than Bikel parser, at least for Stanford dependencies. Due to the poor parsing performance, we only concentrate on the unlexicalized model in the remainder of this paper. The performance of labeled dependency prediction of"
Q13-1025,N10-1091,0,\N,Missing
S14-2080,P10-1035,0,0.0309837,"ify some dependency relations in order to make the graph a tree. Parsing based on graph spanning is quite challenging since computational properties of the semantic graphs given by the shared task are less explored and thus still unknown. On the other hand, finding the best higher-order spanning for general graph is NP complete, and therefore it is not easy, if not impossible, to implement arc-factored models with exact inference. In our work, we use a practical idea to indirectly profile the graph-based parsing techniques for dependency graph parsing. Inspired by the PCFG approximation idea (Fowler and Penn, 2010; Zhang and Krieger, 2011) for deep parsing, we study tree approximation approaches for graph spanning. This tree approximation technique can be applied to both transition-based and graph-based parsers. However, since transition systems that can directly handle build graphs have been developed, we only use this technique to evaluate the possible effectiveness of graph-based models for semantic parsing. 4.1 4.2 Auxiliary Labels In the transformed trees, we use auxiliary labels to carry out information of the original graphs. To encode multiple edges to one, we keep the original label on the dir"
S14-2080,S14-2008,0,0.15362,"Missing"
S14-2080,C08-1095,0,0.179036,"ollowing several well-established syntactic theories, SemEval-2014 task 8 (Oepen et al., 2014) proposes using graphs to represent semantics. Considering that semantic dependency parsing is a quite new topic and there is little previous work, we think it worth appropriately profiling successful tree parsing techniques for graph parsing. To this end, we build a hybrid system Architecture We explore two kinds of basic models: One is transition-based, and the other is tree approximation. Transition-based models are widely used for dependency tree parsing, and they can be adapted to graph parsing (Sagae and Tsujii, 2008; Titov et al., 2009). Here we implement 5 transitionbased models for dependency graph parsing, each of which is based on different transition system. The motivation of developing tree approximation models is to apply existing graph-based tree parsers to generate graphs. At the training time, we convert the dependency graphs from the training data into dependency trees, and train secondorder arc-factored models1 . At the test phase, we parse sentences using this tree parser, and convert the output trees back into semantic graphs. We think tree approximation can appropriately evaluate the possi"
S14-2080,D07-1111,0,0.0168913,"nd a set A of arcs, denoted by hσ, β, Ai. The initial configuration of a sentence x = w1 w2 · · · wn is cs (x) = h[0], [1, 2, · · · , n], {}i, and the terminal configuration set Ct is the set of all configurations with empty buffer. These two transition systems are shown in 1. The transitions of the Titov system are: 3.3 • L EFT-A RCl adds an arc from the front of the buffer to the top of the stack, labeled l, into A. Sentence Reversal Reversing the order the words of a given sentence is a simple way to yield heterogeneous parsing models, thus improving parsing accuracy of the model ensemble (Sagae, 2007). In our experiments, one transition system produces two models, one trained on the normal corpus, and the other on the corpus of reversed sentences. Therefore we can get 10 parse of a sentence based on 5 transition systems. • R IGHT-A RCl adds an arc from the top of the stack to the front of the buffer, labeled l, into A. • S HIFT removes the front of the buffer and push it onto the stack; 460 L EFT-A RCl R IGHT-A RCl S HIFT P OP S WAP L EFT-A RCkl R IGHT-A RCkl S HIFT P OPk (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(j, l, i)}) (σ|i, j|β, A) ⇒ (σ|i, j|β, A ∪ {(i, l, j)}) (σ, j|β, A) ⇒ (σ|j, β, A) (σ|i,"
S14-2080,N10-1091,0,0.150502,"Missing"
S14-2080,W11-2923,0,0.0269142,"ations in order to make the graph a tree. Parsing based on graph spanning is quite challenging since computational properties of the semantic graphs given by the shared task are less explored and thus still unknown. On the other hand, finding the best higher-order spanning for general graph is NP complete, and therefore it is not easy, if not impossible, to implement arc-factored models with exact inference. In our work, we use a practical idea to indirectly profile the graph-based parsing techniques for dependency graph parsing. Inspired by the PCFG approximation idea (Fowler and Penn, 2010; Zhang and Krieger, 2011) for deep parsing, we study tree approximation approaches for graph spanning. This tree approximation technique can be applied to both transition-based and graph-based parsers. However, since transition systems that can directly handle build graphs have been developed, we only use this technique to evaluate the possible effectiveness of graph-based models for semantic parsing. 4.1 4.2 Auxiliary Labels In the transformed trees, we use auxiliary labels to carry out information of the original graphs. To encode multiple edges to one, we keep the original label on the directed edge but may add oth"
S14-2080,P11-2033,0,0.0350252,"Statistical Disambiguation First of all, we derive oracle transition sequences for every sentence, and train Passive-Aggressive models (Crammer et al., 2006) to predict next transition given a configuration. When it comes to parsing, we start with the initial configuration, predicting next transition and updating the configuration with the transition iteratively. And finally we will get a terminal configuration, we then stop and output the arcs of the graph contained in the final configuration. We extracted rich feature for we utilize a set of rich features for disambiguation, referencing to Zhang and Nivre (2011). We examine the several tops of the stack and the one or more fronts of the buffer, and combine the lemmas and POS tags of them in many ways as the features. Additionally, we also derive features from partial parses such as heads and dependents of these nodes. Our Transition Systems We implemented 5 different transition systems for graph parsing. Here we describe two of them in detail, one is the Titov system proposed in (Titov et al., 2009), and the other is our Naive system. The configurations of the two systems each contain a stack σ, a buffer β, and a set A of arcs, denoted by hσ, β, Ai."
S15-2154,C10-1011,0,0.0195341,"transition-based and tree approximation approaches. The transitionbased model use transitions on configurations to obtain graph parses, while the tree approximation model transform graphs into trees for training and test. To further combine the complementary prediction power, DZWS14 applied a voting-based ensemble method. 2.1 Transition-Based Models Tree Approximation Models The core of tree approximation is transformations between graphs and trees. At the training time, we convert the dependency graphs from the training data into dependency trees, and train second-order arc-factored models1 (Bohnet, 2010). At the test phase, we parse sentences using this tree parser, and convert the output trees back into semantic graphs. In DZSW14, We develop several different methods to convert a semantic graph into a tree. The main idea is to apply graph traversal algorithms to convert a directed graph to a directed tree. During the traversal, we may lose or modify some dependency relations in order to make a tree. 2.3 • The model ensemble is quite effective, resulting in a boost in performance. 3 Weighted Tree Approximation Models In our system for SemEval-2015, we develop more tree approximation models fo"
S15-2154,S14-2080,1,0.710053,"eked to stimulate the dependency parsing community to move towards more general graph processing. Quite a number of teams all over the world participated in this shared task, which suggests a growing community interest in parsing into graph-shaped dependency representations. ∗ Email correspondence. SemEval-2015 Task 18 is a subsequent task of SemEval-2014 Task 8. Following several wellestablished syntactic theories, this task proposes using graphs to represent semantics and provides highquality annotations for three typologically different languages. We have developed a system, dubbed DZSW14 (Du et al., 2014) for the task last year. The system employed a hybrid architecture which benefits from both transition-based and graph-based parsing approaches. Evaluation on multiple English data sets provided by SemEval-2014 indicated that DZSW14 is able to obtain high-quality parsing results. Following the key idea to employ heterogeneous models to enhance hybrid parsing, we extend DZSW14 by developing more tree approximation models, namely the weighted tree approximation models. Evaluation on multilingual data sets provided by this year’s task confirms the effectiveness of the techniques we have studied."
S15-2154,S14-2008,0,0.315983,"Missing"
W10-4137,W06-0127,0,0.102443,"rresponding inputs. This model has achieved great successes in word segmentation. In the CRFs model, the conditional distribution P(y|x) of the labels Y givens observations X directly is defined: T 1 P( y / x )  exp{ k f k ( yt 1 , yt , x, t )} Zx t 1 k y is the label sequence, x is observation sequence, Zx is a normalization term that makes the probability of all state sequences sum to one; fk(yt-1, yt, t) is often a binary-valued feature function and λ k is the weight of fk. In our system, we choose six types of tags according to character position in a word. According to Zhao’s work (Zhao et al., 2006a), the 6tag set enables our system to generate a better CRF model than the 4-tag set. In our experiments, we test both the 6-tag set and the 4-tag set, and the 6-tag set truly has a better result. The 6-tag set is defined as below: T = {B, B2, B3, M, E, S} Here B, B2, B3, M, E represent the first, second, third, continuing and end character positions in a multi-character word, and S is the single-character word tag. We adopt 6 n-gram feature templates as features. Some researches have proved that the combination of 6-tag set and 6 n-gram feature template can achieve a better performance (Zhao"
W10-4137,Y06-1012,0,0.0169523,"rresponding inputs. This model has achieved great successes in word segmentation. In the CRFs model, the conditional distribution P(y|x) of the labels Y givens observations X directly is defined: T 1 P( y / x )  exp{ k f k ( yt 1 , yt , x, t )} Zx t 1 k y is the label sequence, x is observation sequence, Zx is a normalization term that makes the probability of all state sequences sum to one; fk(yt-1, yt, t) is often a binary-valued feature function and λ k is the weight of fk. In our system, we choose six types of tags according to character position in a word. According to Zhao’s work (Zhao et al., 2006a), the 6tag set enables our system to generate a better CRF model than the 4-tag set. In our experiments, we test both the 6-tag set and the 4-tag set, and the 6-tag set truly has a better result. The 6-tag set is defined as below: T = {B, B2, B3, M, E, S} Here B, B2, B3, M, E represent the first, second, third, continuing and end character positions in a multi-character word, and S is the single-character word tag. We adopt 6 n-gram feature templates as features. Some researches have proved that the combination of 6-tag set and 6 n-gram feature template can achieve a better performance (Zhao"
W10-4137,J04-1004,0,0.0170716,"orpora meeting our requirements are less. The lower confidence may not guarantee the reliability. So the setting of the confiOur results in this bakeoff Literature 0.922 0.916 0.919 Computer Medicine Finance 0.934 0.911 0.940 0.939 0.917 0.943 0.937 0.914 0.941 From Table 6, we can see our system can achieve a high precision, especially in the domains of computer and finance. This proves our methods are fairly effective. 4 4.1 Discussion Segmentation Features In our system, we only take advantage of the features of the words. We try to add other features to our experiments such as AV feature (Feng et al., 2004a; Feng et al., 2004b; Hai Zhao et al., 2007) with the expectation of improving the results. But the results are not satisfying. We believe that the feature of words frequency may be an important factor, but how to use it is worth studying. So finding some meaningful and effective features is the crucial point. 4.2 OOV In our system, we do not process the words out of vocabulary in the special way. The recognition of OOV is still a problem. In a word, there is still much to be done to improve our system. In the present work, we make use of some surface features, and further study should be con"
W17-3504,W11-2810,0,0.0749955,"Missing"
W17-3504,N13-1136,0,0.06151,"Missing"
W17-3504,W11-2828,0,0.0249757,"art of score has some form of supervision from calculating statistics on aligned training data. 5 5.1 Related Work Sports News Generation To the best of our knowledge, generation of sports news by utilizing commentary texts is not a wellstudied task in related fields. Very few related work can be backtracked other than the study of (Zhang et al., 2016) which treats the task as single document summarization and develop a supervised learningto-rank framework to show the feasibility of this task. A few earlier studies attempted to generate sports report from structured data such as event tables (Lareau et al., 2011) and ontology-based knowledge base (Bouayad-Agha et al., 2011; Bouayad-Agha et al., 2012), based on predefined templates. There exist some related studies that focused on generating textual summaries for sports events from status upto space limit. System Centroid LexRank SVR SVM LTR This work Gold-standard ROUGE-1 0.26201 0.24456 0.30502 0.30934 0.32489 0.33247 0.40802 ROUGE-2 0.05150 0.03533 0.07371 0.07482 0.09464 0.10223 0.12924 ROUGE-SU4 0.08146 0.06609 0.10532 0.10681 0.12319 0.13478 0.16407 Pyramid 0.32483 0.29034 0.42828 0.46276 0.56621 0.80759 0.88219 Table 4: Evaluation results of dif"
W17-3504,N10-1134,0,0.0589787,"Missing"
W17-3504,N03-1020,0,0.0729401,"rk, we perform crossvalidation during evaluation to utilize the dataset more sufficiently and to draw more reliable conclusions. Specifically, we randomly divide the dataset into three parts with equal sizes, each contains 50 pairs of live texts and gold-standard news. Each time we set one of them as the test set and use the remaining two parts for training, or specific types of corpus statistics as used in our method. We will mainly report the averaged results from all three folds. 4.2 Evaluation Metrics Similar to the evaluation for traditional summarization tasks, we use the ROUGE metrics (Lin and Hovy, 2003) to automatically evaluate the quality of produced summaries given the gold-standard reference news. The ROUGE metrics measure summary quality by counting the precision, recall and F-score of overlapping units, such as n-grams and skip grams, between a candidate summary and the reference summaries. Specifically, we report the F-scores of the following metrics in the experimental results: ROUGE1 (unigram-based), ROUGE-2 (bigram-based) and ROUGE-SU4 (based on skip bigrams with a maximum skip distance of 4). Note that the ROUGE scores are computed for each document set, and then the scores are av"
W17-3504,W16-6601,0,0.0450639,"Missing"
W17-3504,N04-1019,0,0.10934,"or subwords, which boosts the ROUGE quantities slightly larger than expected and may incorrectly reflect the preference between each other. The ROUGE distance between system outputs and gold standard manually written news, which should be treated as an upper bound, is somewhat close. In this work the evaluation is based on another popular Chinese word segmentation toolkit called Jieba,5 that performs word segmentation results with satisfactory level of accuracy, when provided external sports dictionary. We also conduct manual evaluation in this study. Specifically, we use the pyramid method (Nenkova and Passonneau, 2004) and modified pyramid scores as described in (Passonneau et al., 2005) to manually evaluate the summaries generated by different methods. We randomly sample 20 games from the data set and manually annotate facts on the gold-standard news. The annotated facts are mostly describing specific events happened during the game. Each fact is treated as a Summarization Content Unit (SCU) (Nenkova and Passonneau, 2004). The number of occurrences for each SCU in the gold-standard news is regarded as the weight of this SCU. Two types of scores for peers were computed from the peer annotations. Both scores"
W17-3504,W00-0403,0,0.188142,"support vector machine (SVM) and support vector regression (SVR) model serve as strong supervised baselines. We utilize the LIBSVM implementation6 (Chang and Lin, 2011) with the RBF kernel for classification/regression. We reimplemented the features described in (Zhang et al., 2016) which turn out to be effective for this task. As in stream data settings, features that depends on future observations are not used. We also generate results from batch processing systems for reference. Specifically, we implemented graph-based document summarization approach including centroid-based summarization (Radev et al., 2000) and the well-known LexRank (Erkan and Radev, 2004). We also rebuilt the learning-to-rank system followed with a probabilistic greedy selection procedure, as used by (Zhang et al., 2016) based on random forests of LambdaMART rankers, and observed similar results as the authors reported. The produced results have been verified to be similar to those reported in their paper, if using the same word segmentation procedure. 4.4 Results Table 4 lists the results for different output systems. The results of this work is significantly different (p &lt; 0.01) with all baseline systems but LTR, 6 https://g"
W17-3504,P16-1129,1,0.483616,"Rather than receiving every piece of text of a sports match before news construction, as in previous related work, we novelly verify the feasibility of a more challenging setting to generate news report on the fly by treating live text input as a stream. We design scoring functions to address different requirements of the task and use stream substitution for sentence selection. Experiments suggest that our proposed framework can already produce comparable results compared with previous work that relies on a supervised learning-to-rank model. 1 As a promising starting point, one recent study (Zhang et al., 2016) successfully demonstrated that it is technically feasible to generate sports news from given live text commentary scripts. They treat the task as a special kind of document summarization and adapt supervised learning-to-rank models to learn preference for which sentences should be extracted for construction. Introduction Live text commentary services are available on the web and are becoming increasingly popular for sports fans who do not have access to live video streams due to copyright reasons. Some people may also prefer live texts on portable devices. The emergence of live texts has prod"
W17-3504,W01-0100,0,\N,Missing
W17-3526,E17-1059,0,0.0655487,"ects get neutral or slightly negative reviews, while all 5-point aspects get definitely positive comments. And 4-point aspects also get reviews biased towards being positive. As for the last example after changing the rating of Comfort from 3-point to 5-point, we can see that except for the review sentence for Comfort, other sentences do not change apparently. But the review sentence of Comfort changes significantly from neutral to positive, which shows the power of our model. 6 Related Work Several previous studies have attempted for review generation (Tang et al., 2016; Lipton et al., 2015; Dong et al., 2017) . They generate personalized reviews according to an overall rating. But they do not consider the product aspects and whether each generated sentence is produced as the user requires. The models they proposed are very similar to SRGMs. And the length of reviews texts are not as long as ours. Therefore, our work can be regarded as a significant improvement of their researches. Many researches of text generation are also closely related to our work. Traditional way for text generation (Genest and Lapalme, 2012; Yan et al., 2011) mainly focus on grammars, templates, and so on. But it is usually"
W17-3526,P12-2069,0,0.0303011,"vious studies have attempted for review generation (Tang et al., 2016; Lipton et al., 2015; Dong et al., 2017) . They generate personalized reviews according to an overall rating. But they do not consider the product aspects and whether each generated sentence is produced as the user requires. The models they proposed are very similar to SRGMs. And the length of reviews texts are not as long as ours. Therefore, our work can be regarded as a significant improvement of their researches. Many researches of text generation are also closely related to our work. Traditional way for text generation (Genest and Lapalme, 2012; Yan et al., 2011) mainly focus on grammars, templates, and so on. But it is usually complicated to make every part of the system work and cooperate perfectly following the traditional techniques, while end-to-end generation systems nowadays, like the ones within encoder-decoder framework (Cho et al., 2014; Sordoni et al., 2015), have distinct architectures and achieve promising performances. Moreover, the recent researches on hierarchical structure help a lot with the improvement of the generation systems. Li et al. (2015) experimented on LSTM autoencoders to show the power of the hierarchic"
W17-3526,P15-1107,0,0.468328,"view generation can be really useful and worthy of study. But recent researches on text generation mainly focus on generation of weather reports, financial news, sports news (Konstas, 2014; Kim et al., 2016; Zhang Traditional generation models are mainly based on rules. It is time consuming to handcraft rules. Thanks to the quick development of neural networks and deep learning, text generation has achieved a breakthrough in recent years in many domains, e.g., image-to-text (Karpathy and Fei-Fei, 2015; Xu et al., 2015), video-to-text (Yu et al., 2016), and textto-text (Sutskever et al., 2014; Li et al., 2015), etc. More and more works show that generation models with neural networks can generate meaningful and grammatical texts (Bahdanau et al., 2015; Sutskever et al., 2011). However, recent studies of text generation mainly focus on generating short texts of sentence level. There are still challenges for modern sequential generation models to handle long texts. And yet there is very few work having been done in generating long reviews. In this paper, we aim to address the challenging task of long review generation within the encoderdecoder neural network framework. Based on the encoder-decoder fr"
W17-3526,N16-1014,0,0.028288,"roblems generating texts consistent with input aspect-sentiment scores, while RNNs cannot deal well with long texts. To overcome these obstacles, we proposed models and find that our model with hierarchical structure and aligned attention can produce long reviews with high quality, which outperforms the baseline methods. However, we can notice that there are still some problems in the texts generated by our models. In some generated texts, the contents are not rich enough compared to human-written reviews, which may be improved by applying diversity decoding methods (Vijayakumar et al., 2016; Li et al., 2016). And there are a few logical problems in some generated texts, which may be improved by generative adversarial nets (Goodfellow et al., 2014) or reinforcement learning (Sutton and Barto, 1998). In future work, we will apply our proposed models to text generation in other domains. As mentioned earlier, our models can be easily adapted for other data-to-text generation tasks, if the alignment between structured data and texts can be provided. We hope our work will not only be an exploration of review generation, but also make contributions to general data-to-text generation. Acknowledgments Thi"
W17-3526,P02-1040,0,0.103673,"IDIA TITANX GPU (12G). Because the limitation of our hardware, we only do experiments with one layer of encoder and one layer of LSTM network. The batch size is 4 in HRGMs, and 32 in SRGMs. The initial learning rate is set to 0.5, and we dynamically adjust the learning rate according to the loss value. As experiments show that the size of hidden layer does not affect the results regularly, we set all of them to 500. All the rest parameters in our model can be learned during training. 6 Baselines github.com/tensorflow/tensorflow/tree/r0.10 172 5.3 Automatic Evaluation We used the popular BLEU (Papineni et al., 2002) scores as evaluation metrics and BLEU has shown good consistent with human evaluation in many machine translation and text generation tasks. High BLEU score means many n-grams in the hypothesis texts meets the gold-standard references. Here, we report BLEU-2 to BLEU-4 scores, and the evaluation is conducted after Chinese word segmentation. The only parameters in BLEU is the weights W for n-gram precisions. In this study, we set W as average weights (Wi = n1 for BLEU-n evaluation). As for multiple answers to the same input, we put all of them into the reference set of the input. The results ar"
W17-3526,W14-4009,0,0.0669297,"Missing"
W17-3526,P15-1152,0,0.0261898,"unit that excels at remembering values for either long or short durations of time(Graves, 2012b; Sundermeyer et al., 2012). It contains an input gate, a forget gate, an output gate and a memory cell. Respectively, at time t, we set the above parts as it , ft , ot , ct . In an LSTM network, we propagate as Equation (3)(4)(5).        it WI  ft  = sigmoid WF  × ht−1  xt ot WO (3)    ht−1 ct = it × tanh WC × + ft × ct−1 (4) xt ht = ct × ot (5) In the past few years, many generation models based on LSTM networks have given promising results in different domains (Xu et al., 2015; Shang et al., 2015; Wu et al., 2016). Compared to other network units of RNN, like GRU (Chung et al., 2014), LSTM is considered the best one in most cases. 4 4.1 Review Generation Models Notations We define our task as receiving a vector of aspect-sentiment scores Vs to generate review 170 texts, which is a long sequence of words Y {y1 , y2 , . . . , y|Y |−1 , hEOSi} (hEOSi is the special word representing the end of a sequence). As mentioned in section 2, we also transform an input vector Vs into a series of new input vectors {V1 , V2 , . . . , V8 } with respect to eight aspects for our models. More specifical"
W17-3526,D11-1040,1,0.821213,"ed for review generation (Tang et al., 2016; Lipton et al., 2015; Dong et al., 2017) . They generate personalized reviews according to an overall rating. But they do not consider the product aspects and whether each generated sentence is produced as the user requires. The models they proposed are very similar to SRGMs. And the length of reviews texts are not as long as ours. Therefore, our work can be regarded as a significant improvement of their researches. Many researches of text generation are also closely related to our work. Traditional way for text generation (Genest and Lapalme, 2012; Yan et al., 2011) mainly focus on grammars, templates, and so on. But it is usually complicated to make every part of the system work and cooperate perfectly following the traditional techniques, while end-to-end generation systems nowadays, like the ones within encoder-decoder framework (Cho et al., 2014; Sordoni et al., 2015), have distinct architectures and achieve promising performances. Moreover, the recent researches on hierarchical structure help a lot with the improvement of the generation systems. Li et al. (2015) experimented on LSTM autoencoders to show the power of the hierarchical structured LSTM"
W17-3526,P16-1129,1,0.899885,"Missing"
W18-6545,P17-1124,0,0.0759134,"ing the few MDS training data to further improve the pre-trained model. We conduct experiment on the benchmark DUC datasets, and experiment results demonstrate our approach is able to achieve considerable improvement over a variety of neural baselines. The contributions of this study are summarized as follows: to model MDS as a budgeted maximum coverage problem, including the prior approach (McDonald, 2007) and improved models (Woodsend and Lapata, 2012; Li et al., 2013; Boudin et al., 2015). There are still recent studies under traditional extractive framework (Peyrard and EckleKohler, 2017; Avinesh and Meyer, 2017). • To the best of our knowledge, our work is one of the very few pioneering works to investigate adapting neural abstractive summarization models of single document summarization to the task of multi-document summarization. Abstractive summarization methods aim at generating the summary based on understanding the original documents. Sequence-to-sequence models with attention mechanism have been applied to the abstractive summarization task. Success attempts are on sentence summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016) or single document summarization (Tan et a"
W18-6545,N03-1020,0,0.452089,"Missing"
W18-6545,D15-1220,0,0.0196207,"5-8, 2018. 2018 Association for Computational Linguistics adapt the pre-trained model to the MDS task. We also study leveraging the few MDS training data to further improve the pre-trained model. We conduct experiment on the benchmark DUC datasets, and experiment results demonstrate our approach is able to achieve considerable improvement over a variety of neural baselines. The contributions of this study are summarized as follows: to model MDS as a budgeted maximum coverage problem, including the prior approach (McDonald, 2007) and improved models (Woodsend and Lapata, 2012; Li et al., 2013; Boudin et al., 2015). There are still recent studies under traditional extractive framework (Peyrard and EckleKohler, 2017; Avinesh and Meyer, 2017). • To the best of our knowledge, our work is one of the very few pioneering works to investigate adapting neural abstractive summarization models of single document summarization to the task of multi-document summarization. Abstractive summarization methods aim at generating the summary based on understanding the original documents. Sequence-to-sequence models with attention mechanism have been applied to the abstractive summarization task. Success attempts are on se"
W18-6545,N16-1012,0,0.0427679,"aditional extractive framework (Peyrard and EckleKohler, 2017; Avinesh and Meyer, 2017). • To the best of our knowledge, our work is one of the very few pioneering works to investigate adapting neural abstractive summarization models of single document summarization to the task of multi-document summarization. Abstractive summarization methods aim at generating the summary based on understanding the original documents. Sequence-to-sequence models with attention mechanism have been applied to the abstractive summarization task. Success attempts are on sentence summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016) or single document summarization (Tan et al., 2017; See et al., 2017; Paulus et al., 2017), which have abundant gold summaries to train an end-to-end system. Until very recently, there occurs attempt for abstractive multi-document summarization under the seq2seq framework. The lack of enough train examples is the major obstacle to this end. To address this, Liu et al. (2018) study the task of generating English Wikipedia under a viewpoint of multi-document summarization. They construct a large corpus with reference summaries, so that end-to-end training of a seq2seq i"
W18-6545,W04-3252,0,0.16487,"ne by one in single document or the first document in the document collection, where documents in the collection are assumed to be ordered by name. Coverage: It takes the first sentence one by one from the first document to the last document in the document collection. LexRank: LexRank (Erkan and Radev, 2004) computes sentence importance based on the concept of eigenvector centrality in a graph representation of sentences. In this model, a connectivity matrix based on intra-sentence cosine similarity is used as the adjacency matrix of the graph representation of sentences. TextRank: TextRank (Mihalcea and Tarau, 2004) builds a graph and adds each sentence as vertices, the overlap of two sentences is treated as the relation that connects sentences. Then graphbased ranking algorithm is applied until convergence. Sentences are sorted based on their final score and a greedy algorithm is employed to impose diversity penalty on each sentence and select summary sentences. Centroid: In centroid-based summarization (Radev et al., 2000) method, a pseudo-sentence of the document called centroid is calculated. The centroid consists of words with TF-IDF scores above a predefined threshold. The score of each sentence is"
W18-6545,C16-1316,0,0.0250468,"d Xiaojun Wan Institute of Computer Science and Technology, Peking University The MOE Key Laboratory of Computational Linguistics, Peking University {zhangjianmin2015,tanjiwei,wanxiaojun}@pku.edu.cn Abstract tive methods (Yao et al., 2017). However, extractive methods suffer from the inherent drawbacks of discourse incoherence and long, redundant sentences, which hampers its application in reality (Tan et al., 2017). Recently, with the success of sequence-to-sequence (seq2seq) models in natural language generation tasks including machine translation (Bahdanau et al., 2014) and dialog systems (Mou et al., 2016), abstractive summarization methods has received increasing attention. With the resource of large-scale corpus of human summaries, it is able to train an abstractive summarization model in an end-to-end framework. Neural abstractive summarization models (See et al., 2017; Tan et al., 2017) have surpass the performance of extractive methods on single document summarization task with abundant training data. Unfortunately, the extension of seq2seq models to MDS is not straightforward. Neural abstractive summarization models are usually trained on about hundreds of thousands of gold summaries, but"
W18-6545,P13-1099,0,0.0223922,"rlands, November 5-8, 2018. 2018 Association for Computational Linguistics adapt the pre-trained model to the MDS task. We also study leveraging the few MDS training data to further improve the pre-trained model. We conduct experiment on the benchmark DUC datasets, and experiment results demonstrate our approach is able to achieve considerable improvement over a variety of neural baselines. The contributions of this study are summarized as follows: to model MDS as a budgeted maximum coverage problem, including the prior approach (McDonald, 2007) and improved models (Woodsend and Lapata, 2012; Li et al., 2013; Boudin et al., 2015). There are still recent studies under traditional extractive framework (Peyrard and EckleKohler, 2017; Avinesh and Meyer, 2017). • To the best of our knowledge, our work is one of the very few pioneering works to investigate adapting neural abstractive summarization models of single document summarization to the task of multi-document summarization. Abstractive summarization methods aim at generating the summary based on understanding the original documents. Sequence-to-sequence models with attention mechanism have been applied to the abstractive summarization task. Succ"
W18-6545,K16-1028,0,0.0426944,"Missing"
W18-6545,P15-1107,0,0.0256753,"Missing"
W18-6545,P17-1100,0,0.0430702,"Missing"
W18-6545,D12-1022,0,0.0251391,"1–390, c Tilburg, The Netherlands, November 5-8, 2018. 2018 Association for Computational Linguistics adapt the pre-trained model to the MDS task. We also study leveraging the few MDS training data to further improve the pre-trained model. We conduct experiment on the benchmark DUC datasets, and experiment results demonstrate our approach is able to achieve considerable improvement over a variety of neural baselines. The contributions of this study are summarized as follows: to model MDS as a budgeted maximum coverage problem, including the prior approach (McDonald, 2007) and improved models (Woodsend and Lapata, 2012; Li et al., 2013; Boudin et al., 2015). There are still recent studies under traditional extractive framework (Peyrard and EckleKohler, 2017; Avinesh and Meyer, 2017). • To the best of our knowledge, our work is one of the very few pioneering works to investigate adapting neural abstractive summarization models of single document summarization to the task of multi-document summarization. Abstractive summarization methods aim at generating the summary based on understanding the original documents. Sequence-to-sequence models with attention mechanism have been applied to the abstractive summari"
W18-6545,W00-0403,0,0.845374,"ation results demonstrate the efficacy of our proposed approach, which outperforms a variety of neural baselines. We organize the paper as follows. In Section 2 we introduce related work. In Section 3 we describe the previous neural abstractive summarization model. Then we introduce our proposed approach in Section 4. Experiment results and discussion are presented in Section 5. Finally, we conclude this paper in Section 6. 2 2.1 Related Work Extractive Summarization Methods The study of MDS is pioneered by (McKeown and Radev, 1995), and early notable works also include (McKeown et al., 1999; Radev et al., 2000). Extractive summarization systems that compose a summary from a number of important sentences from the source documents are by far the most popular solution for MDS (Avinesh and Meyer, 2017). Redundancy is one of the biggest problems for extractive methods (Gambhir and Gupta, 2017), and the Maximal Marginal Relevance (MRR) (Carbonell and Goldstein, 1998) is a well-known algorithm for reducing redundancy. In the past years various models under extractive framework have been proposed (Tao et al., 2008; Wan and Yang, 2008; Wang et al., 2011; Tan et al., 2015). One important architecture is 3 Abs"
W18-6545,D15-1044,0,0.0351295,"nt studies under traditional extractive framework (Peyrard and EckleKohler, 2017; Avinesh and Meyer, 2017). • To the best of our knowledge, our work is one of the very few pioneering works to investigate adapting neural abstractive summarization models of single document summarization to the task of multi-document summarization. Abstractive summarization methods aim at generating the summary based on understanding the original documents. Sequence-to-sequence models with attention mechanism have been applied to the abstractive summarization task. Success attempts are on sentence summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016) or single document summarization (Tan et al., 2017; See et al., 2017; Paulus et al., 2017), which have abundant gold summaries to train an end-to-end system. Until very recently, there occurs attempt for abstractive multi-document summarization under the seq2seq framework. The lack of enough train examples is the major obstacle to this end. To address this, Liu et al. (2018) study the task of generating English Wikipedia under a viewpoint of multi-document summarization. They construct a large corpus with reference summaries, so that end-to-end tr"
W18-6545,P17-1099,0,0.35057,"fer from the inherent drawbacks of discourse incoherence and long, redundant sentences, which hampers its application in reality (Tan et al., 2017). Recently, with the success of sequence-to-sequence (seq2seq) models in natural language generation tasks including machine translation (Bahdanau et al., 2014) and dialog systems (Mou et al., 2016), abstractive summarization methods has received increasing attention. With the resource of large-scale corpus of human summaries, it is able to train an abstractive summarization model in an end-to-end framework. Neural abstractive summarization models (See et al., 2017; Tan et al., 2017) have surpass the performance of extractive methods on single document summarization task with abundant training data. Unfortunately, the extension of seq2seq models to MDS is not straightforward. Neural abstractive summarization models are usually trained on about hundreds of thousands of gold summaries, but there are usually very few human summaries available for the MDS task. More specifically, in the news domain, there is only a few hundred multi-document summaries provided by DUC and TAC conferences in total, which are largely insufficient for training neural abstractiv"
W18-6545,P17-1108,1,0.866675,"Missing"
