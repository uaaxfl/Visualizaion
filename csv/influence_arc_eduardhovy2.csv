1994.amta-1.10,H93-1038,1,0.792898,"Missing"
1994.amta-1.10,1993.tmi-1.4,1,0.733597,"Missing"
1994.amta-1.18,W90-0108,0,0.0148409,"source text and does not, for the most part, include implicit information necessary for target language generation. The job of the inference module is to make that implicit material explicit. Currently, inference is the least developed part of the working system; its only jobs are to reorganize relative clauses and to insert topical items into semantic roles. 3.6 Semantic Ranking (SENSUS) Semantic analysis has now given us a set of candidate meanings, in terms of concepts and relations from our knowledge base. This inventory of concepts is a synthesis of resources like the PENMAN Upper Model (Bateman 1990), ONTOS (Carlson & Nirenburg 1990), the WordNet thesaurus (Miller 1990), and the Longman Dictionary of Contemporary English (Longman-Group 1978). It contains roughly 70,000 terms organized into inheritance networks; (Knight & Luk 1994) describe its construction. The concept inventory and its functional interface are part of a system called SENSUS. The other part is a set of constraints on which concepts are naturally related to which others in our world. Thus, SENSUS is a world model, (mostly) independent of particular natural languages. It can be used to rank candidate meanings produced by an"
1994.amta-1.18,H94-1028,0,0.057023,"Missing"
1994.amta-1.18,J93-2003,0,0.00533199,"mple translations from our system appear in Section 4. 2 System Design: Philosophy At an abstract level, all MT systems are composed of two types of components: transformers (T) and ranker/pruners (R). A T-component takes some input structure and outputs zero or more new structures. The inputs and outputs may be of the same type (e.g., string -> string) or different types (e.g., string —> parse tree). An R-component takes a set of structures of the same type, assigns each structure a score, then prunes away some number of low-scoring structures. For example, the statistical MT system Candide (Brown et al. 1993) operates in a T-R-R fashion. A French string is transformed into many thousands of French-English string pairs. These translations are ranked and pruned first by a translation model, then by an English-only language model. Both rankers are &quot;soft&quot;, never assigning a zero score. The final pruning leaves only a single translation. The LINGSTAT system (Yamron et al. 1994), works with a T-R-T-R design. The first T-component transforms a Japanese sentence into many possible parse trees, as stipulated by a context-free grammar. A statistical ranker chooses one &quot;best&quot; tree and prunes the rest. A seco"
1994.amta-1.18,P85-1008,0,0.00997741,"mantic rules look like: ((NP -> S NP) ((X2 syn form) = (*NOT* rentaidome)) ((X0 sem instance) = rc-modified-object) ((X0 sem head) = (X2 sem)) ((X0 sem rel-mod) = (X1 sem)) (*OR* (((X1 map subject-role) =c X2)) (((X1 map object-role) =c X2)) (((X1 map object2-role) =c X2)))) The current semantic rule base contains at least one rule for every syntactic rule in the grammar. The output of semantic analysis is represented in a number of ways. Inside the analyzer, semantic information takes the form of directed acyclic graphs. For semantic ranking, we view the graphs as lists of assertions, as in (Hobbs 1985). We also use the SPL format of the PENMAN generation system (Penman 1989). Here is a sample output, representing the sentence &quot;the new company plans to launch in February&quot;: (|b-709 |/ (have as a goal| :SENSER (|c-710 |/ |company/business| :Q-MOD (|n-711 |/ |newˆvirgin|)) :PHENOMENON (|f-712 |/ |found, launch| :TEMPORAL-LOCATING (|c-713 |/ |calendar month |:MONTH-INDEX 2) :AGENT |c-710|) :THEME |c-710|) 137 The acquisition of a large Japanese semantic lexicon is a difficult task. We are tackling this problem with a combination of automatic and manual techniques. (Okumura & Hovy 1994) and (Knig"
1994.amta-1.18,P94-1045,0,0.0374657,"un phrase (X2). It also tests an inflectional feature of the clause, inherits syntactic features from the child noun phrase to the parent, and adds new features. HAX outputs a parse forest, represented as a list of constituents with features that were assigned to them during parsing. A full parse may or may not be found. We strive for full parses whenever possible, because we cannot rely on deep semantics to patch everything up. Since one unanticipated word or punctuation mark may splinter the input into four or five pieces, we are working on techniques for word-skipping, inspired in part by (Lavie 1994). Ongoing research project. We are investigating methods to repair trouble spots in grammatical analysis. In particular, we are looking at statistical differences between fully-parsed and not-fully-parsed sentences. These differences include relative distributions of part-of-speech bigrams. Our goal is to provide automatic feedback for grammar development and to identify word skipping or tag repair possibilities for exploitation at runtime. ______________________________________________________________________________ 3.4 Semantic Analyzer (CAPULET) Like our parser HAX, the semantic analyzer C"
1994.amta-1.18,P89-1005,0,0.0136227,"nalyzer (CAPULET) Like our parser HAX, the semantic analyzer CAPULET operates bottom-up. It assigns zero or more semantic interpretations to each syntactic constituent in the HAX parse forest. At the bottom, word meanings are retrieved from a semantic lexicon. Moving up the parse trees, meanings of constituents are computed compositionally from the meanings of their children, according to a database of semantic combination rules. Semantic rules have the same format as syntactic rules; they are keyed to one another via their context-free parts, in the style of (Dowty, Wall, & Peters 1981) and (Moore 1989). Semantic rules look like: ((NP -> S NP) ((X2 syn form) = (*NOT* rentaidome)) ((X0 sem instance) = rc-modified-object) ((X0 sem head) = (X2 sem)) ((X0 sem rel-mod) = (X1 sem)) (*OR* (((X1 map subject-role) =c X2)) (((X1 map object-role) =c X2)) (((X1 map object2-role) =c X2)))) The current semantic rule base contains at least one rule for every syntactic rule in the grammar. The output of semantic analysis is represented in a number of ways. Inside the analyzer, semantic information takes the form of directed acyclic graphs. For semantic ranking, we view the graphs as lists of assertions, as"
1994.amta-1.18,H94-1026,0,0.0215113,"finitive knowledge is missing? There are many approaches to these questions. Our working hypotheses are (1) a great deal of useful knowledge can be extracted from online dictionaries and text; and (2) statistical methods, properly integrated, can effectively fill knowledge gaps until better knowledge bases or linguistic theories arrive. This paper describes completed and ongoing research on these hypotheses. This research is tightly coupled with our development effort on a large-scale Japanese-English MT system, as part of the ARPAsponsored PANGLOSS project (NMSU/CRL, USC/ISI, & CMU/CMT 1994; Nirenburg & Frederking 1994; Knight & Luk 1994). Sample translations from our system appear in Section 4. 2 System Design: Philosophy At an abstract level, all MT systems are composed of two types of components: transformers (T) and ranker/pruners (R). A T-component takes some input structure and outputs zero or more new structures. The inputs and outputs may be of the same type (e.g., string -> string) or different types (e.g., string —> parse tree). An R-component takes a set of structures of the same type, assigns each structure a score, then prunes away some number of low-scoring structures. For example, the statist"
1994.amta-1.18,C92-3168,0,0.0264904,"ract We summarize recent machine translation (MT) research at the Information Sciences Institute of USC, and we describe its application to the development of a Japanese-English newspaper MT system. Our work aims at scaling up grammar-based, knowledge-based MT techniques. This scale-up involves the use of statistical methods, both in acquiring effective knowledge resources and in making reasonable linguistic choices in the face of knowledge gaps. 1 Goals Knowledge-based machine translation (KBMT) techniques have yielded high quality MT systems in narrow problem domains (Nirenburg et al. 1992; Nyberg & Mitamura 1992). This high quality is delivered by algorithms and resources that permit some access to the meaning of texts. But can KBMT be scaled up to unrestricted newspaper articles? We believe it can, provided we address two additional questions: 1. In constructing a KBMT system, how can we acquire knowledge resources (lexical, grammatical, conceptual) on a large scale? 2. In applying a KBMT system, what do we do when definitive knowledge is missing? There are many approaches to these questions. Our working hypotheses are (1) a great deal of useful knowledge can be extracted from online dictionaries and"
1994.amta-1.18,1994.amta-1.23,1,0.809697,"assertions, as in (Hobbs 1985). We also use the SPL format of the PENMAN generation system (Penman 1989). Here is a sample output, representing the sentence &quot;the new company plans to launch in February&quot;: (|b-709 |/ (have as a goal| :SENSER (|c-710 |/ |company/business| :Q-MOD (|n-711 |/ |newˆvirgin|)) :PHENOMENON (|f-712 |/ |found, launch| :TEMPORAL-LOCATING (|c-713 |/ |calendar month |:MONTH-INDEX 2) :AGENT |c-710|) :THEME |c-710|) 137 The acquisition of a large Japanese semantic lexicon is a difficult task. We are tackling this problem with a combination of automatic and manual techniques. (Okumura & Hovy 1994) and (Knight & Luk 1994) describe algorithms for using bilingual dictionaries to propose links between non-English words and concepts in a knowledge base. We have also built an interface called the Acquisitor, which allows a person to identify word-concept links by conceptually tagging words in context. This provides a distribution of senses for each word and potentially offers data for topic-based disambiguation algorithms. This acquisition work requires substantial resources; at present we are working with five part-time knowledge enterers (&quot;acquisitors&quot;). Another issue is handling unknown w"
1994.amta-1.18,H94-1029,0,0.0143834,"ypes (e.g., string —> parse tree). An R-component takes a set of structures of the same type, assigns each structure a score, then prunes away some number of low-scoring structures. For example, the statistical MT system Candide (Brown et al. 1993) operates in a T-R-R fashion. A French string is transformed into many thousands of French-English string pairs. These translations are ranked and pruned first by a translation model, then by an English-only language model. Both rankers are &quot;soft&quot;, never assigning a zero score. The final pruning leaves only a single translation. The LINGSTAT system (Yamron et al. 1994), works with a T-R-T-R design. The first T-component transforms a Japanese sentence into many possible parse trees, as stipulated by a context-free grammar. A statistical ranker chooses one &quot;best&quot; tree and prunes the rest. A second T-component turns that tree into a set of possible English translations, using a bilingual dictionary and word ordering rules. These translations are ranked by a language model similar to the one employed by Candide. 134 KBMT systems typically use a T-T-R-T sequence. First, the source text is turned into a set of syntactic analyses. These analyses are typically not"
1994.amta-1.18,H94-1096,0,\N,Missing
1994.amta-1.23,W90-0108,0,0.194313,"Missing"
1994.amta-1.23,H92-1052,1,0.88897,"Missing"
1994.amta-1.23,H93-1036,0,0.0277464,"Missing"
1997.mtsummit-workshop.7,C94-1052,0,0.0475183,"Missing"
1997.mtsummit-workshop.7,1994.amta-1.23,1,0.726399,"Missing"
1997.tmi-1.6,P88-1020,1,0.745262,"Missing"
1997.tmi-1.6,1994.amta-1.18,1,0.887412,"Missing"
1997.tmi-1.6,W94-0315,0,0.0370838,"Missing"
1997.tmi-1.6,A92-1006,0,0.0280251,"Missing"
1997.tmi-1.6,1996.amta-1.2,0,0.0363292,"Missing"
2003.mtsummit-papers.12,W02-1607,0,0.307446,"Missing"
2003.mtsummit-papers.12,2003.mtsummit-papers.12,1,0.0530913,"Missing"
2003.mtsummit-papers.12,N01-1006,0,0.150298,"Missing"
2003.mtsummit-papers.12,1992.tmi-1.9,0,0.434408,"Missing"
2003.mtsummit-papers.12,J97-2004,0,0.253428,"Missing"
2003.mtsummit-papers.12,P91-1017,0,0.417391,"Missing"
2003.mtsummit-papers.12,P02-1040,0,0.0742788,"Missing"
2003.mtsummit-papers.30,1999.mtsummit-1.85,0,0.0275928,"Missing"
2003.mtsummit-papers.30,W99-0604,0,0.0616643,"Missing"
2003.mtsummit-papers.30,rajman-hartley-2002-automatic,0,0.053801,"Missing"
2003.mtsummit-papers.30,1994.amta-1.25,0,0.106339,"Missing"
2005.sigdial-1.25,A00-2001,1,0.787421,"evan e of events to an agent's goals and for assessing ausal attributions. Plan representations also lie at the heart of many reasoning te hniques (e.g., planning, explanation, natural language pro essing) and fa ilitate their integration. The de ision-theoreti on epts of utility and probability are key for modeling nondeterminism and for assessing the value of alternative negotiation hoi es. Expli it representations of intentions and beliefs are riti al for negotiation and for assessing blame when negotiations fail (Mao and Grat h, 2004). 3.1 Modeling Trust A ording to the dialogue model in (Matheson et al., 2000), the dire t e e t of an assertion is the introdu tion of a ommitment, whether or not either party believes in the assertion. While this is suÆ ient for reasoning about the laims and responsibility for information, we need to go further and potentially hange beliefs and intentions based on ommuni ated information. Trust is used to de ide whether to adopt a new belief based on the ommitments of another. Similar to (Marsella et al., 2004) and (Cassell and Bi kmore, 2001) , trust is modeled as fun tion of underlying variables that are easily derived from our task and dialogue representations. Sol"
2005.sigdial-1.25,traum-etal-2004-evaluation,1,0.813564,"whole body posture of the do tor and use of gestures and expressions as well. For example, when the do tor is feeling more distant and less trusting, he adopts a losed posture (Figure 1). When he is more trusting and open to negotiation, the posture be omes more relaxed (Figure 4). Figure 4: More relaxed and open do tor 5 Current and Future Work The virtual do tor is able to engage in a range of dialogue in this domain similar to those in Figures 2 and 3. Current work involves extensions and evaluation of the ability to robustly engage in this sort of dialogue, following the methodologies in (Traum et al., 2004). Wizard of OZ tests show good results in terms of the ability to have produ tive onversations given the do tor's task model, vo abulary and generation apa ity, but we are still evaluating performan e of the automated system. Future work involves extension of the models to in lude additional negotiation strategies, emotionbased styles of intera tion within the strategies, and appli ation to other s enarios, some involving ultural di eren es in behavior and interpretation, as well as translated and multi-lateral dialogue. A knowledgments The work des ribed in this paper was supported by the Dep"
2020.acl-main.473,E17-1003,0,0.0435235,"Missing"
2020.acl-main.473,W18-5042,0,0.0198791,"Missing"
2020.acl-main.473,W10-3001,0,0.126177,"Missing"
2020.acl-main.473,D13-1181,0,0.119392,"tween people’s language and their forecasting skill. To the best of our knowledge, this is the first work that presents a computational way of exploring this direction. Our work is also closely related to prior research on predicting various phenomenon from users’ language. For example Tan et al. (2014) study the effect of wording on message propagation, Gillick and Bamman (2018) examine the connection between language used by politicians in campaign speeches and applause and P´erez-Rosas and Mihalcea (2015) explored linguistic differences between truthful and deceptive statements. Ganjigunte Ashok et al. (2013) show linguistic cues drawn from authors’ language are strong indicators of the success of their books and Tsur and Rappoport (2009) presented an unsupervised model to analyze the helpfulness of book reviews by analyzing their text. There have been several studies using data from Good Judgment Open or Good Judgment Project (Mellers et al., 2015b). One recent study examining the language side of this data is Schwartz et al. (2017). Their main goal is to suggest objective metrics as alternatives for subjective ratings when evaluating the quality of recommendations. To achieve this, justification"
2020.acl-main.473,N09-1031,0,0.0383996,"rk has also studied persuasive language on crowdfunding platforms (Yang et al., 2019). In contrast, our work focuses on directly measuring forecasting skill based on text justifications. Finally we note that there is a long history of research on financial analysts’ forecasting ability (Crichfield et al., 1978; Chopra, 1998; Loh and Mian, 2006). Most work relies on regression models to test if pre-identified factors are correlated with forecasting skill (e.g., Loh and Mian (2006); Call et al. (2009)). Some work has also explored the use of textual information in financial domain. For example, Kogan et al. (2009) present a study of predicting companies’ risk by using financial reports. We also note a recent paper on studying financial analysts’ decision making process by using text-based features from earning calls (Keith and Stent, 2019). As far as we aware, our work is the first to evaluate analysts’ forecasting skill based on their language. 6 Limitations and Future Work Our experiments demonstrated it is possible to analyze language to estimate people’s skill at making predictions about the future. In this section we 5324 highlight several limitations of our study and ethical issues that should be"
2020.acl-main.473,P14-1018,0,0.0791703,"Missing"
2020.acl-main.473,D18-1004,0,0.0273447,"in addition to rates of heart disease. Demszky et al. (2019) analyzed political polarization in social media and Voigt et al. (2017) examined the connections between police officers’ politeness and race by analyzing language. A number of studies (De Choudhury et al., 2014; Eichstaedt et al., 2018; Benton et al., 2017; Park et al., 2017) have examined the connection between users’ language on social media and depression and alcohol use (Kiciman et al., 2018). Other work has analyzed users’ language to study the effect of attributes, such as gender, in online communication (Bamman et al., 2014; Wang and Jurgens, 2018; Voigt et al., 2018). In this work we study the relationship between people’s language and their forecasting skill. To the best of our knowledge, this is the first work that presents a computational way of exploring this direction. Our work is also closely related to prior research on predicting various phenomenon from users’ language. For example Tan et al. (2014) study the effect of wording on message propagation, Gillick and Bamman (2018) examine the connection between language used by politicians in campaign speeches and applause and P´erez-Rosas and Mihalcea (2015) explored linguistic di"
2020.acl-main.473,H05-1044,0,0.0106245,"language. In this paper, we present the first systematic study of the connection between language and forecasting ability. To do so, we analyze texts written by top forecasters (ranked by accuracy against ground truth) in two domains: geopolitical forecasts from an online prediction forum, and company earnings forecasts made by financial analysts. To shed light on the differences in approach employed by skilled and unskilled forecasters, we investigate a variety of linguistic metrics. These metrics are computed using natural language processing methods to analyze sentiment (Pang et al., 2002; Wilson et al., 2005), uncertainty (de Marneffe et al., 2012; Saur´ı and Pustejovsky, 2012), readability, etc. In addition we make use of word lists taken from the Linguistic Inquiry and Word Count (LIWC) software (Tausczik and Pennebaker, 2010), which is widely used in psychological research. By analyzing forecasters’ texts, we are able to provide evidence to support or refute hypotheses about factors that may influence forecasting skill. For example, we show forecasters whose justifications contain a higher proportion of uncertain statements tend to make more accurate predictions. This supports the hypothesis th"
2020.acl-main.473,N19-1364,1,0.794096,"dgment Project (Mellers et al., 2015b). One recent study examining the language side of this data is Schwartz et al. (2017). Their main goal is to suggest objective metrics as alternatives for subjective ratings when evaluating the quality of recommendations. To achieve this, justifications written by one group are provided as tips to another group. These justifications are then evaluated on their ability to persuade people to update their predictions, leading to real benefits that can be measured by objective metrics. Prior work has also studied persuasive language on crowdfunding platforms (Yang et al., 2019). In contrast, our work focuses on directly measuring forecasting skill based on text justifications. Finally we note that there is a long history of research on financial analysts’ forecasting ability (Crichfield et al., 1978; Chopra, 1998; Loh and Mian, 2006). Most work relies on regression models to test if pre-identified factors are correlated with forecasting skill (e.g., Loh and Mian (2006); Call et al. (2009)). Some work has also explored the use of textual information in financial domain. For example, Kogan et al. (2009) present a study of predicting companies’ risk by using financial"
2020.acl-main.502,D19-1454,0,0.0561322,"Missing"
2020.acl-main.502,L16-1649,0,0.0303937,"−1 , ck , si+1 , . . . , sn ). Due to limited number of passages available to train a robust LM, Transformer-XL (TR.XL) Base (Dai et al., 2019), trained on WikiText-103, is employed to address this task. In order to make decoding time tractable, context length is limited to three sentences before and after the blank. 4.4 Coherence Coherence models assign a continuous score to a sentence sequence indicative of its coherence. This score is usually unnormalized and not needed to be a probability [unlike language models]. We use the local coherence approaches implemented by the COHERE7 framework (Smith et al., 2016). Roughly, this model works on the intuition that successive sentences exhibit regularities in syntactic patterns. Specifically, it uses ngram patterns on linearized syntactic parses (e.g. S NP VP . . . ) of consecutive sentences. Once 5672 7 github.com/karins/CoherenceFramework trained, this model can return a “coherence score” for any sentence sequence. The COHERE model is first trained on all ground-truth passages from our training set, with the ground truth answers filled into the blanks. At test-time, we score each possible answer permutation using the trained COHERE model and pick the hi"
2020.acl-main.502,W17-2623,0,0.032059,"Missing"
2020.acl-main.502,D18-1257,1,0.630977,"Position kContextkw SCDE 3 3 Human Shared Anywhere 319 ROCS TORIES (2016) CLOTH (2018) LAMBADA (2016) CBT (2015) MRSCC (2011) 3 × × × × × 3 × × × Human Human Exhaustive Automatic Human Separated - End Anywhere End End Anywhere 25 243 76 465 20 Table 2: Comparing SCDE with previous cloze datasets. Exhaustive denotes the case where the entire vocabulary is a candidate for a word level cloze. For the single-blank case, candidate sharing is irrelevant. SL and MB mean sentence level and multi-blanks respectively. kContextkw is the average token length of the context. word level blanks. The CLOTH (Xie et al., 2018) dataset collects word level cloze questions from English exams designed by teachers. MRSCC (Zweig and Burges, 2011) consists of 1,040 word level cloze questions created by human annotators. Among recent cloze datasets, ROCStories (Mostafazadeh et al., 2016) is the closest we could find to a sentence level cloze dataset. In this task, the first 4 sentences of a 5-sentence story are provided, and the task is to choose the correct ending from a pair of candidate ending sentences. However, there are several key differences between SCDE and ROCStories. Firstly, there are multiblanks in SCDE which"
2020.acl-main.502,J90-1003,0,\N,Missing
2020.acl-main.502,S14-2001,0,\N,Missing
2020.acl-main.502,P16-1223,0,\N,Missing
2020.acl-main.502,N16-1098,0,\N,Missing
2020.acl-main.502,D16-1241,0,\N,Missing
2020.acl-main.502,D17-1082,1,\N,Missing
2020.acl-main.502,D17-1070,0,\N,Missing
2020.acl-main.502,N18-2017,0,\N,Missing
2020.acl-main.667,P15-1017,0,0.198647,"roduction Event argument detection is a key component in the task of event extraction. It resembles semantic role labeling (SRL) in that the main target is to find argument spans to fill the roles of event frames. However, event arguments can go beyond sentence boundaries: there can be non-local or implicit arguments at the document level. Figure 1 shows such an example: for the purchase event, which is triggered by the word “bought”, its money argument appears in the previous sentence. Implicit arguments have been under-explored in event extraction. Most of previous systems (Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Wang et al., 2019) only consider local arguments in the same sentence of the event trigger. While incorporating implicit arguments requires corresponding annotations, few exists in most of the widely used event datasets, like ACE2005 (LDC, 2005; Walker et al., 2006) and RichERE (LDC, 2015). There are several annotation efforts for implicit arguments ... still bought the more ... ... 1 0 1 1 ... Figure 1: Examples of implicit arguments and model illustration. The bold text indicates the trigger word for the purchase event, while the underlined text indicates its non-local"
2020.acl-main.667,N19-1423,0,0.0256754,"ing the head-words of the arguments, and adopting a second step of head-to-span expansion. Actually, this type of two-step setup is not uncommon in prior work of information extraction, including entity detection (Lin et al., 2019), coreference resolution (Peng et al., 2015) and document-level pseudo-coreference (Jauhar et al., 2015; Liu et al., 2016). By considering only individual tokens in the detection step, the system only needs to handle a candidate space whose size scales linearly in respective to the number of tokens instead of quadratically. With the same setting of fine-tuning BERT (Devlin et al., 2019) encoder, we show the effectiveness of our model by obtaining overall better results than a strong sequence-labeling model. We further provide detailed error analysis, showing that the main difficulties of the task are upon non-local and non-core arguments. Our analysis shows that the implicit argument task is quite challenging, calling for more future work on document-level semantic understanding for this task. 2 Model The goal of event argument detection is to create labeled links between argument spans and the predicate (event trigger). Recent state-of-the-art solutions for sentence-level S"
2020.acl-main.667,P18-2077,0,0.025657,"Missing"
2020.acl-main.667,2020.acl-main.718,0,0.400652,"Missing"
2020.acl-main.667,E14-4044,0,0.0472898,"Missing"
2020.acl-main.667,P10-1160,0,0.0218751,"equires corresponding annotations, few exists in most of the widely used event datasets, like ACE2005 (LDC, 2005; Walker et al., 2006) and RichERE (LDC, 2015). There are several annotation efforts for implicit arguments ... still bought the more ... ... 1 0 1 1 ... Figure 1: Examples of implicit arguments and model illustration. The bold text indicates the trigger word for the purchase event, while the underlined text indicates its non-local “money” argument in the previous sentence. Our model first detects the head-word “dollars”, and then expands it to the whole span. in SRL, including G&C (Gerber and Chai, 2010, 2012), SemEval-2010 (Ruppenhofer et al., 2009, 2010), and 80Days (Feizabadi and Pad´o, 2014). Yet most are performed with different ontologies such as Nombank (G&C) and FrameNet (SemEval-2010 and 80Days); on different domains (e.g. novels); and in smaller scales (G&C and 80Days only cover 10 types of predicates). The lack of annotations poses challenges to train and transfer implicit argument models for event extraction. Recently, Ebner et al. (2020) create the Roles Across Multiple Sentences (RAMS) dataset, which covers multi-sentence implicit arguments for a wide range of event and role ty"
2020.acl-main.667,J12-4003,0,0.0212957,"Missing"
2020.acl-main.667,P18-2058,0,0.0187308,"etter results than a strong sequence-labeling model. We further provide detailed error analysis, showing that the main difficulties of the task are upon non-local and non-core arguments. Our analysis shows that the implicit argument task is quite challenging, calling for more future work on document-level semantic understanding for this task. 2 Model The goal of event argument detection is to create labeled links between argument spans and the predicate (event trigger). Recent state-of-the-art solutions for sentence-level SRL perform the detection in an end-to-end setting, such as span-based (He et al., 2018; Ouchi et al., 2018), and sequence labeling models (He et al., 2017; Shi and Lin, 2019). However, span-based models face great challenges when considering arguments across sentence boundaries, since the computational complexity of such models grows quadratically to deal with O(N 2 ) span candidates given N tokens. While traditional sequence labeling models can run in linear-time, they are less flexible and extensible in complex scenarios like overlapping mentions and multiple roles for one mention. In this work, we take a two-step approach that decomposes the problem explicitly into two sub-p"
2020.acl-main.667,P17-1044,0,0.0296193,"ide detailed error analysis, showing that the main difficulties of the task are upon non-local and non-core arguments. Our analysis shows that the implicit argument task is quite challenging, calling for more future work on document-level semantic understanding for this task. 2 Model The goal of event argument detection is to create labeled links between argument spans and the predicate (event trigger). Recent state-of-the-art solutions for sentence-level SRL perform the detection in an end-to-end setting, such as span-based (He et al., 2018; Ouchi et al., 2018), and sequence labeling models (He et al., 2017; Shi and Lin, 2019). However, span-based models face great challenges when considering arguments across sentence boundaries, since the computational complexity of such models grows quadratically to deal with O(N 2 ) span candidates given N tokens. While traditional sequence labeling models can run in linear-time, they are less flexible and extensible in complex scenarios like overlapping mentions and multiple roles for one mention. In this work, we take a two-step approach that decomposes the problem explicitly into two sub-problems, based on the hypothesis that head-words can usually capture"
2020.acl-main.667,S15-1035,0,0.0203305,"dency-based SRL (Surdeanu et al., 2008; Hajiˇc et al., 2009), we take the syntactical head-words as the proxy for full argument spans, hypothesizing that the head-words can contain enough information to fill the argument roles. Based on this, we adopt a two-step approach: first detecting the head-words of the arguments, and adopting a second step of head-to-span expansion. Actually, this type of two-step setup is not uncommon in prior work of information extraction, including entity detection (Lin et al., 2019), coreference resolution (Peng et al., 2015) and document-level pseudo-coreference (Jauhar et al., 2015; Liu et al., 2016). By considering only individual tokens in the detection step, the system only needs to handle a candidate space whose size scales linearly in respective to the number of tokens instead of quadratically. With the same setting of fine-tuning BERT (Devlin et al., 2019) encoder, we show the effectiveness of our model by obtaining overall better results than a strong sequence-labeling model. We further provide detailed error analysis, showing that the main difficulties of the task are upon non-local and non-core arguments. Our analysis shows that the implicit argument task is qu"
2020.acl-main.667,P13-1008,0,0.0466315,". 0 0 0 0 ... Introduction Event argument detection is a key component in the task of event extraction. It resembles semantic role labeling (SRL) in that the main target is to find argument spans to fill the roles of event frames. However, event arguments can go beyond sentence boundaries: there can be non-local or implicit arguments at the document level. Figure 1 shows such an example: for the purchase event, which is triggered by the word “bought”, its money argument appears in the previous sentence. Implicit arguments have been under-explored in event extraction. Most of previous systems (Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Wang et al., 2019) only consider local arguments in the same sentence of the event trigger. While incorporating implicit arguments requires corresponding annotations, few exists in most of the widely used event datasets, like ACE2005 (LDC, 2005; Walker et al., 2006) and RichERE (LDC, 2015). There are several annotation efforts for implicit arguments ... still bought the more ... ... 1 0 1 1 ... Figure 1: Examples of implicit arguments and model illustration. The bold text indicates the trigger word for the purchase event, while the underlined text indi"
2020.acl-main.667,P19-1511,0,0.0380481,"Missing"
2020.acl-main.667,W16-0705,1,0.831426,"eanu et al., 2008; Hajiˇc et al., 2009), we take the syntactical head-words as the proxy for full argument spans, hypothesizing that the head-words can contain enough information to fill the argument roles. Based on this, we adopt a two-step approach: first detecting the head-words of the arguments, and adopting a second step of head-to-span expansion. Actually, this type of two-step setup is not uncommon in prior work of information extraction, including entity detection (Lin et al., 2019), coreference resolution (Peng et al., 2015) and document-level pseudo-coreference (Jauhar et al., 2015; Liu et al., 2016). By considering only individual tokens in the detection step, the system only needs to handle a candidate space whose size scales linearly in respective to the number of tokens instead of quadratically. With the same setting of fine-tuning BERT (Devlin et al., 2019) encoder, we show the effectiveness of our model by obtaining overall better results than a strong sequence-labeling model. We further provide detailed error analysis, showing that the main difficulties of the task are upon non-local and non-core arguments. Our analysis shows that the implicit argument task is quite challenging, ca"
2020.acl-main.667,N16-1034,0,0.189237,"ument detection is a key component in the task of event extraction. It resembles semantic role labeling (SRL) in that the main target is to find argument spans to fill the roles of event frames. However, event arguments can go beyond sentence boundaries: there can be non-local or implicit arguments at the document level. Figure 1 shows such an example: for the purchase event, which is triggered by the word “bought”, its money argument appears in the previous sentence. Implicit arguments have been under-explored in event extraction. Most of previous systems (Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Wang et al., 2019) only consider local arguments in the same sentence of the event trigger. While incorporating implicit arguments requires corresponding annotations, few exists in most of the widely used event datasets, like ACE2005 (LDC, 2005; Walker et al., 2006) and RichERE (LDC, 2015). There are several annotation efforts for implicit arguments ... still bought the more ... ... 1 0 1 1 ... Figure 1: Examples of implicit arguments and model illustration. The bold text indicates the trigger word for the purchase event, while the underlined text indicates its non-local “money” argument in"
2020.acl-main.667,D18-1191,0,0.138174,"n a strong sequence-labeling model. We further provide detailed error analysis, showing that the main difficulties of the task are upon non-local and non-core arguments. Our analysis shows that the implicit argument task is quite challenging, calling for more future work on document-level semantic understanding for this task. 2 Model The goal of event argument detection is to create labeled links between argument spans and the predicate (event trigger). Recent state-of-the-art solutions for sentence-level SRL perform the detection in an end-to-end setting, such as span-based (He et al., 2018; Ouchi et al., 2018), and sequence labeling models (He et al., 2017; Shi and Lin, 2019). However, span-based models face great challenges when considering arguments across sentence boundaries, since the computational complexity of such models grows quadratically to deal with O(N 2 ) span candidates given N tokens. While traditional sequence labeling models can run in linear-time, they are less flexible and extensible in complex scenarios like overlapping mentions and multiple roles for one mention. In this work, we take a two-step approach that decomposes the problem explicitly into two sub-problems, based on the"
2020.acl-main.667,K15-1002,0,0.0294218,"oses great challenges for the detection. Inspired by dependency-based SRL (Surdeanu et al., 2008; Hajiˇc et al., 2009), we take the syntactical head-words as the proxy for full argument spans, hypothesizing that the head-words can contain enough information to fill the argument roles. Based on this, we adopt a two-step approach: first detecting the head-words of the arguments, and adopting a second step of head-to-span expansion. Actually, this type of two-step setup is not uncommon in prior work of information extraction, including entity detection (Lin et al., 2019), coreference resolution (Peng et al., 2015) and document-level pseudo-coreference (Jauhar et al., 2015; Liu et al., 2016). By considering only individual tokens in the detection step, the system only needs to handle a candidate space whose size scales linearly in respective to the number of tokens instead of quadratically. With the same setting of fine-tuning BERT (Devlin et al., 2019) encoder, we show the effectiveness of our model by obtaining overall better results than a strong sequence-labeling model. We further provide detailed error analysis, showing that the main difficulties of the task are upon non-local and non-core argument"
2020.acl-main.667,W09-2417,0,0.109065,"s in most of the widely used event datasets, like ACE2005 (LDC, 2005; Walker et al., 2006) and RichERE (LDC, 2015). There are several annotation efforts for implicit arguments ... still bought the more ... ... 1 0 1 1 ... Figure 1: Examples of implicit arguments and model illustration. The bold text indicates the trigger word for the purchase event, while the underlined text indicates its non-local “money” argument in the previous sentence. Our model first detects the head-word “dollars”, and then expands it to the whole span. in SRL, including G&C (Gerber and Chai, 2010, 2012), SemEval-2010 (Ruppenhofer et al., 2009, 2010), and 80Days (Feizabadi and Pad´o, 2014). Yet most are performed with different ontologies such as Nombank (G&C) and FrameNet (SemEval-2010 and 80Days); on different domains (e.g. novels); and in smaller scales (G&C and 80Days only cover 10 types of predicates). The lack of annotations poses challenges to train and transfer implicit argument models for event extraction. Recently, Ebner et al. (2020) create the Roles Across Multiple Sentences (RAMS) dataset, which covers multi-sentence implicit arguments for a wide range of event and role types. They further develop a span-based argument"
2020.acl-main.667,S10-1008,0,0.0934867,"Missing"
2020.acl-main.667,D19-1584,0,0.285791,"key component in the task of event extraction. It resembles semantic role labeling (SRL) in that the main target is to find argument spans to fill the roles of event frames. However, event arguments can go beyond sentence boundaries: there can be non-local or implicit arguments at the document level. Figure 1 shows such an example: for the purchase event, which is triggered by the word “bought”, its money argument appears in the previous sentence. Implicit arguments have been under-explored in event extraction. Most of previous systems (Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016; Wang et al., 2019) only consider local arguments in the same sentence of the event trigger. While incorporating implicit arguments requires corresponding annotations, few exists in most of the widely used event datasets, like ACE2005 (LDC, 2005; Walker et al., 2006) and RichERE (LDC, 2015). There are several annotation efforts for implicit arguments ... still bought the more ... ... 1 0 1 1 ... Figure 1: Examples of implicit arguments and model illustration. The bold text indicates the trigger word for the purchase event, while the underlined text indicates its non-local “money” argument in the previous sentenc"
2020.blackboxnlp-1.1,P08-1090,0,0.0606968,"ready knows about event roles and their arguments. Understanding how well event arguments are represented can be a first foray into understanding other aspects about events. Extraction of event arguments is often a prerequisite for more complex event tasks. Some examples are event coreference (Lu and Ng, 2018), detecting event-event temporal (Vashishtha et al., 2019) and causal relations (Dunietz et al., 2017), sub-event structure (Araki et al., 2014) and generating approximate causal paths (Kang et al., 2017). Tuples of event-type and arguments are one way of inducing script like-structures (Chambers and Jurafsky, 2008). In summary, our work makes the following contributions: (q h )T k h j h = P i ized dot products αij h are comh T m (qi ) km puted between the current token’s query projection and other token’s key projections. These dot products a.k.a attention values are then used as weights toP combine all token value projech h tions - ohi = j αij vj gives the current head’s token output ohi . Finally, the outputs from all heads are concatenated and projected to get the per-token embeddings for the current layer oi = |H|−1 W T Concat({o0i , o1i . . . oi }) Henceforth, we refer to the parameter tuple {Qh,l"
2020.blackboxnlp-1.1,W19-4828,0,0.0760178,"refer to the per-example word-word activations at a particular layer-head, while head refers either to the identity of the particular layer-head. We ground these terms more clearly in §2.1 1 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 1–10 c Online, November 20, 2020. 2020 Association for Computational Linguistics 2 trol for both lexical confounding and memorization3 . Lakretz et al. (2019) isolate units of LSTM language models whose activations closely track verb-noun number agreement, particularly for hard, long-distance cases. Clark et al. (2019), whose probing methods we adopt, examine if BERT attention heads capture dependency structure. Methodology 2.1 2.1.1 Transformers The Transformer architecture (Vaswani et al., 2017) consists of |L |layers, each comprised of |H |&gt; 1 “self-attention” heads. Here, we describe the architecture just enough to ground terminology - we defer to the original work for detailed exposition. In a given layer l4 , a single self-attention head h consists of three steps - First, query, key and value projections qih = Qh T ei , kih = Kh T ei , vih = Vh T ei are computed from the previous layer’s token embeddi"
2020.blackboxnlp-1.1,W17-0812,0,0.0272767,"tions qih = Qh T ei , kih = Kh T ei , vih = Vh T ei are computed from the previous layer’s token embedding ei . Then, softmax normalIn this work, we probe what and how much a pretrained BERT representation already knows about event roles and their arguments. Understanding how well event arguments are represented can be a first foray into understanding other aspects about events. Extraction of event arguments is often a prerequisite for more complex event tasks. Some examples are event coreference (Lu and Ng, 2018), detecting event-event temporal (Vashishtha et al., 2019) and causal relations (Dunietz et al., 2017), sub-event structure (Araki et al., 2014) and generating approximate causal paths (Kang et al., 2017). Tuples of event-type and arguments are one way of inducing script like-structures (Chambers and Jurafsky, 2008). In summary, our work makes the following contributions: (q h )T k h j h = P i ized dot products αij h are comh T m (qi ) km puted between the current token’s query projection and other token’s key projections. These dot products a.k.a attention values are then used as weights toP combine all token value projech h tions - ohi = j αij vj gives the current head’s token output ohi . F"
2020.blackboxnlp-1.1,N18-1108,0,0.0163789,"pical lexical associations e.g Russia would typically always be a P LACE or TARGET. Recent 8 We will interchangeably refer to Acc as just “accuracy” in plain-text in the rest of the paper 4 works have shown that BERT does retain such associations, including for first names (Shwartz et al., 2020), and enough so that it can act as a reasonable knowledge base (Petroni et al., 2019). One way of implementing this is to create perturbed test examples where gold arguments are replaced with synthetically created “nonce” words not necessarily related to the context. This is similar to the approach of (Gulordava et al., 2018). Role D EFENDANT D ESTINATION O RIGIN T RANSPORTER I NSTRUMENT B ENEFICIARY ATTACKER TARGET G IVER V ICTIM A RTIFACT C OMMUNICATOR PARTICIPANT R ECIPIENT P LACE • Each gold argument token is replaced by a randomly generated token with the same number of characters as the original string. 35.90 21.43 31.82 31.58 31.37 26.56 33.93 44.61 25.55 46.34 50.42 51.61 28.57 40.78 17.77 the 15 most frequent roles in RAMS, using bert-base-uncased. +ve h indices denote “from” heads, while -ve indices denote “to” heads, as explained in §2.2.1 3. The best head for arguments which are not together present in"
2020.blackboxnlp-1.1,N19-1419,0,0.0354174,"Missing"
2020.blackboxnlp-1.1,S19-1026,0,0.0336317,"Missing"
2020.blackboxnlp-1.1,araki-etal-2014-detecting,1,0.806923,"Vh T ei are computed from the previous layer’s token embedding ei . Then, softmax normalIn this work, we probe what and how much a pretrained BERT representation already knows about event roles and their arguments. Understanding how well event arguments are represented can be a first foray into understanding other aspects about events. Extraction of event arguments is often a prerequisite for more complex event tasks. Some examples are event coreference (Lu and Ng, 2018), detecting event-event temporal (Vashishtha et al., 2019) and causal relations (Dunietz et al., 2017), sub-event structure (Araki et al., 2014) and generating approximate causal paths (Kang et al., 2017). Tuples of event-type and arguments are one way of inducing script like-structures (Chambers and Jurafsky, 2008). In summary, our work makes the following contributions: (q h )T k h j h = P i ized dot products αij h are comh T m (qi ) km puted between the current token’s query projection and other token’s key projections. These dot products a.k.a attention values are then used as weights toP combine all token value projech h tions - ohi = j αij vj gives the current head’s token output ohi . Finally, the outputs from all heads are con"
2020.blackboxnlp-1.1,Q18-1041,0,0.0292545,"Missing"
2020.blackboxnlp-1.1,Q16-1037,0,0.0513064,"Missing"
2020.blackboxnlp-1.1,D18-1151,0,0.0231661,"Missing"
2020.blackboxnlp-1.1,D14-1162,0,0.0821586,"Missing"
2020.blackboxnlp-1.1,N18-1202,0,0.0996054,"Missing"
2020.blackboxnlp-1.1,D19-1250,0,0.039793,"Missing"
2020.blackboxnlp-1.1,2020.emnlp-main.556,0,0.0181702,"omly picking any token from the same sentence Se as the argument, save the event trigger itself. This is motivated by the intuition that event ar|a | guments mostly lie in-sentence. This equals |Ser,e |−1 2.5.3 N ONCE procedure We wish to isolate how much of the heads performance is due to memorized “world knowledge” and typical lexical associations e.g Russia would typically always be a P LACE or TARGET. Recent 8 We will interchangeably refer to Acc as just “accuracy” in plain-text in the rest of the paper 4 works have shown that BERT does retain such associations, including for first names (Shwartz et al., 2020), and enough so that it can act as a reasonable knowledge base (Petroni et al., 2019). One way of implementing this is to create perturbed test examples where gold arguments are replaced with synthetically created “nonce” words not necessarily related to the context. This is similar to the approach of (Gulordava et al., 2018). Role D EFENDANT D ESTINATION O RIGIN T RANSPORTER I NSTRUMENT B ENEFICIARY ATTACKER TARGET G IVER V ICTIM A RTIFACT C OMMUNICATOR PARTICIPANT R ECIPIENT P LACE • Each gold argument token is replaced by a randomly generated token with the same number of characters as the"
2020.blackboxnlp-1.1,2020.acl-main.384,0,0.0617289,"Missing"
2020.blackboxnlp-1.1,P19-1280,0,0.0564848,"Missing"
2020.blackboxnlp-1.1,W18-5423,0,0.0307156,"Missing"
2020.blackboxnlp-1.20,N19-1378,0,0.10149,"ity X is a Y”, where Y contains fine-grained type information about X, leading to strong performance for description-based models like CNN and BERT-Large. Interestingly, Ganea performs relatively poorly on this task compared to the other models, which could be because it is trained only on context around the entities, and doesn’t have direct access to the rich description of the entity. Wiki2V, which is similarly contextbased, is also informed by its links to other entities which may provide additional information as we see in the next sections. The RNN’s poor performance was also observed by Aina et al. (2019), who saw low accuracy when probing an “entity-centric” RNN model for entity type information. Relation Detection. For this task, BigGraph and Wiki2V perform best, which is reasonable as they were both trained explicitly with link prediction tasks. The remaining models perform fairly poorly, though CNN, BERT, and BERT-Large still perform reasonably above chance. The results of this task will primarily be useful to contextualize the results of the remaining relationship tasks. Binary Relation Identification. On relation identification, we see similar results as on relation detection, though ave"
2020.blackboxnlp-1.20,P18-1198,0,0.160739,"nting entities, with embeddings learned through methods such as pretraining, task-based training, and encoding knowledge graphs (Yamada et al., 2016; Ling et al., 2020; Wang et al., 2019). These embeddings can be compared extrinsically by performance on a downstream task, such as entity linking (EL). However, performance depends on several factors, such as the architecture of the model they are used in and how the data is preprocessed, making direct comparison of the embeddings hard. Another way to compare these embeddings is intrinsically using probing tasks (Yaghoobzadeh and Sch¨utze, 2016; Conneau et al., 2018), which have been used to examine entity embeddings for information such as an entity’s type, relation to other entities, and factual information (Yaghoobzadeh and Sch¨utze, 2017; Peters et al., 2019; Petroni et al., 2019; Ling et al., 2020). These prior examinations have often examined only a few methods, and 2 Models We compare eight different approaches to generating entity embeddings, organized along two dimensions: the training process of the underlying model, and the content used to inform the embeddings. Along the training dimension, the first method 1 https://github.com/AJRunge523/enti"
2020.blackboxnlp-1.20,Q19-1004,0,0.0208641,"as noted above, may benefit more from this model’s well-encoded entity type information, and likely also its very strong context word knowledge. Tasks like knowledge base completion or question answering will require additional information and our probing task results may provide guidance for selecting embeddings for those tasks. https://huggingface.co/distilbert-base-uncased 211 6 6.1 Related Work Probing Tasks Interpretation of neural language representations has drawn increased attention in recent years, particularly with the rise of BERT and transformerbased language models (Lipton, 2018; Belinkov and Glass, 2019; Tenney et al., 2019; Liu et al., 2019). We focus on methods for detecting specific attributes in learned representations, referred to as point-based intrinsic evaluations (Yaghoobzadeh and Sch¨utze, 2016), auxiliary prediction tasks (Adi et al., 2017) or probing tasks (Conneau et al., 2018; Kim et al., 2019). In these tasks, a model’s weights are frozen after training and queried for linguistic knowledge using small classification tasks. These techniques have similarly been applied to entity embeddings, though usually to limited extents. Entity type prediction has been among the most common"
2020.blackboxnlp-1.20,K19-1063,0,0.0196916,"(Yamada et al., 2016; Cao et al., 2017; Chen et al., 2018), named entity recognition (Sato et al., 2017), and question answering (Yamada et al., 2017). Some neural EL systems have explicitly included semantic information such as an entity’s type (Huang et al., 2015; Gupta et al., 2017; Onoe and Durrett, 2020; Chen et al., 2020). Recent ap212 proaches have explored integrating BERT with pretrained entity embeddings (Zhang et al., 2019; Peters et al., 2019; P¨orner et al., 2019), while others have used BERT directly to learn entity embeddings for the task (Ling et al., 2020; Wang et al., 2019; Broscheit, 2019). 7 Conclusion In this work, we propose a new set of probing tasks for evaluating entity embeddings which can be applied to any method that creates one embedding per entity. Using these tasks, we find that entity type information is one of the strongest signals present in all but one of the embedding models, followed by coarse information about how likely an entity is to be mentioned. We show that the embeddings are particularly able to use entity type information to bootstrap their way to improved performance on entity relationship and factual information prediction tasks and propose methods"
2020.blackboxnlp-1.20,P17-1149,0,0.019476,"ditionally, we provide extensive analysis of performance and errors on these tasks and demonstrate the importance of carefully designing these tasks to better ascertain the true knowledge captured by the embeddings. 6.2 Neural Entity Linking Early neural EL models learned representations by maximizing the similarity between the KB candidate’s text and the mention’s context (He et al., 2013; Francis-Landau et al., 2016). Approaches based on skip-gram and CBOW models (Mikolov et al., 2013) jointly trained word and entity embeddings, producing state of the art results on EL (Yamada et al., 2016; Cao et al., 2017; Chen et al., 2018), named entity recognition (Sato et al., 2017), and question answering (Yamada et al., 2017). Some neural EL systems have explicitly included semantic information such as an entity’s type (Huang et al., 2015; Gupta et al., 2017; Onoe and Durrett, 2020; Chen et al., 2020). Recent ap212 proaches have explored integrating BERT with pretrained entity embeddings (Zhang et al., 2019; Peters et al., 2019; P¨orner et al., 2019), while others have used BERT directly to learn entity embeddings for the task (Ling et al., 2020; Wang et al., 2019; Broscheit, 2019). 7 Conclusion In this"
2020.blackboxnlp-1.20,D19-1040,0,0.0715455,"AIDA-CoNLL, but better on TAC-KBP, which could indicate its strong type information helps more on the smaller dataset, while word information may be more helpful on AIDA. While the transformer EL model clearly outperforms the CNN and RNN EL models, no single embedding model performs consistently better across these datasets and models. Wiki2V shows the highest potential for generalization across models on 7 CoNLL-AIDA, possibly because of its combination of context word, entity type, and popularity information, the latter of which has been shown to set a non-trivial baseline on this dataset (Chen et al., 2019). Ganea performs extremely well in combination with the Transformer model, approaching the current state of the art on AIDA-CoNLL set by Raiman and Raiman (2018). BERT-Large consistently performs best on TAC-KBP, a smaller dataset which, as noted above, may benefit more from this model’s well-encoded entity type information, and likely also its very strong context word knowledge. Tasks like knowledge base completion or question answering will require additional information and our probing task results may provide guidance for selecting embeddings for those tasks. https://huggingface.co/distilb"
2020.blackboxnlp-1.20,N19-1423,0,0.0261666,"dding methods are able to learn entity type information using probing tasks based on the DBPedia6 ontology. We extract the types from each of the first 3 levels of the ontology, representing increasingly fine-grained entity types, and create one N-way classification task for all types at that level, which we refer to as T-1, T-2, and T-3. Derived Models Our first derived model is a simple bag of vectors model, in which we average the GoogleNews Word2Vec (Mikolov et al., 2013) vectors of the first 512 words of the entity’s Wikipedia page. Our other two derived models are BERT-based embeddings (Devlin et al., 2019) of the first 512 words in the entity’s Wikipedia page. We use BERT-base-uncased and BERT-large-uncased, which generate 768 and 1024 dimensional embeddings for each entity by averaging all the hidden states of all tokens in the final layer. We explored averaging the hidden states of the CLS tokens in different layers in initial experiments, but found averaging all hidden states in the final layer performed best. 3 description-based embeddings. We create a binary prediction task for whether or not a word appears in an entity’s context words for 1,000 high frequency (appearing in >100k Wiki page"
2020.blackboxnlp-1.20,K17-1008,0,0.0816813,"500 words of the document it appears in and encodes candidate KB entities with convolutions over the entity’s name and first 500 words of its Wikipedia page. It computes cosine distance between the outputs of each of the mention and KB convolutions, producing six features which are passed to a linear layer to produce a score for each candidate, trying to maximize the score of the true candidate. We use a kernel size of 150 and concatenate the candidate name and document convolution outputs to get 300-dimensional entity embeddings from this model. Second is the RNN-based model of Eshel et al. (2017), a context-based model which learns a 300dimension embedding for each KB entity. Each mention is represented by two 20-word context windows on its left and right, which are passed through 2 https://code.google.com/archive/p/word2vec https://huggingface.co/bert-base-uncased 4 https://huggingface.co/bert-large-uncased Pretrained Entity Models We evaluate three pretrained embedding models that leverage context and graph-based information to represent entities. For all three models, we train 300 dimensional entity embeddings. First is the context-based model of Ganea and Hofmann (2017) (Ganea). T"
2020.blackboxnlp-1.20,2020.emnlp-main.400,0,0.0413093,"Missing"
2020.blackboxnlp-1.20,N16-1150,0,0.119589,"20 words to create 93.8M training instances. Each mention is assigned a single negative candidate randomly from all entities (Eshel et al., 2017). We train each model for a single epoch on this dataset, following Eshel’s method. 2.2 2.1 Task-Learned Embedding Models For our task-learned models, we re-implement two neural EL models, which learn entity representations for the goal of connecting mentions of entities in text to entities in a knowledge base (KB). We briefly summarize them here and refer interested readers to the original papers for further details. First is the CNN-based model of Francis-Landau et al. (2016), a description and context-based hybrid model. It encodes text mentions of entities by applying convolutions over the mention’s name, context sentence, and the first 500 words of the document it appears in and encodes candidate KB entities with convolutions over the entity’s name and first 500 words of its Wikipedia page. It computes cosine distance between the outputs of each of the mention and KB convolutions, producing six features which are passed to a linear layer to produce a score for each candidate, trying to maximize the score of the true candidate. We use a kernel size of 150 and co"
2020.blackboxnlp-1.20,D17-1277,0,0.0911327,"500 words of the document it appears in and encodes candidate KB entities with convolutions over the entity’s name and first 500 words of its Wikipedia page. It computes cosine distance between the outputs of each of the mention and KB convolutions, producing six features which are passed to a linear layer to produce a score for each candidate, trying to maximize the score of the true candidate. We use a kernel size of 150 and concatenate the candidate name and document convolution outputs to get 300-dimensional entity embeddings from this model. Second is the RNN-based model of Eshel et al. (2017), a context-based model which learns a 300dimension embedding for each KB entity. Each mention is represented by two 20-word context windows on its left and right, which are passed through 2 https://code.google.com/archive/p/word2vec https://huggingface.co/bert-base-uncased 4 https://huggingface.co/bert-large-uncased Pretrained Entity Models We evaluate three pretrained embedding models that leverage context and graph-based information to represent entities. For all three models, we train 300 dimensional entity embeddings. First is the context-based model of Ganea and Hofmann (2017) (Ganea). T"
2020.blackboxnlp-1.20,D17-1284,0,0.227314,"sk-learned and pretrained embedding models, while the derived embedding models are publicly available pre-trained language models.234 single-layer bidirectional GRUs. The RNN outputs are each passed to an MLP attention module which uses the candidate entity embedding as the attention context to pass information from the text to the embeddings. The attention outputs and entity embedding are concatenated and passed through a single-layer MLP followed by a linear layer to compute a score for the candidate. We train these models using an EL dataset built from all of Wikipedia (Eshel et al., 2017; Gupta et al., 2017). We take the anchor text of each intraWiki link in Wikipedia as a mention, with the page it links to as the gold entity, filtering any cross-wiki links, non-entity pages, and entities with fewer than 20 words to create 93.8M training instances. Each mention is assigned a single negative candidate randomly from all entities (Eshel et al., 2017). We train each model for a single epoch on this dataset, following Eshel’s method. 2.2 2.1 Task-Learned Embedding Models For our task-learned models, we re-implement two neural EL models, which learn entity representations for the goal of connecting men"
2020.blackboxnlp-1.20,P13-2006,0,0.0140683,"rediction and context word evaluation. These tasks can be easily applied to any method which produces a single embedding per entity allowing us to compare a much wider range of model architectures than in any prior work. Additionally, we provide extensive analysis of performance and errors on these tasks and demonstrate the importance of carefully designing these tasks to better ascertain the true knowledge captured by the embeddings. 6.2 Neural Entity Linking Early neural EL models learned representations by maximizing the similarity between the KB candidate’s text and the mention’s context (He et al., 2013; Francis-Landau et al., 2016). Approaches based on skip-gram and CBOW models (Mikolov et al., 2013) jointly trained word and entity embeddings, producing state of the art results on EL (Yamada et al., 2016; Cao et al., 2017; Chen et al., 2018), named entity recognition (Sato et al., 2017), and question answering (Yamada et al., 2017). Some neural EL systems have explicitly included semantic information such as an entity’s type (Huang et al., 2015; Gupta et al., 2017; Onoe and Durrett, 2020; Chen et al., 2020). Recent ap212 proaches have explored integrating BERT with pretrained entity embeddi"
2020.blackboxnlp-1.20,D19-1275,0,0.011439,"uct three types of tasks to probe for this information. First is a regression task, predicting the log-scaled number of times an entity is linked to (P-R). The second is a multi-class classification 207 3.6 Probing Experiments For all tasks, we create train and test sets with 500 entities per label. For R-C+I, we include 100 corrupted examples from each relationship type, for 24,400 None-type instances. We use relatively small training sizes and a logistic regression classifier as the probing model to observe how easily the information can be identified from a limited sample and simple model (Hewitt and Liang, 2019). For the popularity regression task, we use 500 training and test instances and a linear model trained with Huber loss. For single entity probing tasks, the input is the embedding of the entity in question. For tasks probing a pair of entities, the input is the concatenation of the two entities’ embeddings, h and t, as well as h − t and the element-wise product h t. We report macro F1 for all tasks except binary relation detection and context word prediction, where we report macro F1 averaged over all sub-tasks, and the popularity regression task where we report RMSE. 4 Probing Experiment Ana"
2020.blackboxnlp-1.20,S19-1026,0,0.0316206,"Missing"
2020.blackboxnlp-1.20,N19-1112,0,0.0243827,"l’s well-encoded entity type information, and likely also its very strong context word knowledge. Tasks like knowledge base completion or question answering will require additional information and our probing task results may provide guidance for selecting embeddings for those tasks. https://huggingface.co/distilbert-base-uncased 211 6 6.1 Related Work Probing Tasks Interpretation of neural language representations has drawn increased attention in recent years, particularly with the rise of BERT and transformerbased language models (Lipton, 2018; Belinkov and Glass, 2019; Tenney et al., 2019; Liu et al., 2019). We focus on methods for detecting specific attributes in learned representations, referred to as point-based intrinsic evaluations (Yaghoobzadeh and Sch¨utze, 2016), auxiliary prediction tasks (Adi et al., 2017) or probing tasks (Conneau et al., 2018; Kim et al., 2019). In these tasks, a model’s weights are frozen after training and queried for linguistic knowledge using small classification tasks. These techniques have similarly been applied to entity embeddings, though usually to limited extents. Entity type prediction has been among the most common task explored when proposing a new entit"
2020.blackboxnlp-1.20,W18-3026,0,0.0979149,"t entity types effectively, then certain types of relationships may be easier to classify based solely on the types of the entities involved. Our typerestricted relationship corruption method should help ensure that good performance on this task requires understanding the relationships themselves rather than just the types of the involved entities. 3.3.3 Relationship Detection Finally, we examine the general task of predicting whether a pair of entities is related or not, which requires an explicit relationship between two entities compared to an entity relatedness task (Hoffart et al., 2012; Newman-Griffis et al., 2018). Effectively encoding this information can help with tasks like knowledge graph completion, where knowing the existence of a link is useful, even if the exact type of the link is unknown. We sample a small number of positive examples and their corruptions from each of the 244 relationship types as described above to create this task (R-D). 3.4 task for the binned number of times an entity is linked to as a coarser popularity estimate, with bins for > 1000, 100−1000, 10−100, and 1−10 links (P-B). The third is a comparative task, where the model must predict which of two entities is linked to m"
2020.blackboxnlp-1.20,2020.findings-emnlp.54,0,0.0188533,"learned representations by maximizing the similarity between the KB candidate’s text and the mention’s context (He et al., 2013; Francis-Landau et al., 2016). Approaches based on skip-gram and CBOW models (Mikolov et al., 2013) jointly trained word and entity embeddings, producing state of the art results on EL (Yamada et al., 2016; Cao et al., 2017; Chen et al., 2018), named entity recognition (Sato et al., 2017), and question answering (Yamada et al., 2017). Some neural EL systems have explicitly included semantic information such as an entity’s type (Huang et al., 2015; Gupta et al., 2017; Onoe and Durrett, 2020; Chen et al., 2020). Recent ap212 proaches have explored integrating BERT with pretrained entity embeddings (Zhang et al., 2019; Peters et al., 2019; P¨orner et al., 2019), while others have used BERT directly to learn entity embeddings for the task (Ling et al., 2020; Wang et al., 2019; Broscheit, 2019). 7 Conclusion In this work, we propose a new set of probing tasks for evaluating entity embeddings which can be applied to any method that creates one embedding per entity. Using these tasks, we find that entity type information is one of the strongest signals present in all but one of the em"
2020.blackboxnlp-1.20,N15-1026,0,0.0608048,"Missing"
2020.blackboxnlp-1.20,D19-1005,0,0.26815,"ings can be compared extrinsically by performance on a downstream task, such as entity linking (EL). However, performance depends on several factors, such as the architecture of the model they are used in and how the data is preprocessed, making direct comparison of the embeddings hard. Another way to compare these embeddings is intrinsically using probing tasks (Yaghoobzadeh and Sch¨utze, 2016; Conneau et al., 2018), which have been used to examine entity embeddings for information such as an entity’s type, relation to other entities, and factual information (Yaghoobzadeh and Sch¨utze, 2017; Peters et al., 2019; Petroni et al., 2019; Ling et al., 2020). These prior examinations have often examined only a few methods, and 2 Models We compare eight different approaches to generating entity embeddings, organized along two dimensions: the training process of the underlying model, and the content used to inform the embeddings. Along the training dimension, the first method 1 https://github.com/AJRunge523/entitylens 204 Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 204–216 c Online, November 20, 2020. 2020 Association for Computational Linguisti"
2020.blackboxnlp-1.20,D19-1250,0,0.0640577,"Missing"
2020.blackboxnlp-1.20,K16-1025,0,0.185149,"mple probing tasks, demonstrating which methods are able to remember words used to describe entities, learn type, relationship and factual information, and identify how frequently an entity is mentioned. We also compare these methods in a unified framework on two entity linking tasks and discuss how they generalize to different model architectures and datasets. 1 Introduction Neural methods for generating entity embeddings have become the dominant approach to representing entities, with embeddings learned through methods such as pretraining, task-based training, and encoding knowledge graphs (Yamada et al., 2016; Ling et al., 2020; Wang et al., 2019). These embeddings can be compared extrinsically by performance on a downstream task, such as entity linking (EL). However, performance depends on several factors, such as the architecture of the model they are used in and how the data is preprocessed, making direct comparison of the embeddings hard. Another way to compare these embeddings is intrinsically using probing tasks (Yaghoobzadeh and Sch¨utze, 2016; Conneau et al., 2018), which have been used to examine entity embeddings for information such as an entity’s type, relation to other entities, and f"
2020.blackboxnlp-1.20,Q17-1028,0,0.121199,"ngs, we replace the candidate document convolution in the CNN model or the randomly initialized embeddings in the RNN and transformer models with the pretrained embeddings during training. Details about dataset preprocessing, candidate selection, and model training can be found in Appendix A. 5.2 Entity Linking Results Table 3 contains the results of our 3 EL models using each of our 8 embedding methods, as well as no pretrained embeddings for comparison. We report micro-averaged and macro-averaged Precision@1 for AIDA-CoNLL and micro-averaged Precision@1 for TAC-KBP, following previous work (Yamada et al., 2017; Eshel et al., 2017; Raiman and Raiman, 2018). Each result is the average of three runs for that configuration. We see clear benefits from pretrained embeddings across all models and datasets. While the CNN and RNN embeddings provide improvements compared to using no pretrained embeddings, they transfer poorly to other models, often performing worse than even the simple BOW embedding approach. BigGraph performs worse than BOW on AIDA-CoNLL, but better on TAC-KBP, which could indicate its strong type information helps more on the smaller dataset, while word information may be more helpful on A"
2020.blackboxnlp-1.20,I17-2017,0,0.0218159,"ors on these tasks and demonstrate the importance of carefully designing these tasks to better ascertain the true knowledge captured by the embeddings. 6.2 Neural Entity Linking Early neural EL models learned representations by maximizing the similarity between the KB candidate’s text and the mention’s context (He et al., 2013; Francis-Landau et al., 2016). Approaches based on skip-gram and CBOW models (Mikolov et al., 2013) jointly trained word and entity embeddings, producing state of the art results on EL (Yamada et al., 2016; Cao et al., 2017; Chen et al., 2018), named entity recognition (Sato et al., 2017), and question answering (Yamada et al., 2017). Some neural EL systems have explicitly included semantic information such as an entity’s type (Huang et al., 2015; Gupta et al., 2017; Onoe and Durrett, 2020; Chen et al., 2020). Recent ap212 proaches have explored integrating BERT with pretrained entity embeddings (Zhang et al., 2019; Peters et al., 2019; P¨orner et al., 2019), while others have used BERT directly to learn entity embeddings for the task (Ling et al., 2020; Wang et al., 2019; Broscheit, 2019). 7 Conclusion In this work, we propose a new set of probing tasks for evaluating entity"
2020.blackboxnlp-1.20,P19-1139,0,0.0194928,"ncis-Landau et al., 2016). Approaches based on skip-gram and CBOW models (Mikolov et al., 2013) jointly trained word and entity embeddings, producing state of the art results on EL (Yamada et al., 2016; Cao et al., 2017; Chen et al., 2018), named entity recognition (Sato et al., 2017), and question answering (Yamada et al., 2017). Some neural EL systems have explicitly included semantic information such as an entity’s type (Huang et al., 2015; Gupta et al., 2017; Onoe and Durrett, 2020; Chen et al., 2020). Recent ap212 proaches have explored integrating BERT with pretrained entity embeddings (Zhang et al., 2019; Peters et al., 2019; P¨orner et al., 2019), while others have used BERT directly to learn entity embeddings for the task (Ling et al., 2020; Wang et al., 2019; Broscheit, 2019). 7 Conclusion In this work, we propose a new set of probing tasks for evaluating entity embeddings which can be applied to any method that creates one embedding per entity. Using these tasks, we find that entity type information is one of the strongest signals present in all but one of the embedding models, followed by coarse information about how likely an entity is to be mentioned. We show that the embeddings are pa"
2020.blackboxnlp-1.20,P19-1452,0,0.0467179,"Missing"
2020.blackboxnlp-1.20,P16-1023,0,0.015245,".8M training instances. Each mention is assigned a single negative candidate randomly from all entities (Eshel et al., 2017). We train each model for a single epoch on this dataset, following Eshel’s method. 2.2 2.1 Task-Learned Embedding Models For our task-learned models, we re-implement two neural EL models, which learn entity representations for the goal of connecting mentions of entities in text to entities in a knowledge base (KB). We briefly summarize them here and refer interested readers to the original papers for further details. First is the CNN-based model of Francis-Landau et al. (2016), a description and context-based hybrid model. It encodes text mentions of entities by applying convolutions over the mention’s name, context sentence, and the first 500 words of the document it appears in and encodes candidate KB entities with convolutions over the entity’s name and first 500 words of its Wikipedia page. It computes cosine distance between the outputs of each of the mention and KB convolutions, producing six features which are passed to a linear layer to produce a score for each candidate, trying to maximize the score of the true candidate. We use a kernel size of 150 and co"
2020.blackboxnlp-1.20,E17-1055,0,0.0257163,"Missing"
2020.coling-main.273,S18-1111,0,0.0578061,"Missing"
2020.coling-main.273,W09-2415,0,0.13014,"Missing"
2020.coling-main.273,J15-4004,0,0.0776355,"Missing"
2020.coling-main.273,Q16-1002,0,0.0217415,"oncept (Genus) and the relations distinguishing it from other members of the same type (Differentia) via syntax and string matching heuristics (Binot and Jensen, 1993; Calzolari, 1984; Chodorow et al., 1985). Recent approaches directly encoded definitions to distributed representations. Tissier (2017) obtained embeddings via a skip-gram model trained on definitions, while Bosc (2018) used an auto-encoder. Other work includes definition generation (Noraset et al., 2017), binary classification of sentences on whether they are definitional (Anke and Schockaert, 2018), reverse dictionary look-up (Hill et al., 2016; Zock and Bilac, 2004), and extraction of hypernymy relations from definitions using syntactic patterns (Boella and Di Caro, 2013). 3 Approach Our framework consists of two parts: the Relation Retriever and the Definition Frame (DF) Encoder. The WordNet definition for any given term is used by the Relation Retriever model to extract the Qualia structure relations. The set of extracted terms pertaining to these relations form the Definition Frame. The DF Encoder encodes this output to a distributed matrix representation, which can be used in downstream NLP tasks. Qualia Structure The Qualia st"
2020.coling-main.273,Q17-1022,0,0.0379624,"Missing"
2020.coling-main.273,D14-1162,0,0.083888,"ful rows, while still being decomposed into distributional vectors. 2 Prior Work Prior research on lexical semantics has established a set of relations that are sufficient to uniquely define a concept. Such work includes the Qualia structure (Boguraev and Pustejovsky, 1990) and the generative lexicon theory (Pustejovsky, 1991). Other related work includes ontological approaches (Baker et al., 1998; Miller, 1995; Lenat, 1995; Speer and Havasi, 2012) and more fine-grained definition-based frames like Semagrams (Moerdijk and others, 2008). In distributional semantics, approaches including GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013), and fastText (Bojanowski et al., 2017) obtain generic word embeddings by pre-training on large corpora. Recent work focused on context-sensitive embeddings like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), which achieve significant improvements in downstream NLP tasks. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 3060 Proceedings of the 28th International Conference on Computational Linguistics, pages 3060–3068 Barcelona, Spain (Online), Decem"
2020.coling-main.273,N18-1202,0,0.0131036,"ualia structure (Boguraev and Pustejovsky, 1990) and the generative lexicon theory (Pustejovsky, 1991). Other related work includes ontological approaches (Baker et al., 1998; Miller, 1995; Lenat, 1995; Speer and Havasi, 2012) and more fine-grained definition-based frames like Semagrams (Moerdijk and others, 2008). In distributional semantics, approaches including GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013), and fastText (Bojanowski et al., 2017) obtain generic word embeddings by pre-training on large corpora. Recent work focused on context-sensitive embeddings like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), which achieve significant improvements in downstream NLP tasks. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 3060 Proceedings of the 28th International Conference on Computational Linguistics, pages 3060–3068 Barcelona, Spain (Online), December 8-13, 2020 ConceptNet → Basis embeddings (GloVe, dict2vec, …) → definition Training data Relation Retriever → Definition Frame Moon IsA: satellite, astronomical body PartOf: Solar System → → Moon Wikipedia Wikipedia Wo"
2020.coling-main.273,J91-4003,0,0.693927,"ed on the Qualia structure suggested in Boguraev and Postojovsky (1990), and they are extracted automatically from definitions via a domain-adaptation approach. To the best of our knowledge, DF is the first hybrid representation, combining an explicit structure through semantically meaningful rows, while still being decomposed into distributional vectors. 2 Prior Work Prior research on lexical semantics has established a set of relations that are sufficient to uniquely define a concept. Such work includes the Qualia structure (Boguraev and Pustejovsky, 1990) and the generative lexicon theory (Pustejovsky, 1991). Other related work includes ontological approaches (Baker et al., 1998; Miller, 1995; Lenat, 1995; Speer and Havasi, 2012) and more fine-grained definition-based frames like Semagrams (Moerdijk and others, 2008). In distributional semantics, approaches including GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013), and fastText (Bojanowski et al., 2017) obtain generic word embeddings by pre-training on large corpora. Recent work focused on context-sensitive embeddings like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), which achieve significant improvements in downstr"
2020.coling-main.273,speer-havasi-2012-representing,0,0.239959,"nitions via a domain-adaptation approach. To the best of our knowledge, DF is the first hybrid representation, combining an explicit structure through semantically meaningful rows, while still being decomposed into distributional vectors. 2 Prior Work Prior research on lexical semantics has established a set of relations that are sufficient to uniquely define a concept. Such work includes the Qualia structure (Boguraev and Pustejovsky, 1990) and the generative lexicon theory (Pustejovsky, 1991). Other related work includes ontological approaches (Baker et al., 1998; Miller, 1995; Lenat, 1995; Speer and Havasi, 2012) and more fine-grained definition-based frames like Semagrams (Moerdijk and others, 2008). In distributional semantics, approaches including GloVe (Pennington et al., 2014), word2vec (Mikolov et al., 2013), and fastText (Bojanowski et al., 2017) obtain generic word embeddings by pre-training on large corpora. Recent work focused on context-sensitive embeddings like ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), which achieve significant improvements in downstream NLP tasks. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/lic"
2020.coling-main.273,D17-1024,0,0.0182776,"vided by Faruqui (2014): SimLex999 (Hill et al., 2015), MC30 (Miller and Charles, 1991), RG65 (Rubenstein and Goodenough, 1965), WS353 (Finkelstein et al., 2002) and MEN (Bruni et al., 2012). Following Agirre (2009), we split them into word-similarity (WS-Sim, SimLex999, MC30, RG65) and word-relatedness (WS-Rel, MEN) datasets, as they evaluate different semantic affinities. We only consider nominal terms that exist in WordNet and report Spearman’s correlation ρ. We perform experiments with three types of embeddings used as Basis: GloVe (Pennington et al., 2014), dict2vec trained on Wikipedia (Tissier et al., 2017), and retrofit embeddings (Faruqui et al., 2015) based on GloVe. Since the task comprises of pairs of words without any context, we do not compare against context-based representations. Ablation Study We perform an ablation study by varying the set of relations used in DF. In this study, both Basis and DF are encoded with dict2vec, as it achieves the best performance (Table 3). The goal of this study is to measure how each extracted relation affects the performance of DF in word similarity tasks. The results (details in Appendix A.2) show that, for similarity tasks, pruning relations sometimes"
2020.coling-main.273,W04-2105,0,0.0241929,"the relations distinguishing it from other members of the same type (Differentia) via syntax and string matching heuristics (Binot and Jensen, 1993; Calzolari, 1984; Chodorow et al., 1985). Recent approaches directly encoded definitions to distributed representations. Tissier (2017) obtained embeddings via a skip-gram model trained on definitions, while Bosc (2018) used an auto-encoder. Other work includes definition generation (Noraset et al., 2017), binary classification of sentences on whether they are definitional (Anke and Schockaert, 2018), reverse dictionary look-up (Hill et al., 2016; Zock and Bilac, 2004), and extraction of hypernymy relations from definitions using syntactic patterns (Boella and Di Caro, 2013). 3 Approach Our framework consists of two parts: the Relation Retriever and the Definition Frame (DF) Encoder. The WordNet definition for any given term is used by the Relation Retriever model to extract the Qualia structure relations. The set of extracted terms pertaining to these relations form the Definition Frame. The DF Encoder encodes this output to a distributed matrix representation, which can be used in downstream NLP tasks. Qualia Structure The Qualia structure (formal, consti"
2020.deelio-1.4,2020.acl-main.676,0,0.215327,"Missing"
2020.deelio-1.4,P18-1225,1,0.881941,"Missing"
2020.deelio-1.4,P98-1013,0,0.0283536,"Missing"
2020.deelio-1.4,K18-1031,0,0.0247595,"y Fluency, also known as naturalness or readability, is a measure of how fluent text is. The higher the fluency, the more it imitates grammatically and logically correct human text.13 1. P ERPLEXITY (PPL) is defined as: 1 P P L(S) = exp(− ln(pM (S))) |S| where S is a piece of text and pM (S) is the probability assigned to S by the language model. We finetune GPT-2 on a two-million review subset of YR (with a 500K additional validation split) and use this finetuned model for PPL evaluation. Outputs less likely to be seen in YR will typically have higher PPL. 2. SLOR (syntactic log-odds ratio) (Kann et al., 2018) is our main fluency metric. It modifies 2.5.3 Semantic Content Preservation (SCP) SCP assesses how closely each generated continuation (hypothesis) matches in semantic content to the ground truth distribution of continuations (reference). Since the latter is unavailable in this case, we use the prompt itself as a proxy for reference.14 We use what we call the Prompt-Continuation BertScore (BPRO). BPRO computes average BertScore (Zhang et al., 2019a) between each continuation and the prompt. BertScore computes pertoken BERT representations for both hypothesis and reference and aligns each hypo"
2020.deelio-1.4,K19-1079,0,0.0274561,"er SBLEU values represent higher inter-continuation diversity. 2. U NIQUE T RIGRAMS (UTR) (Tevet and Berant, 2020; Li et al., 2016) measures the ratio of unique to total trigrams in a population of generations. Higher UTR represents greater diversity. Since UTR is defined at the population level, it can assess the extent of crosscontinuation repetition. 3. T YPE -T OKEN R ATIO (TTR) is the ratio of unique to total tokens in a piece of text, and serves as a measure of intra-continuation diversity. The higher the TTR, the more varied the vocabulary in a continuation. 4. R ARE -W ORDS (RW ORDS) (See et al., 2019) is defined by the following: X ntrain (w) Es∼S [ − log ] Ntrain w∈s PPL by normalizing for individual tokens (e.g. “Zimbabwe” is less frequent than “France” but just as fluent), and serves as a better measure. Higher SLOR represents higher fluency. The equation for SLOR is as follows: Y 1 (ln(pM (S)) − ln( p(t))) SLOR(S) = |S| t∈S where |S |is the length of S (in tokens), pM (S) is the probability of S under language model M , and p(t) are the unconditional probabilities of individual tokens (or unigrams) t in S. We use the same finetuned GPT-2 model on YR as for PPL mentioned above for SLOR."
2020.deelio-1.4,N19-1131,0,0.035261,"e this finetuned model for PPL evaluation. Outputs less likely to be seen in YR will typically have higher PPL. 2. SLOR (syntactic log-odds ratio) (Kann et al., 2018) is our main fluency metric. It modifies 2.5.3 Semantic Content Preservation (SCP) SCP assesses how closely each generated continuation (hypothesis) matches in semantic content to the ground truth distribution of continuations (reference). Since the latter is unavailable in this case, we use the prompt itself as a proxy for reference.14 We use what we call the Prompt-Continuation BertScore (BPRO). BPRO computes average BertScore (Zhang et al., 2019a) between each continuation and the prompt. BertScore computes pertoken BERT representations for both hypothesis and reference and aligns each hypothesis token to a reference token. We prefer BertScore over symbolic measures (e.g BLEU) since it does not rely on exact string matching alone and allows soft matches between different parts of the input pair. 12 This is because we generate 100 continuations per test example. See Section §3.4 for more. 13 We evaluate perplexity and SLOR on the concatenations of the generated continuations with their corresponding prompts, and Spellcheck on the gene"
2020.deelio-1.4,D13-1170,0,0.00489758,"Missing"
2020.deelio-1.4,N03-1033,0,0.140447,"Missing"
2020.emnlp-main.1,W18-5212,0,0.0276102,"credibility), logos (logic), and pathos (appeal to the hearer’s emotion). More recently, Wachsmuth et al. (2017b) summarized various aspects of argument quality studied in argumentation theory and NLP, such as clarity, relevance, and arrangement. Some research took empirical approaches and collected argument evaluation criteria from human evaluators (Habernal and Gurevych, 2016a; Wachsmuth et al., 2017a). By adopting some of these aspects, computational models have been proposed to automatically evaluate argument quality in various settings, such as essays (Ke et al., 2019), online comments (Gu et al., 2018), and pairwise ranking (Habernal and Gurevych, 2016b). While these taxonomies help understand and evaluate the quality of an argument as a whole, little empirical analysis has been done in terms of what to attack in an argument to persuade the arguer. What can be attacked in an argument has been studied more in argumentation theory. Particularly, Walton et al. (2008) present argumentation schemes and critical questions (CQs). Argument schemes 3 Data Here we describe how we collected and labeled our data. 3.1 Data Collection We use online discussions from the ChangeMyView (CMV) subreddit2 . In"
2020.emnlp-main.1,D19-1653,0,0.0350985,"Missing"
2020.emnlp-main.1,D16-1129,0,0.132956,"uasion. This analysis of reasons for attacks, along with argumentation theory and discourse studies, provide insights into what characteristics of sentences are relevant to attackability. Informed by these insights, we extract features that represent relevant sentence characteristics, clustered into four categories: content, external knowledge, proposition Introduction Effectively refuting an argument is an important skill in persuasion dialogue, and the first step is to find appropriate points to attack in the argument. Prior work in NLP has studied argument quality (Wachsmuth et al., 2017a; Habernal and Gurevych, 2016a) and counterargument generation (Hua et al., 2019; Wachsmuth et al., 2018). But these studies mainly concern an argument’s overall quality and making counterarguments toward the main claim, without investigating what parts of an argument are attackable for successful persuasion. Nevertheless, attacking specific points of an argument is common and effective; in our data of online discussions, challengers who successfully change the original poster’s view are 1.5 times more likely to quote specific sentences of the argument for attacks than unsuccessful challengers (Figure 1). In this paper, w"
2020.emnlp-main.1,P16-1150,0,0.33928,"uasion. This analysis of reasons for attacks, along with argumentation theory and discourse studies, provide insights into what characteristics of sentences are relevant to attackability. Informed by these insights, we extract features that represent relevant sentence characteristics, clustered into four categories: content, external knowledge, proposition Introduction Effectively refuting an argument is an important skill in persuasion dialogue, and the first step is to find appropriate points to attack in the argument. Prior work in NLP has studied argument quality (Wachsmuth et al., 2017a; Habernal and Gurevych, 2016a) and counterargument generation (Hua et al., 2019; Wachsmuth et al., 2018). But these studies mainly concern an argument’s overall quality and making counterarguments toward the main claim, without investigating what parts of an argument are attackable for successful persuasion. Nevertheless, attacking specific points of an argument is common and effective; in our data of online discussions, challengers who successfully change the original poster’s view are 1.5 times more likely to quote specific sentences of the argument for attacks than unsuccessful challengers (Figure 1). In this paper, w"
2020.emnlp-main.1,P19-1459,0,0.0611218,"Missing"
2020.emnlp-main.1,E17-1017,0,0.394628,"uisite for effective persuasion. This analysis of reasons for attacks, along with argumentation theory and discourse studies, provide insights into what characteristics of sentences are relevant to attackability. Informed by these insights, we extract features that represent relevant sentence characteristics, clustered into four categories: content, external knowledge, proposition Introduction Effectively refuting an argument is an important skill in persuasion dialogue, and the first step is to find appropriate points to attack in the argument. Prior work in NLP has studied argument quality (Wachsmuth et al., 2017a; Habernal and Gurevych, 2016a) and counterargument generation (Hua et al., 2019; Wachsmuth et al., 2018). But these studies mainly concern an argument’s overall quality and making counterarguments toward the main claim, without investigating what parts of an argument are attackable for successful persuasion. Nevertheless, attacking specific points of an argument is common and effective; in our data of online discussions, challengers who successfully change the original poster’s view are 1.5 times more likely to quote specific sentences of the argument for attacks than unsuccessful challenger"
2020.emnlp-main.1,P19-1054,0,0.0478681,"Missing"
2020.emnlp-main.1,P18-1023,0,0.337091,"d discourse studies, provide insights into what characteristics of sentences are relevant to attackability. Informed by these insights, we extract features that represent relevant sentence characteristics, clustered into four categories: content, external knowledge, proposition Introduction Effectively refuting an argument is an important skill in persuasion dialogue, and the first step is to find appropriate points to attack in the argument. Prior work in NLP has studied argument quality (Wachsmuth et al., 2017a; Habernal and Gurevych, 2016a) and counterargument generation (Hua et al., 2019; Wachsmuth et al., 2018). But these studies mainly concern an argument’s overall quality and making counterarguments toward the main claim, without investigating what parts of an argument are attackable for successful persuasion. Nevertheless, attacking specific points of an argument is common and effective; in our data of online discussions, challengers who successfully change the original poster’s view are 1.5 times more likely to quote specific sentences of the argument for attacks than unsuccessful challengers (Figure 1). In this paper, we examine how to computationally 1 Our data and source code are available at"
2020.emnlp-main.2,W19-4502,1,0.879658,"ive discourse units (ADUs). An ADU is the minimal locution that performs an argumentative function. Given an utterance, ADUs may be identified based on syntactic rules, such as phrases (Stede et al., 2016), clauses (Peldszus and Stede, 2015), or a series of clauses (Al Khatib et al., 2016), or by machine learning models, such as neural networks (Ajjour et al., 2017) or retrieval (Persing and Ng, 2016). None of these methods go further to understand what propositions are asserted in each ADU. More recently, a computational framework has been proposed to extract asserted propositions from ADUs (Jo et al., 2019). This cascade model proposes how to detect reported speech, questions, and imperatives, reconstruct any missing subjects, and make final revisions for grammar correction. While this model was built upon the same goal of extracting asserted propositions from locutions, it does not present computational models to extract implicit propositions in questions, reported speech, and imperatives. Hence, our work fills this gap in the cascade model. • Our work is a first computational study of extracting propositions asserted in questions, reported speech, and imperatives in argumentation. We demonstra"
2020.emnlp-main.2,W17-5115,0,0.0153837,"imperatives (§6). On the other hand, one of the main goals of argument mining is to identify pro- and counterrelations between asserted propositions. In most argument mining systems, asserted propositions are approximated and substituted by argumentative discourse units (ADUs). An ADU is the minimal locution that performs an argumentative function. Given an utterance, ADUs may be identified based on syntactic rules, such as phrases (Stede et al., 2016), clauses (Peldszus and Stede, 2015), or a series of clauses (Al Khatib et al., 2016), or by machine learning models, such as neural networks (Ajjour et al., 2017) or retrieval (Persing and Ng, 2016). None of these methods go further to understand what propositions are asserted in each ADU. More recently, a computational framework has been proposed to extract asserted propositions from ADUs (Jo et al., 2019). This cascade model proposes how to detect reported speech, questions, and imperatives, reconstruct any missing subjects, and make final revisions for grammar correction. While this model was built upon the same goal of extracting asserted propositions from locutions, it does not present computational models to extract implicit propositions in quest"
2020.emnlp-main.2,L18-1257,0,0.0816564,"step guidelines for annotating propositions that are implicitly asserted in imperatives. First, we may group imperatives by their semantics based on theories, such as you-should and you-will (Schwager, 2005). Second, for these imperatives, we may annotate whether the root verb is argumentatively relevant. For instance, if the you-should theory is applicable to an imperative, we may annotate whether its verb is at the core of the main argumentative content that the speaker asserts should happen; the assertive form of this imperative is likely to be a statement that proposes a policy or action (Park and Cardie, 2018). Argumentatively relevant imperatives may be annotated with asserted propositions using predefined transformation templates appropriate for their semantics. On the other hand, argumentatively irrelevant verbs may simply be rhetorical and need to be replaced properly. Annotation of these imperatives should handle many irregular cases, relying on the domain of the argumentation and the annotator’s expertise. Table 3: Root verbs and counts in imperatives. root is a verb with base form or second-person present case (VB/VBP), neither marked (e.g., to go) nor modified by an auxiliary modal verb (e."
2020.emnlp-main.2,W15-0513,0,0.028772,"the readability of the paper, we will defer this discussion to the respective sections of questions (§4), reported speech (§5), and imperatives (§6). On the other hand, one of the main goals of argument mining is to identify pro- and counterrelations between asserted propositions. In most argument mining systems, asserted propositions are approximated and substituted by argumentative discourse units (ADUs). An ADU is the minimal locution that performs an argumentative function. Given an utterance, ADUs may be identified based on syntactic rules, such as phrases (Stede et al., 2016), clauses (Peldszus and Stede, 2015), or a series of clauses (Al Khatib et al., 2016), or by machine learning models, such as neural networks (Ajjour et al., 2017) or retrieval (Persing and Ng, 2016). None of these methods go further to understand what propositions are asserted in each ADU. More recently, a computational framework has been proposed to extract asserted propositions from ADUs (Jo et al., 2019). This cascade model proposes how to detect reported speech, questions, and imperatives, reconstruct any missing subjects, and make final revisions for grammar correction. While this model was built upon the same goal of extr"
2020.emnlp-main.2,N16-1164,0,0.0280299,"d, one of the main goals of argument mining is to identify pro- and counterrelations between asserted propositions. In most argument mining systems, asserted propositions are approximated and substituted by argumentative discourse units (ADUs). An ADU is the minimal locution that performs an argumentative function. Given an utterance, ADUs may be identified based on syntactic rules, such as phrases (Stede et al., 2016), clauses (Peldszus and Stede, 2015), or a series of clauses (Al Khatib et al., 2016), or by machine learning models, such as neural networks (Ajjour et al., 2017) or retrieval (Persing and Ng, 2016). None of these methods go further to understand what propositions are asserted in each ADU. More recently, a computational framework has been proposed to extract asserted propositions from ADUs (Jo et al., 2019). This cascade model proposes how to detect reported speech, questions, and imperatives, reconstruct any missing subjects, and make final revisions for grammar correction. While this model was built upon the same goal of extracting asserted propositions from locutions, it does not present computational models to extract implicit propositions in questions, reported speech, and imperativ"
2020.emnlp-main.2,2020.lrec-1.127,1,0.76707,"instances). PARC3.0 F1 BLEU F1 BLEU Scheible (All) Scheible (Matched) 64.4 75.8 57.1 72.7 37.9 79.3 23.4 76.5 CRF BERT 71.3 82.6 66.3 82.0 72.5 87.1 68.7 89.3 (a) Accuracy of identifying speech content. The accuracies of Scheible for US2016 (italic) result from training it on the training data of PARC3.0. PARC3.0 CRF BERT US2016 Strict F1 Relaxed F1 Strict F1 Relaxed F1 52.4 71.0 59.8 78.6 62.4 70.3 71.6 84.8 (b) Accuracy of identifying speech source. US2016: The second dataset is the instances of reported speech in the corpus of the 2016 U.S. presidential debates and commentary, prepared by Jo et al. (2020)6 . This dataset includes 242 instances of reported speech annotated with speech content and source. The reliability of the annotations was measured by the number non-overlapping words between annotators. The average number of words that are outside of the overlapping text span was 0.2 for speech content and 0.5 for speech sources, suggesting the high reliability of the annotations. Table 2: Accuracy of identifying speech content and source. speech sources, because the source may be mentioned multiple times in reported speech and we do not want to penalize the model when the mention identified"
2020.emnlp-main.2,prasad-etal-2008-penn,0,0.165724,"Missing"
2020.emnlp-main.2,P16-1164,0,0.0633877,"Missing"
2020.emnlp-main.2,L16-1167,0,0.0217072,"mentation. But for the sake of the readability of the paper, we will defer this discussion to the respective sections of questions (§4), reported speech (§5), and imperatives (§6). On the other hand, one of the main goals of argument mining is to identify pro- and counterrelations between asserted propositions. In most argument mining systems, asserted propositions are approximated and substituted by argumentative discourse units (ADUs). An ADU is the minimal locution that performs an argumentative function. Given an utterance, ADUs may be identified based on syntactic rules, such as phrases (Stede et al., 2016), clauses (Peldszus and Stede, 2015), or a series of clauses (Al Khatib et al., 2016), or by machine learning models, such as neural networks (Ajjour et al., 2017) or retrieval (Persing and Ng, 2016). None of these methods go further to understand what propositions are asserted in each ADU. More recently, a computational framework has been proposed to extract asserted propositions from ADUs (Jo et al., 2019). This cascade model proposes how to detect reported speech, questions, and imperatives, reconstruct any missing subjects, and make final revisions for grammar correction. While this model"
2020.emnlp-main.2,D17-1164,0,0.0264567,"dot product of the hidden states from the encoder and the decoder: ← −E → −E exp(aji ) , aji = hD ˆji = P j · [ h i ; h i ], a i0 exp(aji0 ) X → − ← − ¯E = h a ˆji [ h E ; h E ]. Questions In this section, we extract implicit propositions from questions in argumentation. The task is formulated as transforming a question into its asserted proposition. 4.1 Theoretical Background Questions in argumentation may be categorized into rhetorical questions and pure questions. Rhetorical questions are not intended to require an answer; instead, they often make an implicit assertive (as in sentence 4). Zhang et al. (2017) identified finer-grained types of rhetorical questions, such as sharing concerns, agreeing, and conceding. Our work is not aiming to classify these types, but instead focuses on extracting implicit assertives in rhetorical questions. Pure questions, on the other hand, are intended to seek information. According to the speech act theory, non-binary questions have incomplete propositions (Searle, 1969). For instance, the question “How many people were arrested?” has the proposition “X people were arrested”, with the questioned part underspecified and denoted by X. Although the proposition is se"
2020.emnlp-main.520,W13-2322,0,0.0200382,"f our dataset to existing datasets edge, (ii) zero shot learning: during inference on a previously unseen domain, there are previously unseen attributes, entities, and state change types. This makes the problem very challenging and places this task in a novel setting (see §3.1) 3 Related Work Tracking state changes: Procedural text understanding addresses the task of tracking entity states throughout the text (Bosselut et al., 2018; Henaff et al., 2017). This ability is an important part of 6410 text understanding. While syntactic parsing methods such as AMR (abstract meaning representation) (Banarescu et al., 2013) represent “who did what to whom” by uncovering stated facts, tracking entity states uncovers unstated facts such as how ingredients change during a recipe. Datasets with closed state changes: The bAbI dataset (Weston et al., 2015) includes questions about objects moved throughout a paragraph, using machine-generated language over a deterministic domain with a small lexicon. The SCoNE dataset (Long et al., 2016) contains paragraphs describing a changing world state in three synthetic, deterministic domains. However, approaches developed using synthetic data often fail to handle the inherent co"
2020.emnlp-main.520,P16-1138,0,0.0281795,"ut et al., 2018; Henaff et al., 2017). This ability is an important part of 6410 text understanding. While syntactic parsing methods such as AMR (abstract meaning representation) (Banarescu et al., 2013) represent “who did what to whom” by uncovering stated facts, tracking entity states uncovers unstated facts such as how ingredients change during a recipe. Datasets with closed state changes: The bAbI dataset (Weston et al., 2015) includes questions about objects moved throughout a paragraph, using machine-generated language over a deterministic domain with a small lexicon. The SCoNE dataset (Long et al., 2016) contains paragraphs describing a changing world state in three synthetic, deterministic domains. However, approaches developed using synthetic data often fail to handle the inherent complexity in language when applied to organic, real-world data (Hermann et al., 2015; Winograd, 1972). The ProPara dataset (Dalvi et al., 2018) contains three state changes (create, destroy, move) for natural text describing scientific procedures. Other domain specific datasets include recipe domain (Bosselut et al., 2018), and biology experiments (Mysore et al., 2019). These datasets contain a small, closed set"
2020.emnlp-main.520,W19-4007,0,0.173076,"ns fill that gap easily with their commonsense but machines need to model these effects in the form of state changes. For example, when a potato is rubbed on a car window (to defog it), then the unstated effects of this action are the following state changes: windows becomes sticky, opaque, and the potato becomes dirty, etc. These changes can be tracked across the paragraph. An exemplary use case of text with actions is procedural 1 text (recipes, how-to guides, etc.) where modeling such state changes helps in various reasoning-based end tasks, e.g. automatic execution of biology experiments (Mysore et al., 2019), cooking recipes (Bollini et al., 2012) and everyday activities (Yang and Nyberg, 2015). While there has been great progress in tracking entity states in scientific processes (Dalvi et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in simple stories (Rashkin et al., 2018), prior tasks are restricted to a fixed, small set of state change types thus covering only a small fraction of the entire world state. Figure 1 illustrates this for a real-world procedure “How to Keep Car Windows Fog Free Using a"
2020.emnlp-main.520,P18-1213,0,0.231387,"ragraph. An exemplary use case of text with actions is procedural 1 text (recipes, how-to guides, etc.) where modeling such state changes helps in various reasoning-based end tasks, e.g. automatic execution of biology experiments (Mysore et al., 2019), cooking recipes (Bollini et al., 2012) and everyday activities (Yang and Nyberg, 2015). While there has been great progress in tracking entity states in scientific processes (Dalvi et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in simple stories (Rashkin et al., 2018), prior tasks are restricted to a fixed, small set of state change types thus covering only a small fraction of the entire world state. Figure 1 illustrates this for a real-world procedure “How to Keep Car Windows Fog Free Using a Potato”. Existing datasets such as ProPara (Dalvi et al., 2018) only model the existence and location attributes, limiting the fidelity with which they model the world. Specifically: • Attributes from domain-specific datasets such Download O PEN PI at https://allenai.org/data/openpi 6408 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Proc"
2020.emnlp-main.520,D18-1006,1,0.80527,"3 Manually inspecting the 45 predictions made by COMET Person needs to be arrested ) Person is arrested, gets dirty. 3.1 Positioning O PEN PI Figure 2.1 projects existing tasks and models along two different dimensions (open vocabulary, and variable-size low-specificity). We find that models bottom-left quadrant represents majority of the existing work on state changes such as ProPara (Dalvi et al., 2018) and bAbI (Weston et al., 2016)) in NLP community, and ALFRED (Shridhar et al., 2019) and VirtualHome (Puig et al., 2018) in Computer Vision. Correspondingly many models exist in that space ((Tandon et al., 2018), (Bosselut et al., 2018), (Henaff et al., 2017)). Very few models exist that can predict either open vocab (Rashkin et al., 2018), or variable size output (Bosselut et al., 2018). However, no existing task has both open vocabulary and variable-size low specificity– placing O PEN PI in a novel space. 4 4.1 Dataset Data Collection We set up a crowdsourcing task on Amazon Mechanical Turk where the annotators author the y= {yi } for every sentence of a wikihow.com article, filling in a sentence template for each yi as a guide. WikiHow contains a wide variety of goals (e.g., how to wash dishes) br"
2020.emnlp-main.520,H89-1033,0,0.573348,"uncovers unstated facts such as how ingredients change during a recipe. Datasets with closed state changes: The bAbI dataset (Weston et al., 2015) includes questions about objects moved throughout a paragraph, using machine-generated language over a deterministic domain with a small lexicon. The SCoNE dataset (Long et al., 2016) contains paragraphs describing a changing world state in three synthetic, deterministic domains. However, approaches developed using synthetic data often fail to handle the inherent complexity in language when applied to organic, real-world data (Hermann et al., 2015; Winograd, 1972). The ProPara dataset (Dalvi et al., 2018) contains three state changes (create, destroy, move) for natural text describing scientific procedures. Other domain specific datasets include recipe domain (Bosselut et al., 2018), and biology experiments (Mysore et al., 2019). These datasets contain a small, closed set of state change types that are relevant to a specific domain. Our dataset is general domain, and to accommodate this generality we have an open vocabulary of state changes. Datasets with open state changes: (Isola et al., 2015) propose manually defined antonymous adjective pairs (big,"
2020.emnlp-main.529,J08-1001,0,0.0502865,"-level decisions and coherently structuring output text is called a planning process. Where can the model learn such high-level decisions related to long-term coherence? A written paragraph itself can be a pot of golden resources, containing various forms of inductive coherence signals. Different types of coherence signals in a paragraph have been studied and used in many different ways: a sequence of words or sentences (Devlin et al., 2019; Radford et al., 2019), a discourse structure of a text (Appelt, 1982; Hovy, 1991; Kang et al., 2019), an order of sentences (Chambers and Jurafsky, 2008; Barzilay and Lapata, 2008), topic introduction, co-reference, a sequence of events (Tomkins, 1978; Schank and Abelson, 2013), and more. In this work, we primarily focus on the effect of topical content in text planning. Despite the recent advances of contextualized language models (Devlin et al., 2019; Radford et al., 2019), the lack of appropriate tasks makes it difficult to evaluate generation models’ long-term coherence. Prior tasks fall into classification or ranking problems, such as narrative close task (Chambers and Jurafsky, 2008; Mostafazadeh et al., 2016), sentence ordering (Barzilay and Lapata, 2008), and ne"
2020.emnlp-main.529,P08-1090,0,0.0741482,"el realization with such high-level decisions and coherently structuring output text is called a planning process. Where can the model learn such high-level decisions related to long-term coherence? A written paragraph itself can be a pot of golden resources, containing various forms of inductive coherence signals. Different types of coherence signals in a paragraph have been studied and used in many different ways: a sequence of words or sentences (Devlin et al., 2019; Radford et al., 2019), a discourse structure of a text (Appelt, 1982; Hovy, 1991; Kang et al., 2019), an order of sentences (Chambers and Jurafsky, 2008; Barzilay and Lapata, 2008), topic introduction, co-reference, a sequence of events (Tomkins, 1978; Schank and Abelson, 2013), and more. In this work, we primarily focus on the effect of topical content in text planning. Despite the recent advances of contextualized language models (Devlin et al., 2019; Radford et al., 2019), the lack of appropriate tasks makes it difficult to evaluate generation models’ long-term coherence. Prior tasks fall into classification or ranking problems, such as narrative close task (Chambers and Jurafsky, 2008; Mostafazadeh et al., 2016), sentence ordering (Barzil"
2020.emnlp-main.529,N19-1423,0,0.579365,"decisions on what and how to say about before they speak (Byrne, 1979; McKeown, 1985; Hovy, 1990; Swan, 2002; Kang, 2020). Guiding the surfacelevel realization with such high-level decisions and coherently structuring output text is called a planning process. Where can the model learn such high-level decisions related to long-term coherence? A written paragraph itself can be a pot of golden resources, containing various forms of inductive coherence signals. Different types of coherence signals in a paragraph have been studied and used in many different ways: a sequence of words or sentences (Devlin et al., 2019; Radford et al., 2019), a discourse structure of a text (Appelt, 1982; Hovy, 1991; Kang et al., 2019), an order of sentences (Chambers and Jurafsky, 2008; Barzilay and Lapata, 2008), topic introduction, co-reference, a sequence of events (Tomkins, 1978; Schank and Abelson, 2013), and more. In this work, we primarily focus on the effect of topical content in text planning. Despite the recent advances of contextualized language models (Devlin et al., 2019; Radford et al., 2019), the lack of appropriate tasks makes it difficult to evaluate generation models’ long-term coherence. Prior tasks fall"
2020.emnlp-main.529,P19-1254,0,0.118701,"Missing"
2020.emnlp-main.529,P17-1102,0,0.028473,"tra partial information as a set of keywords to guide the surface generator. This is motivated by data-to-text tasks, but our plans are topical content instead of structured data. We then question what types of plan keywords are the most effective for completing the paragraph. We extract keywords using various keyword extraction systems: • Off-the-shelf systems extract keywords for each sentence using the three off-the-shelf systems: YAKE (Campos et al., 2020) using statistical features (e.g., TF, IDF), RAKE (Rose et al., 2010) using graph-based features (e.g., word degree), and PositionRank (Florescu and Caragea, 2017) using position-based PageRank. Then we choose duplicate keywords by majority voting. • Syntactic features (e.g., part-of-speech tags, named entities (Fan et al., 2019), events 6535 S3 Unmasked, Unmasked, context context sentences sentences (a) Sentence masking via permutation: t=1 (left) or t=2 (right): One paragraph has a total of 5+4=9 training instances. Partially Partially masked, masked, target target sentences sentences S2 Permutation masking by t=2 S1 Permutation masking by t=2 S5 Permutation masking by t=1 S1 S1 S2 S2 S3 S3 S4 S4 S5 S5 S4 Masked, Masked, target target sentences senten"
2020.emnlp-main.529,D16-1230,0,0.0331766,"ampling parameters, and so on. We follow the default parameters used in the HuggingFace’s transformer models (Wolf et al., 2019). For a pointer-generator, we follow the default parameters in (See et al., 2017). The maximum number of plan keywords per sentence is 3. For more details, see the Appendix. Metrics. We evaluate our models using both automatic metrics and human evaluation: For automatic metrics, we use two hard metrics: BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), as well as an embedding similarity metric to capture the semantic similarity: Vector Extrema (VE) (Liu et al., 2016). For human evaluation, we measure fluency, coherence with respect to context, and overall quality with 1-5 Likert scale. We randomly select 100 samples from the test set in each Romance, WikiText, and CNNDM (total 300 paragraphs). Each sample is annotated by three crowd-workers then averaged. We also measure how human performs on the task by asking workers to predict the masked text in these 300 paragraphs. 5.1 Automatic and Human Evaluation Table 4 and 5 show automatic and human evaluation result on PAR C OM task. The fine-tuned models ({BERT,GPT2} f inetune )4 and FlowNet models show signif"
2020.emnlp-main.529,D19-5633,0,0.13014,"coherently connected. Table 2 categorize various generation models applied on the open-ended tasks (C⊥⊥T), based on its selfsupervision types: Tasks Content Content Content Surface Selection Planning Ordering Realization ⊂ Data-to-text ⊃ Summarization ≈ Paraphrasing StoryGen, Text Infilling, ⊥⊥ Bridging, PAR C OM (ours) × X 4 × × × X 4 × X X X X X X X Table 1: Comparison of generation tasks by different inclusion relationships between Context and Target. C ⊂ T: Data-to-text produces text from structured data (e.g., table). Moryossef et al. (2019); Puduppully et al. (2019); Shen et al. (2019); Miculicich et al. (2019) combine content planning with surface realization. However, since content Table 2: Comparison of generation models in C⊥⊥T tasks by different self-supervision types. Keskar et al. (2019) conditioned language models with topical words to control the target text. Fan et al. (2019) developed a surface realizer on anonymized entities using semantic role labeling. Hua and Wang (2019) used pre-extracted topics to guide a generator to produce stylized argumentation text. However, they are given the topical content as input (content guidance), while our SSPlanner directly predicts plan words from con"
2020.emnlp-main.529,D19-1055,0,0.444942,"spite the recent advances of contextualized language models (Devlin et al., 2019; Radford et al., 2019), the lack of appropriate tasks makes it difficult to evaluate generation models’ long-term coherence. Prior tasks fall into classification or ranking problems, such as narrative close task (Chambers and Jurafsky, 2008; Mostafazadeh et al., 2016), sentence ordering (Barzilay and Lapata, 2008), and next sentence prediction (Devlin et al., 2019). Some recent works focused on designing generation tasks: story generation (Fan et al., 2019), text infilling (Huang et al., 2019; Fedus et al., 2018; Hua and Wang, 2019), or paragraph bridging (Kang et al., 2019). However, most of them suffer from predicting appropriate topical content given limited context, due to the limited usage of self-supervision 6533 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6533–6543, c November 16–20, 2020. 2020 Association for Computational Linguistics signals from the paragraph. This work proposes a new open-ended paragraph completion task; PAR C OM; predicting the masked sentences in a paragraph. Unlike the prior works, our task uses two effective ways of self-supervision learnt"
2020.emnlp-main.529,D19-1589,1,0.407062,", 2002; Kang, 2020). Guiding the surfacelevel realization with such high-level decisions and coherently structuring output text is called a planning process. Where can the model learn such high-level decisions related to long-term coherence? A written paragraph itself can be a pot of golden resources, containing various forms of inductive coherence signals. Different types of coherence signals in a paragraph have been studied and used in many different ways: a sequence of words or sentences (Devlin et al., 2019; Radford et al., 2019), a discourse structure of a text (Appelt, 1982; Hovy, 1991; Kang et al., 2019), an order of sentences (Chambers and Jurafsky, 2008; Barzilay and Lapata, 2008), topic introduction, co-reference, a sequence of events (Tomkins, 1978; Schank and Abelson, 2013), and more. In this work, we primarily focus on the effect of topical content in text planning. Despite the recent advances of contextualized language models (Devlin et al., 2019; Radford et al., 2019), the lack of appropriate tasks makes it difficult to evaluate generation models’ long-term coherence. Prior tasks fall into classification or ranking problems, such as narrative close task (Chambers and Jurafsky, 2008; M"
2020.emnlp-main.529,N19-1236,0,0.0462365,"Missing"
2020.emnlp-main.529,N16-1098,0,0.0449639,"), an order of sentences (Chambers and Jurafsky, 2008; Barzilay and Lapata, 2008), topic introduction, co-reference, a sequence of events (Tomkins, 1978; Schank and Abelson, 2013), and more. In this work, we primarily focus on the effect of topical content in text planning. Despite the recent advances of contextualized language models (Devlin et al., 2019; Radford et al., 2019), the lack of appropriate tasks makes it difficult to evaluate generation models’ long-term coherence. Prior tasks fall into classification or ranking problems, such as narrative close task (Chambers and Jurafsky, 2008; Mostafazadeh et al., 2016), sentence ordering (Barzilay and Lapata, 2008), and next sentence prediction (Devlin et al., 2019). Some recent works focused on designing generation tasks: story generation (Fan et al., 2019), text infilling (Huang et al., 2019; Fedus et al., 2018; Hua and Wang, 2019), or paragraph bridging (Kang et al., 2019). However, most of them suffer from predicting appropriate topical content given limited context, due to the limited usage of self-supervision 6533 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6533–6543, c November 16–20, 2020. 2020 Assoc"
2020.emnlp-main.529,P02-1040,0,0.106383,"round-truth plan keywords p. ˆ We find the best hyper-parameters on the validation set using a grid search on the learning rate, the number of training epochs, sampling parameters, and so on. We follow the default parameters used in the HuggingFace’s transformer models (Wolf et al., 2019). For a pointer-generator, we follow the default parameters in (See et al., 2017). The maximum number of plan keywords per sentence is 3. For more details, see the Appendix. Metrics. We evaluate our models using both automatic metrics and human evaluation: For automatic metrics, we use two hard metrics: BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), as well as an embedding similarity metric to capture the semantic similarity: Vector Extrema (VE) (Liu et al., 2016). For human evaluation, we measure fluency, coherence with respect to context, and overall quality with 1-5 Likert scale. We randomly select 100 samples from the test set in each Romance, WikiText, and CNNDM (total 300 paragraphs). Each sample is annotated by three crowd-workers then averaged. We also measure how human performs on the task by asking workers to predict the masked text in these 300 paragraphs. 5.1 Automatic and Human Evaluati"
2020.emnlp-main.529,D14-1162,0,0.0834698,"Missing"
2020.emnlp-main.529,P17-1099,0,0.738292,"agraph itself: (1) we augment more training instances via permutation masking and (2) resolve the context sparsity problem by providing a set of ground-truth content keywords and predicting them directly from context at testing time. For the task, we propose a self-supervised text planner (SSPlanner) that explicitly predicts content keywords (content prediction) from context and guides the pretrained language model (surfacerealization) using the predicted content. The distribution of predicted keywords is then combined with the distribution of words in the language model using copy mechanism (See et al., 2017). The predicted content keywords are an approximation of topical intents by the generator, providing a hint to guide the surface realizer to bridge the coherency gap between the given context and text to generate. Overall, SSPlanner combines two advantages; micro-level language fluency from the pre-trained language model (bottom-up) and macro-level content choice controlled by the macro-level planning (top-down). Our experiment shows that SSPlanner achieves significant improvements over the baselines in both automatic and human evaluation. 2 Models (⊥⊥) Keskar et al. (2019) Fan et al. (2019) H"
2020.emnlp-main.529,D19-1054,0,0.0206007,"but they should be coherently connected. Table 2 categorize various generation models applied on the open-ended tasks (C⊥⊥T), based on its selfsupervision types: Tasks Content Content Content Surface Selection Planning Ordering Realization ⊂ Data-to-text ⊃ Summarization ≈ Paraphrasing StoryGen, Text Infilling, ⊥⊥ Bridging, PAR C OM (ours) × X 4 × × × X 4 × X X X X X X X Table 1: Comparison of generation tasks by different inclusion relationships between Context and Target. C ⊂ T: Data-to-text produces text from structured data (e.g., table). Moryossef et al. (2019); Puduppully et al. (2019); Shen et al. (2019); Miculicich et al. (2019) combine content planning with surface realization. However, since content Table 2: Comparison of generation models in C⊥⊥T tasks by different self-supervision types. Keskar et al. (2019) conditioned language models with topical words to control the target text. Fan et al. (2019) developed a surface realizer on anonymized entities using semantic role labeling. Hua and Wang (2019) used pre-extracted topics to guide a generator to produce stylized argumentation text. However, they are given the topical content as input (content guidance), while our SSPlanner directly pr"
2020.emnlp-main.529,P19-3007,0,0.0150372,"e position embeddings by learning from an order of context sentences, plan predictor by learning from the relationship between the given context and important keywords used in the generation of the target text, and content guidance by learning from whether the predicted plan keywords are used or not in the target (See Figure 2). Our planner is motivated by the two-stage generation framework (Moryossef et al., 2019; Miculicich et al., 2019; Fu et al., 2019; Hua and Wang, 2019). While in prior works, the content is explicitly given from the dataset or task itself, our plan 2 https://spacy.io/ 3 Vig (2019) observed that which layers or heads are important for syntactic and semantic tasks. predictor in SSPlanner predicts the plan keywords only from the given context, by learning the topical relationship between context and content in target from training data. Given l length of a paragraph s1 ..sl where each sentence s consists of a n number of words s = w1 ..wn , PAR C OM splits it into the context sentences x=s1 ..s j−1 , s j+t ..sn and t target sentences to predict y=s j ..s j+t−1 . For each target sentence, p number of plan keywords k j,1 ..k j,p for arbitrary target sentence s j are given o"
2020.emnlp-main.529,W19-2304,0,0.0781462,"j,p for arbitrary target sentence s j are given only at training time. The plan keywords are chosen from the entire vocabulary VW and later combined with word distribution from the language model. We describe each selfsupervision module in SSPlanner as follows: Surface realization with pre-trained language models. We use two different types of transformer-based language models: BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019). While GPT2 is trained on bidirectionally tied language modeling, BERT is trained on masked language modeling. For BERT, we use the sequential sampling method (Wang and Cho, 2019). Using them, we encode context x and output the hidden representation h j,i = f(h j−1,i , xk<( j,i) ) for jth word in ith sentence, where f ∈ {BERT, GPT2} is the transformer language model. We then output the sentence vector hi by averaging all word vectors in a sentence. Sentence position embedding. We concatenate the encoded sentence representation with its sentence position embedding. By adding the sentence position embeddings into context encoding, the model is aware of where the context sentence came from (e.g., from the first or last). Compared to the simple concatenation of them (Kang"
2020.emnlp-main.79,D18-1044,0,0.154721,"Missing"
2020.emnlp-main.79,P19-1125,0,0.0751775,"s relatively high decoding latency. To make the decoding more efficient, non-autoregressive translation (NAT) (Gu et al., 2018) is introduced to generate multiple tokens at once instead of one-by-one. However, with the conditional independence property (Gu et al., 2018), NAT models do not directly consider the dependencies among output tokens, which may cause errors of repeated translation and ∗ Zhisong and Xiang contributed equally for this paper incomplete translation (Wang et al., 2019). There have been various methods in previous work (Stern et al., 2019; Gu et al., 2019; Ma et al., 2018; Wei et al., 2019; Ma et al., 2019; Tu et al., 2020) to mitigate this problem, including iterative decoding (Lee et al., 2018; Ghazvininejad et al., 2019). In this work, we introduce a novel mechanism, i.e., local autoregressive translation (LAT), to take local target dependencies into consideration. For a decoding position, instead of generating one token, we predict a short sequence of tokens (which we call a translation piece) for the current and next few positions in an autoregressive way. A simple example is shown in Figure 1. With this mechanism, there can be overlapping tokens between nearby translation"
2020.emnlp-main.79,2020.acl-main.251,0,0.147204,"Missing"
2020.findings-emnlp.300,2020.acl-main.499,0,0.0339123,"Missing"
2020.findings-emnlp.300,W19-4828,0,0.127822,"across different tasks. We apply the widely used parameter sharing approach, where a single representation layer is followed by task specific output layers (Baxter, 1997). This reduces the risk of overfitting to a single task and allows decisions on i, j, di , de to influence each other in the hidden layers of the network. We first describe our encoder and then the other layers on top, see Figure 2 for the model architecture. Encoder: To encode x1 . . . xK and question q we use the BERT architecture (Devlin et al., 2018) that has achieved state-of-the-art performance across several NLP tasks (Clark et al., 2019), 2 Note that this does not assume all sentences have the same directionality of influence. For example, a paragraph could include both positive and negative influences: “Predators arrive. Thus the rabbit population falls...”. Rather, the dj = di assumption is one of narrative coherence: the more predators arrive, the more the rabbit population falls. That is, within a paragraph, we assume enhancing one step will have enhanced effects (both positive or negative effects) on future steps - a property of a coherently authored paragraph. where the question q = qp ⊕ qe (⊕ stands for concatenation)."
2020.findings-emnlp.300,N18-1144,1,0.824342,"ral text understanding: Machine reading has seen tremendous progress. With machines reaching human performance in standard QA benchmarks (Devlin et al., 2018; Rajpurkar et al., 2016), more challenging datasets have been proposed (Dua et al., 2019) that require background knowledge, commonsense reasoning (Talmor et al., 2019) and visual reasoning (Antol et al., 2015; 1 All the code will be publicly shared upon acceptance Zellers et al., 2018). In the context of procedural text understanding which has gained considerable amount of attention recently, (Bosselut et al., 2018; Henaff et al., 2017; Dalvi et al., 2018) address the task of tracking entity states throughout the text. Recently, (Tandon et al., 2019) introduced the WIQA task to predict the effect of perturbations. Understanding the effects of perturbations, specifically, qualitative change, has been studied using formal frameworks in the qualitative reasoning community (Forbus, 1984; Weld and De Kleer, 2013) and counterfactual reasoning in the logic community (Lewis, 2013). The WIQA dataset situates this task in terms of natural language rather than formal reasoning, by treating the task as a mixture of reading comprehension and commonsense rea"
2020.findings-emnlp.300,N19-1246,0,0.0446553,"Missing"
2020.findings-emnlp.300,N18-2017,0,0.156846,"mal frameworks in the qualitative reasoning community (Forbus, 1984; Weld and De Kleer, 2013) and counterfactual reasoning in the logic community (Lewis, 2013). The WIQA dataset situates this task in terms of natural language rather than formal reasoning, by treating the task as a mixture of reading comprehension and commonsense reasoning. However, existing models do not explain the effects of perturbations. Explanations: Despite large-scale QA benchmarks, high scores do not necessarily reflect understanding (Min et al., 2019). Current models may not be robust or exploit annotation artifacts (Gururangan et al., 2018). This makes explanations desirable for interpretation (Selvaraju et al., 2017). Attention based explanation has been successfully used in vision tasks such as object detection (Petsiuk et al., 2018) because pixel information is explainable to humans. These and other token level attention models used in NLP tasks (Wiegreffe and Pinter, 2019) do not provide full-sentence explanations of a model’s decisions. Recently, several datasets with natural language explanations have been introduced, e.g., in natural language inference (Camburu et al., 2018), visual question answering (Park et al., 2018),"
2020.findings-emnlp.300,L18-1433,0,0.0627706,"hungry → (MORE/+) eagle swoops down → (MORE/+) eagle catches mouse → (MORE/+) eagle gets more food Table 1: Examples of our model’s predictions on the dev. set in the format: “qp → di xi → dj xj → de qe ”. Supporting sentences xi , xj are compressed e.g., “the person has his ears less protected” → “ears less protected” procedural while the current input is procedural in nature with a specific chronological structure. Another line of work provides more structure and organization to explanations, e.g., using scene graphs in computer vision (Ghosh et al., 2019). For elementary science questions, Jansen et al. (2018) uses a science knowledge graph. These approaches rely on a knowledge structure or graph but knowledge graphs are incomplete and costly to construct for every domain (Weikum and Theobald, 2010). There are trade-offs between unstructured and structured explanations. Unstructured explanations are available abundantly while structured explanations need to be constructed and hence are less scalable (Camburu et al., 2018). Generating free-form (unstructured) explanations is difficult to evaluate (Cui et al., 2018; Zhang et al., 2019), and adding qualitative structure over them is nontrivial. Taking"
2020.findings-emnlp.300,P19-1416,0,0.018969,"ffects of perturbations, specifically, qualitative change, has been studied using formal frameworks in the qualitative reasoning community (Forbus, 1984; Weld and De Kleer, 2013) and counterfactual reasoning in the logic community (Lewis, 2013). The WIQA dataset situates this task in terms of natural language rather than formal reasoning, by treating the task as a mixture of reading comprehension and commonsense reasoning. However, existing models do not explain the effects of perturbations. Explanations: Despite large-scale QA benchmarks, high scores do not necessarily reflect understanding (Min et al., 2019). Current models may not be robust or exploit annotation artifacts (Gururangan et al., 2018). This makes explanations desirable for interpretation (Selvaraju et al., 2017). Attention based explanation has been successfully used in vision tasks such as object detection (Petsiuk et al., 2018) because pixel information is explainable to humans. These and other token level attention models used in NLP tasks (Wiegreffe and Pinter, 2019) do not provide full-sentence explanations of a model’s decisions. Recently, several datasets with natural language explanations have been introduced, e.g., in natur"
2020.findings-emnlp.300,W19-4007,0,0.0509627,"Missing"
2020.findings-emnlp.300,P19-1487,0,0.0436311,"ee-form and knowledge graphs based explanations, we infer a qualitative structure over the sentences in the paragraph. This retains the rich interpretability and simpler evaluation of structured explanations as well as leverages the large-scale availability of sentences required for these explanation. It is an open research problem whether requiring explanation helps or hurts the original task being explained. On the natural language inference task (e-SNLI), Camburu et al. (2018) observed that models generate correct explanations at the expense of good performance. On the Cos-E task, recently Rajani et al. (2019) showed that explanations help the end-task. Our work extends along this line in a new task setting that involves perturbations and enriches natural language explanations with qualitative structure. 3 Problem definition We adopt the problem definition described in Tandon et al. (2019), and summarize it here. Input: 1. Procedural text with steps x1 . . . xK . Here, xk denotes step k (i.e., a sentence) in a procedural text comprising K steps. 2. A perturbation qp to the procedural text and its likely candidate effect qe . Output: An explanation structure that explains the effect of the perturbat"
2020.findings-emnlp.300,N19-1421,0,0.0475798,"Missing"
2020.findings-emnlp.300,D18-1006,1,0.73696,"(Yang and Nyberg, 2015). However, the goal of procedural text understanding in these settings remains a major challenge and requires two key abilities, (i) understanding the dynamics of the world inside a procedure by tracking entities and what events happen as the narrative unfolds. (ii) understanding the dynamics of the world outside the procedure that can influence the procedure. While recent systems for procedural text comprehension have focused on understanding the dynamics of the world inside the process, such as tracking entities and answering questions about what events happen, e.g., (Tandon et al., 2018; Bosselut et al., 2018; Henaff et al., 2017), the extent to which they understand the influences of outside events remains unclear. In particular, if a system fully understands a process, it should be able to predict what would happen if it was perturbed in some way due to an event from the outside world. Such counterfactual reasoning is particularly challenging because, rather than asking what happened (described in text), it asks about what would happen in an alternative world where the change occurred. Recently, Tandon et al. (2019) introduced the WIQA dataset that contains such problems,"
2020.findings-emnlp.300,D19-1629,1,0.733966,"s and answering questions about what events happen, e.g., (Tandon et al., 2018; Bosselut et al., 2018; Henaff et al., 2017), the extent to which they understand the influences of outside events remains unclear. In particular, if a system fully understands a process, it should be able to predict what would happen if it was perturbed in some way due to an event from the outside world. Such counterfactual reasoning is particularly challenging because, rather than asking what happened (described in text), it asks about what would happen in an alternative world where the change occurred. Recently, Tandon et al. (2019) introduced the WIQA dataset that contains such problems, requiring prediction of the effect of perturbations in a procedural text. They also presented several strong models on this task. However, it is unclear whether those high scores indicate that the mod3345 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3345–3355 c November 16 - 20, 2020. 2020 Association for Computational Linguistics els fully understand the described procedures, i.e., that the models have knowledge of the causal chain from perturbation to effect. To test this, Tandon et al. (2019) also prop"
2020.findings-emnlp.300,D19-1002,0,0.0325042,". However, existing models do not explain the effects of perturbations. Explanations: Despite large-scale QA benchmarks, high scores do not necessarily reflect understanding (Min et al., 2019). Current models may not be robust or exploit annotation artifacts (Gururangan et al., 2018). This makes explanations desirable for interpretation (Selvaraju et al., 2017). Attention based explanation has been successfully used in vision tasks such as object detection (Petsiuk et al., 2018) because pixel information is explainable to humans. These and other token level attention models used in NLP tasks (Wiegreffe and Pinter, 2019) do not provide full-sentence explanations of a model’s decisions. Recently, several datasets with natural language explanations have been introduced, e.g., in natural language inference (Camburu et al., 2018), visual question answering (Park et al., 2018), and multihop reading comprehension (HotpotQA dataset) (Yang et al., 2018). In contrast to these datasets, we explain the effects of perturbations in procedural text. HotpotQA contains explanations based on two sentences from a Wikipedia paragraph. Models on the HotpotQA would not be directly applicable to our task and require substantial mo"
2020.findings-emnlp.300,D18-1259,0,0.0616642,"Selvaraju et al., 2017). Attention based explanation has been successfully used in vision tasks such as object detection (Petsiuk et al., 2018) because pixel information is explainable to humans. These and other token level attention models used in NLP tasks (Wiegreffe and Pinter, 2019) do not provide full-sentence explanations of a model’s decisions. Recently, several datasets with natural language explanations have been introduced, e.g., in natural language inference (Camburu et al., 2018), visual question answering (Park et al., 2018), and multihop reading comprehension (HotpotQA dataset) (Yang et al., 2018). In contrast to these datasets, we explain the effects of perturbations in procedural text. HotpotQA contains explanations based on two sentences from a Wikipedia paragraph. Models on the HotpotQA would not be directly applicable to our task and require substantial modification for the following reasons: (i) HotpotQA models are not trained to predict the qualitative structure (more or less of chosen explanation sentences in Figure 1). (ii) HotpotQA involves reasoning over named entities, whereas the current task focuses on common nouns and actions (models that work well on named entities need"
2020.findings-emnlp.300,D16-1264,0,0.139086,"Missing"
2020.findings-emnlp.300,Q18-1015,0,0.0238835,"the effects of perturbations in procedural text. HotpotQA contains explanations based on two sentences from a Wikipedia paragraph. Models on the HotpotQA would not be directly applicable to our task and require substantial modification for the following reasons: (i) HotpotQA models are not trained to predict the qualitative structure (more or less of chosen explanation sentences in Figure 1). (ii) HotpotQA involves reasoning over named entities, whereas the current task focuses on common nouns and actions (models that work well on named entities need to be adapted to common nouns and actions (Sedghi and Sabharwal, 2018)). (iii) explanation paragraphs in HotpotQA are not 3346 ears less protected → (MORE/+) sound enters the ear → (MORE/+) sound hits ear drum → (MORE/+) more sound detected blood clotting disorder → (LESS/-) blood clots → (LESS/-) scab forms → (MORE/+) less scab formation breathing exercise → (MORE/+) air enters lungs → (MORE/+) air enters windpipe → (MORE/+) oxygen enters bloodstream squirrels store food → (MORE/+) squirrels eat more → (MORE/+) squirrels gain weight → (MORE/+) hard survival in winter less trucks run → (LESS/-) trucks go to refineries → (LESS/-) trucks carry oil → (MORE/+) less"
2020.findings-emnlp.300,N18-1023,0,\N,Missing
2020.findings-emnlp.300,P16-1162,0,\N,Missing
2020.findings-emnlp.344,S19-1028,0,0.0761485,"ch specific event the tweet refers to, hence remove the event specific bias through a reversal gradient. Our experiments represent a reallife crisis management scenario, where the model is evaluated on a new incoming event through a leaveone-out experimental setup, and show substantial improvement over baseline classification methods. Finally, we share our code for reproducibility and ease of use1 . 2 Munro (2011) proposes a system based on a set of features (location, time, n-grams) to label text messages as actionable/ non-actionable. Most recently, the TREC-IS challenge by Mccreadie et al. (2019) proposes a labeling scheme where the actionability of a tweet is replaced by the information type and the criticality score. Higher criticality indicates a post contains more relevant information that could be useful for public safety officers during an emergency. Although Miyazaki et al. (2019) shows a great improvement on information type extraction by using Bi-LSTM attention on BERT embeddings, identifying critical and actionable information is a much harder task (Mccreadie et al., 2019). Related Work Recent work on crisis informatics focuses on developing NLP solutions to classify and ext"
2020.findings-emnlp.344,N19-1423,0,0.0143729,"p the gradient from ce is reversed and scaled by a value λ. In our work, we intend to achieve domain adaptation from previous events to a new incoming event by minimizing the information related to previously seen events provided by ce , while maximizing the information gain obtained from classifier cr , as described in eq. 3. argmin L(cr (h(ti )), yri ) + L(ce (gλ (h(ti ))), yei ) 3861 h,cr ,ce (3) 4 Experiments For our experiments we used two of the main popular word embeddings to represent the tokens of the tweets in the target dataset: GloVe (Pennington et al., 2014) embeddings, and BERT (Devlin et al., 2019) embeddings. We used the 100-dimensional GloVe embeddings pre-trained on Wikipedia and Gigaword, which were made publicly available by the authors3 . For extracting BERT embeddings we used the Python package bert-embeddings4 as we built the networks for our experiments in PyTorch. This package offers a pre-trained 768-dimensional hidden state transformer model with 12-layers and 12-headed attention. In our experiments, the BERT model was frozen with no fine-tuning during training. Throughout all of our experiments the tweet encoder h is an LSTM with two layers. Each of the LSTMs have a hidden"
2020.findings-emnlp.344,D18-1002,0,0.0282786,"actionable information from a stream of messages as the one provided by Twitter. 1 https://salmedina.github.io/EventBiasRemoval/ Processing information without the context of a crisis event is a bottleneck for big data crisis analytics, as discussed by Qadir et al. (2016). The lack of context makes the classification of messages very difficult, since the models are prone to eventspecific biases. Due to the fact that we deal with real-time data, a domain-adaptation approach cannot use fine-tuning in a zero-shot scenario, which results in highly-biased models. Most recent work on bias removal (Elazar and Goldberg, 2018) focuses on using adversarial learning to remove demographic bias from representations. Examples include adversarial generative networks that create fair representations (Madras et al., 2018), metrics to quantify unintended biases (Borkan et al., 2019) and applications that show substantial improvements on traditional NLP tasks like NLI (Lu et al., 2018), Coreference Resolution (Belinkov et al., 2019) and text classification (Zhang et al., 2018) by using unbiased representations. Our approach is inspired by the work of Elazar and Goldberg (2018) on bias removal through an adversarial attack. T"
2020.findings-emnlp.344,L16-1259,0,0.135274,"akes Typhoons Floods Figure 1: Crisis NLP Dataset Distribution. Outer circle: Color defines each of the event categories. Inner circle: The shade of colors describe the different events within a category. 3 Approach In this work we used data from the TREC 2018 Incident Streams challenge2 , which contains labels on criticality and information types (Mccreadie et al., 2019). They define criticality as a score to identify posts that need to be shown to an officer immediately as an alert. The raw data and information about the specific event each tweet belongs to is extracted from the Crisis NLP (Imran et al., 2016) dataset, which contains tweets in English from disaster events that occurred during 2012-2018. The crisis events in our dataset can be split into five main groups: earthquakes, floods, typhoons, wildfires and attacks. In Figure 1, we show that the data mainly consists of multiple earthquake, flood, and typhoon events, only two wildfire events, and five diverse attacks originated by humans. 3.1 high, and critical. The distribution of the labels is highly skewed towards the low and medium labels as shown in Figure 2a. These types of tweets do not provide important information for decision-makin"
2020.findings-emnlp.344,N16-1082,1,0.81272,"ets that contain named entities (e.g. locations, names) or information that is generally important to classify a tweet, such as casualties. For this part, we only used GloVe embeddings, since BERT is context-based and each embedding may encode information from the rest of the tweet. In order to construct the saliency map, we used back-propagation to estimate the first-order derivatives from each word, as a measure of their contribution to the model’s decision. This strategy was adopted from the vision community (Erhan et al., 2009; Simonyan et al., 2013), and recently adapted in NLP research (Li et al., 2016). In Figure 4 we visualize the saliency map of each word embedding for the baseline and adversarial models. The higher the absolute value of the first-order derivative (dark blue and white), the more important role it plays into the classifier’s decision. We observe that, for the first and second sentences, the baseline puts more weight on the location, which is a strong event-bias since it includes information only for a particular event and 3864 ordering mandatory evacuations in the southmoor park neighborhood colorado flooding evacuations broken oil pipeline in weld county president updates"
2020.findings-emnlp.344,P17-1001,0,0.0391469,"pproach is inspired by the work of Elazar and Goldberg (2018) on bias removal through an adversarial attack. The authors use an adversarial setting to remove demographic information from text and construct cleaner representations. In our case, the adversarial classifier attempts to predict the event to which the tweet belongs. Another difference with our work is the imbalanced data used for training the classifier of the main task. Other related work includes domain adaptation based on a gradientreversal layer (Ganin et al., 2016), text classification based on adversarial multi-task learning (Liu et al., 2017), and multi-adversarial domain adaptation across multi-modal data (Pei et al., 2018). 3859 2015 Nepal Earthquake 2014 Chile Earthquake 2012 Costa Rica earthquake 2012 Guatemala earthquake 2012 Italy earthquake 2013 Alberta floods 2013 Queensland floods 2012 Philipinnes floods 2013 Manila floods 2013 Colorado floods 2014 Philippines Typhoon Hagupit 2013 Typhoon Yolanda 2012 Typhoon Pablo 2015 Paris attack 2013 Boston_bombings 2018 Florida shooting 2013 West Texas explosion 2013 LA airport shootings 2013 Australia bushfire 2012 Colorado wildfires Wildfires Attacks Earthquakes Typhoons Floods Fig"
2020.findings-emnlp.344,D19-1660,0,0.168776,"ntial improvement over baseline classification methods. Finally, we share our code for reproducibility and ease of use1 . 2 Munro (2011) proposes a system based on a set of features (location, time, n-grams) to label text messages as actionable/ non-actionable. Most recently, the TREC-IS challenge by Mccreadie et al. (2019) proposes a labeling scheme where the actionability of a tweet is replaced by the information type and the criticality score. Higher criticality indicates a post contains more relevant information that could be useful for public safety officers during an emergency. Although Miyazaki et al. (2019) shows a great improvement on information type extraction by using Bi-LSTM attention on BERT embeddings, identifying critical and actionable information is a much harder task (Mccreadie et al., 2019). Related Work Recent work on crisis informatics focuses on developing NLP solutions to classify and extract information from Twitter streams and other social media data related to an emergency event (e.g. attacks, natural disasters). As discussed by Tapia et al. (2011b), there are several problems under the umbrella of crisis informatics, such as determining if a snippet of text is related to a sp"
2020.findings-emnlp.344,W11-0309,0,0.0375808,"events without any fine-tuning. Since the main task is to classify the importance of the information contained in a tweet (criticality), we use an adversarial classifier that intends to learn which specific event the tweet refers to, hence remove the event specific bias through a reversal gradient. Our experiments represent a reallife crisis management scenario, where the model is evaluated on a new incoming event through a leaveone-out experimental setup, and show substantial improvement over baseline classification methods. Finally, we share our code for reproducibility and ease of use1 . 2 Munro (2011) proposes a system based on a set of features (location, time, n-grams) to label text messages as actionable/ non-actionable. Most recently, the TREC-IS challenge by Mccreadie et al. (2019) proposes a labeling scheme where the actionability of a tweet is replaced by the information type and the criticality score. Higher criticality indicates a post contains more relevant information that could be useful for public safety officers during an emergency. Although Miyazaki et al. (2019) shows a great improvement on information type extraction by using Bi-LSTM attention on BERT embeddings, identifyi"
2020.findings-emnlp.344,I11-1108,0,0.136782,"thy, the type of information it contains, whether the information is actionable, etc. Most previous work focuses on the relevance problem: given a set of tweets or other source of information and a specific event, classify which data refer to that event. Caragea et al. (2016) uses a CNN model to classify tweets related to flood events, while Kruspe (2019) uses a few-shot learning model based on a CNN. Nguyen et al. (2016) also uses a CNN model to classify related tweets and the type of information contained (e.g. infrastructure damage, affected individuals etc) from the Nepal 2015 earthquake. Neubig et al. (2011) introduces a real-time system for the Japan 2011 earthquake that classifies the relatedness of the posts and extracts surface information like named entities. Other approaches include BiLSTM models for tweet classification (Ma), event detection based on Twitter streams (Sakaki et al., 2010), adversarial data augmentation for image classification (Pouyanfar et al., 2019) and domain-adaptation across different events using an adversarial network. It is particularly important to first responders the identification of actionable information from a stream of messages as the one provided by Twitter"
2020.findings-emnlp.344,D14-1162,0,0.0904102,"Missing"
2020.lrec-1.127,W17-5112,0,0.0229115,"oritized on potential positive instances, which are harder to recognize automatically with high precision. 2. 2.1. Background Argument Mining and Proposition Types Argument mining is an expansive field with many applications. Datasets include the Internet Argument Corpus for 1008 1 https://github.com/yohanjo/lrec20 online debate on political topics (Walker et al., 2012; Swanson et al., 2015), student argument in course essays (Stab and Gurevych, 2017), and parliamentary debate (Duthie et al., 2016). State-of-the-art results have been produced using a range of methods including random forests (Aker et al., 2017), integer linear programming for constraint-based inference (Persing and Ng, 2016), graph-based methods that focus on relations between claims (Niculae et al., 2017; Nguyen and Litman, 2018), and more recently, end-to-end neural methods (Cocarascu and Toni, 2018; Frau et al., 2019). But these systems struggle to distinguish between distinctions in argumentative strategy that look intuitively obvious to casual observers, instead relying on coarse notions of claims and supports. Today, automated systems fail to understand the nuanced factuality of these sentences when they appear in argumentatio"
2020.lrec-1.127,W11-0707,0,0.134177,"Missing"
2020.lrec-1.127,J18-4011,0,0.02498,"Argument Corpus for 1008 1 https://github.com/yohanjo/lrec20 online debate on political topics (Walker et al., 2012; Swanson et al., 2015), student argument in course essays (Stab and Gurevych, 2017), and parliamentary debate (Duthie et al., 2016). State-of-the-art results have been produced using a range of methods including random forests (Aker et al., 2017), integer linear programming for constraint-based inference (Persing and Ng, 2016), graph-based methods that focus on relations between claims (Niculae et al., 2017; Nguyen and Litman, 2018), and more recently, end-to-end neural methods (Cocarascu and Toni, 2018; Frau et al., 2019). But these systems struggle to distinguish between distinctions in argumentative strategy that look intuitively obvious to casual observers, instead relying on coarse notions of claims and supports. Today, automated systems fail to understand the nuanced factuality of these sentences when they appear in argumentation. Perceived factuality of propositions is heavily tied to a speaker’s intent (Wentzel et al., 2010); this concept of speakers making claims with only partial certainty or factuality have been collectively studied under the umbrella term of “commitment” to a tru"
2020.lrec-1.127,P13-2022,0,0.0220971,"ate I’d like to see everyone get a fair shot at expressing themselves”. In practical reasoning, a normative conclusion is supported by a certain goal to achieve, and this goal is often expressed as another normative proposition or a desire as in “let’s have paid family leave, because I want us to do more to support people who are struggling to balance family and work.” Prior work has paid little attention to annotating desire propositions. In NLP, the closest work is in subjectivity annotation and the more narrow task of annotating subjectively beneficial events (Somasundaran and Wiebe, 2010; Deng et al., 2013), but these approaches have typically been applied in the context of sentiment analysis; our approach focusing on argument is, to our knowledge, a new contribution in computational linguistics. 4.3. Future Possibility A future possibility proposition claims a possibility or prediction that something may be the case in the future. These future possibilities are independent of whether the speaker desires the forecast to be true, or believes they should be true; the claimed future possibility is just the speaker’s own, or someone else’s, belief about what the future may hold: “US shooting down a"
2020.lrec-1.127,P19-2059,0,0.124786,"tion theory and communication sciences, propositions are typically divided into three types: fact, value, and policy (Hollihan and Baaske, 2015; Wagemans, 2016). Propositions of fact have contents whose truth value is verifiable with empirical evidence, whereas propositions of value are subjective judgments. Propositions of policy propose that an action be carried out. These types have been extended by prior studies. For instance, Park and Cardie (2018) extended fact into non-experiential fact and testimony, and added reference—a text of information source (but not reported speech in itself). Egawa et al. (2019) further added rhetorical statement, judgments of value using figurative language and discourse structure. More recent reviews of the field have made similar observations (Lawrence and Reed, 2019; Janier and Saint-Dizier, 2019). Researchers have suspected that part of the challenge in these problems is data collection and reliable annotation. Collecting span- and sentence-level annotations is a frequently used tool for machine learning researchers seeking to improve their systems. Accurate annotation is timeconsuming and expensive, though, and even when funding is available, annotation tasks o"
2020.lrec-1.127,P12-2034,0,0.0407649,"Missing"
2020.lrec-1.127,W19-3621,0,0.0203135,"Missing"
2020.lrec-1.127,L16-1259,0,0.0229201,"collection and reliable annotation. Collecting span- and sentence-level annotations is a frequently used tool for machine learning researchers seeking to improve their systems. Accurate annotation is timeconsuming and expensive, though, and even when funding is available, annotation tasks often require subject matter expertise that comes from either lived experience or extensive training. This problem is exacerbated by rare phenomena, which results in imbalanced datasets in many domains, like emotional crisis or suicidal ideation detection online and in medical records (Pestian et al., 2012; Imran et al., 2016; Losada and Crestani, 2016), rare occurrence of high- and low-end scores in student data in education domains (Woods et al., 2017; Lugini and Litman, 2018), and rare social behaviors in healthcare settings (Mayfield et al., 2013; Carrell et al., 2016). Our annotation also handles rare phenomena, 1009 and using a conventional annotation methodology allows only moderate inter-annotator agreement even after intensive annotator training, reflecting the difficulty of our task. Many previous papers on text annotation have relied on crowdsourcing, relying on inexperienced editors on services such as"
2020.lrec-1.127,D19-1630,0,0.0342361,"Missing"
2020.lrec-1.127,N18-1010,1,0.894268,"Missing"
2020.lrec-1.127,W19-4502,1,0.365982,"et al., 2011), but the ways of referring to task-oriented norms in these pages are different from general reported speech in argumentation. Park et al. (2015) annotated references (e.g., URLs) in policy-related argumentation, but reported speech was not included as references. As a methodological note, in the original corpus the pronoun “I” has been resolved to the speaker’s name in the process of annotating propositions from locutions (e.g., for the sentence “I believe Americans do have the ability to give their kids a better future”, “I believe” has been replaced with “O’MALLEY believes”) (Jo et al., 2019). As a result, it is difficult to tell whether the source of a reported speech proposition is indeed the speaker or not. For annotation, we are faithful to the text of each proposition as it is, resulting in many instances of reported speech that can be used for machine learning. Since some of these instances are not reported speech in the original debates, however, instances are not treated as reported speech in our analysis experiments (§6.) if the source and the speaker are identical. Reported Speech Our last proposition type is reported speech. A reported speech proposition must convey an"
2020.lrec-1.127,C18-2002,0,0.0363102,"Missing"
2020.lrec-1.127,J19-4006,1,0.751925,"tents whose truth value is verifiable with empirical evidence, whereas propositions of value are subjective judgments. Propositions of policy propose that an action be carried out. These types have been extended by prior studies. For instance, Park and Cardie (2018) extended fact into non-experiential fact and testimony, and added reference—a text of information source (but not reported speech in itself). Egawa et al. (2019) further added rhetorical statement, judgments of value using figurative language and discourse structure. More recent reviews of the field have made similar observations (Lawrence and Reed, 2019; Janier and Saint-Dizier, 2019). Researchers have suspected that part of the challenge in these problems is data collection and reliable annotation. Collecting span- and sentence-level annotations is a frequently used tool for machine learning researchers seeking to improve their systems. Accurate annotation is timeconsuming and expensive, though, and even when funding is available, annotation tasks often require subject matter expertise that comes from either lived experience or extensive training. This problem is exacerbated by rare phenomena, which results in imbalanced datasets in many do"
2020.lrec-1.127,W19-4012,1,0.716029,"and the form(s) of one or more premises. As an example, the scheme of argument from consequences is as follows: Premise: If A is brought about, good consequences will plausibly occur. Conclusion: A should be brought about. These schemes have been adopted by many studies as a framework for analyzing reasoning patterns (Song et al., 2017; Nussbaum, 2011). Researchers in computational linguistics have tried to code the schemes, but this task turned out to be very challenging; as a result, annotations have low agreement between annotators (Lindahl et al., 2019) or are available only from experts (Lawrence et al., 2019). But different schemes are associated with different proposition types, and therefore, we speculate that reliably annotating proposition types may ease the annotation of argumentation schemes. The proposition types in this paper are closely related to common argumentation schemes, including practical reasoning, argument from consequence, argument from cause to effect, and argument from expert opinion. 2.2. Efficient Linguistic Annotation In their overview of argument mining today, Lippi and Torroni (2016) identify three key challenges that limit the field: “How can we criticize China for impr"
2020.lrec-1.127,W17-1604,0,0.0641432,"Missing"
2020.lrec-1.127,W19-4520,0,0.0233903,"daily life. Each scheme defines the form of a conclusion and the form(s) of one or more premises. As an example, the scheme of argument from consequences is as follows: Premise: If A is brought about, good consequences will plausibly occur. Conclusion: A should be brought about. These schemes have been adopted by many studies as a framework for analyzing reasoning patterns (Song et al., 2017; Nussbaum, 2011). Researchers in computational linguistics have tried to code the schemes, but this task turned out to be very challenging; as a result, annotations have low agreement between annotators (Lindahl et al., 2019) or are available only from experts (Lawrence et al., 2019). But different schemes are associated with different proposition types, and therefore, we speculate that reliably annotating proposition types may ease the annotation of argumentation schemes. The proposition types in this paper are closely related to common argumentation schemes, including practical reasoning, argument from consequence, argument from cause to effect, and argument from expert opinion. 2.2. Efficient Linguistic Annotation In their overview of argument mining today, Lippi and Torroni (2016) identify three key challenges"
2020.lrec-1.127,W18-5208,0,0.0189518,"g to improve their systems. Accurate annotation is timeconsuming and expensive, though, and even when funding is available, annotation tasks often require subject matter expertise that comes from either lived experience or extensive training. This problem is exacerbated by rare phenomena, which results in imbalanced datasets in many domains, like emotional crisis or suicidal ideation detection online and in medical records (Pestian et al., 2012; Imran et al., 2016; Losada and Crestani, 2016), rare occurrence of high- and low-end scores in student data in education domains (Woods et al., 2017; Lugini and Litman, 2018), and rare social behaviors in healthcare settings (Mayfield et al., 2013; Carrell et al., 2016). Our annotation also handles rare phenomena, 1009 and using a conventional annotation methodology allows only moderate inter-annotator agreement even after intensive annotator training, reflecting the difficulty of our task. Many previous papers on text annotation have relied on crowdsourcing, relying on inexperienced editors on services such as Crowdflower and Amazon Mechanical Turk (Snow et al., 2008; Swanson et al., 2015). While this approach works for many common-sense tasks, prior work has sho"
2020.lrec-1.127,P13-1011,1,0.805785,"Missing"
2020.lrec-1.127,P17-1091,0,0.0277018,"s Argument mining is an expansive field with many applications. Datasets include the Internet Argument Corpus for 1008 1 https://github.com/yohanjo/lrec20 online debate on political topics (Walker et al., 2012; Swanson et al., 2015), student argument in course essays (Stab and Gurevych, 2017), and parliamentary debate (Duthie et al., 2016). State-of-the-art results have been produced using a range of methods including random forests (Aker et al., 2017), integer linear programming for constraint-based inference (Persing and Ng, 2016), graph-based methods that focus on relations between claims (Niculae et al., 2017; Nguyen and Litman, 2018), and more recently, end-to-end neural methods (Cocarascu and Toni, 2018; Frau et al., 2019). But these systems struggle to distinguish between distinctions in argumentative strategy that look intuitively obvious to casual observers, instead relying on coarse notions of claims and supports. Today, automated systems fail to understand the nuanced factuality of these sentences when they appear in argumentation. Perceived factuality of propositions is heavily tied to a speaker’s intent (Wentzel et al., 2010); this concept of speakers making claims with only partial certa"
2020.lrec-1.127,L18-1257,0,0.0704091,"sh between claims they know to be true, desires they have for the future, amid other epistemological states of reported knowledge (Walton et al., 2008). In argumentation theory and communication sciences, propositions are typically divided into three types: fact, value, and policy (Hollihan and Baaske, 2015; Wagemans, 2016). Propositions of fact have contents whose truth value is verifiable with empirical evidence, whereas propositions of value are subjective judgments. Propositions of policy propose that an action be carried out. These types have been extended by prior studies. For instance, Park and Cardie (2018) extended fact into non-experiential fact and testimony, and added reference—a text of information source (but not reported speech in itself). Egawa et al. (2019) further added rhetorical statement, judgments of value using figurative language and discourse structure. More recent reviews of the field have made similar observations (Lawrence and Reed, 2019; Janier and Saint-Dizier, 2019). Researchers have suspected that part of the challenge in these problems is data collection and reliable annotation. Collecting span- and sentence-level annotations is a frequently used tool for machine learnin"
2020.lrec-1.127,N16-1164,0,0.0737686,"ically with high precision. 2. 2.1. Background Argument Mining and Proposition Types Argument mining is an expansive field with many applications. Datasets include the Internet Argument Corpus for 1008 1 https://github.com/yohanjo/lrec20 online debate on political topics (Walker et al., 2012; Swanson et al., 2015), student argument in course essays (Stab and Gurevych, 2017), and parliamentary debate (Duthie et al., 2016). State-of-the-art results have been produced using a range of methods including random forests (Aker et al., 2017), integer linear programming for constraint-based inference (Persing and Ng, 2016), graph-based methods that focus on relations between claims (Niculae et al., 2017; Nguyen and Litman, 2018), and more recently, end-to-end neural methods (Cocarascu and Toni, 2018; Frau et al., 2019). But these systems struggle to distinguish between distinctions in argumentative strategy that look intuitively obvious to casual observers, instead relying on coarse notions of claims and supports. Today, automated systems fail to understand the nuanced factuality of these sentences when they appear in argumentation. Perceived factuality of propositions is heavily tied to a speaker’s intent (Wen"
2020.lrec-1.127,pianta-etal-2008-textpro,0,0.0234319,"hand, the expressiveness and fidelity of the linguistic construct they are attempting to capture, and on the other the potential for operationalization and quantification in coding manuals and fully automated systems. Particularly in imbalanced tasks, these choices can have the effect of producing an inaccurate picture of the minority class and producing datasets that are no longer a valid representation of the original construct (Corbett-Davies and Goel, 2018). To expedite annotation without sacrificing validity, researchers have developed annotation tools that incorporate machine learning (Pianta et al., 2008; Yimam et al., 2014; Klie et al., 2018). These tools train a machine learning algorithm on a subset of annotations and suggest predicted annotations for new data, producing a hybrid “human-in-theloop” model (da Silva et al., 2019). Our work here follows in this tradition, seeking effective and efficient methods for collecting reliable new data. 3. Domain Description For all annotation and experiments in this work, we use transcripts of the 2016 U.S. presidential debates and reaction to the debates on Reddit (Visser et al., 2019). This corpus is appropriate for our task as it includes various"
2020.lrec-1.127,P18-1080,0,0.0206267,"Missing"
2020.lrec-1.127,D08-1027,0,0.122768,"Missing"
2020.lrec-1.127,W10-0214,0,0.0235106,"e very least for the first debate I’d like to see everyone get a fair shot at expressing themselves”. In practical reasoning, a normative conclusion is supported by a certain goal to achieve, and this goal is often expressed as another normative proposition or a desire as in “let’s have paid family leave, because I want us to do more to support people who are struggling to balance family and work.” Prior work has paid little attention to annotating desire propositions. In NLP, the closest work is in subjectivity annotation and the more narrow task of annotating subjectively beneficial events (Somasundaran and Wiebe, 2010; Deng et al., 2013), but these approaches have typically been applied in the context of sentiment analysis; our approach focusing on argument is, to our knowledge, a new contribution in computational linguistics. 4.3. Future Possibility A future possibility proposition claims a possibility or prediction that something may be the case in the future. These future possibilities are independent of whether the speaker desires the forecast to be true, or believes they should be true; the claimed future possibility is just the speaker’s own, or someone else’s, belief about what the future may hold:"
2020.lrec-1.127,J17-3005,0,0.027331,"ts. This method produces efficient machine filtering, especially of likely negative instances, which covers a large percentage of our corpus. Human annotator time is prioritized on potential positive instances, which are harder to recognize automatically with high precision. 2. 2.1. Background Argument Mining and Proposition Types Argument mining is an expansive field with many applications. Datasets include the Internet Argument Corpus for 1008 1 https://github.com/yohanjo/lrec20 online debate on political topics (Walker et al., 2012; Swanson et al., 2015), student argument in course essays (Stab and Gurevych, 2017), and parliamentary debate (Duthie et al., 2016). State-of-the-art results have been produced using a range of methods including random forests (Aker et al., 2017), integer linear programming for constraint-based inference (Persing and Ng, 2016), graph-based methods that focus on relations between claims (Niculae et al., 2017; Nguyen and Litman, 2018), and more recently, end-to-end neural methods (Cocarascu and Toni, 2018; Frau et al., 2019). But these systems struggle to distinguish between distinctions in argumentative strategy that look intuitively obvious to casual observers, instead relyi"
2020.lrec-1.127,W15-4631,0,0.16917,"tices for hybrid human-machine systems for building datasets. This method produces efficient machine filtering, especially of likely negative instances, which covers a large percentage of our corpus. Human annotator time is prioritized on potential positive instances, which are harder to recognize automatically with high precision. 2. 2.1. Background Argument Mining and Proposition Types Argument mining is an expansive field with many applications. Datasets include the Internet Argument Corpus for 1008 1 https://github.com/yohanjo/lrec20 online debate on political topics (Walker et al., 2012; Swanson et al., 2015), student argument in course essays (Stab and Gurevych, 2017), and parliamentary debate (Duthie et al., 2016). State-of-the-art results have been produced using a range of methods including random forests (Aker et al., 2017), integer linear programming for constraint-based inference (Persing and Ng, 2016), graph-based methods that focus on relations between claims (Niculae et al., 2017; Nguyen and Litman, 2018), and more recently, end-to-end neural methods (Cocarascu and Toni, 2018; Frau et al., 2019). But these systems struggle to distinguish between distinctions in argumentative strategy tha"
2020.lrec-1.127,N18-1074,0,0.0602645,"Missing"
2020.lrec-1.127,walker-etal-2012-corpus,0,0.020699,"additional best practices for hybrid human-machine systems for building datasets. This method produces efficient machine filtering, especially of likely negative instances, which covers a large percentage of our corpus. Human annotator time is prioritized on potential positive instances, which are harder to recognize automatically with high precision. 2. 2.1. Background Argument Mining and Proposition Types Argument mining is an expansive field with many applications. Datasets include the Internet Argument Corpus for 1008 1 https://github.com/yohanjo/lrec20 online debate on political topics (Walker et al., 2012; Swanson et al., 2015), student argument in course essays (Stab and Gurevych, 2017), and parliamentary debate (Duthie et al., 2016). State-of-the-art results have been produced using a range of methods including random forests (Aker et al., 2017), integer linear programming for constraint-based inference (Persing and Ng, 2016), graph-based methods that focus on relations between claims (Niculae et al., 2017; Nguyen and Litman, 2018), and more recently, end-to-end neural methods (Cocarascu and Toni, 2018; Frau et al., 2019). But these systems struggle to distinguish between distinctions in arg"
2020.lrec-1.127,P14-5016,0,0.0382671,"Missing"
2020.sdp-1.1,2020.sdp-1.24,1,0.849693,"Missing"
2020.sdp-1.1,P19-1204,1,0.763326,"ence becomes increasingly important. 3.1 LaySumm 3.3 LongSumm The LongSumm task aims at creating long summaries of around 600 words. Often, for researchers, short summaries (e.g., abstract) are not detailed enough. Thus, longer summaries are mainly intended for helping researchers understand the gist of a paper without the need to read it entirely. The corpus for this task includes a training set that consists of 1705 extractive summaries, and 531 abstractive summaries of NLP and Machine Learning scientific papers. The extractive summaries are based on video talks from associated conferences (Lev et al., 2019) while the abstractive summaries are based on blog posts created by NLP and ML researchers. The test set consists of 22 abstractive summaries for evaluating the submissions. In total, 9 systems participated in the task, with a total of 100 submissions. The CL-SciSumm CL-SciSumm is the first medium-scale shared task on scientific document summarization in the computational linguistics domain with over 500 documents annotated for their citation and citation targets and over a 1000 more documents with human annotated summaries inherited and integrated from SciSummNet (Yasunaga et al., 2019). Last"
2020.sdp-1.1,W04-1013,0,0.104459,"Missing"
2020.sdp-1.24,2020.sdp-1.1,1,0.849693,"Missing"
2020.sdp-1.24,D19-3036,1,0.787409,"Lay Summaries’, or summaries that explain the science contained within the paper in laymen’s terms. Finally, the LongSumm (Long Scientific Document Summarization) task focuses on generating long summaries of scientific text. It is fundamentally different than generating short summaries that mostly aim at teasing the reader. The LongSumm task strives to learn how to cover the salient information conveyed in a given scientific document, taking into account the characteristics and the structure of the text. The motivation for LongSumm was first demonstrated by the IBM Science Summarizer system, (Erera et al., 2019) that retrieves and creates long summaries of scientific documents1 . While Erera et al. (2019) studied some use-cases and proposed a summarization approach with some human evaluation, the authors stressed the need of a large dataset that will unleash the research in this domain. LongSumm aims at filling this gap by providing large dataset of long summaries which are based on blogs written by Machine Learning and NLP experts. In this paper we present the tasks, datasets, description of the participating systems, and provide their results and insights from shared tasks. 2 CL-SciSumm 2.1 Overvie"
2020.sdp-1.24,2020.sdp-1.37,0,0.380967,"Missing"
2020.sdp-1.24,2020.sdp-1.38,0,0.0145278,"ab.org/ competitions/25516 Dimsum (Tiezheng Yu and Fung, 2020) - The system generates a summary by using a joint extractive and abstractive summarization approach, based on the intuition that lay summaries are grounded in sentences that occur within the scientific document. The abstractive summaries are converted to extractive labels, by selecting sentences that maximize the rouge score with the reference summary. The BART encoder (Lewis et al., 2020) is then used to make sentence representations and the model is trained with both extractive and abstractive summarization objectives. Seungwon (Kim, 2020) - The system built by the team from Georgia Tech primarily uses the PEGASUS model (Zhang et al., 2019) to generate lay summaries, combining this with a BERT-based extractive summarization model. After generating a lay summary using PEGASUS, if the generated summary is shorter than a specified length, the extractive model is used to identify candidate sentences in the document that can be included in the summary. Sentences are only included in the summary by the extractive model if they are judged sufficiently readable, according to a sentence readability metric defined by the authors. IIITBH-"
2020.sdp-1.24,P19-1204,1,0.760781,"n be found in research blogs. To address this point, the LongSumm task opted to leverage blog posts created by researchers in the NLP and Machine learning communities that summarize scientific articles and use these posts as reference summaries (Boni et al., 2020). The task is, given a scientific document, generate a 600 words summary. 4.1.1 Corpus The corpus for this task includes a training set that consists of 1705 extractive summaries, and 531 abstractive summaries of NLP and Machine Learning scientific papers. The extractive summaries are based on video talks from associated conferences (Lev et al., 2019), and contain up to 30 sentences. The abstractive summaries are blog posts created by NLP and ML researchers, with length varied between 100-1500 words, an average of 779 (±460) words, and an average of 31 (±18) sentences in a summary. In addition, we created a (blind) test set of 22 abstractive summaries for evalTable 3: ROUGE Recall and F-Measure evaluation on LaySumm test set System Rouge1-F1 Rouge1-Recall Rouge2-F1 Rouge2-Recall RougeL-F1 RougeL-Recall HYTZ seungwonkim Summaformers AUTH DUCS IIITBH-IITP Harita ramesh babu IITP-AI-NLP-ML 0.4600 0.4596 0.4594 0.4456 0.4253 0.4048 0.3524 0.31"
2020.sdp-1.24,2020.acl-main.703,0,0.163464,"generate lay summaries, using the article abstract as input and the lay summary as the reference for training the summarization model. 8 https://competitions.codalab.org/ competitions/25516 Dimsum (Tiezheng Yu and Fung, 2020) - The system generates a summary by using a joint extractive and abstractive summarization approach, based on the intuition that lay summaries are grounded in sentences that occur within the scientific document. The abstractive summaries are converted to extractive labels, by selecting sentences that maximize the rouge score with the reference summary. The BART encoder (Lewis et al., 2020) is then used to make sentence representations and the model is trained with both extractive and abstractive summarization objectives. Seungwon (Kim, 2020) - The system built by the team from Georgia Tech primarily uses the PEGASUS model (Zhang et al., 2019) to generate lay summaries, combining this with a BERT-based extractive summarization model. After generating a lay summary using PEGASUS, if the generated summary is shorter than a specified length, the extractive model is used to identify candidate sentences in the document that can be included in the summary. Sentences are only included"
2020.sdp-1.24,2020.sdp-1.25,0,0.716274,"maries of the research paper: the reference paper’s abstract, a community summary, and a human summary. We provisioned the evaluation scripts and goldtest-set CL-SciSumm Github repository4 . For transparency we published all the system runs submitted by the participants. The participants then ran the evaluation and reported the results back to us. We collate and publish these as the CL-SciSumm’20 official result. 2.2 Systems Overview Following teams submitted systems for evaluation for Task 1a and 1b. Their systems are described in their cited systems papers: NJUST (Zhang et al., 2020), CIST (Li et al., 2020), AUTH (Gidiotis et al., 2020), CiteQA (Umapathy et al., 2020), IIITBH-IITP (Reddy et al., 2020), IITP-AI-NLPML (Mishra et al., 2020), MLU (Huang and Krylova, 2020), MLUHW (Boltze et al., 2020), UniHD (Aumiller et al., 2020), NLP-PINGANTECH (Chai et al., 2020) Following teams submitted systems for evaluation on Task 2 also which is an optional bonus task: AUTH (Gidiotis et al., 2020), CIST (Li et al., 2020), IIITBH-IITP (Reddy et al., 2020), IITP-AI-NLPML (Mishra et al., 2020) Official evaluation results on these systems is presented in the next section. 4 github.com/WING-NUS/scisumm-corpus 2."
2020.sdp-1.24,W04-1013,0,0.39209,". 2.1.3 Evaluation An automatic evaluation script was used to measure system performance for Task 1A, in terms of the sentence ID overlaps between the sentences identified in system output, versus the gold standard created by human annotators. The raw number of overlapping sentences were used to calculate the precision, recall and F1 score for each system. We followed the approach in most SemEval tasks in reporting the overall system performance as its micro-averaged performance over all topics in the blind test set. Additionally, we calculated lexical overlaps in terms of the ROUGE-2 scores (Lin, 2004) between the system output and the human annotated gold standard reference spans. We have been reporting ROUGE score since CLSciSumm-17, for Tasks 1a and Task 2. Task 1B was evaluated as a proportion of the correctly classified discourse facets by the system, contingent on the expected response of Task 1A. As it is a multi-label classification task, we report classification performance in terms of precision, recall and F1 scores averaged over the 4 classes. Task 2 was optional, and also evaluated using the ROUGE–2 between the system output and three types of gold standard summaries of the rese"
2020.sdp-1.24,D19-1387,0,0.237389,"active sentence classification method. They develop an unsupervised approach, selecting sentences from the document using variants of the maximum marginal relevance (MMR) metric. Summaformers (Roy et al., 2020) - This system utilizes the BART model (Lewis et al., 2020) to generate summaries. BART is trained on the CNN/Dailymail summarization dataset (See et al., 2017) and fine-tuned on the Laysumm corpus. IITP-AI-NLP-ML (Mishra et al., 2020) This method uses a standard encoder-decoder framework for abstractive summarization. The system is based on BERT fine-tuned on the CNN/Dailymail dataset (Liu and Lapata, 2019a), with a decoder consisting of six transformer layers. DUCS (Chaturvedi et al., 2020) This system uses a two-stage pipeline. In the first phase, extractive summarization is performed, and relevant sentences are selected from the introduction, discussion and conclusion of the article. The abstract, and the extracted sentences from the introduction, discussion and conclusion are summarized using the BART model (Lewis et al., 2020), and the summaries are concatenated. 3.3 Results Taking these metrics into account, the top 3 systems are: #1 Seungwon Kim, #2 HYTZ, and #3 Summaformers. Next to the"
2020.sdp-1.24,2020.sdp-1.32,0,0.0826038,"Missing"
2020.sdp-1.24,2020.sdp-1.30,0,0.299995,"tion scripts and goldtest-set CL-SciSumm Github repository4 . For transparency we published all the system runs submitted by the participants. The participants then ran the evaluation and reported the results back to us. We collate and publish these as the CL-SciSumm’20 official result. 2.2 Systems Overview Following teams submitted systems for evaluation for Task 1a and 1b. Their systems are described in their cited systems papers: NJUST (Zhang et al., 2020), CIST (Li et al., 2020), AUTH (Gidiotis et al., 2020), CiteQA (Umapathy et al., 2020), IIITBH-IITP (Reddy et al., 2020), IITP-AI-NLPML (Mishra et al., 2020), MLU (Huang and Krylova, 2020), MLUHW (Boltze et al., 2020), UniHD (Aumiller et al., 2020), NLP-PINGANTECH (Chai et al., 2020) Following teams submitted systems for evaluation on Task 2 also which is an optional bonus task: AUTH (Gidiotis et al., 2020), CIST (Li et al., 2020), IIITBH-IITP (Reddy et al., 2020), IITP-AI-NLPML (Mishra et al., 2020) Official evaluation results on these systems is presented in the next section. 4 github.com/WING-NUS/scisumm-corpus 2.3 Results Out of the 11 participants systems, 8 were able complete the final evaluation correctly. We have omitted the remaining 3 te"
2020.sdp-1.24,P17-1099,0,0.0244879,"the summary. Sentences are only included in the summary by the extractive model if they are judged sufficiently readable, according to a sentence readability metric defined by the authors. IIITBH-IITP (Reddy et al., 2020) - The authors use an extractive sentence classification method. They develop an unsupervised approach, selecting sentences from the document using variants of the maximum marginal relevance (MMR) metric. Summaformers (Roy et al., 2020) - This system utilizes the BART model (Lewis et al., 2020) to generate summaries. BART is trained on the CNN/Dailymail summarization dataset (See et al., 2017) and fine-tuned on the Laysumm corpus. IITP-AI-NLP-ML (Mishra et al., 2020) This method uses a standard encoder-decoder framework for abstractive summarization. The system is based on BERT fine-tuned on the CNN/Dailymail dataset (Liu and Lapata, 2019a), with a decoder consisting of six transformer layers. DUCS (Chaturvedi et al., 2020) This system uses a two-stage pipeline. In the first phase, extractive summarization is performed, and relevant sentences are selected from the introduction, discussion and conclusion of the article. The abstract, and the extracted sentences from the introduction"
2020.sdp-1.24,P19-1212,0,0.0226249,". 2 3 https://2020.emnlp.org/ http://www.nist.gov/tac/2014 Annotation. Given each RP and its associated CPs, the annotation group was instructed to find citations to the RP in each CP. Specifically, the citation text, citation marker, reference text, and discourse facet were identified for each citation of the RP found in the CP. The corpus has 40 annotated RPs, exclusive of 1000 auto-annotated RPs added in CL-SciSumm 2019. For CL-SciSumm-20 we encourage participants to use out-of-domain data (i.e., scientific document corpora from papers outside of the ACL anthology corpora; e.g., BIGPATENT (Sharma et al., 2019)) to bootstrap training using transfer learning. From 2019 onward, Task 2, training data (summaries) has been augmented with the SciSummNet corpus (Yasunaga et al., 2019). 2.1.2 Task CL-SciSumm defined two serially dependent tasks that participants could attempt, given a canonical training and testing set of papers. Given: A topic consists of a Reference Paper (RP) and ten or more Citing Papers (CPs) that all contain citations to the RP. In each CP, the text spans (i.e., citances) have been identified that pertain to a particular citation to the RP. Additionally, the dataset provides three typ"
2020.sdp-1.24,2020.sdp-1.41,0,0.462709,"ticle summary. CIST BUPT (Li et al., 2020) - The system supports both an extractive and abstractive summaries using deep-learning architectures. For extractive summaries, they used RNN to compress and represent a sentence, and build a sentences relation graphs which are fed into the Graph Convolutional Network (GCN), and Graph Attention Network (GAN) to create a summary. For abstractive summaries, they used the gap-sentence method in (Zhang et al., 2015) to combine and transform all the data, and then T5 (Raffel et al., 2019), a transformer-liked pre-trained to fine-tune and generation. GUIR (Sotudeh et al., 2020) - A summarization method that utilizes BERT summarizer (Liu and Lapata, 2019b). The idea is based on multi-task learning heuristic, in which two tasks are optimized. The first is a binary classification task, for sentence selection. The second is section prediction, in which the model predicts section labels associated with input sentences. The extractive network is then trained to optimize both tasks. The authors also propose an abstractive summarizer based on BART (Lewis et al., 2020) transformer that runs after the extractive summarizer. IIITBH-IITP (Reddy et al., 2020) - The authors propo"
2020.starsem-1.10,W09-2508,0,0.0431508,"RT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT ‘understands’ a concept, and it cannot be expected to systematically generalize across applicable contexts.1 1 A car is a _____ vehicle Plural Grammatical Number Probe trees cars are _____ Figure 1: Illustration of BERT’s inconsistent predictions on singular and plural hypernymy probes. NLP tasks, such as recognizing textual entailment (RTE), metaphor detection, text generation and question answering (QA) (Girju et al., 2003; Dagan et al., 2006; Prager et al., 2008; Mirkin et al., 2009; Akhmatova and Dras, 2009; Mohler et al., 2013; Biran and McKeown, 2013; Yahya et al., 2013). Recently, Pretrained Language Models (PLMs), such as BERT (Devlin et al., 2019), have emerged as a popular and successful approach to a variety of NLP tasks. Thus, there has been community interest in evaluating their representations for the ‘knowledge’ they contain, including information about concept abstraction (Ettinger, 2020; Talmor et al., 2019; Jiang et al., 2020; Petroni et al., 2019). We distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrumentative and agentive. We v"
2020.starsem-1.10,E12-1004,0,0.0349348,"ed analysis, as ideally we would like AI agents to reason like humans do, it is not necessary from an instrumentative perspective if the representations offer utility for a downstream task. does not account for either of these interpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes d"
2020.starsem-1.10,C18-1152,0,0.015533,"distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrumentative and agentive. We view the instrumentative perspective as treating PLMs as a tool to mine or store knowledge, like hypernymhyponym and other relations, from text (Petroni et al., 2019; Jiang et al., 2020; Bouraoui et al., 2019; Bosselut et al., 2019; Madaan et al., 2020). The primary purpose of these investigations is to identify effective techniques to extract information from PLMs for use in downstream pipelines. In contrast, a growing body of work adopts an agentive perspective (Ettinger et al., 2018; Talmor et al., 2019), Introduction Hierarchical representations of concepts play a central role in reasoning and understanding natural language (Wellman and Gelman, 1992). They have long been studied as a core NLP objective in their own right through tasks requiring the identification of hypernyms (Hearst, 1992; Snow et al., 2005, 2006), and as components for use in downstream * Part of this work was done during an internship at Microsoft Research. 1 Diagnostic framework available at https: //github.com/AbhilashaRavichander/ probe-generalization. This work is licensed under a Creative Common"
2020.starsem-1.10,Q17-1010,0,0.0828281,"Missing"
2020.starsem-1.10,P16-1047,0,0.0620303,"Missing"
2020.starsem-1.10,P19-1470,0,0.092995,"ety of NLP tasks. Thus, there has been community interest in evaluating their representations for the ‘knowledge’ they contain, including information about concept abstraction (Ettinger, 2020; Talmor et al., 2019; Jiang et al., 2020; Petroni et al., 2019). We distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrumentative and agentive. We view the instrumentative perspective as treating PLMs as a tool to mine or store knowledge, like hypernymhyponym and other relations, from text (Petroni et al., 2019; Jiang et al., 2020; Bouraoui et al., 2019; Bosselut et al., 2019; Madaan et al., 2020). The primary purpose of these investigations is to identify effective techniques to extract information from PLMs for use in downstream pipelines. In contrast, a growing body of work adopts an agentive perspective (Ettinger et al., 2018; Talmor et al., 2019), Introduction Hierarchical representations of concepts play a central role in reasoning and understanding natural language (Wellman and Gelman, 1992). They have long been studied as a core NLP objective in their own right through tasks requiring the identification of hypernyms (Hearst, 1992; Snow et al., 2005, 2006),"
2020.starsem-1.10,W98-0707,0,0.0973099,"the NEG-136 diagnostic constructed by Ettinger (2020), selecting the affirmative contexts to test models’ use of hypernym information. Test items are drawn from a human study conducted by Fischler et al. (1983), wherein subject words are 18 concrete nouns and hypernyms belong to nine superordinate categories (Battig and Montague, 1969).6 The final diagnostic set consists of 18 prompts. LM D IAGNOSTIC E XTENDED: In this work, we additionally expand LM D IAGNOSTIC to construct a larger diagnostic set. For each superordinate category (Battig and Montague, 1969), we extract hyponyms from WordNet (Fellbaum, 1998a) such that they are nouns, not named entities, and 6 bird, insect, fish, vehicle, tool, building, tree, flower, vegetable 90 (UNSEEN). All datasets are constructed to enable three-fold cross-validation.7 In all cases, each train instance is provided with multiple contexts from Wikipedia but test sets only feature one context per hyponym-hypernym pair. larity does not work, (4) All examples are grounded in phrasal or sentential context. 2.2.1 We consider the following rank-based metrics: 3 3.1 Probes We follow the work on diagnostic classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et"
2020.starsem-1.10,N15-1098,0,0.0945249,"Missing"
2020.starsem-1.10,N18-2017,0,0.0663242,"Missing"
2020.starsem-1.10,C92-2082,0,0.427317,"oui et al., 2019; Bosselut et al., 2019; Madaan et al., 2020). The primary purpose of these investigations is to identify effective techniques to extract information from PLMs for use in downstream pipelines. In contrast, a growing body of work adopts an agentive perspective (Ettinger et al., 2018; Talmor et al., 2019), Introduction Hierarchical representations of concepts play a central role in reasoning and understanding natural language (Wellman and Gelman, 1992). They have long been studied as a core NLP objective in their own right through tasks requiring the identification of hypernyms (Hearst, 1992; Snow et al., 2005, 2006), and as components for use in downstream * Part of this work was done during an internship at Microsoft Research. 1 Diagnostic framework available at https: //github.com/AbhilashaRavichander/ probe-generalization. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 88 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 88–102 Barcelona, Spain (Online), December 12–13, 2020 treating PLMs as Artificial Intelligence (AI) agents and"
2020.starsem-1.10,D19-1275,0,0.0263793,"age prompt designed to exercise a particular competence; for example, ‘A robin is a [MASK]’ to evaluate knowledge of hypernymy. The word assigned the highest probability at the masked position is considered the PLM’s answer. In this work, we design diagnostics to examine how systematically this “knowledge” generalizes. We consider two kinds of diagnostics—(1) Consistency: We evaluate a PLM’s ability to consistently answer queries reflecting the same conceptual understanding. We use a simple number consistency 3 Consistency tasks can be considered complementary to the control tasks proposed by Hewitt and Liang (2019). While control tasks test attribution, consistency tasks test validity. 4 Our study is based on probes in English. 5 This distinction is concerned with the axis of generalization of probes. In our syntagmatic generalization probes, we are concerned with different lexico-syntactic contexts where a model can demonstrate its knowledge of hypernymy. In the paradigmatic generalization probes, we are concerned with generalizing to novel hypernym/hyponym pairs. 2 We refer to such probes henceforth as zero-shot masked LM probes, since they require no training and use BERT’s masked-LM component to fil"
2020.starsem-1.10,N19-1112,0,0.0732591,"named entities, and 6 bird, insect, fish, vehicle, tool, building, tree, flower, vegetable 90 (UNSEEN). All datasets are constructed to enable three-fold cross-validation.7 In all cases, each train instance is provided with multiple contexts from Wikipedia but test sets only feature one context per hyponym-hypernym pair. larity does not work, (4) All examples are grounded in phrasal or sentential context. 2.2.1 We consider the following rank-based metrics: 3 3.1 Probes We follow the work on diagnostic classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018; Liu et al., 2019; Shwartz and Dagan, 2019) and construct minimal embedinteract-predict probes to assess taxonomic knowledge in pretrained representations. Embed: We embed each word in the hypernymy pair using the embedding model to obtain hw1 , w2 i. These representations can either be functions of the word itself (in static embeddings) or functions of the entire sentence (in contextualized embeddings). Interact: Following Vu and Shwartz (2018), we concatenate the representations w1 , w2 with their difference w2 − w1 , and their element-wise product w1 w2 to form representation ~x. Predict: We then apply a so"
2020.starsem-1.10,N19-1419,0,0.0246161,"t al., 2018; Ribeiro et al., 2020). For example, McCoy et al. (2019) show that BERT finetuned for the natural language inference task, relies heavily on shallow heurestics instead of acquiring adequate commonsense knowledge. Our work is complementary, demonstrating through a simple consistency task that BERT’s capabilities, as discovered through probes, may not correspond to some systematic general ability. Related Work There has been considerable interest in probing the capabilities of PLMs (Rogers et al., 2020). Much recent work focuses on the grammatical and syntactic capabilities of BERT (Hewitt and Manning, 2019; Liu et al., 2019; Swayamdipta et al., 2019; Goldberg, 2019; Wolf, 2019; Coenen et al., 2019; Tenney et al., 2019; Warstadt et al., 2019; Kim et al., 2019). In contrast, our focus is on probing studies that aim to uncover “knowledge” in BERT. There have been several such studies: Forbes et al. (2019) study physical commonsense encoded in BERT. Da and Kasai (2019) probe BERT for its understanding of object attributes, finding that it learns physical concrete norms (is made of wood) better than abstract ones (is strong). Wallace et al. (2019) Our work examines, in particular, hypernymy knowledg"
2020.starsem-1.10,marelli-etal-2014-sick,0,0.0142527,"sideration of design choices. 5 Closest to our work, Kassner and Sch¨utze (2020) find that PLMs do not differentiate between negated and non-negated statements. Negation is a notoriously hard phenomenon for neural NLP models (Morante and Sporleder, 2012; Fancellu et al., 2016; Naik et al., 2018); our work demonstrates that even affirmative factual knowledge that can be extracted from BERT does not systematically generalize. Our work is also closely related to recent challenge set construction efforts, which aim to serve as sanity checks on the knowledge and commonsense capabilities of models (Marelli et al., 2014; Naik et al., 2018; Glockner et al., 2018; Ribeiro et al., 2020). For example, McCoy et al. (2019) show that BERT finetuned for the natural language inference task, relies heavily on shallow heurestics instead of acquiring adequate commonsense knowledge. Our work is complementary, demonstrating through a simple consistency task that BERT’s capabilities, as discovered through probes, may not correspond to some systematic general ability. Related Work There has been considerable interest in probing the capabilities of PLMs (Rogers et al., 2020). Much recent work focuses on the grammatical and s"
2020.starsem-1.10,2020.tacl-1.28,0,0.332855,"RTE), metaphor detection, text generation and question answering (QA) (Girju et al., 2003; Dagan et al., 2006; Prager et al., 2008; Mirkin et al., 2009; Akhmatova and Dras, 2009; Mohler et al., 2013; Biran and McKeown, 2013; Yahya et al., 2013). Recently, Pretrained Language Models (PLMs), such as BERT (Devlin et al., 2019), have emerged as a popular and successful approach to a variety of NLP tasks. Thus, there has been community interest in evaluating their representations for the ‘knowledge’ they contain, including information about concept abstraction (Ettinger, 2020; Talmor et al., 2019; Jiang et al., 2020; Petroni et al., 2019). We distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrumentative and agentive. We view the instrumentative perspective as treating PLMs as a tool to mine or store knowledge, like hypernymhyponym and other relations, from text (Petroni et al., 2019; Jiang et al., 2020; Bouraoui et al., 2019; Bosselut et al., 2019; Madaan et al., 2020). The primary purpose of these investigations is to identify effective techniques to extract information from PLMs for use in downstream pipelines. In contrast, a growing body of work adopt"
2020.starsem-1.10,P19-1334,0,0.0435905,"ot differentiate between negated and non-negated statements. Negation is a notoriously hard phenomenon for neural NLP models (Morante and Sporleder, 2012; Fancellu et al., 2016; Naik et al., 2018); our work demonstrates that even affirmative factual knowledge that can be extracted from BERT does not systematically generalize. Our work is also closely related to recent challenge set construction efforts, which aim to serve as sanity checks on the knowledge and commonsense capabilities of models (Marelli et al., 2014; Naik et al., 2018; Glockner et al., 2018; Ribeiro et al., 2020). For example, McCoy et al. (2019) show that BERT finetuned for the natural language inference task, relies heavily on shallow heurestics instead of acquiring adequate commonsense knowledge. Our work is complementary, demonstrating through a simple consistency task that BERT’s capabilities, as discovered through probes, may not correspond to some systematic general ability. Related Work There has been considerable interest in probing the capabilities of PLMs (Rogers et al., 2020). Much recent work focuses on the grammatical and syntactic capabilities of BERT (Hewitt and Manning, 2019; Liu et al., 2019; Swayamdipta et al., 2019"
2020.starsem-1.10,2020.acl-main.698,0,0.0624841,"Missing"
2020.starsem-1.10,D18-1546,0,0.0542722,"Missing"
2020.starsem-1.10,P09-1089,0,0.0144145,"autionary: even if BERT demonstrates high probing accuracy for a particular competence, it does not necessarily follow that BERT ‘understands’ a concept, and it cannot be expected to systematically generalize across applicable contexts.1 1 A car is a _____ vehicle Plural Grammatical Number Probe trees cars are _____ Figure 1: Illustration of BERT’s inconsistent predictions on singular and plural hypernymy probes. NLP tasks, such as recognizing textual entailment (RTE), metaphor detection, text generation and question answering (QA) (Girju et al., 2003; Dagan et al., 2006; Prager et al., 2008; Mirkin et al., 2009; Akhmatova and Dras, 2009; Mohler et al., 2013; Biran and McKeown, 2013; Yahya et al., 2013). Recently, Pretrained Language Models (PLMs), such as BERT (Devlin et al., 2019), have emerged as a popular and successful approach to a variety of NLP tasks. Thus, there has been community interest in evaluating their representations for the ‘knowledge’ they contain, including information about concept abstraction (Ettinger, 2020; Talmor et al., 2019; Jiang et al., 2020; Petroni et al., 2019). We distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrume"
2020.starsem-1.10,S19-1026,0,0.0482428,"Missing"
2020.starsem-1.10,W13-0904,0,0.0150334,"ng accuracy for a particular competence, it does not necessarily follow that BERT ‘understands’ a concept, and it cannot be expected to systematically generalize across applicable contexts.1 1 A car is a _____ vehicle Plural Grammatical Number Probe trees cars are _____ Figure 1: Illustration of BERT’s inconsistent predictions on singular and plural hypernymy probes. NLP tasks, such as recognizing textual entailment (RTE), metaphor detection, text generation and question answering (QA) (Girju et al., 2003; Dagan et al., 2006; Prager et al., 2008; Mirkin et al., 2009; Akhmatova and Dras, 2009; Mohler et al., 2013; Biran and McKeown, 2013; Yahya et al., 2013). Recently, Pretrained Language Models (PLMs), such as BERT (Devlin et al., 2019), have emerged as a popular and successful approach to a variety of NLP tasks. Thus, there has been community interest in evaluating their representations for the ‘knowledge’ they contain, including information about concept abstraction (Ettinger, 2020; Talmor et al., 2019; Jiang et al., 2020; Petroni et al., 2019). We distinguish research that investigates knowledge encoded in BERT through two broad perspectives: instrumentative and agentive. We view the instrumentati"
2020.starsem-1.10,J12-2001,0,0.0653652,"Missing"
2020.starsem-1.10,N16-1098,0,0.0316754,"ander/ probe-generalization. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 88 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 88–102 Barcelona, Spain (Online), December 12–13, 2020 treating PLMs as Artificial Intelligence (AI) agents and analyzing their linguistic competencies and world knowledge, sometimes through tasks such as natural language inference (Williams et al., 2018; Wang et al., 2018) or story completion (Zellers et al., 2018, 2019; Mostafazadeh et al., 2016). In this work, we examine the agentive perspective, focusing specifically on the validity of conclusions drawn from probing studies. A popular approach to probing knowledge in pre-trained language models is the zero-shot masked-LM probing task. For example, given the statement ‘A robin is a [MASK]’, a PLM that produces the correct completion ‘bird’ is considered successful.2 Past work has studied this competency in BERT (Ettinger, 2020), offering BERT’s ability to correctly retrieve noun hypernyms in cloze tasks as evidence that it successfully encodes hypernymy information. But to what exten"
2020.starsem-1.10,E14-1054,0,0.0251545,"ly we would like AI agents to reason like humans do, it is not necessary from an instrumentative perspective if the representations offer utility for a downstream task. does not account for either of these interpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve"
2020.starsem-1.10,2020.tacl-1.54,0,0.0241367,"knowledge and commonsense capabilities of models (Marelli et al., 2014; Naik et al., 2018; Glockner et al., 2018; Ribeiro et al., 2020). For example, McCoy et al. (2019) show that BERT finetuned for the natural language inference task, relies heavily on shallow heurestics instead of acquiring adequate commonsense knowledge. Our work is complementary, demonstrating through a simple consistency task that BERT’s capabilities, as discovered through probes, may not correspond to some systematic general ability. Related Work There has been considerable interest in probing the capabilities of PLMs (Rogers et al., 2020). Much recent work focuses on the grammatical and syntactic capabilities of BERT (Hewitt and Manning, 2019; Liu et al., 2019; Swayamdipta et al., 2019; Goldberg, 2019; Wolf, 2019; Coenen et al., 2019; Tenney et al., 2019; Warstadt et al., 2019; Kim et al., 2019). In contrast, our focus is on probing studies that aim to uncover “knowledge” in BERT. There have been several such studies: Forbes et al. (2019) study physical commonsense encoded in BERT. Da and Kasai (2019) probe BERT for its understanding of object attributes, finding that it learns physical concrete norms (is made of wood) better"
2020.starsem-1.10,C14-1097,0,0.0571385,"Missing"
2020.starsem-1.10,C18-1198,1,0.897181,"Missing"
2020.starsem-1.10,R11-2019,0,0.0184258,"but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve to illuminate a systematic, general competence in the underlying PLMs. We suggest that future studies carefully evaluate the generalizability of their methods, and always be accompanied by consistency checks and controls to ensure that cla"
2020.starsem-1.10,L16-1722,0,0.0266883,"Missing"
2020.starsem-1.10,W15-4208,0,0.0164282,"terpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve to illuminate a systematic, general competence in the underlying PLMs. We suggest that future studies carefully evaluate the generalizability of their methods, and always be accompanied by consistency checks and con"
2020.starsem-1.10,D14-1162,0,0.0954969,"Missing"
2020.starsem-1.10,D19-1250,0,0.100386,"Missing"
2020.starsem-1.10,D16-1159,0,0.0185649,"e extract hyponyms from WordNet (Fellbaum, 1998a) such that they are nouns, not named entities, and 6 bird, insect, fish, vehicle, tool, building, tree, flower, vegetable 90 (UNSEEN). All datasets are constructed to enable three-fold cross-validation.7 In all cases, each train instance is provided with multiple contexts from Wikipedia but test sets only feature one context per hyponym-hypernym pair. larity does not work, (4) All examples are grounded in phrasal or sentential context. 2.2.1 We consider the following rank-based metrics: 3 3.1 Probes We follow the work on diagnostic classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018; Liu et al., 2019; Shwartz and Dagan, 2019) and construct minimal embedinteract-predict probes to assess taxonomic knowledge in pretrained representations. Embed: We embed each word in the hypernymy pair using the embedding model to obtain hw1 , w2 i. These representations can either be functions of the word itself (in static embeddings) or functions of the entire sentence (in contextualized embeddings). Interact: Following Vu and Shwartz (2018), we concatenate the representations w1 , w2 with their difference w2 − w1 , and their el"
2020.starsem-1.10,S18-2023,0,0.0605095,"Missing"
2020.starsem-1.10,Q19-1027,0,0.0179213,"nd 6 bird, insect, fish, vehicle, tool, building, tree, flower, vegetable 90 (UNSEEN). All datasets are constructed to enable three-fold cross-validation.7 In all cases, each train instance is provided with multiple contexts from Wikipedia but test sets only feature one context per hyponym-hypernym pair. larity does not work, (4) All examples are grounded in phrasal or sentential context. 2.2.1 We consider the following rank-based metrics: 3 3.1 Probes We follow the work on diagnostic classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018; Liu et al., 2019; Shwartz and Dagan, 2019) and construct minimal embedinteract-predict probes to assess taxonomic knowledge in pretrained representations. Embed: We embed each word in the hypernymy pair using the embedding model to obtain hw1 , w2 i. These representations can either be functions of the word itself (in static embeddings) or functions of the entire sentence (in contextualized embeddings). Interact: Following Vu and Shwartz (2018), we concatenate the representations w1 , w2 with their difference w2 − w1 , and their element-wise product w1 w2 to form representation ~x. Predict: We then apply a softmax classifier over the"
2020.starsem-1.10,K15-1018,0,0.0153998,"is not necessary from an instrumentative perspective if the representations offer utility for a downstream task. does not account for either of these interpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve to illuminate a systematic, general competence in the underlyin"
2020.starsem-1.10,P06-1101,0,0.195341,"Missing"
2020.starsem-1.10,L18-1239,0,0.0295604,"Missing"
2020.starsem-1.10,S18-2020,0,0.0137491,"the following rank-based metrics: 3 3.1 Probes We follow the work on diagnostic classifiers (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018; Hupkes et al., 2018; Liu et al., 2019; Shwartz and Dagan, 2019) and construct minimal embedinteract-predict probes to assess taxonomic knowledge in pretrained representations. Embed: We embed each word in the hypernymy pair using the embedding model to obtain hw1 , w2 i. These representations can either be functions of the word itself (in static embeddings) or functions of the entire sentence (in contextualized embeddings). Interact: Following Vu and Shwartz (2018), we concatenate the representations w1 , w2 with their difference w2 − w1 , and their element-wise product w1 w2 to form representation ~x. Predict: We then apply a softmax classifier over the formed representation~o = sof tmax(W · ReLU (Dropout(h(~x)))) where h is a 300-dimensional hidden layer, dropout probability = 0.2, W ∈ Rn×300 , and n=2. 2.2.2 Syntagmatic Generalization Metrics Open vocabulary accuracy: We compute mean precision@k (Open Voc.) where for a given hyponym, the value is 1 if the hypernym is ranked in the top k results and 0 otherwise. We report results with both k = 1 and k"
2020.starsem-1.10,N18-1103,0,0.0274912,"Missing"
2020.starsem-1.10,S17-1004,0,0.0190867,"s on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve to illuminate a systematic, general competence in the underlying PLMs. We suggest that future studies carefully evaluate the generalizability of their methods, and always be accompanied by consistency checks and controls to ensure that claims based on model behavi"
2020.starsem-1.10,D19-1534,0,0.0205141,"e grammatical and syntactic capabilities of BERT (Hewitt and Manning, 2019; Liu et al., 2019; Swayamdipta et al., 2019; Goldberg, 2019; Wolf, 2019; Coenen et al., 2019; Tenney et al., 2019; Warstadt et al., 2019; Kim et al., 2019). In contrast, our focus is on probing studies that aim to uncover “knowledge” in BERT. There have been several such studies: Forbes et al. (2019) study physical commonsense encoded in BERT. Da and Kasai (2019) probe BERT for its understanding of object attributes, finding that it learns physical concrete norms (is made of wood) better than abstract ones (is strong). Wallace et al. (2019) Our work examines, in particular, hypernymy knowledge encoded in BERT representations. The identification of hypernyms is studied extensively in cognitive science and philosophy. Some prominent theories include Rosch’s category theory (Rosch and Lloyd, 1978) and Tversky’s category resemblance approach (Tversky, 1977). This work 95 Dual Perspectives on PLMs: In this work, we characterize two perspectives on uncovering knowledge in PLMs: instrumentative and agent-based. We emphasize that while systematicity is a necessary requirement for agent-based analysis, as ideally we would like AI agents"
2020.starsem-1.10,W18-5446,0,0.0638054,"Missing"
2020.starsem-1.10,D19-1286,0,0.0189088,"s heavily on shallow heurestics instead of acquiring adequate commonsense knowledge. Our work is complementary, demonstrating through a simple consistency task that BERT’s capabilities, as discovered through probes, may not correspond to some systematic general ability. Related Work There has been considerable interest in probing the capabilities of PLMs (Rogers et al., 2020). Much recent work focuses on the grammatical and syntactic capabilities of BERT (Hewitt and Manning, 2019; Liu et al., 2019; Swayamdipta et al., 2019; Goldberg, 2019; Wolf, 2019; Coenen et al., 2019; Tenney et al., 2019; Warstadt et al., 2019; Kim et al., 2019). In contrast, our focus is on probing studies that aim to uncover “knowledge” in BERT. There have been several such studies: Forbes et al. (2019) study physical commonsense encoded in BERT. Da and Kasai (2019) probe BERT for its understanding of object attributes, finding that it learns physical concrete norms (is made of wood) better than abstract ones (is strong). Wallace et al. (2019) Our work examines, in particular, hypernymy knowledge encoded in BERT representations. The identification of hypernyms is studied extensively in cognitive science and philosophy. Some promi"
2020.starsem-1.10,C14-1212,0,0.0185275,"n like humans do, it is not necessary from an instrumentative perspective if the representations offer utility for a downstream task. does not account for either of these interpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered by standard probes does not serve to illuminate a systematic, general compe"
2020.starsem-1.10,W03-1011,0,0.139551,"uirement for agent-based analysis, as ideally we would like AI agents to reason like humans do, it is not necessary from an instrumentative perspective if the representations offer utility for a downstream task. does not account for either of these interpretations of hypernymy, but instead relies on prior cognitive studies on category norms (Fischler et al., 1983; Battig and Montague, 1969) and relations defined with these super-ordinate categories in WordNet (Fellbaum, 1998b; Oltramari et al.). Additionally, our work ties into the rich history on modeling hypernymy in NLP systems (Lin, 1998; Weeds and Weir, 2003; Baroni et al., 2012; Rimell, 2014; Roller et al., 2014; Weeds et al., 2014; Shwartz et al., 2015; Vuli´c and Mrkˇsi´c, 2018) and evaluating distributional semantic models on their ability to represent it (Baroni and Lenci, 2011; Santus et al., 2015, 2016; Necsulescu, 2011; Vyas and Carpuat, 2017). 6 Implications for future work: In this work, we provide an investigation of current approaches to probing contextualized representations. Our tests for systematic generalization present a clearer picture of the conclusions that can be drawn from probing studies. We find that ‘knowledge’ discovered"
2020.starsem-1.10,N18-1101,0,0.0190512,"Microsoft Research. 1 Diagnostic framework available at https: //github.com/AbhilashaRavichander/ probe-generalization. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 88 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 88–102 Barcelona, Spain (Online), December 12–13, 2020 treating PLMs as Artificial Intelligence (AI) agents and analyzing their linguistic competencies and world knowledge, sometimes through tasks such as natural language inference (Williams et al., 2018; Wang et al., 2018) or story completion (Zellers et al., 2018, 2019; Mostafazadeh et al., 2016). In this work, we examine the agentive perspective, focusing specifically on the validity of conclusions drawn from probing studies. A popular approach to probing knowledge in pre-trained language models is the zero-shot masked-LM probing task. For example, given the statement ‘A robin is a [MASK]’, a PLM that produces the correct completion ‘bird’ is considered successful.2 Past work has studied this competency in BERT (Ettinger, 2020), offering BERT’s ability to correctly retrieve noun hypernyms"
2020.starsem-1.10,D18-1009,0,0.0123462,"//github.com/AbhilashaRavichander/ probe-generalization. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. 88 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 88–102 Barcelona, Spain (Online), December 12–13, 2020 treating PLMs as Artificial Intelligence (AI) agents and analyzing their linguistic competencies and world knowledge, sometimes through tasks such as natural language inference (Williams et al., 2018; Wang et al., 2018) or story completion (Zellers et al., 2018, 2019; Mostafazadeh et al., 2016). In this work, we examine the agentive perspective, focusing specifically on the validity of conclusions drawn from probing studies. A popular approach to probing knowledge in pre-trained language models is the zero-shot masked-LM probing task. For example, given the statement ‘A robin is a [MASK]’, a PLM that produces the correct completion ‘bird’ is considered successful.2 Past work has studied this competency in BERT (Ettinger, 2020), offering BERT’s ability to correctly retrieve noun hypernyms in cloze tasks as evidence that it successfully encodes hypern"
2020.starsem-1.10,P19-1472,0,0.0245132,"Missing"
2020.tacl-1.39,N19-4010,0,0.0396695,"Missing"
2020.tacl-1.39,C18-1139,0,0.0491323,"Missing"
2020.tacl-1.39,W07-1009,0,0.143279,"Missing"
2020.tacl-1.39,D19-1539,0,0.0288337,"Missing"
2020.tacl-1.39,P12-1064,0,0.0737577,"Missing"
2020.tacl-1.39,W16-2922,0,0.232576,"Missing"
2020.tacl-1.39,Q16-1026,0,0.0700584,"Missing"
2020.tacl-1.39,D19-1367,0,0.0382831,"Missing"
2020.tacl-1.39,N19-1423,0,0.0429849,"Missing"
2020.tacl-1.39,doddington-etal-2004-automatic,0,0.310765,"Missing"
2020.tacl-1.39,Q14-1037,0,0.0844951,"Missing"
2020.tacl-1.39,D09-1015,0,0.307572,"Missing"
2020.tacl-1.39,N18-1131,0,0.0834752,"Missing"
2020.tacl-1.39,P10-1050,0,0.0947952,"Missing"
2020.tacl-1.39,N18-1079,0,0.251306,"odel that enumerates and classifies all possible spans. These methods, however, achieve high performance at the cost of time complexity. To reduce the running time, they set a threshold to discard longer entity mentions. If the hyperparameter is set low, running time is reduced but longer mentions are missed. In contrast, Muis and Lu (2017) proposed a sequence labeling approach that assigns tags to gaps between words, which efficiently handles sequences using Viterbi decoding. However, this approach suffers from structural ambiguity issues during inference, as explained by Wang and Lu (2018). Katiyar and Cardie (2018) proposed another hypergraph-based approach that learns the structure in a greedy manner. However, their method uses an additional hyperparameter as the threshold for selecting multiple mention candidates. This hyperparameter affects the trade-off between recall and precision. In this paper, we propose new learning and decoding methods to extract nested entities without When an entity name contains other names within it, the identification of all combinations of names can become difficult and expensive. We propose a new method to recognize not only outermost named entities but also inner neste"
2020.tacl-1.39,N16-1030,0,0.242878,"Missing"
2020.tacl-1.39,P19-1511,0,0.306268,"Missing"
2020.tacl-1.39,P16-1101,1,0.885449,"Missing"
2020.tacl-1.39,P14-5010,0,0.00430165,"Missing"
2020.tacl-1.39,P19-2026,0,0.0352592,"Missing"
2020.tacl-1.39,P19-1233,0,0.0341957,"Missing"
2020.tacl-1.39,D17-1276,0,0.344721,"ntities have been proposed. Many of them rely on producing and rating all possible (sub)spans, which can be computationally expensive. Wang and Lu (2018) provided a hypergraph-based approach to consider all possible spans. Sohrab and Miwa (2018) proposed a neural exhaustive model that enumerates and classifies all possible spans. These methods, however, achieve high performance at the cost of time complexity. To reduce the running time, they set a threshold to discard longer entity mentions. If the hyperparameter is set low, running time is reduced but longer mentions are missed. In contrast, Muis and Lu (2017) proposed a sequence labeling approach that assigns tags to gaps between words, which efficiently handles sequences using Viterbi decoding. However, this approach suffers from structural ambiguity issues during inference, as explained by Wang and Lu (2018). Katiyar and Cardie (2018) proposed another hypergraph-based approach that learns the structure in a greedy manner. However, their method uses an additional hyperparameter as the threshold for selecting multiple mention candidates. This hyperparameter affects the trade-off between recall and precision. In this paper, we propose new learning"
2020.tacl-1.39,D15-1102,0,0.49613,"Missing"
2020.tacl-1.39,Q16-1016,0,0.0614106,"Missing"
2020.tacl-1.39,D15-1104,0,0.0344391,"Missing"
2020.tacl-1.39,tateisi-tsujii-2004-part,0,0.161103,"Missing"
2020.tacl-1.39,D17-1035,0,0.0247792,"Missing"
2020.tacl-1.39,D18-1019,0,0.493424,"taken from the GENIA dataset (Kim et al., 2003). Name nesting is common, especially in technical domains (Alex et al., 2007; Byrne, 2007; Wang, 2009). The assumption of no nesting leads to loss of potentially important information and may negatively impact subsequent downstream tasks. For instance, a downstream entity linking system that relies on NER may fail to link the correct entity if the entity mention is nested. Various approaches to recognizing nested entities have been proposed. Many of them rely on producing and rating all possible (sub)spans, which can be computationally expensive. Wang and Lu (2018) provided a hypergraph-based approach to consider all possible spans. Sohrab and Miwa (2018) proposed a neural exhaustive model that enumerates and classifies all possible spans. These methods, however, achieve high performance at the cost of time complexity. To reduce the running time, they set a threshold to discard longer entity mentions. If the hyperparameter is set low, running time is reduced but longer mentions are missed. In contrast, Muis and Lu (2017) proposed a sequence labeling approach that assigns tags to gaps between words, which efficiently handles sequences using Viterbi decod"
2020.tacl-1.39,D18-1309,0,0.409419,"echnical domains (Alex et al., 2007; Byrne, 2007; Wang, 2009). The assumption of no nesting leads to loss of potentially important information and may negatively impact subsequent downstream tasks. For instance, a downstream entity linking system that relies on NER may fail to link the correct entity if the entity mention is nested. Various approaches to recognizing nested entities have been proposed. Many of them rely on producing and rating all possible (sub)spans, which can be computationally expensive. Wang and Lu (2018) provided a hypergraph-based approach to consider all possible spans. Sohrab and Miwa (2018) proposed a neural exhaustive model that enumerates and classifies all possible spans. These methods, however, achieve high performance at the cost of time complexity. To reduce the running time, they set a threshold to discard longer entity mentions. If the hyperparameter is set low, running time is reduced but longer mentions are missed. In contrast, Muis and Lu (2017) proposed a sequence labeling approach that assigns tags to gaps between words, which efficiently handles sequences using Viterbi decoding. However, this approach suffers from structural ambiguity issues during inference, as ex"
2020.tacl-1.39,D18-1124,0,0.347087,"Missing"
2020.tacl-1.39,H90-1004,0,0.586207,"Missing"
2020.tacl-1.39,P09-3003,0,0.0251957,"Missing"
2020.tacl-1.39,P19-1527,0,0.300385,"Missing"
2020.tacl-1.39,D19-1034,0,0.405815,"Missing"
2020.tacl-1.39,D17-1283,0,0.0767012,"Missing"
2021.acl-long.185,E06-1042,0,0.0235692,"Missing"
2021.acl-long.185,E17-2092,0,0.0135879,"istically less appropriate text. We believe our benchmark and case studies help explore interesting future directions for crossstyle research. The preprocessed datasets and code are publicly available.1 1 Introduction People often use style as a strategic choice for their personal or social goals in communication (Hovy, ∗ This work was done while DK was at CMU. 1 https://github.com/dykang/xslue 1987; Silverstein, 2003; Jaffe et al., 2009; Kang, 2020). Some stylistic choices implicitly reflect the author’s characteristics, like personality, demographic traits (Kang et al., 2019), and emotions (Buechel and Hahn, 2017), whereas others are explicitly controlled by the author’s choices for their social goals like using polite language, for better relationship with the elder (Danescu et al., 2013). In this work, we broadly call each individual linguistic phenomena as one specific type of style. Style is not a single variable, but multiple variables have their own degrees of freedom and they co-vary together. Imagine an orchestra, as a metaphor of style. What we hear from the orchestra is the harmonized sound of complex combinations of individual instruments played. A conductor, on top of it, controls their com"
2021.acl-long.185,W18-1111,0,0.0166682,"limited to styles only if we can obtain publicly available resources for computing. We call the individual phenomena a specific type of “style” in this work. We admit that there are many other kinds of styles not covered in this work, such as inter-linguistic variables in grammars and phonology, or high-level style variations like individual’s writing style or genres. Cross-style analysis. Some recent works have provided empirical evidence of style interdependencies but in a very limited range: Warriner et al. (2013) analyzed emotional norms and their correlation in lexical features of text. Chhaya et al. (2018) studied a correlation of formality, frustration, and politeness but on small samples (i.e., 960 emails). Nguyen et al. (2014) focused on correlation across demographic information (e.g., gender, age) and with some other factors such as emotions (Preo¸tiuc-Pietro and Ungar, 2018). Dankers et al. (2019); Mohammad et al. (2016) studied the interplay of metaphor and emotion in text. Liu et al. (2010) studied sarcasm detection using sentiment as a sub-problem. Brooke and Hirst (2013) conducted a topical analysis of six styles: literary, abstract, objective, colloquial, concrete, and subjective, on"
2021.acl-long.185,P13-1025,0,0.123488,"publicly available.1 1 Introduction People often use style as a strategic choice for their personal or social goals in communication (Hovy, ∗ This work was done while DK was at CMU. 1 https://github.com/dykang/xslue 1987; Silverstein, 2003; Jaffe et al., 2009; Kang, 2020). Some stylistic choices implicitly reflect the author’s characteristics, like personality, demographic traits (Kang et al., 2019), and emotions (Buechel and Hahn, 2017), whereas others are explicitly controlled by the author’s choices for their social goals like using polite language, for better relationship with the elder (Danescu et al., 2013). In this work, we broadly call each individual linguistic phenomena as one specific type of style. Style is not a single variable, but multiple variables have their own degrees of freedom and they co-vary together. Imagine an orchestra, as a metaphor of style. What we hear from the orchestra is the harmonized sound of complex combinations of individual instruments played. A conductor, on top of it, controls their combinatory choices among them, such as tempo or score. Some instruments under the same category, such as violin and cello for bowed string type, make a similar pattern of sound. Sim"
2021.acl-long.185,D19-1227,0,0.108755,"text reflects complex combination of multiple styles. Each has its own lexical and syntactic features and some are dependent on each other. Consistent combination of them by the author will produce stylistically appropriate text. To the best of our knowledge, only a few recent works have studied style inter-dependencies in a very limited range such across demographical traits (Nguyen et al., 2014; Preo¸tiuc-Pietro and Ungar, 2018), across emotions (Warriner et al., 2013), across lexical styles (Brooke and Hirst, 2013), across genres (Passonneau et al., 2014), or between metaphor and emotion (Dankers et al., 2019; Mohammad et al., 2016). Unlike the prior works, this work proposes the first comprehensive understanding of cross-stylistic language variation, particularly focusing on how different styles co-vary together in written text, which styles are dependent on each other, and how they are systematically composed to generate text. 2376 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2376–2387 August 1–6, 2021. ©2021 Association for Computational Linguistics Our work has following"
2021.acl-long.185,N19-1423,0,0.0184662,"of evaluation for cross-style classification. Models. We compare two types of models: single and cross model. The single model is trained on individual style dataset separately, whereas the cross model is trained on shuffled set of every dataset together. For single model, we use various baseline models, such as majority classifier by choosing the majority label in training data, Bidirectional LSTM (biLSTM) (Hochreiter and Schmidhuber, 1997) with GloVe embeddings (Pennington et al., 2014), and variants of fine-tuned transformers; Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), robustly optimized BERT (RoBERTa) (Liu et al., 2019), and text-to-text transformer (T5) (Raffel et al., 2019).2 For cross model, we propose an encoder-decoder based model that learns cross-style patterns with the shared internal representation across styles (Figure 1). It encodes different styles of input as text (e.g., “STYLE: formality TEXT: would you please..”) and decodes output label as text (e.g., “formal”). We use the pretrained encoder-decoder model from T5 (Raffel et al., 2019), and finetune it using the combined, shuffled datasets in XSLUE. Due to the nature of encoder-decoder mode"
2021.acl-long.185,W16-0425,0,0.0356459,"Missing"
2021.acl-long.185,W18-6202,0,0.0421351,"Missing"
2021.acl-long.185,S16-2003,0,0.116613,"combination of multiple styles. Each has its own lexical and syntactic features and some are dependent on each other. Consistent combination of them by the author will produce stylistically appropriate text. To the best of our knowledge, only a few recent works have studied style inter-dependencies in a very limited range such across demographical traits (Nguyen et al., 2014; Preo¸tiuc-Pietro and Ungar, 2018), across emotions (Warriner et al., 2013), across lexical styles (Brooke and Hirst, 2013), across genres (Passonneau et al., 2014), or between metaphor and emotion (Dankers et al., 2019; Mohammad et al., 2016). Unlike the prior works, this work proposes the first comprehensive understanding of cross-stylistic language variation, particularly focusing on how different styles co-vary together in written text, which styles are dependent on each other, and how they are systematically composed to generate text. 2376 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2376–2387 August 1–6, 2021. ©2021 Association for Computational Linguistics Our work has following contributions: • Aggreg"
2021.acl-long.185,L16-1668,0,0.0499364,"Missing"
2021.acl-long.185,D19-1327,1,0.889247,"Missing"
2021.acl-long.185,C14-1184,0,0.0401015,"Missing"
2021.acl-long.185,D19-1179,1,0.85057,"ictive styles likely generate stylistically less appropriate text. We believe our benchmark and case studies help explore interesting future directions for crossstyle research. The preprocessed datasets and code are publicly available.1 1 Introduction People often use style as a strategic choice for their personal or social goals in communication (Hovy, ∗ This work was done while DK was at CMU. 1 https://github.com/dykang/xslue 1987; Silverstein, 2003; Jaffe et al., 2009; Kang, 2020). Some stylistic choices implicitly reflect the author’s characteristics, like personality, demographic traits (Kang et al., 2019), and emotions (Buechel and Hahn, 2017), whereas others are explicitly controlled by the author’s choices for their social goals like using polite language, for better relationship with the elder (Danescu et al., 2013). In this work, we broadly call each individual linguistic phenomena as one specific type of style. Style is not a single variable, but multiple variables have their own degrees of freedom and they co-vary together. Imagine an orchestra, as a metaphor of style. What we hear from the orchestra is the harmonized sound of complex combinations of individual instruments played. A cond"
2021.acl-long.185,P11-2016,0,0.0340832,"Missing"
2021.acl-long.185,C04-1200,1,0.314107,"Missing"
2021.acl-long.185,I17-1099,0,0.043347,"Missing"
2021.acl-long.185,2021.ccl-1.108,0,0.0441359,"Missing"
2021.acl-long.185,C14-1054,0,0.125094,"ed string type, make a similar pattern of sound. Similarly, text reflects complex combination of multiple styles. Each has its own lexical and syntactic features and some are dependent on each other. Consistent combination of them by the author will produce stylistically appropriate text. To the best of our knowledge, only a few recent works have studied style inter-dependencies in a very limited range such across demographical traits (Nguyen et al., 2014; Preo¸tiuc-Pietro and Ungar, 2018), across emotions (Warriner et al., 2013), across lexical styles (Brooke and Hirst, 2013), across genres (Passonneau et al., 2014), or between metaphor and emotion (Dankers et al., 2019; Mohammad et al., 2016). Unlike the prior works, this work proposes the first comprehensive understanding of cross-stylistic language variation, particularly focusing on how different styles co-vary together in written text, which styles are dependent on each other, and how they are systematically composed to generate text. 2376 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2376–2387 August 1–6, 2021. ©2021 Associati"
2021.acl-long.185,D14-1162,0,0.0857994,"individually, can be effective in style classification task. Particularly, the annotated cross-set in XSLUE will be used as a part of evaluation for cross-style classification. Models. We compare two types of models: single and cross model. The single model is trained on individual style dataset separately, whereas the cross model is trained on shuffled set of every dataset together. For single model, we use various baseline models, such as majority classifier by choosing the majority label in training data, Bidirectional LSTM (biLSTM) (Hochreiter and Schmidhuber, 1997) with GloVe embeddings (Pennington et al., 2014), and variants of fine-tuned transformers; Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019), robustly optimized BERT (RoBERTa) (Liu et al., 2019), and text-to-text transformer (T5) (Raffel et al., 2019).2 For cross model, we propose an encoder-decoder based model that learns cross-style patterns with the shared internal representation across styles (Figure 1). It encodes different styles of input as text (e.g., “STYLE: formality TEXT: would you please..”) and decodes output label as text (e.g., “formal”). We use the pretrained encoder-decoder model from T5 ("
2021.acl-long.185,C18-1130,0,0.0254373,"Missing"
2021.acl-long.185,N18-1012,0,0.060937,"Missing"
2021.acl-long.185,D13-1170,0,0.00504567,"Missing"
2021.acl-long.494,2020.emnlp-main.451,0,0.243483,"; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent efforts (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Zhang and Qian, 2020; Chen et al., 2020; Liang et al., 2020; Wang et al., 2020; Tang et al., 2020) have been de6319 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6319–6329 August 1–6, 2021. ©2021 Association for Computational Linguistics voted to graph convolutional networks (GCNs) and graph attention networks (GATs) over dependency trees, which explicitly exploit the syntactic structure of a sentence. Consider the dependency tree in Figure 1; the syntactic dependency can establish connections between the word"
2021.acl-long.494,D17-1047,0,0.253233,", 2012; Li and Hovy, 2017). Aspect-based sentiment analysis (ABSA) talks an entity-level oriented fine-grained sentiment analysis task that aims to determine sentiment polarities of given aspects in a sentence. In ∗ Corresponding author. The key point in solving the ABSA task is to model the dependency relationship between an aspect and its corresponding opinion expressions. Nevertheless, there probably exist multiple aspects and different opinion expressions in a sentence. To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent efforts (Zhang et al., 2019; Sun et al., 2019b; Huang an"
2021.acl-long.494,N19-1423,0,0.48565,"r instance, (Wang et al., 2016) proposed attentionbased LSTMs for aspect-level sentiment classification. (Tang et al., 2016b) and (Chen et al., 2017) both introduced a hierarchical attention network to identify important sentiment information related to the given aspect. (Fan et al., 2018) exploited a multi-grained attention mechanism to capture the word-level interaction between aspects and their context. (Tan et al., 2019) designed a dual attention 6320 1 https://github.com/CCChenhao997/DualGCN-ABSA network to recognize conflicting opinions. In addition, the pre-trained language model BERT (Devlin et al., 2019) has achieved remarkable performance in many NLP tasks, including ABSA. (Sun et al., 2019a) transformed ABSA task into a sentence pair classification task by constructing an auxiliary sentence. (Xu et al., 2019) proposed a post-training approach on the BERT to enhance the performance of fine-tuning stage for the ABSA task. Another trend explicitly leverages syntactic knowledge. This type of knowledge helps to establish connections between the aspects and the other words in a sentence to learn syntax-aware feature representations of aspects. (Dong et al., 2014) proposed a recursive neural netwo"
2021.acl-long.494,P14-2009,0,0.233771,"re-trained language model BERT (Devlin et al., 2019) has achieved remarkable performance in many NLP tasks, including ABSA. (Sun et al., 2019a) transformed ABSA task into a sentence pair classification task by constructing an auxiliary sentence. (Xu et al., 2019) proposed a post-training approach on the BERT to enhance the performance of fine-tuning stage for the ABSA task. Another trend explicitly leverages syntactic knowledge. This type of knowledge helps to establish connections between the aspects and the other words in a sentence to learn syntax-aware feature representations of aspects. (Dong et al., 2014) proposed a recursive neural network to adaptively propagate the sentiment of words to the aspect along the dependency tree. (He et al., 2018) introduced an attention model that incorporated syntactic information to compute attention weights. (Phan and Ogunbona, 2020) utilized the syntactic relative distance to reduce the impact of irrelevant words. Following this line, a few works extend the GCN and GAT models by means of a syntactical dependency tree and develop several outstanding models (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Wang et al., 2020; Tang et al., 2020). T"
2021.acl-long.494,D18-1380,0,0.0264466,"Missing"
2021.acl-long.494,C18-1066,0,0.0852116,"is (ABSA) talks an entity-level oriented fine-grained sentiment analysis task that aims to determine sentiment polarities of given aspects in a sentence. In ∗ Corresponding author. The key point in solving the ABSA task is to model the dependency relationship between an aspect and its corresponding opinion expressions. Nevertheless, there probably exist multiple aspects and different opinion expressions in a sentence. To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent efforts (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Zhang and Qian, 2020; Chen et al., 2020;"
2021.acl-long.494,C18-1096,0,0.015737,"transformed ABSA task into a sentence pair classification task by constructing an auxiliary sentence. (Xu et al., 2019) proposed a post-training approach on the BERT to enhance the performance of fine-tuning stage for the ABSA task. Another trend explicitly leverages syntactic knowledge. This type of knowledge helps to establish connections between the aspects and the other words in a sentence to learn syntax-aware feature representations of aspects. (Dong et al., 2014) proposed a recursive neural network to adaptively propagate the sentiment of words to the aspect along the dependency tree. (He et al., 2018) introduced an attention model that incorporated syntactic information to compute attention weights. (Phan and Ogunbona, 2020) utilized the syntactic relative distance to reduce the impact of irrelevant words. Following this line, a few works extend the GCN and GAT models by means of a syntactical dependency tree and develop several outstanding models (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Wang et al., 2020; Tang et al., 2020). These works explicitly exploit the syntactic structure information to learn node representations from adjacent nodes. Thus, the dependency tree"
2021.acl-long.494,D19-1549,0,0.0548722,"l., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent efforts (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Zhang and Qian, 2020; Chen et al., 2020; Liang et al., 2020; Wang et al., 2020; Tang et al., 2020) have been de6319 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6319–6329 August 1–6, 2021. ©2021 Association for Computational Linguistics voted to graph convolutional networks (GCNs) and graph attention networks (GATs) over dependency trees, which explicitly exploit the syntactic structure of a sentence. Consider the dependency tree in Figure 1; the syntactic dependency c"
2021.acl-long.494,P11-1016,0,0.0172756,"e SemGCN network to learn semantic features distinct from the syntactic ones built from the SynGCN network. • We conduct extensive experiments on the SemEval 2014 and Twitter datasets. The experimental results demonstrate the effectiveness of our DualGCN model. Additionally, the source code and preprocessed datasets used in our work are provided on GitHub1 . 2 Related Work Traditional sentiment analysis tasks are sentencelevel or document-level oriented. In contrast, ABSA is an entity-level oriented and a more finegrained task for sentiment analysis. Earlier methods (Titov and McDonald, 2008; Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) are usually based on handcrafted features and fail to model the dependency between the given aspect and its context. Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019). For instance, (Wang et al., 2016) proposed attentionbased LSTMs for aspect-level"
2021.acl-long.494,S14-2076,0,0.0315179,"learn semantic features distinct from the syntactic ones built from the SynGCN network. • We conduct extensive experiments on the SemEval 2014 and Twitter datasets. The experimental results demonstrate the effectiveness of our DualGCN model. Additionally, the source code and preprocessed datasets used in our work are provided on GitHub1 . 2 Related Work Traditional sentiment analysis tasks are sentencelevel or document-level oriented. In contrast, ABSA is an entity-level oriented and a more finegrained task for sentiment analysis. Earlier methods (Titov and McDonald, 2008; Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) are usually based on handcrafted features and fail to model the dependency between the given aspect and its context. Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019). For instance, (Wang et al., 2016) proposed attentionbased LSTMs for aspect-level sentiment classification."
2021.acl-long.494,K18-1018,0,0.157335,"nted and a more finegrained task for sentiment analysis. Earlier methods (Titov and McDonald, 2008; Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) are usually based on handcrafted features and fail to model the dependency between the given aspect and its context. Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019). For instance, (Wang et al., 2016) proposed attentionbased LSTMs for aspect-level sentiment classification. (Tang et al., 2016b) and (Chen et al., 2017) both introduced a hierarchical attention network to identify important sentiment information related to the given aspect. (Fan et al., 2018) exploited a multi-grained attention mechanism to capture the word-level interaction between aspects and their context. (Tan et al., 2019) designed a dual attention 6320 1 https://github.com/CCChenhao997/DualGCN-ABSA network to recognize conflicting opinions. In addition, the pre-train"
2021.acl-long.494,P18-1087,0,0.168273,"nted and a more finegrained task for sentiment analysis. Earlier methods (Titov and McDonald, 2008; Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) are usually based on handcrafted features and fail to model the dependency between the given aspect and its context. Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019). For instance, (Wang et al., 2016) proposed attentionbased LSTMs for aspect-level sentiment classification. (Tang et al., 2016b) and (Chen et al., 2017) both introduced a hierarchical attention network to identify important sentiment information related to the given aspect. (Fan et al., 2018) exploited a multi-grained attention mechanism to capture the word-level interaction between aspects and their context. (Tan et al., 2019) designed a dual attention 6320 1 https://github.com/CCChenhao997/DualGCN-ABSA network to recognize conflicting opinions. In addition, the pre-train"
2021.acl-long.494,2020.coling-main.13,0,0.331249,"have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent efforts (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Zhang and Qian, 2020; Chen et al., 2020; Liang et al., 2020; Wang et al., 2020; Tang et al., 2020) have been de6319 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6319–6329 August 1–6, 2021. ©2021 Association for Computational Linguistics voted to graph convolutional networks (GCNs) and graph attention networks (GATs) over dependency trees, which explicitly exploit the syntactic structure of a sentence. Consider the dependency tree in Figure 1; the syntactic dependency can establish connections between the words in a sentence. For"
2021.acl-long.494,D17-1159,0,0.0337305,"rate of the SynGCN and SemGCN modules is set to 0.1, and the number of SynGCN and SemGCN layers is set to 2. All the model weights are initialized from a uniform distribution. We use the Adam optimizer with a learning rate of 0.002. The DualGCN model is trained in 50 epochs with a batch size of 16. The regularization coefficients, λ1 and λ2 are set to (0.2, 0.3), (0.2, 0.2) and (0.3, 0.2) for the three datasets, respectively, and λ3 is set to 10−4 . For DualGCN+BERT, we use the bert-base-uncased4 English version. See our code for more details about BERT’s experiments. Additionally, following (Marcheggiani and Titov, 2017), we add a self-loop for each node in 2 https://github.com/KhalilMrini/LAL-Parser https://nlp.stanford.edu/projects/glove/ 4 https://github.com/huggingface/transformers 3 Baseline Methods We compare DualGCN with state-of-the-art baselines. The models are briefly described as follows. 1) ATAE-LSTM (Wang et al., 2016) utilizes aspect embedding and the attention mechanism in aspectlevel sentiment classification. 2) IAN (Ma et al., 2017) employs two LSTMs and an interactive attention mechanism to generate representations for the aspect and sentence. 3) RAM (Chen et al., 2017) uses multiple attenti"
2021.acl-long.494,D14-1162,0,0.09061,"t al., 2017), we remove the instances using the “conflict” label. In addition, the Twitter dataset is a collection of tweets (Dong et al., 2014). All three datasets have three sentiment polarities: positive, negative and neutral. Each sentence in these datasets is annotated with marked aspects and their corresponding polarities. Statistics for the three datasets are shown in Table 1. 5.2 Implementation Details The LAL-Parser (Mrini et al., 2019), which is used for dependency parsing, provides an off-the-shelf parser2 . For all the experiments, we use pretrained 300-dimensional Glove3 vectors (Pennington et al., 2014) to initialize the word embeddings. The dimensionality of the position (i.e., the relative position of each word in a sentence with respect to the aspect) embeddings and part-of-speech (POS) embeddings is set to 30. Thus, we concatenate the word, POS and position embeddings and then input them into a BiLSTM model, whose hidden size is set to 50. To alleviate overfitting, we apply dropout at a rate of 0.7 to the input word embeddings of the BiLSTM. The dropout rate of the SynGCN and SemGCN modules is set to 0.1, and the number of SynGCN and SemGCN layers is set to 2. All the model weights are i"
2021.acl-long.494,2020.acl-main.588,0,0.39527,"networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent efforts (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Zhang and Qian, 2020; Chen et al., 2020; Liang et al., 2020; Wang et al., 2020; Tang et al., 2020) have been de6319 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6319–6329 August 1–6, 2021. ©2021 Association for Computational Linguistics voted to graph convolutional networks (GCNs) and graph attention networks (GATs) over dependency trees, which explicitly exploit the syntactic structure of a sentence. Consider the dependency tree in Figure 1; the syntactic dependency can establish connections between the words in a sentence. For example, a dependency relation exists"
2021.acl-long.494,2020.acl-main.293,0,0.024098,") proposed a post-training approach on the BERT to enhance the performance of fine-tuning stage for the ABSA task. Another trend explicitly leverages syntactic knowledge. This type of knowledge helps to establish connections between the aspects and the other words in a sentence to learn syntax-aware feature representations of aspects. (Dong et al., 2014) proposed a recursive neural network to adaptively propagate the sentiment of words to the aspect along the dependency tree. (He et al., 2018) introduced an attention model that incorporated syntactic information to compute attention weights. (Phan and Ogunbona, 2020) utilized the syntactic relative distance to reduce the impact of irrelevant words. Following this line, a few works extend the GCN and GAT models by means of a syntactical dependency tree and develop several outstanding models (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Wang et al., 2020; Tang et al., 2020). These works explicitly exploit the syntactic structure information to learn node representations from adjacent nodes. Thus, the dependency tree shortens the distance between the aspects and opinion words of a sentence and alleviates the problem of long-range dependency"
2021.acl-long.494,S14-2004,0,0.0483271,"= − log p(a) (12) (s,a)∈D c∈C where D contains all sentence-aspect pairs and C is the collection of distinct sentiment polarities. 5 5.1 Experiments Datasets We conduct experiments on three public standard datasets. The Restaurant and Laptop datasets 6323 Dataset Restaurant Laptop Twitter Division # Positive # Negative # Neutral Training Testing Training Testing Training Testing 2164 727 976 337 1507 172 807 196 851 128 1528 169 637 196 455 167 3016 336 the SynGCN and SemGCN modules. 5.3 Table 1: Statistics for the three experimental datasets. are made public from the SemEval ABSA challenge (Pontiki et al., 2014). Following (Chen et al., 2017), we remove the instances using the “conflict” label. In addition, the Twitter dataset is a collection of tweets (Dong et al., 2014). All three datasets have three sentiment polarities: positive, negative and neutral. Each sentence in these datasets is annotated with marked aspects and their corresponding polarities. Statistics for the three datasets are shown in Table 1. 5.2 Implementation Details The LAL-Parser (Mrini et al., 2019), which is used for dependency parsing, provides an off-the-shelf parser2 . For all the experiments, we use pretrained 300-dimension"
2021.acl-long.494,N19-1035,0,0.125413,"l., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent efforts (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Zhang and Qian, 2020; Chen et al., 2020; Liang et al., 2020; Wang et al., 2020; Tang et al., 2020) have been de6319 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6319–6329 August 1–6, 2021. ©2021 Association for Computational Linguistics voted to graph convolutional networks (GCNs) and graph attention networks (GATs) over dependency trees, which explicitly exploit the syntactic structure of a sentence. Consider the dependency tree in Figure 1; t"
2021.acl-long.494,D19-1569,0,0.282285,"l., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent efforts (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Zhang and Qian, 2020; Chen et al., 2020; Liang et al., 2020; Wang et al., 2020; Tang et al., 2020) have been de6319 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6319–6329 August 1–6, 2021. ©2021 Association for Computational Linguistics voted to graph convolutional networks (GCNs) and graph attention networks (GATs) over dependency trees, which explicitly exploit the syntactic structure of a sentence. Consider the dependency tree in Figure 1; t"
2021.acl-long.494,D19-1342,0,0.0140214,"negrained task for sentiment analysis. Earlier methods (Titov and McDonald, 2008; Jiang et al., 2011; Kiritchenko et al., 2014; Vo and Zhang, 2015) are usually based on handcrafted features and fail to model the dependency between the given aspect and its context. Recently, various attention-based neural networks have been proposed to implicitly model the semantic relation of an aspect and its context to capture the opinion expression component (Wang et al., 2016; Tang et al., 2016a,b; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018; Li et al., 2018a; Tan et al., 2019). For instance, (Wang et al., 2016) proposed attentionbased LSTMs for aspect-level sentiment classification. (Tang et al., 2016b) and (Chen et al., 2017) both introduced a hierarchical attention network to identify important sentiment information related to the given aspect. (Fan et al., 2018) exploited a multi-grained attention mechanism to capture the word-level interaction between aspects and their context. (Tan et al., 2019) designed a dual attention 6320 1 https://github.com/CCChenhao997/DualGCN-ABSA network to recognize conflicting opinions. In addition, the pre-trained language model BE"
2021.acl-long.494,D16-1021,0,0.306294,"c in natural language processing (Liu, 2012; Li and Hovy, 2017). Aspect-based sentiment analysis (ABSA) talks an entity-level oriented fine-grained sentiment analysis task that aims to determine sentiment polarities of given aspects in a sentence. In ∗ Corresponding author. The key point in solving the ABSA task is to model the dependency relationship between an aspect and its corresponding opinion expressions. Nevertheless, there probably exist multiple aspects and different opinion expressions in a sentence. To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent efforts (Zhang et a"
2021.acl-long.494,2020.acl-main.295,0,0.310926,"s recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent efforts (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Zhang and Qian, 2020; Chen et al., 2020; Liang et al., 2020; Wang et al., 2020; Tang et al., 2020) have been de6319 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6319–6329 August 1–6, 2021. ©2021 Association for Computational Linguistics voted to graph convolutional networks (GCNs) and graph attention networks (GATs) over dependency trees, which explicitly exploit the syntactic structure of a sentence. Consider the dependency tree in Figure 1; the syntactic dependency can establish connections between the words in a sentence. For example, a depende"
2021.acl-long.494,D16-1058,0,0.543233,"come a popular topic in natural language processing (Liu, 2012; Li and Hovy, 2017). Aspect-based sentiment analysis (ABSA) talks an entity-level oriented fine-grained sentiment analysis task that aims to determine sentiment polarities of given aspects in a sentence. In ∗ Corresponding author. The key point in solving the ABSA task is to model the dependency relationship between an aspect and its corresponding opinion expressions. Nevertheless, there probably exist multiple aspects and different opinion expressions in a sentence. To judge the sentiment of a particular aspect, previous studies (Wang et al., 2016; Tang et al., 2016a; Ma et al., 2017; Chen et al., 2017; Fan et al., 2018; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent"
2021.acl-long.494,N19-1242,0,0.0214595,"rtant sentiment information related to the given aspect. (Fan et al., 2018) exploited a multi-grained attention mechanism to capture the word-level interaction between aspects and their context. (Tan et al., 2019) designed a dual attention 6320 1 https://github.com/CCChenhao997/DualGCN-ABSA network to recognize conflicting opinions. In addition, the pre-trained language model BERT (Devlin et al., 2019) has achieved remarkable performance in many NLP tasks, including ABSA. (Sun et al., 2019a) transformed ABSA task into a sentence pair classification task by constructing an auxiliary sentence. (Xu et al., 2019) proposed a post-training approach on the BERT to enhance the performance of fine-tuning stage for the ABSA task. Another trend explicitly leverages syntactic knowledge. This type of knowledge helps to establish connections between the aspects and the other words in a sentence to learn syntax-aware feature representations of aspects. (Dong et al., 2014) proposed a recursive neural network to adaptively propagate the sentiment of words to the aspect along the dependency tree. (He et al., 2018) introduced an attention model that incorporated syntactic information to compute attention weights. (P"
2021.acl-long.494,D19-1464,0,0.0570016,"Missing"
2021.acl-long.494,2020.emnlp-main.286,0,0.389373,"18; Huang et al., 2018; Gu et al., 2018) have proposed various recurrent neural networks (RNNs) with attention mechanisms to generate aspect-specific sentence representations and have achieved appealing results. However, an inherent defect makes the attention mechanism vulnerable to noise in the sentence. Take Figure 1 as an example; for the aspect “service”, the opinion word “reasonable” may receive more attention than the opinion word “poor”. However, the “reasonable” refers to another aspect, i.e., “price”. More recent efforts (Zhang et al., 2019; Sun et al., 2019b; Huang and Carley, 2019; Zhang and Qian, 2020; Chen et al., 2020; Liang et al., 2020; Wang et al., 2020; Tang et al., 2020) have been de6319 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6319–6329 August 1–6, 2021. ©2021 Association for Computational Linguistics voted to graph convolutional networks (GCNs) and graph attention networks (GATs) over dependency trees, which explicitly exploit the syntactic structure of a sentence. Consider the dependency tree in Figure 1; the syntactic dependency can establish connectio"
2021.acl-long.94,D15-1075,0,0.0569429,"Missing"
2021.acl-long.94,voorhees-tice-2000-trec,0,0.313543,"t the first sub-layer of the encoder. Thus, to decouple dv from this constraint, we keep dv = de and add each head’s output.8 5 Classification Tasks For the empirical analysis of our proposed solutions as mentioned in §4, we conduct our experiments on the following varied text classification tasks: 5.1 Small Scale Datasets IMDB (Maas et al., 2011). The dataset for the task of sentiment classification consist of IMDB movie reviews with their sentiment as positive or negative. Each of the train and test sets contain 25,000 data samples equally distributed in both the sentiment polarities. TREC (Voorhees and Tice, 2000). We use the 6-class version of the dataset for the task of question classification consisting of open-domain, facet-based questions. There are 5,452 and 500 samples for training and testing, respectively. SST (Socher et al., 2013). Stanford sentiment analysis dataset consist of 11,855 sentences obtained from movie reviews. We use the 3-class version of the dataset for the task of sentiment classification. Each review is labeled as positive, neutral, or negative. The provided train/test/valid split is 8,544/2,210/1,101. 8 ds-max &lt; de as in the regular Transformer setting. 5.2 Large Scale Datas"
2021.acl-long.94,D19-1002,0,0.0493033,"elf-attention (Radford et al., 2018; Devlin et al., 2018). A model employing an attention-based mechanism generates a probability distribution a = {a1 , . . . , an } over the n input units z = {z1 , . . . , zn }. The idea is to perform a P weighted sum of inputs, denoted by ni=1 ai zi , to produce a more context-involved output. The attention vector, a, are commonly interpreted as scores signifying the relative importance of input units. However, counter-intuitively, it is recently observed that the weights generated in the model do not provide meaningful explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Attention weights are (structurally) identifiable if we can uniquely determine them from the output of the attention unit (Brunner et al., 2019). Identifiability of the attention weights is critical to the model’s prediction to be interpretable and replicable. If the weights are not unique, explanatory insights from them might be misleading. The self -attention transforms an input sequence of vectors z = {z1 , . . . , zn } to a contextualized P output sequence y = {y1 , . . . , yn }, where yk = ni=1 a(k,i) zi . The scalar a(k,i) captures how much of the ith token contributes to the contextua"
2021.acl-long.94,N19-1357,0,0.0313071,"mer that is multi-head self-attention (Radford et al., 2018; Devlin et al., 2018). A model employing an attention-based mechanism generates a probability distribution a = {a1 , . . . , an } over the n input units z = {z1 , . . . , zn }. The idea is to perform a P weighted sum of inputs, denoted by ni=1 ai zi , to produce a more context-involved output. The attention vector, a, are commonly interpreted as scores signifying the relative importance of input units. However, counter-intuitively, it is recently observed that the weights generated in the model do not provide meaningful explanations (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Attention weights are (structurally) identifiable if we can uniquely determine them from the output of the attention unit (Brunner et al., 2019). Identifiability of the attention weights is critical to the model’s prediction to be interpretable and replicable. If the weights are not unique, explanatory insights from them might be misleading. The self -attention transforms an input sequence of vectors z = {z1 , . . . , zn } to a contextualized P output sequence y = {y1 , . . . , yn }, where yk = ni=1 a(k,i) zi . The scalar a(k,i) captures how much of the ith token"
2021.acl-long.94,P11-1015,0,0.0813041,"input tokens, i.e., dv ≥ ds-max . In Vaswani et al. (2017), dv was bound to be equal to de /h, where de is token embedding dimension and h is number of heads. This constraint on dv is because of the concatenation of h self-attention heads to produce de -sized output at the first sub-layer of the encoder. Thus, to decouple dv from this constraint, we keep dv = de and add each head’s output.8 5 Classification Tasks For the empirical analysis of our proposed solutions as mentioned in §4, we conduct our experiments on the following varied text classification tasks: 5.1 Small Scale Datasets IMDB (Maas et al., 2011). The dataset for the task of sentiment classification consist of IMDB movie reviews with their sentiment as positive or negative. Each of the train and test sets contain 25,000 data samples equally distributed in both the sentiment polarities. TREC (Voorhees and Tice, 2000). We use the 6-class version of the dataset for the task of question classification consisting of open-domain, facet-based questions. There are 5,452 and 500 samples for training and testing, respectively. SST (Socher et al., 2013). Stanford sentiment analysis dataset consist of 11,855 sentences obtained from movie reviews."
2021.acl-long.94,D13-1170,0,0.00513298,"r experiments on the following varied text classification tasks: 5.1 Small Scale Datasets IMDB (Maas et al., 2011). The dataset for the task of sentiment classification consist of IMDB movie reviews with their sentiment as positive or negative. Each of the train and test sets contain 25,000 data samples equally distributed in both the sentiment polarities. TREC (Voorhees and Tice, 2000). We use the 6-class version of the dataset for the task of question classification consisting of open-domain, facet-based questions. There are 5,452 and 500 samples for training and testing, respectively. SST (Socher et al., 2013). Stanford sentiment analysis dataset consist of 11,855 sentences obtained from movie reviews. We use the 3-class version of the dataset for the task of sentiment classification. Each review is labeled as positive, neutral, or negative. The provided train/test/valid split is 8,544/2,210/1,101. 8 ds-max &lt; de as in the regular Transformer setting. 5.2 Large Scale Datasets SNLI (Bowman et al., 2015). The dataset contain 549,367 samples in the training set, 9,842 samples in the validation set, and 9,824 samples in the test set. For the task of recognizing textual entailment, each sample consists o"
2021.eacl-main.259,2020.lrec-1.232,0,0.0459699,"Missing"
2021.eacl-main.259,H94-1010,0,0.534753,"greater community interest in the issues that arise when our systems actually need to be of utility to humans.1 1 Introduction Everyday users now benefit from powerful QA technologies in a range of consumer-facing applications including health (Jacquemart and Zweigenbaum, 2003; Luo et al., 2015; Abacha and DemnerFushman, 2016; Kilicoglu et al., 2018; Guo et al., 2018), privacy (Sathyendra et al., 2017; Harkous et al., 2018; Ravichander et al., 2019), personal finance (Alloatti et al., 2019), search (Yang, 2015; Bajaj et al., 2016; He et al., 2018; Kwiatkowski et al., 2019) and dialog agents (Dahl et al., 1994; Raux et al., 2005). Voice assistants such as Amazon Alexa2 or Google Home3 have brought natural language technologies to several million homes globally (Osborne, 2016; Jeffs, 2018). Yet, even with 1 All resources available at noiseqa.github.io . developer.amazon.com/alexa 3 assistant.google.com 2 millions of users now interacting with these technologies on a daily basis, there has been surprisingly little research attention devoted to studying the issues that arise when people use QA systems. Traditional QA evaluations do not reflect the needs of many users who can benefit from QA technologi"
2021.eacl-main.259,N19-1423,0,0.528759,"cenario of the noise being introduced to the question by the interface through which the user interacts with the QA engine. For each type of noise, we both build a synthetic generator that can introduce noise on a large scale, as well as manually create ‘natural’ noise challenge sets to imitate real-world noise. Our challenge sets are based on SQuAD 1.1 (Rajpurkar et al., 2016),7 a large-scale machine comprehension dataset based on Wikipedia articles where the answer to each question is a span in a provided context. We choose SQuAD both for its popularity as a benchmark (Gardner et al., 2018; Devlin et al., 2019; Radford et al., 2018; Wolf et al., 2019) and to avoid additional confounds such as unanswerable questions (Rajpurkar et al., 2018).8 We use the standard ∼90K/10K train/development split and construct the challenge sets from the XQuAD data (Artetxe et al., 2020), a subset of 1,190 SQuAD development set questions accompanied by professional translations into ten languages.9 Below we discuss each challenge set in more detail. 7 Though in principle, these constructions could be applied to any kind of QA dataset 8 Future work would pursue a context-driven evaluation of unanswerability, identifyin"
2021.eacl-main.259,D17-1091,0,0.0246754,"9; Asai and Choi, 2020). 9 Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. 2977 3.1 MT Noise O RIGINAL How many Panthers defense players were seQ UESTION lected for the Pro Bowl? Our first challenge set emulates machine translation noise introduced when the question is asked in a language other than the language of the QA system’s training data. We use English as the QA system language, pairing English contexts with non-English questions. Synthetic Challenge Set Our synthetic noise generator employs the back-translation technique (Sennrich et al., 2016; Dong et al., 2017; Yu et al., 2018). In our case, back-translation is not meant to act as a data augmentation technique but rather to simulate noise that could be introduced by an MT engine when translating the question from another language. We imperfectly approximate natural non-English input by automatically translating English questions into a pivot language (German); we then translate them back to English, imitating a scenario where the user submits a query through an MT engine. We use the HuggingFace implementation (Wolf et al., 2019) of MarianNMT (JunczysDowmunt et al., 2018).10 Natural Challenge Set To"
2021.eacl-main.259,W18-2501,0,0.0161915,"eplicate a realistic scenario of the noise being introduced to the question by the interface through which the user interacts with the QA engine. For each type of noise, we both build a synthetic generator that can introduce noise on a large scale, as well as manually create ‘natural’ noise challenge sets to imitate real-world noise. Our challenge sets are based on SQuAD 1.1 (Rajpurkar et al., 2016),7 a large-scale machine comprehension dataset based on Wikipedia articles where the answer to each question is a span in a provided context. We choose SQuAD both for its popularity as a benchmark (Gardner et al., 2018; Devlin et al., 2019; Radford et al., 2018; Wolf et al., 2019) and to avoid additional confounds such as unanswerable questions (Rajpurkar et al., 2018).8 We use the standard ∼90K/10K train/development split and construct the challenge sets from the XQuAD data (Artetxe et al., 2020), a subset of 1,190 SQuAD development set questions accompanied by professional translations into ten languages.9 Below we discuss each challenge set in more detail. 7 Though in principle, these constructions could be applied to any kind of QA dataset 8 Future work would pursue a context-driven evaluation of unansw"
2021.eacl-main.259,P18-2103,0,0.0285693,"similar to how our synthetic set is constructed. However, our results show that TTS does not realistically replicate human voice variation. Besides, stakeholders relying on commercial transcription services will not have white2983 box access to ASR; our post-hoc mitigation strategies would be better suited for such cases. Challenge sets Model robustness evaluation with adversarial schemes is common in NLP tasks (Smith, 2012), including dependency parsing (Rimell et al., 2009), information extraction (Schneider et al., 2017), natural language inference (Marelli et al., 2014; Naik et al., 2018; Glockner et al., 2018), machine translation (Isabelle et al., 2017; Belinkov and Bisk, 2018; Bawden et al., 2018; Burlot and Yvon, 2017) and QA (Jia and Liang, 2017; Aspillaga et al., 2020). Unlike most prior work, we do not create our challenge sets to break QA systems, but rather for a more realistic evaluation of the systems’ real-world utility. 6 Conclusion In this work, we advocate for QA evaluations that reflect challenges associated with real-world use. In particular, we focus on questions that are written in another language, spoken, or typed, and the noise introduced into them by the corresponding interfac"
2021.eacl-main.259,W18-2605,0,0.0160728,"consider real-world use, and hope that our findings will spur greater community interest in the issues that arise when our systems actually need to be of utility to humans.1 1 Introduction Everyday users now benefit from powerful QA technologies in a range of consumer-facing applications including health (Jacquemart and Zweigenbaum, 2003; Luo et al., 2015; Abacha and DemnerFushman, 2016; Kilicoglu et al., 2018; Guo et al., 2018), privacy (Sathyendra et al., 2017; Harkous et al., 2018; Ravichander et al., 2019), personal finance (Alloatti et al., 2019), search (Yang, 2015; Bajaj et al., 2016; He et al., 2018; Kwiatkowski et al., 2019) and dialog agents (Dahl et al., 1994; Raux et al., 2005). Voice assistants such as Amazon Alexa2 or Google Home3 have brought natural language technologies to several million homes globally (Osborne, 2016; Jeffs, 2018). Yet, even with 1 All resources available at noiseqa.github.io . developer.amazon.com/alexa 3 assistant.google.com 2 millions of users now interacting with these technologies on a daily basis, there has been surprisingly little research attention devoted to studying the issues that arise when people use QA systems. Traditional QA evaluations do not re"
2021.eacl-main.259,D19-1259,0,0.0166616,"g natural data is infeasible, but individual practitioners should carefully identify and simulate the likely sources of error appropriate for their applications. 5 Related Work Question Answering QA systems have a rich history in NLP, with early successes in domainspecific applications (Green et al., 1961; Woods, 1977; Wilensky et al., 1988; Hirschman and Gaizauskas, 2001). Considerable research effort has been devoted to collecting datasets to support a wider variety of applications (Quaresma and Pimenta Rodrigues, 2005; Monroy et al., 2009; Feng et al., 2015; Liu et al., 2015; Nguyen, 2019; Jin et al., 2019) and improving model performance on them (Lally et al., 2017; Wang et al., 2018; Yu et al., 2018; Yang et al., 2019). We too focus on QA systems but center the utility to users rather than new applications or techniques. There has also been interest in studying the interaction between speech and QA systems. Lee et al. (2018a) examine transcription errors for Chinese QA, and Lee et al. (2018b) propose Spoken SQuAD, with spoken contexts and text-based questions, but they address a fundamentally different use case of searching through speech. Closest to our work is that of Peskov et al. (2019), w"
2021.eacl-main.259,P18-4020,0,0.0317587,"Missing"
2021.eacl-main.259,Q19-1026,0,0.0241109,"rld use, and hope that our findings will spur greater community interest in the issues that arise when our systems actually need to be of utility to humans.1 1 Introduction Everyday users now benefit from powerful QA technologies in a range of consumer-facing applications including health (Jacquemart and Zweigenbaum, 2003; Luo et al., 2015; Abacha and DemnerFushman, 2016; Kilicoglu et al., 2018; Guo et al., 2018), privacy (Sathyendra et al., 2017; Harkous et al., 2018; Ravichander et al., 2019), personal finance (Alloatti et al., 2019), search (Yang, 2015; Bajaj et al., 2016; He et al., 2018; Kwiatkowski et al., 2019) and dialog agents (Dahl et al., 1994; Raux et al., 2005). Voice assistants such as Amazon Alexa2 or Google Home3 have brought natural language technologies to several million homes globally (Osborne, 2016; Jeffs, 2018). Yet, even with 1 All resources available at noiseqa.github.io . developer.amazon.com/alexa 3 assistant.google.com 2 millions of users now interacting with these technologies on a daily basis, there has been surprisingly little research attention devoted to studying the issues that arise when people use QA systems. Traditional QA evaluations do not reflect the needs of many use"
2021.eacl-main.259,P18-1157,0,0.0123339,"evaluation of QA systems that takes into account the challenges associated with their real-world deployment. We hope to encourage development of future user-centered or participatory design approaches to building QA datasets and evaluations, where practitioners work with potential users to understand user requirements and the contexts in which systems are used in practice. Community priorities for QA systems: While leaderboards on established benchmarks have facilitated rapid progress (Rajpurkar et al., 2016, 2018) and bolstered development of a variety of semantic models (Xiong et al., 2018; Liu et al., 2018; Huang et al., 2018; Devlin et al., 2019), we call for practitioners to consider the orthogonal direction of system utility in their model design. We believe these subareas to be complementary, and community attention towards both will help produce NLP systems that are both accurate and usable. Acknowledgments We thank Aakanksha Naik, Sujeath Pareddy, Taylor Berg-Kirkpatrick, and Matthew Gormley for helpful discussion and the anonymous reviewers for their valuable feedback. This work used the Bridges system, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing"
2021.eacl-main.259,L18-1429,1,0.894188,"Missing"
2021.eacl-main.259,C18-1198,1,0.92991,"ons paired with professional translations into ten other languages.11 For each of the test set languages, we use Google’s commercial translation engine12 to produce the English translation of the question. This allows us to construct ten challenge sets of translations from different languages with 1,190 questions each. 3.2 Keyboard Noise This challenge set represents the noise introduced in the process of typing a question up on a keyboard, for example, when a question is submitted to a QA system through a search engine. Synthetic Challenge Set Inspired by prior work (Belinkov and Bisk, 2018; Naik et al., 2018), our basic noise generator introduces per-character 10 huggingface.co/Helsinki-NLP/ opus-mt-{en-de|de-en} 11 A subtle nuance is that XQuAD questions are not originally written in these languages but translated from English; acknowledging this, we use XQuAD data as the natural challenge set because its fully parallel nature allows varying input language while controlling for content for fair comparison. 12 translate.google.com G OOGLE ASR how many Santa’s defense players selected for the Pro Bowl ESP NET how many pantols the tent places were slected ( WITH LM) for the probol K ALDI how many fr"
2021.eacl-main.259,P19-2008,0,0.021171,"when collecting natural data is infeasible, but individual practitioners should carefully identify and simulate the likely sources of error appropriate for their applications. 5 Related Work Question Answering QA systems have a rich history in NLP, with early successes in domainspecific applications (Green et al., 1961; Woods, 1977; Wilensky et al., 1988; Hirschman and Gaizauskas, 2001). Considerable research effort has been devoted to collecting datasets to support a wider variety of applications (Quaresma and Pimenta Rodrigues, 2005; Monroy et al., 2009; Feng et al., 2015; Liu et al., 2015; Nguyen, 2019; Jin et al., 2019) and improving model performance on them (Lally et al., 2017; Wang et al., 2018; Yu et al., 2018; Yang et al., 2019). We too focus on QA systems but center the utility to users rather than new applications or techniques. There has also been interest in studying the interaction between speech and QA systems. Lee et al. (2018a) examine transcription errors for Chinese QA, and Lee et al. (2018b) propose Spoken SQuAD, with spoken contexts and text-based questions, but they address a fundamentally different use case of searching through speech. Closest to our work is that of Pesk"
2021.eacl-main.259,marelli-etal-2014-sick,0,0.0223911,"nscribes speech using TTS–ASR pipelines, similar to how our synthetic set is constructed. However, our results show that TTS does not realistically replicate human voice variation. Besides, stakeholders relying on commercial transcription services will not have white2983 box access to ASR; our post-hoc mitigation strategies would be better suited for such cases. Challenge sets Model robustness evaluation with adversarial schemes is common in NLP tasks (Smith, 2012), including dependency parsing (Rimell et al., 2009), information extraction (Schneider et al., 2017), natural language inference (Marelli et al., 2014; Naik et al., 2018; Glockner et al., 2018), machine translation (Isabelle et al., 2017; Belinkov and Bisk, 2018; Bawden et al., 2018; Burlot and Yvon, 2017) and QA (Jia and Liang, 2017; Aspillaga et al., 2020). Unlike most prior work, we do not create our challenge sets to break QA systems, but rather for a more realistic evaluation of the systems’ real-world utility. 6 Conclusion In this work, we advocate for QA evaluations that reflect challenges associated with real-world use. In particular, we focus on questions that are written in another language, spoken, or typed, and the noise introdu"
2021.eacl-main.259,N18-1202,0,0.0114719,"three kinds of interface errors. For practitioners, this could suggest that simply choosing the highest-accuracy QA model without separately evaluating robustness to interface noise may lead to sub-optimal performance in practice. Below we discuss the effect of each interface in more detail. 14 F1 scores on SQuAD dev set: BiDAF: 77.8; BiDAFELMo: 80.7; BERT: 88.8; RoBERTa: 89.9. For hyperparameters and implementation details, see Appendix A. 15 Uncased detokenized BLEU using SacreBLEU (Post, 2018). 2979 XQuADE N Model EM ASR F1 EM MT Keyboard F1 EM F1 EM F1 BiDAF (Seo et al., 2017) BiDAF-ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) RoBERTa (Liu et al., 2019) 60.08 62.61 72.77 72.35 Synthetic 71.96 54.62 75.38 56.81 84.66 61.93 84.42 68.07 66.39 70.30 77.02 81.38 55.97 57.39 67.23 68.40 68.01 70.05 79.08 80.93 45.21 50.93 61.76 65.04 57.78 63.80 73.64 76.97 BiDAF (Seo et al., 2017) BiDAF-ELMo (Peters et al., 2018) BERT (Devlin et al., 2019) RoBERTa (Liu et al., 2019) 60.08 62.61 72.77 72.35 Natural 71.96 45.97 75.38 49.16 84.66 52.94 84.42 60.08 57.64 62.49 67.13 73.61 54.87 59.24 68.82 70.00 66.90 71.06 79.98 82.13 56.89 60.76 69.16 70.92 68.33 73.32 81.84 83.37 Table 4: Performance of the QA"
2021.eacl-main.259,W18-6319,0,0.0200446,"Missing"
2021.eacl-main.259,D16-1264,0,0.73846,"with QA systems.6 We analyze errors introduced by three interface types that could be connected to a QA engine: speech recognizers converting spoken queries to text, keyboards used to type queries into the system, and translation systems processing queries in other languages. Our contributions are as follows: 1. We identify and describe the problem of interface noise for QA systems. We construct a challenge set framework for errors introduced by three kinds of interfaces: speech recognizers, keyboard interfaces, and translation engines, based on the popular SQuAD questionanswering benchmark (Rajpurkar et al., 2016). We define synthetic noise generators, as well 4 More than 3.4 million American adults over the age of 40 have a form of visual impairment (Congdon et al., 2004). 5 As of 2021-01-24, there are 6,235,415 articles on English Wikipedia making it the largest edition: wikicount.net 6 ‘QA system’ refers to any computing engine that receives a users’ question and constructs an answer. It may consist of an end-to-end neural architecture or a structured pipeline. 2976 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2976–2992 April 19 -"
2021.eacl-main.259,D19-1500,1,0.939257,"for progress before QA systems can be effectively deployed, highlight the need for QA evaluation to expand to consider real-world use, and hope that our findings will spur greater community interest in the issues that arise when our systems actually need to be of utility to humans.1 1 Introduction Everyday users now benefit from powerful QA technologies in a range of consumer-facing applications including health (Jacquemart and Zweigenbaum, 2003; Luo et al., 2015; Abacha and DemnerFushman, 2016; Kilicoglu et al., 2018; Guo et al., 2018), privacy (Sathyendra et al., 2017; Harkous et al., 2018; Ravichander et al., 2019), personal finance (Alloatti et al., 2019), search (Yang, 2015; Bajaj et al., 2016; He et al., 2018; Kwiatkowski et al., 2019) and dialog agents (Dahl et al., 1994; Raux et al., 2005). Voice assistants such as Amazon Alexa2 or Google Home3 have brought natural language technologies to several million homes globally (Osborne, 2016; Jeffs, 2018). Yet, even with 1 All resources available at noiseqa.github.io . developer.amazon.com/alexa 3 assistant.google.com 2 millions of users now interacting with these technologies on a daily basis, there has been surprisingly little research attention devoted"
2021.eacl-main.259,D09-1085,0,0.054855,"in QA, assuming whitebox access to the ASR systems. Most such work automatically generates and transcribes speech using TTS–ASR pipelines, similar to how our synthetic set is constructed. However, our results show that TTS does not realistically replicate human voice variation. Besides, stakeholders relying on commercial transcription services will not have white2983 box access to ASR; our post-hoc mitigation strategies would be better suited for such cases. Challenge sets Model robustness evaluation with adversarial schemes is common in NLP tasks (Smith, 2012), including dependency parsing (Rimell et al., 2009), information extraction (Schneider et al., 2017), natural language inference (Marelli et al., 2014; Naik et al., 2018; Glockner et al., 2018), machine translation (Isabelle et al., 2017; Belinkov and Bisk, 2018; Bawden et al., 2018; Burlot and Yvon, 2017) and QA (Jia and Liang, 2017; Aspillaga et al., 2020). Unlike most prior work, we do not create our challenge sets to break QA systems, but rather for a more realistic evaluation of the systems’ real-world utility. 6 Conclusion In this work, we advocate for QA evaluations that reflect challenges associated with real-world use. In particular,"
2021.eacl-main.259,W17-5402,0,0.055776,"Missing"
2021.eacl-main.259,P16-1009,0,0.0126541,"Ravichander et al., 2019; Asai and Choi, 2020). 9 Spanish, German, Greek, Russian, Turkish, Arabic, Vietnamese, Thai, Chinese, and Hindi. 2977 3.1 MT Noise O RIGINAL How many Panthers defense players were seQ UESTION lected for the Pro Bowl? Our first challenge set emulates machine translation noise introduced when the question is asked in a language other than the language of the QA system’s training data. We use English as the QA system language, pairing English contexts with non-English questions. Synthetic Challenge Set Our synthetic noise generator employs the back-translation technique (Sennrich et al., 2016; Dong et al., 2017; Yu et al., 2018). In our case, back-translation is not meant to act as a data augmentation technique but rather to simulate noise that could be introduced by an MT engine when translating the question from another language. We imperfectly approximate natural non-English input by automatically translating English questions into a pivot language (German); we then translate them back to English, imitating a scenario where the user submits a query through an MT engine. We use the HuggingFace implementation (Wolf et al., 2019) of MarianNMT (JunczysDowmunt et al., 2018).10 Natur"
2021.eacl-main.259,J88-4003,0,0.21038,"ise improves model robustness to natural noise for all noise types in this study (Table 6), suggesting that synthetic noise generators may be capturing some aspects of natural noise. Our proposed generators could serve as templates for synthesizing interface noise when collecting natural data is infeasible, but individual practitioners should carefully identify and simulate the likely sources of error appropriate for their applications. 5 Related Work Question Answering QA systems have a rich history in NLP, with early successes in domainspecific applications (Green et al., 1961; Woods, 1977; Wilensky et al., 1988; Hirschman and Gaizauskas, 2001). Considerable research effort has been devoted to collecting datasets to support a wider variety of applications (Quaresma and Pimenta Rodrigues, 2005; Monroy et al., 2009; Feng et al., 2015; Liu et al., 2015; Nguyen, 2019; Jin et al., 2019) and improving model performance on them (Lally et al., 2017; Wang et al., 2018; Yu et al., 2018; Yang et al., 2019). We too focus on QA systems but center the utility to users rather than new applications or techniques. There has also been interest in studying the interaction between speech and QA systems. Lee et al. (2018"
2021.eacl-main.259,P15-2006,0,0.0113234,"d for QA evaluation to expand to consider real-world use, and hope that our findings will spur greater community interest in the issues that arise when our systems actually need to be of utility to humans.1 1 Introduction Everyday users now benefit from powerful QA technologies in a range of consumer-facing applications including health (Jacquemart and Zweigenbaum, 2003; Luo et al., 2015; Abacha and DemnerFushman, 2016; Kilicoglu et al., 2018; Guo et al., 2018), privacy (Sathyendra et al., 2017; Harkous et al., 2018; Ravichander et al., 2019), personal finance (Alloatti et al., 2019), search (Yang, 2015; Bajaj et al., 2016; He et al., 2018; Kwiatkowski et al., 2019) and dialog agents (Dahl et al., 1994; Raux et al., 2005). Voice assistants such as Amazon Alexa2 or Google Home3 have brought natural language technologies to several million homes globally (Osborne, 2016; Jeffs, 2018). Yet, even with 1 All resources available at noiseqa.github.io . developer.amazon.com/alexa 3 assistant.google.com 2 millions of users now interacting with these technologies on a daily basis, there has been surprisingly little research attention devoted to studying the issues that arise when people use QA systems."
2021.eacl-main.259,N19-4013,0,0.0435842,"Missing"
2021.eacl-main.295,K19-1033,1,0.843684,"ed to address this limitation: 1) Diagnostic examples, where a small number of samples in a test set are annotated with linguistic phenomena of interest, and task accuracy is reported on these samples (Williams et al., 2018; Joshi et al., 2020). However, it is difficult to determine if models perform well on diagnostic examples because they actually learn the linguistic competency, or if they exploit spurious correlations in the data (McCoy et al., 2019; Gururangan et al., 2018; Poliak et al., 2018). 2) External challenge tests (Naik et al., 2018; Isabelle et al., 2017; Glockner et al., 2018; Ravichander et al., 2019; McCoy et al., 2019), where examples are constructed, either through automatic methods or by experts, exercising a specific phenomenon in isolation. However, it is challenging and expensive to build these evaluations, and non-trivial to isolate phenomena (Liu et al., 2019). Thus, probing or diagnostic classification presents a compelling alternative, wherein learned representations can directly be probed for linguistic properties of interest (Ettinger et al., 2016; Be3364 linkov et al., 2017; Adi et al., 2017; Tenney et al., 2019; Zhang and Bowman, 2018; Warstadt et al., 2019). There has been"
2021.eacl-main.295,2020.nlp4convai-1.15,1,0.842062,"for the auxiliary task— predicting the tense of the verb in the main clause of the sentence. A separate classifier, henceforth called the probing classifier, is trained to predict this property based on the constructed representation. The probing task itself is typically selected to be relevant to the training task, and high probing performance is considered as evidence that the property is encoded in the learned representation. Due to its simplicity, a growing body of work uses this approach to pinpoint the information models rely on to do a task (Alt et al., 2020; Giulianelli et al., 2018; Saleh et al., 2020). In this work, we examine the connection between the information encoded in a representation and the information a model relies on. Through a set of carefully designed experiments on the benchmark SentEval probing framework (Conneau et al., 2018), we shed light on information use in neural models. Our story unfolds in four parts: 1. First, we establish careful control versions of the training task such that task performance is invariant to a chosen linguistic property (Figure 1). We show that even when models cannot use a linguistic property to perform the task, the property can be reliably r"
2021.eacl-main.295,D16-1159,0,0.0320055,"a specific phenomenon in isolation. However, it is challenging and expensive to build these evaluations, and non-trivial to isolate phenomena (Liu et al., 2019). Thus, probing or diagnostic classification presents a compelling alternative, wherein learned representations can directly be probed for linguistic properties of interest (Ettinger et al., 2016; Be3364 linkov et al., 2017; Adi et al., 2017; Tenney et al., 2019; Zhang and Bowman, 2018; Warstadt et al., 2019). There has been a variety of research that employs probing to test hypotheses about the mechanisms models used to perform tasks. Shi et al. (2016) examine learned representations in machine translation for syntactic knowledge. Vanmassenhove et al. (2017) investigate aspect in neural machine translation systems, finding that tense information could be extracted from the encoder, but that part of this information may be lost when decoding. Conneau et al. (2018) use probing to examine the correlation between linguistic properties and downstream tasks (including MT and NLI). Hupkes et al. (2018) train a ’diagnostic classifier’ to extract information from a sequence of hidden representations in a neural network. If the classifier achieves hi"
2021.eacl-main.295,D14-1162,0,0.0861278,"c training. We can observe that probing performance decreases sharply for all models when word embeddings are randomly initialized, suggesting a considerable component of probing performance comes from pretraining word embeddings rather than what a model learns during the task. present in word embeddings, and proposed methods to measure this effect, such as comparing with bag-of-word baselines or random encoders (Wieting and Kiela, 2018). However, these methods fail to isolate the contribution of the training task. To study this, we compare models initialized with pre-trained word embeddings (Pennington et al., 2014) and then trained for the main task, to models initialized with random word embeddings and then updated during the main task. These results are presented in Table 3. We observe that probing accuracies drop across linguistic properties in this setting (compare rows with Word and Rand in the table), indicating that models with randomly initialized embeddings generate representations that contain less linguistic information than the models with pretrained embeddings. This result calls into question how to interpret the contribution of the main task to the encoding of a linguistic property, when t"
2021.eacl-main.295,2020.acl-main.420,0,0.0220406,". (2019) study what different NLP tasks teach models about function word comprehension. Alt et al. (2020) analyze learned representations in relation extraction, through a set of fourteen probing tasks for relevant linguistic properties. Saleh et al. (2020) examine the representations learned by neural dialog models for insights into what the model learns about engaging in dialog. See the survey by Belinkov and Glass (2019) for many more examples. Closely related to our work is that of Hewitt and Liang (2019), which studies the role of lexical memorization in probing, and recently the work of Pimentel et al. (2020) and Voita and Titov (2020) who analyze probing from an information-theoretic perspective. These works join an ongoing debate on the correct way to characterize the expressivity of the probing classifier, with the latter proposing ease of extractability as a criterion for selecting appropriate probes. Our work pursues an orthogonal line of inquiry, demonstrating that relying on diagnostic classifiers to interpret model reasoning for a task suffers from a fundamental limitation: properties may be incidentally encoded even when not required for a task. Thus, our work is also related to a broader"
2021.eacl-main.295,S18-2023,0,0.0576668,"Missing"
2021.eacl-main.295,D16-1264,0,0.0407401,"led synthetic scenario we demonstrate that neural models can encode information incidentally, even if it is distributed as random noise with respect to the training task (§5). We discuss several considerations when interpreting the results of probing experiments and highlight avenues for future research needed in this important area of understanding models, tasks and datasets (§6). 2 Background and Related Work Progress in Natural Language Understanding (NLU) has been driven by a history of defining tasks and corresponding benchmarks for the community (Marcus et al., 1993; Dagan et al., 2006; Rajpurkar et al., 2016). These tasks are often tied to specific practical applications, or to developing models demonstrating competencies that transfer across applications. The corresponding benchmark datasets are utilized as proxies for the tasks themselves. How can we estimate their quality as proxies? While annotation artifacts are one facet that affects proxy-quality (Gururangan et al., 2018; Poliak et al., 2018; Kaushik and Lipton, 2018; Naik et al., 2018; Glockner et al., 2018), a dataset might simply not have coverage across competencies required for a task. Additionally, it might consist of alternate “expla"
2021.eacl-main.295,D19-1448,0,0.0186393,"ctive. These works join an ongoing debate on the correct way to characterize the expressivity of the probing classifier, with the latter proposing ease of extractability as a criterion for selecting appropriate probes. Our work pursues an orthogonal line of inquiry, demonstrating that relying on diagnostic classifiers to interpret model reasoning for a task suffers from a fundamental limitation: properties may be incidentally encoded even when not required for a task. Thus, our work is also related to a broader investigation of how neural models encode information (Tishby and Zaslavsky, 2015; Voita et al., 2019), studying to what extent information encoded in neural representations is indicative of information needed to perform tasks. 3 Methodology In this section we describe our modified probing pipeline (Figure 1), where we construct control datasets, such that a particular linguistic feature is not required in making task judgements.2 Control datasets are based on the intuition that a linguistic feature is not informative for a model to discriminate between classes if the linguistic feature remains constant across classes. For a task label T and linguistic property L, when every example in the con"
2021.eacl-main.295,2020.emnlp-main.14,0,0.0173423,"nt NLP tasks teach models about function word comprehension. Alt et al. (2020) analyze learned representations in relation extraction, through a set of fourteen probing tasks for relevant linguistic properties. Saleh et al. (2020) examine the representations learned by neural dialog models for insights into what the model learns about engaging in dialog. See the survey by Belinkov and Glass (2019) for many more examples. Closely related to our work is that of Hewitt and Liang (2019), which studies the role of lexical memorization in probing, and recently the work of Pimentel et al. (2020) and Voita and Titov (2020) who analyze probing from an information-theoretic perspective. These works join an ongoing debate on the correct way to characterize the expressivity of the probing classifier, with the latter proposing ease of extractability as a criterion for selecting appropriate probes. Our work pursues an orthogonal line of inquiry, demonstrating that relying on diagnostic classifiers to interpret model reasoning for a task suffers from a fundamental limitation: properties may be incidentally encoded even when not required for a task. Thus, our work is also related to a broader investigation of how neura"
2021.eacl-main.295,D19-1286,0,0.0160416,"et al., 2018; Ravichander et al., 2019; McCoy et al., 2019), where examples are constructed, either through automatic methods or by experts, exercising a specific phenomenon in isolation. However, it is challenging and expensive to build these evaluations, and non-trivial to isolate phenomena (Liu et al., 2019). Thus, probing or diagnostic classification presents a compelling alternative, wherein learned representations can directly be probed for linguistic properties of interest (Ettinger et al., 2016; Be3364 linkov et al., 2017; Adi et al., 2017; Tenney et al., 2019; Zhang and Bowman, 2018; Warstadt et al., 2019). There has been a variety of research that employs probing to test hypotheses about the mechanisms models used to perform tasks. Shi et al. (2016) examine learned representations in machine translation for syntactic knowledge. Vanmassenhove et al. (2017) investigate aspect in neural machine translation systems, finding that tense information could be extracted from the encoder, but that part of this information may be lost when decoding. Conneau et al. (2018) use probing to examine the correlation between linguistic properties and downstream tasks (including MT and NLI). Hupkes et al. (2018)"
2021.eacl-main.295,N18-1101,0,0.564763,"bing (Conneau et al., 2018), auxiliary prediction tasks (Adi et al., 2017) and diagnostic classification (Veldhoen et al., 2016; Hupkes et al., 2018). As an example of this approach, let us walk through an application to analyze information about tense stored in a Natural Language Inference (NLI) model. In Conneau et al. (2018), three sentence-encoder models are trained on a 3363 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3363–3377 April 19 - 23, 2021. ©2021 Association for Computational Linguistics NLI dataset (MultiNLI; Williams et al., 2018). The encoder weights are frozen, and the encoders are then used to form sentence representations for the auxiliary task— predicting the tense of the verb in the main clause of the sentence. A separate classifier, henceforth called the probing classifier, is trained to predict this property based on the constructed representation. The probing task itself is typically selected to be relevant to the training task, and high probing performance is considered as evidence that the property is encoded in the learned representation. Due to its simplicity, a growing body of work uses this approach to p"
2021.eacl-main.295,W18-5448,0,0.014751,"et al., 2017; Glockner et al., 2018; Ravichander et al., 2019; McCoy et al., 2019), where examples are constructed, either through automatic methods or by experts, exercising a specific phenomenon in isolation. However, it is challenging and expensive to build these evaluations, and non-trivial to isolate phenomena (Liu et al., 2019). Thus, probing or diagnostic classification presents a compelling alternative, wherein learned representations can directly be probed for linguistic properties of interest (Ettinger et al., 2016; Be3364 linkov et al., 2017; Adi et al., 2017; Tenney et al., 2019; Zhang and Bowman, 2018; Warstadt et al., 2019). There has been a variety of research that employs probing to test hypotheses about the mechanisms models used to perform tasks. Shi et al. (2016) examine learned representations in machine translation for syntactic knowledge. Vanmassenhove et al. (2017) investigate aspect in neural machine translation systems, finding that tense information could be extracted from the encoder, but that part of this information may be lost when decoding. Conneau et al. (2018) use probing to examine the correlation between linguistic properties and downstream tasks (including MT and NLI"
2021.emnlp-main.508,P19-1470,0,0.0292449,"rk draws on these ideas, using inference graphs to represent the machine’s “mental model” of the problem at hand. Building the inference graph can be viewed as first asking clarification questions about the context before answering. This is similar to self-talk (Shwartz et al., 2020) but directed towards eliciting chains of influence. Injecting Commonsense Knowledge Many prior systems use commonsense knowledge to aid question-answering, e.g., using sentences retrieved from a corpus (Yang et al., 2019; Guu et al., 2020), or with knowledge generated from a separate source (Shwartz et al., 2020; Bosselut et al., 2019); and injected either as extra sentences fed directly to the model (Clark et al., 2020), via the loss function (Tandon et al., 2018), or via attention (Ma et al., 2019). Unlike prior work, we use conditional language generation techniques to create graphs that are relevant to answering a question. use a graph generated from the query for answering the commonsense question. The graphs consumed by these works contain entities grounded in knowledge graphs like ConceptNet (Speer et al., 2017), whereas we perform reasoning over event inference graphs where each node describes an event. Our best mod"
2021.emnlp-main.508,D15-1075,0,0.0338193,"s, and scenario respectively, and y denotes whether S strengthens/weakens the plausible conclusion that H follows from P, as described in Section 1. 3 Figure 2: An overview of CURIOUS vealed that the graphs can often be erroneous, and CURIOUS also includes an error correction module to generate higher quality inference graphs. This was important because we found that better graphs are more helpful in the downstream QA task. The generated inference graph is then used for the QA task on three existing defeasible inference datasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015), δ-SOCIAL (reasoning about social norms) (Forbes et al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way the graph is encoded for input is important. If we simply augment the question with the generated graphs, there are some gains on all datasets. However, the accuracy improves substantially across all datasets with a more judicious encoding of the graph-augmented question that accounts for interactions between the graph nodes. To achieve this, we use the mixture of experts approach (Jacobs et al., 1991) to include a mixture of experts layers during encod"
2021.emnlp-main.508,P19-1299,0,0.101858,"on. We encode the defeasible query x and the nodes of the graph using L. Query representation is computed as hx = L(x), and we similarly obtain a matrix of node representations hV by encoding each node v in G with L as follows: 3.4.2 Graph representations using MoE Recently, mixture-of-experts (Jacobs et al., 1991; Shazeer et al., 2017; Fedus et al., 2021) has emerged as a promising method of combining multiple feature types. Mixture-of-experts (MoE) is especially useful when the input consists of multiple facets, where each facet has a specific semantic meaning. Previously, Gu et al. (2018); Chen et al. (2019) have used the ability of MoE to pool disparate features on low-resource and cross-lingual language tasks. Since each node in the inference graphs used by us plays a specific role in defeasible reasoning (contextualizer, situation node, or mediator), we take inspiration from these works to design a hierarchical MoE model (Jordan and Xu, 1995) to pool node representations hV into a graph representation hG . An MoE consists of n expert networks E1 , E2 , . . . , En and a gating network M. Given an input x ∈ Rd , each expert network Ei : Rd → Rk learns a transform over the input. The gating netwo"
2021.emnlp-main.508,P18-1128,0,0.0123958,"we report the published numbers for this baseline. For an additional baseline, we directly use the initial inference graph G0 generated by GENinit , and provide it to the model simply as a string (i.e., sequence of tokens; a simple, often-used approach). This baseline is called E 2 E-STR. We use the same hyperparameters as Rudinger et al. (2020), and add a detailed description of the hyperparameters in Appendix §C. For all the QA experiments, we report the accuracy on the test set using the checkpoint with the highest accuracy on the development set. We use the McNemar’s test (McNemar, 1947; Dror et al., 2018) and use p &lt; 0.05 as a threshold for statistical significance. All the p-values are reported in Appendix §G. Experiments In this section, we empirically investigate if CURI OUS can improve defeasible inference by first modeling the question scenario using inference graphs. We also study the reasons for any improvements. 4.1 Datasets Our end task performance is measured on the three existing datasets for defeasible inference provided by Rudinger et al. (2020):3 δATOMIC , δ- SNLI , δ- SOCIAL (Table 1). These datasets exhibit substantial diversity because of their domains: δ-SNLI (natural languag"
2021.emnlp-main.508,2020.emnlp-main.48,0,0.0268856,"ns/weakens the plausible conclusion that H follows from P, as described in Section 1. 3 Figure 2: An overview of CURIOUS vealed that the graphs can often be erroneous, and CURIOUS also includes an error correction module to generate higher quality inference graphs. This was important because we found that better graphs are more helpful in the downstream QA task. The generated inference graph is then used for the QA task on three existing defeasible inference datasets from diverse domains, viz., δ-SNLI (natural language inference) (Bowman et al., 2015), δ-SOCIAL (reasoning about social norms) (Forbes et al., 2020), and δ-ATOMIC (commonsense reasoning) (Sap et al., 2019). We show that the way the graph is encoded for input is important. If we simply augment the question with the generated graphs, there are some gains on all datasets. However, the accuracy improves substantially across all datasets with a more judicious encoding of the graph-augmented question that accounts for interactions between the graph nodes. To achieve this, we use the mixture of experts approach (Jacobs et al., 1991) to include a mixture of experts layers during encoding, enabling the ability to attend to specific nodes while cap"
2021.emnlp-main.508,N18-1032,0,0.0935709,"uence representation. We encode the defeasible query x and the nodes of the graph using L. Query representation is computed as hx = L(x), and we similarly obtain a matrix of node representations hV by encoding each node v in G with L as follows: 3.4.2 Graph representations using MoE Recently, mixture-of-experts (Jacobs et al., 1991; Shazeer et al., 2017; Fedus et al., 2021) has emerged as a promising method of combining multiple feature types. Mixture-of-experts (MoE) is especially useful when the input consists of multiple facets, where each facet has a specific semantic meaning. Previously, Gu et al. (2018); Chen et al. (2019) have used the ability of MoE to pool disparate features on low-resource and cross-lingual language tasks. Since each node in the inference graphs used by us plays a specific role in defeasible reasoning (contextualizer, situation node, or mediator), we take inspiration from these works to design a hierarchical MoE model (Jordan and Xu, 1995) to pool node representations hV into a graph representation hG . An MoE consists of n expert networks E1 , E2 , . . . , En and a gating network M. Given an input x ∈ Rd , each expert network Ei : Rd → Rk learns a transform over the inp"
2021.emnlp-main.508,2020.acl-main.386,0,0.0237982,"Missing"
2021.emnlp-main.508,D19-1282,0,0.052542,"Missing"
2021.emnlp-main.508,2021.ccl-1.108,0,0.0392943,"Missing"
2021.emnlp-main.508,D19-6003,0,0.0152107,"larification questions about the context before answering. This is similar to self-talk (Shwartz et al., 2020) but directed towards eliciting chains of influence. Injecting Commonsense Knowledge Many prior systems use commonsense knowledge to aid question-answering, e.g., using sentences retrieved from a corpus (Yang et al., 2019; Guu et al., 2020), or with knowledge generated from a separate source (Shwartz et al., 2020; Bosselut et al., 2019); and injected either as extra sentences fed directly to the model (Clark et al., 2020), via the loss function (Tandon et al., 2018), or via attention (Ma et al., 2019). Unlike prior work, we use conditional language generation techniques to create graphs that are relevant to answering a question. use a graph generated from the query for answering the commonsense question. The graphs consumed by these works contain entities grounded in knowledge graphs like ConceptNet (Speer et al., 2017), whereas we perform reasoning over event inference graphs where each node describes an event. Our best model uses a mixture-of-experts (MoE) (Jacobs et al., 1991) model to pool multifaceted input. Prior work has shown the effectiveness of using MoE for graph classification"
2021.emnlp-main.508,2021.findings-acl.456,1,0.384764,"h inference graphs help machines in defeasible reasoning? Our approach is as follows. First, given a question, generate an inference graph describing important influences between question elements. Then, use that graph as an additional input when answering the defeasible reasoning query. Our proposed system, CURIOUS, comprises a graph generation module and a graph encoding module to use the generated graph for the query (Figure 2). To generate inference graphs, we build upon past 1 work that uses a sequence to sequence approach Data and code located at https://github.com/ madaan/thinkaboutit (Madaan et al., 2021). However, our analysis re6291 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6291–6310 c November 7–11, 2021. 2021 Association for Computational Linguistics sentences describing a premise, hypothesis, and scenario respectively, and y denotes whether S strengthens/weakens the plausible conclusion that H follows from P, as described in Section 1. 3 Figure 2: An overview of CURIOUS vealed that the graphs can often be erroneous, and CURIOUS also includes an error correction module to generate higher quality inference graphs. This was important becaus"
2021.emnlp-main.508,2021.naacl-main.67,1,0.722496,"017). Here we consider the specific formulation and challenge in Rudinger et al. (2020): Given that some premise P plausibly implies a hypothesis H, does new information that the situation is S weaken or strengthen the conclusion H? For example, consider the premise “The drinking glass fell” with a possible implication “The glass broke”. New information that “The glass fell on a pillow” here weakens the implication. We borrow ideas from the cognitive science literature that supports defeasible reasoning for humans with an inference graph (Pollock, 2009, 1987). Inference graphs formulation in (Madaan and Yang, 2021), which we use in this paper, draws connections between the P, H, and S through mediating Figure 1: CURIOUS improves defeasible reasoning by modeling the question scenario with an inference graph G adapted from cognitive science literature. The graph is encoded judiciously using our graph encoder h(.), resulting in end task performance improvement. events. This can be seen as a mental model of the question scenario before answering the question (Johnson-Laird, 1983). This paper asks the natural question: can modeling the question scenario with inference graphs help machines in defeasible reaso"
2021.emnlp-main.508,2020.findings-emnlp.418,0,0.0538706,"Missing"
2021.emnlp-main.508,2020.emnlp-main.373,0,0.0431211,"Missing"
2021.emnlp-main.508,D18-1455,0,0.0622376,"Missing"
2021.emnlp-main.508,N19-4013,0,0.0467657,"Missing"
2021.emnlp-main.508,D18-1006,1,0.735186,"ce graph can be viewed as first asking clarification questions about the context before answering. This is similar to self-talk (Shwartz et al., 2020) but directed towards eliciting chains of influence. Injecting Commonsense Knowledge Many prior systems use commonsense knowledge to aid question-answering, e.g., using sentences retrieved from a corpus (Yang et al., 2019; Guu et al., 2020), or with knowledge generated from a separate source (Shwartz et al., 2020; Bosselut et al., 2019); and injected either as extra sentences fed directly to the model (Clark et al., 2020), via the loss function (Tandon et al., 2018), or via attention (Ma et al., 2019). Unlike prior work, we use conditional language generation techniques to create graphs that are relevant to answering a question. use a graph generated from the query for answering the commonsense question. The graphs consumed by these works contain entities grounded in knowledge graphs like ConceptNet (Speer et al., 2017), whereas we perform reasoning over event inference graphs where each node describes an event. Our best model uses a mixture-of-experts (MoE) (Jacobs et al., 1991) model to pool multifaceted input. Prior work has shown the effectiveness of"
2021.emnlp-main.508,D19-1629,1,0.832164,"ng a system to “think about&quot; a question before answering can improve performance. Approach Inspired by past results (Madaan et al., 2021) that humans found inference graphs useful for defeasible inference, we investigate whether neural models can benefit from envisioning the question scenario using an inference graph before answering a defeasible inference query. Inference graphs As inference graphs are central to our work, we give a brief description of their structure next. Inference graphs were introduced in philosophy by Pollock (2009) to aid defeasible reasoning for humans, and in NLP by Tandon et al. (2019) for a counterfactual reasoning task. We interpret the inference graphs as having four kinds of nodes (Pollock, 2009; Madaan et al., 2021): i. Contextualizers (C-, C+): these nodes set the context around a situation and connect to the P. ii. Situations (S, S-): these nodes are new situations that emerge which might overturn an inference. iii. Hypothesis (H-, H+): Hypothesis nodes describe the outcome/conclusion of the situation. iv. Mediators (M-, M+): Mediators are nodes that help bridge the knowledge gap between a situation and a hypothesis node by explaining their connection explicitly. The"
2021.emnlp-main.592,2021.findings-acl.357,1,0.804028,"Missing"
2021.emnlp-main.592,D18-1060,0,0.0182661,"ruct literal translations for two popular make a more direct comparison, we will next com- figurative constructs: metaphors and idioms. Thus, pare results when using figurative contexts versus the proposed mitigation approach does not require their literal counterparts. To perform this experi- any retraining of dialog models. ment, we utilize the human written literal versions Metaphor Detection Through Classifier: We in DD-Fig, and experiment with the GPT2 model train a metaphor detection classifier based on the (which is the best performing model as per human VUA corpus (Steen et al., 2010; Gao et al., 2018)2 . rating on the overall dataset). We report results To better generalize to external data via recent conunder two setups: (1) when figurative language is Gao et al. present in the last utterance of the dialog history, textual models, we skip using model by met by fine(2018), and instead learn a classifier C bert and (2) when figurative language is present anytuning the bert-base-uncased (Devlin et al., 2018) where in the dialog history. Human ratings are met checkpoint from Wolf et al. (2019). On VUA, Cbert collected using the same procedure as described gets a test F1 of 0.724, which is clo"
2021.emnlp-main.592,W15-4640,0,0.0767342,"Missing"
2021.emnlp-main.592,2020.emnlp-main.739,1,0.846699,"Missing"
2021.emnlp-main.592,P18-1113,0,0.0493907,"Missing"
2021.emnlp-main.592,S16-2003,0,0.019653,"sources (Bosselut et al., 2019) which have been separately shown to be useful in dialog domain (Majumder et al., 2020) and in generating figurative language (Chakrabarty et al., 2020). 5 (2020) analyze how well dialog models respond to utterances from infrequent sentence function types (e.g Negative Declarative utterances like I feel bad today.). Louis et al. (2020) propose to identify the categorical mapping of an indirect response with respect a polar question in a task oriented dialog setup. Challenges in handling metaphors and idioms has been explored in prior work on machine translation (Mohammad et al., 2016; Kordoni, 2018; Mao et al., 2018). Mao et al. (2018) propose a method to identify metaphors in English text, and paraphrase them into their literal counterparts before translating to Chinese. Our work on analyzing dialog models against figurative language contexts is along similar direction, though the task setup and scope of figurative language involved are different. Figurative language generation has received reasonable attention such as simile generation (Chakrabarty et al., 2020) and idiom generation (Zhou et al., 2021). Compared to them, our focus is on analyzing capability of popular c"
2021.emnlp-main.592,zesch-etal-2008-extracting,0,0.0274865,"Missing"
2021.emnlp-main.592,P18-1205,0,0.140587,"the best extent possible. Figure 1: An example illustrating how model responses are affected by figurative constructs in dialog context. Here, the model conflates the metaphorical use of build on the sand with its literal meaning, leading to an inappropriate, atopical response. 1), and the response seems to rely on the unintended literal sense of ‘sand’. In this work, we investigate the performance of existing dialog models when faced with inputs containing figurative language use. (1) First, we identify the subsets in existing datasets (such as DailyDialog (Li et al., 2017) and PersonaChat (Zhang et al., 2018)) which have figurative language use such as metaphors and similes. We observe that the performance of all the dialog models under consideration is lower on such subsets containing 1 Introduction figurative language use compared to the dataset Human frequently employ figurative language such as a whole. (2) Second, we gather manually writas metaphors (Carbonell, 1982) and idioms (Jack- ten literal/non-figurative equivalents of the dialog endoff, 1995) for effective and/or stylistic com- utterances in DailyDialog and PersonaChat which munication. Thus, dialog models interacting with exhibit fig"
2021.emnlp-main.592,P17-1061,0,0.0720658,"Missing"
2021.emnlp-main.592,2020.nlp4convai-1.15,0,0.0753144,"Missing"
2021.emnlp-main.592,P19-1004,0,0.0216582,"language detection. Our work is limited to a couple of open domain dialog datasets in English language. Similar analyses could be conducted on goal oriented dialog setups and datasets in other languages. Related Work Acknowledgements Past work has explored fine-grained analysis and We thank anonymous EMNLP reviewers for inunderstanding of the performance of dialog models sightful comments and feedback. (Roller et al., 2020). Saleh et al. (2020) analyze open domain dialog systems for skills such as inEthics Statement ferring contradictions and determining the topic of conversation inter alia. Sankar et al. (2019) analyze Our human preference/appropriateness ratings the change in perplexity when applying certain per- are collected over source content either directly turbations in dialog history. Past work has analyzed sourced from, or based on typical, off-the-shelf dialog models from the point of view of safety from models trained on already existing, publicly availtoxic language (Xu et al., 2020; Dinan et al., 2019), able and widely used dialog datasets - namely, Daiand gender biases (Dinan et al., 2020). Gao et al. lyDialog (Li et al., 2017) and Personachat (Zhang 7480 et al., 2018) as well as the m"
2021.emnlp-main.64,2020.acl-main.386,0,0.0980244,"Model XLNet-Base S ELF E XPLAIN-XLNet + LIL S ELF E XPLAIN-XLNet + GIL S ELF E XPLAIN-XLNet + GIL + LIL 93.4 94.3 94.0 94.6 RoBERTa-Base S ELF E XPLAIN-RoBERTa + LIL S ELF E XPLAIN-RoBERTa + GIL S ELF E XPLAIN-RoBERTa + GIL + LIL 94.8 94.8 94.8 95.1 Table 3: Ablation: S ELF E XPLAIN-XLNet and S ELF E XPLAIN-RoBERTa base models on SST-2. 4 Explanation Evaluation Explanations are notoriously difficult to evaluate quantitatively (Doshi-Velez et al., 2017). A good model explanation should be (i) relevant to the current input and predictions and (ii) understandable to humans (DeYoung et al., 2020; Jacovi and Goldberg, 2020; Wiegreffe et al., 2020; Jain et al., 2020). Towards this, we evaluate whether the explanations along the following diverse criteria: • Sufficiency – Do explanations sufficiently reflect the model predictions? Classification Results : We first evaluate the utility of classification models after incorporating 6 Accuracy • Plausibility – Do explanations appear plausible and understandable to humans? https://cogcomp.seas.upenn.edu/Data/QA/QC/ 840 • Trustability – Do explanations improve human trust in model predictions? From S ELF E XPLAIN, we extracted (i) Most relevant local concepts: these ar"
2021.emnlp-main.64,W18-5408,0,0.041297,"Missing"
2021.emnlp-main.64,N19-1357,0,0.0288508,"nsparency since explanation capability is embedded directly within the model (Kim et al., 2014; Doshi-Velez and Kim, 2017; Rudin, 2019). In natural language applications, feature attribution based on attention scores (Xu et al., 2015) has been the predominant method for developing inherently interpretable neural classifiers. Such methods interpret model decisions locally by explaining the classifier’s decision as a function of relevance of features (words) in input samples. However, such interpretations were shown to be unreliable (Serrano and Smith, 2019; Pruthi et al., 2020) and unfaithful (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Moreover, with natural language being structured and compositional, explaining the role of higher-level compositional concepts like phrasal structures (beyond individual word-level feature attributions) remains an open challenge. Another known limitation of such feature attribution based methods is that the explanations are limited to the 1 input feature space and often require additional Code and data is publicly available at https:// github.com/dheerajrajagopal/SelfExplain methods (e.g. Han et al., 2020) for providing global 836 Proceedings of the 2021 Conferen"
2021.emnlp-main.64,2020.acl-main.409,0,0.0533416,"XPLAIN-XLNet + GIL S ELF E XPLAIN-XLNet + GIL + LIL 93.4 94.3 94.0 94.6 RoBERTa-Base S ELF E XPLAIN-RoBERTa + LIL S ELF E XPLAIN-RoBERTa + GIL S ELF E XPLAIN-RoBERTa + GIL + LIL 94.8 94.8 94.8 95.1 Table 3: Ablation: S ELF E XPLAIN-XLNet and S ELF E XPLAIN-RoBERTa base models on SST-2. 4 Explanation Evaluation Explanations are notoriously difficult to evaluate quantitatively (Doshi-Velez et al., 2017). A good model explanation should be (i) relevant to the current input and predictions and (ii) understandable to humans (DeYoung et al., 2020; Jacovi and Goldberg, 2020; Wiegreffe et al., 2020; Jain et al., 2020). Towards this, we evaluate whether the explanations along the following diverse criteria: • Sufficiency – Do explanations sufficiently reflect the model predictions? Classification Results : We first evaluate the utility of classification models after incorporating 6 Accuracy • Plausibility – Do explanations appear plausible and understandable to humans? https://cogcomp.seas.upenn.edu/Data/QA/QC/ 840 • Trustability – Do explanations improve human trust in model predictions? From S ELF E XPLAIN, we extracted (i) Most relevant local concepts: these are the top ranked phrases based on r(nt)1:J f"
2021.emnlp-main.64,D19-1455,0,0.0634909,"Missing"
2021.emnlp-main.64,P18-1249,0,0.0265728,"inary classification dataset (Pang and Lee, 2005). The dataset statistics are shown in Table 1. Experimental Settings: For our S ELF E X PLAIN experiments, we consider two transformer encoder configurations as our base models: (1) RoBERTa encoder (Liu et al., 2019) — a robustly optimized version of BERT (Devlin et al., 2019). (2) XLNet encoder (Yang et al., 2019) — a transformer model based on Transformer-XL (Dai et al., 2019) architecture. We incorporate S ELF E XPLAIN into RoBERTa and XLNet, and use the above encoders without the GIL and LIL layers as the baselines. We generate parse trees (Kitaev and Klein, 2018) to extract target concepts for the input and follow same pre-processing steps as the original encoder configurations for the rest. We also maintain the hyperparameters and weights from the pre-training of the encoders. The architecture with GIL and LIL modules are fine-tuned on datasets described in §3. For the number of global influential concepts K, we consider two settings K = 5, 10. We also perform hyperparameter tuning on α, β = {0.01, 0.1, 0.5, 1.0} and report results on the best model configuration. All models were trained on an NVIDIA V-100 GPU. GIL and LIL layers in Table 2. Across t"
2021.emnlp-main.64,D16-1011,0,0.0494078,"Missing"
2021.emnlp-main.64,D19-1523,0,0.0401229,"Missing"
2021.emnlp-main.64,N16-1082,1,0.863053,"extracted explanations without the rest of the input. An explanation that achieves high accuracy using this classifier is indicative of its ability to recover the original model prediction. We evaluate the explanations on the sentiment analysis task. Explanations from S ELF E X PLAIN are incorporated to the FRESH framework and we compare the predictive accuracy of the explanations in comparison to baseline explanation methods. Following Jain et al. (2020), we use the same experimental setup and saliency-based baselines such as attention (Lei et al., 2016; Bastings et al., 2019) and gradient (Li et al., 2016) based explanation methods. From Table 47 , we observe that S ELF E XPLAIN explanations from LIL and GIL show high predictive performance compared to all the baseline methods. Additionally, GIL explanations outperform full-text (an explanation that uses all of the input sample) performance, which is often considered an upper-bound for span-based explanation approaches. We hypothesize that this is because GIL explanation concepts from the training data are very relevant to help disambiguate the input text. In summary, outputs from S ELF E X PLAIN are more predictive of the label compared to pri"
2021.emnlp-main.64,C02-1150,0,0.19989,"a S ELF E XPLAIN-RoBERTa (K=5) S ELF E XPLAIN-RoBERTa (K=10) 94.8 95.1 95.1 53.5 54.3 54.1 97.0 97.6 97.6 89.0 89.4 89.2 96.2 96.3 96.3 Table 2: Performance comparison of models with and without GIL and LIL layers. All experiments used the same encoder configurations. We use the development set for SST-2 results (test set of SST-2 is part of GLUE benchmark) and test sets for - SST-5, TREC-6, TREC-50 and SUBJ α, β = 0.1 for all the above settings. same dataset as before, but modifies it into a finergrained 5-class classification task. (iii) TREC-6 6 : a question classification task proposed by Li and Roth (2002), where each question should be classified into one of 6 question types. (iv) TREC-50: a fine-grained version of the same TREC-6 question classification task with 50 classes (v) SUBJ: subjective/objective binary classification dataset (Pang and Lee, 2005). The dataset statistics are shown in Table 1. Experimental Settings: For our S ELF E X PLAIN experiments, we consider two transformer encoder configurations as our base models: (1) RoBERTa encoder (Liu et al., 2019) — a robustly optimized version of BERT (Devlin et al., 2019). (2) XLNet encoder (Yang et al., 2019) — a transformer model based"
2021.emnlp-main.64,2021.ccl-1.108,0,0.110428,"Missing"
2021.emnlp-main.64,D15-1166,0,0.0609058,"we show that our interpreted model outputs are perceived as more trustworthy, understandable, and adequate for explaining model decisions compared to previous approaches to explainability. This opens an exciting research direction for building inherently interpretable models for text classification. Future work will extend the framework to other tasks and to longer contexts, beyond single input sentence. Inherently Intepretable Models: Heat maps based on attention (Bahdanau et al., 2014) are one of the commonly used interpretability tools for many downstream tasks such as machine translation (Luong et al., 2015), summarization (Rush et al., 2015) and reading comprehension Hermann et al. (2015). Another recent line of work explores collecting rationales (Lei et al., 2016) through expert annotations (Zaidan and Eisner, 2008). No- Acknowledgements table work in collecting external rationales include Cos-E (Rajani et al., 2019), e-SNLI (Camburu et al., This material is based upon work funded 2018) and recently, Eraser benchmark (DeYoung by the DARPA CMO under Contract et al., 2020). Alternative lines of work in this class No. HR001120C0124, and by the United of models include Card et al. (2019) that reli"
2021.emnlp-main.64,P19-1334,0,0.0354235,"ion datasets show that S ELF E X PLAIN facilitates interpretability without sacrificing performance. Most importantly, explanations from S ELF E XPLAIN show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.1 1 Introduction Neural network models are often opaque: they provide limited insight into interpretations of model decisions and are typically treated as “black boxes” (Lipton, 2018). There has been ample evidence that such models overfit to spurious artifacts (Gururangan et al., 2018; McCoy et al., 2019; Kumar et al., 2019) and amplify biases in data (Zhao et al., 2017; Sun et al., 2019). This underscores the need to understand model decision making. Prior work in interpretability for neural text classification predominantly follows two approaches: (i) post-hoc explanation methods that explain predictions for previously trained models based on model internals, and (ii) inherently interpretable models whose interpretability is built-in and optimized jointly with the end task. While post-hoc methods (Simonyan et al., 2014; Koh and Liang, 2017; Ribeiro et al., 2016) are often the only option Th"
2021.emnlp-main.64,P05-1015,0,0.354132,"onfigurations. We use the development set for SST-2 results (test set of SST-2 is part of GLUE benchmark) and test sets for - SST-5, TREC-6, TREC-50 and SUBJ α, β = 0.1 for all the above settings. same dataset as before, but modifies it into a finergrained 5-class classification task. (iii) TREC-6 6 : a question classification task proposed by Li and Roth (2002), where each question should be classified into one of 6 question types. (iv) TREC-50: a fine-grained version of the same TREC-6 question classification task with 50 classes (v) SUBJ: subjective/objective binary classification dataset (Pang and Lee, 2005). The dataset statistics are shown in Table 1. Experimental Settings: For our S ELF E X PLAIN experiments, we consider two transformer encoder configurations as our base models: (1) RoBERTa encoder (Liu et al., 2019) — a robustly optimized version of BERT (Devlin et al., 2019). (2) XLNet encoder (Yang et al., 2019) — a transformer model based on Transformer-XL (Dai et al., 2019) architecture. We incorporate S ELF E XPLAIN into RoBERTa and XLNet, and use the above encoders without the GIL and LIL layers as the baselines. We generate parse trees (Kitaev and Klein, 2018) to extract target concept"
2021.emnlp-main.64,D19-1425,1,0.855195,"at S ELF E X PLAIN facilitates interpretability without sacrificing performance. Most importantly, explanations from S ELF E XPLAIN show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.1 1 Introduction Neural network models are often opaque: they provide limited insight into interpretations of model decisions and are typically treated as “black boxes” (Lipton, 2018). There has been ample evidence that such models overfit to spurious artifacts (Gururangan et al., 2018; McCoy et al., 2019; Kumar et al., 2019) and amplify biases in data (Zhao et al., 2017; Sun et al., 2019). This underscores the need to understand model decision making. Prior work in interpretability for neural text classification predominantly follows two approaches: (i) post-hoc explanation methods that explain predictions for previously trained models based on model internals, and (ii) inherently interpretable models whose interpretability is built-in and optimized jointly with the end task. While post-hoc methods (Simonyan et al., 2014; Koh and Liang, 2017; Ribeiro et al., 2016) are often the only option The fantastic actors el"
2021.emnlp-main.64,D13-1170,0,0.0109943,"Missing"
2021.emnlp-main.64,2020.acl-main.432,0,0.0318201,"fister, 2020) may provide greater transparency since explanation capability is embedded directly within the model (Kim et al., 2014; Doshi-Velez and Kim, 2017; Rudin, 2019). In natural language applications, feature attribution based on attention scores (Xu et al., 2015) has been the predominant method for developing inherently interpretable neural classifiers. Such methods interpret model decisions locally by explaining the classifier’s decision as a function of relevance of features (words) in input samples. However, such interpretations were shown to be unreliable (Serrano and Smith, 2019; Pruthi et al., 2020) and unfaithful (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Moreover, with natural language being structured and compositional, explaining the role of higher-level compositional concepts like phrasal structures (beyond individual word-level feature attributions) remains an open challenge. Another known limitation of such feature attribution based methods is that the explanations are limited to the 1 input feature space and often require additional Code and data is publicly available at https:// github.com/dheerajrajagopal/SelfExplain methods (e.g. Han et al., 2020) for providing glob"
2021.emnlp-main.64,P19-1487,0,0.0223033,"extend the framework to other tasks and to longer contexts, beyond single input sentence. Inherently Intepretable Models: Heat maps based on attention (Bahdanau et al., 2014) are one of the commonly used interpretability tools for many downstream tasks such as machine translation (Luong et al., 2015), summarization (Rush et al., 2015) and reading comprehension Hermann et al. (2015). Another recent line of work explores collecting rationales (Lei et al., 2016) through expert annotations (Zaidan and Eisner, 2008). No- Acknowledgements table work in collecting external rationales include Cos-E (Rajani et al., 2019), e-SNLI (Camburu et al., This material is based upon work funded 2018) and recently, Eraser benchmark (DeYoung by the DARPA CMO under Contract et al., 2020). Alternative lines of work in this class No. HR001120C0124, and by the United of models include Card et al. (2019) that relies on States Department of Energy (DOE) National interpreting a given sample as a weighted sum of Nuclear Security Administration (NNSA) Office the training samples while Croce et al. (2019) iden- of Defense Nuclear Nonproliferation Research tifies influential training samples using a kernel- and Development (DNN R&D"
2021.emnlp-main.64,N16-3020,0,0.419917,"facts (Gururangan et al., 2018; McCoy et al., 2019; Kumar et al., 2019) and amplify biases in data (Zhao et al., 2017; Sun et al., 2019). This underscores the need to understand model decision making. Prior work in interpretability for neural text classification predominantly follows two approaches: (i) post-hoc explanation methods that explain predictions for previously trained models based on model internals, and (ii) inherently interpretable models whose interpretability is built-in and optimized jointly with the end task. While post-hoc methods (Simonyan et al., 2014; Koh and Liang, 2017; Ribeiro et al., 2016) are often the only option The fantastic actors elevated the movie predicted sentiment: positive Input Word Attributions SelfExplain The fantastic actors elevated the movie Top relevant concepts fantastic actors (0.7) elevated (0.1).. Influential training concepts fabulous acting (0.4) stunning (0.2) .. Figure 1: A sample of interpretable concepts from S ELF E XPLAIN for a binary sentiment analysis task. Compared to saliency-map style word attributions, S ELF E XPLAIN can provide explanations via concepts in the input sample and the concepts in the training data for already-trained models, inh"
2021.emnlp-main.64,D15-1044,0,0.0527456,"utputs are perceived as more trustworthy, understandable, and adequate for explaining model decisions compared to previous approaches to explainability. This opens an exciting research direction for building inherently interpretable models for text classification. Future work will extend the framework to other tasks and to longer contexts, beyond single input sentence. Inherently Intepretable Models: Heat maps based on attention (Bahdanau et al., 2014) are one of the commonly used interpretability tools for many downstream tasks such as machine translation (Luong et al., 2015), summarization (Rush et al., 2015) and reading comprehension Hermann et al. (2015). Another recent line of work explores collecting rationales (Lei et al., 2016) through expert annotations (Zaidan and Eisner, 2008). No- Acknowledgements table work in collecting external rationales include Cos-E (Rajani et al., 2019), e-SNLI (Camburu et al., This material is based upon work funded 2018) and recently, Eraser benchmark (DeYoung by the DARPA CMO under Contract et al., 2020). Alternative lines of work in this class No. HR001120C0124, and by the United of models include Card et al. (2019) that relies on States Department of Energy ("
2021.emnlp-main.64,P19-1282,0,0.0205655,"aakkola, 2018; Arik and Pfister, 2020) may provide greater transparency since explanation capability is embedded directly within the model (Kim et al., 2014; Doshi-Velez and Kim, 2017; Rudin, 2019). In natural language applications, feature attribution based on attention scores (Xu et al., 2015) has been the predominant method for developing inherently interpretable neural classifiers. Such methods interpret model decisions locally by explaining the classifier’s decision as a function of relevance of features (words) in input samples. However, such interpretations were shown to be unreliable (Serrano and Smith, 2019; Pruthi et al., 2020) and unfaithful (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Moreover, with natural language being structured and compositional, explaining the role of higher-level compositional concepts like phrasal structures (beyond individual word-level feature attributions) remains an open challenge. Another known limitation of such feature attribution based methods is that the explanations are limited to the 1 input feature space and often require additional Code and data is publicly available at https:// github.com/dheerajrajagopal/SelfExplain methods (e.g. Han et al., 20"
2021.emnlp-main.64,D19-1002,0,0.0329771,"ion capability is embedded directly within the model (Kim et al., 2014; Doshi-Velez and Kim, 2017; Rudin, 2019). In natural language applications, feature attribution based on attention scores (Xu et al., 2015) has been the predominant method for developing inherently interpretable neural classifiers. Such methods interpret model decisions locally by explaining the classifier’s decision as a function of relevance of features (words) in input samples. However, such interpretations were shown to be unreliable (Serrano and Smith, 2019; Pruthi et al., 2020) and unfaithful (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019). Moreover, with natural language being structured and compositional, explaining the role of higher-level compositional concepts like phrasal structures (beyond individual word-level feature attributions) remains an open challenge. Another known limitation of such feature attribution based methods is that the explanations are limited to the 1 input feature space and often require additional Code and data is publicly available at https:// github.com/dheerajrajagopal/SelfExplain methods (e.g. Han et al., 2020) for providing global 836 Proceedings of the 2021 Conference on Empirical Methods in Na"
2021.emnlp-main.64,D19-1420,0,0.0304888,"Missing"
2021.emnlp-main.64,D08-1004,0,0.0655718,"g research direction for building inherently interpretable models for text classification. Future work will extend the framework to other tasks and to longer contexts, beyond single input sentence. Inherently Intepretable Models: Heat maps based on attention (Bahdanau et al., 2014) are one of the commonly used interpretability tools for many downstream tasks such as machine translation (Luong et al., 2015), summarization (Rush et al., 2015) and reading comprehension Hermann et al. (2015). Another recent line of work explores collecting rationales (Lei et al., 2016) through expert annotations (Zaidan and Eisner, 2008). No- Acknowledgements table work in collecting external rationales include Cos-E (Rajani et al., 2019), e-SNLI (Camburu et al., This material is based upon work funded 2018) and recently, Eraser benchmark (DeYoung by the DARPA CMO under Contract et al., 2020). Alternative lines of work in this class No. HR001120C0124, and by the United of models include Card et al. (2019) that relies on States Department of Energy (DOE) National interpreting a given sample as a weighted sum of Nuclear Security Administration (NNSA) Office the training samples while Croce et al. (2019) iden- of Defense Nuclear"
2021.emnlp-main.64,D17-1323,0,0.0306271,"without sacrificing performance. Most importantly, explanations from S ELF E XPLAIN show sufficiency for model predictions and are perceived as adequate, trustworthy and understandable by human judges compared to existing widely-used baselines.1 1 Introduction Neural network models are often opaque: they provide limited insight into interpretations of model decisions and are typically treated as “black boxes” (Lipton, 2018). There has been ample evidence that such models overfit to spurious artifacts (Gururangan et al., 2018; McCoy et al., 2019; Kumar et al., 2019) and amplify biases in data (Zhao et al., 2017; Sun et al., 2019). This underscores the need to understand model decision making. Prior work in interpretability for neural text classification predominantly follows two approaches: (i) post-hoc explanation methods that explain predictions for previously trained models based on model internals, and (ii) inherently interpretable models whose interpretability is built-in and optimized jointly with the end task. While post-hoc methods (Simonyan et al., 2014; Koh and Liang, 2017; Ribeiro et al., 2016) are often the only option The fantastic actors elevated the movie predicted sentiment: positive"
2021.findings-acl.357,P15-2073,0,0.166414,"xts occurring in some of the training dialogues, given a sufficient number of them. Using retrieval, we can identify such contexts and use their responses as pseudo-references for the test-time response. Specifically, for retrieval, we use the BM25 function Sbm25 (x, y) (Robertson et al., 1995) to compare each element {dpast , dresp , dft uture } of t t the turn under evaluation dt (the query) with those resp f uture of the candidate turn xt0 , {xpast }. t0 , xt0 , xt0 past f uture Here, dt and dt are the windows of turn sequences before and after response dresp . t Our approach is related to Galley et al. (2015), who propose ∆-BLEU measure which uses retrieval to produce pseudo-references. However, unlike here, they require annotator quality scores to weigh them during evaluation. Moreover, though we utilize retrieval for evaluation, methods of this kind have found success in many generation setups (Li et al., 2018; Peng et al., 2019; Khandelwal et al., 2019). Besides being automatic, our method differs from the above ones in that it explores the added utility of future information for retrieval. For instance, for the dialog shown in Figure 1, besides matching “Great!” in the response, our retrieval"
2021.findings-acl.357,W19-5944,0,0.259337,"ion methods which correlate with human evaluations. Automated metrics such as BLEU (Papineni et al., 2002) work well for tasks such as machine translation, but often correlate poorly with human ratings in tasks such as open domain dialog which admit a wide variety of ∗ VG and HJ contributed equally for this paper. Order decided by coin flip. 1 Code and data are available at https://github.com/harsh19/ Diverse-Reference-Augmentation/ valid response for given context, often due to small number of human written references (Zhao et al., 2017; Sai et al., 2020b). Prior work (Sugiyama et al., 2019; Gupta et al., 2019) has demonstrated that having multiple valid references for the same context leads to automated metrics being better correlated to human judgements for appropriateness. However, collecting human written responses is difficult to scale, can be costly, and may find it difficult to cover a large variety of correct responses (Celikyilmaz et al., 2020). In this work, we automatically extract a large number of diverse references to be used with such reference-based metrics, without resorting to expensive crowd-sourcing. Intuitively, since opendomain dialog pertains to everyday life, its utterance te"
2021.findings-acl.357,N18-1169,0,0.0227905,"Missing"
2021.findings-acl.357,I17-1099,0,0.147212,"ly on broader data sources compared to dialog models. For example, we use future context and human written reference for retrieval, while a dialog model cannot. Our contributions are as follows: (1) We propose a method for automated reference set augmentation for automated dialog evaluation. Compared to collecting more human-written responses, our approach is inexpensive and scalable, and fetches a diverse set of references. (2) We observe high correlations of various automated metrics with human ratings when proposed reference augmentation is applied to the test split of DailyDialog dataset (Li et al., 2017). We additionally observe that paraphrasing, a popular data augmentation technique, performs much worse. (3) We employ novel use of commonsense knowledge and dialog corpus instances, and unsupervised techniques for adapting retrieved references into more fluent forms. 2 Method Figure 1 shows an overview of our proposed methodology. We first fetch plausible candidates from two types of knowledge sources. Thereafter, the retrieved candidate references are adapted so that they are fluent in the target context. We refer to our proposed method as S CARCE ( SCalable Automated Reference Construction"
2021.findings-acl.357,P16-1009,0,0.0439003,"t al., 2020). We compare the correlations across following setups: S INGLE (Li et al., 2017): Original DailyDialog dataset which had one reference per context; S CARCE -S INGLE: Proposed method along with S INGLE reference; M ULTI (Gupta et al., 2019): 4 human written references. S CARCE -M ULTI: Reference responses from the proposed method along with M ULTI references. Additionally, we report the results when using PARAPHRASE instead of S CARCE: PARAPHRASE-S INGLE and PARAPHRASE-M ULTI. Paraphrasing is a popular approach for automated data augmentation. Paraphrasing via backtranslation (BT) (Sennrich et al., 2016) is known to be an effective, domain-independent way to generate good quality paraphrases (Wieting and Gimpel, 2017). We use the BT model from (Xie et al., 2020) with its default hyperparameters to sample 5 paraphrases per human written reference Results: We observe that most of the metrics show large improvements in correlations to human ratings for appropriateness when used along with S INGLE or M ULTI (Table 1). In fact, rank correlations across most of the metrics are better for S CARCE -S INGLE compared to M ULTI, even though former uses only single human written reference while latter us"
2021.findings-acl.357,P17-1061,0,0.357034,"ive and time consuming. Much focus has therefore been on automated evaluation methods which correlate with human evaluations. Automated metrics such as BLEU (Papineni et al., 2002) work well for tasks such as machine translation, but often correlate poorly with human ratings in tasks such as open domain dialog which admit a wide variety of ∗ VG and HJ contributed equally for this paper. Order decided by coin flip. 1 Code and data are available at https://github.com/harsh19/ Diverse-Reference-Augmentation/ valid response for given context, often due to small number of human written references (Zhao et al., 2017; Sai et al., 2020b). Prior work (Sugiyama et al., 2019; Gupta et al., 2019) has demonstrated that having multiple valid references for the same context leads to automated metrics being better correlated to human judgements for appropriateness. However, collecting human written responses is difficult to scale, can be costly, and may find it difficult to cover a large variety of correct responses (Celikyilmaz et al., 2020). In this work, we automatically extract a large number of diverse references to be used with such reference-based metrics, without resorting to expensive crowd-sourcing. Intu"
2021.findings-acl.357,D19-1670,0,0.0138095,"le. 5 Related Work Prior work explores many ways to improve over single-reference evaluation without collecting multiple ones. Fomicheva et al. (2020) obviate need for multiple references in MT by generating many “althypotheses&quot; via test-time dropout from the same model. Sai et al. (2020a) and Gupta et al. (2019) collect additional manually annotated responses for dialog contexts. Compare to them, our method of automatically collecting additional references automatically is more scalable. Automatic data augmentation in NLP has largely been used for increasing training data (Feng et al., 2020; Wei and Zou, 2019; Feng et al., 2021). In this work, we use retrieved dialog instances and commonsense knowledge base to augment reference set for a given dialog context. ∆-Bleu (Galley et al., 2015) and uBLEU (Yuma et al., 2020) also use retrieval to produce pseudo-references for dialog response evaluation. Compared to ∆-Bleu and uBLEU, our work is different since we utilize commonsense knowledge base and perform contextual adaptation. Prior work in dialog response generation has explored the use of commonsense knowledge base (Majumder et al., 2020) as well as retrieval (Song et al., 2016; Majumder et al., 20"
2021.findings-acl.357,2020.acl-srw.27,0,0.0326577,"y “althypotheses&quot; via test-time dropout from the same model. Sai et al. (2020a) and Gupta et al. (2019) collect additional manually annotated responses for dialog contexts. Compare to them, our method of automatically collecting additional references automatically is more scalable. Automatic data augmentation in NLP has largely been used for increasing training data (Feng et al., 2020; Wei and Zou, 2019; Feng et al., 2021). In this work, we use retrieved dialog instances and commonsense knowledge base to augment reference set for a given dialog context. ∆-Bleu (Galley et al., 2015) and uBLEU (Yuma et al., 2020) also use retrieval to produce pseudo-references for dialog response evaluation. Compared to ∆-Bleu and uBLEU, our work is different since we utilize commonsense knowledge base and perform contextual adaptation. Prior work in dialog response generation has explored the use of commonsense knowledge base (Majumder et al., 2020) as well as retrieval (Song et al., 2016; Majumder et al., 2021) – in contrast, our focus is on augmenting reference set for improving evaluation. Automatic model-based metrics like ADEM (Lowe et al., 2017) and RUBER (Tao et al., 2017), which incorporate context while scor"
2021.findings-acl.84,2020.acl-main.676,0,0.0267569,"§4). Numerous previously mentioned DA techniques, e.g. (Wei and Zou, 2019; Chen et al., 2020b; Anaby-Tavor et al., 2020), have been used or can be used for text classification tasks. 5.1 Question Answering (QA) 5.4 Parsing Tasks Jia and Liang (2016) propose DATA RECOMBINA TION for injecting task-specific priors to neural semantic parsers. A synchronous context-free grammar (SCFG) is induced from training data, and new ""recombinant"" examples are sampled. Yu et al. (2020) introduce G RAPPA, a pretraining approach for table semantic parsing, and generate synthetic question-SQL pairs via an SCFG. Andreas (2020) 973 use compositionality to construct synthetic examples for downstream tasks like semantic parsing. Fragments of original examples are replaced with fragments from other examples in similar contexts. Vania et al. (2019) investigate DA for lowresource dependency parsing including dependency tree morphing from Sahin ¸ and Steedman (2018) (Figure 2) and modified nonce sentence generation from Gulordava et al. (2018), which replaces content words with other words of the same POS, morphological features, and dependency labels. 5.5 Grammatical Error Correction (GEC) Lack of parallel data is typica"
2021.findings-acl.84,2020.acl-main.499,0,0.0168343,"Longpre et al. (2019) investigate various DA and sampling techniques for domain-agnostic QA including paraphrasing by backtranslation. Yang et al. (2019) propose a DA method using distant supervision to improve BERT finetuning for opendomain QA. Riabi et al. (2020) leverage Question Generation models to produce augmented examples for zero-shot cross-lingual QA. Singh et al. (2019) propose XLDA, or CROSS - LINGUAL DA, which substitutes a portion of the input text with its translation in another language, improving performance across multiple languages on NLI tasks including the SQuAD QA task. Asai and Hajishirzi (2020) use logical and linguistic knowledge to generate additional training data to improve the accuracy and consistency of QA responses by models. Yu et al. (2018) introduce a new QA architecture called QANet that shows improved performance on SQuAD when combined with augmented data generated using backtranslation. 5.3 Tasks Summarization Fabbri et al. (2020) investigate backtranslation as a DA method for few-shot abstractive summarization with the use of a consistency loss inspired by UDA. Parida and Motlicek (2019) propose an iterative DA approach for abstractive summarization that uses a mix of"
2021.findings-acl.84,W18-6111,0,0.0244136,"Vania et al. (2019) investigate DA for lowresource dependency parsing including dependency tree morphing from Sahin ¸ and Steedman (2018) (Figure 2) and modified nonce sentence generation from Gulordava et al. (2018), which replaces content words with other words of the same POS, morphological features, and dependency labels. 5.5 Grammatical Error Correction (GEC) Lack of parallel data is typically a barrier for GEC. Various works have thus looked at DA methods for GEC. We discuss some here, and more can be found in Table 2 in Appendix C. There is work that makes use of additional resources. Boyd (2018) use German edits from Wikipedia revision history and use those relating to GEC as augmented training data. Zhang et al. (2019b) explore multi-task transfer, or the use of annotated data from other tasks. There is also work that adds synthetic errors to noise the text. Wang et al. (2019a) investigate two approaches: token-level perturbations and training error generation models with a filtering strategy to keep generations with sufficient errors. Grundkiewicz et al. (2019) use confusion sets generated by a spellchecker for noising. Choe et al. (2019) learn error patterns from small annotated s"
2021.findings-acl.84,P19-1175,0,0.0555398,"Missing"
2021.findings-acl.84,2020.acl-main.12,0,0.0165365,"recent work in vision (Gontijo-Lopes et al., 2020) has proposed that affinity (the distributional shift caused by DA) and diversity (the complexity of the Working in specialized domains such as those with domain-specific vocabulary and jargon (e.g. medicine) can present challenges. Many pretrained models and external knowledge (e.g. WordNet) cannot be effectively used. Studies have shown that DA becomes less beneficial when applied to out-of-domain data, likely because the distribution of augmented data can substantially differ from the original data (Zhang et al., 2019a; Herzig et al., 2020; Campagna et al., 2020; Zhong et al., 2020). 975 Working with low-resource languages may present similar difficulties as specialized domains. Further, DA techniques successful in the highresource scenario may not be effective for lowresource languages that are of a different language family or very distinctive in linguistic and typological terms. For example, those which are language isolates or lack high-resource cognates. More vision-inspired techniques: Although many NLP DA methods have been inspired by analogous approaches in CV, there is potential for drawing further connections. Many CV DA techniques motivate"
2021.findings-acl.84,W17-4714,0,0.062208,"Missing"
2021.findings-acl.84,W19-4423,0,0.0135446,"re is work that makes use of additional resources. Boyd (2018) use German edits from Wikipedia revision history and use those relating to GEC as augmented training data. Zhang et al. (2019b) explore multi-task transfer, or the use of annotated data from other tasks. There is also work that adds synthetic errors to noise the text. Wang et al. (2019a) investigate two approaches: token-level perturbations and training error generation models with a filtering strategy to keep generations with sufficient errors. Grundkiewicz et al. (2019) use confusion sets generated by a spellchecker for noising. Choe et al. (2019) learn error patterns from small annotated samples along with POS-specific noising. There have also been approaches to improve the diversity of generated errors. Wan et al. (2020) investigate noising through editing the latent representations of grammatical sentences, and Xie et al. (2018) use a neural sequence transduction model and beam search noising procedures. 5.6 Neural Machine Translation (NMT) There are many works which have investigated DA for NMT. We highlighted some in §3 and §4.1, e.g. (Sennrich et al., 2016; Fadaee et al., 2017; Xia et al., 2019). We discuss some further ones here"
2021.findings-acl.84,2020.coling-main.343,0,0.028145,"ation (QMDS) called Q MDS C NN and Q MD S I R by modifying CNN/DM (Hermann et al., 2015) and mining search-query logs, respectively. Sequence Tagging Tasks Ding et al. (2020) propose DAGA, a two-step DA process. First, a language model over sequences of tags and words linearized as per a certain scheme is learned. Second, sequences are sampled from this language model and de-linearized to generate new examples. Sahin ¸ and Steedman (2018), discussed in §3.1, use dependency tree morphing (Figure 2) to generate additional training examples on the downstream task of part-of-speech (POS) tagging. Dai and Adel (2020) modify DA techniques proposed for sentence-level tasks for named entity recognition (NER), including label-wise token and synonym replacement, and show improved performance using both recurrent and transformer models. Zhang et al. (2020) propose a DA method based on M IX U P called S EQ M IX for active sequence labeling by augmenting queried samples, showing improvements on NER and Event Detection. In this section, we discuss several DA works for common NLP tasks.2 We focus on nonclassification tasks as classification is worked on by default, and well covered in earlier sections (e.g. §3 and"
2021.findings-acl.84,N19-1423,0,0.014469,"tation. Gao et al. (2019) advocate retaining the full distribution through ""soft"" augmented examples, showing gains on machine translation. Nie et al. (2020) augment word representations with a context-sensitive attention-based mixture of their semantic neighbors from a pretrained embedding space, and show its effectiveness for NER on social media text. Inspired by denoising autoencoders, Ng et al. (2020) use a corrupt-andreconstruct approach, with the corruption function q(x0 |x) masking an arbitrary number of word positions and the reconstruction function r(x|x0 ) unmasking them using BERT (Devlin et al., 2019). Their approach works well on domain-shifted test sets across 9 datasets on sentiment, NLI, and NMT. Feng et al. (2019) propose a task called S EMAN TIC T EXT E XCHANGE (STE) which involves adjusting the overall semantics of a text to fit the context of a new word/phrase that is inserted called the replacement entity (RE). They do so by using a system called SMERTI and a masked LM approach. While not proposed directly for DA, it can be used as such, as investigated in Feng et al. (2020). Rather than starting from an existing example and modifying it, some model-based DA approaches directly es"
2021.findings-acl.84,2020.emnlp-main.488,0,0.0364984,"rida and Motlicek (2019) propose an iterative DA approach for abstractive summarization that uses a mix of synthetic and real data, where the former is generated from Common Crawl. Zhu et al. (2019) introduce a query-focused summarization (Dang, 2005) dataset collected using Wikipedia called W IKI R EF which can be used for DA. Pasunuru et al. (2021) use DA methods to construct two training datasets for Query-focused Multi-Document Summarization (QMDS) called Q MDS C NN and Q MD S I R by modifying CNN/DM (Hermann et al., 2015) and mining search-query logs, respectively. Sequence Tagging Tasks Ding et al. (2020) propose DAGA, a two-step DA process. First, a language model over sequences of tags and words linearized as per a certain scheme is learned. Second, sequences are sampled from this language model and de-linearized to generate new examples. Sahin ¸ and Steedman (2018), discussed in §3.1, use dependency tree morphing (Figure 2) to generate additional training examples on the downstream task of part-of-speech (POS) tagging. Dai and Adel (2020) modify DA techniques proposed for sentence-level tasks for named entity recognition (NER), including label-wise token and synonym replacement, and show im"
2021.findings-acl.84,P17-2090,0,0.174971,"on NLP applications.2 4.1 Low-Resource Languages Low-resource languages are an important and challenging application for DA, typically for neural machine translation (NMT). Techniques using external knowledge such as WordNet (Miller, 1995) may be difficult to use effectively here.3 There are ways to leverage high-resource languages for low-resource languages, particularly if they have similar linguistic properties. Xia et al. (2019) use this approach to improve low-resource NMT. Li et al. (2020b) use backtranslation and selflearning to generate augmented training data. Inspired by work in CV, Fadaee et al. (2017) generate additional training examples that contain lowfrequency (rare) words in synthetically created contexts. Qin et al. (2020) present a DA framework to generate multi-lingual code-switching data to finetune multilingual-BERT. It encourages the alignment of representations from source and multiple target languages once by mixing their context information. They see improved performance across 5 tasks with 19 languages. 3 Low-resource language challenges discussed more in §6. DA Method S YNONYM R EPLACEMENT (Zhang et al., 2015) R ANDOM D ELETION (Wei and Zou, 2019) R ANDOM S WAP (Wei and Zou"
2021.findings-acl.84,2020.deelio-1.4,1,0.884715,"an arbitrary number of word positions and the reconstruction function r(x|x0 ) unmasking them using BERT (Devlin et al., 2019). Their approach works well on domain-shifted test sets across 9 datasets on sentiment, NLI, and NMT. Feng et al. (2019) propose a task called S EMAN TIC T EXT E XCHANGE (STE) which involves adjusting the overall semantics of a text to fit the context of a new word/phrase that is inserted called the replacement entity (RE). They do so by using a system called SMERTI and a masked LM approach. While not proposed directly for DA, it can be used as such, as investigated in Feng et al. (2020). Rather than starting from an existing example and modifying it, some model-based DA approaches directly estimate a generative process from the training set and sample from it. AnabyTavor et al. (2020) learn a label-conditioned generator by finetuning GPT-2 (Radford et al., 2019) on the training data, using this to generate candidate examples per class. A classifier trained on the original training set is then used to select top k candidate examples which confidently belong to the respective class for augmentation. Quteineh et al. (2020) use a similar label-conditioned GPT-2 generation method"
2021.findings-acl.84,P19-1555,0,0.106378,"yashi (2018) and show its effectiveness as DA for several classification tasks. Pretrained language models such as RNNs (Kobayashi, 2018) and transformers (Yang et al., 2020) have also been used for augmentation. Kobayashi (2018) generate augmented examples by replacing words with others randomly drawn according to the recurrent language model’s distribution based on the current context (illustration in Figure 3). Yang et al. (2020) propose GDAUGc which generates synthetic examples using pretrained transformer language models, and selects the most informative and diverse set for augmentation. Gao et al. (2019) advocate retaining the full distribution through ""soft"" augmented examples, showing gains on machine translation. Nie et al. (2020) augment word representations with a context-sensitive attention-based mixture of their semantic neighbors from a pretrained embedding space, and show its effectiveness for NER on social media text. Inspired by denoising autoencoders, Ng et al. (2020) use a corrupt-andreconstruct approach, with the corruption function q(x0 |x) masking an arbitrary number of word positions and the reconstruction function r(x|x0 ) unmasking them using BERT (Devlin et al., 2019). The"
2021.findings-acl.84,2020.acl-main.60,0,0.044451,"Missing"
2021.findings-acl.84,W17-3518,0,0.0229672,"the vocabulary. Nguyen et al. (2020) propose DATA D IVERSIFICATION which merges original training data with the predictions of several forward and backward models. 5.7 Data-to-Text NLG Data-to-text NLG refers to tasks which require generating natural language descriptions of structured or semi-structured data inputs, e.g. game score tables (Wiseman et al., 2017). Randomly perturbing game score values without invalidating overall game outcome is one DA strategy explored in game summary generation (Hayashi et al., 2019). Two popular recent benchmarks are E2E-NLG (Dušek et al., 2018) and WebNLG (Gardent et al., 2017). Both involve generation from structured inputs - meaning representation (MR) sequences and triple sequences, respectively. Montella et al. (2020) show performance gains on WebNLG by DA using Wikipedia sentences as targets and parsed OpenIE triples as inputs. Tandon et al. (2018) propose DA for E2E-NLG based on permuting the input MR sequence. Kedzie and McKeown (2019) inject Gaussian noise into a trained decoder’s hidden states and sample diverse augmented examples from it. This sample-augmentretrain loop helps performance on E2E-NLG. 5.8 Open-Ended & Conditional Generation There has been li"
2021.findings-acl.84,P18-2103,0,0.0136442,"ion facilitates curriculum learning for training triplet networks for few-shot text classification. Lee et al. (2021) use T5 to generate additional examples for data-scarce classes. 4.5 Adversarial Examples (AVEs) Adversarial examples can be generated using innocuous label-preserving transformations (e.g. paraphrasing) that fool state-of-the-art NLP models, as shown in Jia et al. (2019). Specifically, they add sentences with distractor spans to passages to construct AVEs for span-based QA. Zhang et al. (2019d) construct AVEs for paraphrase detection using word swapping. Kang et al. (2018) and Glockner et al. (2018) create AVEs for textual entailment using WordNet relations. 5 5.2 Longpre et al. (2019) investigate various DA and sampling techniques for domain-agnostic QA including paraphrasing by backtranslation. Yang et al. (2019) propose a DA method using distant supervision to improve BERT finetuning for opendomain QA. Riabi et al. (2020) leverage Question Generation models to produce augmented examples for zero-shot cross-lingual QA. Singh et al. (2019) propose XLDA, or CROSS - LINGUAL DA, which substitutes a portion of the input text with its translation in another language, improving performance ac"
2021.findings-acl.84,W19-5205,0,0.0602769,"Missing"
2021.findings-acl.84,2021.tacl-1.3,0,0.0429607,"Missing"
2021.findings-acl.84,W19-4427,0,0.0680803,"u, 2019) BACKTRANSLATION (Sennrich et al., 2016) SCPN (Wieting and Gimpel, 2017) S EMANTIC T EXT E XCHANGE (Feng et al., 2019) C ONTEXTUAL AUG (Kobayashi, 2018) LAMBADA (Anaby-Tavor et al., 2020) GECA (Andreas, 2020) S EQ M IX U P (Guo et al., 2020) S WITCH O UT (Wang et al., 2018b) E MIX (Jindal et al., 2020a) S PEECH M IX (Jindal et al., 2020b) M IX T EXT (Chen et al., 2020c) S IGNED G RAPH (Chen et al., 2020b) DT REE M ORPH (Sahin ¸ and Steedman, 2018) Sub2 (Shi et al., 2021) DAGA (Ding et al., 2020) WN-H YPERS (Feng et al., 2020) S YNTHETIC N OISE (Feng et al., 2020) UE DIN -MS (DA part) (Grundkiewicz et al., 2019) N ONCE (Gulordava et al., 2018) XLDA (Singh et al., 2019) S EQ M IX (Zhang et al., 2020) S LOT-S UB -LM (Louvan and Magnini, 2020) UBT & TBT (Vaibhav et al., 2019) S OFT C ONTEXTUAL DA (Gao et al., 2019) DATA D IVERSIFICATION (Nguyen et al., 2020) D I PS (Kumar et al., 2019a) AUGMENTED SBERT (Thakur et al., 2021) Ext.Know Pretrained Preprocess × × × 3 3 3 3 3 × × × × × × × × × × × × × × 3 3 3 3 3 3 3 3 3 × × × × × × × × × × × × × × × × × 3 × 3 3 × × × × × × × × tok tok tok Depends const const tok tok tok dep dep tok const+KWE tok tok const Depends tok tok Depends tok Depends tok - Level Task-"
2021.findings-acl.84,D19-1530,0,0.037302,"Missing"
2021.findings-acl.84,D19-5601,0,0.0162949,"nts randomly chosen words in a sentence using a contextual mixture of multiple related words over the vocabulary. Nguyen et al. (2020) propose DATA D IVERSIFICATION which merges original training data with the predictions of several forward and backward models. 5.7 Data-to-Text NLG Data-to-text NLG refers to tasks which require generating natural language descriptions of structured or semi-structured data inputs, e.g. game score tables (Wiseman et al., 2017). Randomly perturbing game score values without invalidating overall game outcome is one DA strategy explored in game summary generation (Hayashi et al., 2019). Two popular recent benchmarks are E2E-NLG (Dušek et al., 2018) and WebNLG (Gardent et al., 2017). Both involve generation from structured inputs - meaning representation (MR) sequences and triple sequences, respectively. Montella et al. (2020) show performance gains on WebNLG by DA using Wikipedia sentences as targets and parsed OpenIE triples as inputs. Tandon et al. (2018) propose DA for E2E-NLG based on permuting the input MR sequence. Kedzie and McKeown (2019) inject Gaussian noise into a trained decoder’s hidden states and sample diverse augmented examples from it. This sample-augmentre"
2021.findings-acl.84,2020.acl-main.398,0,0.0158664,"-scale experiment. A recent work in vision (Gontijo-Lopes et al., 2020) has proposed that affinity (the distributional shift caused by DA) and diversity (the complexity of the Working in specialized domains such as those with domain-specific vocabulary and jargon (e.g. medicine) can present challenges. Many pretrained models and external knowledge (e.g. WordNet) cannot be effectively used. Studies have shown that DA becomes less beneficial when applied to out-of-domain data, likely because the distribution of augmented data can substantially differ from the original data (Zhang et al., 2019a; Herzig et al., 2020; Campagna et al., 2020; Zhong et al., 2020). 975 Working with low-resource languages may present similar difficulties as specialized domains. Further, DA techniques successful in the highresource scenario may not be effective for lowresource languages that are of a different language family or very distinctive in linguistic and typological terms. For example, those which are language isolates or lack high-resource cognates. More vision-inspired techniques: Although many NLP DA methods have been inspired by analogous approaches in CV, there is potential for drawing further connections. Many CV"
2021.findings-acl.84,C18-1105,0,0.0264481,"TIC N OISE (randomly perturbing non-terminal characters in words) are useful, and the quality of generated text improves to a peak at ≈ 3x the original amount of training data. 5.9 Dialogue Most DA approaches for dialogue focus on taskoriented dialogue. We outline some below, and more can be found in Table 4 in Appendix C. Quan and Xiong (2019) present sentence and word-level DA approaches for end-to-end taskoriented dialogue. Louvan and Magnini (2020) propose LIGHTWEIGHT AUGMENTATION, a set of word-span and sentence-level DA methods for lowresource slot filling and intent classification. 974 Hou et al. (2018) present a seq2seq DA framework to augment dialogue utterances for dialogue language understanding (Young et al., 2013), including a diversity rank to produce diverse utterances. Zhang et al. (2019c) propose MADA to generate diverse responses using the property that several valid responses exist for a dialogue context. There is also DA work for spoken dialogue. Hou et al. (2018), Kim et al. (2019), Zhao et al. (2019), and Yoo et al. (2019) investigate DA methods for dialogue and spoken language understanding (SLU), including generative latent variable models. 5.10 Multimodal Tasks DA technique"
2021.findings-acl.84,P18-1225,1,0.822205,"how that data augmentation facilitates curriculum learning for training triplet networks for few-shot text classification. Lee et al. (2021) use T5 to generate additional examples for data-scarce classes. 4.5 Adversarial Examples (AVEs) Adversarial examples can be generated using innocuous label-preserving transformations (e.g. paraphrasing) that fool state-of-the-art NLP models, as shown in Jia et al. (2019). Specifically, they add sentences with distractor spans to passages to construct AVEs for span-based QA. Zhang et al. (2019d) construct AVEs for paraphrase detection using word swapping. Kang et al. (2018) and Glockner et al. (2018) create AVEs for textual entailment using WordNet relations. 5 5.2 Longpre et al. (2019) investigate various DA and sampling techniques for domain-agnostic QA including paraphrasing by backtranslation. Yang et al. (2019) propose a DA method using distant supervision to improve BERT finetuning for opendomain QA. Riabi et al. (2020) leverage Question Generation models to produce augmented examples for zero-shot cross-lingual QA. Singh et al. (2019) propose XLDA, or CROSS - LINGUAL DA, which substitutes a portion of the input text with its translation in another languag"
2021.findings-acl.84,N19-1333,0,0.0422133,"Missing"
2021.findings-acl.84,2020.webnlg-1.9,0,0.0351998,"backward models. 5.7 Data-to-Text NLG Data-to-text NLG refers to tasks which require generating natural language descriptions of structured or semi-structured data inputs, e.g. game score tables (Wiseman et al., 2017). Randomly perturbing game score values without invalidating overall game outcome is one DA strategy explored in game summary generation (Hayashi et al., 2019). Two popular recent benchmarks are E2E-NLG (Dušek et al., 2018) and WebNLG (Gardent et al., 2017). Both involve generation from structured inputs - meaning representation (MR) sequences and triple sequences, respectively. Montella et al. (2020) show performance gains on WebNLG by DA using Wikipedia sentences as targets and parsed OpenIE triples as inputs. Tandon et al. (2018) propose DA for E2E-NLG based on permuting the input MR sequence. Kedzie and McKeown (2019) inject Gaussian noise into a trained decoder’s hidden states and sample diverse augmented examples from it. This sample-augmentretrain loop helps performance on E2E-NLG. 5.8 Open-Ended & Conditional Generation There has been limited work on DA for open-ended and conditional text generation. Feng et al. (2020) experiment with a suite of DA methods for finetuning GPT-2 on a"
2021.findings-acl.84,2020.emnlp-main.97,0,0.0312233,"ntext (illustration in Figure 3). Yang et al. (2020) propose GDAUGc which generates synthetic examples using pretrained transformer language models, and selects the most informative and diverse set for augmentation. Gao et al. (2019) advocate retaining the full distribution through ""soft"" augmented examples, showing gains on machine translation. Nie et al. (2020) augment word representations with a context-sensitive attention-based mixture of their semantic neighbors from a pretrained embedding space, and show its effectiveness for NER on social media text. Inspired by denoising autoencoders, Ng et al. (2020) use a corrupt-andreconstruct approach, with the corruption function q(x0 |x) masking an arbitrary number of word positions and the reconstruction function r(x|x0 ) unmasking them using BERT (Devlin et al., 2019). Their approach works well on domain-shifted test sets across 9 datasets on sentiment, NLI, and NMT. Feng et al. (2019) propose a task called S EMAN TIC T EXT E XCHANGE (STE) which involves adjusting the overall semantics of a text to fit the context of a new word/phrase that is inserted called the replacement entity (RE). They do so by using a system called SMERTI and a masked LM app"
2021.findings-acl.84,P15-2070,0,0.0490698,"Missing"
2021.findings-acl.84,2020.emnlp-main.600,0,0.0356232,"directly for DA, it can be used as such, as investigated in Feng et al. (2020). Rather than starting from an existing example and modifying it, some model-based DA approaches directly estimate a generative process from the training set and sample from it. AnabyTavor et al. (2020) learn a label-conditioned generator by finetuning GPT-2 (Radford et al., 2019) on the training data, using this to generate candidate examples per class. A classifier trained on the original training set is then used to select top k candidate examples which confidently belong to the respective class for augmentation. Quteineh et al. (2020) use a similar label-conditioned GPT-2 generation method, and demonstrate its effectiveness as a DA method in an active learning setup. Other approaches include syntactic or controlled paraphrasing (Iyyer et al., 2018; Kumar et al., 2020), document or story-level paraphrasing (Gangal et al., 2021), augmenting misclassified examples (Dreossi et al., 2018), BERT cross-encoder labeling of new inputs (Thakur et al., 2021), and guided generation using large-scale generative language models (Liu et al., 2020b,c). Models can also learn to combine together simpler DA primitives (Cubuk et al., 2018; Ra"
2021.findings-acl.84,D18-1545,0,0.301879,"ways DA helps: averaging of features and variance regularization. Chen et al. (2020d) show that DA leads to variance reduction by averaging over orbits of the group that keep the data distribution approximately invariant. 3 Techniques & Methods We now discuss some methodologically representative DA techniques which are relevant to all tasks via the extensibility of their formulation.2 969 2 Table 1 compares several DA methods by various aspects relating to their applicability, dependencies, and requirements. 3.2 Figure 2: Dependency tree morphing DA applied to a Turkish sentence, Sahin ¸ and Steedman (2018) 3.1 Rule-Based Techniques Here, we cover DA primitives which use easyto-compute, predetermined transforms sans model components. Feature space DA approaches generate augmented examples in the model’s feature space rather than input data. Many few-shot learning approaches (Hariharan and Girshick, 2017; Schwartz et al., 2018) leverage estimated feature space ""analogy"" transformations between examples of known classes to augment for novel classes (see §4.4). Paschali et al. (2019) use iterative affine transformations and projections to maximally ""stretch"" an example along the class-manifold. Wei"
2021.findings-acl.84,P16-1009,0,0.621579,"P (Guo et al., 2020) generalizes M IX U P for sequence transduction tasks in two ways - the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like S WITCH O UT (Wang et al., 2018a). 3.3 Model-Based Techniques Seq2seq and language models have also been used for DA. The popular BACKTRANSLATION method (Sennrich et al., 2016) translates a sequence into another language and then back into the original language. Kumar et al. (2019a) train seq2seq models with their proposed method DiPS which learns to generate diverse paraphrases of input text using a modified decoder with a submodular objective, 970 Figure 3: Contextual Augmentation, Kobayashi (2018) and show its effectiveness as DA for several classification tasks. Pretrained language models such as RNNs (Kobayashi, 2018) and transformers (Yang et al., 2020) have also been used for augmentation. Kobayashi (2018) generate augmented examples by replacing words with o"
2021.findings-acl.84,2021.findings-acl.307,0,0.0729355,"Missing"
2021.findings-acl.84,D19-6504,0,0.0596753,"Missing"
2021.findings-acl.84,2021.naacl-main.28,0,0.0634242,"per class. A classifier trained on the original training set is then used to select top k candidate examples which confidently belong to the respective class for augmentation. Quteineh et al. (2020) use a similar label-conditioned GPT-2 generation method, and demonstrate its effectiveness as a DA method in an active learning setup. Other approaches include syntactic or controlled paraphrasing (Iyyer et al., 2018; Kumar et al., 2020), document or story-level paraphrasing (Gangal et al., 2021), augmenting misclassified examples (Dreossi et al., 2018), BERT cross-encoder labeling of new inputs (Thakur et al., 2021), and guided generation using large-scale generative language models (Liu et al., 2020b,c). Models can also learn to combine together simpler DA primitives (Cubuk et al., 2018; Ratner et al., 2017) or add human-in-the-loop (Kaushik et al., 2020, 2021). 971 4 Applications In this section, we discuss several DA methods for some common NLP applications.2 4.1 Low-Resource Languages Low-resource languages are an important and challenging application for DA, typically for neural machine translation (NMT). Techniques using external knowledge such as WordNet (Miller, 1995) may be difficult to use effe"
2021.findings-acl.84,N19-1190,0,0.0614498,"Missing"
2021.findings-acl.84,D19-1102,0,0.0207881,"arsing Tasks Jia and Liang (2016) propose DATA RECOMBINA TION for injecting task-specific priors to neural semantic parsers. A synchronous context-free grammar (SCFG) is induced from training data, and new ""recombinant"" examples are sampled. Yu et al. (2020) introduce G RAPPA, a pretraining approach for table semantic parsing, and generate synthetic question-SQL pairs via an SCFG. Andreas (2020) 973 use compositionality to construct synthetic examples for downstream tasks like semantic parsing. Fragments of original examples are replaced with fragments from other examples in similar contexts. Vania et al. (2019) investigate DA for lowresource dependency parsing including dependency tree morphing from Sahin ¸ and Steedman (2018) (Figure 2) and modified nonce sentence generation from Gulordava et al. (2018), which replaces content words with other words of the same POS, morphological features, and dependency labels. 5.5 Grammatical Error Correction (GEC) Lack of parallel data is typically a barrier for GEC. Various works have thus looked at DA methods for GEC. We discuss some here, and more can be found in Table 2 in Appendix C. There is work that makes use of additional resources. Boyd (2018) use Germ"
2021.findings-acl.84,2020.coling-main.200,0,0.0290317,"l. (2019b) explore multi-task transfer, or the use of annotated data from other tasks. There is also work that adds synthetic errors to noise the text. Wang et al. (2019a) investigate two approaches: token-level perturbations and training error generation models with a filtering strategy to keep generations with sufficient errors. Grundkiewicz et al. (2019) use confusion sets generated by a spellchecker for noising. Choe et al. (2019) learn error patterns from small annotated samples along with POS-specific noising. There have also been approaches to improve the diversity of generated errors. Wan et al. (2020) investigate noising through editing the latent representations of grammatical sentences, and Xie et al. (2018) use a neural sequence transduction model and beam search noising procedures. 5.6 Neural Machine Translation (NMT) There are many works which have investigated DA for NMT. We highlighted some in §3 and §4.1, e.g. (Sennrich et al., 2016; Fadaee et al., 2017; Xia et al., 2019). We discuss some further ones here, and more can be found in Table 3 in Appendix C. Wang et al. (2018a) propose S WITCH O UT, a DA method that randomly replaces words in both source and target sentences with other"
2021.findings-acl.84,D18-1100,0,0.238676,"tailored mixing schemes (Jindal et al., 2020b) and interpolation with adversarial examples (Cheng et al., 2020), among others. S EQ 2M IX U P (Guo et al., 2020) generalizes M IX U P for sequence transduction tasks in two ways - the ""hard"" version samples a binary mask (from a Bernoulli with a β(α, α) prior) and picks from one of two sequences at each token position, while the ""soft"" version softly interpolates between sequences based on a coefficient sampled from β(α, α). The ""soft"" version is found to outperform the ""hard"" version and earlier interpolation-based techniques like S WITCH O UT (Wang et al., 2018a). 3.3 Model-Based Techniques Seq2seq and language models have also been used for DA. The popular BACKTRANSLATION method (Sennrich et al., 2016) translates a sequence into another language and then back into the original language. Kumar et al. (2019a) train seq2seq models with their proposed method DiPS which learns to generate diverse paraphrases of input text using a modified decoder with a submodular objective, 970 Figure 3: Contextual Augmentation, Kobayashi (2018) and show its effectiveness as DA for several classification tasks. Pretrained language models such as RNNs (Kobayashi, 2018)"
2021.findings-acl.84,2021.naacl-main.434,1,0.882504,"invariances is less obvious. What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two. Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize. Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance. Kashefi and Hwa (2"
2021.findings-acl.84,2021.eacl-main.252,1,0.894767,"invariances is less obvious. What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two. Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize. Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance. Kashefi and Hwa (2"
2021.findings-acl.84,D19-1670,1,0.54652,"capture the desired invariances is less obvious. What are the goals and trade-offs? Despite challenges associated with text, many DA techniques for NLP have been proposed, ranging from rule-based manipulations (Zhang et al., 2015) to more complicated generative approaches (Liu et al., 2020b). As DA aims to provide an alternative to collecting more data, an ideal DA technique should be both easy-to-implement and improve model performance. Most offer trade-offs between these two. Rule-based techniques are easy-to-implement but usually offer incremental performance improvements (Li et al., 2017; Wei and Zou, 2019; Wei et al., 2021b). Techniques leveraging trained models may be more costly to implement but introduce more data variation, leading to better performance boosts. Model-based techniques customized for downstream tasks can have strong effects on performance but be difficult to develop and utilize. Further, the distribution of augmented data should neither be too similar nor too different from the original. This may lead to greater overfitting or poor performance through training on examples not representative of the given domain, respectively. Effective DA approaches should aim for a balance."
2021.findings-acl.84,2020.bea-1.21,0,0.0846676,"Missing"
2021.findings-acl.84,P17-1190,0,0.0491557,"Missing"
2021.findings-acl.84,D17-1239,0,0.023004,"h source and target sentences with other random words from their corresponding vocabularies. Gao et al. (2019) introduce S OFT C ONTEXTUAL DA that softly augments randomly chosen words in a sentence using a contextual mixture of multiple related words over the vocabulary. Nguyen et al. (2020) propose DATA D IVERSIFICATION which merges original training data with the predictions of several forward and backward models. 5.7 Data-to-Text NLG Data-to-text NLG refers to tasks which require generating natural language descriptions of structured or semi-structured data inputs, e.g. game score tables (Wiseman et al., 2017). Randomly perturbing game score values without invalidating overall game outcome is one DA strategy explored in game summary generation (Hayashi et al., 2019). Two popular recent benchmarks are E2E-NLG (Dušek et al., 2018) and WebNLG (Gardent et al., 2017). Both involve generation from structured inputs - meaning representation (MR) sequences and triple sequences, respectively. Montella et al. (2020) show performance gains on WebNLG by DA using Wikipedia sentences as targets and parsed OpenIE triples as inputs. Tandon et al. (2018) propose DA for E2E-NLG based on permuting the input MR sequen"
2021.findings-acl.84,P19-1579,0,0.109762,"ives (Cubuk et al., 2018; Ratner et al., 2017) or add human-in-the-loop (Kaushik et al., 2020, 2021). 971 4 Applications In this section, we discuss several DA methods for some common NLP applications.2 4.1 Low-Resource Languages Low-resource languages are an important and challenging application for DA, typically for neural machine translation (NMT). Techniques using external knowledge such as WordNet (Miller, 1995) may be difficult to use effectively here.3 There are ways to leverage high-resource languages for low-resource languages, particularly if they have similar linguistic properties. Xia et al. (2019) use this approach to improve low-resource NMT. Li et al. (2020b) use backtranslation and selflearning to generate augmented training data. Inspired by work in CV, Fadaee et al. (2017) generate additional training examples that contain lowfrequency (rare) words in synthetically created contexts. Qin et al. (2020) present a DA framework to generate multi-lingual code-switching data to finetune multilingual-BERT. It encourages the alignment of representations from source and multiple target languages once by mixing their context information. They see improved performance across 5 tasks with 19 l"
2021.findings-acl.84,N18-1057,0,0.0237125,"adds synthetic errors to noise the text. Wang et al. (2019a) investigate two approaches: token-level perturbations and training error generation models with a filtering strategy to keep generations with sufficient errors. Grundkiewicz et al. (2019) use confusion sets generated by a spellchecker for noising. Choe et al. (2019) learn error patterns from small annotated samples along with POS-specific noising. There have also been approaches to improve the diversity of generated errors. Wan et al. (2020) investigate noising through editing the latent representations of grammatical sentences, and Xie et al. (2018) use a neural sequence transduction model and beam search noising procedures. 5.6 Neural Machine Translation (NMT) There are many works which have investigated DA for NMT. We highlighted some in §3 and §4.1, e.g. (Sennrich et al., 2016; Fadaee et al., 2017; Xia et al., 2019). We discuss some further ones here, and more can be found in Table 3 in Appendix C. Wang et al. (2018a) propose S WITCH O UT, a DA method that randomly replaces words in both source and target sentences with other random words from their corresponding vocabularies. Gao et al. (2019) introduce S OFT C ONTEXTUAL DA that soft"
2021.findings-acl.84,W19-4415,0,0.0277437,"Missing"
2021.findings-acl.84,2020.findings-emnlp.90,0,0.0394786,"Missing"
2021.findings-acl.84,L18-1436,0,0.0281775,"C. Beginning with speech, Wang et al. (2020) propose a DA method to improve the robustness of downstream dialogue models to speech recognition errors. Wiesner et al. (2018) and Renduchintala et al. (2018) propose DA methods for end-to-end automatic speech recognition (ASR). Looking at images or video, Xu et al. (2020) learn a cross-modality matching network to produce synthetic image-text pairs for multimodal classifiers. Atliha and Šešok (2020) explore DA methods such as synonym replacement and contextualized word embeddings augmentation using BERT for image captioning. Kafle et al. (2017), Yokota and Nakayama (2018), and Tang et al. (2020) propose methods for visual QA including question generation and adversarial examples. 6 augmentation) can predict DA performance, but it is unclear how these results might translate to NLP. Minimal benefit for pretrained models on indomain data: With the popularization of large pretrained language models, it has recently come to light that a couple of previously effective DA techniques for certain text classification tasks in English (Wei and Zou, 2019; Sennrich et al., 2016) provide little benefit for models like BERT and RoBERTa, which already achieve high performanc"
2021.findings-acl.84,2020.emnlp-main.691,0,0.0351694,"er sequences of tags and words linearized as per a certain scheme is learned. Second, sequences are sampled from this language model and de-linearized to generate new examples. Sahin ¸ and Steedman (2018), discussed in §3.1, use dependency tree morphing (Figure 2) to generate additional training examples on the downstream task of part-of-speech (POS) tagging. Dai and Adel (2020) modify DA techniques proposed for sentence-level tasks for named entity recognition (NER), including label-wise token and synonym replacement, and show improved performance using both recurrent and transformer models. Zhang et al. (2020) propose a DA method based on M IX U P called S EQ M IX for active sequence labeling by augmenting queried samples, showing improvements on NER and Event Detection. In this section, we discuss several DA works for common NLP tasks.2 We focus on nonclassification tasks as classification is worked on by default, and well covered in earlier sections (e.g. §3 and §4). Numerous previously mentioned DA techniques, e.g. (Wei and Zou, 2019; Chen et al., 2020b; Anaby-Tavor et al., 2020), have been used or can be used for text classification tasks. 5.1 Question Answering (QA) 5.4 Parsing Tasks Jia and L"
2021.findings-acl.84,D19-1537,0,0.0428563,"Missing"
2021.findings-acl.84,N19-1131,0,0.0338603,"Missing"
2021.findings-acl.84,N18-2003,0,0.0239995,"pplicability, dependencies, and requirements. Ext.Know, KWE, tok, const, and dep stand for External Knowledge, keyword extraction, tokenization, constituency parsing, and dependency parsing, respectively. Ext.Know refers to whether the DA method requires external knowledge (e.g. WordNet) and Pretrained if it requires a pretrained model (e.g. BERT). Preprocess denotes preprocessing required, Level denotes the depth at which data is modified by the DA, and Task-Agnostic refers to whether the DA method can be applied to different tasks. See Appendix B for further explanation. 4.2 Mitigating Bias Zhao et al. (2018) attempt to mitigate gender bias in coreference resolution by creating an augmented dataset identical to the original but biased towards the underrepresented gender (using gender swapping of entities such as replacing ""he"" with ""she"") and train on the union of the two datasets. Lu et al. (2020) formally propose COUN TERFACTUAL DA (CDA) for gender bias mitigation, which involves causal interventions that break associations between gendered and gender-neutral words. Zmigrod et al. (2019) and Hall Maudslay et al. (2019) propose further improvements to CDA. Moosavi et al. (2020) augment training s"
2021.findings-acl.84,D19-1375,0,0.0254998,"d dialogue. Louvan and Magnini (2020) propose LIGHTWEIGHT AUGMENTATION, a set of word-span and sentence-level DA methods for lowresource slot filling and intent classification. 974 Hou et al. (2018) present a seq2seq DA framework to augment dialogue utterances for dialogue language understanding (Young et al., 2013), including a diversity rank to produce diverse utterances. Zhang et al. (2019c) propose MADA to generate diverse responses using the property that several valid responses exist for a dialogue context. There is also DA work for spoken dialogue. Hou et al. (2018), Kim et al. (2019), Zhao et al. (2019), and Yoo et al. (2019) investigate DA methods for dialogue and spoken language understanding (SLU), including generative latent variable models. 5.10 Multimodal Tasks DA techniques have also been proposed for multimodal tasks where aligned data for multiple modalities is required. We look at ones that involve language or text. Some are discussed below, and more can be found in Table 5 in Appendix C. Beginning with speech, Wang et al. (2020) propose a DA method to improve the robustness of downstream dialogue models to speech recognition errors. Wiesner et al. (2018) and Renduchintala et al. ("
2021.findings-acl.84,2020.emnlp-main.558,0,0.0178819,"Gontijo-Lopes et al., 2020) has proposed that affinity (the distributional shift caused by DA) and diversity (the complexity of the Working in specialized domains such as those with domain-specific vocabulary and jargon (e.g. medicine) can present challenges. Many pretrained models and external knowledge (e.g. WordNet) cannot be effectively used. Studies have shown that DA becomes less beneficial when applied to out-of-domain data, likely because the distribution of augmented data can substantially differ from the original data (Zhang et al., 2019a; Herzig et al., 2020; Campagna et al., 2020; Zhong et al., 2020). 975 Working with low-resource languages may present similar difficulties as specialized domains. Further, DA techniques successful in the highresource scenario may not be effective for lowresource languages that are of a different language family or very distinctive in linguistic and typological terms. For example, those which are language isolates or lack high-resource cognates. More vision-inspired techniques: Although many NLP DA methods have been inspired by analogous approaches in CV, there is potential for drawing further connections. Many CV DA techniques motivated by real-world invar"
2021.findings-acl.84,P19-1161,0,0.0283339,"her the DA method can be applied to different tasks. See Appendix B for further explanation. 4.2 Mitigating Bias Zhao et al. (2018) attempt to mitigate gender bias in coreference resolution by creating an augmented dataset identical to the original but biased towards the underrepresented gender (using gender swapping of entities such as replacing ""he"" with ""she"") and train on the union of the two datasets. Lu et al. (2020) formally propose COUN TERFACTUAL DA (CDA) for gender bias mitigation, which involves causal interventions that break associations between gendered and gender-neutral words. Zmigrod et al. (2019) and Hall Maudslay et al. (2019) propose further improvements to CDA. Moosavi et al. (2020) augment training sentences with their corresponding predicate-argument structures, improving the robustness of transformer models against various types of biases. 4.3 M INORITY OVERSAMPLING T ECHNIQUE (SMOTE) (Chawla et al., 2002), which generates augmented minority class examples through interpolation, still remains popular (Fernández et al., 2018). M ULTILABEL SMOTE (MLSMOTE) (Charte et al., 2015) modifies SMOTE to balance classes for multi-label classification, where classifiers predict more than one"
2021.findings-emnlp.264,D15-1075,0,0.0459476,"ith an incoming entity on WikidataEx (e.g., “England” with “Yorkshire”). This achieves two goals. First, P 0 includes an entity that is an example of another entity in H so that the (P 0 , H) pair requires example-based inference, with the same expected relation as the (P, H) pair. Second, this example relation comes from our KG so that the NLI model learns how to use the KG. Generating similar NLI pairs for causality-based inference is more challenging, and we leave it to future work. Our data mainly come from public NLI datasets: MNLI (Williams et al., 2018), ANLI (Nie et al., 2020b), SNLI (Bowman et al., 2015), and FEVER- Inference Evaluation: We use additional NLI (Nie et al., 2019). We split the data into train, datasets to evaluate NLI models’ inference abilivalidation, and test sets as originally or conven- ties. For example-based inference, we first use a tionally set up for each dataset (Table 1). Due to diagnostic subset of ANLI that has been annotated limited computational resources, our training set with various categories of required inference, such includes only MNLI and ANLI. as counting, negation, and coreference (Williams 3077 et al., 2020). We choose the instances of the ‘Containment"
2021.findings-emnlp.264,P17-1171,0,0.0300088,"en describe evaluation settings and results. 4.1 Stages ment to verify, we retrieve candidate documents from Wikipedia, Bing, and Google. For Wikipedia, we use the Spacy Entity Linker to retrieve the articles of Wikidata entities linked to the statement. And for each linked entity, we additionally sample at most five of their instance entities and the corresponding articles, which potentially include counterexamples to the statement. We retrieve additional Wikipedia pages by using named entities in the statement as queries for the wikipedia library3 . We also conduct TF-IDF search using DrQA (Chen et al., 2017) indexed for the FEVER task. For Bing and Google, we use their search APIs. Wikipedia pages are excluded from their search results, and PDF files are processed using the pdfminer library. Document Ranking: Retrieved documents are ranked via DeSePtion with some adaptation. First, RoBERTa is trained to predict whether each document is relevant or not, on the FEVER data. It takes the concatenation of a document snippet and the statement to verify as input. For documents from Bing and Google, we use search result snippets; for Wikipedia, we obtain snippets by concatenating the title of each Wikipe"
2021.findings-emnlp.264,P18-1224,0,0.0166759,"ences of the target argument (Reisert mainly to learn better embeddings of tokens and enet al., 2015; Sato et al., 2015). Recently, a human- tities (Wang et al., 2021; Peters et al., 2019; Zhang curated corpus was developed (Orbach et al., 2020). et al., 2019; Lauscher et al., 2020). Once learnNeural language generation approaches take ing is done, the model does not require external 3075 knowledge during inference. The second type of models use the triple information of entities linked to the input text during inference, either by encoding knowledge using a separate network (He et al., 2020; Chen et al., 2018) or by converting the KG to input text tokens (Liu et al., 2020). Our work adopts the second approach, as it depends less on the snapshot of the KG used for pretraining. But our model design has clear distinctions from previous work in the way that KGs are integrated and entity paths are taken. 3 Knowledge-Enhanced NLI A natural language inference (NLI) model is the core of our entire system (Figure 1). Given a statement to refute, the system retrieves and ranks relevant documents, and then obtains a set of candidate sentences for counterevidence. For each candidate, the NLI model decides whet"
2021.findings-emnlp.264,W17-0812,0,0.0316623,"Missing"
2021.findings-emnlp.264,P18-1023,0,0.0649081,"Missing"
2021.findings-emnlp.264,N18-1101,0,0.0348173,"P 0 by replacing an entity that occurs in both P and H with an incoming entity on WikidataEx (e.g., “England” with “Yorkshire”). This achieves two goals. First, P 0 includes an entity that is an example of another entity in H so that the (P 0 , H) pair requires example-based inference, with the same expected relation as the (P, H) pair. Second, this example relation comes from our KG so that the NLI model learns how to use the KG. Generating similar NLI pairs for causality-based inference is more challenging, and we leave it to future work. Our data mainly come from public NLI datasets: MNLI (Williams et al., 2018), ANLI (Nie et al., 2020b), SNLI (Bowman et al., 2015), and FEVER- Inference Evaluation: We use additional NLI (Nie et al., 2019). We split the data into train, datasets to evaluate NLI models’ inference abilivalidation, and test sets as originally or conven- ties. For example-based inference, we first use a tionally set up for each dataset (Table 1). Due to diagnostic subset of ANLI that has been annotated limited computational resources, our training set with various categories of required inference, such includes only MNLI and ANLI. as counting, negation, and coreference (Williams 3077 et a"
2021.findings-emnlp.264,P19-1139,0,0.0421298,"Missing"
2021.inlg-1.21,P19-1470,0,0.0269359,"ing instance of constrained text generation that assesses 1) relational reasoning abilities using commonsense knowledge, and 2) compositional generalization capabilities to piece together concept combinations. Further, CommonGen’s task formulation and evaluation methodology are quite broadly applicable and encompassing, making it a good benchmark for general constrained text generation capability. Further, this is an opportune moment to investigate this task as commonsense ability of NLP models, particularly for generation, has received increasing community attention through works like COMET (Bosselut et al., 2019). We perform experiments on varying sizes of two 1 Code at https://github.com/styfeng/SAPPHIRE 212 Proceedings of the 14th International Conference on Natural Language Generation (INLG), pages 212–225, Aberdeen, Scotland, UK, 20-24 September 2021. ©2021 Association for Computational Linguistics Dataset Stats # concept sets size = 3 size = 4 size = 5 # sentences TrainCG 32,651 25,020 4,240 3,391 67,389 DevO 993 493 250 250 4,018 TestO 1,497 747 750 7,644 DevCG 240 120 60 60 984 ModelMetrics Reported BART-large Reported T5-base Reported T5-Large Our BART-base Our BART-large Our T5-base Our T5-l"
2021.inlg-1.21,2020.tacl-1.48,0,0.0171039,"-to-text NLG: A wide range of data-to-text NLG benchmarks have been proposed, e.g. for generating weather reports (Liang et al., 2009), game commentary (Jhamtani et al., 2018), and recipes (Kiddon et al., 2016). E2E-NLG (Duˇsek et al., 2018) and WebNLG (Gardent et al., 2017) are two benchmarks that involve generating text from meaning representation (MR) and triple sequences. Montella et al. (2020) use target Wiki sentences with parsed OpenIE triples as weak supervision for WebNLG. Tandon et al. (2018) permute input MRs to augment examples for E2E-NLG. Commonsense Reasoning and Incorporation: Talmor et al. (2020) show that not all pretrained LMs can reason through commonsense tasks. Other works investigate commonsense injection into models; one popular way is through knowledge graphs (KGs). One large commonsense KG is COMET, which trains on KG edges to learn connections between words and phrases. COSMIC (Ghosal et al., 2020) uses COMET to inject commonsense. EKI-BART (Fan et al., 2020) and KG-BART (Liu et al., 2021) show that external knowledge (from corpora and KGs) can improve performance on CommonGen. Distinctly, SAPPHIRE obviates reliance on external knowledge. 219 ModelsMetrics T5-base (reported"
2021.inlg-1.21,2021.findings-acl.269,0,0.0709334,"Missing"
2021.naacl-main.171,Q18-1031,0,0.179491,"t al., 2018; Wu et al., 2019), politeness (Madaan et al., 2020), formality (Rao and Tetreault, 2018; Liu et al., 2020; Krishna et al., 2020), writing styles (Jhamtani et al., 2017; Syed et al., 2020; Jin et al., 2020) and some other styles (Kang and Hovy, 2019). However, these only focus on only high-level styles, unlike S TYLE PTB. Computational models for style transfer span statistical NLP methods (Hovy, 1987; Xu et al., 2012), neural generative models (Prabhumoye et al., 2018; Lample et al., 2019; He et al., 2020), and Retrieve-and-Edit approaches (Li et al., 2018; Hashimoto et al., 2018; Guu et al., 2018; Sudhakar et al., 2019; Madaan et al., 2020). These approaches work for a predefined set of styles but are unable to generalize to compositions of styles. Evaluating style transfer is difficult due to the diversity of plausible transferred sentences. In addition to automatic scores such as BLEU, perplexity, or binary classification accuracy of style transfer (Hu et al., 2017; Lample et al., 2019; He et al., 2020), other automatic metrics (Fu et al., 2018; Mir et al., 2019) and human evaluation are also commonly used (Li et al., 2018; Shen et al., 2017). 3 Fine-Grained Style Constructs As a st"
2021.sigmorphon-1.22,P17-1183,0,0.014285,"stance algorithm (Mohri, 2009). For the neural models trained using conventional methods, decoding strategies that optimize for the output likelihood (e.g. beam search with a large beam size) have been shown to be susceptible to favoring empty outputs (Stahlberg and Byrne, 2019) and generating repetitions (Holtzman et al., 2020). Prior work on leveraging the strength of the two approaches proposes complex joint parameterizations, such as neural weighting of WFST arcs or paths (Rastogi et al., 2016; Lin et al., 2019) or encoding alignment constraints into the attention layer of seq2seq models (Aharoni and Goldberg, 2017; Wu et al., 2018; Wu and Cotterell, 2019; Makarov et al., 2017). We study whether performance can be improved with simpler decodingtime model combinations, reranking and product of experts, which have been used effectively for other model classes (Charniak and Johnson, 2005; Hieber and Riezler, 2015), evaluating on two unsupervised tasks: decipherment of informal roman2 Tasks Informal romanization Informal romanization is an idiosyncratic transformation that renders a non-Latin-script language in Latin alphabet, extensively used online by speakers of Arabic (Darwish, 2014), Russian (Paulsen,"
2021.sigmorphon-1.22,b-etal-2010-resource,0,0.0163283,"ll, 2019; Makarov et al., 2017). We study whether performance can be improved with simpler decodingtime model combinations, reranking and product of experts, which have been used effectively for other model classes (Charniak and Johnson, 2005; Hieber and Riezler, 2015), evaluating on two unsupervised tasks: decipherment of informal roman2 Tasks Informal romanization Informal romanization is an idiosyncratic transformation that renders a non-Latin-script language in Latin alphabet, extensively used online by speakers of Arabic (Darwish, 2014), Russian (Paulsen, 2014), and many Indic languages (Sowmya et al., 2010). Figure 1 shows examples of romanized Russian (top left) and Kannada (top right) sentences along with their “canonicalized” representations in Cyrillic and Kannada scripts respectively. Unlike official romanization systems such as pinyin, this type of transliteration is not standardized: character substitution choices vary between users and are based on the specific user’s perception of how similar characters in different scripts are. Although the substitutions are primarily phonetic (e.g. Russian n /n/ → n), i.e. based on the pronunciation of a specific character in or out of context, users"
2021.sigmorphon-1.22,W14-3612,0,0.0136932,"ied to all datasets are described in §3.4.2 3.1 Informal romanization Source: Filtered: de el menu:) de el menu<...> Target: Gloss: <...> éJÖ Ï @ ø X ‘This is the menu’ Figure 2: A parallel example from the LDC BOLT Arabizi dataset, written in Latin script (source) and converted to Arabic (target) semi-manually. Some source-side segments (in red) are removed by annotators; we use the version without such segments (filtered) for our task. The annotators also standardize spacing on the target side, which results in difference with the source (in blue). Arabic We use the LDC BOLT Phase 2 corpus (Bies et al., 2014; Song et al., 2014) for training and testing the Arabic transliteration models (Figure 2). The corpus consists of short SMS and chat in Egyptian Arabic represented using Latin script (Arabizi). The corpus is fully parallel: each message is automatically converted into the standardized dialectal Arabic orthography (CODA; Habash et al., 2012) and then manually corrected by human annotators. We split and preprocess the data according to Ryskina et al. (2020), discarding the target (native script) and source (romanized) parallel sentences to create the source and target monolingual training split"
2021.sigmorphon-1.22,P05-1022,0,0.0134739,"019) and generating repetitions (Holtzman et al., 2020). Prior work on leveraging the strength of the two approaches proposes complex joint parameterizations, such as neural weighting of WFST arcs or paths (Rastogi et al., 2016; Lin et al., 2019) or encoding alignment constraints into the attention layer of seq2seq models (Aharoni and Goldberg, 2017; Wu et al., 2018; Wu and Cotterell, 2019; Makarov et al., 2017). We study whether performance can be improved with simpler decodingtime model combinations, reranking and product of experts, which have been used effectively for other model classes (Charniak and Johnson, 2005; Hieber and Riezler, 2015), evaluating on two unsupervised tasks: decipherment of informal roman2 Tasks Informal romanization Informal romanization is an idiosyncratic transformation that renders a non-Latin-script language in Latin alphabet, extensively used online by speakers of Arabic (Darwish, 2014), Russian (Paulsen, 2014), and many Indic languages (Sowmya et al., 2010). Figure 1 shows examples of romanized Russian (top left) and Kannada (top right) sentences along with their “canonicalized” representations in Cyrillic and Kannada scripts respectively. Unlike official romanization system"
2021.sigmorphon-1.22,W14-3629,0,0.0194065,"models (Aharoni and Goldberg, 2017; Wu et al., 2018; Wu and Cotterell, 2019; Makarov et al., 2017). We study whether performance can be improved with simpler decodingtime model combinations, reranking and product of experts, which have been used effectively for other model classes (Charniak and Johnson, 2005; Hieber and Riezler, 2015), evaluating on two unsupervised tasks: decipherment of informal roman2 Tasks Informal romanization Informal romanization is an idiosyncratic transformation that renders a non-Latin-script language in Latin alphabet, extensively used online by speakers of Arabic (Darwish, 2014), Russian (Paulsen, 2014), and many Indic languages (Sowmya et al., 2010). Figure 1 shows examples of romanized Russian (top left) and Kannada (top right) sentences along with their “canonicalized” representations in Cyrillic and Kannada scripts respectively. Unlike official romanization systems such as pinyin, this type of transliteration is not standardized: character substitution choices vary between users and are based on the specific user’s perception of how similar characters in different scripts are. Although the substitutions are primarily phonetic (e.g. Russian n /n/ → n), i.e. based"
2021.sigmorphon-1.22,W16-2409,0,0.037721,"Missing"
2021.sigmorphon-1.22,habash-etal-2012-conventional,0,0.039387,"(in red) are removed by annotators; we use the version without such segments (filtered) for our task. The annotators also standardize spacing on the target side, which results in difference with the source (in blue). Arabic We use the LDC BOLT Phase 2 corpus (Bies et al., 2014; Song et al., 2014) for training and testing the Arabic transliteration models (Figure 2). The corpus consists of short SMS and chat in Egyptian Arabic represented using Latin script (Arabizi). The corpus is fully parallel: each message is automatically converted into the standardized dialectal Arabic orthography (CODA; Habash et al., 2012) and then manually corrected by human annotators. We split and preprocess the data according to Ryskina et al. (2020), discarding the target (native script) and source (romanized) parallel sentences to create the source and target monolingual training splits respectively. Russian We use the romanized Russian dataset collected by Ryskina et al. (2020), augmented with the monolingual Cyrillic data from the Taiga corpus of Shavrina and Shapovalova (2017) (Figure 3). The romanized data is split into training, validation, and test portions, and all validation and test sentences are converted to Cyr"
2021.sigmorphon-1.22,C14-1218,0,0.0266832,"ds and abugidas, where graphemes correspond to consonants or consonant-vowel syllables, increasingly use many-to-one alignment in their romanization (Figure 1, top right), which makes learning the latent alignments, and therefore decoding, more challenging. In this work, we experiment with three languages spanning over three major types of writing systems—Russian (alphabetic), Arabic (abjad), and Kannada (abugida)—and compare how well-suited character-level models are for learning these varying alignment patterns. 2.2 Related language translation As shown by Pourdamghani and Knight (2017) and Hauer et al. (2014), character-level models can be used effectively to translate between languages that are closely enough related to have only small lexical and grammatical differences, such as Serbian and Bosnian (Ljubeši´c and Klubiˇcka, 2014). We focus on this specific language pair and tie the languages to specific orthographies (Cyrillic for Serbian and Latin for Bosnian), approaching the task as an unsupervised orthography conversion problem. However, the transliteration framing of the translation problem is inherently limited since the task is not truly character-level in nature, as shown by the alignmen"
2021.sigmorphon-1.22,N15-1123,0,0.0120977,"ons (Holtzman et al., 2020). Prior work on leveraging the strength of the two approaches proposes complex joint parameterizations, such as neural weighting of WFST arcs or paths (Rastogi et al., 2016; Lin et al., 2019) or encoding alignment constraints into the attention layer of seq2seq models (Aharoni and Goldberg, 2017; Wu et al., 2018; Wu and Cotterell, 2019; Makarov et al., 2017). We study whether performance can be improved with simpler decodingtime model combinations, reranking and product of experts, which have been used effectively for other model classes (Charniak and Johnson, 2005; Hieber and Riezler, 2015), evaluating on two unsupervised tasks: decipherment of informal roman2 Tasks Informal romanization Informal romanization is an idiosyncratic transformation that renders a non-Latin-script language in Latin alphabet, extensively used online by speakers of Arabic (Darwish, 2014), Russian (Paulsen, 2014), and many Indic languages (Sowmya et al., 2010). Figure 1 shows examples of romanized Russian (top left) and Kannada (top right) sentences along with their “canonicalized” representations in Cyrillic and Kannada scripts respectively. Unlike official romanization systems such as pinyin, this type"
2021.sigmorphon-1.22,2021.eacl-demos.3,0,0.0327922,"Missing"
2021.sigmorphon-1.22,P06-2065,0,0.108451,"ntroduction and prior work Many natural language sequence transduction tasks, such as transliteration or grapheme-to-phoneme conversion, call for a character-level parameterization that reflects the linguistic knowledge of the underlying generative process. Character-level transduction approaches have even been shown to perform well for tasks that are not entirely characterlevel in nature, such as translating between related languages (Pourdamghani and Knight, 2017). Weighted finite-state transducers (WFSTs) have traditionally been used for such character-level tasks (Knight and Graehl, 1998; Knight et al., 2006). Their structured formalization makes it easier to encode additional constraints, imposed either 1 Code will be published at https://github.com/ ryskina/error-analysis-sigmorphon2021 Figure 1: Parallel examples from our test sets for two character-level transduction tasks: converting informally romanized text to its original script (top; examples in Russian and Kannada) and translating between closely related languages (bottom; Bosnian–Serbian). Informal romanization is idiosyncratic and relies on both visual (q → 4) and phonetic (t → t) character similarity, while translation is more standar"
2021.sigmorphon-1.22,P07-2045,0,0.0130082,"his observation. As can be seen from Figure 7, the seq2seq model is likely to either predict the word correctly (CER of 0) or entirely wrong (CER of 1), while the the WFST more often predicts the word partially correctly—examples in Table 4 illustrate this as well. We also see this in the Kannada outputs: WFST typically gets all the consonants right but makes mistakes in the vowels, while the seq2seq tends to replace the entire word. Figure 7: Character error rate per word for the WFST (left) and seq2seq (right) bos→srp translation outputs. The predictions are segmented using Moses tokenizer (Koehn et al., 2007) and aligned to ground truth with word-level edit distance. The increased frequency of CER=1 for the seq2seq model as compared to the WFST indicates that it replaces entire words more often. 6 Conclusion We perform comparative error analysis in finitestate and seq2seq models and their combinations for two unsupervised character-level tasks, informal romanization decipherment and related language translation. We find that the two model types tend towards different errors: seq2seq models are more prone to word-level errors caused by distributional shifts while WFSTs produce more characterlevel n"
2021.sigmorphon-1.22,song-etal-2014-collecting,0,0.0109605,"are described in §3.4.2 3.1 Informal romanization Source: Filtered: de el menu:) de el menu<...> Target: Gloss: <...> éJÖ Ï @ ø X ‘This is the menu’ Figure 2: A parallel example from the LDC BOLT Arabizi dataset, written in Latin script (source) and converted to Arabic (target) semi-manually. Some source-side segments (in red) are removed by annotators; we use the version without such segments (filtered) for our task. The annotators also standardize spacing on the target side, which results in difference with the source (in blue). Arabic We use the LDC BOLT Phase 2 corpus (Bies et al., 2014; Song et al., 2014) for training and testing the Arabic transliteration models (Figure 2). The corpus consists of short SMS and chat in Egyptian Arabic represented using Latin script (Arabizi). The corpus is fully parallel: each message is automatically converted into the standardized dialectal Arabic orthography (CODA; Habash et al., 2012) and then manually corrected by human annotators. We split and preprocess the data according to Ryskina et al. (2020), discarding the target (native script) and source (romanized) parallel sentences to create the source and target monolingual training splits respectively. Russ"
araki-etal-2014-detecting,N10-1138,0,\N,Missing
araki-etal-2014-detecting,D11-1116,1,\N,Missing
araki-etal-2014-detecting,C02-1165,0,\N,Missing
araki-etal-2014-detecting,W97-1311,0,\N,Missing
araki-etal-2014-detecting,D08-1031,0,\N,Missing
araki-etal-2014-detecting,W99-0201,0,\N,Missing
araki-etal-2014-detecting,W11-1902,0,\N,Missing
araki-etal-2014-detecting,P94-1019,0,\N,Missing
araki-etal-2014-detecting,P08-1090,0,\N,Missing
araki-etal-2014-detecting,D10-1033,0,\N,Missing
araki-etal-2014-detecting,P09-1068,0,\N,Missing
araki-etal-2014-detecting,P10-1143,0,\N,Missing
araki-etal-2014-detecting,P10-1100,0,\N,Missing
araki-etal-2014-detecting,W13-1203,1,\N,Missing
araki-etal-2014-detecting,mendes-etal-2012-dbpedia,0,\N,Missing
araki-etal-2014-detecting,P13-2083,1,\N,Missing
araki-etal-2014-detecting,D13-1178,0,\N,Missing
araki-etal-2014-detecting,W09-4303,0,\N,Missing
araki-etal-2014-detecting,W04-3205,0,\N,Missing
araki-etal-2014-detecting,bejan-harabagiu-2008-linguistic,0,\N,Missing
C00-1072,J90-1003,0,0.0318813,"l systems in a series of annual comparisons. This data set contains essential text fragments (phrases, clauses, and sentences) which must be included in summaries to answer some TREC topics. These fragments are each judged by a human judge. As described in Section 3, SUMMARIST employs several independent modules to assign a score to each sentence, and then combines the scores to decide which sentences to extract from the input text. One can gauge the eÆcacy 6 The mutual information is de ned according to chapter 2 of (Cover and Thomas, 1991) and is not the pairwise mutual information used in (Church and Hanks, 1990). TREC Topic Description hnumi Number: 151 htitlei Topic: Coping with overcrowded prisons hdesci Description: The document will provide information on jail and prison overcrowding and how inmates are forced to cope with those conditions; or it will reveal plans to relieve the overcrowded condition. narr Narrative: A relevant document will describe scenes of overcrowding that have become all too common in jails and prisons around the country. The document will identify how inmates are forced to cope with those overcrowded conditions, and/or what the Correctional System is doing, or planning to"
C00-1072,J93-1003,0,0.022173,"elated terms. Each ti is an term highly correlated to topic with association weight wi . The number of related terms n can be set empirically according to a cuto associated weight. We describe how to acquire related terms and their associated weights in the next section. 4.1 Signature Term Extraction and Weight Estimation On the assumption that semantically related terms tend to co-occur, one can construct topic signatures from preclassi ed text using the 2 test, mutual information, or other standard statistic tests and information-theoretic measures. Instead of 2 , we use likelihood ratio (Dunning, 1993) , since  is more appropriate for sparse data than 2 test and the quantity 2log is asymptotically 2 distributed5 . Therefore, we can determine the con dence level for a speci c 2log value by looking up 2 distribution table and use the value to select an appropriate cuto associated weight. We have documents preclassi ed into a set R of ~ of nonrelevant texts for a relevant texts and a set R given topic. Assuming the following two hypotheses: Hypothesis 1 (H1 ): P (Rjti ) = p = P (Rjt~i ), i.e. the relevancy of a document is independent of ti . Hypothesis 2 (H2 ): P (Rjti ) = p1 6= p2 = P"
C00-1072,J97-1003,0,0.609167,"ant (central) topics of the texts. SUMMARIST uses positional importance, topic signature, and term frequency. Importance based on discourse structure will be added later. This is the most developed stage in SUMMARIST. Topic Interpretation: To fuse concepts such as waiter, menu, and food into one generalized concept restaurant, we need more than the simple word aggregation used in traditional information retrieval. We have investigated concept 3 We would like to use only the relevant parts of documents to generate topic signatures in the future. Text segmentation algorithms such as TextTiling (Hearst, 1997) can be used to nd subtopic segments in text. ABCNEWS.com : Delay in Handing Flight 990 Probe to FBI NTSB Chairman James Hall says Egyptian officials want to review results of the investigation into the crash of EgyptAir Flight 990 before the case is turned over to the FBI. Nov. 16 - U.S. investigators appear to be leaning more than ever toward the possibility that one of the co-pilots of EgyptAir Flight 990 may have deliberately crashed the plane last month, killing all 217 people on board. However, U.S. officials say the National Transportation Safety Board will delay transferring the invest"
C00-1072,W97-0704,1,\N,Missing
C02-1042,A00-1041,0,0.0611023,"ord density within a fixed n-word window, to pinpoint answers. Robust though this may be, the window method is not accurate enough. In response, factoid question answering systems have evolved into two types: • Use-Knowledge: extract query words from the input question, perform IR against the source corpus, possibly segment resulting documents, identify a set of segments containing likely answers, apply a set of heuristics that each consults a different source of knowledge to score each candidate, rank them, and select the best (Harabagiu et al., 2001; Hovy et al., 2001; Srihari and Li, 2000; Abney et al., 2000). • Use-the-Web: extract query words from the question, perform IR against the web, extract likely answer-bearing sentences, canonicalize the results, and select the most frequent answer(s). Then, for justification, locate examples of the answers in the source corpus (Brill et al., 2001; Buchholz, 2001). Of course, these techniques can be combined: the popularity ratings from Use-the-Web can also be applied as a filtering criterion (Clarke et al., 2001), or the knowledge resource heuristics can filter the web results. However, simply going to the web without using further knowledge (Brill et a"
C02-1042,P97-1062,1,0.880343,"Missing"
C02-1042,W01-1203,1,0.907763,"Missing"
C02-1042,C02-1026,1,0.781267,"ry to explain what Washington is: Ex: “Later in the day, the president returned to Washington, the capital of the United States.” While WordNet’s definition Wordnet: “Washington—the capital of the United States” directly provides the answer to the matcher, it also allows the IR module to focus its search on passages containing “Washington”, “capital”, and “United States”, and the matcher to pick a good motivating passage in the source corpus. Clearly, this capability can be extended to include (definitional and other) information provided by other sources, including encyclopedias and the web (Lin 2002). 3.8 Type 8: Semantic Relation Matching So far, we have considered individual words and groups of words. But often this is insufficient to accurately score an answer. As also noted in (Buchholz, 2001), pinpointing can be improved significantly by matching semantic relations among constituents: Q: Who killed Lee Harvey Oswald? Qtargets: PROPER-PERSON & PROPER-NAME, PROPER-ORGANIZATION S1: “Belli’s clients have included Jack Ruby, who killed John F. Kennedy assassin Lee Harvey Oswald, and Jim and Tammy Bakker.” S2: “On Nov. 22, 1963, the building gained national notoriety when Lee Harvey Oswald"
C02-1042,P02-1006,1,0.686785,"bject something that in the ontology is subordinate to TEMP-QUANTIFIABLE-ABSTRACT with, as well, the word “how” paired with “warm”, “cold”, “hot”, etc., or the phrase “how many degrees” and a TEMPERATUREUNIT (as defined in the ontology). 3.3 Type 3: Surface Pattern Matching Often qtarget answers are expressed using rather stereotypical words or phrases. For example, the year of birth of a person is typically expressed using one of these phrases: <name> was born in <birthyear> <name> (<birthyear>–<deathyear>) We have developed a method to learn such patterns automatically from text on the web (Ravichandran and Hovy, 2002). We have added into the QA Typology the patterns for appropriate qtargets (qtargets with closed-list answers, such as PLANETS, require no patterns). Where some QA systems use such patterns exclusively (Soubbotin and Soubbotin, 2001) or partially (Wang et al., 2001; Lee et al., 2001), we employ them as an additional source of evidence for the answer. Preliminary results on for a range of qtargets, using the TREC-10 questions and the TREC corpus, are: Question type (qtarget) BIRTHYEAR INVENTORS DISCOVERERS DEFINITIONS WHY-FAMOUS LOCATIONS Number of questions 8 6 4 102 3 16 MRR on TREC docs 0.47"
C02-1042,A00-1023,0,0.103532,"technique, question word density within a fixed n-word window, to pinpoint answers. Robust though this may be, the window method is not accurate enough. In response, factoid question answering systems have evolved into two types: • Use-Knowledge: extract query words from the input question, perform IR against the source corpus, possibly segment resulting documents, identify a set of segments containing likely answers, apply a set of heuristics that each consults a different source of knowledge to score each candidate, rank them, and select the best (Harabagiu et al., 2001; Hovy et al., 2001; Srihari and Li, 2000; Abney et al., 2000). • Use-the-Web: extract query words from the question, perform IR against the web, extract likely answer-bearing sentences, canonicalize the results, and select the most frequent answer(s). Then, for justification, locate examples of the answers in the source corpus (Brill et al., 2001; Buchholz, 2001). Of course, these techniques can be combined: the popularity ratings from Use-the-Web can also be applied as a filtering criterion (Clarke et al., 2001), or the knowledge resource heuristics can filter the web results. However, simply going to the web without using further"
C02-1130,P00-1011,0,0.0150205,"ious work using learning algorithms to perform more fine-grained classification. Wacholder et al. (1997) use hand-written rules and knowledge bases to classify proper names into broad categories. They employ an aggregation method similar to MemRun, but do not use multiple thresholds to increase accuracy. MacDonald (1993) also uses hand-written rules for coarse named entity categorization. However, where Wacholder et al. use evidence internal to the entity name, MacDonald employs local context to aid in classification. Such hand-written heuristic rules resemble those we automatically generate. Bechet et al. (2000) use a decision tree algorithm to classify unknown proper names into the categories: first name, last name, country, town, and organization. This is still a much coarser distinction than that focused on in this research. Further, Bechet et al. focused only on those proper names embedded in complex noun phrases (NPs), using only elements in the NP as its feature set. 60.7 59.4 7. Conclusions 55 50 45 40 47.3 Baseline Freq WN Sig Freq & WN Freq & Sig Sig & WN All Figure 6. Results of using different combinations of feature sets. Results shown on validation set using C4.5 classifier without MemRu"
C02-1130,W99-0613,0,0.185426,"Missing"
C02-1130,J93-1003,0,0.0238058,"therefore created features that use topic signatures for each of the person subcategories. A topic signature, as described in (Lin and Hovy, 2000), is a list of terms that can be used to signal the membership of a text in the relevant topic or category. Each term in a text is given a topic signature score that indicates its ability to signal that the text is in a relevant category (the higher the score, the more that term is indicative of that category). The topic signatures are automatically generated for each specific term by computing the likelihood ratio (λ-score) between two hypotheses (Dunning, 1993). The first hypothesis (h1) is that the probability (p1) that the text is in the relevant category, given a specific term, is equivalent to the probability (p2) that the text is in the relevant category, given any other term (h1: p1=p2). The second hypothesis (h2) is that these two probabilities are not equivalent, and that p1 is much greater than p2 (h2: p1&gt;&gt;p2). The calculation of this likelihood ratio [-2logL(h1)/L(h2)] for each feature and for each category gives a list of all the terms in a document set with scores indicating how much the presence of that term in a specific document indic"
C02-1130,W93-0104,0,0.152236,"Missing"
C02-1130,A97-1030,0,0.0177685,"Missing"
C02-1130,J00-4006,0,\N,Missing
C02-1130,C00-1072,1,\N,Missing
C04-1111,P99-1008,0,0.188158,"Missing"
C04-1111,J95-4004,0,0.0678,"7.80 joke 5 7.37 V:subj:N joke 39 7.11 tape 10 7.09 poke 15 6.87 host 40 6.47 co-host 4 6.14 banter 3 6.00 interview 20 5.89 N:appo:N host 127 12.46 comedian 12 11.02 King 13 9.49 star 6 7.47 4 We propose an algorithm for learning highly scalable lexico-POS patterns. Given two sentences with their surface form and part of speech tags, the algorithm finds the optimal lexico-POS alignment. For example, consider the following 2 sentences: Figure 1. Excerpt of the grammatical signature for the television host class. 1) Platinum is a precious metal. 2) Molybdenum is a metal. Applying a POS tagger (Brill 1995) gives the following output: where n is the number of elements to be clustered, cef is the frequency count of word e in grammatical context f, and N is the total frequency count of all features of all words. 3.2 Surface POS Surface POS Phase II Platinum NNP is VBZ a DT Molybdenum NNP precious JJ is VBZ a DT metal NN . . metal NN . . A very good pattern to generalize from the alignment of these two strings would be Following (Pantel and Lin 2002), a committee for each semantic class is constructed. A committee is a set of representative elements that unambiguously describe the members of a poss"
C04-1111,P99-1016,0,0.103552,". These systems mostly employ clustering algorithms to group words according to their meanings in text. Assuming the distributional hypothesis (Harris 1985), words that occur in similar grammatical contexts are similar in meaning. Curran and Moens (2002) experimented with corpus size and complexity of proximity features in building automatic thesauri. CBC (Clustering by Committee) proposed by Pantel and Lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses. However, such clustering algorithms fail to name their classes. Caraballo (1999) was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency features for each noun. 3 Hypernym azalea bipolar disorder Bordeaux Flintstones salmon Riloff and Shepherd (1997) used a semiautomatic method for discovering similar words using a few seed examples by using pattern-based techniques and human supervision. Berland and Charniak (1999) used similar pattern-based techniques and other heuristics to extract meronymy (part"
C04-1111,P02-1030,0,0.0138495,", tangerine, ... (B) Phil Donahue, Pat Sajak, Arsenio Hall, Geraldo Rivera, Don Imus, Larry King, David Letterman, Conan O&apos;Brien, Rosie O&apos;Donnell, Jenny Jones, Sally Jessy Raphael, Oprah Winfrey, Jerry Springer, Howard Stern, Jay Leno, Johnny Carson, ... Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics (Hindle 1990, Lin 1998). These systems mostly employ clustering algorithms to group words according to their meanings in text. Assuming the distributional hypothesis (Harris 1985), words that occur in similar grammatical contexts are similar in meaning. Curran and Moens (2002) experimented with corpus size and complexity of proximity features in building automatic thesauri. CBC (Clustering by Committee) proposed by Pantel and Lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses. However, such clustering algorithms fail to name their classes. Caraballo (1999) was the first to use clustering for labeling is-a relations using conjunction and apposition features to build noun clusters. Recently, Pantel and Ravichandran (2004) extended this approach by making use of all syntactic dependency featu"
C04-1111,J93-1003,0,0.0269929,"X and Y and a reasonable way to delimit them would be to use POS information. All the patterns produced by the multi-level pattern learning algorithm were generated from positive examples. From amongst these patterns, we need to find the most important ones. This is a critical step because frequently occurring patterns have low precision whereas rarely occurring patterns have high precision. From the Information Extraction point of view neither of these patterns is very useful. We need to find patterns with relatively high occurrence and high precision. We apply the log likelihood principle (Dunning 1993) to compute this score. The top 15 patterns according to this metric are listed in Table 3 (we omit the POS variations for visibility). Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al. (2003). Y like X and _NN, X and other Y Y, including X, Y, such as X Y, especially X keeping track of the edit operations (which is the second part of the algorithm). Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m d"
C04-1111,P03-1001,1,0.884551,"l step because frequently occurring patterns have low precision whereas rarely occurring patterns have high precision. From the Information Extraction point of view neither of these patterns is very useful. We need to find patterns with relatively high occurrence and high precision. We apply the log likelihood principle (Dunning 1993) to compute this score. The top 15 patterns according to this metric are listed in Table 3 (we omit the POS variations for visibility). Some of these patterns are similar to the ones discovered by Hearst (1992) while other patterns are similar to the ones used by Fleischman et al. (2003). Y like X and _NN, X and other Y Y, including X, Y, such as X Y, especially X keeping track of the edit operations (which is the second part of the algorithm). Algorithm for calculating the minimal edit distance between two strings D[0,0]=0 for i = 1 to n do D[i,0] = D[i-1,0] + cost(insertion) for j = 1 to m do D[0,j] = D[0,j-1] + cost(deletion) for i = 1 to n do for j = 1 to m do D[i,j] = min( D[i-1,j-1] + cost(substitution), D[i-1,j] + cost(insertion), D[i,j-1] + cost(deletion)) Print (D[n,m]) Algorithm for optimal pattern retrieval 4.3 i = n, j = m; while i ≠ 0 and j ≠ 0 if D[i,j] = D[i-1,"
C04-1111,N03-1011,0,0.0528261,"Missing"
C04-1111,C92-2082,0,0.322965,"formance to a linguisticallyrich approach. The current state of the art cooccurrence model requires an estimated 10 years just to parse a 1TB corpus (see Table 1). Instead of using a syntactically motivated co-occurrence approach as above, our system uses lexico-syntactic rules. In particular, it finds lexico-POS patterns by making modifications to the basic edit distance algorithm. Once these patterns have been learnt, 2 Relevant Work Previous approaches to extracting is-a relations fall under two categories: pattern-based and cooccurrence-based approaches. 2.1 Pattern-based approaches Marti Hearst (1992) was the first to use a pattern-based approach to extract hyponym relations from a raw corpus. She used an iterative process to semi-automatically learn patterns. However, a corpus of 20MB words yielded only 400 examples. Our pattern-based algorithm is very similar to the one used by Hearst. She uses seed examples to manually discover her patterns whearas we use a minimal edit distance algorithm to automatically discover the patterns. 771 Table 1. Approximate processing time on a single Pentium-4 2.5 GHz machine. TOOL 15 GB ORPUS 1 TB CORPUS POS Tagger NP Chunker Dependency Parser Syntactic Pa"
C04-1111,P90-1034,0,0.14707,"two semantic classes discovered by CBC: (A) peach, pear, pineapple, apricot, mango, raspberry, lemon, cherry, strawberry, melon, blueberry, fig, apple, plum, nectarine, avocado, grapefruit, papaya, banana, cantaloupe, cranberry, blackberry, lime, orange, tangerine, ... (B) Phil Donahue, Pat Sajak, Arsenio Hall, Geraldo Rivera, Don Imus, Larry King, David Letterman, Conan O&apos;Brien, Rosie O&apos;Donnell, Jenny Jones, Sally Jessy Raphael, Oprah Winfrey, Jerry Springer, Howard Stern, Jay Leno, Johnny Carson, ... Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics (Hindle 1990, Lin 1998). These systems mostly employ clustering algorithms to group words according to their meanings in text. Assuming the distributional hypothesis (Harris 1985), words that occur in similar grammatical contexts are similar in meaning. Curran and Moens (2002) experimented with corpus size and complexity of proximity features in building automatic thesauri. CBC (Clustering by Committee) proposed by Pantel and Lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses. However, such clustering algorithms fail to name thei"
C04-1111,C94-1079,0,0.0470748,"ly set (by trial and error): cost(insertion) = 3 cost(deletion) = 3 cost(substitution) = 0 if a1i=b1j = 1 if a1i≠b1j, a2i=b2j = 2 if a1i≠b1j, a2i≠b2j 4.2 Time complexity 5.1 Implementation and filtering Experimental Setup We use a 15GB newspaper corpus consisting of TREC9, TREC 2002, Yahoo! News ~0.5GB, AP newswire ~2GB, New York Times ~2GB, Reuters ~0.8GB, Wall Street Journal ~1.2GB, and various online news website ~1.5GB. For our experiments, we extract from this corpus six data sets of different sizes: 1.5MB, 15 MB, 150 MB, 1.5GB, 6GB and 15GB. For the co-occurrence model, we used Minipar (Lin 1994), a broad coverage parser, to parse each data set. We collected the frequency counts of the grammatical relationships (contexts) output by Minipar and used them to compute the pointwise mutual information vectors described in Section 3.1. For the pattern-based approach, we use Brill’s POS tagger (1995) to tag each data set. The above algorithm takes O(y2) time for every pair of strings of length at most y. Hence, if there are x strings in the collection, each string having at most length y, the algorithm has time complexity O(x2y2) to extract all the patterns in the collection. Applying the ab"
C04-1111,P98-2127,0,0.0377977,"classes discovered by CBC: (A) peach, pear, pineapple, apricot, mango, raspberry, lemon, cherry, strawberry, melon, blueberry, fig, apple, plum, nectarine, avocado, grapefruit, papaya, banana, cantaloupe, cranberry, blackberry, lime, orange, tangerine, ... (B) Phil Donahue, Pat Sajak, Arsenio Hall, Geraldo Rivera, Don Imus, Larry King, David Letterman, Conan O&apos;Brien, Rosie O&apos;Donnell, Jenny Jones, Sally Jessy Raphael, Oprah Winfrey, Jerry Springer, Howard Stern, Jay Leno, Johnny Carson, ... Co-occurrence-based approaches The second class of algorithms uses cooccurrence statistics (Hindle 1990, Lin 1998). These systems mostly employ clustering algorithms to group words according to their meanings in text. Assuming the distributional hypothesis (Harris 1985), words that occur in similar grammatical contexts are similar in meaning. Curran and Moens (2002) experimented with corpus size and complexity of proximity features in building automatic thesauri. CBC (Clustering by Committee) proposed by Pantel and Lin (2002) achieves high recall and precision in generating similarity lists of words discriminated by their meaning and senses. However, such clustering algorithms fail to name their classes."
C04-1111,P02-1038,0,0.0292064,"this paper, we study the challenges of working at the terascale. We present an algorithm, designed for the terascale, for mining is-a relations that achieves similar performance to a state-of-the-art linguistically-rich method. We focus on the accuracy of these two systems as a function of processing time and corpus size. 1 Introduction The Natural Language Processing (NLP) community has recently seen a growth in corpus-based methods. Algorithms light in linguistic theories but rich in available training data have been successfully applied to several applications such as machine translation (Och and Ney 2002), information extraction (Etzioni et al. 2004), and question answering (Brill et al. 2001). In the last decade, we have seen an explosion in the amount of available digital text resources. It is estimated that the Internet contains hundreds of terabytes of text data, most of which is in an unstructured format. Yet, many NLP algorithms tap into only megabytes or gigabytes of this information. In this paper, we make a step towards acquiring semantic knowledge from terabytes of data. We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of th"
C04-1111,N04-1041,1,0.8846,"nd question answering (Brill et al. 2001). In the last decade, we have seen an explosion in the amount of available digital text resources. It is estimated that the Internet contains hundreds of terabytes of text data, most of which is in an unstructured format. Yet, many NLP algorithms tap into only megabytes or gigabytes of this information. In this paper, we make a step towards acquiring semantic knowledge from terabytes of data. We present an algorithm for extracting is-a relations, designed for the terascale, and compare it to a state of the art method that employs deep analysis of text (Pantel and Ravichandran 2004). We show that by simply utilizing more data on this task, we can achieve similar performance to a linguisticallyrich approach. The current state of the art cooccurrence model requires an estimated 10 years just to parse a 1TB corpus (see Table 1). Instead of using a syntactically motivated co-occurrence approach as above, our system uses lexico-syntactic rules. In particular, it finds lexico-POS patterns by making modifications to the basic edit distance algorithm. Once these patterns have been learnt, 2 Relevant Work Previous approaches to extracting is-a relations fall under two categories:"
C04-1111,W97-0313,0,0.018009,"Missing"
C04-1179,J96-1002,0,0.0068879,"py and second the use of sentence-wide features such as Syntactic patterns and previously identified frame element roles. It is not surprising that there is a dependency between each constituent’s role in a sentence and sentence level features reflecting this dependency improve the performance. In this paper, we extend our previous work (KFH) by adopting sentence level features even for frame element identification. 3 ME models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence, but otherwise is as uniform as possible (Berger et al. 1996). We model the probability of a class c given a vector of features x according to the ME formulation below: n Zx exp[ Σ λi f i (c, x)] i =0 Here Z x is normalization constant, f i (c, x) is a feature function which maps each class and vector element to a binary value, n is the total number of feature functions, and λi is a weight for the feature function. The final classification is just the class with the highest probability given its feature vector and the model. It is important to note that the feature functions described here are not equivalent to the subset conditional distributions that"
C04-1179,P97-1003,0,0.0166292,"Missing"
C04-1179,W03-1007,1,0.868676,"Missing"
C04-1179,J02-3001,0,0.176255,"Missing"
C04-1179,N03-2009,0,0.0301298,"Missing"
C04-1200,P97-1023,0,0.399713,"within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results. 1 Introduction What is an opinion? The many opinions on opinions are reflected in a considerable literature (Aristotle 1954; Perelman 1970; Toulmin et al. 1979; Wallace 1975; Toulmin 2003). Recent computational work either focuses on sentence ‘subjectivity’ (Wiebe et al. 2002; Riloff et al. 2003), concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al. 2002), or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives. We wish to study opinion in general; our work most closely resembles that of (Yu and Hatzivassiloglou 2003). Since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion. For our purposes, we describe an opinion as a quadruple [Topic, Holder, Claim, Sentiment] in which the Holder believes a Claim about the Topic, and in many cases associates a Sentiment, such as good or bad, with the belief. For example, the following opinions contain Claims but no Sentiments: “I believe"
C04-1200,W02-1011,0,0.0765265,"Missing"
C04-1200,W03-0404,0,0.501556,"he people who hold opinions about that topic and the sentiment of each opinion. The system contains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results. 1 Introduction What is an opinion? The many opinions on opinions are reflected in a considerable literature (Aristotle 1954; Perelman 1970; Toulmin et al. 1979; Wallace 1975; Toulmin 2003). Recent computational work either focuses on sentence ‘subjectivity’ (Wiebe et al. 2002; Riloff et al. 2003), concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al. 2002), or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives. We wish to study opinion in general; our work most closely resembles that of (Yu and Hatzivassiloglou 2003). Since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion. For our purposes, we describe an opinion as a quadruple [Topic, Holder, Claim, Sentiment] in which the Holder bel"
C04-1200,P02-1053,0,0.11988,"ains a module for determining word sentiment and another for combining sentiments within a sentence. We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results. 1 Introduction What is an opinion? The many opinions on opinions are reflected in a considerable literature (Aristotle 1954; Perelman 1970; Toulmin et al. 1979; Wallace 1975; Toulmin 2003). Recent computational work either focuses on sentence ‘subjectivity’ (Wiebe et al. 2002; Riloff et al. 2003), concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al. 2002), or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives. We wish to study opinion in general; our work most closely resembles that of (Yu and Hatzivassiloglou 2003). Since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion. For our purposes, we describe an opinion as a quadruple [Topic, Holder, Claim, Sentiment] in which the Holder believes a Claim about the Topic, and in many cases associates a Sentiment, such as good o"
C04-1200,W03-1017,0,0.386284,"romising results. 1 Introduction What is an opinion? The many opinions on opinions are reflected in a considerable literature (Aristotle 1954; Perelman 1970; Toulmin et al. 1979; Wallace 1975; Toulmin 2003). Recent computational work either focuses on sentence ‘subjectivity’ (Wiebe et al. 2002; Riloff et al. 2003), concentrates just on explicit statements of evaluation, such as of films (Turney 2002; Pang et al. 2002), or focuses on just one aspect of opinion, e.g., (Hatzivassiloglou and McKeown 1997) on adjectives. We wish to study opinion in general; our work most closely resembles that of (Yu and Hatzivassiloglou 2003). Since an analytic definition of opinion is probably impossible anyway, we will not summarize past discussion or try to define formally what is and what is not an opinion. For our purposes, we describe an opinion as a quadruple [Topic, Holder, Claim, Sentiment] in which the Holder believes a Claim about the Topic, and in many cases associates a Sentiment, such as good or bad, with the belief. For example, the following opinions contain Claims but no Sentiments: “I believe the world is flat” “The Gap is likely to go bankrupt” “Bin Laden is hiding in Pakistan” Eduard Hovy Information Sciences I"
C08-1133,S07-1074,0,0.0319516,"Missing"
C08-1133,W06-2911,0,0.0156954,"D system evaluates each instance in the second part. If the output of the WSD system disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2,"
C08-1133,D07-1108,0,0.0183533,"in the second part. If the output of the WSD system disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2, W-1, W0, W1, W2, W3), relative to the"
C08-1133,W02-0817,0,0.0470056,"Missing"
C08-1133,N06-2015,1,0.815676,"Missing"
C08-1133,S01-1004,0,0.0230452,"Missing"
C08-1133,W02-1006,0,0.0361716,"Missing"
C08-1133,W97-0207,0,0.205135,"Missing"
C08-1133,H93-1061,0,0.368997,"Missing"
C08-1133,P96-1006,0,0.206719,"Missing"
C08-1133,W04-2807,0,0.0754251,"Missing"
C08-1133,D07-1107,0,0.0332866,"Missing"
C08-1133,P07-1006,0,0.0170204,"disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2, W-1, W0, W1, W2, W3), relative to the target word W0. Similarly, the multi-word n-grams include"
C08-1133,S07-1057,0,0.0240467,"luates each instance in the second part. If the output of the WSD system disagrees with the two annotators, the instance is considered to be a suspicious candidate, otherwise it is considered to be clean and stored into the corpus. The set of suspicious candidates is collected and subsequently evaluated by the adjudicator to identify erroneous annotations. 3 ARG1 Word Sense Disambiguation This study takes a supervised learning approach to build a WSD system from the OntoNotes corpus. The feature set used herein is similar to several state-of-the-art WSD systems (Lee and Ng., 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre and Lopez de Lacalle, 2007; Specia et al., 2007), which is further integrated into a Naïve Bayes classifier (Lee and Ng., 2002; Mihalcea, 2007). In addition, a new feature, predicate-argument structure, provided by the OntoNotes corpus is also integrated. The feature set includes: Part-of-Speech (POS) tags: This feature includes the POS tags in the positions (P-3, P-2, P-1, P0, P1, P2, P3), relative to the POS tag of the target word. Local Collocations: This feature includes single words and multi-word n-grams. The single words include (W-3, W-2, W-1, W0, W1, W2, W3)"
C08-1133,D07-1082,1,0.928022,"or management. management.02: The people in charge. The ones actually doing the managing. Management wants to start downsizing. John was promoted to Management. I spoke to their management, and they&apos;re ready to make a deal. Table 2. Example sentence for the target word management along with its sense definitions. tags and definitions for the word arm (noun sense). The OntoNotes sense tags have been used for many applications, including the SemEval2007 evaluation (Pradhan et al., 2007b), sense merging (Snow et al., 2007), sense pool verification (Yu et al., 2007), and class imbalance problems (Zhu and Hovy, 2007). In creating OntoNotes, each word sense annotation involves two annotators and an adjudicator. First, all sentences containing the target word along with its sense distinctions are presented independently to two annotators for sense annotation. If the two annotators agree on the same sense for the target word in a given sentence, then their selection is stored in the corpus. Otherwise, this sentence is double-checked by the adjudicator for the final decision. The major problem of the above annotation scheme is that only the instances where the two annotators disagreed are double-checked, whil"
C08-1133,N07-1025,0,\N,Missing
C08-1133,S07-1016,0,\N,Missing
C08-1142,J96-1002,0,0.0592474,"Missing"
C08-1142,P94-1020,0,0.0386717,"Missing"
C08-1142,N06-1016,0,0.0716357,"Missing"
C08-1142,N06-2015,1,0.78772,"Missing"
C08-1142,W02-1006,0,0.0160402,"Missing"
C08-1142,P96-1006,0,0.072439,"Missing"
C08-1142,D07-1082,1,\N,Missing
C08-1142,P04-1075,0,\N,Missing
C08-1142,P00-1016,0,\N,Missing
C08-1142,P02-1016,0,\N,Missing
C08-1142,I08-1048,1,\N,Missing
C10-2052,J96-1002,0,0.0344118,"tion. We use a most-frequentsense baseline. In addition, we compare to the state-of-the-art systems for both types of granularity (O’Hara and Wiebe, 2009; Tratz and Hovy, 2009). Their results show what has been achieved so far in terms of accuracy, and serve as a second measure for comparison beyond the baseline. 16995 16000 frequency 4 18000 10332 10000 8000 5414 6000 4000 1781 2000 1071 280 44 EXT BNF 0 LOC TMP DIR MNR PRP classes Figure 1: Distribution of Class Labels in the WSJ Section of the Penn TreeBank. We use the MALLET implementation (McCallum, 2002) of a Maximum Entropy classifier (Berger et al., 1996) to construct our models. This classifier was also used by two state-of-the-art systems (Ye and Baldwin, 2007; Tratz and Hovy, 2009). For fine-grained PSD, we train a separate model for each preposition due to the high number of possible classes for each individual preposition. For coarse-grained PSD, we use a single model for all prepositions, because they all share the same classes. results of this task to the findings of O’Hara and Wiebe (2009). For the fine-grained task, we use data from the SemEval 2007 workshop (Litkowski and Hargraves, 2007), separate XML files for the 34 most frequent"
C10-2052,S07-1005,0,0.119838,"nk (PTB) (Marcus et al., 1993) and FrameNet (Baker et al., 1998) to classify prepositions. They show that using high level features, such as semantic roles, significantly aid disambiguation. They caution that using collocations and neighboring words indiscriminately may yield high accuracy, but has the risk of overfitting. O’Hara and Wiebe (2009) show comparisons of various semantic repositories as labels for PSD approaches. They also provide some results for PTB-based coarse-grained senses, using a fiveword window for lexical and hypernym features in a decision tree classifier. SemEval 2007 (Litkowski and Hargraves, 2007) included a task for fine-grained PSD (more than 290 senses). The best participating system, that of Ye and Baldwin (2007), extracted part-ofspeech and WordNet (Fellbaum, 1998) features using a word window of seven words in a Maximum Entropy classifier. Tratz and Hovy (2009) present a higher-performing system using a set of 20 positions that are syntactically related to the preposition instead of a fixed window size. Though using a variety of different extraction methods, contexts, and feature words, none of these approaches explores the optimal configurations for PSD. 3 Theoretical Background"
C10-2052,J93-2004,0,0.0421825,"Missing"
C10-2052,W03-0411,0,0.528019,"Missing"
C10-2052,J09-2002,0,0.266408,"Missing"
C10-2052,P07-1005,0,0.0138999,"fitting, and b) while possibly improving accuracy, it is not always clear where this improvement comes from and which features are actually informative. While parameter studies exist for general word sense disambiguation (WSD) tasks (Yarowsky and Florian, 2002), and PSD accuracy has been steadily increasing, there has been no exploration of the parameters of prepositions to guide engineering decisions. We go beyond simply improving accuracy to analyze various parameters in order to determine which ones are actually informative. We explore the different options for context and feature se1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454–462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity. Using the resulting parameters in a Maximum Entropy classifier, we are able to improve significantly over existing results. The general outline we present can potentially be extended to other word classes and improve WSD in general. 2 Related Work Rudzicz and Mokhov (2003) use syntactic and lexical features from the go"
C10-2052,N09-1025,0,0.0144601,"ar where this improvement comes from and which features are actually informative. While parameter studies exist for general word sense disambiguation (WSD) tasks (Yarowsky and Florian, 2002), and PSD accuracy has been steadily increasing, there has been no exploration of the parameters of prepositions to guide engineering decisions. We go beyond simply improving accuracy to analyze various parameters in order to determine which ones are actually informative. We explore the different options for context and feature se1 See (Chan et al., 2007) for the relevance of word sense disambiguation and (Chiang et al., 2009) for the role of prepositions in MT. 454 Coling 2010: Poster Volume, pages 454–462, Beijing, August 2010 lection, the influence of different preprocessing methods, and different levels of sense granularity. Using the resulting parameters in a Maximum Entropy classifier, we are able to improve significantly over existing results. The general outline we present can potentially be extended to other word classes and improve WSD in general. 2 Related Work Rudzicz and Mokhov (2003) use syntactic and lexical features from the governor and the preposition itself in coarse-grained PP classification wit"
C10-2052,P07-1096,0,0.0149883,"traction. Both O’Hara and Wiebe (2009) and Tratz and Hovy (2009) use constituency parsers to preprocess the data. However, parsing accuracy varies, 456 context and the problem of PP attachment ambiguity increases the likelihood of wrong extractions. This is especially troublesome in the present case, where we focus on prepositions.5 We use the MALT parser (Nivre et al., 2007), a state-of-theart dependency parser, to extract the governor and object. The alternative is a POS-based heuristics approach. The only preprocessing step needed is POS tagging of the data, for which we used the system of Shen et al. (2007). We then use simple heuristics to locate the prepositions and their related words. In order to determine the governor in the absence of constituent phrases, we consider the possible governing noun, verb, and adjective. The object of the preposition is extracted as first noun phrase head to the right. This approach is faster than parsing, but has problems with longrange dependencies and fronting of the PP (e.g., the PP appearing earlier in the sentence than its governor). Context coarse fine 2-word window 91.6 80.4 3-word window 92.0 81.4 4-word window 91.6 79.8 5-word window 91.0 78.7 Governo"
C10-2052,W10-0801,0,0.0123844,"nd Hargraves, 2005), making them difficult to disambiguate. Preposition sense disambiguation (PSD) has many potential uses. For example, due to the relational nature of prepositions, disambiguating their senses can help with all-word sense disambiguation. In machine translation, different senses of the same English preposition often correspond to different translations in the foreign language. Thus, disambiguating prepositions correctly may help improve translation quality.1 Coarse-grained PSD can also be valuable for information extraction, where the sense acts as a label. In a recent study, Hwang et al. (2010) identified preposition related features, among them the coarse-grained PP labels used here, as the most informative feature in identifying caused-motion constructions. Understanding the constraints that hold for prepositional constructions could help improve PP attachment in parsing, one of the most frequent sources of parse errors. Several papers have successfully addressed PSD with a variety of different approaches (Rudzicz and Mokhov, 2003; O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; O’Hara and Wiebe, 2009; Tratz and Hovy, 2009). However, while it is often possible to increase accuracy b"
C10-2052,N09-3017,1,0.56161,"traction, where the sense acts as a label. In a recent study, Hwang et al. (2010) identified preposition related features, among them the coarse-grained PP labels used here, as the most informative feature in identifying caused-motion constructions. Understanding the constraints that hold for prepositional constructions could help improve PP attachment in parsing, one of the most frequent sources of parse errors. Several papers have successfully addressed PSD with a variety of different approaches (Rudzicz and Mokhov, 2003; O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; O’Hara and Wiebe, 2009; Tratz and Hovy, 2009). However, while it is often possible to increase accuracy by using a different classifier and/or more features, adding more features creates two problems: a) it can lead to overfitting, and b) while possibly improving accuracy, it is not always clear where this improvement comes from and which features are actually informative. While parameter studies exist for general word sense disambiguation (WSD) tasks (Yarowsky and Florian, 2002), and PSD accuracy has been steadily increasing, there has been no exploration of the parameters of prepositions to guide engineering decisions. We go beyond sim"
C10-2052,S07-1051,0,0.654367,"ed PSD can also be valuable for information extraction, where the sense acts as a label. In a recent study, Hwang et al. (2010) identified preposition related features, among them the coarse-grained PP labels used here, as the most informative feature in identifying caused-motion constructions. Understanding the constraints that hold for prepositional constructions could help improve PP attachment in parsing, one of the most frequent sources of parse errors. Several papers have successfully addressed PSD with a variety of different approaches (Rudzicz and Mokhov, 2003; O’Hara and Wiebe, 2003; Ye and Baldwin, 2007; O’Hara and Wiebe, 2009; Tratz and Hovy, 2009). However, while it is often possible to increase accuracy by using a different classifier and/or more features, adding more features creates two problems: a) it can lead to overfitting, and b) while possibly improving accuracy, it is not always clear where this improvement comes from and which features are actually informative. While parameter studies exist for general word sense disambiguation (WSD) tasks (Yarowsky and Florian, 2002), and PSD accuracy has been steadily increasing, there has been no exploration of the parameters of prepositions t"
C10-2052,P98-1013,0,\N,Missing
C10-2052,C98-1013,0,\N,Missing
C10-2113,P03-1054,0,0.0143365,"Missing"
C10-2113,P06-2094,0,0.0346074,"suring that enough amounts of documents related to the domain will be processed with a complete NLP pipeline. Doing so provides cleaner and canonical representations (our propositions) and even higher counts than TextRunner for our domain. This level of processing will be scalable in the midterm; various people including (Huang and Sagae, 2010) are working in linear time parsers with state-of-the-art performance. Another intermediate point between a collection of domain documents and the general web, reached by restricting processing to the results of a web query, is explored in IE-on-demand (Sekine 2006; Shinyama and Sekine 2006). However, they use a predefined set of entity classes, preventing from discovering the appropriate granularity level that enables retrieval of relevant background knowledge. We do not predefine the concepts/classes and relations, but discover them from what it is explicitly said in the collection. The process of building the BKB described here is closely related to DART (Clark and Harrison, 2009) which in turn is related to KNEXT (Van Durme and Schubert, 2008). Perhaps the most important extension we performed is the inclusion of lexical relations (like “hasinstance"
C10-2113,N06-1039,0,0.0273276,"nough amounts of documents related to the domain will be processed with a complete NLP pipeline. Doing so provides cleaner and canonical representations (our propositions) and even higher counts than TextRunner for our domain. This level of processing will be scalable in the midterm; various people including (Huang and Sagae, 2010) are working in linear time parsers with state-of-the-art performance. Another intermediate point between a collection of domain documents and the general web, reached by restricting processing to the results of a web query, is explored in IE-on-demand (Sekine 2006; Shinyama and Sekine 2006). However, they use a predefined set of entity classes, preventing from discovering the appropriate granularity level that enables retrieval of relevant background knowledge. We do not predefine the concepts/classes and relations, but discover them from what it is explicitly said in the collection. The process of building the BKB described here is closely related to DART (Clark and Harrison, 2009) which in turn is related to KNEXT (Van Durme and Schubert, 2008). Perhaps the most important extension we performed is the inclusion of lexical relations (like “hasinstance”) that activate more power"
C10-2113,W08-2219,0,0.0268008,"Missing"
C10-2113,W08-1301,0,\N,Missing
C10-2113,P10-1110,0,\N,Missing
C14-1066,P98-1013,0,0.0472108,"s between multiple participants. While it is useful to consider relations that draw their origins from semantic roles such as Agent, Patient and Recipient, it remains unclear what this set of semantic roles should be. This problem is one that has long troubled linguists (Fillmore, 1967; Sowa, 1991), and has been previously noted by researchers in NLP as well (Màrquez et al., 2008). Proposed solutions range from a small set of generic Agent-like or Patient-like roles in Propbank (Kingsbury and Palmer, 2002) to an effectively open-ended set of highly specific and fine-grained roles in Framenet (Baker et al., 1998). In addition to the theoretic uncertainty of the set of semantic relations there is the very real problem of the lack of high-performance, robust semantic parsers to annotate corpora. These issues effectively render the use of pre-defined, linguistically ordained semantic relations intractable for use in SDSM. In this paper we propose a novel approach to structuring distributional semantic models with latent relations that are automatically discovered from corpora. This approach effectively solves the conceptual dilemma of selecting the most expressive set of semantic relations. To the best o"
C14-1066,J10-4006,0,0.430146,"me time being similar to a shoe along another facet for both being articles of clothing, while being dissimilar along yet another facet to a t-shirt because one is stitched from linen while the other is made from polyester. Structured distributional semantic models (SDSMs) aim to remedy this fault with DSMs by decomposing distributional signatures over discrete relation dimensions, or facets. This leads to a representation that characterizes the semantics of a word by a distributional tensor, rather than a vector. Previous attempts in the literature include the work of Padó and Lapata (2007), Baroni and Lenci (2010) and Goyal et al. (2013). However, all these approaches assume a simplified representation in which truly semantic relations are substituted by syntactic relations obtained from a dependency parser. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 698 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 698–708, Dublin, Ireland, August 23-29 2014. We believe that there ar"
C14-1066,S07-1025,0,0.034619,"n this topic are the ones by Collobert et al. (2011), Turian et al. (2010) and Socher et al. (2010). Other related work to note is the body of research concerned with semantic relation classification, which is one of our evaluation tasks. Research community wide efforts in the SemEval-2007 task 4 (Girju et al., 2007), the SemEval-2010 task 8 (Hendrickx et al., 2009) and the SemEval-2012 task 2 (Jurgens et al., 2012) are notable examples. However, different from our work, most previous attempts at semantic relation classification operate on the basis of feature engineering and contextual cues (Bethard and Martin, 2007). 3 Structured Distributional Semantics and Latent Semantic Relation Induction In this section we formalize the notion of SDSM as an extension of DSM and present a novel SDSM with latent relation dimensions. A DSM is a vector space V that contains |Σ |elements in Rn , where Σ = {w1 , w2 , ..., wk } is a vocabulary of k distinct words. Every vocabulary word wi has an associated semantic vector v~i representing its distributional signature. Each of the n elements of v~i is associated with a single dimension of its distribution. This dimension may correspond to another word — that may or may not"
C14-1066,J81-4005,0,0.77849,"Missing"
C14-1066,D08-1094,0,0.0302047,"lier work in this regard is a paper by Turney (2012), who proposes that the semantics of a word is not obtained along a single distributional axis but simultaneously in two different spaces. He proposes a DSM in which co-occurrence statistics are computed for neighbouring nouns and verbs separately to yield independent domain and function spaces of semantics. This intuition is taken further by a stance which proposes that a word’s semantics is distributionally decomposed over many independent spaces – each of which is a unique relation dimension. Authors who have endorsed this perspective are Erk and Padó (2008), Goyal et al. (2013), Reisinger and Mooney (2010) and Baroni and Lenci (2010). Our work relates to these papers in that we subscribe to the multiple space semantics view. However, we crucially differ from them by structuring our semantic space with information obtained from latent semantic relations rather than from a syntactic parser. In this paper the instantiation of the SDSM with latent relation dimensions is obtained using LRA (Turney, 2005), which is an extension of LSA (Deerwester et al., 1990) to induce relational embeddings for pairs of words. From a modelling perspective, SDSMs char"
C14-1066,P07-1028,0,0.0172796,"th (1957) in the popular dictum “You shall know the word by the company it keeps”, has established itself as one of the most popular models of modern computational semantics. With the rise of massive and easily-accessible digital corpora, computation of co-occurrence statistics has enabled researchers in NLP to build distributional semantic models (DSMs) that have found relevance in many application areas. These include information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), word-sense disambiguation (McCarthy et al., 2004) and selectional preference modelling (Erk, 2007), to name only a few. The standard DSM framework, which models the semantics of a word by co-occurrence statistics computed over its neighbouring words, has several known short-comings. One severe short-coming derives from the fundamental nature of the vector space model, which characterizes the semantics of a word by a single vector in a high dimensional space (or some lower dimensional embedding thereof). Such a modelling paradigm goes against the grain of the intuition that the semantics of a word is neither unique nor constant. Rather, it is composed of many facets of meaning, and similari"
C14-1066,S07-1003,0,0.0268935,"Sadrzadeh (2011) and the tensor-factorization model of Van de Cruys et al. (2013). A different, partially overlapping strain of research attempts to induce word embeddings using meth699 ods from deep learning, yielding state-of-the-art results on a number of different tasks. Notable research papers on this topic are the ones by Collobert et al. (2011), Turian et al. (2010) and Socher et al. (2010). Other related work to note is the body of research concerned with semantic relation classification, which is one of our evaluation tasks. Research community wide efforts in the SemEval-2007 task 4 (Girju et al., 2007), the SemEval-2010 task 8 (Hendrickx et al., 2009) and the SemEval-2012 task 2 (Jurgens et al., 2012) are notable examples. However, different from our work, most previous attempts at semantic relation classification operate on the basis of feature engineering and contextual cues (Bethard and Martin, 2007). 3 Structured Distributional Semantics and Latent Semantic Relation Induction In this section we formalize the notion of SDSM as an extension of DSM and present a novel SDSM with latent relation dimensions. A DSM is a vector space V that contains |Σ |elements in Rn , where Σ = {w1 , w2 , ..."
C14-1066,P13-2083,1,0.900767,"ther facet for both being articles of clothing, while being dissimilar along yet another facet to a t-shirt because one is stitched from linen while the other is made from polyester. Structured distributional semantic models (SDSMs) aim to remedy this fault with DSMs by decomposing distributional signatures over discrete relation dimensions, or facets. This leads to a representation that characterizes the semantics of a word by a distributional tensor, rather than a vector. Previous attempts in the literature include the work of Padó and Lapata (2007), Baroni and Lenci (2010) and Goyal et al. (2013). However, all these approaches assume a simplified representation in which truly semantic relations are substituted by syntactic relations obtained from a dependency parser. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 698 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 698–708, Dublin, Ireland, August 23-29 2014. We believe that there are limiting factors to th"
C14-1066,D11-1129,0,0.030161,"nstantiation of the SDSM with latent relation dimensions is obtained using LRA (Turney, 2005), which is an extension of LSA (Deerwester et al., 1990) to induce relational embeddings for pairs of words. From a modelling perspective, SDSMs characterize the semantics of a word by a distributional tensor. Other notable papers on tensor based semantics or semantics of compositional structures are the simple additive and multiplicative models of Mitchell and Lapata (2009), the matrix-vector neural network approach of Socher et al. (2012), the physics inspired quantum view of semantic composition of Grefenstette and Sadrzadeh (2011) and the tensor-factorization model of Van de Cruys et al. (2013). A different, partially overlapping strain of research attempts to induce word embeddings using meth699 ods from deep learning, yielding state-of-the-art results on a number of different tasks. Notable research papers on this topic are the ones by Collobert et al. (2011), Turian et al. (2010) and Socher et al. (2010). Other related work to note is the body of research concerned with semantic relation classification, which is one of our evaluation tasks. Research community wide efforts in the SemEval-2007 task 4 (Girju et al., 20"
C14-1066,W09-2415,0,0.0289966,"model of Van de Cruys et al. (2013). A different, partially overlapping strain of research attempts to induce word embeddings using meth699 ods from deep learning, yielding state-of-the-art results on a number of different tasks. Notable research papers on this topic are the ones by Collobert et al. (2011), Turian et al. (2010) and Socher et al. (2010). Other related work to note is the body of research concerned with semantic relation classification, which is one of our evaluation tasks. Research community wide efforts in the SemEval-2007 task 4 (Girju et al., 2007), the SemEval-2010 task 8 (Hendrickx et al., 2009) and the SemEval-2012 task 2 (Jurgens et al., 2012) are notable examples. However, different from our work, most previous attempts at semantic relation classification operate on the basis of feature engineering and contextual cues (Bethard and Martin, 2007). 3 Structured Distributional Semantics and Latent Semantic Relation Induction In this section we formalize the notion of SDSM as an extension of DSM and present a novel SDSM with latent relation dimensions. A DSM is a vector space V that contains |Σ |elements in Rn , where Σ = {w1 , w2 , ..., wk } is a vocabulary of k distinct words. Every"
C14-1066,S12-1047,0,0.115183,"tially overlapping strain of research attempts to induce word embeddings using meth699 ods from deep learning, yielding state-of-the-art results on a number of different tasks. Notable research papers on this topic are the ones by Collobert et al. (2011), Turian et al. (2010) and Socher et al. (2010). Other related work to note is the body of research concerned with semantic relation classification, which is one of our evaluation tasks. Research community wide efforts in the SemEval-2007 task 4 (Girju et al., 2007), the SemEval-2010 task 8 (Hendrickx et al., 2009) and the SemEval-2012 task 2 (Jurgens et al., 2012) are notable examples. However, different from our work, most previous attempts at semantic relation classification operate on the basis of feature engineering and contextual cues (Bethard and Martin, 2007). 3 Structured Distributional Semantics and Latent Semantic Relation Induction In this section we formalize the notion of SDSM as an extension of DSM and present a novel SDSM with latent relation dimensions. A DSM is a vector space V that contains |Σ |elements in Rn , where Σ = {w1 , w2 , ..., wk } is a vocabulary of k distinct words. Every vocabulary word wi has an associated semantic vecto"
C14-1066,kingsbury-palmer-2002-treebank,0,0.0960932,"ena – such as the description of buying and selling – in which implicit semantics are tacit from complex interactions between multiple participants. While it is useful to consider relations that draw their origins from semantic roles such as Agent, Patient and Recipient, it remains unclear what this set of semantic roles should be. This problem is one that has long troubled linguists (Fillmore, 1967; Sowa, 1991), and has been previously noted by researchers in NLP as well (Màrquez et al., 2008). Proposed solutions range from a small set of generic Agent-like or Patient-like roles in Propbank (Kingsbury and Palmer, 2002) to an effectively open-ended set of highly specific and fine-grained roles in Framenet (Baker et al., 1998). In addition to the theoretic uncertainty of the set of semantic relations there is the very real problem of the lack of high-performance, robust semantic parsers to annotate corpora. These issues effectively render the use of pre-defined, linguistically ordained semantic relations intractable for use in SDSM. In this paper we propose a novel approach to structuring distributional semantic models with latent relations that are automatically discovered from corpora. This approach effecti"
C14-1066,P98-2127,0,0.517393,"Missing"
C14-1066,J08-2001,0,0.0132404,"multiple syntactic realizations are manifested. Additionally, syntax falls utterly short in explaining more complex phenomena – such as the description of buying and selling – in which implicit semantics are tacit from complex interactions between multiple participants. While it is useful to consider relations that draw their origins from semantic roles such as Agent, Patient and Recipient, it remains unclear what this set of semantic roles should be. This problem is one that has long troubled linguists (Fillmore, 1967; Sowa, 1991), and has been previously noted by researchers in NLP as well (Màrquez et al., 2008). Proposed solutions range from a small set of generic Agent-like or Patient-like roles in Propbank (Kingsbury and Palmer, 2002) to an effectively open-ended set of highly specific and fine-grained roles in Framenet (Baker et al., 1998). In addition to the theoretic uncertainty of the set of semantic relations there is the very real problem of the lack of high-performance, robust semantic parsers to annotate corpora. These issues effectively render the use of pre-defined, linguistically ordained semantic relations intractable for use in SDSM. In this paper we propose a novel approach to struct"
C14-1066,P04-1036,0,0.0159481,"ntroduction The distributional hypothesis, articulated by Firth (1957) in the popular dictum “You shall know the word by the company it keeps”, has established itself as one of the most popular models of modern computational semantics. With the rise of massive and easily-accessible digital corpora, computation of co-occurrence statistics has enabled researchers in NLP to build distributional semantic models (DSMs) that have found relevance in many application areas. These include information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), word-sense disambiguation (McCarthy et al., 2004) and selectional preference modelling (Erk, 2007), to name only a few. The standard DSM framework, which models the semantics of a word by co-occurrence statistics computed over its neighbouring words, has several known short-comings. One severe short-coming derives from the fundamental nature of the vector space model, which characterizes the semantics of a word by a single vector in a high dimensional space (or some lower dimensional embedding thereof). Such a modelling paradigm goes against the grain of the intuition that the semantics of a word is neither unique nor constant. Rather, it is"
C14-1066,N13-1090,0,0.213299,"Relation”, “Cause-Purpose” and “Space-Time”. For example, an instance of a word pair that exemplifies the “Part-Whole” relationship is “engine:car”. Note that, as with previous experiments, word pairs are given without any context. 5.2 Results We compare LR-SDSM on the semantic relation classification task to several different models. These include the additive vector composition (AVC) and multiplicative vector composition methods (MVC) proposed by Mitchell and Lapata (2009); we present both DSM and SENNA based variants of these models. We also compare against the vector difference method of Mikolov et al. (2013) (SENNAMik) which sees semantic relations as a meaning preserving vector translation in an RNN embedded vector space. Finally, we note the performance of random classification as a baseline, for reference. We attempted to produce results of a syntactic SDSM on the task; however, the hard constraint imposed by syntactic adjacency meant that effectively all the word pairs in the dataset yielded zero feature vectors. To avoid overfitting on all 130 original dimensions in our optimal SDSM, and also to render results comparable, we reduce the number of latent relation dimensions of LR-SDSM to 50. W"
C14-1066,D09-1045,0,0.124712,"rom them by structuring our semantic space with information obtained from latent semantic relations rather than from a syntactic parser. In this paper the instantiation of the SDSM with latent relation dimensions is obtained using LRA (Turney, 2005), which is an extension of LSA (Deerwester et al., 1990) to induce relational embeddings for pairs of words. From a modelling perspective, SDSMs characterize the semantics of a word by a distributional tensor. Other notable papers on tensor based semantics or semantics of compositional structures are the simple additive and multiplicative models of Mitchell and Lapata (2009), the matrix-vector neural network approach of Socher et al. (2012), the physics inspired quantum view of semantic composition of Grefenstette and Sadrzadeh (2011) and the tensor-factorization model of Van de Cruys et al. (2013). A different, partially overlapping strain of research attempts to induce word embeddings using meth699 ods from deep learning, yielding state-of-the-art results on a number of different tasks. Notable research papers on this topic are the ones by Collobert et al. (2011), Turian et al. (2010) and Socher et al. (2010). Other related work to note is the body of research"
C14-1066,J09-2002,0,0.0638424,"Missing"
C14-1066,J07-2002,0,0.0157321,"coloured blue, at the same time being similar to a shoe along another facet for both being articles of clothing, while being dissimilar along yet another facet to a t-shirt because one is stitched from linen while the other is made from polyester. Structured distributional semantic models (SDSMs) aim to remedy this fault with DSMs by decomposing distributional signatures over discrete relation dimensions, or facets. This leads to a representation that characterizes the semantics of a word by a distributional tensor, rather than a vector. Previous attempts in the literature include the work of Padó and Lapata (2007), Baroni and Lenci (2010) and Goyal et al. (2013). However, all these approaches assume a simplified representation in which truly semantic relations are substituted by syntactic relations obtained from a dependency parser. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 698 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 698–708, Dublin, Ireland, August 23-29 2014."
C14-1066,N10-1013,0,0.0310077,"ney (2012), who proposes that the semantics of a word is not obtained along a single distributional axis but simultaneously in two different spaces. He proposes a DSM in which co-occurrence statistics are computed for neighbouring nouns and verbs separately to yield independent domain and function spaces of semantics. This intuition is taken further by a stance which proposes that a word’s semantics is distributionally decomposed over many independent spaces – each of which is a unique relation dimension. Authors who have endorsed this perspective are Erk and Padó (2008), Goyal et al. (2013), Reisinger and Mooney (2010) and Baroni and Lenci (2010). Our work relates to these papers in that we subscribe to the multiple space semantics view. However, we crucially differ from them by structuring our semantic space with information obtained from latent semantic relations rather than from a syntactic parser. In this paper the instantiation of the SDSM with latent relation dimensions is obtained using LRA (Turney, 2005), which is an extension of LSA (Deerwester et al., 1990) to induce relational embeddings for pairs of words. From a modelling perspective, SDSMs characterize the semantics of a word by a distribution"
C14-1066,D12-1110,0,0.0319948,"latent semantic relations rather than from a syntactic parser. In this paper the instantiation of the SDSM with latent relation dimensions is obtained using LRA (Turney, 2005), which is an extension of LSA (Deerwester et al., 1990) to induce relational embeddings for pairs of words. From a modelling perspective, SDSMs characterize the semantics of a word by a distributional tensor. Other notable papers on tensor based semantics or semantics of compositional structures are the simple additive and multiplicative models of Mitchell and Lapata (2009), the matrix-vector neural network approach of Socher et al. (2012), the physics inspired quantum view of semantic composition of Grefenstette and Sadrzadeh (2011) and the tensor-factorization model of Van de Cruys et al. (2013). A different, partially overlapping strain of research attempts to induce word embeddings using meth699 ods from deep learning, yielding state-of-the-art results on a number of different tasks. Notable research papers on this topic are the ones by Collobert et al. (2011), Turian et al. (2010) and Socher et al. (2010). Other related work to note is the body of research concerned with semantic relation classification, which is one of ou"
C14-1066,D11-1116,1,0.823989,"top-ranking score yielding the semantically closest candidate to the target. Systems are evaluated on the basis of their accuracy at discriminating the top-ranked candidate. 4.3 Results We compare our model (LR-SDSM) to several other distributional models in these experiments. These include a standard distributional vector space model (DSM) trained on the combined text of English Wikipedia and Gigaword with a window-size of 3 words to either side of a target, a syntax-based SDSM (Goyal et al., 2013; Baroni and Lenci, 2010) (synSDSM) trained on the same corpus parsed with a dependency parser (Tratz and Hovy, 2011) and the state-of-the-art neural network embeddings from Collobert et al. (2011) (SENNA). We also give the expected evaluation scores from a random baseline, for comparison. An important factor to consider when constructing an SDSM using LRA is the number of latent dimensions selected in the SVD projection. In Figure 1 we investigate the effects of selecting different number of latent relation dimensions on both semantic evaluation tasks, starting with 10 dimensions up to a maximum of 800 (which was the maximum that was computationally feasible), in increments of 10. We note that optimal resul"
C14-1066,P10-1040,0,0.0112734,"itional structures are the simple additive and multiplicative models of Mitchell and Lapata (2009), the matrix-vector neural network approach of Socher et al. (2012), the physics inspired quantum view of semantic composition of Grefenstette and Sadrzadeh (2011) and the tensor-factorization model of Van de Cruys et al. (2013). A different, partially overlapping strain of research attempts to induce word embeddings using meth699 ods from deep learning, yielding state-of-the-art results on a number of different tasks. Notable research papers on this topic are the ones by Collobert et al. (2011), Turian et al. (2010) and Socher et al. (2010). Other related work to note is the body of research concerned with semantic relation classification, which is one of our evaluation tasks. Research community wide efforts in the SemEval-2007 task 4 (Girju et al., 2007), the SemEval-2010 task 8 (Hendrickx et al., 2009) and the SemEval-2012 task 2 (Jurgens et al., 2012) are notable examples. However, different from our work, most previous attempts at semantic relation classification operate on the basis of feature engineering and contextual cues (Bethard and Martin, 2007). 3 Structured Distributional Semantics and Laten"
C14-1066,N13-1134,0,0.0414197,"Missing"
C14-1066,C98-1013,0,\N,Missing
C14-1066,C98-2122,0,\N,Missing
C14-1066,S10-1006,0,\N,Missing
C14-1123,S07-1002,0,0.062714,"on the distributional statistics of words which occur in the proximity of the target words. Hence, we first obtain the distributional statistics from a very large corpus to facilitate generalization and reliable estimation of different possible senses. Then we use these statistics in a novel manner to obtain a representation for the senses of the target word. In this paper, we discus the performance of induced senses on the Semeval 2010 WSD/WSI(Manandhar et al., 2010) task. 2 Related Work Much of the work on word sense induction has been quite recent following the Semeval tasks on WSI in 2007(Agirre and Soroa, 2007) and 2010, but the task was recognized much earlier and various semisupervised and unsupervised efforts were directed towards the problem.Yarowsky (1995) proposed a This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1302 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1302–1310, Dublin, Ireland, August 23-29 2014. semi-supervised approach, which required humans to spec"
C14-1123,E09-1013,0,0.0430387,"one sense per discourse for an ambiguous word. The unsupervised approaches mainly focus on clustering the instances of the target words in a corpus, using first-order vectors, second-order vectors (Purandare and Pedersen, 2004)(Sch¨utze, 1998) etc. Pantel and Lin (2002) used various syntactic and surface features for clustering the various occurences of a target word. Co-occurence graph-based approaches(V´eronis, 2004) have also been used, which represent the words co-occuring with the target words as nodes and then identify the highly dense subgraphs or ‘hubs’ within this co-occurence graph. Brody and Lapata (2009) and Lau et al. (2012) proposed bayesian WSI systems which cluster the instances by applying Latent Dirichlet Allocation (LDA)(Blei et al., 2003), Hierarchical Dirichlet Processes (HDP)(Teh et al., 2006) etc. wherein each occurence of a target word is represented as a ‘document’ and its surrounding context as the ‘observable content’. Choe and Charniak (2013) propose a ‘naive bayes’ model for WSI which assumes one sense per discourse and uses Expectation Maximization(EM) to estimate model parameters like the probability of generating an instance feature like a word in the context, given the se"
C14-1123,D13-1148,0,0.497228,"rget word. Co-occurence graph-based approaches(V´eronis, 2004) have also been used, which represent the words co-occuring with the target words as nodes and then identify the highly dense subgraphs or ‘hubs’ within this co-occurence graph. Brody and Lapata (2009) and Lau et al. (2012) proposed bayesian WSI systems which cluster the instances by applying Latent Dirichlet Allocation (LDA)(Blei et al., 2003), Hierarchical Dirichlet Processes (HDP)(Teh et al., 2006) etc. wherein each occurence of a target word is represented as a ‘document’ and its surrounding context as the ‘observable content’. Choe and Charniak (2013) propose a ‘naive bayes’ model for WSI which assumes one sense per discourse and uses Expectation Maximization(EM) to estimate model parameters like the probability of generating an instance feature like a word in the context, given the sense of the target word in a particular instance. Reisinger and Mooney (2010) and Huang et al. (2012) have proposed sense dependent multiple prototypes for a word instead of the conventional one vector representation per word and have shown that this sense differentiation improves semantic similarity measurements between words. 3 Basic Motivation: Co-occurence"
C14-1123,N06-2015,1,0.869643,"enses of words. WSI has potential to be extremely useful in downstream applications because, apart from the savings on annotation costs, it also mitigates several theoretical conflicts associated with supervised WSD tasks, which generally involve deciding on the granularity of senses. Ideally, a WSI algorithm would be able to adapt to different tasks requiring different sense granularities. WSI algorithms can also be used to model the evolution of the senses of a word with time and hence can be much easier to maintain than existing fixed sense inventories like WordNet(Miller, 1995), Ontonotes(Hovy et al., 2006) etc. Automatic sense identification systems also have the potential to generalize well to large amounts of diverse data and hence be useful in various difficult domain independent tasks such as machine translation and information retrieval. Several factors make the problem of word sense induction very challenging. Most importantly, it is not clear what should be the ‘true’ senses of a word. The semantic continuum makes it always possible to break a sense into finer grained subsenses. Thus, the problem is one of finding the optimal granularity for any given task. Even in a semi-supervised sett"
C14-1123,P12-1092,0,0.0562167,"by applying Latent Dirichlet Allocation (LDA)(Blei et al., 2003), Hierarchical Dirichlet Processes (HDP)(Teh et al., 2006) etc. wherein each occurence of a target word is represented as a ‘document’ and its surrounding context as the ‘observable content’. Choe and Charniak (2013) propose a ‘naive bayes’ model for WSI which assumes one sense per discourse and uses Expectation Maximization(EM) to estimate model parameters like the probability of generating an instance feature like a word in the context, given the sense of the target word in a particular instance. Reisinger and Mooney (2010) and Huang et al. (2012) have proposed sense dependent multiple prototypes for a word instead of the conventional one vector representation per word and have shown that this sense differentiation improves semantic similarity measurements between words. 3 Basic Motivation: Co-occurence graphs Conventionally, each word is represented as a co-occurence vector which may contain frequency, point wise mutual information or some lower dimensional representation of context and this representation conflates all the senses of a word. These vectors can be viewed as a graph where words are nodes which have an edge between them i"
C14-1123,E12-1060,0,0.296061,"an ambiguous word. The unsupervised approaches mainly focus on clustering the instances of the target words in a corpus, using first-order vectors, second-order vectors (Purandare and Pedersen, 2004)(Sch¨utze, 1998) etc. Pantel and Lin (2002) used various syntactic and surface features for clustering the various occurences of a target word. Co-occurence graph-based approaches(V´eronis, 2004) have also been used, which represent the words co-occuring with the target words as nodes and then identify the highly dense subgraphs or ‘hubs’ within this co-occurence graph. Brody and Lapata (2009) and Lau et al. (2012) proposed bayesian WSI systems which cluster the instances by applying Latent Dirichlet Allocation (LDA)(Blei et al., 2003), Hierarchical Dirichlet Processes (HDP)(Teh et al., 2006) etc. wherein each occurence of a target word is represented as a ‘document’ and its surrounding context as the ‘observable content’. Choe and Charniak (2013) propose a ‘naive bayes’ model for WSI which assumes one sense per discourse and uses Expectation Maximization(EM) to estimate model parameters like the probability of generating an instance feature like a word in the context, given the sense of the target word"
C14-1123,W09-2419,0,0.094787,".0 72.7 52.9 52.5 53.5 - Supervised F-score all nouns verbs 60.9 55.2 69.2 60.7 55.1 68.8 58.7 53.2 66.6 65.4 62.6 69.5 64.0 60.0 69.0 #cl 2.45 1.87 1.00 3.42 - Table 1: Performance on Paired F-score and supervised F-score. LDA and Spectral are the two methods proposed in this paper. Lau is the baseline in which LDA system of (Lau et al., 2012) is considered. It should be noted that in their paper, (Lau et al., 2012) did not report their performance on Paired F-score. The Semeval-2010 task provides us with 3 evaluation metrics: V-measure, Paired F-score and Supervised F-score. It was noticed (Manandhar and Klapaftis, 2009) that V-measure tends to favour systems that produce a higher number of clusters than the gold standard and hence is not a reliable estimate of the performance of WSI systems. But, we report our results on V-measure too as it gives useful insight about the nature of data and the WSI algorithms. It is important to note that all the measures treat Ontonotes sense annotations as the gold standard, which makes this task unfit for our evaluation purposes. As mentioned earlier, our argument is that several decisions related to the granularity of senses and definition of senses are a topic of dispute"
C14-1123,S10-1011,0,0.102174,"it is unknown which sense inventories are most suited as starting points in a sense bootstrapping procedure. Our unsupervised approach relies heavily on the distributional statistics of words which occur in the proximity of the target words. Hence, we first obtain the distributional statistics from a very large corpus to facilitate generalization and reliable estimation of different possible senses. Then we use these statistics in a novel manner to obtain a representation for the senses of the target word. In this paper, we discus the performance of induced senses on the Semeval 2010 WSD/WSI(Manandhar et al., 2010) task. 2 Related Work Much of the work on word sense induction has been quite recent following the Semeval tasks on WSI in 2007(Agirre and Soroa, 2007) and 2010, but the task was recognized much earlier and various semisupervised and unsupervised efforts were directed towards the problem.Yarowsky (1995) proposed a This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1302 Proceedings of COLING 2014, the 25th International Conference on Co"
C14-1123,W04-2406,0,0.0550372,"Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1302 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1302–1310, Dublin, Ireland, August 23-29 2014. semi-supervised approach, which required humans to specify seed words for every ambiguous word and assumed one sense per discourse for an ambiguous word. The unsupervised approaches mainly focus on clustering the instances of the target words in a corpus, using first-order vectors, second-order vectors (Purandare and Pedersen, 2004)(Sch¨utze, 1998) etc. Pantel and Lin (2002) used various syntactic and surface features for clustering the various occurences of a target word. Co-occurence graph-based approaches(V´eronis, 2004) have also been used, which represent the words co-occuring with the target words as nodes and then identify the highly dense subgraphs or ‘hubs’ within this co-occurence graph. Brody and Lapata (2009) and Lau et al. (2012) proposed bayesian WSI systems which cluster the instances by applying Latent Dirichlet Allocation (LDA)(Blei et al., 2003), Hierarchical Dirichlet Processes (HDP)(Teh et al., 2006)"
C14-1123,N10-1013,0,0.0567054,"ems which cluster the instances by applying Latent Dirichlet Allocation (LDA)(Blei et al., 2003), Hierarchical Dirichlet Processes (HDP)(Teh et al., 2006) etc. wherein each occurence of a target word is represented as a ‘document’ and its surrounding context as the ‘observable content’. Choe and Charniak (2013) propose a ‘naive bayes’ model for WSI which assumes one sense per discourse and uses Expectation Maximization(EM) to estimate model parameters like the probability of generating an instance feature like a word in the context, given the sense of the target word in a particular instance. Reisinger and Mooney (2010) and Huang et al. (2012) have proposed sense dependent multiple prototypes for a word instead of the conventional one vector representation per word and have shown that this sense differentiation improves semantic similarity measurements between words. 3 Basic Motivation: Co-occurence graphs Conventionally, each word is represented as a co-occurence vector which may contain frequency, point wise mutual information or some lower dimensional representation of context and this representation conflates all the senses of a word. These vectors can be viewed as a graph where words are nodes which hav"
C14-1123,J98-1004,0,0.479075,"Missing"
C14-1123,P95-1026,0,0.543005,"e corpus to facilitate generalization and reliable estimation of different possible senses. Then we use these statistics in a novel manner to obtain a representation for the senses of the target word. In this paper, we discus the performance of induced senses on the Semeval 2010 WSD/WSI(Manandhar et al., 2010) task. 2 Related Work Much of the work on word sense induction has been quite recent following the Semeval tasks on WSI in 2007(Agirre and Soroa, 2007) and 2010, but the task was recognized much earlier and various semisupervised and unsupervised efforts were directed towards the problem.Yarowsky (1995) proposed a This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1302 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1302–1310, Dublin, Ireland, August 23-29 2014. semi-supervised approach, which required humans to specify seed words for every ambiguous word and assumed one sense per discourse for an ambiguous word. The unsupervised approaches mainly focus on clustering"
C14-1134,J10-4006,0,0.0199705,"t predicate and the role fillers “prefer” some fillers for other roles. For example, given that the predicate is writes, the agent author prefers the patient book, while the agent programmer prefers the patient code. This idea is used by Elman (2009), and is very similar to the role-filler composition that we use for anomaly detection. Erk et al. (2010) also model selectional preferences using vector spaces. They measure the goodness of the fit of a noun with a verb in terms of the similarity between the vector of the noun and some “exemplar” nouns taken by the verb in the same argument role. Baroni and Lenci (2010) also measure selectional preference similarly, but instead of exemplar nouns, they calculate a prototype vector for that role based on the vectors of the most common nouns occurring in that role for the given verb. Lenci (2011) builds on this work and models the phenomenon that the expectations of the verb or its role-fillers change dynamically given other role fillers. 2.2 Recursive Neural Networks Recursive Neural Networks (RNN), first introduced by Goller and Kuchler (1996), are multilayer neural network models used for efficient processing of structured objects of arbitrary shape. These h"
C14-1134,J90-1003,0,0.116137,"Missing"
C14-1134,J81-4005,0,0.776533,"Missing"
C14-1134,J10-4007,0,0.0222159,"Missing"
C14-1134,N13-1132,1,0.885312,"Missing"
C14-1134,W11-0607,0,0.151565,"lman (2009), and is very similar to the role-filler composition that we use for anomaly detection. Erk et al. (2010) also model selectional preferences using vector spaces. They measure the goodness of the fit of a noun with a verb in terms of the similarity between the vector of the noun and some “exemplar” nouns taken by the verb in the same argument role. Baroni and Lenci (2010) also measure selectional preference similarly, but instead of exemplar nouns, they calculate a prototype vector for that role based on the vectors of the most common nouns occurring in that role for the given verb. Lenci (2011) builds on this work and models the phenomenon that the expectations of the verb or its role-fillers change dynamically given other role fillers. 2.2 Recursive Neural Networks Recursive Neural Networks (RNN), first introduced by Goller and Kuchler (1996), are multilayer neural network models used for efficient processing of structured objects of arbitrary shape. These have been successfully used for modeling semantics of sentences of arbitrary length by Socher et al. (2010), for sentiment analysis by Socher et al. (2013b), for syntactic parsing by Socher et al. (2013a) and for learning morphol"
C14-1134,P98-2127,0,0.0338432,"concepts the events contain. Usually more general concepts cause events to be more normal since they convey lesser information. For example, an American soldier shooting another American soldier may be considered unusual, while a soldier shooting another soldier may not be as unusual, and at the highest level of generalization, a person shooting another person is normal. This information of generality has to be incorporated into the event model. This can be achieved by integrating real world knowledge from knowledge bases like Wordnet (Miller, 1995) or from corpus statistics like the work by Lin (1998) into the event model. Bordes et al. (2011) learn continuous representations of entities and relations in knowledge bases. More recently, an alternative approach for doing the same was proposed by Chen et al. (2013). These representations can greatly help modeling events. Finally, the idea of modeling event composition can help processing event data in general and can be applied to other tasks like finding co-referent events. 6 Conclusion We introduced the problem of anomalous newswire event detection and illustrated its difficulty. Our approach is similar to the ones successfully used for mod"
C14-1134,W13-3512,0,0.0129605,"nomenon that the expectations of the verb or its role-fillers change dynamically given other role fillers. 2.2 Recursive Neural Networks Recursive Neural Networks (RNN), first introduced by Goller and Kuchler (1996), are multilayer neural network models used for efficient processing of structured objects of arbitrary shape. These have been successfully used for modeling semantics of sentences of arbitrary length by Socher et al. (2010), for sentiment analysis by Socher et al. (2013b), for syntactic parsing by Socher et al. (2013a) and for learning morphologically aware word representations by Luong et al. (2013). RNN are attractive because they can encode compositions of meaning guided by syntax or some other linguistic structure known a priori. Moreover, they provide flexibility in terms of learning composition weights based on supervised or unsupervised objectives. Consequently RNN learn feature representations depending on the task. Hence, this is a good choice for modeling event composition. In its simplest form, an RNN processes information backed by a Directed Acyclic Graph (DAG), where each node represents a neural network with the same parameters. The output produced at each intermediate step"
C14-1134,J05-1004,0,0.136275,"Missing"
C14-1134,P05-1044,0,0.052814,"of the inputs, y1 , y2 ∈ Rn×1 are the inputs, W ∈ Rn×2n is the composition weight matrix and b ∈ Rn×1 is the bias. Sg is a element wise sigmoid 1415 Figure 1: Example of a Recursive Neural Network backed by a binary tree Figure 2: Example of an event tree function. Apart from encoding the composition, RNN also produce a score of composition s = S|c where S ∈ Rn×1 is a scoring operator and s is a score that shows how good the composition is. (Collobert et al., 2011) take an unsupervised approach to training RNN for semantic composition based on the contrastive estimation technique proposed by (Smith and Eisner, 2005) and assuming that any word and its context is a positive example and a random word in the same context is a negative training example. (Socher et al., 2013b) among others use a supervised objective that is based on the label error at the topmost node in the RNN. The parameters of the simplest model are W , b and S. For representation learning, the inputs xi are also made parameters. Goller and Kuchler (1996) propose Backpropagation through structure (BPTS), that respects the underlying DAG structure during backpropagation of gradients. 3 Neural Event Model We define an event as the pair (V, A"
C14-1134,P13-1045,0,0.14307,"r detail in Section 4.4. Automatic detection of anomaly requires encoding complex information, which has to be composed from the semantics of the individual words in the sentence. A fundamental problem in doing so is the sparsity in semantic space due to the discrete representations of meaning of words. In this paper, we describe an attempt to model newswire events as a composition of the predicate with its semantic arguments. Our approach is based on the recent models used for semantic composition using recursive neural networks (RNN). It has been previously shown by Socher et al. (2010) and Socher et al. (2013b) among others that RNN can effectively deal with sparsity in semantic space by representing meaning at a higher level of abstraction than the surface forms of words, and thus being able to learn more general patterns. These models are very relevant to modeling event semantics because the sparsity problem ranges from polysemy and synonymy at the lexical semantic level to entity and event co-reference at the discourse level. 2 2.1 Background Selectional Preference and Thematic Fit Selectional preference, a notion introduced by Wilks (1973), refers to the phenomenon of the predicate and the fil"
C14-1134,D13-1170,0,0.0925441,"r detail in Section 4.4. Automatic detection of anomaly requires encoding complex information, which has to be composed from the semantics of the individual words in the sentence. A fundamental problem in doing so is the sparsity in semantic space due to the discrete representations of meaning of words. In this paper, we describe an attempt to model newswire events as a composition of the predicate with its semantic arguments. Our approach is based on the recent models used for semantic composition using recursive neural networks (RNN). It has been previously shown by Socher et al. (2010) and Socher et al. (2013b) among others that RNN can effectively deal with sparsity in semantic space by representing meaning at a higher level of abstraction than the surface forms of words, and thus being able to learn more general patterns. These models are very relevant to modeling event semantics because the sparsity problem ranges from polysemy and synonymy at the lexical semantic level to entity and event co-reference at the discourse level. 2 2.1 Background Selectional Preference and Thematic Fit Selectional preference, a notion introduced by Wilks (1973), refers to the phenomenon of the predicate and the fil"
C14-1134,C98-2122,0,\N,Missing
C18-1007,baumann-pierrehumbert-2014-using,0,0.0298959,"anual verification to create distantly supervised data. Another is to explore typical machine learning techniques, for example adversarial training of bilingual word representations. We find that in event-type detection task—the task to classify [parts of] documents into a fixed set of labels—they give about the same performance. We explore ways in which the two methods can be complementary and also see how to best utilize a limited budget for manual annotation to maximize performance gain. 1 Introduction For most languages of the world, few or no language processing tools or resources exist (Baumann and Pierrehumbert, 2014). This hinders efforts to apply certain language technologies enjoyed by languages like English, in which much current research is done. To perform natural language processing tasks in resource-poor languages, one way to overcome data scarcity is to tap on resources from another resource-rich language. Assuming that there are already good resources and tools to solve the same tasks in the more resource-rich language (henceforth, auxiliary language), the only remaining challenge is to transfer the learning process into the resource-poor language (henceforth, target language) and adapt it to the"
C18-1007,D16-1136,0,0.0214924,"ent state-of-the-art methods for CLTC work in our task, we implemented a convolutional neural classifier. We compare this against a simple keyword-based method. Figure 1: Architecture of the neural classifier with adversarial domain (language) adaptation by Ganin and Lempitsky (2015). Arrows show the flow of gradient. 3.1 Adversarial Convolutional Network The first step is to train a bilingual word embedding as a shared feature representation space between the two languages. We trained our bilingual word embedding for English and the incident language using the method proposed in XlingualEmb (Duong et al., 2016). This method is a cross-lingual extension from word2vec model (Mikolov et al., 2013b) to bilingual text using two large monolingual corpora and a bilingual dictionary. Based on the shared representation, we then used a convolutional neural network (CNN) (Kim, 2014) to perform the classification. There are two main advantages of choosing a deep neural classifier over a shallow one. First, CNN outperforms shallow models like SVM or Logistic Regression in various text classification benchmark datasets (Kim, 2014; Lai et al., 2015; Johnson and Zhang, 2015; Xu and Yang, 2017). Second, CNN takes de"
C18-1007,D16-1105,0,0.0139784,"ant information contained in documents (Marujo et al., 2015). Such methods have ¨ ur et al., 2005) and information retrieval (Marujo been used in different tasks like text categorization (Ozg¨ et al., 2013) to extract the required information. Keyword heuristics have also been used to overcome language and domain barriers using bilingual dictionaries (Szarvas, 2008; Tran et al., 2013). However, a weak bilingual dictionary could result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictionary to translate documents between languages (Bel et al., 2003; Shi et al., 2010; Mihalcea et al., 2007). However, in resource-poor languages, bilingual dictionaries may be small and sparse. Therefore, the performance of direct word translation will be u"
C18-1007,E14-1049,0,0.0237372,"view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art methods for CLTC work in our task, we implemented a convolutional neural classifier. We compare this against a simple keyword-based method. Figure 1: Architecture of the neural classifier with adversarial domain (language) adaptation by Ganin and Lempitsky (2015). Arrows show the flo"
C18-1007,D11-1086,0,0.0641385,"Missing"
C18-1007,W15-0805,0,0.0308877,"is organized as follows: we first describe the related work on cross-lingual NLP tasks in low-resource settings, specifically how the available resources are used. Based on previous work, we then apply our proposed training data augmentation methods and run experiments to show the effectiveness of our methods. We then analyze the results, and follow with a few suggestions on how to best utilize the available annotation effort for maximum gain. 2 Related Work Keyword-based Models A keyword-based heuristic model is a simple yet effective approach to extract specific information such as events (Keane et al., 2015), because keywords often indicate a strong presence of important information contained in documents (Marujo et al., 2015). Such methods have ¨ ur et al., 2005) and information retrieval (Marujo been used in different tasks like text categorization (Ozg¨ et al., 2013) to extract the required information. Keyword heuristics have also been used to overcome language and domain barriers using bilingual dictionaries (Szarvas, 2008; Tran et al., 2013). However, a weak bilingual dictionary could result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ"
C18-1007,D14-1181,0,0.00426904,"ky (2015). Arrows show the flow of gradient. 3.1 Adversarial Convolutional Network The first step is to train a bilingual word embedding as a shared feature representation space between the two languages. We trained our bilingual word embedding for English and the incident language using the method proposed in XlingualEmb (Duong et al., 2016). This method is a cross-lingual extension from word2vec model (Mikolov et al., 2013b) to bilingual text using two large monolingual corpora and a bilingual dictionary. Based on the shared representation, we then used a convolutional neural network (CNN) (Kim, 2014) to perform the classification. There are two main advantages of choosing a deep neural classifier over a shallow one. First, CNN outperforms shallow models like SVM or Logistic Regression in various text classification benchmark datasets (Kim, 2014; Lai et al., 2015; Johnson and Zhang, 2015; Xu and Yang, 2017). Second, CNN takes dense word vector representations as input, allowing one to incorporate the state-of-the-art bilingual word embedding methods into the pipeline. The CNN model takes a sequence of word embeddings as input and applies 1-D convolutional operation on the input to extract"
C18-1007,W11-3607,0,0.0329797,"nce of important information contained in documents (Marujo et al., 2015). Such methods have ¨ ur et al., 2005) and information retrieval (Marujo been used in different tasks like text categorization (Ozg¨ et al., 2013) to extract the required information. Keyword heuristics have also been used to overcome language and domain barriers using bilingual dictionaries (Szarvas, 2008; Tran et al., 2013). However, a weak bilingual dictionary could result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictionary to translate documents between languages (Bel et al., 2003; Shi et al., 2010; Mihalcea et al., 2007). However, in resource-poor languages, bilingual dictionaries may be small and sparse. Therefore, the performance of direct wo"
C18-1007,W15-1521,0,0.0236723,"Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art methods for CLTC work in our task, we implemented a convolutional neural classifier. We compare this against a simple keyword-based method. Figure 1: Architecture of the neural classifier with adversarial domain (language) adaptation by Ganin and Lempitsky (2015). Arrows show the flow of gradient. 3.1 Ad"
C18-1007,P15-2105,0,0.0575202,"Missing"
C18-1007,P07-1123,0,0.0286022,"rage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictionary to translate documents between languages (Bel et al., 2003; Shi et al., 2010; Mihalcea et al., 2007). However, in resource-poor languages, bilingual dictionaries may be small and sparse. Therefore, the performance of direct word translation will be unsatisfactory. Some researchers utilized the bilingual dictionary to translate the models instead (Xu et al., 2016; Littell et al., 2017). Another line of work focuses on the use of automatic machine translation as an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Gu"
C18-1007,D10-1025,0,0.0271954,"al., 2016; Littell et al., 2017). Another line of work focuses on the use of automatic machine translation as an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefo"
C18-1007,D10-1103,0,0.0202148,"result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictionary to translate documents between languages (Bel et al., 2003; Shi et al., 2010; Mihalcea et al., 2007). However, in resource-poor languages, bilingual dictionaries may be small and sparse. Therefore, the performance of direct word translation will be unsatisfactory. Some researchers utilized the bilingual dictionary to translate the models instead (Xu et al., 2016; Littell et al., 2017). Another line of work focuses on the use of automatic machine translation as an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix"
C18-1007,P08-1033,0,0.0128768,"maximum gain. 2 Related Work Keyword-based Models A keyword-based heuristic model is a simple yet effective approach to extract specific information such as events (Keane et al., 2015), because keywords often indicate a strong presence of important information contained in documents (Marujo et al., 2015). Such methods have ¨ ur et al., 2005) and information retrieval (Marujo been used in different tasks like text categorization (Ozg¨ et al., 2013) to extract the required information. Keyword heuristics have also been used to overcome language and domain barriers using bilingual dictionaries (Szarvas, 2008; Tran et al., 2013). However, a weak bilingual dictionary could result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictio"
C18-1007,I13-1088,0,0.0253113,"2 Related Work Keyword-based Models A keyword-based heuristic model is a simple yet effective approach to extract specific information such as events (Keane et al., 2015), because keywords often indicate a strong presence of important information contained in documents (Marujo et al., 2015). Such methods have ¨ ur et al., 2005) and information retrieval (Marujo been used in different tasks like text categorization (Ozg¨ et al., 2013) to extract the required information. Keyword heuristics have also been used to overcome language and domain barriers using bilingual dictionaries (Szarvas, 2008; Tran et al., 2013). However, a weak bilingual dictionary could result in low coverage with this method. Hence, to overcome the limiting bilingual dictionary people employ bootstrapping methods to improve the coverage (Knopp, 2011; Ebrahimi et al., 2016). Cross-Lingual Text Classification Cross-lingual event type detection is closely related to crosslingual text classification (CLTC), which aims to classify text in a target language using training data from an auxiliary language (Bel et al., 2003). To bridge the language gap, early approaches of CLTC relied on a comprehensive bilingual dictionary to translate do"
C18-1007,P09-1027,0,0.121107,"ents between languages (Bel et al., 2003; Shi et al., 2010; Mihalcea et al., 2007). However, in resource-poor languages, bilingual dictionaries may be small and sparse. Therefore, the performance of direct word translation will be unsatisfactory. Some researchers utilized the bilingual dictionary to translate the models instead (Xu et al., 2016; Littell et al., 2017). Another line of work focuses on the use of automatic machine translation as an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing doc"
C18-1007,P17-1130,1,0.832009,"view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art methods for CLTC work in our task, we implemented a convolutional neural classifier. We compare this again"
C18-1007,P15-1042,0,0.0204976,"chine translation as an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art meth"
C18-1007,D16-1024,0,0.0144787,"s an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art methods for CLTC work i"
C18-1007,P16-1133,0,0.0153345,"s an oracle. The various learning algorithms treated the translations as a second view of document and facilitate cross-lingual learning with co-training (Wan, 2009), majority learning (Amini et al., 2009), matrix completion (Xiao and Guo, 2013) and multi-view co-regularization (Guo and Xiao, 2012a). Instead of word-level or sentence-level translation, various other approaches seek some cross-lingual mapping between document representation (Littman et al., 1998; Vinokourov et al., 2002; Platt et al., 2010; Jagarlamudi et al., 2011; De Smet et al., 2011; Guo and Xiao, 2012b; Zhou et al., 2015; Zhou et al., 2016a; Zhou et al., 2016b; Chen et al., 2017) or label distribution (Xu and Yang, 2017). Bilingual Word Embedding The most recent method for sharing document representation between languages is bilingual word embedding (Mikolov et al., 2013a; Faruqui and Dyer, 2014; Luong et al., 2015). The goal is to learn a shared embedding space between words in two languages. With the shared embedding, we are able to project all documents into a shared space. The model trained in one language can, therefore, be used in the other language. 3 Models To see how well recent state-of-the-art methods for CLTC work i"
C18-1309,araki-etal-2014-detecting,1,0.88093,"u/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tasks (Mitamura et al., 2015). There is a small but growing amount of work on conducting event coreference on the TAC-KBP datasets (Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017). The TAC dataset uses a relaxed coreference definition comparing to other corpora, requiring two event mentions to intuitively refer t"
C18-1309,P98-1012,0,0.403292,"Missing"
C18-1309,P14-1005,0,0.0359091,"Missing"
C18-1309,P08-1090,0,0.272255,"016; Lu and Ng, 2017). The TAC dataset uses a relaxed coreference definition comparing to other corpora, requiring two event mentions to intuitively refer to the same real-world event despite differences of their participants. For event sequencing, there are few supervised methods on script-like relation classification due to the lack of data. To the best of our knowledge, the only work in this direction is by Araki et al. (2014). This work focuses on the other type of relations in the event sequencing task: Subevent relations. There is also a rich literature on unsupervised script induction (Chambers and Jurafsky, 2008; Cheung et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016; Ferraro and Durme, 2016) that extracts scripts as a type of common-sense knowledge from raw documents. The focus of this work is to make use of massive collections of text documents to mine event co-occurrence patterns. In contrast, our work focuses on parsing the detailed relations between event mentions in each document. Another line of work closely related to event sequencing is to detect other temporal relations between events. Recent computational approaches for temporal detection are mainly conducted on the TimeBank"
C18-1309,chambers-jurafsky-2010-database,0,0.0318085,"rder them. We design separate feature sets to capture these aspects: the Script Compatibility set considers whether mentions should belong to the same script; the Event Ordering set determines the relative ordering of the mentions. Our final features are the cross products of features from the following 3 sets. 1. Surface-Based Script Compatibility: these features capture whether two mentions are script compatible based on the surface information, including: • Mention headword pair. • Event type pair. • Whether two event mentions appear in the same cluster in Chambers’s event schema database (Chambers and Jurafsky, 2010). • Whether the two event mentions share arguments, and the semantic frame name of the shared argument (produced by the Semafor parser (Das and Smith, 2011)). 2. Discourse-Based Script Compatibility: these features capture whether two event mentions are related given the discourse context. • Dependency path between the two mentions. • Function words (words other than Noun, Verb, Adjective and Adverb) in between the two mentions. • The types of other event mentions between the two mentions. • The sentence distance of two event mentions. • Whether there are temporal expressions (AGM-TMP slot fro"
C18-1309,P11-1098,0,0.0299351,"tem beats a strong temporal-based, oracle-informed baseline. We discuss the challenges of studying these event relations. Title and Abstract in Chinese 基于图的事件共指消解以及依序关系解码算法 文本中提及的事件之间常存在着复杂的关系。在这篇文章中，我们主要研究两种关系： 事件间的共指关系以及依序关系。我们指出，常被应用在事件指代消歧上的传统的树结 构解码方式并不适用于解决事件依序问题。为了解决这个问题，我们提出了一种适用于 两种情况的新的图结构解码算法。这个算法让我们可以为这两个任务设计灵活的特征。 在实验上，我们的事件共指消解系统在TAC-KBP 2015的共指消解任务上达到了目前最佳 的水平。同时，我们的依序关系系统的结果超过了一个基于时间分析并有部分正确答案 的基线系统。最后我们分析了结果，并讨论了事件关系判别任务的一些挑战。 1 Introduction Events are important building blocks of documents. They play a key role in document understanding tasks, such as information extraction (Chambers and Jurafsky, 2011), news summarization (Vossen and Caselli, 2015), story understanding (Mostafazadeh et al., 2016). Conceptually, events correspond to state changes and normally include a location, a time interval, and several entities/participants. In a text, events are realized as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documen"
C18-1309,W09-3208,0,0.0288666,"n the corresponding task documents2 . 2 Related Work Many researchers have worked on event coreference tasks since Humphreys et al. (1997). Recent advances in event coreference have been promoted by the availability of annotated corpora. However, due to 1 2 Detailed definition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-K"
C18-1309,I13-1100,0,0.0146042,"hers have worked on event coreference tasks since Humphreys et al. (1997). Recent advances in event coreference have been promoted by the availability of annotated corpora. However, due to 1 2 Detailed definition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tasks (Mitamura et al., 2015). There is a small b"
C18-1309,N15-1116,0,0.013159,"event coreference tasks since Humphreys et al. (1997). Recent advances in event coreference have been promoted by the availability of annotated corpora. However, due to 1 2 Detailed definition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tasks (Mitamura et al., 2015). There is a small but growing amount of"
C18-1309,W09-4303,0,0.0316458,"task documents2 . 2 Related Work Many researchers have worked on event coreference tasks since Humphreys et al. (1997). Recent advances in event coreference have been promoted by the availability of annotated corpora. However, due to 1 2 Detailed definition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tas"
C18-1309,N13-1104,0,0.163821,"C dataset uses a relaxed coreference definition comparing to other corpora, requiring two event mentions to intuitively refer to the same real-world event despite differences of their participants. For event sequencing, there are few supervised methods on script-like relation classification due to the lack of data. To the best of our knowledge, the only work in this direction is by Araki et al. (2014). This work focuses on the other type of relations in the event sequencing task: Subevent relations. There is also a rich literature on unsupervised script induction (Chambers and Jurafsky, 2008; Cheung et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016; Ferraro and Durme, 2016) that extracts scripts as a type of common-sense knowledge from raw documents. The focus of this work is to make use of massive collections of text documents to mine event co-occurrence patterns. In contrast, our work focuses on parsing the detailed relations between event mentions in each document. Another line of work closely related to event sequencing is to detect other temporal relations between events. Recent computational approaches for temporal detection are mainly conducted on the TimeBank corpus (Pustejovsky"
C18-1309,W02-1001,0,0.0876307,"ntions to represent the full event graph by taking one single mention from each event. Following the “easiest” intuition, we select the single mention that will result in the highest score given the current feature weight w. Match Criteria: We consider two graphs to match when their inferred graphs are the same. The inferred graph is defined by taking the transitive closure of the graph and propagate the links through the coreference relations. For example, in Figure 1, the mention fired will be linked to two killed mentions after propagation. Feature Delta: In structural perceptron training (Collins, 2002), the weights are updated directly by the feature delta. For all the features f˜ of the gold standard graph z˜ and features fˆ of a decoded graph zˆ, the feature delta is simply: ∆ = f˜ − fˆ. However, a decoded graph may contain links that are not directly presented but inferable from the gold standard graph. For example, in Figure 2, the prediction graph has a link from M 5 to M 1 (the orange arc), which is absent but inferable from the gold standard tree. If we keep these links when computing ∆, the model does not converge well. We thus remove the features on the inferable links from fˆ when"
C18-1309,cybulska-vossen-2014-using,0,0.119546,"s/participants. In a text, events are realized as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types of relation: Event Hopper Coreference (EH) and Event Sequencing (ES). Event Coreference: There is a rich literature on the Event Coreference problem (Liu et al., 2014; Cybulska and Vossen, 2014; Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017; Araki, 2018). By analogy to entity coreference, the “same” conceptual event may be realized by multiple text spans (event mentions). The coreference problem aims at identifying these relations to recover events from the text spans. The Event Hopper Coreference task in the TAC-KBP evaluation campaign defines coreference links as follows (Mitamura et al., 2018): Two event mentions are considered coreferent if they refer to the conceptually same underlying event, even if their arguments are not strictly identical. For example, This work is li"
C18-1309,P11-1144,0,0.156628,"tic Headword token and lemma pair, and whether they are the same. The pair of event types, and whether they are the same. The pair of realis types and whether they are the same. POS pair of the two mentions and whether they are the same. Whether the 5-word windows of the two mentions matches exactly. Sentence distance between the two mentions. Frame name pair of the two mentions and whether they are the same. Whether a mention is the syntactic ancestor of another. Table 1: Coreference Features. Parsing is done using Stanford CoreNLP (Manning et al., 2014); frame names are produced by Semafor (Das and Smith, 2011). by 2. For example, in Figure 2 the prediction graph (bottom right) incorrectly links m4 to Root and misses a link to m3 , which cause a total loss of 3. In addition, to be consistent with the feature delta computation, we do not compute loss for predicted links that are inferable from the gold standard. 3.2 Features 3.2.1 Event Coreference Features For event coreference, we design a simple feature set to capture syntactic and semantic similarity of arcs. The main features are summarized in Table 1. In the TAC KBP 2015 coreference task setting, the event mentions are annotated with two attrib"
C18-1309,D12-1062,0,0.118648,"se knowledge from raw documents. The focus of this work is to make use of massive collections of text documents to mine event co-occurrence patterns. In contrast, our work focuses on parsing the detailed relations between event mentions in each document. Another line of work closely related to event sequencing is to detect other temporal relations between events. Recent computational approaches for temporal detection are mainly conducted on the TimeBank corpus (Pustejovsky et al., 2002). There have been several studies on building automatic temporal reasoning systems (Uzzaman and Allen, 2010; Do et al., 2012; Chambers et al., 2014). In comparison, the Event Sequencing task is motivated by the Script theory, which places more emphasis on common-sense knowledge about event chronology. 3 Model 3.1 Graph-Based Decoding Model In the Latent Antecedent Tree (LAT) model popularly used for entity coreference decoding (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014), each node represents an event mention and each arc a coreference relation, and new mentions are connected to some past mention considered most similar. Thus the LAT model represents the decoding structure as a tree. This can represent any"
C18-1309,Q14-1037,0,0.0258159,"on a single timeline; 2) temporal relations for events occurring at similar time points may be complicated. Script-based relations may alleviate the problem. For example, if a bombing kills some people, the temporal relation of the bombing and kill may be “inclusion” or “after”. This is considered an After relation in ES because bombing causes the killing. For structure prediction, decoding — recovering the complex structure from local decisions — is one of the core problems. The most successful decoding algorithm for coreference nowadays is mention ranking based (Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014; Lee et al., 2017). These models rank the antecedents (mentions that appear earlier in discourse) and recover the full coreference clusters from local decisions. However, unlike coreference relations, sequencing relations are directed. Coreference decoding algorithms cannot be directly applied to such relations (§3.1). To solve this problem, we propose a unified graph-based framework that tackles both event coreference and event sequencing. Our method achieves state-of-the-art results on the event coreference task (§4.4) and beats an informed baseline on the event sequencing task (§4.5). Fina"
C18-1309,W13-1203,1,0.88579,"finition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tasks (Mitamura et al., 2015). There is a small but growing amount of work on conducting event coreference on the TAC-KBP datasets (Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017). The TAC dataset uses a relaxed coreference definition comparing to"
C18-1309,W97-1311,0,0.382807,"coding algorithms cannot be directly applied to such relations (§3.1). To solve this problem, we propose a unified graph-based framework that tackles both event coreference and event sequencing. Our method achieves state-of-the-art results on the event coreference task (§4.4) and beats an informed baseline on the event sequencing task (§4.5). Finally, we analyze the results and discuss the difficult challenges for both tasks (§5). Detailed definitions of these tasks can be found in the corresponding task documents2 . 2 Related Work Many researchers have worked on event coreference tasks since Humphreys et al. (1997). Recent advances in event coreference have been promoted by the availability of annotated corpora. However, due to 1 2 Detailed definition of relations can be found in http://cairo.lti.cs.cmu.edu/kbp/2016/after/ http://cairo.lti.cs.cmu.edu/kbp/2017/event/documents 3646 the complex nature of events, approaches to event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the de"
C18-1309,D12-1045,0,0.0264432,"event coreference adopt quite different assumptions and definitions. Most of event coreference researches are conducted on the popular ACE corpus (Chen and Ji, 2009; Chen et al., 2009; Sangeetha and Arock, 2012; Chen and Ng, 2013; Chen and Ng, 2015). Unlike the TAC KBP setting, the definition of event coreference in the ACE corpus requires strict argument matching. Work on the Intelligence Community (IC) Corpus (Hovy et al., 2013; Cybulska and Vossen, 2012; Liu et al., 2014; Araki et al., 2014) considers event relations on a restricted domain (i.e., terrorist events). Works on the ECB corpus (Lee et al., 2012; Cybulska and Vossen, 2014) focuses on both within-document and cross-document coreference. Our work follows the line of work promoted by the TAC-KBP event nugget tasks (Mitamura et al., 2015). There is a small but growing amount of work on conducting event coreference on the TAC-KBP datasets (Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017). The TAC dataset uses a relaxed coreference definition comparing to other corpora, requiring two event mentions to intuitively refer to the same real-world event despite differences of their participants. For event sequencing, there are few supervised"
C18-1309,D17-1018,0,0.0145507,"temporal relations for events occurring at similar time points may be complicated. Script-based relations may alleviate the problem. For example, if a bombing kills some people, the temporal relation of the bombing and kill may be “inclusion” or “after”. This is considered an After relation in ES because bombing causes the killing. For structure prediction, decoding — recovering the complex structure from local decisions — is one of the core problems. The most successful decoding algorithm for coreference nowadays is mention ranking based (Bj¨orkelund and Kuhn, 2014; Durrett and Klein, 2014; Lee et al., 2017). These models rank the antecedents (mentions that appear earlier in discourse) and recover the full coreference clusters from local decisions. However, unlike coreference relations, sequencing relations are directed. Coreference decoding algorithms cannot be directly applied to such relations (§3.1). To solve this problem, we propose a unified graph-based framework that tackles both event coreference and event sequencing. Our method achieves state-of-the-art results on the event coreference task (§4.4) and beats an informed baseline on the event sequencing task (§4.5). Finally, we analyze the"
C18-1309,liu-etal-2014-supervised,1,0.910364,"nd several entities/participants. In a text, events are realized as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types of relation: Event Hopper Coreference (EH) and Event Sequencing (ES). Event Coreference: There is a rich literature on the Event Coreference problem (Liu et al., 2014; Cybulska and Vossen, 2014; Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017; Araki, 2018). By analogy to entity coreference, the “same” conceptual event may be realized by multiple text spans (event mentions). The coreference problem aims at identifying these relations to recover events from the text spans. The Event Hopper Coreference task in the TAC-KBP evaluation campaign defines coreference links as follows (Mitamura et al., 2018): Two event mentions are considered coreferent if they refer to the conceptually same underlying event, even if their arguments are not strictly identical. F"
C18-1309,P17-1009,0,0.0535241,"ormally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types of relation: Event Hopper Coreference (EH) and Event Sequencing (ES). Event Coreference: There is a rich literature on the Event Coreference problem (Liu et al., 2014; Cybulska and Vossen, 2014; Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017; Araki, 2018). By analogy to entity coreference, the “same” conceptual event may be realized by multiple text spans (event mentions). The coreference problem aims at identifying these relations to recover events from the text spans. The Event Hopper Coreference task in the TAC-KBP evaluation campaign defines coreference links as follows (Mitamura et al., 2018): Two event mentions are considered coreferent if they refer to the conceptually same underlying event, even if their arguments are not strictly identical. For example, This work is licensed under a Creative Commons Attribution 4.0 Inter"
C18-1309,H05-1004,0,0.195691,"Missing"
C18-1309,P14-5010,0,0.0044398,"Missing"
C18-1309,N16-1098,0,0.0151667,"these event relations. Title and Abstract in Chinese 基于图的事件共指消解以及依序关系解码算法 文本中提及的事件之间常存在着复杂的关系。在这篇文章中，我们主要研究两种关系： 事件间的共指关系以及依序关系。我们指出，常被应用在事件指代消歧上的传统的树结 构解码方式并不适用于解决事件依序问题。为了解决这个问题，我们提出了一种适用于 两种情况的新的图结构解码算法。这个算法让我们可以为这两个任务设计灵活的特征。 在实验上，我们的事件共指消解系统在TAC-KBP 2015的共指消解任务上达到了目前最佳 的水平。同时，我们的依序关系系统的结果超过了一个基于时间分析并有部分正确答案 的基线系统。最后我们分析了结果，并讨论了事件关系判别任务的一些挑战。 1 Introduction Events are important building blocks of documents. They play a key role in document understanding tasks, such as information extraction (Chambers and Jurafsky, 2011), news summarization (Vossen and Caselli, 2015), story understanding (Mostafazadeh et al., 2016). Conceptually, events correspond to state changes and normally include a location, a time interval, and several entities/participants. In a text, events are realized as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types of relation: Event Hopper Coreference (EH) and E"
C18-1309,P02-1014,0,0.0497368,"on as the original graph. In the example above, structure 1 is a transitive reduction graph for structure 2. We call the decoding structures that corresponding to the reduction graphs as minimum decoding structures. For LAG, we further restrict Z(A) to contain only minimum decoding structures. 3.1.2 Training Details in Latent Antecedent Graph In this section, we describe the decoding details for LAG. Note that if we enforce a single antecedent for each node (as in our coreference model), it falls back to the LAT model (Bj¨orkelund and Kuhn, 2014). Decoding: We use a greedy best-first decoder (Ng and Cardie, 2002), which makes a left-to-right pass over the mentions. The decoding step is the same for line 6 and 8. The only difference is that we will ˜ at line 8. For each node mj , we keep all links that score higher than the root use gold antecedent set (A) link h0, mj , ri. Cycle and Structure Check: Incremental decoding a DAG may introduce cycles to the graph, or violate the minimum decoding structure criterion. To solve this, we maintain a set R(mi ) that is reachable from mi during the decoding process. We reject a new link (hmj , mi i if mj ∈ R(mi )) to avoid cycles. We also reject a redundant link"
C18-1309,D16-1038,0,0.0581449,"ed as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types of relation: Event Hopper Coreference (EH) and Event Sequencing (ES). Event Coreference: There is a rich literature on the Event Coreference problem (Liu et al., 2014; Cybulska and Vossen, 2014; Lu et al., 2016; Peng et al., 2016; Lu and Ng, 2017; Araki, 2018). By analogy to entity coreference, the “same” conceptual event may be realized by multiple text spans (event mentions). The coreference problem aims at identifying these relations to recover events from the text spans. The Event Hopper Coreference task in the TAC-KBP evaluation campaign defines coreference links as follows (Mitamura et al., 2018): Two event mentions are considered coreferent if they refer to the conceptually same underlying event, even if their arguments are not strictly identical. For example, This work is licensed under a Creative Commons Attr"
C18-1309,P16-1027,0,0.116574,"ion comparing to other corpora, requiring two event mentions to intuitively refer to the same real-world event despite differences of their participants. For event sequencing, there are few supervised methods on script-like relation classification due to the lack of data. To the best of our knowledge, the only work in this direction is by Araki et al. (2014). This work focuses on the other type of relations in the event sequencing task: Subevent relations. There is also a rich literature on unsupervised script induction (Chambers and Jurafsky, 2008; Cheung et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016; Ferraro and Durme, 2016) that extracts scripts as a type of common-sense knowledge from raw documents. The focus of this work is to make use of massive collections of text documents to mine event co-occurrence patterns. In contrast, our work focuses on parsing the detailed relations between event mentions in each document. Another line of work closely related to event sequencing is to detect other temporal relations between events. Recent computational approaches for temporal detection are mainly conducted on the TimeBank corpus (Pustejovsky et al., 2002). There have been several studies on"
C18-1309,D15-1195,0,0.0507238,"Missing"
C18-1309,D11-1116,1,0.787984,"vent mentions share arguments, and the semantic frame name of the shared argument (produced by the Semafor parser (Das and Smith, 2011)). 2. Discourse-Based Script Compatibility: these features capture whether two event mentions are related given the discourse context. • Dependency path between the two mentions. • Function words (words other than Noun, Verb, Adjective and Adverb) in between the two mentions. • The types of other event mentions between the two mentions. • The sentence distance of two event mentions. • Whether there are temporal expressions (AGM-TMP slot from a semantic parser (Tratz and Hovy, 2011)) in the sentences of the two mentions. 3. Event Ordering: this feature set tries to capture the ordering of events. We use the discourse ordering of two mentions (forward: the antecedent is the parent; backward: the antecedent is the child), and temporal ordering produced by Caevo (Chambers et al., 2014). 3650 Taking the after arc from fired to killed in Figure 1 as an example, a feature after the cross product is: Event type pair is Conflict.Attack and Life.Die, discourse ordering is backward, and sentence distance is 0. 4 Experiments 4.1 Dataset We conduct experiments on the dataset release"
C18-1309,S10-1062,0,0.0378593,"s as a type of common-sense knowledge from raw documents. The focus of this work is to make use of massive collections of text documents to mine event co-occurrence patterns. In contrast, our work focuses on parsing the detailed relations between event mentions in each document. Another line of work closely related to event sequencing is to detect other temporal relations between events. Recent computational approaches for temporal detection are mainly conducted on the TimeBank corpus (Pustejovsky et al., 2002). There have been several studies on building automatic temporal reasoning systems (Uzzaman and Allen, 2010; Do et al., 2012; Chambers et al., 2014). In comparison, the Event Sequencing task is motivated by the Script theory, which places more emphasis on common-sense knowledge about event chronology. 3 Model 3.1 Graph-Based Decoding Model In the Latent Antecedent Tree (LAT) model popularly used for entity coreference decoding (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014), each node represents an event mention and each arc a coreference relation, and new mentions are connected to some past mention considered most similar. Thus the LAT model represents the decoding structure as a tree. This c"
C18-1309,S13-2001,0,0.219916,"Missing"
C18-1309,W15-4507,0,0.0234149,"baseline. We discuss the challenges of studying these event relations. Title and Abstract in Chinese 基于图的事件共指消解以及依序关系解码算法 文本中提及的事件之间常存在着复杂的关系。在这篇文章中，我们主要研究两种关系： 事件间的共指关系以及依序关系。我们指出，常被应用在事件指代消歧上的传统的树结 构解码方式并不适用于解决事件依序问题。为了解决这个问题，我们提出了一种适用于 两种情况的新的图结构解码算法。这个算法让我们可以为这两个任务设计灵活的特征。 在实验上，我们的事件共指消解系统在TAC-KBP 2015的共指消解任务上达到了目前最佳 的水平。同时，我们的依序关系系统的结果超过了一个基于时间分析并有部分正确答案 的基线系统。最后我们分析了结果，并讨论了事件关系判别任务的一些挑战。 1 Introduction Events are important building blocks of documents. They play a key role in document understanding tasks, such as information extraction (Chambers and Jurafsky, 2011), news summarization (Vossen and Caselli, 2015), story understanding (Mostafazadeh et al., 2016). Conceptually, events correspond to state changes and normally include a location, a time interval, and several entities/participants. In a text, events are realized as text spans, normally as verbs and nouns that indicate state changes (Vendler, 1957). The text spans are often referred to as event mentions or event nuggets (We use the term event mention in this paper.). The textual mentions of events have rich relations among them, and collectively convey the meaning of one or more related documents. In this paper, we study two different types"
C18-1309,C98-1012,0,\N,Missing
C18-1309,Q14-1022,0,\N,Missing
C18-1309,N15-1082,0,\N,Missing
C18-1309,M93-1007,0,\N,Missing
D07-1017,P01-1008,0,0.203747,"Missing"
D07-1017,P99-1071,0,0.0691133,"Missing"
D07-1017,W04-3205,1,0.29143,"approaches produce collections of inference rules that have good recall, they suffer from the complementary problem of low precision. They also make no attempt to distinguish between symmetric and asymmetric inference rules. Given the potential positive impact shown in Section 2.1 of learning the directionality of inference rules, there is a need for methods, such as the one we present, to improve existing automatically created resources. 2.3 Learning Directionality There have been a few approaches at learning the directionality of restricted sets of semantic relations, mostly between verbs. Chklovski and Pantel (2004) used lexico-syntactic patterns over the Web to detect certain types of symmetric and asymmetric relations between verbs. They manually examined and obtained lexico-syntactic patterns that help identify the types of relations they considered and used these lexico-syntactic patterns over the Web to detect these relations among a set of candidate verb pairs. Their approach however is limited only to verbs and to specific types of verb-verb relations. Zanzotto et al. (2006) explored a selectional preference-based approach to learn asymmetric inference rules between verbs. They used the selectiona"
D07-1017,P03-1003,0,0.0173803,"Missing"
D07-1017,P06-1114,0,0.170016,"s distributional hypothesis, we use selectional preferences to gather evidence of inference directionality and plausibility. Experiments show empirical evidence that our approach can classify inference rules significantly better than several baselines. 1 Introduction Paraphrases are textual expressions that convey the same meaning using different surface forms. Textual entailment is a similar phenomenon, in which the presence of one expression licenses the validity of another. Paraphrases and inference rules are known to improve performance in various NLP applications like Question Answering (Harabagiu and Hickl 2006), summarization (Barzilay et al. 1999) and Information Retrieval (Anick and Tipirneni 1999). Paraphrase and entailment involve inference rules that license a conclusion when a premise is given. Deciding whether a proposed inference rule is fully valid is difficult, however, and most NL systems instead focus on plausible inference. In this case, one statement has some likelihood of All rules in DIRT are considered symmetric. Though here, one is most likely to infer that “X eats Y” ⇒ “X likes Y”, because if someone eats something, he most probably likes it 1, but if he likes something he might n"
D07-1017,P93-1016,0,0.0370205,"atural, here, we simply mean that a manual inspection by the authors showed that, at depth four, the resulting clusters had struck a better granularity balance than other cutoff points. We acknowledge that this is a very coarse way of extracting concepts from WordNet. on the relational selectional preferences, something we do not address this in this paper. 4.3 Implementation We implemented LEDIR with both the JRM and IRM models using inference rules from DIRT and semantic classes from both CBC and WordNet. We parsed the 1999 AP newswire collection consisting of 31 million words with Minipar (Lin 1993) and used this to obtain the probability statistics for the models (as described in section 3.2). We performed both system-wide evaluations and intrinsic evaluations with different values of α and β parameters. Section 5 presents these results and our error analysis. 4.4 Gold Standard Construction In order to evaluate the performance of the different systems, we compare their outputs against a manually annotated gold standard. To create this gold standard, we randomly sampled 160 inference rules of the form pi ⇔ pj from DIRT. We discarded three rules since they contained nominalizations5. For"
D07-1017,N03-1022,0,0.0107569,"erences as the basis for our algorithm. We provide empirical evidence to validate the following main contribution: Claim: Relational selectional preferences can be used to automatically determine the plausibility and directionality of an inference rule. 2 Related Work In this section, we describe applications that can benefit by using inference rules and their directionality. We then talk about some previous work in this area. 2.1 Applications Open domain question answering approaches often cast QA as the problem of finding some kind of semantic inference between a question and its answer(s) (Moldovan et al. 2003; Echiabi and Marcu 2003). Harabagiu and Hickl (2006) recently demonstrated that textual entailment inference information, which in this system is a set of directional inference relations, improves the performance of a QA system significantly even without using any other form of semantic inference. This evidence supports the idea that learning the directionality of other sets of inference rules may improve QA performance. In Multi-Document Summarization (MDS), paraphrasing is useful for determining sentences that have similar meanings (Barzilay et al. 1999). Knowing the directionality between"
D07-1017,N07-1071,1,0.620214,"erence rules and to identify the directionality of the correct ones. Let pi ⇔ pj be an inference rule where each p is a binary semantic relation between two entities x and y. Let <x, p, y> be an instance of relation p. Formal problem definition: Given the inference rule pi ⇔ pj, we want to conclude which one of the following is more appropriate: 1. pi ⇔ pj 2. pi ⇒ pj 3. pi ⇐ pj 4. No plausible inference Consider the example (1) from section 1. There, it is most plausible to conclude “X eats Y” ⇒ “X likes Y”. Our algorithm LEDIR uses selectional preferences along the lines of Resnik (1996) and Pantel et al. (2007) to determine the plausibility and directionality of inference rules. 3.1 Underlying Assumption Many approaches to modeling lexical semantics have relied on the distributional hypothesis (Harris 1954), which states that words that appear in the same contexts tend to have similar meanings. The idea is that context is a good indicator of a word meaning. Lin and Pantel (2001) proposed an extension to the distributional hypothesis and applied it to paths in dependency trees, where if two paths tend to occur in similar contexts it is hypothesized that the meanings of the paths tend to be similar. I"
D07-1017,W04-3206,0,0.660215,"versity of Southern California Marina del Rey, CA {rahul,pantel,hovy}@isi.edu being identical in meaning to, or derivable from, the other. In the rest of this paper we discuss plausible inference only. Given the importance of inference, several researchers have developed inference rule collections. While manually built resources like WordNet (Fellbaum 1998) and Cyc (Lenat 1995) have been around for years, for coverage and domain adaptability reasons many recent approaches have focused on automatic acquisition of paraphrases (Barzilay and McKeown 2001) and inference rules (Lin and Pantel 2001; Szpektor et al. 2004). The downside of these approaches is that they often result in incorrect inference rules or in inference rules that are underspecified in directionality (i.e. asymmetric but are wrongly considered symmetric). For example, consider an inference rule from DIRT (Lin and Pantel 2001): X eats Y ⇔ X likes Y (1) Abstract Semantic inference is a core component of many natural language applications. In response, several researchers have developed algorithms for automatically learning inference rules from textual corpora. However, these rules are often either imprecise or underspecified in directionali"
D07-1017,N06-1008,0,0.0279927,"s over the Web to detect these relations among a set of candidate verb pairs. Their approach however is limited only to verbs and to specific types of verb-verb relations. Zanzotto et al. (2006) explored a selectional preference-based approach to learn asymmetric inference rules between verbs. They used the selectional preferences of a single verb, i.e. the semantic types of a verb’s arguments, to infer an asymmetric inference between the verb and the verb form of its argument type. Their approach however applies also only to verbs and is limited to some specific types of verb-argument pairs. Torisawa (2006) presented a method to acquire inference rules with temporal constraints, between verbs. They used co-occurrences between verbs in Japanese coordinated sentences and co-occurrences between verbs and nouns to learn the verb-verb inference rules. Like the previous two methods, their approach too deals only with verbs and is limited to learning inference rules that are temporal in nature. Geffet and Dagan (2005) proposed an extension to the distributional hypothesis to discover entailment relation between words. They model the context of a word using its syntactic features and compare the context"
D07-1017,P06-1107,0,0.0360578,"ve been a few approaches at learning the directionality of restricted sets of semantic relations, mostly between verbs. Chklovski and Pantel (2004) used lexico-syntactic patterns over the Web to detect certain types of symmetric and asymmetric relations between verbs. They manually examined and obtained lexico-syntactic patterns that help identify the types of relations they considered and used these lexico-syntactic patterns over the Web to detect these relations among a set of candidate verb pairs. Their approach however is limited only to verbs and to specific types of verb-verb relations. Zanzotto et al. (2006) explored a selectional preference-based approach to learn asymmetric inference rules between verbs. They used the selectional preferences of a single verb, i.e. the semantic types of a verb’s arguments, to infer an asymmetric inference between the verb and the verb form of its argument type. Their approach however applies also only to verbs and is limited to some specific types of verb-argument pairs. Torisawa (2006) presented a method to acquire inference rules with temporal constraints, between verbs. They used co-occurrences between verbs in Japanese coordinated sentences and co-occurrence"
D07-1017,P05-1014,0,\N,Missing
D07-1082,H92-1022,0,0.0164247,"rom the resulting classifier (Lewis and Catlett, 1994). We utilize a maximum entropy (ME) model (Berger et al., 1996) to design the basic classifier used in active learning for WSD. The advantage of the ME model is the ability to freely incorporate features from diverse sources into a single, wellgrounded statistical model. A publicly available ME toolkit (Zhang et. al., 2004) was used in our experiments. In order to extract the linguistic features necessary for the ME model, all sentences containing the target word were automatically part786 of-speech (POS) tagged using the Brill POS tagger (Brill, 1992). Three knowledge sources were used to capture contextual information: unordered single words in topical context, POS of neighboring words with position information, and local collocations. These are same as three of the four knowledge sources used in (Lee and Ng, 2002). Their fourth knowledge source (named syntactic relations) was not used in our work. 5 Stopping Conditions In active learning algorithm, defining the stopping condition for active learning is a critical problem, because it is almost impossible for the human annotator to label all unlabeled samples. This is a problem of estimati"
D07-1082,J98-1001,0,0.0132267,"pling, we propose a bootstrap-based oversampling (BootOS) method that works better than ordinary over-sampling in active learning for WSD. Finally, we investigate when to stop active learning, and adopt two strategies, max-confidence and min-error, as stopping conditions for active learning. According to experimental results, we suggest a prediction solution by considering max-confidence as the upper bound and min-error as the lower bound for stopping conditions. 1 Introduction Word sense ambiguity is a major obstacle to accurate information extraction, summarization, and machine translation (Ide and Veronis, 1998). In recent years, a variety of techniques for machine learning algorithms have demonstrated remarkable performance for automated word sense disambiguation (WSD) (Chan and Ng, 2006; Dagan et. al., 2006; Xue et. al., 2006; Kohomban and Lee. 2005; Dang and Palmer, 2005), when enough labeled training data is available. However, creating Eduard Hovy University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu a large sense-tagged corpus is very expensive and time-consuming, because these data have to be annotated by human experts. A"
D07-1082,J93-2004,0,0.0308527,"Missing"
D07-1082,P06-1057,0,0.0172745,"Missing"
D07-1082,S01-1020,0,\N,Missing
D07-1082,N06-1016,0,\N,Missing
D07-1082,W02-1006,0,\N,Missing
D07-1082,N06-2015,1,\N,Missing
D07-1082,P06-1012,0,\N,Missing
D07-1082,P05-1005,0,\N,Missing
D07-1082,P04-1036,0,\N,Missing
D07-1082,P06-2118,0,\N,Missing
D07-1082,P05-1006,0,\N,Missing
D07-1088,W97-0304,0,0.0108238,"9 documents in the training set and 19 in the testing set. 1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248 842 Table 4 gives the numbers of documents and data records in the training and the testing set. 4.2 Evaluation Metrics To evaluate data record extraction, we notice it is not fair to strictly evaluate the boundaries of data records because this does not penalize the nearmiss and false positive of data records in a reasonable way; sentences near a boundary that contain no relevant record information can be included or omitted without affecting the results. Hence the standard Pk (Beeferman et al., 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task. As we are concerned with the usefulness of knowledge in extracted data records, we instead evaluate from the perspective of IE. We measure system performance on the quality of the extracted data records. For each extracted data record, it will be aligned to one of the data records in the gold standard using the “dominance rule” (if the data record can be aligned to multiple records in the gold standard, it will be aligned to the one with highest overlap). Then we evaluate the precision, rec"
D07-1088,P06-1009,0,0.0352334,"l features into a discourse level CRF model to represent the template for extracting data records in the future. Recently an increasing number of research efforts on text mining and IE have used CRF models (e.g., Peng and McCallum, 2004). The CRF model provides a compact way to integrate different types of features when sequential labeling is important. 839 Recent work includes improved model variants (e.g., Jiao et al., 2006; Okanohara et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), and word alignment (Blunsom and Cohn, 2006). But none of them have used CRFs for discourse level data record extraction. We use a CRF model to represent a data record template and integrate various knowledge as CRF features. Instead of traditional work on the sentence level, our focus here is on the discourse level. As this has not been carefully explored, we experiment with various selected features. For the biomedical domain, our work will facilitate biomedical research by supporting the construction of Knowledge Base Management Systems (e.g., Stephan et al., 2001; Hahn et al., 2002; Burns and Cheng, 2006). Unlike the well-studied pr"
D07-1088,A00-2004,0,0.0120238,"does not apply to unstructured text. The reason lies in the difficulty of representing a data record template in free text without formatting tags and integrating it into a learning system. We show how to address this problem by deriving data record templates through language analysis and representing them with a discourse level CRF model. Given the problem of identifying one or more records in free text, it is natural to turn toward text segmentation. The Natural Language Processing (NLP) community has come up with various solutions towards topic-based text segmentation (e.g., Hearst, 1994; Choi, 2000; Malioutov and Barzilay, 2006). Most unsupervised text segmentation approaches work under optimization criteria to maximize the intra-segment similarity and minimize the inter-segment similarity based on word distribution statistics. However, this approach cannot be applied directly to data record extraction. A careful study of our corpus shows that data records share many words and phrases and are not distinguishable based on word similairties. In other words, different experiments (records) always belong to the same topic and there is no way to segment them using standard topic segmentation"
D07-1088,P94-1002,0,0.159054,"this approach does not apply to unstructured text. The reason lies in the difficulty of representing a data record template in free text without formatting tags and integrating it into a learning system. We show how to address this problem by deriving data record templates through language analysis and representing them with a discourse level CRF model. Given the problem of identifying one or more records in free text, it is natural to turn toward text segmentation. The Natural Language Processing (NLP) community has come up with various solutions towards topic-based text segmentation (e.g., Hearst, 1994; Choi, 2000; Malioutov and Barzilay, 2006). Most unsupervised text segmentation approaches work under optimization criteria to maximize the intra-segment similarity and minimize the inter-segment similarity based on word distribution statistics. However, this approach cannot be applied directly to data record extraction. A careful study of our corpus shows that data records share many words and phrases and are not distinguishable based on word similairties. In other words, different experiments (records) always belong to the same topic and there is no way to segment them using standard topic"
D07-1088,P06-1027,0,0.046043,"antic understanding of text is necessary. We therefore first perform some sentence level extraction (following the HP problem) and then integrate semantic labels and semantic language model features into a discourse level CRF model to represent the template for extracting data records in the future. Recently an increasing number of research efforts on text mining and IE have used CRF models (e.g., Peng and McCallum, 2004). The CRF model provides a compact way to integrate different types of features when sequential labeling is important. 839 Recent work includes improved model variants (e.g., Jiao et al., 2006; Okanohara et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), and word alignment (Blunsom and Cohn, 2006). But none of them have used CRFs for discourse level data record extraction. We use a CRF model to represent a data record template and integrate various knowledge as CRF features. Instead of traditional work on the sentence level, our focus here is on the discourse level. As this has not been carefully explored, we experiment with various selected features. For the biomedical domain, our work will fac"
D07-1088,P06-1004,0,0.0117978,"ply to unstructured text. The reason lies in the difficulty of representing a data record template in free text without formatting tags and integrating it into a learning system. We show how to address this problem by deriving data record templates through language analysis and representing them with a discourse level CRF model. Given the problem of identifying one or more records in free text, it is natural to turn toward text segmentation. The Natural Language Processing (NLP) community has come up with various solutions towards topic-based text segmentation (e.g., Hearst, 1994; Choi, 2000; Malioutov and Barzilay, 2006). Most unsupervised text segmentation approaches work under optimization criteria to maximize the intra-segment similarity and minimize the inter-segment similarity based on word distribution statistics. However, this approach cannot be applied directly to data record extraction. A careful study of our corpus shows that data records share many words and phrases and are not distinguishable based on word similairties. In other words, different experiments (records) always belong to the same topic and there is no way to segment them using standard topic segmentation techniques (even if one views"
D07-1088,P06-1059,0,0.0611777,"of text is necessary. We therefore first perform some sentence level extraction (following the HP problem) and then integrate semantic labels and semantic language model features into a discourse level CRF model to represent the template for extracting data records in the future. Recently an increasing number of research efforts on text mining and IE have used CRF models (e.g., Peng and McCallum, 2004). The CRF model provides a compact way to integrate different types of features when sequential labeling is important. 839 Recent work includes improved model variants (e.g., Jiao et al., 2006; Okanohara et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), and word alignment (Blunsom and Cohn, 2006). But none of them have used CRFs for discourse level data record extraction. We use a CRF model to represent a data record template and integrate various knowledge as CRF features. Instead of traditional work on the sentence level, our focus here is on the discourse level. As this has not been carefully explored, we experiment with various selected features. For the biomedical domain, our work will facilitate biomedical resear"
D07-1088,N04-1042,0,0.301365,"lem (finding individual attribute values) is solved using a sentence-level CRF labeling model that integrates a rich set of linguistic features. For the VP problem, we apply a discourse-level CRF model to identify individual experiments (data records). This model utilizes deep 838 semantic knowledge from the HP results (attribute labels within sentences) together with semantic language models and achieves significant improvements over baseline systems. This paper mainly focuses on the VP problem, since linguistic features for the HP problem is the general IE topic of much past research (e.g., Peng and McCallum, 2004). We apply various feature combinations to learn the most suitable and indicative linguistic features. The remainder of this paper is organized as follows: in the next section we discuss related work. Following that, we present the approach to extract data records in Section 3. We give extensive experimental evaluations in Section 4 and conclude in Section 5. 2 Related Work As mentioned, data record extraction has been extensively studied for structured and semistructured resources (e.g., Muslea et al., 2001; Arasu and Garcia-Molina, 2003; Liu et al., 2003; Zhu et al., 2006). Most of those app"
D07-1088,J02-1002,0,0.0134737,"9 in the testing set. 1 http://www3.interscience.wiley.com/cgi-bin/jhome/31248 842 Table 4 gives the numbers of documents and data records in the training and the testing set. 4.2 Evaluation Metrics To evaluate data record extraction, we notice it is not fair to strictly evaluate the boundaries of data records because this does not penalize the nearmiss and false positive of data records in a reasonable way; sentences near a boundary that contain no relevant record information can be included or omitted without affecting the results. Hence the standard Pk (Beeferman et al., 1997) and WinDiff (Pevzner and Hearst, 2002) measures for text segmentation are not so suitable for our task. As we are concerned with the usefulness of knowledge in extracted data records, we instead evaluate from the perspective of IE. We measure system performance on the quality of the extracted data records. For each extracted data record, it will be aligned to one of the data records in the gold standard using the “dominance rule” (if the data record can be aligned to multiple records in the gold standard, it will be aligned to the one with highest overlap). Then we evaluate the precision, recall, and F1 scores of extracted units o"
D07-1088,W06-1671,0,0.0728317,"r, this approach cannot be applied directly to data record extraction. A careful study of our corpus shows that data records share many words and phrases and are not distinguishable based on word similairties. In other words, different experiments (records) always belong to the same topic and there is no way to segment them using standard topic segmentation techniques (even if one views the problem as a finer-level segmentation than traditional text segmentation). In addition, most text segmentation approaches require a prespecified number of segments, which in our domain cannot be provided. (Wick et al., 2006) report extracting database records by learning record field compatibility. However, in our case, the field compatibility is hard to distinguish even by a human expert. Cluster-based or pairwise field similarity measures do not apply to our corpora without complex knowledge reasoning. Most of Wick et al.’s data (faculty and student’s homepages) contains one record. In addition, as explained below, we have found that surface word statistics alone are not sufficient to derive data record templates for extraction. Some (limited) form of semantic understanding of text is necessary. We therefore fi"
D07-1113,H05-1017,0,0.0116618,"duct reviews. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004; Popescu et al., 2005) and news articles (Kim and Hovy, 2004; Wilson et al., 2005). In text classification, systems typically use bagof-words models, mostly with supervised learning algorithms using Naive Bayes or Support Vector Machines (Joachims, 1998) to classify documents into several categories such as sports, art, politics, and religion. Liu et al. (2004) and Gliozzo et al. (2005) address the difficulty of obtaining training corpora for supervised learning and propose unsupervised learning approaches. Another recent related classification task focuses on academic and commercial efforts to detect email spam messages. For an SVM-based approach, see (Drucker et al., 1999). In our study, we explore the use of generalized lexical features for predictive opinion analysis and compare it with the bag-of-words approach. 3 Modeling Prediction In this section, we define the task of analyzing predictive opinions in the electoral domain. Message text Predicted winning party Riding"
D07-1113,C04-1200,1,0.135589,"d subjective expressions from sentences using a bootstrapping pattern learning process. Wiebe et. al (2004) and Riloff et. al (2005) adopted pattern learning with lexical feature generalization for subjective expression detection. Dave et. al (2003) and Jindal and Liu (2006) also learned patterns of opinion expression in product reviews. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004; Popescu et al., 2005) and news articles (Kim and Hovy, 2004; Wilson et al., 2005). In text classification, systems typically use bagof-words models, mostly with supervised learning algorithms using Naive Bayes or Support Vector Machines (Joachims, 1998) to classify documents into several categories such as sports, art, politics, and religion. Liu et al. (2004) and Gliozzo et al. (2005) address the difficulty of obtaining training corpora for supervised learning and propose unsupervised learning approaches. Another recent related classification task focuses on academic and commercial efforts to detect email spam messages. For an SVM-based approach, see"
D07-1113,W02-1011,0,0.0117313,"ribes our data set. Section 4 describes our system Crystal with proposed feature generalization algorithm. Section 5 The resulting corpus is available at http://www.isi.edu/ ~skim/Download/Data/predictive.htm 1 1057 reports empirical evidence that Crystal outperforms several baseline systems. Finally, Section 6 concludes with a description of the impact of this work. 2 Related Work This work is closely related to opinion analysis and text classification. Most research on opinion analysis in computational linguistics has focused on sentiment analysis, subjectivity detection, and review mining. Pang et al. (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. Wiebe et al. (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Wiebe et. al (2004) and Riloff et. al (2005) adopted pattern learning with lexical feature generalization for subjective expression detection. Dave et. al (2003) and Jindal and Liu (2006) also learned patterns of opinion expression in product reviews. Yu a"
D07-1113,W03-0404,0,0.0198397,"Missing"
D07-1113,P99-1032,0,0.0279511,"Missing"
D07-1113,J04-3002,0,0.036231,"Missing"
D07-1113,W03-1017,0,0.0516991,"002) and Turney (2002) classified sentiment polarity of reviews at the document level. Wiebe et al. (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Wiebe et. al (2004) and Riloff et. al (2005) adopted pattern learning with lexical feature generalization for subjective expression detection. Dave et. al (2003) and Jindal and Liu (2006) also learned patterns of opinion expression in product reviews. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004; Popescu et al., 2005) and news articles (Kim and Hovy, 2004; Wilson et al., 2005). In text classification, systems typically use bagof-words models, mostly with supervised learning algorithms using Naive Bayes or Support Vector Machines (Joachims, 1998) to classify documents into several categories such as sports, art, politics, and religion. Liu et al. (2004) and Gliozzo et al. (2005) address the difficulty"
D07-1113,H05-1044,0,\N,Missing
D07-1113,H05-2017,0,\N,Missing
D07-1113,H05-1043,0,\N,Missing
D07-1113,P02-1053,0,\N,Missing
D09-1099,P99-1008,0,0.0373775,"ut considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query, and kept only the unique ones. In total, we collected 1.1 GB of snippets for"
D09-1099,P99-1016,0,0.035074,"performance of our approach not by measuring the top951 ranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retriev"
D09-1099,C02-1130,1,0.200375,"passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the cont"
D09-1099,N03-1011,0,0.526242,"hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query, and kept only the unique ones. In total, we collected 1.1 GB of snippets for Animals and 1.5 GB f"
D09-1099,C92-2082,0,0.155172,"l the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learning. KnowItAll (Etzioni et al., 2005) applies the hyponym pattern"
D09-1099,P08-1119,1,0.498746,"xample of (when the first argument is an instance/example of the second). Section 2 describes our method for harvesting; Section 3 discusses related work; and Section 4 describes the experiments and the results. 2 Term and Relation Extraction using the Doubly-Anchored Pattern Our goal is to develop a technique that automatically ‘fills in’ the concept space in the taxonomy below any root concept, by harvesting terms through repeated web queries. We perform this in two alternating stages. 949 Stage 1: Basic-level/Instance concept collection: We use the Doubly-Anchored Pattern DAP developed in (Kozareva et al., 2008): DAP: [SeedTerm1] such as [SeedTerm2] and <X> which learns a list of basic-level concepts or instances (depending on whether SeedTerm2 expresses a basic-level concept or an instance).1 DAP is very reliable because it is instantiated with examples at both ‘ends’ of the space to be filled (the higher-level (root) concept SeedTerm1 and a basiclevel term or instance (SeedTerm2)), which mutually disambiguate each other. For example, “presidents” for SeedTerm1 can refer to the leader of a country, corporation, or university, and “Ford” for SeedTerm2 can refer to a car company, an automobile pioneer"
D09-1099,W02-1111,0,0.0154932,"g the top951 ranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for People we force harvesting down to the level of individual instances). To collect data, we submitted the DAP patterns as web queries to Google, retrieved the top 1000 web snippets per query,"
D09-1099,I08-2112,0,0.018117,"Missing"
D09-1099,N04-1041,0,0.165862,"ncept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus."
D09-1099,W97-0313,1,0.420074,"ept such as Concept and <X> If (b) returns more web hits than (a), then the concept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent y"
D09-1099,P98-2182,0,0.111571,"If (b) returns more web hits than (a), then the concept passes the test, otherwise it fails. The first (most highly ranked) concept that passes CPT becomes the new seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers h"
D09-1099,W02-1028,1,0.342189,"seed concept for the next bootstrapping cycle. In principle, we could use all the concepts that pass the CPT for bootstrapping2 . However, for practical reasons (primarily limitations on web querying), we run the algorithm for 10 iterations. 3 Related Work Many algorithms have been developed to automatically acquire semantic class members using a variety of techniques, including co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). The work most closely related to ours is that of (Hearst, 1992) who introduced the idea of applying hyponym patterns to text, which explicitly identify a hyponym relation between two terms (e.g., 2 The number of ranked concepts that pass CPT changes in each iteration. Also, the wildcard * is important for counts, as can be verified with a quick experiment using Google. “such authors as <X>”). In recent years, several researchers have followed up on this idea using the web as a corpus. (Pasca, 2004) applies lexicosyntactic hyponym patterns to the Web and use the contexts around them for learn"
D09-1099,C02-1114,0,0.0176554,"sion the basic-level concepts for given root concept. Thus, we almost entirely eliminate the need for humans to provide hyponym seeds. Second, we evaluate the performance of our approach not by measuring the top951 ranked 5 hypernyms given a basic-level concept, but considering all harvested hypernyms of the concept. Unlike (Etzioni et al., 2005), (Pasca, 2007) and (Snow et al., 2005), we learn both instances and concepts simultaneously. Some researchers have also worked on reorganizing, augmenting, or extending semantic concepts that already exist in manually built resources such as WordNet (Widdows and Dorow, 2002; Snow et al., 2005) or Wikipedia (Ponzetto and Strube, 2007). Work in automated ontology construction has created lexical hierarchies (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002), and learned semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). 4 Evaluation The root concepts discussed in this paper are Animals and People, because they head large taxonomic structures that are well-represented in WordNet. Throughout these experiments, we used as the initial SeedTerm2 lions for Animals and Madonna for People (by specifically choosing a proper name for"
D09-1099,C98-2177,0,\N,Missing
D10-1108,C02-1144,0,\N,Missing
D10-1108,C92-2082,0,\N,Missing
D10-1108,P06-1100,0,\N,Missing
D10-1108,P07-1030,0,\N,Missing
D10-1108,P06-1038,0,\N,Missing
D10-1108,P06-1015,0,\N,Missing
D10-1108,N03-1036,0,\N,Missing
D10-1108,P06-1101,0,\N,Missing
D10-1108,P08-1119,1,\N,Missing
D10-1108,D09-1099,1,\N,Missing
D10-1108,P09-1031,0,\N,Missing
D10-1108,P98-2127,0,\N,Missing
D10-1108,C98-2122,0,\N,Missing
D10-1108,N03-1011,0,\N,Missing
D10-1108,P10-1044,0,\N,Missing
D10-1108,P08-1078,0,\N,Missing
D11-1116,S07-1018,0,0.0177516,"ing algorithm. Semantically-enriched output. The 2008 and 2009 CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), which required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encourage the development of parsers with additional semantic annotation. These tasks relied upon PropBank (2005) and NomBank (2004) for the semantic roles. A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al., 2007) and work by Das et al. (2010). 7 Conclusion In this paper, we have described a new high-quality dependency tree conversion of the Penn Treebank (Marcus, et al., 1993) along with its labeled dependency scheme and presented a parser that is fast, accurate, supports non-projective trees and provides rich output, including not only informative dependency labels similar to Stanford’s but also additional semantic annotation for prepositions, possessives, and noun compound relations. We showed how the easy-first algorithm of Goldberg and Elhadad (Goldberg and Elhadad, 2010) can be extended to suppor"
D11-1116,J96-1002,0,0.00831979,"Missing"
D11-1116,W06-2920,0,0.0565736,"‘wash with cold water’ → means, ‘shave with the grain’ → in the same direction as). While, in some cases, it may be possible to use the output from a separate system for this purpose, doing so is often difficult in practice due to a wide variety of complications, including programming language differences, alternative data formats, and, sometimes, other parsers. 3 3.1 Dependency Conversion Relations and Structure Most recent English dependency parsers produce one of three sets of dependency types: unlabeled, some variant of the coarse labels used by the CoNLL dependency parsing shared-tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) (e.g., ADV, NMOD, PMOD), or Stanford’s dependency labels (de Marneffe and Manning, 2008). Unlabeled dependencies are clearly too impoverished for many tasks. Similarly, the coarse labels of the CoNLL tasks are not very specific; for example, the same relation, NMOD, is used for determiners, adjectives, nouns, participle modifiers, relative clauses, etc. that modify nouns. In contrast, the Stanford relations provide a more reasonable level of granularity. Our dependency relation scheme is similar to Stanford’s basic scheme but has several differences. It introduces several"
D11-1116,D07-1101,0,0.0392866,"thms, such as Nivre’s (2003) algorithm and an expected linear time dynamic programming approach presented by Huang and Sagae (2010), a few other fast alternatives exist. Goldberg and Elhadad’s (2010) easy-first algorithm is one such example. Another example, is Roark and Hollingshead’s (2009) work that uses chart constraints to achieve linear time complexity for constituency parsing. Effective features for parsing. A variety of work has investigated the use of more informative features for parsing. This includes work that integrates second and even third order features (McDonald et al., 2006; Carreras, 2007; Koo and Collins, 2010). Also, some work has incorporated unsupervised word clusters as features, including that of Koo et al. (2008) and Suzuki et al. (2009), who utilized unsupervised word clusters created using the Brown et al. (1992) hierarchical clustering algorithm. Semantically-enriched output. The 2008 and 2009 CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), which required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encourage the development of parsers wi"
D11-1116,cer-etal-2010-parsing,0,0.0125423,"Missing"
D11-1116,A00-2018,0,0.601295,"ambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, M ALT PARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall speed. Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which Cer et al. (2010) show that this isn’t the case. The optional semantic annotation modules also Proceedings of the 2011 Conference on Empirical Methods"
D11-1116,N10-1138,0,0.012791,"ched output. The 2008 and 2009 CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), which required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encourage the development of parsers with additional semantic annotation. These tasks relied upon PropBank (2005) and NomBank (2004) for the semantic roles. A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al., 2007) and work by Das et al. (2010). 7 Conclusion In this paper, we have described a new high-quality dependency tree conversion of the Penn Treebank (Marcus, et al., 1993) along with its labeled dependency scheme and presented a parser that is fast, accurate, supports non-projective trees and provides rich output, including not only informative dependency labels similar to Stanford’s but also additional semantic annotation for prepositions, possessives, and noun compound relations. We showed how the easy-first algorithm of Goldberg and Elhadad (Goldberg and Elhadad, 2010) can be extended to support non-projective trees by addi"
D11-1116,W08-1301,0,0.0145371,"Missing"
D11-1116,C96-1058,0,0.85645,"(46.2) * 37.8 (39.4) 32.1 (35.6) 40.6 (42.3) 28.4 (31.3) 35.0 (36.4) 31.6 (34.2) 37.4 (38.5) 31.4 (34.1) 36.1 (37.3) 34.5 (37.5) 40.4 (41.9) 34.8 (37.7) 40.6 (42.5) * * 43.5 43.6 Non-Proj Arcs Labeled Unlabeled 66.5 (69.7) 69.3 (71.7) 67.3 (70.9) 69.3 (72.5) 21.1 (21.1) 22.7 (21.9) * 15.1 (16.3) 62.5 (65.3) 63.7 (66.9) 62.9 (65.3) 64.1 (66.5) 19.5 (19.5) 20.3 (19.9) 13.1 (12.0) 13.9 (12.7) 51.8 (53.8) 53.8 (55.4) 61.8 (63.3) 63.3 (65.3) * * 32.3 34.3 Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were produced using gold POS tags. † Eisner (1996) algorithm with non-projective rewriting and second order features. ‡ Results not directly comparable; see text. ∗ Labeled dependencies not available/comparable. 4.5 Evaluation The following split of the Penn Treebank (Marcus, et al., 1993) was used for the experiments: sections 2–21 for training, 22 for development, and 23 for testing. For part-of-speech (POS) tagging, we used an inhouse SVM-based POS tagger modeled after the work of Giménez and Márquez (2004) 7 . The training data was tagged in a 10-fold fashion; each fold was tagged using a tagger trained from the nine remaining folds. The"
D11-1116,J02-3001,0,0.00891057,"des work that integrates second and even third order features (McDonald et al., 2006; Carreras, 2007; Koo and Collins, 2010). Also, some work has incorporated unsupervised word clusters as features, including that of Koo et al. (2008) and Suzuki et al. (2009), who utilized unsupervised word clusters created using the Brown et al. (1992) hierarchical clustering algorithm. Semantically-enriched output. The 2008 and 2009 CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), which required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encourage the development of parsers with additional semantic annotation. These tasks relied upon PropBank (2005) and NomBank (2004) for the semantic roles. A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al., 2007) and work by Das et al. (2010). 7 Conclusion In this paper, we have described a new high-quality dependency tree conversion of the Penn Treebank (Marcus, et al., 1993) along with its labeled dependency scheme and presented a parser that is fast, ac"
D11-1116,gimenez-marquez-2004-svmtool,0,0.0168283,".3 Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded). Results in parentheses were produced using gold POS tags. † Eisner (1996) algorithm with non-projective rewriting and second order features. ‡ Results not directly comparable; see text. ∗ Labeled dependencies not available/comparable. 4.5 Evaluation The following split of the Penn Treebank (Marcus, et al., 1993) was used for the experiments: sections 2–21 for training, 22 for development, and 23 for testing. For part-of-speech (POS) tagging, we used an inhouse SVM-based POS tagger modeled after the work of Giménez and Márquez (2004) 7 . The training data was tagged in a 10-fold fashion; each fold was tagged using a tagger trained from the nine remaining folds. The development and test sections were tagged by an instance of the tagger trained using the entire training set. The full details of the POS tagger are outside the scope of this paper; it is included with the parser download. The final parser was trained for 31 iterations, which is the point at which its performance on the development set peaked. One test run was performed with non-projectivity support disabled in order to get some idea of the impact of the move o"
D11-1116,N10-1115,0,0.502489,"on (Section 3) of the Penn Treebank (Marcus, et al., 1993) along with the associated dependency label scheme, which is based upon the Stanford parser’s popular scheme (de Marneffe and Manning, 2008), and a fast, accurate dependency parser with non-projectivity support (Section 4) and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, M ALT PARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall spee"
D11-1116,W09-1201,0,0.0136211,"Missing"
D11-1116,C10-2052,1,0.747545,"Penn Treebank (Marcus, et al., 1993) along with the associated dependency label scheme, which is based upon the Stanford parser’s popular scheme (de Marneffe and Manning, 2008), and a fast, accurate dependency parser with non-projectivity support (Section 4) and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, M ALT PARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall spee"
D11-1116,W07-2416,0,0.0507252,"] P |S * |UCP |NML |PR [ NT ] |RRC |NX |NAC |FRAG |INTJ |AD [ JV ] P |LST |WH * |X VBD |AUX VBN MD VBZ VB VBG VBP VP POS *- PRD ADJP JJ NN NNS NP |NML CC JJ WRB ADJP CC WRB |RB [ QNVP ] P |S * |UCP |NML |PR [ NT ] |RRC |NX |NAC |FRAG |INTJ |AD [ JV ] P |LST |WH * |X |CONJP LS : DT |NN |SYM Figure 2: Modified head-finding rules. Underline indicates that the search is performed in a left-to-right fashion instead of the default right-to-left order. NML and JJP are both products of Vadas and Curran’s (2007) patch. Bold indicates an added or moved element; for the original rules, see the paper by Johansson and Nugues (2007). in bold, are provided in Figure 2. Finally, an additional script makes additional changes and converts the intermediate output into the dependency scheme. This dependency conversion has several advantages to it. Using the modified head-finding rules for Johansson and Nugues’ (2007) converter results in fewer buggy trees than were present in the CoNLL shared tasks, including fewer trees in which words are headed by punctuation marks. For sections 2– 21, there are far fewer generic dep/DEP relations (2,765) than with the Stanford conversion (34,134) or the CoNLL 2008 shared task conversion (23"
D11-1116,P08-1068,0,0.0590451,"nt number of extensions. Various feature templates are specifically designed to produce features that help with several syntactic issues including preposition attachment, coordination, adverbial clauses, clausal complements, and relative clauses. Unfortunately, there is insufficient space in this paper to describe them all here. However, a list of feature templates will be provided with the parser download. Several of the feature templates use unsupervised word clusters created with the Brown et al. (1992) hierarchical clustering algorithm. The use of this algorithm was inspired by Koo et al. (2008), who used the top branches of the cluster hierarchy as features. However, unlike Koo et al.’s (2008) parser, the finegrained cluster identifiers are used instead of just the top 4-6 branches of the cluster hierarchy. The 175 word clusters utilized by the parser were created from the New York Times corpus (Sandhaus, 2008). Some examples from the clusters are presented in Figure 3. The ideal number of such clusters was not thoroughly investigated. while where when although despite unless unlike ... why what whom whatever whoever whomever whence ... based died involved runs ended lived charged b"
D11-1116,P10-1001,0,0.020287,"vre’s (2003) algorithm and an expected linear time dynamic programming approach presented by Huang and Sagae (2010), a few other fast alternatives exist. Goldberg and Elhadad’s (2010) easy-first algorithm is one such example. Another example, is Roark and Hollingshead’s (2009) work that uses chart constraints to achieve linear time complexity for constituency parsing. Effective features for parsing. A variety of work has investigated the use of more informative features for parsing. This includes work that integrates second and even third order features (McDonald et al., 2006; Carreras, 2007; Koo and Collins, 2010). Also, some work has incorporated unsupervised word clusters as features, including that of Koo et al. (2008) and Suzuki et al. (2009), who utilized unsupervised word clusters created using the Brown et al. (1992) hierarchical clustering algorithm. Semantically-enriched output. The 2008 and 2009 CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), which required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encourage the development of parsers with additional semantic a"
D11-1116,S07-1005,0,0.0543578,"Missing"
D11-1116,J93-2004,0,0.0750155,"for information extraction, question answering, machine translation, recognition of textual entailment, summarization, and many others. Unfortunately, currently available dependency parsers suffer from at least one of several weaknesses including high running time, limited accuracy, vague dependency labels, and lack of non-projectivity support. Furthermore, few parsers include any sort of 1257 additional semantic interpretation, such as interpretations for prepositions, possessives, or noun compounds. In this paper, we describe 1) a new dependency conversion (Section 3) of the Penn Treebank (Marcus, et al., 1993) along with the associated dependency label scheme, which is based upon the Stanford parser’s popular scheme (de Marneffe and Manning, 2008), and a fast, accurate dependency parser with non-projectivity support (Section 4) and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, a"
D11-1116,H05-1066,0,0.080508,"Missing"
D11-1116,E06-1011,0,0.442906,"r automatic preposition sense disambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, M ALT PARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall speed. Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which Cer et al. (2010) show that this isn’t the case. The optional semantic annotation modules also Proceedings of the 2011 Conference"
D11-1116,W04-2705,0,0.0450314,"Missing"
D11-1116,P09-1040,0,0.279327,"s a couple potential advantages over standard shift-reduce style parsing algorithms. The first advantage is that performing easy actions first may make the originally difficult decisions easier. The second advantage is that performing parse actions in a more flexible order than leftto-right/right-to-left shift-reduce parsing reduces the chance of error propagation. Unfortunately, the original algorithm does not support non-projective trees. To extend the algorithm to support non-projective trees, we introduce move-right and move-left operations similar to the stack-to-buffer swaps proposed by Nivre (2009) for shift-reduce style parsing. Thus, instead of attaching a token to one of its neighbors at each step, the algorithm may instead decide to move a token past one of its neighbors. Provided that no node is allowed to be moved past a token in such a way that a previous move operation is undone, there can be at most O(n2 ) moves and the overall worst-case complexity becomes O(n2 log n). While theoretically slower, this has a limited impact upon actual parsing times 4 See Goldberg and Elhadad (2010) for more explanation. 1261 in practice, especially for languages with relatively fixed word order"
D11-1116,W03-3017,0,0.256519,"Missing"
D11-1116,W09-3811,0,0.0433569,"Missing"
D11-1116,nivre-etal-2006-maltparser,0,0.122339,"nd additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, M ALT PARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall speed. Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which Cer et al. (2010) show that this isn’t the case. The optional sema"
D11-1116,P05-1013,0,0.113855,"Missing"
D11-1116,J05-1004,0,0.207777,"Missing"
D11-1116,N07-1051,0,0.498474,"Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, M ALT PARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall speed. Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which Cer et al. (2010) show that this isn’t the case. The optional semantic annotation modules also Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1257–1268, c Edinburgh, Scotland, UK, J"
D11-1116,P06-1055,0,0.324091,"tion 5). We show how Nivre’s (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results of our parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad’s (2010) implementation, M ALT PARSER (Nivre et al., 2006), MSTPARSER (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007). The experimental results show that the parser is substantially more accurate than Goldberg and Elhadad’s original implementation, with fairly similar overall speed. Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which Cer et al. (2010) show that this isn’t the case. The optional semantic annotation modules also Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1257–1268, c E"
D11-1116,N09-1073,0,0.0389087,"Missing"
D11-1116,W08-2121,0,0.109371,"till under active development. PropBank SRL The PropBank-based semantic role labeling system achieves 86.8 combined F1 measure for automatically-generated parse trees calculated over both predicate disambiguation and argument/adjunct classification (89.5 F1 on predicate disambiguation, 85.6 F1 on argument and adjuncts corresponding to dependency links, and 86.8 F1 ); this score is not directly comparable to any previous work due to some differences, including differences in both the parse tree conversion and the PropBank conversion. The most similar work is that of the CoNLL shared task work (Surdeanu et al., 2008; Hajiˇc et al., 2009). 6 Related Work Non-projectivity. There are two main approaches used in recent NLP literature for handling nonprojectivity in parse trees. The first is to use an algorithm, like the one presented in this paper, that has inherent support for non-projective trees. Examples of this include the Chu-Liu-Edmonds’ approach for maximum spanning tree (MST) parsing (McDonald et al., 2005) and Nivre’s (2009) swapbased reordering method for shift-reduce parsing. The second approach is to create an initial projective parse and then apply transformations to intro11 These accuracy figu"
D11-1116,D09-1058,0,0.0124044,"ernatives exist. Goldberg and Elhadad’s (2010) easy-first algorithm is one such example. Another example, is Roark and Hollingshead’s (2009) work that uses chart constraints to achieve linear time complexity for constituency parsing. Effective features for parsing. A variety of work has investigated the use of more informative features for parsing. This includes work that integrates second and even third order features (McDonald et al., 2006; Carreras, 2007; Koo and Collins, 2010). Also, some work has incorporated unsupervised word clusters as features, including that of Koo et al. (2008) and Suzuki et al. (2009), who utilized unsupervised word clusters created using the Brown et al. (1992) hierarchical clustering algorithm. Semantically-enriched output. The 2008 and 2009 CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009), which required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Jurafsky, 2002), are the most notable attempts to encourage the development of parsers with additional semantic annotation. These tasks relied upon PropBank (2005) and NomBank (2004) for the semantic roles. A variety of other systems have focused o"
D11-1116,A97-1011,0,0.251253,"Missing"
D11-1116,N09-3017,1,0.841813,"Missing"
D11-1116,P10-1070,1,0.534689,"o main approaches used in recent NLP literature for handling nonprojectivity in parse trees. The first is to use an algorithm, like the one presented in this paper, that has inherent support for non-projective trees. Examples of this include the Chu-Liu-Edmonds’ approach for maximum spanning tree (MST) parsing (McDonald et al., 2005) and Nivre’s (2009) swapbased reordering method for shift-reduce parsing. The second approach is to create an initial projective parse and then apply transformations to intro11 These accuracy figures are higher than what should be expected for unseen datasets; see Tratz and Hovy (2010) for more detail. 1265 duce non-projectivity into it. Examples of this include McDonald and Pereira’s (2006) rewriting of projective trees produced by the Eisner (1996) algorithm, and Nivre and Nilsson’s (2005) pseudoprojective approach that creates projective trees with specially marked arcs that are later transformed into non-projective dependencies. Descriptive dependency labels. While most recent dependency parsing research has used either vague labels, such as those of the CoNLL shared tasks, or no labels at all, some descriptive dependency label schemes exist. By far the most prominent o"
D11-1116,P07-1031,0,0.0254038,"s the head of the clause instead. 3 The parsing system includes an optional script that can convert vch arcs into aux and auxpass and the subject relations into csubjpass and nsubjpass. 1259 Figure 1: Example comparing Stanford’s (top) handling of copula and coordinating conjunctions with ours (bottom). 3.2 Conversion Process A three-step process is used to convert the Penn Treebank (Marcus, et al., 1993) from constituent parses into dependency trees labeled according to the dependency scheme presented in the prior section. The first step is to apply the noun phrase structure patch created by Vadas and Curran (2007), which adds structure to the otherwise flat noun phrases (NPs) of the Penn Treebank (e.g., ‘(metal soup pot cover)’ would become ‘(metal (soup pot) cover)’). The second step is to apply a version of Johansson and Nugues’ (2007) constituent-todependency converter with some head-finding rule modifications; these rules, with changes highlighted ( WH )? NP |NX |NML |NAC FW |NML |NN * JJR $|# CD |FW QP JJ |NAC JJS PRP ADJP RB [ SR ] VBG |DT |WP RB NP - S |SBAR |UCP |PP SINV |SBARQ |SQ UH VP |NP VB |VBP ADJP |JJP NNS QP NN $|# JJ VBN VBG ( AD |J ) JP ADVP JJR NP |NML JJS DT FW RBR RBS SBAR RB ADVP"
D11-1116,W03-3023,0,0.313136,"Missing"
D11-1116,P98-1013,0,\N,Missing
D11-1116,C98-1013,0,\N,Missing
D11-1116,J92-4003,0,\N,Missing
D11-1116,P10-1110,0,\N,Missing
D11-1116,P05-1022,0,\N,Missing
D11-1116,D07-1096,0,\N,Missing
D13-1144,W10-2919,0,0.0193462,"Missing"
D13-1144,P02-1034,0,0.13343,"e this kernel using simple matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results. 1 love we tree pairs crush toys green kissed she they puppies semantic syntactic score ჴ ✓ high ✓ ჴ low little gave cat she friend kiss her her feline a Table 1: Traditional tree kernels do not capture semantic similarity Introduction Capturing semantic similarity between sentences is a fundamental issue in NLP, with applications in a wide range of tasks. Previously, tree kernels based on common substructures have been used to model similarity between parse trees (Collins and Duffy, 2002; Moschitti, 2004; Moschitti, 2006b). These kernels encode a high number of latent syntactic features within a concise representation, and compute the similarity between two parse trees based on the matching of node-labels (words, POS tags, etc.), as well as the overlap of tree structures. While this is sufficient to capture syntactic similarity, it does not capture semantic similarity very well, even when using discrete semantic types as node labels. This constrains the utility of many traditional tree kernels in two ways: i) two sentences that are syntactically identical, but have no semanti"
D13-1144,D11-1096,0,0.22871,"k Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our examples, as it requires matching the adjective “feline” with “cat”, and verb “kissed” with “kiss”. Walk based kernels Kernels for structured data derive from the seminal Convolution Kernel formalism by Haussler (1999) for designing kernels for structured objects through local decompositions. Our proposed kernel for parse trees is most"
D13-1144,I05-5003,0,0.0258954,"Missing"
D13-1144,W13-0907,1,0.285677,"ted to work well here, while syntactic context could help disambiguation. Next, we try our approach on the MSR paraphrase corpus. The data contains a training set of 4077 pairs of sentences, annotated as paraphrases and non-paraphrases, and a test-set of 1726 sentence pairs. Each instance consists of a pair of sentences, so the VTK cannot be directly used by a kernel machine for classification. Instead, we generate 16 kernel values based for each pair on different parameter settings of the kernel, and feed these as features to a linear SVM. We finally look at the annotated Metaphor corpus by (Hovy et al., 2013). The dataset consists of sentences with specified target phrases. The task here is to classify the target use as literal or metaphorical. We focus on target phrases by upweighting walks that pass through target nodes. This is done by simply multiplying the corresponding entries in the adjacency matrix by a constant factor. 5 of SENNA embeddings (DSM), as well as Wordnet Affective Database-based (WNA) labels perform poorly (Carrillo de Albornoz et al., 2010), showing the importance of syntax for this particular problem. On the other hand, a syntactic tree kernel (SSTK) that ignores distributio"
D13-1144,P09-3004,0,0.0420388,"Missing"
D13-1144,J08-2003,0,0.0134264,"k for incorporating both syntax and semantics of sentence level constructions. 3. Our generic kernel shows state-of-the-art performance on three eclectic NLP tasks. 1411 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411–1416, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the simil"
D13-1144,P04-1043,0,0.0420077,"e matrix operations. We present our results on three diverse NLP tasks, showing state-of-the-art results. 1 love we tree pairs crush toys green kissed she they puppies semantic syntactic score ჴ ✓ high ✓ ჴ low little gave cat she friend kiss her her feline a Table 1: Traditional tree kernels do not capture semantic similarity Introduction Capturing semantic similarity between sentences is a fundamental issue in NLP, with applications in a wide range of tasks. Previously, tree kernels based on common substructures have been used to model similarity between parse trees (Collins and Duffy, 2002; Moschitti, 2004; Moschitti, 2006b). These kernels encode a high number of latent syntactic features within a concise representation, and compute the similarity between two parse trees based on the matching of node-labels (words, POS tags, etc.), as well as the overlap of tree structures. While this is sufficient to capture syntactic similarity, it does not capture semantic similarity very well, even when using discrete semantic types as node labels. This constrains the utility of many traditional tree kernels in two ways: i) two sentences that are syntactically identical, but have no semantic similarity can"
D13-1144,E06-1015,0,0.248167,"ns. We present our results on three diverse NLP tasks, showing state-of-the-art results. 1 love we tree pairs crush toys green kissed she they puppies semantic syntactic score ჴ ✓ high ✓ ჴ low little gave cat she friend kiss her her feline a Table 1: Traditional tree kernels do not capture semantic similarity Introduction Capturing semantic similarity between sentences is a fundamental issue in NLP, with applications in a wide range of tasks. Previously, tree kernels based on common substructures have been used to model similarity between parse trees (Collins and Duffy, 2002; Moschitti, 2004; Moschitti, 2006b). These kernels encode a high number of latent syntactic features within a concise representation, and compute the similarity between two parse trees based on the matching of node-labels (words, POS tags, etc.), as well as the overlap of tree structures. While this is sufficient to capture syntactic similarity, it does not capture semantic similarity very well, even when using discrete semantic types as node labels. This constrains the utility of many traditional tree kernels in two ways: i) two sentences that are syntactically identical, but have no semantic similarity can receive a high ma"
D13-1144,P05-1015,0,0.113914,"ernel by the number of edges in the product graph. 4 Experiments We evaluate the Vector Tree Kernel (VTK) on three NLP tasks. We create dependency trees using the FANSE parser (Tratz and Hovy, 2011), and use distribution-based SENNA word embeddings by Collobert et al. (2011) as word representations. These embeddings provide low-dimensional vector representations of words, while encoding distributional semantic characteristics. We use LibSVM for classification. For sake of brevity, we only report results for the best performing kernel. We first consider the Cornell Sentence Polarity dataset by Pang and Lee (2005). The task is to identify the polarity of a given sentence. The data consists of 5331 sentences from positive and negative movie reviews. Many phrases denoting sentiments are lexically ambiguous (cf. “terribly entertaining” vs “terribly written”), so simple lexical approaches are not expected to work well here, while syntactic context could help disambiguation. Next, we try our approach on the MSR paraphrase corpus. The data contains a training set of 4077 pairs of sentences, annotated as paraphrases and non-paraphrases, and a test-set of 1726 sentence pairs. Each instance consists of a pair o"
D13-1144,C08-1088,0,0.0122148,"sentence level constructions. 3. Our generic kernel shows state-of-the-art performance on three eclectic NLP tasks. 1411 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1411–1416, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Tree kernels in NLP Tree kernels have been extensively used to capture syntactic information about parse trees in tasks such as parsing (Collins and Duffy, 2002), NER (Wang et al., 2010; Cumby and Roth, 2003), SRL (Moschitti et al., 2008) and relation extraction (Qian et al., 2008). These kernels are based on the paradigm that parse trees are similar if they contain many common substructures, consisting of nodes with identical labels (Vishwanathan and Smola, 2003; Collins and Duffy, 2002). Moschitti (2006a) proposed a partial tree kernel that adds flexibility in matching tree substructures. Croce et al. (2011) introduce a lexical semantic tree kernel that incorporates continuous similarity values between node labels, albeit with a different focus than ours and would not match words with different POS. This would miss the similarity of “feline friend” and “cat” in our ex"
D13-1144,W06-1603,0,0.0319862,"Missing"
D13-1144,W04-3219,0,0.105424,"Missing"
D13-1144,D11-1116,1,0.850183,"nce they involve products of more factors (generally all less than unity). The above kernel provides a similarity measure between any two pairs of dependency parse-trees. Depending on whether we consider directional relations in the parse tree, the edge set Ep changes, while the procedure for the kernel computation remains the same. Finally, to avoid larger trees yielding larger values for the kernel, we normalize the kernel by the number of edges in the product graph. 4 Experiments We evaluate the Vector Tree Kernel (VTK) on three NLP tasks. We create dependency trees using the FANSE parser (Tratz and Hovy, 2011), and use distribution-based SENNA word embeddings by Collobert et al. (2011) as word representations. These embeddings provide low-dimensional vector representations of words, while encoding distributional semantic characteristics. We use LibSVM for classification. For sake of brevity, we only report results for the best performing kernel. We first consider the Cornell Sentence Polarity dataset by Pang and Lee (2005). The task is to identify the polarity of a given sentence. The data consists of 5331 sentences from positive and negative movie reviews. Many phrases denoting sentiments are lexi"
D13-1144,U05-1023,0,0.0609032,"Missing"
D14-1053,W06-1655,0,0.0153919,"vernment”) are retained (Example 3). Some words (i.e. U.S. imperialism) can be both target and expression, and there can be multiple targets (Example 2) within one sentence. We use a semi-Markov Conditional Random Fields (semi-CRFs) (Sarawagi and Cohen, 2004; Okanohara et al., 2006) algorithm for target and expression extraction. Semi-CRF are CRFs that relax the Markovian assumptions and allow for sequence labeling at the segment level. It has been demonstrated more powerful that CRFs in multiple sequence labeling applications including NER (Okanohara et al., 2006), Chinese word segmentation (Andrew, 2006) and opinion expression identification (Yang and Cardie, 2012). Our approach is an extension of Yang and Cardie (2012)’s system9 . Features we adopted included: • word, part of speech tag, word length. • left and right context words within a window of 2 and the correspondent POS tags. • NER feature. • subjectivity lexicon features from dictionary10 . The lexicon consists of a set of Chinese words that can act as strong or weak cues to subjectivity. • segment-level syntactic features defined in (Yang and Cardie, 2012). Most existing NER systems can barely recognize entities such as [ Vietnamese"
D14-1053,O07-6006,0,0.0647467,"Missing"
D14-1053,P06-1059,0,0.0232479,"related work is the approach introduced by O’Connor et al. (O’Connor et al., 2013) that extracts international relations from political contexts. 3 While the majority of subjective sentences omit the opinion holder, as in Examples 1 and 2, there are still a few circumstances where opinion holders (e.g., “we”, “Chinese people”, “Chinese government”) are retained (Example 3). Some words (i.e. U.S. imperialism) can be both target and expression, and there can be multiple targets (Example 2) within one sentence. We use a semi-Markov Conditional Random Fields (semi-CRFs) (Sarawagi and Cohen, 2004; Okanohara et al., 2006) algorithm for target and expression extraction. Semi-CRF are CRFs that relax the Markovian assumptions and allow for sequence labeling at the segment level. It has been demonstrated more powerful that CRFs in multiple sequence labeling applications including NER (Okanohara et al., 2006), Chinese word segmentation (Andrew, 2006) and opinion expression identification (Yang and Cardie, 2012). Our approach is an extension of Yang and Cardie (2012)’s system9 . Features we adopted included: • word, part of speech tag, word length. • left and right context words within a window of 2 and the correspo"
D14-1053,H05-1045,0,0.0304063,"tweet-level (Agarwal et al., 2011; Go et al., 2009), which can be treated as a classification/regression problem by employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approach. Yang and Cardie (2013) introduced a sequence tagging model based on CRF to jointly identify opinion holders, opinion targets, and expressions. Methods that relate to our approach include semi-superv"
D14-1053,P13-1108,0,0.067508,"Missing"
D14-1053,W06-1651,0,0.0348598,"traction. In one direction, researchers look into predicting overall sentiment polarity at document-level (Pang and Lee, 2008), aspect-level (Wang et al., 2010; Jo and Oh, 2011), sentence-level (Yang and Cardie, 2014) or tweet-level (Agarwal et al., 2011; Go et al., 2009), which can be treated as a classification/regression problem by employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations"
D14-1053,W02-1011,0,0.0160184,"in this specific task: First, the heavy use of linguistic phenomenon in the People’s Daily including rhetoric, metaphor, proverb, or even nicknames, makes existing approaches less effective for sentiment inference as identifying these expressions is a hard NLP problem in nature. Second, as we are more interested in the degree of sentiment rather than binary classification (i.e., positive versus negative) towards an entity (e.g. country or individual) in the news article, straightforward algorithms to apply would be documentlevel sentiment analysis approaches such as vector machine/regression (Pang et al., 2002) or supervised LDA (Blei and McAuliffe, 2010). A single news article, usually contains different attitudes towards multiple countries or individuals simultaneously (say praising “friends” and criticizing “enemies”), as shown in the following example from the People’s Daily of Mar. 17th, 1966: We propose a semi-supervised bootstrapping algorithm for analyzing China’s foreign relations from the People’s Daily. Our approach addresses sentiment target clustering, subjective lexicons extraction and sentiment prediction in a unified framework. Different from existing algorithms in the literature, ti"
D14-1053,J11-1002,0,0.0203972,"ted time period, China held a pretty negative attitude towards the USA based on clues such as common negative expressions (e.g., “evil” or “reactionary”), we can easily induce that “a tiger made of paper”, is a negative word. Based on aforementioned two assumptions, we formulate our approach as a semi-supervised model, which simultaneously bootstrap sentiment target lists, extracts subjective vocabularies and 3 Leader of South Vietnam Ruling political party of Vietnam. 5 One of Founders of Democratic Republic of Vietnam (North Vietnam) and Vietnam Workers’ party. 4 468 propagation algorithms (Qiu et al., 2011; Qiu et al., 2009; Zhang et al., 2010; Duyu et al., 2013). Concretely, Qiu et al. (2011) proposed a rulebased semi-supervised framework called double propagation for jointly extracting opinion words and targets. Compared to existing bootstrapping approaches, our framework is more general one with less restrictions6 . In addition, our approach harness global information (e.g. document-level, time-level) to guide the bootstrapping algorithm. Another related work is the approach introduced by O’Connor et al. (O’Connor et al., 2013) that extracts international relations from political contexts. 3"
D14-1053,W10-2910,0,0.0218248,"employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approach. Yang and Cardie (2013) introduced a sequence tagging model based on CRF to jointly identify opinion holders, opinion targets, and expressions. Methods that relate to our approach include semi-supervised approaches such as pipeline or 1. In a single news article, sentiment towards an entity is consistent. 2. Over a certain pe"
D14-1053,C08-1103,0,0.0179182,"t al., 2009), which can be treated as a classification/regression problem by employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approach. Yang and Cardie (2013) introduced a sequence tagging model based on CRF to jointly identify opinion holders, opinion targets, and expressions. Methods that relate to our approach include semi-supervised approaches such as pipeline or 1. In a sing"
D14-1053,C04-1200,1,0.426499,"Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approach. Yang and Cardie (2013) introduced a sequence tagging model based on CRF to jointly identify opinion holders, opinion targets, and expressions. Methods that relate to our approach include semi-supervised approaches such as pipeline or 1. In a single news article, sentiment towards an entity is consistent. 2. Over a certain period of time, sentiments towards an entity ar"
D14-1053,W06-0301,1,0.7609,"rection, researchers look into predicting overall sentiment polarity at document-level (Pang and Lee, 2008), aspect-level (Wang et al., 2010; Jo and Oh, 2011), sentence-level (Yang and Cardie, 2014) or tweet-level (Agarwal et al., 2011; Go et al., 2009), which can be treated as a classification/regression problem by employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approac"
D14-1053,D12-1122,0,0.126061,"ning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holders (Choi et al., 2005), topics of opinions (Stoyanov and Cardie, 2008) or opinion expressions (e.g. (Breck et al., 2007; Johansson and Moschitti, 2010; Yang and Cardie, 2012)). Kim and Hovy (2004; 2006) identified opinion holders and targets by exploring their semantics rules related to the opinion words. Choi et al. (2006) jointly extracted opinion expressions, holders and their is-from relations using an ILP approach. Yang and Cardie (2013) introduced a sequence tagging model based on CRF to jointly identify opinion holders, opinion targets, and expressions. Methods that relate to our approach include semi-supervised approaches such as pipeline or 1. In a single news article, sentiment towards an entity is consistent. 2. Over a certain period of time, sentiments"
D14-1053,P14-1031,0,0.0316619,"or metaphor recognition. 2. In Analytical Political Science, the quantitative evaluation of diplomatic relations is usually a manual task (Robinson and Shambaugh, 1995). We are hopeful that our algorithm can enable automated political analysis and facilitate political scientists’ and historians’ work. 2 Related Works Significant research efforts have been invested into sentiment analysis and opinion extraction. In one direction, researchers look into predicting overall sentiment polarity at document-level (Pang and Lee, 2008), aspect-level (Wang et al., 2010; Jo and Oh, 2011), sentence-level (Yang and Cardie, 2014) or tweet-level (Agarwal et al., 2011; Go et al., 2009), which can be treated as a classification/regression problem by employing standard machine-learning techniques, such as Naive Bayesian, SVM (Pang et al., 2002) or supervisedLDA (Blei and McAuliffe, 2010) with different types of features (i.e., unigram, bigram, POS). Other efforts are focused on targeted sentiment extraction (Choi et al., 2006; Kim and Hovy, 2006; Jin et al., 2009; Kim and Hovy, 2006). Usually, sequence labeling models such as CRF (Lafferty et al., 2001) or HMM (LIU et al., 2004) are employed for identifying opinion holder"
D14-1053,W03-1730,0,0.0110686,"m 1950 to 2010, across a 60-year time span. 15 Negative effect of strict sentence selection can be partly compensated by the consideration of time-level information 471 antagonism (m=1) tension (m=2) disharmony (m=3) neutrality (m=4) goodness (m=5) friendship (m=6) brotherhood (m=7) P 残暴(extremely cruel), 敌人(enemy) 愤慨(indignation), 侵犯(offend) 失望(disappointed), 遗憾(regret) 关切, 关注(concern) 发展的(developmental), 尊重(respect) 友谊(friendship), 朋友(friend) 伟大(firmly), 兄弟(brother) Table 1: Illustration of subjective list M News articles are first segmented using ICTCLAS Chinese segmentation word system16 (Zhang et al., 2003). Articles with fewer than 200 Chinese words are discarded. News articles are clustered by the presence of a country’s name more than 2 times based on a country name list from Wikipedia17 . Articles mentioning more than 5 different countries are discarded since they usually talk about international conferences. Note that one article can appear in different collections (example in Section 1 will appear in both Vietnam and the U.S. collection). Compound sentences are segmented into clauses based on dependency parse tree. Then those containing more than 50 characters or less than 4 characters are"
D14-1053,C10-2167,0,0.0491108,"Missing"
D14-1053,W11-0705,0,\N,Missing
D14-1214,P11-2000,0,0.166133,"Missing"
D14-1214,P11-1040,0,0.0160923,"sonal topic needs to be adequately discussed by the user and their followers in order to be detected16 . Public Event Extraction from Twitter Twitter serves as a good source for event detection owing to its real time nature and large number of users. These approaches include identifying bursty public topics (e.g.,(Diao et al., 2012)), topic evolution (Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structu"
D14-1214,D13-1114,0,0.00534075,"roughly constant, but recall increases as more life events and C ONGRATULA TIONS and C ONDOLENCES are discovered. 8 Related Work Our work is related to three lines of NLP researches. (1) user-level information extraction on social media (2) public event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitl"
D14-1214,P07-1030,0,0.0127633,"disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major li"
D14-1214,D13-1192,0,0.0182098,"social media (2) public event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly identify a global category of life events (and tweets discussing correspondent event) but identifies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoid"
D14-1214,P12-1056,0,0.222329,"important life events on which algorithms can rely for extraction or classification. Introduction Social networking websites such as Facebook and Twitter have recently challenged mainstream media as the freshest source of information on important news events. In addition to an important source for breaking news, social media presents a unique source of information on private events, for example a friend’s engagement or college graduation (examples are presented in Figure 1). While a significant amount of previous work has investigated event extraction from Twitter (e.g., (Ritter et al., 2012; Diao et al., 2012)), existing approaches mostly focus on public bursty event extraction, and little progress has been made towards the problem of automatically extracting the major life events of ordinary users. A system which can automatically extract major life events and generate fine-grained descriptions as in Figure 1 will not only help Twitter Challenge 2: Noisiness of Twitter Data: The user-generated text found in social media websites such as Twitter is extremely noisy. The language used to describe life events is highly varied and ambiguous and social media users frequently discuss public news and mund"
D14-1214,P11-1055,0,0.00961363,"of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity of Twitter data by relying on minimum efforts of human supervision, and s"
D14-1214,D14-1108,0,0.00709329,"e Stanford PragBank10 , 8 Most tweets in the bootstrapping output are positive. The majority of results returned by Twitter Search are negative examples. 10 http://compprag.christopherpotts.net/ factbank.html 2002 9 an extension of FactBank (Saur´ı and Pustejovsky, 2009) which contains a list of modal words such as “might”, “will”, “want to” etc11 . • I: Whether the subject of the tweet is first person singular. • Dependency: If the subject is first person singular and the u is a verb, the dependency path between the subject and u (or nondependency). Tweet dependency paths were obtained from (Kong et al., 2014). As the tweet parser we use only supports one-to-one dependency path identification but no dependency properties, Dependency is a binary feature. The subject of each tweet is determined by the dependency link to the root of the tweet from the parser. Among the features we explore, Word encodes the general information within the tweet. Window addresses the information around topic key word. The rest of the features specifically address each of the negative situations described in Challenge 2, Section 1: Tense captures past event description, Factuality filters out wishes or imagination, I and"
D14-1214,P10-1150,1,0.666806,"(Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a w"
D14-1214,N10-1087,1,0.304045,"(Becker et al., 2011) or disaster outbreak (Sakaki et al., 2010; Li and Cardie, 2013) by spotting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a w"
D14-1214,P08-1119,1,0.214259,"potting the increase/decrease of word frequency. Some other approaches are focused on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a rel"
D14-1214,P14-1016,1,0.167411,"ic event extraction on social media. (3) Data harvesting in Information Extraction, each of which contains large amount of related work, to which we can not do fully justice. User Information Extraction from Twitter Some early approaches towards understanding user level information on social media is focused on user profile/attribute prediction (e.g.,(Ciot et al., 2013)) user-specific content extraction (Diao 15 which are 24, 38, 42-class classifiers, where 24, 38, 42 denoted the number of topics discovered in each step of bootstrapping (see Figure 5). 2004 et al., 2012; Diao and Jiang, 2013; Li et al., 2014) or user personalization (Low et al., 2011) identification. The problem of user life event extraction was first studied by Li and Cardie’s (2014). They attempted to construct a chronological timeline for Twitter users from their published tweets based on two criterion: a personal event should be personal and time-specific. Their system does not explicitly identify a global category of life events (and tweets discussing correspondent event) but identifies the topics/events that are personal and timespecific to a given user using an unsupervised approach, which helps them avoids the nuisance of"
D14-1214,D11-1024,0,0.00393037,"by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not stopping: 1. For unprocessed conversation t ∈ T if t contains reply e ∈ E, • add t to D: D = D + t. • remove t from T : T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics"
D14-1214,P09-1113,0,0.0027458,"on generating a structured representation of events (Ritter et al., 2012; Benson et al., 2011). Data Acquisition in Information Extraction Our work is also related with semi-supervised data harvesting approaches, the key idea of which is that some patterns are learned based on seeds. They are then used to find additional terms, which are subsequently used as new seeds in the patterns to search for additional new patterns (Kozareva and Hovy, 2010b; Davidov et al., 2007; Riloff et al., 1999; Igo and Riloff, 2009; Kozareva et al., 2008). Also related approaches are distant or weakly supervision (Mintz et al., 2009; Craven et al., 1999; Hoffmann et al., 2011) that rely on available structured data sources as a weak source of supervision for pattern extraction from related text corpora. 16 The reason is that topic models use word frequency for topic modeling. 9 Conclusion and Discussion In this paper, we propose a pipelined system for major life event extraction from Twitter. Experimental results show that our model is able to extract a wide variety of major life events. The key strategy adopted in this work is to obtain a relatively clean training dataset from large quantity of Twitter data by relying o"
D14-1214,D08-1027,0,0.0347829,"Missing"
D14-1214,N10-1012,0,0.00569359,"2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not stopping: 1. For unprocessed conversation t ∈ T if t contains reply e ∈ E, • add t to D: D = D + t. • remove t from T : T = T − t 2. Run streaming LDA (Yao et al., 2009) on newly added tweets in D. 3. Manually Identify meaningful/trash topics, giving label to meaningful topics. 4. Add newly detected meaningful topic l to L. 5. For conversation t belonging to trash topics • remove t from D: D ="
D14-1214,N13-1039,0,0.0248148,"Missing"
D14-1214,D11-1135,0,0.0127003,"lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4 . Our methodology is inspired by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twitter. Similar strategies have been widely used in unsupervised information extraction (Bejan et al., 2009; Yao et al., 2011) and selectional preference 3 Each whole conversation usually contains multiple tweets and users. 4 While we applied manual labeling and coherence evaluation in this work, an interesting direction for future work is automatically labeling major life event categories following previous work on labeling topics in traditional documentbased topic models (Mimno et al., 2011; Newman et al., 2010). 1999 Figure 3: Illustration of bootstrapping process. Input: Reply seed list E = {e}, Tweet conversation collection T = {t}, Retrieved Tweets Collection D = φ. Identified topic list L=φ Begin While not sto"
D14-1214,N10-1020,1,0.423031,"nd C ONDOLENCES, including the phrases: ”Congratulations”, ”Congrats”, ”Sorry to hear that”, ”Awesome”, and gather tweets that were observed with seed responses. Next, an LDA (Blei et al., 2003)2 based topic model is used to cluster the gathered 2 Topic Number is set to 120. tweets to automatically identify important categories of major life events in an unsupervised way. In our approach, we model the whole conversation dialogue as a document3 with the response seeds (e.g., congratulation) masked out. We furthermore associate each sentence with a single topic, following strategies adopted by (Ritter et al., 2010; Gruber et al., 2007). We limit the words in our document collection to verbs and nouns which we found to lead to clearer topic representations, and used collapsed Gibbs Sampling for inference (Griffiths and Steyvers, 2004). Next one of the authors manually inspected the resulting major life event types inferred by the model, and manually assigned them labels such as ”getting a job”, ”graduation” or ”marriage” and discarded incoherent topics4 . Our methodology is inspired by (Ritter et al., 2012) that uses a LDA - CLUSTERING + HUMAN - IDENTIFICATION strategy to identify public events from Twi"
D14-1214,D11-1141,1,0.538137,"Missing"
D14-1214,D11-1091,0,0.0235403,"Missing"
D14-1214,W09-1703,0,\N,Missing
D14-1218,P11-2022,0,0.482017,"equest from the first author. Little of this work survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues to ordering (Elsner and Charniak, 2008), named-entity categories (Eisner and Charniak, 2011), syntactic features (Louis and Nenkova, 2012), and others. Besides being time-intensive (feature engineering usually requites considerable 2039 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples. effort and can depend greatly on upstream feature extraction algorithms), it is not immediately apparent which aspects of a clause or a coherent text to consider when"
D14-1218,P08-2011,0,0.828832,"le. 1 Code available at stanford.edu/˜jiweil/ or by request from the first author. Little of this work survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues to ordering (Elsner and Charniak, 2008), named-entity categories (Eisner and Charniak, 2011), syntactic features (Louis and Nenkova, 2012), and others. Besides being time-intensive (feature engineering usually requites considerable 2039 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples. effort and can depend greatly on upstream feature extraction algorithms), it is not immediately apparent which as"
D14-1218,N07-1055,0,0.0447232,"pplied to the recursive network with only minor parameter altering that is excluded for brevity. To minimize the objective J(Θ), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapting differently for different parameters at different steps. Concretely, for parameter updates, let We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011): Sentence Ordering and Readability Assessment. 5.1 Sentence Ordering We follow (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 5 For more details on backpropagation through RNNs, see Socher et al. (2010). 2043 2011) that all use pairs of articles, one containing the original document order and the other a random permutation of the sentences from the same document. The pairwise approach is predicated on the assumption that the original article is always more coherent than a random permutation; this assumption has been verified in Lin et"
D14-1218,W07-2321,0,0.0776975,"ependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness (Filippova and Strube, 2007) or adding more useful types of features such as coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), and discourse relations (Lin et al., 2011). Other systems include the global graph model (Guinaudeau and Strube, 2013) which projects entities into a global graph. Louis and Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in"
D14-1218,D12-1106,0,0.274934,"k survives. Modern research tries simply to order a collection of clauses or sentences without giving an account of which order(s) is/are coherent or what the overall text structure is. The research focuses on identifying and defining a set of increasingly sophisticated features by which algorithms can be trained to propose orderings. Features being explored include the clause entities, organized into a grid (Lapata and Barzilay, 2005; Barzilay and Lapata, 2008), coreference clues to ordering (Elsner and Charniak, 2008), named-entity categories (Eisner and Charniak, 2011), syntactic features (Louis and Nenkova, 2012), and others. Besides being time-intensive (feature engineering usually requites considerable 2039 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2039–2048, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 1: Illustrations of coherent (positive) vs not-coherent (negative) training examples. effort and can depend greatly on upstream feature extraction algorithms), it is not immediately apparent which aspects of a clause or a coherent text to consider when deciding on ordering. More importantly, the f"
D14-1218,J86-3001,0,0.370411,"f which include: Rhetorical Structure Theory (RST; (Mann and Thompson, 1988)), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Coherence is what makes a multi-sentence text meaningful, both logically and syntactically. To solve"
D14-1218,J95-2003,0,0.34486,"es and the proposed model produces state-ofart performance in multiple standard evaluations for coherence models (Barzilay and Lee, 2004). The rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document ve"
D14-1218,P13-1010,0,0.763213,"or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for example, by pre-grouping entities based on semantic relatedness (Filippova and Strube, 2007) or adding more useful types of features such as coreference (Elsner and Charniak, 2008), named entities (Eisner and Charniak, 2011), and discourse relations (Lin et al., 2011). Other systems include the global graph model (Guinaudeau and Strube, 2013) which projects entities into a global graph. Louis and Nenkova (2012) introduced an HMM system in which the coherence between adjacent sentences is modeled by a hidden Markov framework captured by the 2040 Figure 2: Sentential compositionality obtained from (a) recurrent / (b) recursive neural network. The bottom layer represents word vectors in the sentence. The top layer hs denotes the resulting sentence vector. transition rules of different topics. Recurrent and Recursive Neural Networks In the context of NLP, recurrent neural networks view a sentence as a sequence of tokens and incorporat"
D14-1218,P88-1012,0,0.172648,"1 and Eduard Hovy3 Science Department, Stanford University, Stanford, CA 94305, USA Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA jiweil@stanford.edu ehovy@andrew.cmu.edu Abstract Several researchers in the 1980s and 1990s addressed the problem, the most influential of which include: Rhetorical Structure Theory (RST; (Mann and Thompson, 1988)), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to gover"
D14-1218,P88-1020,1,0.68675,"t adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Coherence is what makes a multi-sentence text meaningful, both logically and syntactically. To solve the challenge of ordering a set of sentences into coherent order, existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships. But both argume"
D14-1218,W13-3214,0,0.0117049,"ly, deep architectures, have been applied to various natural language processing tasks (see Section 2). Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens (e.g., (Bengio et al., 2006)), entities, N-grams (Wang and Manning, 2012), or phrases (Socher et al., 2013). More recent researches have begun looking at higher level distributed representations that transcend the token level, such as sentence-level (Le and Mikolov, 2014) or even discourse-level (Kalchbrenner and Blunsom, 2013) aspects. Just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs? In this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence representations learned in a deep learning framework. Specifically, we consider a WINDOW approach for sentences, as shown in Figure 1, where positive examples are windows of sentences selected from original articles generated by humans, and negatives examples are genera"
D14-1218,P91-1008,0,0.771789,"Stanford, CA 94305, USA Technology Institute, Carnegie Mellon University, Pittsburgh, PA 15213, USA jiweil@stanford.edu ehovy@andrew.cmu.edu Abstract Several researchers in the 1980s and 1990s addressed the problem, the most influential of which include: Rhetorical Structure Theory (RST; (Mann and Thompson, 1988)), which defined about 25 relations that govern clause interdependencies and ordering and give rise to text tree structures; the stepwise assembly of semantic graphs to support adductive inference toward the best explanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defi"
D14-1218,P11-1100,0,0.0697077,"Missing"
D14-1218,P00-1052,0,0.0375599,"he rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input to machine learning classifiers such as SVM. Many frameworks have extended the entity approach, for examp"
D14-1218,P89-1025,0,0.708875,"xplanation (Hobbs et al., 1988); Discourse Representation Theory (DRT; (Lascarides and Asher, 1991)), a formal semantic model of discourse contexts that constrain coreference and quantification scoping; the model of intentionoriented conversation blocks and their stack-based queueing to model attention flow (Grosz and Sidner, 1986), and more recently an inventory of a hundred or so binary inter-clause relations and associated annotated corpus (Penn Discourse Treebank. Work in text planning implemented some of these models, especially operationalized RST (Hovy, 1988) and explanation relations (Moore and Paris, 1989) to govern the planning of coherent paragraphs. Other computational work defined so called schemas (McKeown, 1985), frames with fixed sequences of clause types to achieve stereotypical communicative intentions. Coherence is what makes a multi-sentence text meaningful, both logically and syntactically. To solve the challenge of ordering a set of sentences into coherent order, existing approaches focus mostly on defining and using sophisticated features to capture the cross-sentence argumentation logic and syntactic relationships. But both argumentation semantics and crosssentence syntax (such a"
D14-1218,P14-1028,0,0.00637534,"with 0. Hidden layer number H is set to 100. Word embeddings {e} are borrowed from Senna (Collobert et al., 2011; Collobert, 2011). The dimension for these embeddings is 50. 5 Experiments Θ = [WRecurrent , Wsen , Usen ] The regularization part is paralyzed by Q to avoid overfitting. A similar loss function is applied to the recursive network with only minor parameter altering that is excluded for brevity. To minimize the objective J(Θ), we use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapting differently for different parameters at different steps. Concretely, for parameter updates, let We evaluate the proposed coherence model on two common evaluation approaches adopted in existing work (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 2011): Sentence Ordering and Readability Assessment. 5.1 Sentence Ordering We follow (Barzilay and Lapata, 2008; Louis and Nenkova, 2012; Elsner et al., 2007; Lin et al., 5 For more details on backpropagation through RNNs, see Socher et al. (2010). 2043 2011) that all use"
D14-1218,J04-3003,0,0.022481,"Missing"
D14-1218,P05-1065,0,0.017419,"Missing"
D14-1218,P12-2018,0,0.00599174,"clause or a coherent text to consider when deciding on ordering. More importantly, the features developed to date are still incapable of fully specifying the acceptable ordering(s) within a context, let alone describe why they are coherent. Recently, deep architectures, have been applied to various natural language processing tasks (see Section 2). Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens (e.g., (Bengio et al., 2006)), entities, N-grams (Wang and Manning, 2012), or phrases (Socher et al., 2013). More recent researches have begun looking at higher level distributed representations that transcend the token level, such as sentence-level (Le and Mikolov, 2014) or even discourse-level (Kalchbrenner and Blunsom, 2013) aspects. Just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs? In this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence representation"
D14-1218,P05-1044,0,0.00959549,"ate children recursively in a bottom-up fashion until reaching the where Wsen is a H × (L × K) dimensional matrix root of the tree. Concretely, for a given parent p and bsen is a H × 1 dimensional bias vector. in the tree and its two children c1 (associated with 4 vector representation hc1 ) and c2 (associated with instead of a binary classification (correct/incorrect), another commonly used approach is the contrastive approach vector representation hc2 ), standard recursive netthat minimizes the score function max(0, 1 − s + sc ) (Colworks calculates hp for p as follows: lobert et al., 2011; Smith and Eisner, 2005). s denotes the hp = f (WRecursive · [hc1 , hc2 ] + bRecursive ) (3) where [hc1 , hc2 ] denotes the concatenating vector for children vector representation hc1 and hc2 . score of a true (coherent) window and sc the score of a corrupt (containing incoherence) one) in an attempt to make the score of true windows larger and corrupt windows smaller. We tried the contrastive one for both recurrent and recursive networks but the binary approach constantly outperformed the contrastive one in this task. 2042 Figure 3: An example of coherence model based on a window of sentences (clique). The output la"
D14-1218,P13-2032,0,0.018192,"been explored to learn these embeddings in an unsupervised manner from a large corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013), which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. These vector representations can to some extent capture interesting semantic relationships, such as King −man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named entity recognition, tagging, segmentation (Wang et al., 2013), and machine translation (e.g.,(Collobert and Weston, 2008; Zou et al., 2013)). 3 Sentence Model In this section, we demonstrate the strategy adopted to compute a vector for a sentence given the sequence of its words and their embeddings. We implemented two approaches, Recurrent and Recursive neural networks, following the descriptions in for example (Mikolov et al., 2010; Sutskever et al., 2011; Socher et al., 2013). As 2041 the details of both approaches can be readily found there, we make this section brief and omit the details for brevity. Let s denote a sentence, comprised of a sequence"
D14-1218,D13-1141,0,0.00588231,"corpus (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013), which might have different generalization capabilities and are able to capture the semantic meanings depending on the specific task at hand. These vector representations can to some extent capture interesting semantic relationships, such as King −man ≈ Queue−woman (Mikolov et al., 2010), and recently have been successfully used in various NLP applications, including named entity recognition, tagging, segmentation (Wang et al., 2013), and machine translation (e.g.,(Collobert and Weston, 2008; Zou et al., 2013)). 3 Sentence Model In this section, we demonstrate the strategy adopted to compute a vector for a sentence given the sequence of its words and their embeddings. We implemented two approaches, Recurrent and Recursive neural networks, following the descriptions in for example (Mikolov et al., 2010; Sutskever et al., 2011; Socher et al., 2013). As 2041 the details of both approaches can be readily found there, we make this section brief and omit the details for brevity. Let s denote a sentence, comprised of a sequence of words s = {w1 , w2 , ..., wns }, where ns denotes the number of words withi"
D14-1218,D12-1110,0,0.00876346,"urrent framework, long-distance dependencies are difficult to capture due to the vanishing gradient problem (Bengio et al., 1994); two tokens may be structurally close to each other, even though they are far away in word sequence3 . Recursive neural networks comprise another class of architecture, one that relies and operates on structured inputs (e.g., parse trees). It computes the representation for each parent based on its children iteratively in a bottom-up fashion. A series of variations have been proposed, each tailored to different task-specific requirements, such as Matrix-Vector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Networks (Socher et al., 2013) that allow the model to have greater 3 For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). interactions between the input vectors. Many tasks have benefited from this recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), and paraphrase detection (Socher et al., 2011a). 2.1 Distributed Repre"
D14-1218,D13-1170,0,0.0254596,"r when deciding on ordering. More importantly, the features developed to date are still incapable of fully specifying the acceptable ordering(s) within a context, let alone describe why they are coherent. Recently, deep architectures, have been applied to various natural language processing tasks (see Section 2). Such deep connectionist architectures learn a dense, low-dimensional representation of their problem in a hierarchical way that is capable of capturing both semantic and syntactic aspects of tokens (e.g., (Bengio et al., 2006)), entities, N-grams (Wang and Manning, 2012), or phrases (Socher et al., 2013). More recent researches have begun looking at higher level distributed representations that transcend the token level, such as sentence-level (Le and Mikolov, 2014) or even discourse-level (Kalchbrenner and Blunsom, 2013) aspects. Just as words combine to form meaningful sentences, can we take advantage of distributional semantic representations to explore the composition of sentences to form coherent meanings in paragraphs? In this paper, we demonstrate that it is feasible to discover the coherent structure of a text using distributed sentence representations learned in a deep learning frame"
D14-1218,J99-3001,0,0.0253628,"ofart performance in multiple standard evaluations for coherence models (Barzilay and Lee, 2004). The rest of this paper is organized as follows: We describe related work in Section 2, then describe how to obtain a distributed representation for sentences in Section 3, and the window composition in Section 4. Experimental results are shown in Section 5, followed by a conclusion. 2 Related Work Coherence In addition to the early computational work discussed above, local coherence was extensively studied within the modeling framework of Centering Theory (Grosz et al., 1995; Walker et al., 1998; Strube and Hahn, 1999; Poesio et al., 2004), which provides principles to form a coherence metric (Miltsakaki and Kukich, 2000; Hasler, 2004). Centering approaches suffer from a severe dependence on manually annotated input. A recent popular approach is the entity grid model introduced by Barzilay and Lapata (2008) , in which sentences are represented by a vector of discourse entities along with their grammatical roles (e.g., subject or object). Probabilities of transitions between adjacent sentences are derived from entity features and then concatenated to a document vector representation, which is used as input"
D14-1218,N04-1015,0,\N,Missing
D14-1218,J08-1001,0,\N,Missing
D14-1220,W05-0613,0,0.0430951,"to recognize these signals and use them to appropriately compose the relationship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs. . . . Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernaul"
D14-1220,P09-1075,0,0.120896,"discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea for discourse parsing is to train two classifiers, namely a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernault et al., 2010b). Current state-of-art performance for relation identification is achieved by the recent representation learning approach proposed by (Ji and Eisenstein, 2014). The proposed framework presented in this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler"
D14-1220,P12-1007,0,0.547761,"e parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context"
D14-1220,P14-1048,0,0.252924,"core in terms of the comparison between tree structures. But these are the same when manual segmentation is used (Marcu, 2000b). 2066 Approach HILDA Joty et al. Feng and Hirst Ji and Eisenstein Unified (with feature) Ours (no feature) Ours (with feature) human Span 75.3 82.5 85.7 82.1 82.0 82.4 84.0 88.7 Nuclearity 60.0 68.4 71.0 71.1 70.0 69.2 70.8 77.7 relies on two linear-chain CRFs to obtain a sequence of discourse constituents. Relation 46.8 55.7 58.2 61.6 57.1 56.8 58.6 65.7 Table 1: Performances for different approaches. Performances for baselines are reprinted from (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). Also, we do not train a separate classifier for NU CLEUS and SATELLITE identification. The nuclearity decision is made based on the relation type produced by the multi-class classifier. 6.1 hp = f (Wsen · [hei , hej ] + bsen ) Parameter Tuning The regularization parameter Q constitutes the only parameter to tune in our framework. We tune it on the 347 training documents. Concretely, we employ a five-fold cross validation on the RST dataset and tune Q on 5 different values: 0.01, 0.1, 0.5, 1.5, 2.5. The final model was tested on the testing set after parameter tuning"
D14-1220,P07-1062,0,0.664439,"possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous"
D14-1220,D10-1039,0,0.0555268,"etely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it pl"
D14-1220,P13-2032,0,0.0187173,"children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad et al., 2008). In this paper, we select the former. In RST (Mann and Thompson, 1988), a coherent context or a document is represented as a hierarchical tree structure, the leaves of which are clause-sized units called Elementary Discourse Units (EDUs). Adjacent nodes (siblings in the tree) are linked with disco"
D14-1220,P14-1002,0,0.622711,"a binary structure classifier for determining whether two adjacent text units should be merged to form a new subtree, followed by a multi-class relation classifier for determining which discourse relation label should be assigned to the new subtree. The idea is proposed by Hernault and his colleagues (Duverle and Prendinger, 2009; Hernault et al., 2010a) and followed by other work using more sophisticated features (Feng and Hirst, 2012; Hernault et al., 2010b). Current state-of-art performance for relation identification is achieved by the recent representation learning approach proposed by (Ji and Eisenstein, 2014). The proposed framework presented in this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requ"
D14-1220,D12-1083,0,0.697012,"arly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level"
D14-1220,P13-1048,0,0.855845,"ntify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how discourse units are connected, one has to understand the communicative function of each unit, and the role it plays within the context that encapsulates it"
D14-1220,D09-1036,0,0.0597413,"Missing"
D14-1220,W10-4327,0,0.181757,"n a coherent text, units (clauses, sentences, and larger multi-clause groupings) are tightly connected semantically, syntactically, and logically. Mann and Thompson (1988) define a text to be coherent when it is possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the se"
D14-1220,J00-3005,0,0.874306,"communicative (rhetorical) function. The functions are reflected in text as signals of the author’s intentions, and take various forms (including expressions such as “therefore”, “for example”, “the answer is”, and so on; patterns of tense or pronoun usage; syntactic forms; etc.). The signals govern discourse blocks ranging from a clause to an entire text , each one associated with some discourse relation. In order to build a text’s hierarchical structure, a discourse parser needs to recognize these signals and use them to appropriately compose the relationship and nesting. Early approaches (Marcu, 2000a; LeThanh et al., 2004) rely mainly on overt discourse markers (or cue words) and use handcoded rules to build text structure trees, bottom-up from clauses to sentences to paragraphs. . . . Since a hierarchical discourse tree structure is analogous to a constituency based syntactic tree, modern research explored syntactic parsing techniques (e.g., CKY) for discourse parsing based on multiple text-level or sentence-level features (Soricut and Marcu, 2003; Reitter, 2003; Baldridge and Lascarides, 2005; Subba and Di Eugenio, 2009; Lin et al., 2009; Luong et al., 2014). A recent prevailing idea f"
D14-1220,P14-1028,0,0.00774173,"× n × n dynamic programming table P r, the cell P r[r, i, j] of which represents the span contained EDUs from i to j and stores the probability that relation r holds between the two spans within i to j. P r[r, i, j] is computed as follows: P r[r, i, j] =maxr1 ,r2 ,k P r[r1 , i, k] · P r[r2 , k, j] • POS at the beginning and end of the EDUs. ×P (tbinary (e[i,k] , e[k,j] ) = 1) • Whether two EDUs are in the same sentence. 5.7 Optimization We use the diagonal variant of AdaGrad (Duchi et al., 2011) with minibatches, which is widely applied in deep learning literature (e.g.,(Socher et al., 2011a; Pei et al., 2014)). The learning rate in AdaGrad is adapted differently for different parameters at different steps. Concretely, let gτi denote the subgradient at time step t for parameter θi obtained from backpropagation, the parameter update at time step t is given by: α p gτi gτi2 t=0 θτ = θτ −1 − Pτ (11) where α denotes the learning rate and is set to 0.01 in our approach. Elements in {Wr }, W , Gbinary , Gmulti , Ubinary , Umulti are initialized by randomly drawing from the uniform distribution [−, ], where  is calculated as suggested in (Collobert et al., 2011). All bias vectors are initialized with 0"
D14-1220,prasad-etal-2008-penn,0,0.114236,"Missing"
D14-1220,D12-1110,0,0.0322764,"this paper is similar to (Ji and Eisenstein, 2014) for transforming the discourse units to the abstract representations. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the R"
D14-1220,D13-1170,0,0.022025,"ions. 2.2 Recursive Deep Learning Recursive neural networks constitute one type of deep learning frameworks which was first proposed in (Goller and Kuchler, 1996). The recursive framework relies and operates on structured inputs (e.g., a parse tree) and computes the representation for each parent based on its children 2062 iteratively in a bottom-up fashion. A series of variations of RNN has been proposed to tailor different task-specific requirements, including MatrixVector RNN (Socher et al., 2012) that represents every word as both a vector and a matrix, or Recursive Neural Tensor Network (Socher et al., 2013) that allows the model to have greater interactions between the input vectors. Many tasks have benefited from the recursive framework, including parsing (Socher et al., 2011b), sentiment analysis (Socher et al., 2013), textual entailment (Bowman, 2013), segmentation (Wang and Mansur, 2013; Houfeng et al., 2013), and paraphrase detection (Socher et al., 2011a). 3 The RST Discourse Treebank There are today two primary alternative discourse treebanks suitable for training data: the Rhetorical Structure Theory Discourse Treebank RSTDT (Carlson et al., 2003) and the Penn Discourse Treebank (Prasad"
D14-1220,N03-1030,0,0.412895,"each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less straightforward to define. Because there are no clause-level ‘parts of discourse’ analogous to word-level parts of speech, there is no discourse-level grammar analogous to sentence-level grammar. To understand how"
D14-1220,H05-1033,0,0.0225916,"s, and larger multi-clause groupings) are tightly connected semantically, syntactically, and logically. Mann and Thompson (1988) define a text to be coherent when it is possible to describe clearly the role that each discourse unit (at any level of grouping) plays with respect to the whole. In a coherent text, no unit is completely isolated. Discourse parsing tries to identify how the units are connected with each other and thereby uncover the hierarchical structure of the text, from which multiple NLP tasks can benefit, including text summarization (Louis et al., 2010), sentence compression (Sporleder and Lapata, 2005) or questionanswering (Verberne et al., 2007). Despite recent progress in automatic discourse segmentation and sentence-level parsing (e.g., (Fisher and Roark, 2007; Joty et al., 2012; Soricut and Marcu, 2003), document-level discourse parsing remains a significant challenge. Recent attempts (e.g., (Hernault et al., 2010b; Feng and Hirst, 2012; Joty et al., 2013)) are still considerably inferior when compared to human goldstandard discourse analysis. The challenge stems from the fact that compared with sentence-level dependency parsing, the set of relations between discourse units is less stra"
D14-1220,N09-1064,0,0.284224,"Missing"
D14-1220,miltsakaki-etal-2004-penn,0,\N,Missing
D14-1220,C04-1048,0,\N,Missing
D14-1220,W01-1605,0,\N,Missing
D14-1220,Q13-1026,0,\N,Missing
D14-1220,D13-1031,0,\N,Missing
D15-1154,W06-2920,0,0.21934,"are generated before outer ones (t). Thus, using already computed “best” labels of inner dependency edges makes us get rid of maximizing over two labels, l0 and l. Moreover, we do not have to extend each incomplete span by the augmentation with a “label” index. This makes the space complexity remains O(n2 ), which is important in practice. The graphical specification is provided in Figure 1 (d). 3 Experiments 3.1 Setup We conduct our experiments on 14 languages, including the English treebank from CoNLL-2008 shared task (Surdeanu et al., 2008) and all 13 treebanks from CoNLL-2006 shared task (Buchholz and Marsi, 2006). We train our parser using The kbest version of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003; Crammer et al., 2006; McDonald, 2006). In our experiments, we set k = 1 and fix the number of iteration to 10, instead of tuning these parameters on development sets. Following previous work, all experiments are evaluated on the metrics of unlabeled attachment score (UAS) and Labeled attachment score (LAS), using the official scorer3 of CoNLL2006 shared task. 3.2 Non-Projective Parsing The parsing algorithms described in the paper fall into the category of projective dependen"
D15-1154,D07-1101,0,0.0209256,"led dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a). A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high. On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004). Previous studies explored the trade-off between computational costs and parsing performance. Some work (McDonald, 2006; Carreras, 2007) simplified labeled information to only single label features. Other work (Johansson and Nugues, 2008; Bohnet, 2010) used richer label features but increased systems’ complexities significantly, while achieving better parsing accuracy. Yet, there are no previous work addressing the problem of good balance between parsing accuracy and computational costs for joint parsing models. In this paper, we propose a new dependency parsing algorithm that can utilize edge-label information of more than one edge, while simultaneously maintaining low computational complexity. The component needed to solve t"
D15-1154,D14-1082,0,0.170857,".65 79.58 76.76 65.02 78.35 94.08 diff 12.10 10.31 5.22 4.32 4.09 3.29 2.60 2.57 2.39 TST 52.23 86.61 72.74 69.86 72.60 64.50 62.42 63.11 90.62 F1 (LAS) Ours 62.15 89.89 77.03 73.14 76.91 68.59 64.61 65.83 93.23 diff 9.92 3.28 4.29 3.28 4.30 4.09 2.19 2.72 2.61 Table 3: Top 10 dependency labels on which our algorithm achieves most improvements on the F1 score of UAS, together with the corresponding improvements of LAS. “TST” indicates the two-stage system. The first column is the label name in the treebank. The second column is the label’s description from Surdeanu et al. (2008). (DNNParser) (Chen and Manning, 2014). The results are listed in Table 2. Clearly, our parser is superior in terms of both UAS and LAS. 3.5 Analysis To better understand the performance of our parser, we analyze the distribution of our parser’s UAS and LAS over different dependency labels on the English CoNLL treebank, compared with the ones of the two-stage model. Table 3 lists the top 10 dependency labels on which our algorithm achieves most improvements on the F1 score of UAS, together with the corresponding improvements of LAS. From Table 3 we can see among the 10 labels, there are 5 labels — “MNR”, “ADV”, “TMP”, “DIR”, “LOC”"
D15-1154,de-marneffe-etal-2006-generating,0,0.0547718,"Missing"
D15-1154,D13-1203,0,0.0761815,"proposed an efficient dependency parsing algorithm that is capable of capturing multiple edge-label features, while maintaining low computational complexity. We evaluate our parser on 14 different languages. Our parser consistently obtains more accurate results than three baseline systems and three popular, off-the-shelf parsers. 1 Introduction Natural language processing (NLP) systems, like machine translation (Xie et al., 2011), resourcelow languages processing (McDonald et al., 2013; Ma and Xia, 2014), word sense disambiguation (Fauceglia et al., 2015) , and entity coreference resolution (Durrett and Klein, 2013), are becoming more sophisticated, in part because of utilizing syntacitc knowledges such as dependency parsing trees. Dependency parsers predict dependency structures and dependency type labels on each edge. However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a). A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high. On the other hand, joint le"
D15-1154,C96-1058,0,0.121569,"arsing model which achieves both effectiveness and efficience. (iii) giving empirical evaluations of this parser on different treebanks over 14 languages. 2 2.1 Joint Parsing Algorithm Basic Notations In the following, x represents a generic input sentence, and y represents a generic dependency tree. Formally, for a sentence x, dependency parsing is the task of finding the dependency tree y with the highest-score for x: y ∗ (x) = argmax Score(x, y). y∈Y(x) (1) Here Y(x) denotes the set of possible dependency trees for sentence x. In this paper, we adopt the second-order sibling factorization (Eisner, 1996; McDonald and 1322 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1322–1328, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Pereira, 2006), in which each sibling part consists of a tuple of indices (h, m, c) where (h, m) and (h, c) are a pair of adjacent edges to the same side of the head h. By adding labele information to this factorization, Score(x, y) can be rewritten as: l l = s = P (h,m,c,l1 ,l2 )∈y P (h,m,c,l1 ,l2 )∈y s t l*(s, r) = r s r r r l*(s, r) = t s t r+1 t s s r t s r t s t t s r s t t s t"
D15-1154,W15-0802,1,0.198554,"ty, which makes joint learning impractical. In this paper, we proposed an efficient dependency parsing algorithm that is capable of capturing multiple edge-label features, while maintaining low computational complexity. We evaluate our parser on 14 different languages. Our parser consistently obtains more accurate results than three baseline systems and three popular, off-the-shelf parsers. 1 Introduction Natural language processing (NLP) systems, like machine translation (Xie et al., 2011), resourcelow languages processing (McDonald et al., 2013; Ma and Xia, 2014), word sense disambiguation (Fauceglia et al., 2015) , and entity coreference resolution (Durrett and Klein, 2013), are becoming more sophisticated, in part because of utilizing syntacitc knowledges such as dependency parsing trees. Dependency parsers predict dependency structures and dependency type labels on each edge. However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a). A two-stage method (McDonald, 2006) is often used because the complexity of some joint lea"
D15-1154,W08-2123,0,0.05721,"Missing"
D15-1154,P10-1001,0,0.308247,"ems, like machine translation (Xie et al., 2011), resourcelow languages processing (McDonald et al., 2013; Ma and Xia, 2014), word sense disambiguation (Fauceglia et al., 2015) , and entity coreference resolution (Durrett and Klein, 2013), are becoming more sophisticated, in part because of utilizing syntacitc knowledges such as dependency parsing trees. Dependency parsers predict dependency structures and dependency type labels on each edge. However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a). A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high. On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004). Previous studies explored the trade-off between computational costs and parsing performance. Some work (McDonald, 2006; Carreras, 2007) simplified labeled information to only single label features. Other work (Johansson"
D15-1154,P14-1126,1,0.814717,"significantly increase computational complexity, which makes joint learning impractical. In this paper, we proposed an efficient dependency parsing algorithm that is capable of capturing multiple edge-label features, while maintaining low computational complexity. We evaluate our parser on 14 different languages. Our parser consistently obtains more accurate results than three baseline systems and three popular, off-the-shelf parsers. 1 Introduction Natural language processing (NLP) systems, like machine translation (Xie et al., 2011), resourcelow languages processing (McDonald et al., 2013; Ma and Xia, 2014), word sense disambiguation (Fauceglia et al., 2015) , and entity coreference resolution (Durrett and Klein, 2013), are becoming more sophisticated, in part because of utilizing syntacitc knowledges such as dependency parsing trees. Dependency parsers predict dependency structures and dependency type labels on each edge. However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a). A two-stage method (McDonald, 2006) is"
D15-1154,C12-2077,1,0.865992,"lation (Xie et al., 2011), resourcelow languages processing (McDonald et al., 2013; Ma and Xia, 2014), word sense disambiguation (Fauceglia et al., 2015) , and entity coreference resolution (Durrett and Klein, 2013), are becoming more sophisticated, in part because of utilizing syntacitc knowledges such as dependency parsing trees. Dependency parsers predict dependency structures and dependency type labels on each edge. However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a). A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high. On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004). Previous studies explored the trade-off between computational costs and parsing performance. Some work (McDonald, 2006; Carreras, 2007) simplified labeled information to only single label features. Other work (Johansson and Nugues, 2008;"
D15-1154,E06-1011,0,0.135573,"s from CoNLL shared tasks, together with three baseline systems and the best systems for each language reported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08 is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a language. Red values represent statistically significant improvements over two-stage baseline system on the corresponding metrics with p < 0.01, using McNemar’s test. Blue values indicate statistically significant improvements with p < 0.05. climbing” non-projective parsing algorithm proposed in McDonald and Pereira (2006). This approximating algorithm first searches the highest scoring projective parse tree and then it rearranges edges in the tree until the rearrangements do not increase the score for the tree anymore 4 . 3.3 Results and Comparison Table 1 illustrates the parsing results our parser with non-projective parsing algorithm, together with three baseline systems—the two-stage system (McDonald, 2006) and the two intermediate models, Model 0 and Model 1—and the best systems reported in CoNLL shared tasks for each language. Our parser achieves better parsing performance on both UAS and LAS than all the"
D15-1154,H05-1066,0,0.527747,"Missing"
D15-1154,W06-2932,0,0.0389807,"74 93.54 91.80 91.54 87.68 84.39 73.74 86.44 83.29 89.94 83.09 75.32 60.39 88.08 81.84 Best in CoNLL UAS LAS System 79.34 66.91 MD06 92.04 87.57 MD06 93.18 89.96 RD06 87.30 80.18 MD06 90.58 84.79 MD06 83.57 79.19 MD06 92.38 90.13 JN08 90.38 87.34 MD06 93.10 91.65 NV06 91.22 87.60 NV06 83.17 73.44 MD06 86.05 82.25 MD06 89.50 84.58 NV06 75.82 65.68 NV06 87.69 82.23 –– Table 1: UAS and LAS of non-projective versions of our parsing algorithms on 14 treebanks from CoNLL shared tasks, together with three baseline systems and the best systems for each language reported in CoNLL shared tasks. MD06 is McDonald et al. (2006), RD06 is Riedel et al. (2006), JN08 is Johansson and Nugues (2008), and NV06 is Nivre et al. (2006) Bold indicates the best result for a language. Red values represent statistically significant improvements over two-stage baseline system on the corresponding metrics with p < 0.01, using McNemar’s test. Blue values indicate statistically significant improvements with p < 0.05. climbing” non-projective parsing algorithm proposed in McDonald and Pereira (2006). This approximating algorithm first searches the highest scoring projective parse tree and then it rearranges edges in the tree until the"
D15-1154,C04-1010,0,0.50354,"redict dependency structures and dependency type labels on each edge. However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a). A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high. On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004). Previous studies explored the trade-off between computational costs and parsing performance. Some work (McDonald, 2006; Carreras, 2007) simplified labeled information to only single label features. Other work (Johansson and Nugues, 2008; Bohnet, 2010) used richer label features but increased systems’ complexities significantly, while achieving better parsing accuracy. Yet, there are no previous work addressing the problem of good balance between parsing accuracy and computational costs for joint parsing models. In this paper, we propose a new dependency parsing algorithm that can utilize edg"
D15-1154,W06-2933,0,0.0662355,"Missing"
D15-1154,W06-2934,0,0.0721693,"Missing"
D15-1154,W08-2121,0,0.296867,"Missing"
D15-1154,J93-2004,0,0.0516197,"Missing"
D15-1154,D11-1020,0,0.0373833,"curacies than two-stage methods. However, classical joint parsing algorithms significantly increase computational complexity, which makes joint learning impractical. In this paper, we proposed an efficient dependency parsing algorithm that is capable of capturing multiple edge-label features, while maintaining low computational complexity. We evaluate our parser on 14 different languages. Our parser consistently obtains more accurate results than three baseline systems and three popular, off-the-shelf parsers. 1 Introduction Natural language processing (NLP) systems, like machine translation (Xie et al., 2011), resourcelow languages processing (McDonald et al., 2013; Ma and Xia, 2014), word sense disambiguation (Fauceglia et al., 2015) , and entity coreference resolution (Durrett and Klein, 2013), are becoming more sophisticated, in part because of utilizing syntacitc knowledges such as dependency parsing trees. Dependency parsers predict dependency structures and dependency type labels on each edge. However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Marti"
D15-1154,P13-2109,0,0.138638,"2011), resourcelow languages processing (McDonald et al., 2013; Ma and Xia, 2014), word sense disambiguation (Fauceglia et al., 2015) , and entity coreference resolution (Durrett and Klein, 2013), are becoming more sophisticated, in part because of utilizing syntacitc knowledges such as dependency parsing trees. Dependency parsers predict dependency structures and dependency type labels on each edge. However, most graph-based dependency parsing algorithms only produce unlabeled dependency trees, particularly when higher-order factorizations are used (Koo and Collins, 2010; Ma and Zhao, 2012b; Martins et al., 2013; Ma and Zhao, 2012a). A two-stage method (McDonald, 2006) is often used because the complexity of some joint learning models is unacceptably high. On the other hand, joint learning models can benefit from edge-label information that has proven to be important to provide more accurate tree structures and labels (Nivre and Scholz, 2004). Previous studies explored the trade-off between computational costs and parsing performance. Some work (McDonald, 2006; Carreras, 2007) simplified labeled information to only single label features. Other work (Johansson and Nugues, 2008; Bohnet, 2010) used rich"
D15-1154,D08-1059,0,0.113916,"Missing"
D15-1154,P11-2033,0,0.0932076,"Missing"
D15-1154,P13-2017,0,\N,Missing
D15-1278,P14-2009,0,0.0279958,"based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direc"
D15-1278,D13-1137,0,0.036062,"bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the par"
D15-1278,W09-2415,0,0.0297319,"Missing"
D15-1278,D14-1070,0,0.0537703,"Missing"
D15-1278,D13-1176,0,0.00928056,"ly important, and if so for which tasks, or whether other issues are at play. Indeed, the reliance of recursive models on parsing is also a potential disadvantage, given that parsing is relatively slow, domain-dependent, and can be errorful. On the other hand, recent progress in multiple subfields of neural NLP has suggested that recurrent nets may be sufficient to deal with many of the tasks for which recursive models have been proposed. Recurrent models without parse structures have shown good results in sequenceto-sequence generation (Sutskever et al., 2014) for machine translation (e.g., (Kalchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase"
D15-1278,D14-1218,1,0.0712307,"parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but they are adjacent in the parse tree (Irsoy and Cardie, 2013). H"
D15-1278,D14-1220,1,0.925709,"3): comprehensive labels are found for words and phrases where local compositionally (such as from negation, mood, or others cued by phrase-structure) is to be learned. • Sentence-Target Matching on the UMDQA dataset (Iyyer et al., 2014): Learns matches between target and components in the source sentences, which are parse tree nodes for recursive models and different time-steps for recurrent models. • Semantic Relation Classification on the SemEval-2010 task (Hendrickx et al., 2009). Learns long-distance relationships between two words that may be far apart sequentially. • Discourse Parsing (Li et al., 2014; Hernault et al., 2010): Learns sentence-to-sentence relations based on calculated representations. In each case we followed the protocols described in the original papers. We first group the algorithm variants into two groups as follows: • Standard tree models vs standard sequence models vs standard bi-directional sequence models sentences with 215,154 phrases, the reconstructed dataset for recurrent models comprises 215,154 examples. Models are evaluated at both the phrase level (82,600 instances) and the sentence root level (2,210 instances). Tree Sequence P-value Bi-Sequence P-value • LST"
D15-1278,N13-1090,0,0.0122332,"ks (like semantic relation extraction) that require longdistance connection modeling, particularly on very long sequences. We then introduce a method for allowing recurrent models to achieve similar performance: breaking long sentences into clause-like units at punctuation and processing them separately before combining. Our results thus help understand the limitations of both classes of models, and suggest directions for improving recurrent models. 1 Introduction Deep learning based methods learn lowdimensional, real-valued vectors for word tokens, mostly from large-scale data corpus (e.g., (Mikolov et al., 2013; Le and Mikolov, 2014; Collobert et al., 2011)), successfully capturing syntactic and semantic aspects of text. For tasks where the inputs are larger text units (e.g., phrases, sentences or documents), a compositional model is first needed to aggregate tokens into a vector with fixed dimensionality that can be used as a feature for other NLP tasks. Models for achieving this usually fall into two categories: recurrent models and recursive models: Recurrent models (also referred to as sequence models) deal successfully with time-series data (Pearlmutter, 1989; Dorffner, 1996) like speech (Robin"
D15-1278,W02-1011,0,0.0304553,"alchbrenner and Blunsom, 2013; 3; Luong et al., 2014)), parsing (Vinyals et al., 2014), and sentiment, where for example recurrent-based paragraph vectors (Le and Mikolov, 2014) outperform recursive models (Socher et al., 2013) on the Stanford sentimentbank dataset. Our goal in this paper is thus to investigate a number of tasks with the goal of understanding for which kinds of problems recurrent models may be sufficient, and for which kinds recursive models offer specific advantages. We investigate four tasks with different properties. • Binary sentiment classification at the sentence level (Pang et al., 2002) and phrase level (Socher et al., 2013) that focus on understanding the role of recursive models in dealing with semantic compositionally in various scenarios such as different lengths of inputs and whether or not supervision is comprehensive. • Phrase Matching on the UMD-QA dataset (Iyyer et al., 2014) can help see the difference between outputs from intermediate components from different models, i.e., representations for intermediate parse tree nodes and outputs from recurrent models at different time steps. It also helps see whether parsing is useful for finding similarities between questio"
D15-1278,D13-1170,0,0.526172,"uentially, recursive models combine neighbors based on the recursive structure of parse trees, starting from the leaves and proceeding recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For"
D15-1278,D11-1014,0,0.164886,"pacted into one component, and the error propagation is thus given by: error→ second-clause → first-clause → was→plot→the→as→simple. Propagation with clause segmentation consists of only 8 operations. Such a procedure thus tends to attenuate the gradient vanishing problem, potentially yielding better performance. 3.2 Binary Sentiment Classification (Pang) Task Description: The sentiment dataset of Pang et al. (2002) consists of sentences with a sentiment label for each sentence. We divide the original dataset into training(8101)/dev(500)/testing(2000). No pretraining procedure as described in Socher et al. (2011b) is employed. Word embeddings are initialized using skip-grams and kept fixed in the learning procedure. We trained skip-gram embeddings on the Wikipedia+Gigaword dataset using the word2vec package4 . Sentence level embeddings are fed into a sigmoid classifier. Performances for 50 dimensional vectors are given in the table below: Discussion Why don’t parse trees help on this task? One possible explanation is the distance 4 https://code.google.com/p/word2vec/ of the supervision signal from the local compositional structure. The Pang et al. dataset has an average sentence length of 22.5 words,"
D15-1278,D12-1110,0,0.765115,"ing recursively in a bottom-up fashion until the root of the parse tree is reached. For example, for the phrase the food is delicious, following the operation sequence ( (the food) (is delicious) ) rather than the sequential order (((the food) is) delicious). Many recursive models have been proposed (e.g., (Paulus et al., 2014; Irsoy and Cardie, 2014)), and applied to various NLP tasks, among them entailment (Bowman, 2013; Bowman et al., 2014), sentiment analysis (Socher et al., 2013; Irsoy and Cardie, 2013; Dong et al., 2014), question-answering (Iyyer et al., 2014), relation classification (Socher et al., 2012; Hashimoto et al., 2013), and discourse (Li and Hovy, 2014). 2304 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2304–2314, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. One possible advantage of recursive models is their potential for capturing long-distance dependencies: two tokens may be structurally close to each other, even though they are far away in word sequence. For example, a verb and its corresponding direct object can be far away in terms of tokens if many adjectives lies in between, but the"
D15-1278,W01-1605,0,\N,Missing
D15-1278,P15-1150,0,\N,Missing
D15-1278,P15-1002,1,\N,Missing
D15-1278,D14-1162,0,\N,Missing
D15-1284,W09-2004,0,0.0176664,"cation performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorou"
D15-1284,P11-2016,0,0.0936674,"ics. biguity, interpersonal effect and phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognition techniques, where they"
D15-1284,N15-1070,1,0.765615,"ing distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory Ambiguity (Bucaria, 2004), the disambiguation of words with multiple meanings (Bekinschtein et al., 2011), is a crucial component of many humor jokes (Miller and Gurevych, 2015). Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another 5 https://code.google.com/p/word2vec/ We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vectors associated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task. 2369 6 meaning. Ambiguity occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whole left side was cut off? He’s all right now. The multiple possible meanings of words provide readers with different understandings. To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet (Fellbaum, 1998) and capture the ambiguity as follows: • Sense Combination: the sense"
D15-1284,P12-2030,0,0.0360941,"stances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devoted to produce humorous acronyms mainly by exploiting incongruity theories (Stock and Strapparava, 2003). In contrast to research on humor recognition and generation, there are few studies"
D15-1284,H05-1067,0,0.36465,"d phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognition techniques, where they learned statistical patterns of text in N-gra"
D15-1284,P15-1070,0,0.0619138,"ncongruity is hard to achieve, however, it is relatively easier to measure the semantic disconnection in a sentence. Taking advantage of Word2Vec5 , we extract two types of features to evaluate the meaning distance6 between content word pairs in a sentence (Mikolov et al., 2013): • Disconnection: the maximum meaning distance of word pairs in a sentence. • Repetition: the minimum meaning distance of word pairs in a sentence. 4.2 Ambiguity Theory Ambiguity (Bucaria, 2004), the disambiguation of words with multiple meanings (Bekinschtein et al., 2011), is a crucial component of many humor jokes (Miller and Gurevych, 2015). Humor and ambiguity often come together when a listener expects one meaning, but is forced to use another 5 https://code.google.com/p/word2vec/ We take the generic Word2Vec vectors without training new vectors for our specific domain. In addition, vectors associated with senses (Kumar Jauhar et al., 2015) might be alternative advantageous in this task. 2369 6 meaning. Ambiguity occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whol"
D15-1284,W06-1625,0,0.213259,"n for Computational Linguistics. biguity, interpersonal effect and phonetic style. For each latent structure, we design a set of features to capture the potential indicators of humor. With high classification accuracy, we then extract humor anchors in sentences via a simple and effective method. Both quantitative and qualitative experimental results are provided to validate the classification and anchor extraction performance. 2 Related Work Most existing studies on humor recognition are formulated as a binary classification problem and try to recognize jokes via a set of linguistic features (Purandare and Litman, 2006; Kiddon and Brun, 2011). For example, Mihalcea and Strapparava (2005) defined three types of humorspecific stylistic features: Alliteration, Antonym and Adult Slang, and trained a classifier based on these feature representations. Similarly, Zhang and Liu (2014) designed several categories of humor-related features, derived from influential humor theories, linguistic norms, and affective dimensions, and input around fifty features into the Gradient Boosting Regression Tree model for humor recognition. Taylor and Mazlack (2004) recognized wordplay jokes based on statistical language recognitio"
D15-1284,N12-2012,0,0.0706261,"First, a universal definition of humor is hard to achieve, because different people hold different understandings of even the same sentence. Second, humor is always situated in a broader context that sometimes requires a lot of external knowledge to fully understand it. For example, consider the sentence, “The one who invented the door knocker got a No Bell prize” and “Veni, Vidi, Visa: I came, I saw, I did a little shopping”. One needs a larger cultural context to figure out the subtle humorous meaning expressed in these two sentences. Last but not least, there are different types of humor (Raz, 2012), such as wordplay, irony and sarcasm, but there exist few formal taxonomies of humor characteristics. Thus it is almost impossible to design a general algorithm that can classify all the different types of humor, since even human cannot perfectly classify all of them. Although it is impossible to understand universal humor characteristics, one can still capture the possible latent structures behind humor (Bucaria, 2004; Binsted and Ritchie, 1997). In this work, we uncover several latent semantic structures behind humor, in terms of meaning incongruity, ambiguity, phonetic style and personal a"
D15-1284,W05-1614,0,0.109392,"e high classification performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and generator) devot"
D15-1284,P05-3029,0,0.0413941,"r positive instances will have high classification performance, but are not necessarily good classifiers. There are few existing benchmark datasets for humor recognition and most studies select negative instances specifically. For example, Mihalcea and Strapparava (2005) constructed the set of negative examples by using news title from Reuters news, proverbs and British National Corpus. (Zhang, el. al 2014) randomly sampled 1500 tweets and then asked annotators to filter out humorous tweets. Compared to humor recognition, humor generation has received quite a lot attention in the past decades(Stock and Strapparava, 2005; Ritchie, 2005; Hong and Ong, 2009). Most generation work draws on humor theories to account for humor factors, such as the Script-based Semantic Theory of Humor (Raskin, 1985; Labutov and Lipson, 2012) and employs templates to generate jokes. For example, Ozbal and Strapparava (2012) created humorous neologism using WordNet and ConceptNet. In detail, their system combined several linguistic resources to generate creative names, more specifically neologisms based on homophonic puns and metaphors. Stock and Strapparava (2005) introduced HAHACRONYM, a system (an acronym ironic re-analyzer and g"
D15-1284,N03-1033,0,0.0277565,"occurs when the words of the surface sentence structure can be grouped in more than one way, thus yielding more than one associated deep structures, as shown in the example below. Did you hear about the guy whose whole left side was cut off? He’s all right now. The multiple possible meanings of words provide readers with different understandings. To capture the ambiguity contained in a sentence, we utilize the lexical resource WordNet (Fellbaum, 1998) and capture the ambiguity as follows: • Sense Combination: the sense combination in a sentence computed as follows: we first use a POS tagger (Toutanova et al., 2003) to identify Noun, Verb, Adj, Adv. Then we consider the possible meanings of such words {w1 , w2 · · · wk } via WordNet and Qkcalculate the sense combinations as log( i=1 nwi ). nwi is the total number of senses of word wi . • Sense Farmost: the largest Path Similarity7 of any word senses in a sentence. • Sense Closest: the smallest Path Similarity of any word senses in a sentence. 4.3 Interpersonal Effect Besides humor theories and linguistic style modeling, one important theory behind humor is its social/hostility focus, especially regarding its interpersonal effect on receivers. That is, hu"
D15-1284,P06-1134,0,0.0105404,"ds {w1 , w2 · · · wk } via WordNet and Qkcalculate the sense combinations as log( i=1 nwi ). nwi is the total number of senses of word wi . • Sense Farmost: the largest Path Similarity7 of any word senses in a sentence. • Sense Closest: the smallest Path Similarity of any word senses in a sentence. 4.3 Interpersonal Effect Besides humor theories and linguistic style modeling, one important theory behind humor is its social/hostility focus, especially regarding its interpersonal effect on receivers. That is, humor is essentially associated with sentiment (Zhang and Liu, 2014) and subjectivity (Wiebe and Mihalcea, 2006). For example, a sentence is likely to be humorous if it contains some words carrying strong sentiment, such as ‘idiot’ as follows. Your village called. They want their Idiot back. Each word is associated with positive or negative sentiments and such measurements reflect the emotion expressed by the writer. To identify the word-associated sentiment, we use the word association resource in the work by (Wilson et al., 2005), which provides annotations and clues to measure the subjectivity and sentiment associated with words. This enables us to design the following features. • Negative (Positive)"
D15-1284,H05-1044,0,0.031023,"tility focus, especially regarding its interpersonal effect on receivers. That is, humor is essentially associated with sentiment (Zhang and Liu, 2014) and subjectivity (Wiebe and Mihalcea, 2006). For example, a sentence is likely to be humorous if it contains some words carrying strong sentiment, such as ‘idiot’ as follows. Your village called. They want their Idiot back. Each word is associated with positive or negative sentiments and such measurements reflect the emotion expressed by the writer. To identify the word-associated sentiment, we use the word association resource in the work by (Wilson et al., 2005), which provides annotations and clues to measure the subjectivity and sentiment associated with words. This enables us to design the following features. • Negative (Positive) Polarity: the number of occurrences of all Negative (Positive) words. 7 Path Similarity: http://www.nltk.org/howto/ wordnet.html • Weak (Strong) Subjectivity: the number of occurrences of all Weak (Strong) Subjectivity oriented words in a sentence. It is the linguistic expression of people’s opinions, evaluations, beliefs or speculations. 4.4 Phonetic Style Many humorous texts play with sounds, creating incongruous sound"
D15-1284,P12-1074,0,\N,Missing
D17-1082,P16-1223,0,0.105417,"u/ ˜glai1/data/race/ and the code is available at https://github.com/ qizhex/RACE_AR_baselines 1 Introduction Constructing an intelligence agent capable of understanding text as people is the major challenge of NLP research. With recent advances in deep learning techniques, it seems possible to achieve human-level performance in certain language understanding tasks, and a surge of effort has been devoted to the machine comprehension task where people aim to construct a system with the ability to ⇤ * indicates equal contribution answer questions related to a document that it has to comprehend (Chen et al., 2016; Kadlec et al., 2016; Dhingra et al., 2016; Yang et al., 2017). Towards this goal, several large-scale datasets (Rajpurkar et al., 2016; Onishi et al., 2016; Hill et al., 2015; Trischler et al., 2016; Hermann et al., 2015) have been proposed, which allow researchers to train deep learning systems and obtain results comparable to the human performance. While having a suitable dataset is crucial for evaluating the system’s true ability in reading comprehension, the existing datasets suffer several critical limitations. Firstly, in all datasets, the candidate options are directly extracted from"
D17-1082,P16-1086,0,0.125437,"Missing"
D17-1082,N03-1020,1,0.295683,"Missing"
D17-1082,D16-1241,0,0.121052,"understanding text as people is the major challenge of NLP research. With recent advances in deep learning techniques, it seems possible to achieve human-level performance in certain language understanding tasks, and a surge of effort has been devoted to the machine comprehension task where people aim to construct a system with the ability to ⇤ * indicates equal contribution answer questions related to a document that it has to comprehend (Chen et al., 2016; Kadlec et al., 2016; Dhingra et al., 2016; Yang et al., 2017). Towards this goal, several large-scale datasets (Rajpurkar et al., 2016; Onishi et al., 2016; Hill et al., 2015; Trischler et al., 2016; Hermann et al., 2015) have been proposed, which allow researchers to train deep learning systems and obtain results comparable to the human performance. While having a suitable dataset is crucial for evaluating the system’s true ability in reading comprehension, the existing datasets suffer several critical limitations. Firstly, in all datasets, the candidate options are directly extracted from the context (as a single entity or a text span), which leads to the fact that lots of questions can be solved trivially via word-based search and context-mat"
D17-1082,P02-1040,0,0.105281,"Missing"
D17-1082,D14-1162,0,0.104838,"refer readers to (Dhingra et al., 2016) for more details. After obtaining a query specific document representation sd , we use the same method as bilinear operation listed in Equation 2 to get the output. Note that our implementation slightly differs from the original GA reader. Specifically, the Attention Sum layer is not applied at the final layer and no character-level embeddings are used. Implementation Details We follow Chen et al. (2016) in our experiment settings. The vocabulary size is set to 50k. We choose word embedding size d = 100 and use the 100-dimensional Glove word embedding (Pennington et al., 2014) as embedding initialization. GRU weights are initialized from Gaussian distribution N (0, 0.1). Other parameters are initialized from a uniform distribution on ( 0.01, 0.01). The hidden dimensionality is set to 128 and the number of layers is set to one for both Stanford AR and GA. We use vanilla stochastic gradient descent (SGD) to train our models. We apply dropout on word embeddings and the gradient is clipped when the norm 791 of the gradient is larger than 10. We use a grid search on validation set to choose the learning rate within {0.05, 0.1, 0.3, 0.5} and dropout rate within {0.2, 0.5"
D17-1082,D16-1264,0,0.418418,"ligence agent capable of understanding text as people is the major challenge of NLP research. With recent advances in deep learning techniques, it seems possible to achieve human-level performance in certain language understanding tasks, and a surge of effort has been devoted to the machine comprehension task where people aim to construct a system with the ability to ⇤ * indicates equal contribution answer questions related to a document that it has to comprehend (Chen et al., 2016; Kadlec et al., 2016; Dhingra et al., 2016; Yang et al., 2017). Towards this goal, several large-scale datasets (Rajpurkar et al., 2016; Onishi et al., 2016; Hill et al., 2015; Trischler et al., 2016; Hermann et al., 2015) have been proposed, which allow researchers to train deep learning systems and obtain results comparable to the human performance. While having a suitable dataset is crucial for evaluating the system’s true ability in reading comprehension, the existing datasets suffer several critical limitations. Firstly, in all datasets, the candidate options are directly extracted from the context (as a single entity or a text span), which leads to the fact that lots of questions can be solved trivially via word-based s"
D17-1082,D13-1020,0,0.323127,"exams were designed by domain experts (instructors) for evaluating the reading comprehension ability of students, with ensured quality and broad topic coverage. Furthermore, the answers by machines or by humans can be objectively graded for evaluation 785 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 785–794 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics and comparison using the same evaluation metrics. Although efforts have been made with a similar motivation, including the MCTest dataset created by (Richardson et al., 2013) (containing 500 passages and 2000 questions) and several others (Pe˜nas et al., 2014; Rodrigo et al., 2015; Khashabi et al., 2016; Shibuki et al., 2014), the usefulness of those datasets is significantly restricted due to their small sizes, especially not suitable for training powerful deep neural networks whose success relies on the availability of relatively large training sets. Our new dataset, namely RACE, consists of 27,933 passages and 97,687 questions. After reading each passage, each student is asked to answer several questions where each question is provided with four candidate answe"
D17-1082,P17-1096,0,0.0250147,"Missing"
D17-1213,E12-1036,0,0.754677,"isions to collect relevant edits for sentence simplification. Max and Wisniewski (2010) constructed a corpus of rewritings that can be used for spelling errors and paraphrases (Zesch, 2012). Similarly, Zanzotto and Pennacchiotti (2010) used edits as training data for textual entailment recognition, and Recasens et al. (2013) analyzed real instances of human edits designed to remove bias from Wikipedia articles. Most of these work employed manually defined rules or filters to collect relevant edits to the NLP task at hand. Towards analyzing revisions and developing unified revision taxonomies (Bronner and Monz, 2012; Liu and Ram, 2011), Fong and Biuk-Aghai (2010) built machine learning models to distinguish between factual and fluency edits in revision histories. Faigley and Witte (1981) made a distinction between changes that affect meaning, called text-base changes and changes which do not affect meaning, called surface changes. The two categories are further divided into formal changes, meaning-preserving changes, microstructure changes and macro-structure changes. This taxonomy was later extended by Jones (2008) to take into account edit categories such as significant deletion, style, image insertion"
D17-1213,C12-1044,0,0.783679,"al., 2010), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), language bias detection (Recasens et al., 2013), spelling errors and paraphrases (Zesch, 2012; Max and Wisniewski, 2010). To avoid building different approaches to extract the information needed by different NLP tasks (Ferschke et al., 2013), a unified framework to recognize edits from revisions is needed. Prior research on revision editing primarily develop syntactic edit action categories, from which they try to understand the effects of edits on meaning (Faigley and Witte, 1981; Yang et al., 2016). For instance, Daxenberger and Gurevych (2012) categorized edits based on whether edits affect the text meaning, resulting in syntactic edit categories such as file deletion, reference modification, etc. However, simply understanding the syntactic revision operation types does not provide the information we seek: why do editors do what they do? how effective are their actions? For example, syntactic edit type taxonomies cannot tell the difference between simplifying a paragraph and maliciously damaging that paragraph, since both involve deleting a sentence. In this work, we focus explicitly on revision intention. We introduce a fine-grain"
D17-1213,D13-1055,0,0.232512,"ffective are their actions? For example, syntactic edit type taxonomies cannot tell the difference between simplifying a paragraph and maliciously damaging that paragraph, since both involve deleting a sentence. In this work, we focus explicitly on revision intention. We introduce a fine-grained taxonomy of the reasons why an author in Wikipedia made an edit. Example edit intentions include copy editing, elaboration, verification, and simplification. Compared to taxonomies that either focus on low-level syntactic operations (Faigley and Witte, 1981) or that mix syntactic and semantic classes (Daxenberger and Gurevych, 2013), a clean higher-level semantic categorization enables us to easily identify textual meaning changes, and to connect revisions to “what happens in the mind of the revising author during the revision” 2000 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2000–2010 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics (Fitzgerald, 1987; Daxenberger, 2016). In order to capture the meaning behind edits, we worked with 13 Wikipedians to build a taxonomy that captured the meaning of an revision, which we term edit int"
D17-1213,P09-2044,0,0.072332,"ion and article quality. Specifically, we examined whether edit intentions in newcomers’ first editing sessions predict their retention, and examined how edits with different intentions lead to changes in article quality. These analyses showed that specific types of editing work were positively correlated with newcomer survival and articles in different stages of development benefited differently from different types of edits. 2 Related Work Wikipedia revision histories have been used for a wide range of NLP tasks (Yamangil and Nelken, 2008; Aji et al., 2010; Zanzotto and Pennacchiotti, 2010; Ganter and Strube, 2009; Nelken and Yamangil, 2008). For instance, Yatskar et al. (2010) used Wikipedia comments associated with revisions to collect relevant edits for sentence simplification. Max and Wisniewski (2010) constructed a corpus of rewritings that can be used for spelling errors and paraphrases (Zesch, 2012). Similarly, Zanzotto and Pennacchiotti (2010) used edits as training data for textual entailment recognition, and Recasens et al. (2013) analyzed real instances of human edits designed to remove bias from Wikipedia articles. Most of these work employed manually defined rules or filters to collect rel"
D17-1213,P11-2015,0,0.0281516,"In terms of revision intentions, Zhang and Litman (2016) incorporated both argumentative writing features and surface changes from Faigley and Witte (1981) and constructed eight categories of revision purposes, such as claims/ideas, warrant/reasoning/backing, rebuttal/reservation, organization, clarify, etc. Tan and Lee (2014) used revisions to understand statement strength in academic writings. There are multiple works on the detection of specific subsets of revision intentions in Wikipedia, such as vandalism detection where the goal is to classify revisions as vandalized or non-vandalized (Harpalani et al., 2011; Adler et al., 2011) and language bias/neutral point of view detection (Recasens et al., 2013). Instead of recognizing a specific type of revision intention each time, our work aims at designing a systematic and comprehensive edit intention taxonomy to capture intentions behind textual changes. Prior work also used edit types and intentions to better understand the process of collaborative writing, such as article quality improvement (Kittur and Kraut, 2008). For example, Liu and Ram (2011) found that Wikipedia article quality correlates with different types of contributors; similarly Yang et"
D17-1213,max-wisniewski-2010-mining,0,0.605119,"vides an amazing corpus for studying the types and effectiveness of revisions. Specifically, differences between revisions contain valuable information for modeling document quality or extracting users’ expertise, and can additionally support various natural language processing (NLP) tasks such as sentence compression (Yamangil and Nelken, 2008), lexical simplification (Yatskar et al., 2010), information retrieval (Aji et al., 2010), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), language bias detection (Recasens et al., 2013), spelling errors and paraphrases (Zesch, 2012; Max and Wisniewski, 2010). To avoid building different approaches to extract the information needed by different NLP tasks (Ferschke et al., 2013), a unified framework to recognize edits from revisions is needed. Prior research on revision editing primarily develop syntactic edit action categories, from which they try to understand the effects of edits on meaning (Faigley and Witte, 1981; Yang et al., 2016). For instance, Daxenberger and Gurevych (2012) categorized edits based on whether edits affect the text meaning, resulting in syntactic edit categories such as file deletion, reference modification, etc. However, s"
D17-1213,P13-1162,0,0.38333,"revisions growing at a rate of about 2 revisions per second. This provides an amazing corpus for studying the types and effectiveness of revisions. Specifically, differences between revisions contain valuable information for modeling document quality or extracting users’ expertise, and can additionally support various natural language processing (NLP) tasks such as sentence compression (Yamangil and Nelken, 2008), lexical simplification (Yatskar et al., 2010), information retrieval (Aji et al., 2010), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), language bias detection (Recasens et al., 2013), spelling errors and paraphrases (Zesch, 2012; Max and Wisniewski, 2010). To avoid building different approaches to extract the information needed by different NLP tasks (Ferschke et al., 2013), a unified framework to recognize edits from revisions is needed. Prior research on revision editing primarily develop syntactic edit action categories, from which they try to understand the effects of edits on meaning (Faigley and Witte, 1981; Yang et al., 2016). For instance, Daxenberger and Gurevych (2012) categorized edits based on whether edits affect the text meaning, resulting in syntactic edit"
D17-1213,P14-2066,0,0.631392,"multi-label classification to extract edit categories based on unparsed source text (Daxenberger and Gurevych, 2012). However, most taxonomies of edit categories contain only syntactic actions or a mixture of syntactic and semantic actions, failing to capturing the intention of revisions. In terms of revision intentions, Zhang and Litman (2016) incorporated both argumentative writing features and surface changes from Faigley and Witte (1981) and constructed eight categories of revision purposes, such as claims/ideas, warrant/reasoning/backing, rebuttal/reservation, organization, clarify, etc. Tan and Lee (2014) used revisions to understand statement strength in academic writings. There are multiple works on the detection of specific subsets of revision intentions in Wikipedia, such as vandalism detection where the goal is to classify revisions as vandalized or non-vandalized (Harpalani et al., 2011; Adler et al., 2011) and language bias/neutral point of view detection (Recasens et al., 2013). Instead of recognizing a specific type of revision intention each time, our work aims at designing a systematic and comprehensive edit intention taxonomy to capture intentions behind textual changes. Prior work"
D17-1213,P08-2035,0,0.64902,"intain a history of revisions made by millions of participants. As Wikipedia statistics as of January 2017 show, English Wikipedia has 5.3 million articles with an average of 162.89 revisions per article, with revisions growing at a rate of about 2 revisions per second. This provides an amazing corpus for studying the types and effectiveness of revisions. Specifically, differences between revisions contain valuable information for modeling document quality or extracting users’ expertise, and can additionally support various natural language processing (NLP) tasks such as sentence compression (Yamangil and Nelken, 2008), lexical simplification (Yatskar et al., 2010), information retrieval (Aji et al., 2010), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), language bias detection (Recasens et al., 2013), spelling errors and paraphrases (Zesch, 2012; Max and Wisniewski, 2010). To avoid building different approaches to extract the information needed by different NLP tasks (Ferschke et al., 2013), a unified framework to recognize edits from revisions is needed. Prior research on revision editing primarily develop syntactic edit action categories, from which they try to understand the effects o"
D17-1213,N10-1056,0,0.565387,"rticipants. As Wikipedia statistics as of January 2017 show, English Wikipedia has 5.3 million articles with an average of 162.89 revisions per article, with revisions growing at a rate of about 2 revisions per second. This provides an amazing corpus for studying the types and effectiveness of revisions. Specifically, differences between revisions contain valuable information for modeling document quality or extracting users’ expertise, and can additionally support various natural language processing (NLP) tasks such as sentence compression (Yamangil and Nelken, 2008), lexical simplification (Yatskar et al., 2010), information retrieval (Aji et al., 2010), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), language bias detection (Recasens et al., 2013), spelling errors and paraphrases (Zesch, 2012; Max and Wisniewski, 2010). To avoid building different approaches to extract the information needed by different NLP tasks (Ferschke et al., 2013), a unified framework to recognize edits from revisions is needed. Prior research on revision editing primarily develop syntactic edit action categories, from which they try to understand the effects of edits on meaning (Faigley and Witte, 1981; Ya"
D17-1213,E12-1054,0,0.728182,"ond. This provides an amazing corpus for studying the types and effectiveness of revisions. Specifically, differences between revisions contain valuable information for modeling document quality or extracting users’ expertise, and can additionally support various natural language processing (NLP) tasks such as sentence compression (Yamangil and Nelken, 2008), lexical simplification (Yatskar et al., 2010), information retrieval (Aji et al., 2010), textual entailment recognition (Zanzotto and Pennacchiotti, 2010), language bias detection (Recasens et al., 2013), spelling errors and paraphrases (Zesch, 2012; Max and Wisniewski, 2010). To avoid building different approaches to extract the information needed by different NLP tasks (Ferschke et al., 2013), a unified framework to recognize edits from revisions is needed. Prior research on revision editing primarily develop syntactic edit action categories, from which they try to understand the effects of edits on meaning (Faigley and Witte, 1981; Yang et al., 2016). For instance, Daxenberger and Gurevych (2012) categorized edits based on whether edits affect the text meaning, resulting in syntactic edit categories such as file deletion, reference mo"
D17-1213,N16-1168,0,0.492916,"l et al. (2006) proposed a 13-category taxonomy based on the data and performed manual annotation to compare cultural differences in the writing process in different versions of Wikipedia. Daxenberger and Gurevych (2013) introduced a finer-grained edit taxonomy, and performed multi-label classification to extract edit categories based on unparsed source text (Daxenberger and Gurevych, 2012). However, most taxonomies of edit categories contain only syntactic actions or a mixture of syntactic and semantic actions, failing to capturing the intention of revisions. In terms of revision intentions, Zhang and Litman (2016) incorporated both argumentative writing features and surface changes from Faigley and Witte (1981) and constructed eight categories of revision purposes, such as claims/ideas, warrant/reasoning/backing, rebuttal/reservation, organization, clarify, etc. Tan and Lee (2014) used revisions to understand statement strength in academic writings. There are multiple works on the detection of specific subsets of revision intentions in Wikipedia, such as vandalism detection where the goal is to classify revisions as vandalized or non-vandalized (Harpalani et al., 2011; Adler et al., 2011) and language"
D17-1292,P98-1013,0,0.271606,"ust. However, it is difficult to understand how the statistically verified factors actually cause the changes, and whether there is a latent causal structure relating the two. This paper addresses the challenge of finding such latent causal structures, in the form of causal explanations that connect the given cause-effect pair. Table 1 shows example causal explanation that our system found between party and Facebook’s stock fall (↓). To construct a general causal graph, we extract all potential causal expressions from a large corpus of text. We refer to this graph as CG RAPH. We use FrameNet (Baker et al., 1998) semantics to provide various causative expressions (verbs, relations, and patterns), which we apply to a resource of 183, 253, 995 sentences of text and tweets. These expressions are considerably richer than previous rule-based patterns (Riaz and Girju, 2013; Kozareva, 2012). CG RAPH contains 5,025,636 causal edges. Our experiment demonstrates that our causality detection algorithm outperforms other baseline methods for forecasting future time series values. Also, we tested the neural reasoner on the inference generation task using the BLEU score. Additionally, our human evaluation shows the"
D17-1292,bethard-etal-2008-building,0,0.0220497,"reva, 2012; Do et al., 2011) with additional features (Blanco et al., 2008). (Kozareva, 2012) extracted cause-effect relations, where the verb pattern for bootstrapping has a form of X ∗ 7−−∗→ Z Y from which terms X ∗ and Z ∗ was learned. The syntax based approaches, however, are not robust to semantic variation. As a part of SemEval (Girju et al., 2007), (Mirza and Tonelli, 2016) also uses syntactic causative patterns (Mirza and Tonelli, 2014) and supervised classifier to achieve the state-of-the-art performance. Extracting the cause-effect tuples with such syntactic features or temporality (Bethard et al., 2008) would be our next step for better causal graph construction. (Grivaz, 2010) conducts very insightful annotation study of what features are used in human reasoning on causation. Beyond the linguistic tests and causal chains for explaining causality in our work, other features such as counterfactuality, temporal order, and ontological asymmetry remain as our future direction to study. Textual entailment also seeks a directional relation between two given text fragments (Dagan et al., 2006). Recently, (Rockt¨aschel et al., 2015) developed an attention-based neural network method, trained on larg"
D17-1292,blanco-etal-2008-causal,0,0.84299,"y), then e1 7→ e2 , ... 7→ et might be a good causal explanation between x and y. Section 3 and 4 describe how to bridge the causal gap between given events (x, y) by (1) constructing a large general cause-effect graph (CG RAPH) from text, (2) linking the given events to their equivalent entities in the causal graph by finding the internal paths (x 7→ e1 , ...et 7→ y) as causal explanations, using neural algorithms. CG RAPH is a knowledge base graph where edges are directed and causally related between entities. To address less representational variability of rule based methods (Girju, 2003; Blanco et al., 2008; Sharp et al., 2016) in the causal graph construction, we used FrameNet (Baker et al., 1998) semantics. Using a semantic parser such 2760 Table 2: Example (relation, cause, effect) tuples in different categories (manually labeled): general, company, country, and people. FrameNet labels related to causation are listed inside parentheses. The number of distinct relation types are 892. General causes (Causation) cause (Causation) forced (Causation) the virus (Cause) greenhouse gases (Cause) the reality of world war ii (Cause) Company heats (Cause temperature change) promotes (Cause change of pos"
D17-1292,D15-1075,0,0.0359591,"sts and causal chains for explaining causality in our work, other features such as counterfactuality, temporal order, and ontological asymmetry remain as our future direction to study. Textual entailment also seeks a directional relation between two given text fragments (Dagan et al., 2006). Recently, (Rockt¨aschel et al., 2015) developed an attention-based neural network method, trained on large annotated pairs of textual entailment, for classifying the types of relations with decomposable attention (Parikh et al., 2016) or sequential tree structure (Chen et al., 2016). However, the dataset (Bowman et al., 2015) used for training entailment deals with just three categories, contradiction, neutral, and entailment, and focuses on relatively simple lexical and syntactic transformations (Kolesnyk et al., 2016). Our causal explanation generation task is also similar to future scenario generation (Hashimoto et al., 2014, 2015). Their scoring 2765 Table 8: Example causal chains for explaining the rise (↑) and fall (↓) of companies’ stock price. The temporally causal f eature and target are linked through a sequence of predicted cause-effect tuples by different reasoning algorithms: a symbolic graph traverse"
D17-1292,S10-1059,0,0.021743,"orced (Causation) the virus (Cause) greenhouse gases (Cause) the reality of world war ii (Cause) Company heats (Cause temperature change) promotes (Cause change of position on a scale) makes (Causation) microsoft vague on windows (Item) chrome (Item) twitter (Cause) Country Cause 7→ Effect developing (Cause to make progress) improve (Cause to make progress) forced (Causation) north korea (Agent) china (Agent) war with china (Cause) People Relation attracts (Cause motion) draws (Cause motion) made (Causation) obama (Agent) on america ’s economic brains (Goal) michael jordan (Cause) as SEMAFOR (Chen et al., 2010) that produces a FrameNet style analysis of semantic predicateargument structures, we could obtain lexical tuples of causation in the sentence. Since our goal is to collect only causal relations, we extract total 36 causation related frames1 from the parsed sentences. Table 3: Number of sentences parsed, number of entities and tuples, and number of edges (KB-KB, KBcross) expanded by Freebase in CG RAPH. # Sentences # Entities # Tuples # KB-KB # KBcross 183,253,995 5,623,924 5,025,636 470,250 151,752 To generate meaningful explanations, high coverage of the knowledge is necessary. We collect si"
D17-1292,D11-1027,0,0.251283,"emperature) mainly use Granger (Granger, 1988) ability for predicting future values of a time series using past values of its own and another time series. (Hlav´acˇ kov´a-Schindler et al., 2007) studies more theoretical investigation for measuring causal influence in multivariate time series based on the entropy and mutual information estimation. However, none of them attempts generating explanation on the temporal causality. Previous works on text causality detection use verb syntactic patterns such as X 7−−→ Y , where the verb is causative (Girju, 2003; Riaz and Girju, 2013; Kozareva, 2012; Do et al., 2011) with additional features (Blanco et al., 2008). (Kozareva, 2012) extracted cause-effect relations, where the verb pattern for bootstrapping has a form of X ∗ 7−−∗→ Z Y from which terms X ∗ and Z ∗ was learned. The syntax based approaches, however, are not robust to semantic variation. As a part of SemEval (Girju et al., 2007), (Mirza and Tonelli, 2016) also uses syntactic causative patterns (Mirza and Tonelli, 2014) and supervised classifier to achieve the state-of-the-art performance. Extracting the cause-effect tuples with such syntactic features or temporality (Bethard et al., 2008) would"
D17-1292,W03-1210,0,0.601226,"), ... (et 7→ y), then e1 7→ e2 , ... 7→ et might be a good causal explanation between x and y. Section 3 and 4 describe how to bridge the causal gap between given events (x, y) by (1) constructing a large general cause-effect graph (CG RAPH) from text, (2) linking the given events to their equivalent entities in the causal graph by finding the internal paths (x 7→ e1 , ...et 7→ y) as causal explanations, using neural algorithms. CG RAPH is a knowledge base graph where edges are directed and causally related between entities. To address less representational variability of rule based methods (Girju, 2003; Blanco et al., 2008; Sharp et al., 2016) in the causal graph construction, we used FrameNet (Baker et al., 1998) semantics. Using a semantic parser such 2760 Table 2: Example (relation, cause, effect) tuples in different categories (manually labeled): general, company, country, and people. FrameNet labels related to causation are listed inside parentheses. The number of distinct relation types are 892. General causes (Causation) cause (Causation) forced (Causation) the virus (Cause) greenhouse gases (Cause) the reality of world war ii (Cause) Company heats (Cause temperature change) promotes"
D17-1292,S07-1003,0,0.100279,"Missing"
D17-1292,grivaz-2010-human,0,0.0264688,"va, 2012) extracted cause-effect relations, where the verb pattern for bootstrapping has a form of X ∗ 7−−∗→ Z Y from which terms X ∗ and Z ∗ was learned. The syntax based approaches, however, are not robust to semantic variation. As a part of SemEval (Girju et al., 2007), (Mirza and Tonelli, 2016) also uses syntactic causative patterns (Mirza and Tonelli, 2014) and supervised classifier to achieve the state-of-the-art performance. Extracting the cause-effect tuples with such syntactic features or temporality (Bethard et al., 2008) would be our next step for better causal graph construction. (Grivaz, 2010) conducts very insightful annotation study of what features are used in human reasoning on causation. Beyond the linguistic tests and causal chains for explaining causality in our work, other features such as counterfactuality, temporal order, and ontological asymmetry remain as our future direction to study. Textual entailment also seeks a directional relation between two given text fragments (Dagan et al., 2006). Recently, (Rockt¨aschel et al., 2015) developed an attention-based neural network method, trained on large annotated pairs of textual entailment, for classifying the types of relati"
D17-1292,W12-4107,0,0.63986,"nations that connect the given cause-effect pair. Table 1 shows example causal explanation that our system found between party and Facebook’s stock fall (↓). To construct a general causal graph, we extract all potential causal expressions from a large corpus of text. We refer to this graph as CG RAPH. We use FrameNet (Baker et al., 1998) semantics to provide various causative expressions (verbs, relations, and patterns), which we apply to a resource of 183, 253, 995 sentences of text and tweets. These expressions are considerably richer than previous rule-based patterns (Riaz and Girju, 2013; Kozareva, 2012). CG RAPH contains 5,025,636 causal edges. Our experiment demonstrates that our causality detection algorithm outperforms other baseline methods for forecasting future time series values. Also, we tested the neural reasoner on the inference generation task using the BLEU score. Additionally, our human evaluation shows the relative effectiveness of neural reasoners in generating appropriate lexicons in explanations. 2 CS PIKES: Temporal Causality Detection from Textual Features The objective of our model is, given a target time series y, to find the best set of textual features F = {f1 , ..., f"
D17-1292,C14-1198,0,0.034946,"mporal causality. Previous works on text causality detection use verb syntactic patterns such as X 7−−→ Y , where the verb is causative (Girju, 2003; Riaz and Girju, 2013; Kozareva, 2012; Do et al., 2011) with additional features (Blanco et al., 2008). (Kozareva, 2012) extracted cause-effect relations, where the verb pattern for bootstrapping has a form of X ∗ 7−−∗→ Z Y from which terms X ∗ and Z ∗ was learned. The syntax based approaches, however, are not robust to semantic variation. As a part of SemEval (Girju et al., 2007), (Mirza and Tonelli, 2016) also uses syntactic causative patterns (Mirza and Tonelli, 2014) and supervised classifier to achieve the state-of-the-art performance. Extracting the cause-effect tuples with such syntactic features or temporality (Bethard et al., 2008) would be our next step for better causal graph construction. (Grivaz, 2010) conducts very insightful annotation study of what features are used in human reasoning on causation. Beyond the linguistic tests and causal chains for explaining causality in our work, other features such as counterfactuality, temporal order, and ontological asymmetry remain as our future direction to study. Textual entailment also seeks a directio"
D17-1292,C16-1007,0,0.0485158,". However, none of them attempts generating explanation on the temporal causality. Previous works on text causality detection use verb syntactic patterns such as X 7−−→ Y , where the verb is causative (Girju, 2003; Riaz and Girju, 2013; Kozareva, 2012; Do et al., 2011) with additional features (Blanco et al., 2008). (Kozareva, 2012) extracted cause-effect relations, where the verb pattern for bootstrapping has a form of X ∗ 7−−∗→ Z Y from which terms X ∗ and Z ∗ was learned. The syntax based approaches, however, are not robust to semantic variation. As a part of SemEval (Girju et al., 2007), (Mirza and Tonelli, 2016) also uses syntactic causative patterns (Mirza and Tonelli, 2014) and supervised classifier to achieve the state-of-the-art performance. Extracting the cause-effect tuples with such syntactic features or temporality (Bethard et al., 2008) would be our next step for better causal graph construction. (Grivaz, 2010) conducts very insightful annotation study of what features are used in human reasoning on causation. Beyond the linguistic tests and causal chains for explaining causality in our work, other features such as counterfactuality, temporal order, and ontological asymmetry remain as our fu"
D17-1292,P02-1040,0,0.0986937,"r (RMSE) for forecasting with different step size (time steps to predict), different set of features, and different regression algorithms on stock and poll data. The forecasting error is summation of errors over moving a window (30 days) by 10 days over the period. Our Ccomposition method outperforms other time series only models and time series plus text models in both stock and poll data. 5.3 Generating Causality with Neural Reasoner The reasoner needs to predict the next effect phrase (or previous cause phrase) so the model should be evaluated in terms of generation task. We used the BLEU (Papineni et al., 2002) metric to evaluate the predicted phrases on held out phrases in our CG RAPH . Since our CG RAPH has many edges, there may be many good paths (explanations), possibly making our prediction diverse. To evaluate such diversity in prediction, we used ranking-based BLEU method on the k set of 2764 Table 7: BLEU ranking. Additional word representation +WE and relation specific alignment +REL help the model learn the cause and effect generation task especially for diverse patterns. S2S S2S + WE S2S + WE + REL B@1 B@3A B@5A 10.15 11.86 12.42 8.80 10.78 12.28 8.69 10.04 11.53 predicted phrases by beam"
D17-1292,D16-1244,0,0.127993,"Missing"
D17-1292,P14-1093,0,0.0581183,"ckt¨aschel et al., 2015) developed an attention-based neural network method, trained on large annotated pairs of textual entailment, for classifying the types of relations with decomposable attention (Parikh et al., 2016) or sequential tree structure (Chen et al., 2016). However, the dataset (Bowman et al., 2015) used for training entailment deals with just three categories, contradiction, neutral, and entailment, and focuses on relatively simple lexical and syntactic transformations (Kolesnyk et al., 2016). Our causal explanation generation task is also similar to future scenario generation (Hashimoto et al., 2014, 2015). Their scoring 2765 Table 8: Example causal chains for explaining the rise (↑) and fall (↓) of companies’ stock price. The temporally causal f eature and target are linked through a sequence of predicted cause-effect tuples by different reasoning algorithms: a symbolic graph traverse algorithm SYMB and a neural causality reasoning model NEUR. swept SYMB match improving widened match cause changed make turned excess7−−−−→excess materialism7−−−→people make films7−−−→money 7−−−−−→ twitter 7−−−−→facebook ↓ match raised match match beats clinton 7−−−−→president clinton 7−−−−→antitrust case"
D17-1292,W13-4004,0,0.216422,"e form of causal explanations that connect the given cause-effect pair. Table 1 shows example causal explanation that our system found between party and Facebook’s stock fall (↓). To construct a general causal graph, we extract all potential causal expressions from a large corpus of text. We refer to this graph as CG RAPH. We use FrameNet (Baker et al., 1998) semantics to provide various causative expressions (verbs, relations, and patterns), which we apply to a resource of 183, 253, 995 sentences of text and tweets. These expressions are considerably richer than previous rule-based patterns (Riaz and Girju, 2013; Kozareva, 2012). CG RAPH contains 5,025,636 causal edges. Our experiment demonstrates that our causality detection algorithm outperforms other baseline methods for forecasting future time series values. Also, we tested the neural reasoner on the inference generation task using the BLEU score. Additionally, our human evaluation shows the relative effectiveness of neural reasoners in generating appropriate lexicons in explanations. 2 CS PIKES: Temporal Causality Detection from Textual Features The objective of our model is, given a target time series y, to find the best set of textual features"
D17-1292,D16-1014,0,0.0293428,"... 7→ et might be a good causal explanation between x and y. Section 3 and 4 describe how to bridge the causal gap between given events (x, y) by (1) constructing a large general cause-effect graph (CG RAPH) from text, (2) linking the given events to their equivalent entities in the causal graph by finding the internal paths (x 7→ e1 , ...et 7→ y) as causal explanations, using neural algorithms. CG RAPH is a knowledge base graph where edges are directed and causally related between entities. To address less representational variability of rule based methods (Girju, 2003; Blanco et al., 2008; Sharp et al., 2016) in the causal graph construction, we used FrameNet (Baker et al., 1998) semantics. Using a semantic parser such 2760 Table 2: Example (relation, cause, effect) tuples in different categories (manually labeled): general, company, country, and people. FrameNet labels related to causation are listed inside parentheses. The number of distinct relation types are 892. General causes (Causation) cause (Causation) forced (Causation) the virus (Cause) greenhouse gases (Cause) the reality of world war ii (Cause) Company heats (Cause temperature change) promotes (Cause change of position on a scale) mak"
D17-1292,H05-1044,0,0.0276045,"ten frequent words in each topic and count their occurrence in the text to get the day-by-day time series. For example, xhealthcare means how popular the topic healthcare that consists of insurance, obamacare etc, is through time. Fsenti is time series of sentiments (positive or negative) for each topic. The top ten frequent words in each topic are used as the keywords, and tweets, blogs and news that contain at least one of these keywords are chosen to calculate the sentiment score. The day-by-day sentiment series are then obtained by counting positive and negative words using OpinionFinder (Wilson et al., 2005), and normalized by the total number of the items that day. 2.3 Temporal Causality Detection We define a causality function C for calculating causality score between target time series y and source time series x. The causality function C uses Granger causality (Granger, 1988) by fitting the two time series with a Vector AutoRegressive model with exogenous variables (VARX) (Hamilton, 1994): yt = αyt−l + βxt−l + t where t is a white Gaussian random vector at time t and l is a lag term. In our problem, the number of source time series x is not single so the prediction happens in the k multi-var"
D17-1292,C98-1013,0,\N,Missing
D17-1292,P17-1152,0,\N,Missing
D17-1315,P16-1160,0,0.0344896,"ur data driven approach. Also, their focus is on brand names. Hiranandani et al. (2017) have proposed an approach to recommend brand names based on brand/product description. However, they consider only a limited number of features like memorability and readability. Smith et al. (2014) devise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good. Generating a portmanteau from two root words can be viewed as a S2S problem. Recently, neural approaches have been used for S2S problems (Sutskever et al., 2014) such as MT. Ling et al. (2015) and Chung et al. (2016) have shown that character-level neural sequence models work as well as word-level ones for language modelling and MT. Zoph and Knight (2016) propose S2S models for multi-source MT, which have multi-sequence inputs, similar to our case. 7 Conclusion We have proposed an end-to-end neural system to model portmanteau generation. Our experiments show the efficacy of proposed system in predicting portmanteaus given the root words. We conclude that pre-training character embeddings on the English vocabulary helps the model. Through human evaluation we show that our model’s predictions are superior t"
D17-1315,N15-1021,0,0.486256,"odern Hebrew BatEl (1996); Berman (1989) and Spanish Pi˜neros (2004). Their short length makes them ideal for headlines and brandnames (Gabler, 2015). Unlike better-defined morphological phenomenon such as inflection and derivation, portmanteau generation ∗ * denotes equal contribution is difficult to capture using a set of rules. For instance, Shaw et al. (2014) state that the composition of the portmanteau from its root words depends on several factors, two important ones being maintaining prosody and retaining character segments from the root words, especially the head. An existing work by Deri and Knight (2015) aims to solve the problem of predicting portmanteau using a multi-tape FST model, which is datadriven, unlike prior approaches. Their methods rely on a grapheme to phoneme converter, which takes into account the phonetic features of the language, but may not be available or accurate for non-dictionary words, or low resource languages. Prior works, such as Faruqui et al. (2016), have demonstrated the efficacy of neural approaches for morphological tasks such as inflection. We hypothesize that such neural methods can (1) provide a simpler and more integrated end-to-end framework than multiple F"
D17-1315,N16-1077,1,0.680948,"composition of the portmanteau from its root words depends on several factors, two important ones being maintaining prosody and retaining character segments from the root words, especially the head. An existing work by Deri and Knight (2015) aims to solve the problem of predicting portmanteau using a multi-tape FST model, which is datadriven, unlike prior approaches. Their methods rely on a grapheme to phoneme converter, which takes into account the phonetic features of the language, but may not be available or accurate for non-dictionary words, or low resource languages. Prior works, such as Faruqui et al. (2016), have demonstrated the efficacy of neural approaches for morphological tasks such as inflection. We hypothesize that such neural methods can (1) provide a simpler and more integrated end-to-end framework than multiple FSTs used in the previous work, and (2) automatically capture features such as phonetic similarity through the use of character embeddings, removing the need for explicit 2917 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2917–2922 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics grapheme-"
D17-1315,W04-3250,0,0.073361,"Missing"
D17-1315,P12-1074,0,0.426566,"Missing"
D17-1315,N16-1004,0,0.0265837,"based on brand/product description. However, they consider only a limited number of features like memorability and readability. Smith et al. (2014) devise an approach to generate portmanteaus, which requires user-defined weights for attributes like sounding good. Generating a portmanteau from two root words can be viewed as a S2S problem. Recently, neural approaches have been used for S2S problems (Sutskever et al., 2014) such as MT. Ling et al. (2015) and Chung et al. (2016) have shown that character-level neural sequence models work as well as word-level ones for language modelling and MT. Zoph and Knight (2016) propose S2S models for multi-source MT, which have multi-sequence inputs, similar to our case. 7 Conclusion We have proposed an end-to-end neural system to model portmanteau generation. Our experiments show the efficacy of proposed system in predicting portmanteaus given the root words. We conclude that pre-training character embeddings on the English vocabulary helps the model. Through human evaluation we show that our model’s predictions are superior to the baseline. We have also released our dataset and code6 to encourage further research on the phenomenon of portmanteaus. We also release"
D18-1154,araki-etal-2014-detecting,1,0.82887,"ent salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using several oracle event attributes. In contrast, our proposed"
D18-1154,P98-1013,0,0.666037,"tence or across the whole document are used to capture 1226 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1226–1236 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics interactions on both local and global aspects (§4). The model significantly outperforms a strong “Frequency” baseline in our experiments. However, there are other discourse relations beyond lexical similarity. Figure 1 showcases some: the script relation (Schank and Abelson, 1977)1 between “charge” and “trial”, and the frame relation (Baker et al., 1998) between “attacks” and “trial” (“attacks” fills the “charges” role of “trial”). Since it is unclear which ones contribute more to salience, we design a Kernel based Centrality Estimation (KCE) model (§5) to capture salient specific interactions between discourse units automatically. In KCE, discourse units are projected to embeddings, which are trained end-to-end towards the salience task to capture rich semantic information. A set of soft-count kernels are trained to weigh salient specific latent relations between discourse units. With the capacity to model richer relations, KCE outperforms t"
D18-1154,D13-1178,0,0.109023,"de, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using several oracle event attributes. In contrast, our proposed models are fully learned and applied on more general domains and at a larger"
D18-1154,W98-1501,0,0.106467,"ntrast, our proposed models are fully learned and applied on more general domains and at a larger scale. We also do not restrict to a single most important event per document. There is a small but growing line of work on entity salience (Dunietz and Gillick, 2014; Dojchinovski et al., 2016; Xiong et al., 2018; Ponza et al., 2018). In this work, we study the case for events. Text relations have been studied in tasks like text summarization, which mainly focused on cohesion (Halliday and Hasan, 1976). Grammatical cohesion methods make use of document level structures such as anaphora relations (Baldwin and Morton, 1998) and discourse parse trees (Marcu, 1999). Lexical cohesion based methods focus on repetitions and synonyms on the lexical level (Skorochod’ko, 1971; Morris and Hirst, 1991; Erkan and Radev, 2004). Though sharing similar intuitions, our proposed models are designed to learn richer semantic relations in the embedding space. Comparing to the traditional summarization task, we focus on events, which are at a different granularity. Our experiments also unveil interesting phenomena among events and other discourse units. 3 The Event Salience Corpus This section introduces our approach to construct a"
D18-1154,D12-1062,0,0.0326899,"n these areas. Finally, we construct a large scale event salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using s"
D18-1154,J08-1001,0,0.19003,"relevant, but not easily identifiable by NLP systems. Hence it is important to be able to quantify the “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of the need to identify central events in applications like detecting salient relations (Zhang et al., 2015), and identifying climax in storyline (Vossen and Caselli, 2015). Generally, the salience of discourse units is important for language understanding tasks, such as document analysis (Barzilay and Lapata, 2008), information retrieval (Xiong et al., 2018), and semantic role labeling (Cheng and Erk, 2018). Thus, proper models for finding important events are desired. In this work, we study the task of event salience detection, to find events that are most relevant to the main content of documents. To build a salience detection model, one core observation is that salient discourse units are forming discourse relations. In Figure 1, the “trial” event is connected to many other events: “charge” is pressed before “trial”; “trial” is being “delayed”. We present two salience detection systems based on the o"
D18-1154,L16-1527,0,0.0181484,"rd frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using several oracle event attributes. In contrast, our proposed models are fully learned and applied on more general domains and at a larger scale. We also do not restrict to a single most important event per document. There is a small but growing line of work on entity salience (Dunietz and Gillick, 2014; Dojchinovski et al., 2016; Xiong et al., 2018; Ponza et al., 2018). In this work, we study the case for events. Text relations have been studied in tasks like text summarization, which mainly focused on cohesion (Halliday and Hasan, 1976). Grammatical cohesion methods make use of document level structures such as anaphora relations (Baldwin and Morton, 1998) and discourse parse trees (Marcu, 1999). Lexical cohesion based methods focus on repetitions and synonyms on the lexical level (Skorochod’ko, 1971; Morris and Hirst, 1991; Erkan and Radev, 2004). Though sharing similar intuitions, our proposed models are designed"
D18-1154,W17-2712,0,0.0246946,", we focus on events, which are at a different granularity. Our experiments also unveil interesting phenomena among events and other discourse units. 3 The Event Salience Corpus This section introduces our approach to construct a large-scale event salience corpus, including methods for finding event mentions and obtaining saliency labels. The studies are based on the Annotated New York Times corpus (Sandhaus, 2008), a newswire corpus with expert-written abstracts. 1227 3.1 Automatic Corpus Creation Event Mention Annotation: Despite many annotation attempts on events (Pustejovsky et al., 2002; Brown et al., 2017), automatic labeling of them in general domain remains an open problem. Most of the previous work follows empirical approaches. For example, Chambers and Jurafsky (2008) consider all verbs together with their subject and object as events. Do et al. (2011) additionally include nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). There are two main challenges in labeling event mentions. First, we need to decide which lexical items are event triggers. Second, we have to disambiguate the word sense to correctly identify event"
D18-1154,E14-4040,0,0.504409,"he feature based one by a large margin. Our analyses demonstrate that our neural model captures interesting connections between salience and discourse unit relations (e.g., scripts and frame structures). 1 Figure 1: Examples annotations. Underlying words are annotated event triggers; the red bold ones are annotated as salient. Introduction Automatic extraction of prominent information from text has always been a core problem in language research. While traditional methods mostly concentrate on the word level, researchers start to analyze higher-level discourse units in text, such as entities (Dunietz and Gillick, 2014) and events (Choubey et al., 2018). Events are important discourse units that form the backbone of our communication. They play various roles in documents. Some are more central in discourse: connecting other entities and events, or providing key information of a story. Others are less relevant, but not easily identifiable by NLP systems. Hence it is important to be able to quantify the “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of t"
D18-1154,P08-1090,0,0.554492,"d for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using several oracle event attributes. In contrast, our proposed models are fully learned and applied on more g"
D18-1154,N18-1076,0,0.0416231,"he “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of the need to identify central events in applications like detecting salient relations (Zhang et al., 2015), and identifying climax in storyline (Vossen and Caselli, 2015). Generally, the salience of discourse units is important for language understanding tasks, such as document analysis (Barzilay and Lapata, 2008), information retrieval (Xiong et al., 2018), and semantic role labeling (Cheng and Erk, 2018). Thus, proper models for finding important events are desired. In this work, we study the task of event salience detection, to find events that are most relevant to the main content of documents. To build a salience detection model, one core observation is that salient discourse units are forming discourse relations. In Figure 1, the “trial” event is connected to many other events: “charge” is pressed before “trial”; “trial” is being “delayed”. We present two salience detection systems based on the observations. First is a feature based learning to rank model. Beyond basic features like frequ"
D18-1154,N13-1104,0,0.0399892,"Missing"
D18-1154,N18-2055,0,0.118711,"Our analyses demonstrate that our neural model captures interesting connections between salience and discourse unit relations (e.g., scripts and frame structures). 1 Figure 1: Examples annotations. Underlying words are annotated event triggers; the red bold ones are annotated as salient. Introduction Automatic extraction of prominent information from text has always been a core problem in language research. While traditional methods mostly concentrate on the word level, researchers start to analyze higher-level discourse units in text, such as entities (Dunietz and Gillick, 2014) and events (Choubey et al., 2018). Events are important discourse units that form the backbone of our communication. They play various roles in documents. Some are more central in discourse: connecting other entities and events, or providing key information of a story. Others are less relevant, but not easily identifiable by NLP systems. Hence it is important to be able to quantify the “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of the need to identify central events"
D18-1154,P11-1144,0,0.0601145,"with their subject and object as events. Do et al. (2011) additionally include nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). There are two main challenges in labeling event mentions. First, we need to decide which lexical items are event triggers. Second, we have to disambiguate the word sense to correctly identify events. For example, the word “phone” can refer to an entity (a physical phone) or an event (a phone call event). We use FrameNet to solve these problems. We first use a FrameNet based parser: Semafor (Das and Smith, 2011), to find and disambiguate triggers into frame classes. We then use the FrameNet ontology to select event mentions. Our frame based selection method follows the Vendler classes (Vendler, 1957), a four way classification of eventuality: states, activities, accomplishments and achievements. The last three classes involve state change, and are normally considered as events. Following this, we create an “eventevoking frame” list using the following procedure: 1. We keep frames that are subframes of Event and Process in the FrameNet ontology. 2. We discard frames that are subframes of state, entity"
D18-1154,D11-1027,0,0.0640051,"orpus, including methods for finding event mentions and obtaining saliency labels. The studies are based on the Annotated New York Times corpus (Sandhaus, 2008), a newswire corpus with expert-written abstracts. 1227 3.1 Automatic Corpus Creation Event Mention Annotation: Despite many annotation attempts on events (Pustejovsky et al., 2002; Brown et al., 2017), automatic labeling of them in general domain remains an open problem. Most of the previous work follows empirical approaches. For example, Chambers and Jurafsky (2008) consider all verbs together with their subject and object as events. Do et al. (2011) additionally include nominal predicates, using the nominal form of verbs and lexical items under the Event frame in FrameNet (Baker et al., 1998). There are two main challenges in labeling event mentions. First, we need to decide which lexical items are event triggers. Second, we have to disambiguate the word sense to correctly identify events. For example, the word “phone” can refer to an entity (a physical phone) or an event (a phone call event). We use FrameNet to solve these problems. We first use a FrameNet based parser: Semafor (Das and Smith, 2011), to find and disambiguate triggers in"
D18-1154,P13-1008,0,0.0335072,"e observe interesting connections between salience and various discourse relations (§7.1 and Table 5), implying potential research on these areas. Finally, we construct a large scale event salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant ev"
D18-1154,liu-etal-2014-supervised,1,0.800884,"tions (§7.1 and Table 5), implying potential research on these areas. Finally, we construct a large scale event salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and impo"
D18-1154,P17-1009,0,0.0174103,"ble 5), implying potential research on these areas. Finally, we construct a large scale event salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundred"
D18-1154,P14-5010,0,0.00271144,"vi , d) + b where F (evi , d) is the features for evi in d (Table 2); Wf and b are the parameters to learn. The model is trained with pairwise loss: This section presents the feature-based model, including the features and the learning process. 4.1 X max(0, 1 − f (ev + , d) + f (ev − , d)), (2) ev + ,ev − ∈d w.r.t. y(ev + , d) = +1 & y(ev − , d) = −1. Features Our features are summarized in Table 2. Basic Discourse Features: We first use two basic features similar to Dunietz and Gillick (2014): Frequency and Sentence Location. Frequency is the lemma count of the mention’s syntactic head word (Manning et al., 2014). Sentence Location is the sentence index of the mention, since the first few sentences are normally more important. These two features are often used to estimate salience (Barzilay and Lapata, 2008; Vossen and Caselli, 2015). Content Features: We then design several lexical similarity features, to reflect Grimes’ content relatedness (Grimes, 1975). In addition to events, the relations between events and entities are also important. For example, Figure 1 shows some related entities in the legal domain, such as “prosecutors” and “court”. Ideally, they should help promote the salience status for"
D18-1154,J91-1002,0,0.431721,". There is a small but growing line of work on entity salience (Dunietz and Gillick, 2014; Dojchinovski et al., 2016; Xiong et al., 2018; Ponza et al., 2018). In this work, we study the case for events. Text relations have been studied in tasks like text summarization, which mainly focused on cohesion (Halliday and Hasan, 1976). Grammatical cohesion methods make use of document level structures such as anaphora relations (Baldwin and Morton, 1998) and discourse parse trees (Marcu, 1999). Lexical cohesion based methods focus on repetitions and synonyms on the lexical level (Skorochod’ko, 1971; Morris and Hirst, 1991; Erkan and Radev, 2004). Though sharing similar intuitions, our proposed models are designed to learn richer semantic relations in the embedding space. Comparing to the traditional summarization task, we focus on events, which are at a different granularity. Our experiments also unveil interesting phenomena among events and other discourse units. 3 The Event Salience Corpus This section introduces our approach to construct a large-scale event salience corpus, including methods for finding event mentions and obtaining saliency labels. The studies are based on the Annotated New York Times corpu"
D18-1154,P15-2060,0,0.0241662,"ting connections between salience and various discourse relations (§7.1 and Table 5), implying potential research on these areas. Finally, we construct a large scale event salience corpus, providing a testbed for future research. Our code, dataset and models are publicly available2 . 1 Scripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They"
D18-1154,D16-1038,0,0.0991669,"Missing"
D18-1154,P16-1027,0,0.0203681,"ripts are prototypical sequences of events: a restaurant script normally contains events like “order”, “eat” and “pay”. 2 https://github.com/hunterhector/ EventSalience 2 Related Work Events have been studied on many aspects due to their importance in language. To name a few: event detection (Li et al., 2013; Nguyen and Grishman, 2015; Peng et al., 2016), coreference (Liu et al., 2014; Lu and Ng, 2017), temporal analysis (Do et al., 2012; Chambers et al., 2014), sequencing (Araki et al., 2014), script induction (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013; Rudinger et al., 2015; Pichotta and Mooney, 2016). However, studies on event salience are premature. Some previous work attempts to approximate event salience with word frequency or discourse position (Vossen and Caselli, 2015; Zhang et al., 2015). Parallel to ours, Choubey et al. (2018) propose a task to find the most dominant event in news articles. They draw connections between event coreference and importance, on hundreds of closeddomain documents, using several oracle event attributes. In contrast, our proposed models are fully learned and applied on more general domains and at a larger scale. We also do not restrict to a single most im"
D18-1154,W15-4507,0,0.367892,"n documents. Some are more central in discourse: connecting other entities and events, or providing key information of a story. Others are less relevant, but not easily identifiable by NLP systems. Hence it is important to be able to quantify the “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of the need to identify central events in applications like detecting salient relations (Zhang et al., 2015), and identifying climax in storyline (Vossen and Caselli, 2015). Generally, the salience of discourse units is important for language understanding tasks, such as document analysis (Barzilay and Lapata, 2008), information retrieval (Xiong et al., 2018), and semantic role labeling (Cheng and Erk, 2018). Thus, proper models for finding important events are desired. In this work, we study the task of event salience detection, to find events that are most relevant to the main content of documents. To build a salience detection model, one core observation is that salient discourse units are forming discourse relations. In Figure 1, the “trial” event is connect"
D18-1154,Q15-1009,0,0.175292,"e backbone of our communication. They play various roles in documents. Some are more central in discourse: connecting other entities and events, or providing key information of a story. Others are less relevant, but not easily identifiable by NLP systems. Hence it is important to be able to quantify the “importance” of events. For example, Figure 1 is a news excerpt describing a debate around a jurisdiction process: “trial” is central as the main discussing topic, while “war” is not. Researchers are aware of the need to identify central events in applications like detecting salient relations (Zhang et al., 2015), and identifying climax in storyline (Vossen and Caselli, 2015). Generally, the salience of discourse units is important for language understanding tasks, such as document analysis (Barzilay and Lapata, 2008), information retrieval (Xiong et al., 2018), and semantic role labeling (Cheng and Erk, 2018). Thus, proper models for finding important events are desired. In this work, we study the task of event salience detection, to find events that are most relevant to the main content of documents. To build a salience detection model, one core observation is that salient discourse units are formin"
D18-1154,N13-1110,0,0.0192343,"nd Process in the FrameNet ontology. 2. We discard frames that are subframes of state, entity and attribute frames, such as Entity, Attributes, Locale, etc. 3. We manually inspect frames that are not subframes of the above-mentioned ones (around 200) to keep event related ones (including subframes), such as Arson, Delivery, etc. This gives us a total of 569 frames. We parse the documents with Semafor and consider predicates that trigger a frame in the list as candidates. We finish the process by removing the light verbs3 and reporting events4 from the candidates, similar to previous research (Recasens et al., 2013). Salience Labeling: For all articles with a human written abstract (around 664,911) in the New York 3 Light verbs carry little semantic information: “appear”, “be”, “become”, “do”, “have”, “seem”, “do”, “get”, “give”, “go”, “have”, “keep”, “make”, “put”, “set”, “take”. 4 Reporting verbs are normally associated with the narrator: “argue”, “claim”, “say”, “suggest”, “tell”. Train Dev Test # Documents 526126 64000 63589 Avg. # Word 794.12 790.27 798.68 Avg. # Events 61.96 60.65 61.34 Avg. # Entities 197.63 196.95 198.40 8.77 8.79 8.90 Avg. # Salience Table 1: Dataset Statistics. Times Annotated"
D18-1154,recasens-etal-2010-typology,1,0.860753,"Missing"
D18-1257,P17-1191,1,0.828148,"context. In addition, the machine comprehension community may also find CLOTH useful in evaluating machine’s understanding of language phenomena including vocabulary, reasoning and grammar, which are key components of comprehending natural language. In our future work, we would like to design algorithms to better model a long context, to utilize external knowledge, and to explore more effective semi-supervised learning approaches. Firstly, we would like to investigate efficient ways of utilizing external knowledge such as paraphrasing and semantic concepts like prior works (Dong et al., 2017; Dasigi et al., 2017). In comparison, training on a large external dataset is actually a time-consuming way of utilizing external knowledge. Secondly, to use the generated questions more effectively, the representative-based semisupervised approach might be improved by techniques studied in active learning and hard example mining (Settles, 2009; Shrivastava et al., 2016; Chang et al., 2017). Acknowledgement We thank Yulun Du, Kaiyu Shi and Zhilin Yang for insightful discussions and suggestions on the draft. We thank Shi Feng for the script to highlight representative words. This research was supported in part by D"
D18-1257,D17-1091,0,0.0268425,"in modeling a long context. In addition, the machine comprehension community may also find CLOTH useful in evaluating machine’s understanding of language phenomena including vocabulary, reasoning and grammar, which are key components of comprehending natural language. In our future work, we would like to design algorithms to better model a long context, to utilize external knowledge, and to explore more effective semi-supervised learning approaches. Firstly, we would like to investigate efficient ways of utilizing external knowledge such as paraphrasing and semantic concepts like prior works (Dong et al., 2017; Dasigi et al., 2017). In comparison, training on a large external dataset is actually a time-consuming way of utilizing external knowledge. Secondly, to use the generated questions more effectively, the representative-based semisupervised approach might be improved by techniques studied in active learning and hard example mining (Settles, 2009; Shrivastava et al., 2016; Chang et al., 2017). Acknowledgement We thank Yulun Du, Kaiyu Shi and Zhilin Yang for insightful discussions and suggestions on the draft. We thank Shi Feng for the script to highlight representative words. This research was"
D18-1257,P17-1147,0,0.0301743,"unity can use CLOTH to evaluate their models’ abilities in modeling long contexts, while the machine comprehension community can use CLOTH to test machine’s understanding of language phenomena. 2 Related Work Large-scale automatically-generated cloze tests (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016) lead to significant research advancements. However, generated questions do not consider language phenomenon to be tested and are relatively easy to solve. Recently proposed reading comprehension datasets are all labeled by humans to ensure a high quality (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2016; Nguyen et al., 2016). Perhaps the closet work to CLOTH is the LAMBADA dataset (Paperno et al., 2016). LAMBADA also targets at finding challenging words to test LM’s ability in comprehending a longer context. However, LAMBADA does not provide a candidate set for each question, which can cause ambiguities when multiple words can fit in. Furthermore, only test set and development set are labeled manually. The provided training set is the unlabeled Book Corpus (Zhu et al., 2015). Such unlabeled data do not emphasize long-dependency questions and have a mismatched distribu"
D18-1257,R13-2016,0,0.0301405,"achieves an F1 score of 36.5 on the test set, which is understandable since 7 6 CLOTH CLOTH-M CLOTH-H The script to generate the Figure is obtained at https://gist.github.com/ihsgnef/ f13c35cd46624c8f458a4d23589ac768 2351 Figure 1: Representativeness prediction for each word. Lighter color means less representative. The words deleted by human as blanks are in bold text. there are many plausible questions within a passage. It has been shown that features such as morphology information and readability are beneficial in cloze test prediction (Skory and Eskenazi, 2010; Correia et al., 2012, 2010; Kurtasov, 2013). We leave investigating the advanced approaches of automatically designing cloze test to future work. 7 Conclusion and Discussion In this paper, we propose a large-scale cloze test dataset CLOTH that is designed by teachers. With missing blanks and candidate options carefully created by teachers to test different aspects of language phenomena, CLOTH requires a deep language understanding and better captures the complexity of human language. We find that human outperforms 1B-LM by a significant margin. After detailed analysis, we find that the performance gap is due to the model’s inability to"
D18-1257,D16-1241,0,0.459463,"rm context to be the key bottleneck. 1 Introduction Being a classic language exercise, the cloze test (Taylor, 1953) is an accurate assessment of language proficiency (Fotos, 1991; Jonz, 1991; Tremblay, 2011) and has been widely employed in language examinations. Under a typical setting, a cloze test requires examinees to fill in missing words (or sentences) to best fit the surrounding context. To facilitate natural language understanding, automatically-generated cloze datasets are introduced to measure the ability of machines in reading comprehension (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016). In these datasets, each cloze question typically consists of ∗ Equal contribution. CLOTH (CLOze test by TeacHers) is available at http://www.cs.cmu.edu/˜glai1/data/cloth/. 2 The leaderboard is available at http://www. qizhexie.com/data/CLOTH_leaderboard.html 1 a context paragraph and a question sentence. By randomly replacing a particular word in the question sentence with a blank symbol, a single test case is created. For instance, CNN/Daily Mail datasets (Hermann et al., 2015) use news articles as contexts and summary bullet points as the question sentence. Only named entities are removed"
D18-1257,P16-1144,0,0.0680268,"Missing"
D18-1257,D14-1162,0,0.0835649,"Missing"
D18-1257,N18-1202,0,0.066965,"Missing"
D18-1257,D16-1264,0,0.421239,"e language modeling community can use CLOTH to evaluate their models’ abilities in modeling long contexts, while the machine comprehension community can use CLOTH to test machine’s understanding of language phenomena. 2 Related Work Large-scale automatically-generated cloze tests (Hermann et al., 2015; Hill et al., 2016; Onishi et al., 2016) lead to significant research advancements. However, generated questions do not consider language phenomenon to be tested and are relatively easy to solve. Recently proposed reading comprehension datasets are all labeled by humans to ensure a high quality (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2016; Nguyen et al., 2016). Perhaps the closet work to CLOTH is the LAMBADA dataset (Paperno et al., 2016). LAMBADA also targets at finding challenging words to test LM’s ability in comprehending a longer context. However, LAMBADA does not provide a candidate set for each question, which can cause ambiguities when multiple words can fit in. Furthermore, only test set and development set are labeled manually. The provided training set is the unlabeled Book Corpus (Zhu et al., 2015). Such unlabeled data do not emphasize long-dependency questions and have a"
D18-1257,D17-1004,0,0.0264373,"ased supervised models, we train a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to predict the missing word given the context with only labeled data. The implementation details are in Appendix A.3. ment the supervised LSTM model with the attention mechanism (Bahdanau et al., 2014), so that the representation at the blank is used as a query to find the relevant context in the document and a blank-specific representation of the document is used to score each candidate answer. Specifically, we adapt the Stanford Attentive Reader (Chen et al., 2016) and the positionaware attention model (Zhang et al., 2017) to the cloze test problem. With the position-aware attention model, the attention scores are based on both the context match and the distance from a context to the blank. Both attention models are trained only with human-created blanks just as the LSTM model. LM In cloze test, the context on both sides may be enough to determine the correct answer. Suppose xi is the missing word and x1 , · · · , xi−1 , xi+1 , · · · , xn are the context, we choose xi that maximizes the joint probability p(x1 , · · · , xn ), which essentially maximizes the conditional likelihood p(xi |x1 , · · · , xi−1 , xi+1 ,"
D18-1257,W10-1007,0,0.030883,"not want to share with others”. Our prediction model achieves an F1 score of 36.5 on the test set, which is understandable since 7 6 CLOTH CLOTH-M CLOTH-H The script to generate the Figure is obtained at https://gist.github.com/ihsgnef/ f13c35cd46624c8f458a4d23589ac768 2351 Figure 1: Representativeness prediction for each word. Lighter color means less representative. The words deleted by human as blanks are in bold text. there are many plausible questions within a passage. It has been shown that features such as morphology information and readability are beneficial in cloze test prediction (Skory and Eskenazi, 2010; Correia et al., 2012, 2010; Kurtasov, 2013). We leave investigating the advanced approaches of automatically designing cloze test to future work. 7 Conclusion and Discussion In this paper, we propose a large-scale cloze test dataset CLOTH that is designed by teachers. With missing blanks and candidate options carefully created by teachers to test different aspects of language phenomena, CLOTH requires a deep language understanding and better captures the complexity of human language. We find that human outperforms 1B-LM by a significant margin. After detailed analysis, we find that the perfo"
D18-1257,P17-1018,0,0.026868,"e correct answers. There is only one best answer among four candidates, although several candidates may seem correct. corpus. Human performance We measure the performance of Amazon Mechanical Turkers on 3, 000 sampled questions when the whole passage is given. Results The comparison is shown in Table 4. Both attentive readers achieve similar accuracy to the LSTM. We hypothesize that the reason of the attention model’s unsatisfactory performance is that the evidence of a question cannot be simply found by matching the context. Similarly, on reading comprehension, though attention-based models (Wang et al., 2017; Seo et al., 2016; Dhingra Short-term Dataset CLOTH CLOTH-M CLOTH-H Long-term GM STR MP LTR O 0.265 0.330 0.240 0.503 0.413 0.539 0.044 0.068 0.035 0.180 0.174 0.183 0.007 0.014 0.004 Table 3: The question type statistics of 3000 sampled questions where GM, STR, MP, LTR and O denotes grammar, short-term-reasoning, matching/paraphrasing, long-term-reasoning and others respectively. Model CLOTH CLOTH-M CLOTH-H LSTM Stanford AR Position-aware AR 0.484 0.487 0.485 0.518 0.529 0.523 0.471 0.471 0.471 LM 1B-LM (one sent.) 1B-LM (three sent.) 0.548 0.695 0.707 0.646 0.723 0.745 0.506 0.685 0.693 Hum"
D19-1179,W05-0909,0,0.00881577,"etrics: Style Diversity measures how much produced sentences (or stories) differ amongst themselves. Higher the diversity, better the stylistic variation in language it contains. We use an entropy measure to capture the variance of n-gram features between annotated sentences: Entropy (Gaussian-Mixture) that combines the N-Gram entropies (Shannon, 1951) using Gaussian mixture model (N=3). Meaning Preservation measures semantic similarity of the produced sentence (or story) with the reference sentence (or story). Higher the similarity, better the meaning preserved. We use a hardmeasure, METEOR (Banerjee and Lavie, 2005), that calculates F-score of word overlaps between the output and reference sentences3 . Since the hard measures do not take into account all semantic similarities 4 , we also use a soft measure, Vec3 Other measures (e.g., BLEU (Papineni et al., 2002), ROUGE (Lin and Hovy, 2003)) show relatively similar performance. 4 METEOR does consider synonymy and paraphrasing but is limited by its predefined model/dictionaries/resources for the respective language, such as Wordnet torExtrema (VecExt) (Liu et al., 2016). It computes cosine similarity of averaged word embeddings (i.e., GloVe (Pennington et"
D19-1179,N19-1423,0,0.0328153,"h style variables are relatively difficult or easy to predict from the text given, and (2) what types of textual features are salient for each type of style Features. Stylistic language has a variety of features at different levels such as lexical choices, syntactic structure and more. Thus, we use following features: • lexical features: ngram’s frequency (n=3), number of named entities, number of stop-words • syntax features: sentence length, number of each Part-of-Speech (POS) tag, number of outof-vocabulary, number of named entities • deep features: pre-trained sentence encoder using BERT (Devlin et al., 2019) • semantic feature: sentiment score where named entities, POS tags, and sentiment scores are obtained using the off-the-shelf tools such as Spacy6 library. We use 70K n-gram lexical features, 300 dimensional embeddings, and 14 hand-written features. 5 The distribution of number of training instances per variable is given in Appendix 1701 6 https://spacy.io/ Models. We train a binary classifier for each personal style with different models: logistic regression, SVM with linear/RBF kernels, Random Forest, Nearest Neighbors, Multi-layer Perceptron, AdaBoost, and Naive Bayes. For each style, we c"
D19-1179,P09-1080,0,0.0456678,"akespearean text (Jhamtani et al., 2017), sarcastic and literal tweets (Peled and Reichart, 2017), and formal and informal text (Heylighen and Dewaele, 1999; Rao and Tetreault, 2018). Compared to these, we aim to understand and demonstrate style variation owing to multiple demographic attributes. Besides the style transfer, other applications using stylistic features have been studied such as poetry generation (Ghazvininejad et al., 2017), stylometry with demographic information (Verhoeven and Daelemans, 2014), modeling style bias (Vogel and Jurafsky, 2012) and modeling biographic attributes (Garera and Yarowsky, 2009). A series of works by (Koppel et al., 2011, 2009; Argamon et al., 2009; Koppel and Winter, 2014) and their shared tasks (Stamatatos et al., 2018) show huge progress on author profiling and attribute classification tasks. However, none of the prior works have collected a stylistic language dataset to have multiple styles in conjunction, parallely annotated by a human. The multiple styles in conjunction in PASTEL enable an appropriate experiment setting for controlled style classification task in Section 5.1. 3 Denotation Experiment We first provide a preliminary study to find the best input se"
D19-1179,P17-4008,0,0.0220179,"for evaluation and present limited evaluation for meaning preservation. Style transfer with parallel corpus: Few recent works use parallel text for style transfer between modern and Shakespearean text (Jhamtani et al., 2017), sarcastic and literal tweets (Peled and Reichart, 2017), and formal and informal text (Heylighen and Dewaele, 1999; Rao and Tetreault, 2018). Compared to these, we aim to understand and demonstrate style variation owing to multiple demographic attributes. Besides the style transfer, other applications using stylistic features have been studied such as poetry generation (Ghazvininejad et al., 2017), stylometry with demographic information (Verhoeven and Daelemans, 2014), modeling style bias (Vogel and Jurafsky, 2012) and modeling biographic attributes (Garera and Yarowsky, 2009). A series of works by (Koppel et al., 2011, 2009; Argamon et al., 2009; Koppel and Winter, 2014) and their shared tasks (Stamatatos et al., 2018) show huge progress on author profiling and attribute classification tasks. However, none of the prior works have collected a stylistic language dataset to have multiple styles in conjunction, parallely annotated by a human. The multiple styles in conjunction in PASTEL"
D19-1179,N16-1147,0,0.195772,"put score. Top five uni/bigram keywords are chosen at each story, which are called global keywords. On the other hand, another top three uni/bigram keywords are chosen at each image/sentence in a story, which are called local keywords. Local keywords for each image/sentence help annotators not deviate too much. For example, local keywords look like (restaurant, hearing, friends) → (pictures, menu, difficult) → (salad, corn, chose) for three sentences/images, while global keywords look like (wait, salad, restaurant) for a story of the three sentences/images. We use Visual Story Telling (ViST) (Huang et al., 2016) dataset as our input source. The dataset contains stories, and each story has five pairs of images and sentences. We filter out stories that are not temporally ordered using the timestamps of images. The final number of stories after filtering the non-temporally-ordered stories is 28,130. For the denotation experiment, we only use randomly chosen 100 stories. The detailed pre-processing steps are described in Appendix. 3.2 Experimental Setup In order to find the best input setting that preserves meaning as well as promotes a stylistic diversity, we conduct a denotation experiment as described"
D19-1179,P15-1162,0,0.0261539,"Missing"
D19-1179,W17-4902,1,0.860771,"ng corpora (e.g, paper and news (Han et al., 2017), or real and synthetic reviews (Lipton et al., 2015)). They use different types of generative models in the same way as style transfer in images, where meaning preservation is not controlled systematically. Prabhumoye et al. (2018) proposes backtranslation to get a style-agnostic sentence representation. However, they lack parallel ground truth for evaluation and present limited evaluation for meaning preservation. Style transfer with parallel corpus: Few recent works use parallel text for style transfer between modern and Shakespearean text (Jhamtani et al., 2017), sarcastic and literal tweets (Peled and Reichart, 2017), and formal and informal text (Heylighen and Dewaele, 1999; Rao and Tetreault, 2018). Compared to these, we aim to understand and demonstrate style variation owing to multiple demographic attributes. Besides the style transfer, other applications using stylistic features have been studied such as poetry generation (Ghazvininejad et al., 2017), stylometry with demographic information (Verhoeven and Daelemans, 2014), modeling style bias (Vogel and Jurafsky, 2012) and modeling biographic attributes (Garera and Yarowsky, 2009). A series of"
D19-1179,N03-1020,1,0.402303,"Missing"
D19-1179,D16-1230,0,0.131976,"her the similarity, better the meaning preserved. We use a hardmeasure, METEOR (Banerjee and Lavie, 2005), that calculates F-score of word overlaps between the output and reference sentences3 . Since the hard measures do not take into account all semantic similarities 4 , we also use a soft measure, Vec3 Other measures (e.g., BLEU (Papineni et al., 2002), ROUGE (Lin and Hovy, 2003)) show relatively similar performance. 4 METEOR does consider synonymy and paraphrasing but is limited by its predefined model/dictionaries/resources for the respective language, such as Wordnet torExtrema (VecExt) (Liu et al., 2016). It computes cosine similarity of averaged word embeddings (i.e., GloVe (Pennington et al., 2014)) between the output and reference sentences. Table 2 shows results of the two metrics across different input settings we define. For the sentence level, as expected, single reference sentence has the highest meaning preservation across all the metrics because it is basically paraphrasing the reference sentence. In general, Story (images + local keywords) shows a great performance with the highest diversity regardless of the levels, as well as the highest preservation at the soft measure on the st"
D19-1179,P02-1040,0,0.109773,"n annotated sentences: Entropy (Gaussian-Mixture) that combines the N-Gram entropies (Shannon, 1951) using Gaussian mixture model (N=3). Meaning Preservation measures semantic similarity of the produced sentence (or story) with the reference sentence (or story). Higher the similarity, better the meaning preserved. We use a hardmeasure, METEOR (Banerjee and Lavie, 2005), that calculates F-score of word overlaps between the output and reference sentences3 . Since the hard measures do not take into account all semantic similarities 4 , we also use a soft measure, Vec3 Other measures (e.g., BLEU (Papineni et al., 2002), ROUGE (Lin and Hovy, 2003)) show relatively similar performance. 4 METEOR does consider synonymy and paraphrasing but is limited by its predefined model/dictionaries/resources for the respective language, such as Wordnet torExtrema (VecExt) (Liu et al., 2016). It computes cosine similarity of averaged word embeddings (i.e., GloVe (Pennington et al., 2014)) between the output and reference sentences. Table 2 shows results of the two metrics across different input settings we define. For the sentence level, as expected, single reference sentence has the highest meaning preservation across all"
D19-1179,verhoeven-daelemans-2014-clips,0,0.395123,". Connecting such textual features to someone’s persona is an important study to understand stylistic variation of language. For example, do highly educated people write longer sentences (Bloomfield, 1927)? Are Hispanic and East Asian people more likely to drop pronouns (White, 1985)? Are elder people likely to use lesser anaphora (Ulatowska et al., 1986)? To computationally model a meaning-preserved variance of text across styles, many recent works have developed systems that transfer styles (Reddy and Knight, 2016; Hu et al., 2017; Prabhumoye et al., 2018) or profiles authorships from text (Verhoeven and Daelemans, 2014; Koppel et al., 2009; Stamatatos et al., 2018) without parallel corpus of stylistic text. However, the absence of such a parallel dataset makes it difficult both to systematically learn the textual variation of multiple styles as well as properly evaluate the models. In this paper, we propose a large scale, humanannotated, parallel stylistic dataset called PASTEL, with focus on multiple types of personas in conjunction. Ideally, annotations for a parallel 1696 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on N"
D19-1179,P17-1155,0,0.0140068,"real and synthetic reviews (Lipton et al., 2015)). They use different types of generative models in the same way as style transfer in images, where meaning preservation is not controlled systematically. Prabhumoye et al. (2018) proposes backtranslation to get a style-agnostic sentence representation. However, they lack parallel ground truth for evaluation and present limited evaluation for meaning preservation. Style transfer with parallel corpus: Few recent works use parallel text for style transfer between modern and Shakespearean text (Jhamtani et al., 2017), sarcastic and literal tweets (Peled and Reichart, 2017), and formal and informal text (Heylighen and Dewaele, 1999; Rao and Tetreault, 2018). Compared to these, we aim to understand and demonstrate style variation owing to multiple demographic attributes. Besides the style transfer, other applications using stylistic features have been studied such as poetry generation (Ghazvininejad et al., 2017), stylometry with demographic information (Verhoeven and Daelemans, 2014), modeling style bias (Vogel and Jurafsky, 2012) and modeling biographic attributes (Garera and Yarowsky, 2009). A series of works by (Koppel et al., 2011, 2009; Argamon et al., 2009"
D19-1179,W12-3204,0,0.0152437,"rks use parallel text for style transfer between modern and Shakespearean text (Jhamtani et al., 2017), sarcastic and literal tweets (Peled and Reichart, 2017), and formal and informal text (Heylighen and Dewaele, 1999; Rao and Tetreault, 2018). Compared to these, we aim to understand and demonstrate style variation owing to multiple demographic attributes. Besides the style transfer, other applications using stylistic features have been studied such as poetry generation (Ghazvininejad et al., 2017), stylometry with demographic information (Verhoeven and Daelemans, 2014), modeling style bias (Vogel and Jurafsky, 2012) and modeling biographic attributes (Garera and Yarowsky, 2009). A series of works by (Koppel et al., 2011, 2009; Argamon et al., 2009; Koppel and Winter, 2014) and their shared tasks (Stamatatos et al., 2018) show huge progress on author profiling and attribute classification tasks. However, none of the prior works have collected a stylistic language dataset to have multiple styles in conjunction, parallely annotated by a human. The multiple styles in conjunction in PASTEL enable an appropriate experiment setting for controlled style classification task in Section 5.1. 3 Denotation Experiment"
D19-1179,D14-1162,0,0.0899269,"d Lavie, 2005), that calculates F-score of word overlaps between the output and reference sentences3 . Since the hard measures do not take into account all semantic similarities 4 , we also use a soft measure, Vec3 Other measures (e.g., BLEU (Papineni et al., 2002), ROUGE (Lin and Hovy, 2003)) show relatively similar performance. 4 METEOR does consider synonymy and paraphrasing but is limited by its predefined model/dictionaries/resources for the respective language, such as Wordnet torExtrema (VecExt) (Liu et al., 2016). It computes cosine similarity of averaged word embeddings (i.e., GloVe (Pennington et al., 2014)) between the output and reference sentences. Table 2 shows results of the two metrics across different input settings we define. For the sentence level, as expected, single reference sentence has the highest meaning preservation across all the metrics because it is basically paraphrasing the reference sentence. In general, Story (images + local keywords) shows a great performance with the highest diversity regardless of the levels, as well as the highest preservation at the soft measure on the story-level. Thus, we use Story(images+local keywords) as the input setting for our final data colle"
D19-1179,P18-1080,0,0.347677,"the original meaning of given text (DiMarco and Hirst, 1990). Connecting such textual features to someone’s persona is an important study to understand stylistic variation of language. For example, do highly educated people write longer sentences (Bloomfield, 1927)? Are Hispanic and East Asian people more likely to drop pronouns (White, 1985)? Are elder people likely to use lesser anaphora (Ulatowska et al., 1986)? To computationally model a meaning-preserved variance of text across styles, many recent works have developed systems that transfer styles (Reddy and Knight, 2016; Hu et al., 2017; Prabhumoye et al., 2018) or profiles authorships from text (Verhoeven and Daelemans, 2014; Koppel et al., 2009; Stamatatos et al., 2018) without parallel corpus of stylistic text. However, the absence of such a parallel dataset makes it difficult both to systematically learn the textual variation of multiple styles as well as properly evaluate the models. In this paper, we propose a large scale, humanannotated, parallel stylistic dataset called PASTEL, with focus on multiple types of personas in conjunction. Ideally, annotations for a parallel 1696 Proceedings of the 2019 Conference on Empirical Methods in Natural La"
D19-1179,N18-1012,0,0.239612,"tive models in the same way as style transfer in images, where meaning preservation is not controlled systematically. Prabhumoye et al. (2018) proposes backtranslation to get a style-agnostic sentence representation. However, they lack parallel ground truth for evaluation and present limited evaluation for meaning preservation. Style transfer with parallel corpus: Few recent works use parallel text for style transfer between modern and Shakespearean text (Jhamtani et al., 2017), sarcastic and literal tweets (Peled and Reichart, 2017), and formal and informal text (Heylighen and Dewaele, 1999; Rao and Tetreault, 2018). Compared to these, we aim to understand and demonstrate style variation owing to multiple demographic attributes. Besides the style transfer, other applications using stylistic features have been studied such as poetry generation (Ghazvininejad et al., 2017), stylometry with demographic information (Verhoeven and Daelemans, 2014), modeling style bias (Vogel and Jurafsky, 2012) and modeling biographic attributes (Garera and Yarowsky, 2009). A series of works by (Koppel et al., 2011, 2009; Argamon et al., 2009; Koppel and Winter, 2014) and their shared tasks (Stamatatos et al., 2018) show huge"
D19-1179,W16-5603,0,0.167784,"e) and even pragmatics, while preserving the original meaning of given text (DiMarco and Hirst, 1990). Connecting such textual features to someone’s persona is an important study to understand stylistic variation of language. For example, do highly educated people write longer sentences (Bloomfield, 1927)? Are Hispanic and East Asian people more likely to drop pronouns (White, 1985)? Are elder people likely to use lesser anaphora (Ulatowska et al., 1986)? To computationally model a meaning-preserved variance of text across styles, many recent works have developed systems that transfer styles (Reddy and Knight, 2016; Hu et al., 2017; Prabhumoye et al., 2018) or profiles authorships from text (Verhoeven and Daelemans, 2014; Koppel et al., 2009; Stamatatos et al., 2018) without parallel corpus of stylistic text. However, the absence of such a parallel dataset makes it difficult both to systematically learn the textual variation of multiple styles as well as properly evaluate the models. In this paper, we propose a large scale, humanannotated, parallel stylistic dataset called PASTEL, with focus on multiple types of personas in conjunction. Ideally, annotations for a parallel 1696 Proceedings of the 2019 Co"
D19-1327,P11-1049,0,0.0114967,"ems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015) proposed a local attention-based sequenceto-sequence model. On top of the seq2seq framework, many other variants have been studied using convolutional networks (Cheng and Lapata, 2016; Allamanis et al., 2016), pointer networks (See et al., 2017), scheduled sampling (Bengio et al., 2015), and reinforcement learning (Paulus et al., 2017). In extractive systems,"
D19-1327,N13-1030,0,0.0224751,"Yogatama et al. (2015) difficult tasks that require multiples aspects toused semantic volumes of bigram features for exgether. tractive summarization. Internal structures of doc• Biases do exist in current summarization sysuments have been used in summarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015) proposed a local attention-based sequenceto-sequence model. On top of the seq2seq f"
D19-1327,D15-1220,0,0.0629354,"ummarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015) proposed a local attention-based sequenceto-sequence model. On top of the seq2seq framework, many other variants have been studied using convolutional networks (Cheng and Lapata, 2016; Allamanis et al., 2016), pointer networks (See et al., 2017), scheduled sampling (Bengio et al., 2015), and reinforcement learning (Paulus et al"
D19-1327,P16-1046,0,0.292374,"r-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015) proposed a local attention-based sequenceto-sequence model. On top of the seq2seq framework, many other variants have been studied using convolutional networks (Cheng and Lapata, 2016; Allamanis et al., 2016), pointer networks (See et al., 2017), scheduled sampling (Bengio et al., 2015), and reinforcement learning (Paulus et al., 2017). In extractive systems, different types of encoders (Cheng and Lapata, 2016; Nallapati et al., 2017; Kedzie et al., 2018) and optimization techniques (Narayan et al., 2018b) have been developed. Our goal is to explore which types of systems learns which sub-aspect of summarization. 3 et al., 2017)2 . We use the last layer from BERT as a representation of each token, and then average them to get final representation of a sentence. All tokens"
D19-1327,C08-1018,0,0.0303535,"Missing"
D19-1327,P16-1188,0,0.0154762,"bling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015) proposed a local attention-based sequenceto-sequence model. On top of the seq2seq framework, many other variants have been studied using convolutional networks (Cheng and Lapata, 2016; Allamanis et al., 2016), pointer networks (See et al., 2017), scheduled sampling (Bengio et al., 2015), and reinforcement learning (Paulus et al., 2017). In extractive systems, different types of enc"
D19-1327,C10-1037,0,0.0327603,"t less redundant. Yogatama et al. (2015) difficult tasks that require multiples aspects toused semantic volumes of bigram features for exgether. tractive summarization. Internal structures of doc• Biases do exist in current summarization sysuments have been used in summarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015) proposed a local attention-based sequenceto-sequence model"
D19-1327,C10-1039,0,0.0796068,"extremely relevant but less redundant. Yogatama et al. (2015) difficult tasks that require multiples aspects toused semantic volumes of bigram features for exgether. tractive summarization. Internal structures of doc• Biases do exist in current summarization sysuments have been used in summarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015) proposed a local attention-based sequence"
D19-1327,D18-1443,0,0.0325196,"substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that different types of summarization systems (e.g., neural-based) are composed of different degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system. 1 Introduction Despite numerous recent developments in neural summarization systems (Narayan et al., 2018b; Nallapati et al., 2016; See et al., 2017; Kedzie et al., 2018; Gehrmann et al., 2018; Paulus et al., 2017) the underlying rationales behind the improvements and their dependence on the training corpus remain largely unexplored. Edmundson (1969) put forth the position hypothesis: important sentences appear in preferred positions in the document. Lin and Hovy (1997) provide a method to empirically identify such positions. Later, Hong and Nenkova (2014) showed an intentional lead ∗ Equal contribution, name order decided by coin flip. bias in news writing, suggesting that sentences appearing early in news articles are more important for summarization tasks. More generally, it is"
D19-1327,D14-1168,0,0.0255911,"meeting minutes) are extremely relevant but less redundant. Yogatama et al. (2015) difficult tasks that require multiples aspects toused semantic volumes of bigram features for exgether. tractive summarization. Internal structures of doc• Biases do exist in current summarization sysuments have been used in summarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015) proposed a local at"
D19-1327,W09-1802,0,0.282324,"rent summarization sysuments have been used in summarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015) proposed a local attention-based sequenceto-sequence model. On top of the seq2seq framework, many other variants have been studied using convolutional networks (Cheng and Lapata, 2016; Allamanis et al., 2016), pointer networks (See et al., 2017), scheduled sampling (Bengio et al., 201"
D19-1327,N15-1113,0,0.171246,"f sub-aspect analysis across various corpora and corpus and each system. The portion is measured by systems. calculating ROUGE score between (a) summaries obWe analyze the sub-aspects on different dotained from each aspect and target summaries or (b) summaries obtained from each aspect and each system. mains of summarization corpora: news articles (Nallapati et al., 2016; Grusky et al., 2018; Narayan et al., 2018a), academic papers or jourfactors called system bias (§7). At last, we sumnals (Kang et al., 2018; Kedzie et al., 2018), movie marize our actionable messages for future summascripts (Gorinski and Lapata, 2015), books (Mirization researches (§8). We summarize some nohalcea and Ceylan, 2007), personal posts (Ouyang table findings as follows: et al., 2017), and meeting minutes (Carletta et al., 2005) as described further in §5. • Summarization of personal post and news artiBeyond the corpora themselves, a variety of cles except for XSum (Narayan et al., 2018a) are summarization systems have been developed: Mibiased to the position aspect, while academic halcea and Tarau (2004); Erkan and Radev (2004) papers are well balanced among the three asused graph-based keyword ranking algorithms. pects (see Fig"
D19-1327,N18-1065,0,0.093148,"Missing"
D19-1327,E14-1075,0,0.355444,"ting a new summarization dataset or developing a new system. 1 Introduction Despite numerous recent developments in neural summarization systems (Narayan et al., 2018b; Nallapati et al., 2016; See et al., 2017; Kedzie et al., 2018; Gehrmann et al., 2018; Paulus et al., 2017) the underlying rationales behind the improvements and their dependence on the training corpus remain largely unexplored. Edmundson (1969) put forth the position hypothesis: important sentences appear in preferred positions in the document. Lin and Hovy (1997) provide a method to empirically identify such positions. Later, Hong and Nenkova (2014) showed an intentional lead ∗ Equal contribution, name order decided by coin flip. bias in news writing, suggesting that sentences appearing early in news articles are more important for summarization tasks. More generally, it is well known that recent state-of-the-art models (Nallapati et al., 2016; See et al., 2017) are often marginally better than the first-k baseline on single-document news summarization. In order to address the position bias of news articles, Narayan et al. (2018a) collected a new dataset called XSum to create single sentence summaries that include material from multiple"
D19-1327,P14-1115,0,0.0145249,"ghly versations (e.g. meeting minutes) are extremely relevant but less redundant. Yogatama et al. (2015) difficult tasks that require multiples aspects toused semantic volumes of bigram features for exgether. tractive summarization. Internal structures of doc• Biases do exist in current summarization sysuments have been used in summarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015"
D19-1327,N18-1149,1,0.879819,"Missing"
D19-1327,D07-1040,0,0.0505911,"itten by the original author. • PeerRead (Kang et al., 2018): consists of scientific paper drafts in top-tier computer science venues as well as arxiv.org. We use full text of introduction section as source document and of abstract section as target summaries. • PubMed (Kedzie et al., 2018): is 25,000 medical journal papers from the PubMed Open Access Subset.7 Unlike PeerRead, full paper except for abstract is used as source documents. • MScript (Gorinski and Lapata, 2015): is a collection of movie scripts from ScriptBase corpus and their corresponding user summaries of the movies. • BookSum (Mihalcea and Ceylan, 2007): is a dataset of classic books paired to summaries from Grade Saver8 and Cliffs Notes9 . Due to a large number of sentences, we only choose the first 1K sentences for source document and the first 50 sentences for target summaries. • Reddit (Ouyang et al., 2017): is a collection of personal posts from reddit.com. We use a single abstractive summary per post. The same data split from Kedzie et al. (2018) is used. • AMI (Carletta et al., 2005): is documented meeting minutes from a hundred hours of recordings and their abstractive summaries. 7 https://www.ncbi.nlm.nih.gov/pmc/tools/ openftlist/"
D19-1327,D18-1208,0,0.127694,"ile position exhibits substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that different types of summarization systems (e.g., neural-based) are composed of different degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system. 1 Introduction Despite numerous recent developments in neural summarization systems (Narayan et al., 2018b; Nallapati et al., 2016; See et al., 2017; Kedzie et al., 2018; Gehrmann et al., 2018; Paulus et al., 2017) the underlying rationales behind the improvements and their dependence on the training corpus remain largely unexplored. Edmundson (1969) put forth the position hypothesis: important sentences appear in preferred positions in the document. Lin and Hovy (1997) provide a method to empirically identify such positions. Later, Hong and Nenkova (2014) showed an intentional lead ∗ Equal contribution, name order decided by coin flip. bias in news writing, suggesting that sentences appearing early in news articles are more important for summarization tasks."
D19-1327,W04-3252,0,0.00911517,"they are. O∩T shows N-gram overlap between oracle and target summaries. The higher the more overlapped words in between. TS is a proportion of N-grams in target summaries not occurred in source document. The lower the more abstractive (i.e., new words) target summaries. tems shows comparable performance to the singleaspect systems. Existing systems. We compare various extractive and abstractive systems: For extractive systems, we use K-Means (Lin and Bilmes, 2010), Maximal Marginal Relevance (MMR) (Carbonell and Goldstein, 1998), cILP (Gillick and Favre, 2009; Boudin et al., 2015), TexRank (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004) and three recent neural systems; CL (Cheng and Lapata, 2016), SumRun (Nallapati et al., 2017), and S2SExt (Kedzie et al., 2018). For abstractive systems, we use WordILP (Banerjee et al., 2015) and four neural systems; S2SAbs (Rush et al., 2015), Pointer (See et al., 2017), Teacher (Bengio et al., 2015), and RL (Paulus et al., 2017). The detailed description and experimental setup for each algorithm are in Appendix. Proposed ensemble systems. Motivated by the sub-aspect theory (Lin and Bilmes, 2012, 2011), we combine different types of systems together from two"
D19-1327,W04-1013,0,0.111418,"Missing"
D19-1327,A97-1042,1,0.781335,"rovides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system. 1 Introduction Despite numerous recent developments in neural summarization systems (Narayan et al., 2018b; Nallapati et al., 2016; See et al., 2017; Kedzie et al., 2018; Gehrmann et al., 2018; Paulus et al., 2017) the underlying rationales behind the improvements and their dependence on the training corpus remain largely unexplored. Edmundson (1969) put forth the position hypothesis: important sentences appear in preferred positions in the document. Lin and Hovy (1997) provide a method to empirically identify such positions. Later, Hong and Nenkova (2014) showed an intentional lead ∗ Equal contribution, name order decided by coin flip. bias in news writing, suggesting that sentences appearing early in news articles are more important for summarization tasks. More generally, it is well known that recent state-of-the-art models (Nallapati et al., 2016; See et al., 2017) are often marginally better than the first-k baseline on single-document news summarization. In order to address the position bias of news articles, Narayan et al. (2018a) collected a new data"
D19-1327,C00-1072,1,0.63728,"(1998) found summary sentences which are highly versations (e.g. meeting minutes) are extremely relevant but less redundant. Yogatama et al. (2015) difficult tasks that require multiples aspects toused semantic volumes of bigram features for exgether. tractive summarization. Internal structures of doc• Biases do exist in current summarization sysuments have been used in summarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed"
D19-1327,N10-1134,0,0.190179,"§8). We summarize some nohalcea and Ceylan, 2007), personal posts (Ouyang table findings as follows: et al., 2017), and meeting minutes (Carletta et al., 2005) as described further in §5. • Summarization of personal post and news artiBeyond the corpora themselves, a variety of cles except for XSum (Narayan et al., 2018a) are summarization systems have been developed: Mibiased to the position aspect, while academic halcea and Tarau (2004); Erkan and Radev (2004) papers are well balanced among the three asused graph-based keyword ranking algorithms. pects (see Figure 1 (a)). Summarizing long docLin and Bilmes (2010); Carbonell and Goldstein uments (e.g. books and movie scripts) and con(1998) found summary sentences which are highly versations (e.g. meeting minutes) are extremely relevant but less redundant. Yogatama et al. (2015) difficult tasks that require multiples aspects toused semantic volumes of bigram features for exgether. tractive summarization. Internal structures of doc• Biases do exist in current summarization sysuments have been used in summarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable"
D19-1327,P11-1052,0,0.0436835,"nally better than the first-k baseline on single-document news summarization. In order to address the position bias of news articles, Narayan et al. (2018a) collected a new dataset called XSum to create single sentence summaries that include material from multiple positions in the source document. Kedzie et al. (2018) showed that the position bias in news articles is not the same across other domains such as meeting minutes (Carletta et al., 2005). In addition to position, Lin and Bilmes (2012) defined other sub-aspect functions of summarization including coverage, diversity, and information. Lin and Bilmes (2011) claim that many existing summarization systems are instances of mixtures of such sub-aspect functions; for example, maximum marginal relevance (MMR) (Carbonell and Goldstein, 1998) can be seen as an combination of diversity and importance functions. Following the sub-aspect theory, we explore three important aspects of summarization (§3): position for choosing sentences by their position, importance for choosing relevant contents, and diversity for ensuring minimal redundancy between summary sentences. We then conduct an in-depth analysis of these aspects over nine different domains of summar"
D19-1327,N15-1114,0,0.014399,"aspects toused semantic volumes of bigram features for exgether. tractive summarization. Internal structures of doc• Biases do exist in current summarization sysuments have been used in summarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015) proposed a local attention-based sequenceto-sequence model. On top of the seq2seq framework, many other variants have been studied using co"
D19-1327,K16-1028,0,0.487391,"vie script, books, posts). We find that while position exhibits substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that different types of summarization systems (e.g., neural-based) are composed of different degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system. 1 Introduction Despite numerous recent developments in neural summarization systems (Narayan et al., 2018b; Nallapati et al., 2016; See et al., 2017; Kedzie et al., 2018; Gehrmann et al., 2018; Paulus et al., 2017) the underlying rationales behind the improvements and their dependence on the training corpus remain largely unexplored. Edmundson (1969) put forth the position hypothesis: important sentences appear in preferred positions in the document. Lin and Hovy (1997) provide a method to empirically identify such positions. Later, Hong and Nenkova (2014) showed an intentional lead ∗ Equal contribution, name order decided by coin flip. bias in news writing, suggesting that sentences appearing early in news articles are"
D19-1327,D18-1206,0,0.450549,"rs, meeting minutes, movie script, books, posts). We find that while position exhibits substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that different types of summarization systems (e.g., neural-based) are composed of different degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system. 1 Introduction Despite numerous recent developments in neural summarization systems (Narayan et al., 2018b; Nallapati et al., 2016; See et al., 2017; Kedzie et al., 2018; Gehrmann et al., 2018; Paulus et al., 2017) the underlying rationales behind the improvements and their dependence on the training corpus remain largely unexplored. Edmundson (1969) put forth the position hypothesis: important sentences appear in preferred positions in the document. Lin and Hovy (1997) provide a method to empirically identify such positions. Later, Hong and Nenkova (2014) showed an intentional lead ∗ Equal contribution, name order decided by coin flip. bias in news writing, suggesting that sentences appearing ea"
D19-1327,N18-1158,0,0.309124,"rs, meeting minutes, movie script, books, posts). We find that while position exhibits substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that different types of summarization systems (e.g., neural-based) are composed of different degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system. 1 Introduction Despite numerous recent developments in neural summarization systems (Narayan et al., 2018b; Nallapati et al., 2016; See et al., 2017; Kedzie et al., 2018; Gehrmann et al., 2018; Paulus et al., 2017) the underlying rationales behind the improvements and their dependence on the training corpus remain largely unexplored. Edmundson (1969) put forth the position hypothesis: important sentences appear in preferred positions in the document. Lin and Hovy (1997) provide a method to empirically identify such positions. Later, Hong and Nenkova (2014) showed an intentional lead ∗ Equal contribution, name order decided by coin flip. bias in news writing, suggesting that sentences appearing ea"
D19-1327,E17-2008,0,0.0669895,"(Kedzie et al., 2018): is 25,000 medical journal papers from the PubMed Open Access Subset.7 Unlike PeerRead, full paper except for abstract is used as source documents. • MScript (Gorinski and Lapata, 2015): is a collection of movie scripts from ScriptBase corpus and their corresponding user summaries of the movies. • BookSum (Mihalcea and Ceylan, 2007): is a dataset of classic books paired to summaries from Grade Saver8 and Cliffs Notes9 . Due to a large number of sentences, we only choose the first 1K sentences for source document and the first 50 sentences for target summaries. • Reddit (Ouyang et al., 2017): is a collection of personal posts from reddit.com. We use a single abstractive summary per post. The same data split from Kedzie et al. (2018) is used. • AMI (Carletta et al., 2005): is documented meeting minutes from a hundred hours of recordings and their abstractive summaries. 7 https://www.ncbi.nlm.nih.gov/pmc/tools/ openftlist/ 8 http://www.gradesaver.com 9 http://www.cliffsnotes.com/ Table 1 summarizes the characteristics of each dataset. We note that the Gigaword (Graff et al., 2003), New York Times10 , and Document Understanding Conference (DUC)11 are also popular datasets commonly u"
D19-1327,D14-1162,0,0.0819883,"e source document. (Figure 2 (a)). However, its volume space does not guarantee to maximize the volume size because of the non-convex polygonality. In order to find a convex maximum volume, we consider two different algorithms described below. Heuristic. Yogatama et al. (2015) heuristically choose a set of summary sentences using a greedy algorithm: It first chooses a sentence which has the farthest vector representation from the centroid of whole source sentences, and then repeatedly finds sentences whose representation is farthest from 2 The other encoders such as averaging word embeddings (Pennington et al., 2014) show comparable performance. 3326 the centroid of vector representations of the chosen sentences. Unlike the original algorithm in (Yogatama et al., 2015) restricting the number of words, we constrain the total number of selected sentences to k. This heuristic algorithm can fail to find the maximum volume depending on its starting point and/or the farther distance between two points detected (Figure 2 (b)). ConvexFall. Here we first find the convexhull3 using Quickhull (Barber et al., 1996), implemented by Qhull library4 . It guarantees the maximum volume size of selected points with minimum"
D19-1327,P19-1101,0,0.20869,"Read et al., 2018a) but not in other domains such as conPubmed Reddit AMI BookSum MScript Position versations (Kedzie et al., 2018). Narayan et al. 50 Position (2018a) collected a new corpus to address the bias 50 by compressing multiple contents of source doc25 ument in the single target summary. In the bias 25 analysis of systems, Lin and Bilmes (2012, 2011) 0 studied the sub-aspect hypothesis of summariza0 tion systems. Our study extends the hypothesis Importance Diversity to various corpora as well as systems. With a specific focus on importance aspect, a recent Importance Diversity work (Peyrard, 2019a) divided it into three sub(b) System bias categories; redundancy, relevance, and informativeness, and provided quantities of each to meaFigure 1: Corpus and system biases with the three subsure. Compared to this, ours provide broader scale aspects, showing what portion of aspect is used for each of sub-aspect analysis across various corpora and corpus and each system. The portion is measured by systems. calculating ROUGE score between (a) summaries obWe analyze the sub-aspects on different dotained from each aspect and target summaries or (b) summaries obtained from each aspect and each syst"
D19-1327,P19-1502,0,0.192353,"Read et al., 2018a) but not in other domains such as conPubmed Reddit AMI BookSum MScript Position versations (Kedzie et al., 2018). Narayan et al. 50 Position (2018a) collected a new corpus to address the bias 50 by compressing multiple contents of source doc25 ument in the single target summary. In the bias 25 analysis of systems, Lin and Bilmes (2012, 2011) 0 studied the sub-aspect hypothesis of summariza0 tion systems. Our study extends the hypothesis Importance Diversity to various corpora as well as systems. With a specific focus on importance aspect, a recent Importance Diversity work (Peyrard, 2019a) divided it into three sub(b) System bias categories; redundancy, relevance, and informativeness, and provided quantities of each to meaFigure 1: Corpus and system biases with the three subsure. Compared to this, ours provide broader scale aspects, showing what portion of aspect is used for each of sub-aspect analysis across various corpora and corpus and each system. The portion is measured by systems. calculating ROUGE score between (a) summaries obWe analyze the sub-aspects on different dotained from each aspect and target summaries or (b) summaries obtained from each aspect and each syst"
D19-1327,D15-1044,0,0.207668,"hdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with grammatical and anarphorcity constraints. With a large scale of corpora for training, neural network based systems have recently been developed. In abstractive systems, Rush et al. (2015) proposed a local attention-based sequenceto-sequence model. On top of the seq2seq framework, many other variants have been studied using convolutional networks (Cheng and Lapata, 2016; Allamanis et al., 2016), pointer networks (See et al., 2017), scheduled sampling (Bengio et al., 2015), and reinforcement learning (Paulus et al., 2017). In extractive systems, different types of encoders (Cheng and Lapata, 2016; Nallapati et al., 2017; Kedzie et al., 2018) and optimization techniques (Narayan et al., 2018b) have been developed. Our goal is to explore which types of systems learns which sub-asp"
D19-1327,P17-1099,0,0.719969,"). We find that while position exhibits substantial bias in news articles, this is not the case, for example, with academic papers and meeting minutes. Furthermore, our empirical study shows that different types of summarization systems (e.g., neural-based) are composed of different degrees of the sub-aspects. Our study provides useful lessons regarding consideration of underlying sub-aspects when collecting a new summarization dataset or developing a new system. 1 Introduction Despite numerous recent developments in neural summarization systems (Narayan et al., 2018b; Nallapati et al., 2016; See et al., 2017; Kedzie et al., 2018; Gehrmann et al., 2018; Paulus et al., 2017) the underlying rationales behind the improvements and their dependence on the training corpus remain largely unexplored. Edmundson (1969) put forth the position hypothesis: important sentences appear in preferred positions in the document. Lin and Hovy (1997) provide a method to empirically identify such positions. Later, Hong and Nenkova (2014) showed an intentional lead ∗ Equal contribution, name order decided by coin flip. bias in news writing, suggesting that sentences appearing early in news articles are more important for"
D19-1327,D11-1038,0,0.0112851,"thms. pects (see Figure 1 (a)). Summarizing long docLin and Bilmes (2010); Carbonell and Goldstein uments (e.g. books and movie scripts) and con(1998) found summary sentences which are highly versations (e.g. meeting minutes) are extremely relevant but less redundant. Yogatama et al. (2015) difficult tasks that require multiples aspects toused semantic volumes of bigram features for exgether. tractive summarization. Internal structures of doc• Biases do exist in current summarization sysuments have been used in summarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013), and abstract meaning representation (Liu et al., 2015). Concept-based Integer-Linear Programming (ILP) solver (McDonald, 2007) is used for optimizing the summarization problem (Gillick and Favre, 2009; Banerjee et al., 2015; Boudin et al., 2015; Berg-Kirkpatrick et al., 2011). Durrett et al. (2016) optimized the problem with gramm"
D19-1327,D15-1228,0,0.271906,"ost and news artiBeyond the corpora themselves, a variety of cles except for XSum (Narayan et al., 2018a) are summarization systems have been developed: Mibiased to the position aspect, while academic halcea and Tarau (2004); Erkan and Radev (2004) papers are well balanced among the three asused graph-based keyword ranking algorithms. pects (see Figure 1 (a)). Summarizing long docLin and Bilmes (2010); Carbonell and Goldstein uments (e.g. books and movie scripts) and con(1998) found summary sentences which are highly versations (e.g. meeting minutes) are extremely relevant but less redundant. Yogatama et al. (2015) difficult tasks that require multiples aspects toused semantic volumes of bigram features for exgether. tractive summarization. Internal structures of doc• Biases do exist in current summarization sysuments have been used in summarization: syntactems (Figure 1 (b)). Simple ensembling of multic parse trees (Woodsend and Lapata, 2011; Cohn tiple aspects of systems show comparable per(a) Corpus bias 3325 and Lapata, 2008), topics (Zajic et al., 2004; Lin and Hovy, 2000), semantic word graphs (Mehdad et al., 2014; Gerani et al., 2014; Ganesan et al., 2010; Filippova, 2010; Boudin and Morin, 2013)"
D19-1437,Q19-1042,0,0.0622647,"com/XuezheMax/flowseq T Y P✓ (yt |y&lt;t , x). (1) Each factor, P✓ (yt |y&lt;t , x), can be implemented by function approximators such as RNNs (Bahdanau et al., 2015) and Transformers (Vaswani et al., 2017). This factorization takes the complicated problem of joint estimation over an exponentially large output space of outputs y, and turns it into a sequence of tractable multi-class classification problems predicting yt given the previous words, allowing for simple maximum loglikelihood training. However, this assumption of left-to-right factorization may be sub-optimal from a modeling perspective (Gu et al., 2019; Stern et al., 2019), and generation of outputs must be done through a linear left-to-right pass through the output tokens using beam search, which is not easily parallelizable on hardware such as GPUs. Recently, there has been work on nonautoregressive sequence generation for neural machine translation (NMT; Gu et al. (2018); Lee et al. (2018); Ghazvininejad et al. (2019)) and language modeling (Ziegler and Rush, 2019). Nonautoregressive models attempt to model the joint distribution P✓ (y|x) directly, decoupling the dependencies of decoding history during generation. 4282 Proceedings of the"
D19-1437,D18-1149,0,0.119466,"sequence of tractable multi-class classification problems predicting yt given the previous words, allowing for simple maximum loglikelihood training. However, this assumption of left-to-right factorization may be sub-optimal from a modeling perspective (Gu et al., 2019; Stern et al., 2019), and generation of outputs must be done through a linear left-to-right pass through the output tokens using beam search, which is not easily parallelizable on hardware such as GPUs. Recently, there has been work on nonautoregressive sequence generation for neural machine translation (NMT; Gu et al. (2018); Lee et al. (2018); Ghazvininejad et al. (2019)) and language modeling (Ziegler and Rush, 2019). Nonautoregressive models attempt to model the joint distribution P✓ (y|x) directly, decoupling the dependencies of decoding history during generation. 4282 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4282–4292, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics A na¨ıve solution is to assume that each token of the target sequence is independent given the inpu"
D19-1437,D18-1336,0,0.188113,"Missing"
D19-1437,P18-1130,1,0.823148,"oder, the decoder and the posterior networks, together with the multi-scale architecture of the prior flow. The architecture of each flow step is in Figure 3. “correct” token at each step t with zt as input. In this case, FlowSeq reduces to the baseline model in Eq. (2). To escape this undesired local optimum, we apply token-level dropout to randomly drop an entire token when calculating the posterior, to ensure the model also has to learn how to use contextual information. This technique is similar to the “masked language model” in previous studies (Melamud et al., 2016; Devlin et al., 2018; Ma et al., 2018). 3.3 Decoder As the decoder, we take the latent sequence z as input, run it through several layers of a neural sequence model such as a Transformer, then directly predict the output tokens in y individually and independently. Notably, unlike standard seq2seq decoders, we do not perform causal masking to prevent attending to future tokens, making the model fully non-autoregressive. 3.4 Flow Architecture for Prior The flow architecture is based on Glow (Kingma and Dhariwal, 2018). It consists of a series of steps of flow, combined in a multi-scale architecture (see Figure 2.) Each step of flow"
D19-1437,K16-1006,0,0.0154009,"architecture of FlowSeq, including the encoder, the decoder and the posterior networks, together with the multi-scale architecture of the prior flow. The architecture of each flow step is in Figure 3. “correct” token at each step t with zt as input. In this case, FlowSeq reduces to the baseline model in Eq. (2). To escape this undesired local optimum, we apply token-level dropout to randomly drop an entire token when calculating the posterior, to ensure the model also has to learn how to use contextual information. This technique is similar to the “masked language model” in previous studies (Melamud et al., 2016; Devlin et al., 2018; Ma et al., 2018). 3.3 Decoder As the decoder, we take the latent sequence z as input, run it through several layers of a neural sequence model such as a Transformer, then directly predict the output tokens in y individually and independently. Notably, unlike standard seq2seq decoders, we do not perform causal masking to prevent attending to future tokens, making the model fully non-autoregressive. 3.4 Flow Architecture for Prior The flow architecture is based on Glow (Kingma and Dhariwal, 2018). It consists of a series of steps of flow, combined in a multi-scale architec"
D19-1437,N19-4009,0,0.0332461,"all the input tokens can be fed into the RNN in parallel. This makes it possible to use highly-optimized implementations of RNNs such as those provided by cuDNN.3 Thus while RNNs do experience some drop in speed, it is less extreme than that experienced when using autoregressive models. 4 4.1 Experiments Experimental Setups Translation Datasets We evaluate FlowSeq on three machine translation benchmark datasets: WMT2014 DE-EN (around 4.5M sentence pairs), WMT2016 RO-EN (around 610K sentence pairs) and a smaller dataset IWSLT2014 DE-EN (around 150K sentence pairs). We use scripts from fairseq (Ott et al., 2019) to preprocess WMT2014 and IWSLT2014, where the preprocessing steps follow Vaswani et al. (2017) for WMT2014. We use the data provided in Lee et al. (2018) for WMT2016. For both WMT datasets, the source and target languages share the same set of BPE embeddings while for IWSLT2014 we use separate embeddings. During training, we filter out sentences longer than 80 for WMT dataset and 60 for IWSLT, respectively. 3 https://devblogs.nvidia.com/optimizing-recurrentneural-networks-cudnn-5/ 4287 Models WMT2014 EN-DE DE-EN WMT2016 EN-RO RO-EN IWSLT2014 DE-EN WMT2014 EN-DE DE-EN Models Raw Data WMT2016"
D19-1437,D15-1044,0,0.0570656,"al latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art nonautoregressive NMT models and almost constant decoding time w.r.t the sequence length.1 1 (c) (b) Figure 1: (a) Autoregressive (b) non-autoregressive and (c) our proposed sequence generation models. x is the source, y is the target, and z are latent variables. ken in the sequence given the input sequence and previously generated tokens: P✓ (y|x) = t=1 Introduction Neural sequence-to-sequence (seq2seq) models (Bahdanau et al., 2015; Rush et al., 2015; Vinyals et al., 2015; Vaswani et al., 2017) generate an output sequence y = {y1 , . . . , yT } given an input sequence x = {x1 , . . . , xT 0 } using conditional probabilities P✓ (y|x) predicted by neural networks (parameterized by ✓). Most seq2seq models are autoregressive, meaning that they factorize the joint probability of the output sequence given the input sequence P✓ (y|x) into the product of probabilities over the next to1 ⇤ Equal contribution, in alphabetical order. https://github.com/XuezheMax/flowseq T Y P✓ (yt |y&lt;t , x). (1) Each factor, P✓ (yt |y&lt;t , x), can be implemented by fu"
D19-1589,W05-0909,0,0.023792,"him what I wanted. [4] He knew me well and always thought about what I wanted or needed, in and out of bed. Figure 2: Bridging task: given [1] and [4] sentences, guessing [2,3] sentences (red, underlined). Figure 2 shows our bridging task. It requires a generation of masked sentences in the middle of a paragraph given the first and the last sentences. If only the first sentence is given, the generation can be too divergent. The existence of the last sentence makes the generation more coherent and converged to some point. We evaluate it with one hard and one soft automatic metrics: METEOR (M) (Banerjee and Lavie, 2005) and VectorExtrema (VE) (Liu et al., 2016) by calculating cosine similarity of averaged word embeddings (Pennington et al., 2014), and human performance. 4.3 and last sentences, and decode the intermediate words: S2S is attentional seq2seq model (Bahdanau et al., 2014), and HS2S: is a hierarchical version of the S2S by combining two baselines: HRNN (Lin et al., 2015) hierarchically models sequence of words and sentences, and HRED (Serban et al., 2017; Sordoni et al., 2015) encodes the given context and decodes the words. FlowNet (delta/disc.) is our proposed language model with delta and disco"
D19-1589,P14-1002,0,0.466176,"es) and hand-written rules. Kang et al. (2017); Gardent et al. (2017); Kang et al. (2018b); Wang et al. (2018) used an external knowledge base to micro-planning for generating a corresponding text, while our work focuses on comparing two forms of relations from the text itself. Moore and Paris (1993); Young and Moore (1994) utilized discourse structures such as rhetorical structure theory (RST) (Mann and Thompson, 1988) for parsing a document. A script (Tomkins, 1978) is another structured representation that describes a typical sequence of events in a particular context. Zhang et al. (2016); Ji and Eisenstein (2014) proposed better discourse parsers using neural networks. The prior works, however, used the discourse representations to describe the structure of the paragraph, while we focus on applicability of the discourse relations to language generation. Latent relations use implicit information in a document such as hierarchical structure of the document: Lin et al. (2015); Chung et al. (2016) used hierarchical RNN for modeling a document. Similarly, the hierarchical model can be extended to other variants such as attention (Yang et al., 2016), encoder-decoder framework (Serban et al., 2017; Sordoni e"
D19-1589,N18-1149,1,0.901631,"Missing"
D19-1589,D17-1292,1,0.846534,"age Processing and the 9th International Joint Conference on Natural Language Processing, pages 5809–5815, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Linguistic relations are explicitly represented as external labels in the form of predefined rules or plans, formats, knowledge base, discourse parses, and more. Hovy (1985, 1990); Dalianis and Hovy (1996) integrated text planning in generation, where the plans are considered in knowledge, formatted rules and so forth. However, they are limited to small scale (i.e. few examples) and hand-written rules. Kang et al. (2017); Gardent et al. (2017); Kang et al. (2018b); Wang et al. (2018) used an external knowledge base to micro-planning for generating a corresponding text, while our work focuses on comparing two forms of relations from the text itself. Moore and Paris (1993); Young and Moore (1994) utilized discourse structures such as rhetorical structure theory (RST) (Mann and Thompson, 1988) for parsing a document. A script (Tomkins, 1978) is another structured representation that describes a typical sequence of events in a particular context. Zhang et al. (2016); Ji and Eisenstein (2014) proposed better disco"
D19-1589,P18-1225,1,0.832814,"int Conference on Natural Language Processing, pages 5809–5815, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Linguistic relations are explicitly represented as external labels in the form of predefined rules or plans, formats, knowledge base, discourse parses, and more. Hovy (1985, 1990); Dalianis and Hovy (1996) integrated text planning in generation, where the plans are considered in knowledge, formatted rules and so forth. However, they are limited to small scale (i.e. few examples) and hand-written rules. Kang et al. (2017); Gardent et al. (2017); Kang et al. (2018b); Wang et al. (2018) used an external knowledge base to micro-planning for generating a corresponding text, while our work focuses on comparing two forms of relations from the text itself. Moore and Paris (1993); Young and Moore (1994) utilized discourse structures such as rhetorical structure theory (RST) (Mann and Thompson, 1988) for parsing a document. A script (Tomkins, 1978) is another structured representation that describes a typical sequence of events in a particular context. Zhang et al. (2016); Ji and Eisenstein (2014) proposed better discourse parsers using neural networks. The pr"
D19-1589,P15-1107,0,0.0285046,"rsers using neural networks. The prior works, however, used the discourse representations to describe the structure of the paragraph, while we focus on applicability of the discourse relations to language generation. Latent relations use implicit information in a document such as hierarchical structure of the document: Lin et al. (2015); Chung et al. (2016) used hierarchical RNN for modeling a document. Similarly, the hierarchical model can be extended to other variants such as attention (Yang et al., 2016), encoder-decoder framework (Serban et al., 2017; Sordoni et al., 2015), auto-encoding (Li et al., 2015), and multiscale (Chung et al., 2016). However, the hierarchical recurrence of sentences, which is dependent on topics, are less likely modeling a flow of a document. We further summarize the fundamental differences between the two forms of relations in Appendix. 3 FlowNet: Language Modeling with Inter-sentential Relations We propose language models that incorporate each relation to capture a high-level flow of text. 3.1 Discourse-driven FlowNet As a linguistic relation, we employ RST (Mann and Thompson, 1988) trees to represent discourse connections in the text. For simplicity, we limit usage"
D19-1589,P17-1017,0,0.0690616,"he 9th International Joint Conference on Natural Language Processing, pages 5809–5815, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Linguistic relations are explicitly represented as external labels in the form of predefined rules or plans, formats, knowledge base, discourse parses, and more. Hovy (1985, 1990); Dalianis and Hovy (1996) integrated text planning in generation, where the plans are considered in knowledge, formatted rules and so forth. However, they are limited to small scale (i.e. few examples) and hand-written rules. Kang et al. (2017); Gardent et al. (2017); Kang et al. (2018b); Wang et al. (2018) used an external knowledge base to micro-planning for generating a corresponding text, while our work focuses on comparing two forms of relations from the text itself. Moore and Paris (1993); Young and Moore (1994) utilized discourse structures such as rhetorical structure theory (RST) (Mann and Thompson, 1988) for parsing a document. A script (Tomkins, 1978) is another structured representation that describes a typical sequence of events in a particular context. Zhang et al. (2016); Ji and Eisenstein (2014) proposed better discourse parsers using neur"
D19-1589,D15-1106,0,0.411797,"structure theory (RST) (Mann and Thompson, 1988) for parsing a document. A script (Tomkins, 1978) is another structured representation that describes a typical sequence of events in a particular context. Zhang et al. (2016); Ji and Eisenstein (2014) proposed better discourse parsers using neural networks. The prior works, however, used the discourse representations to describe the structure of the paragraph, while we focus on applicability of the discourse relations to language generation. Latent relations use implicit information in a document such as hierarchical structure of the document: Lin et al. (2015); Chung et al. (2016) used hierarchical RNN for modeling a document. Similarly, the hierarchical model can be extended to other variants such as attention (Yang et al., 2016), encoder-decoder framework (Serban et al., 2017; Sordoni et al., 2015), auto-encoding (Li et al., 2015), and multiscale (Chung et al., 2016). However, the hierarchical recurrence of sentences, which is dependent on topics, are less likely modeling a flow of a document. We further summarize the fundamental differences between the two forms of relations in Appendix. 3 FlowNet: Language Modeling with Inter-sentential Relatio"
D19-1589,D16-1230,0,0.0382562,"thought about what I wanted or needed, in and out of bed. Figure 2: Bridging task: given [1] and [4] sentences, guessing [2,3] sentences (red, underlined). Figure 2 shows our bridging task. It requires a generation of masked sentences in the middle of a paragraph given the first and the last sentences. If only the first sentence is given, the generation can be too divergent. The existence of the last sentence makes the generation more coherent and converged to some point. We evaluate it with one hard and one soft automatic metrics: METEOR (M) (Banerjee and Lavie, 2005) and VectorExtrema (VE) (Liu et al., 2016) by calculating cosine similarity of averaged word embeddings (Pennington et al., 2014), and human performance. 4.3 and last sentences, and decode the intermediate words: S2S is attentional seq2seq model (Bahdanau et al., 2014), and HS2S: is a hierarchical version of the S2S by combining two baselines: HRNN (Lin et al., 2015) hierarchically models sequence of words and sentences, and HRED (Serban et al., 2017; Sordoni et al., 2015) encodes the given context and decodes the words. FlowNet (delta/disc.) is our proposed language model with delta and discourse relations, respectively. We find the"
D19-1589,J93-4004,0,0.645514,"al labels in the form of predefined rules or plans, formats, knowledge base, discourse parses, and more. Hovy (1985, 1990); Dalianis and Hovy (1996) integrated text planning in generation, where the plans are considered in knowledge, formatted rules and so forth. However, they are limited to small scale (i.e. few examples) and hand-written rules. Kang et al. (2017); Gardent et al. (2017); Kang et al. (2018b); Wang et al. (2018) used an external knowledge base to micro-planning for generating a corresponding text, while our work focuses on comparing two forms of relations from the text itself. Moore and Paris (1993); Young and Moore (1994) utilized discourse structures such as rhetorical structure theory (RST) (Mann and Thompson, 1988) for parsing a document. A script (Tomkins, 1978) is another structured representation that describes a typical sequence of events in a particular context. Zhang et al. (2016); Ji and Eisenstein (2014) proposed better discourse parsers using neural networks. The prior works, however, used the discourse representations to describe the structure of the paragraph, while we focus on applicability of the discourse relations to language generation. Latent relations use implicit i"
D19-1589,D14-1162,0,0.0820329,"k: given [1] and [4] sentences, guessing [2,3] sentences (red, underlined). Figure 2 shows our bridging task. It requires a generation of masked sentences in the middle of a paragraph given the first and the last sentences. If only the first sentence is given, the generation can be too divergent. The existence of the last sentence makes the generation more coherent and converged to some point. We evaluate it with one hard and one soft automatic metrics: METEOR (M) (Banerjee and Lavie, 2005) and VectorExtrema (VE) (Liu et al., 2016) by calculating cosine similarity of averaged word embeddings (Pennington et al., 2014), and human performance. 4.3 and last sentences, and decode the intermediate words: S2S is attentional seq2seq model (Bahdanau et al., 2014), and HS2S: is a hierarchical version of the S2S by combining two baselines: HRNN (Lin et al., 2015) hierarchically models sequence of words and sentences, and HRED (Serban et al., 2017; Sordoni et al., 2015) encodes the given context and decodes the words. FlowNet (delta/disc.) is our proposed language model with delta and discourse relations, respectively. We find the best hyper-parameters on validation set using grid search. Here are the final parameter"
D19-1589,P15-1150,0,0.0421482,"si−1 ) (1) j where si ={wi1 , wi2 , . . . , wiTi }, and Ti is the number of tokens of si . To better guide the model with discourse context, we use the shared representations to predict RST relations at the same time. For each paragraph, we run the pre-trained RST parser (Ji and Eisenstein, 2014) and flatten the parse tree to obtain RST relations for each sentence Yi =(y1 , . . . , yKi ), where Ki is the number of discourse relations in si . We then make a label sequence over tokens in the sentence with 2 The full discourse tree can be incorporated using other types of language model such as Tai et al. (2015). 5810 O O CAUSE O w21 w22 w31 w32 ... was dead because she ... CAUSE (a) Discourse-driven (b) Delta-driven Figure 1: FlowNet with linguistic (i.e., discourse) versus latent (i.e., delta) relation. (a) For each word, a form of discourse relation and next word are jointly predicted using CRF (}) and language model, respectively. (b) Decoding wi is conditioned on previous word (wi−1 ), previous sentence (si−1 ), and delta between two previous sentences (di−2 ). Best viewed in color. by placing y at the first word of EDUs and filling up the rest with a null relation o: Yi0 = (o, . . . , o, y1 , o"
D19-1589,W18-6502,0,0.0195454,"ural Language Processing, pages 5809–5815, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Linguistic relations are explicitly represented as external labels in the form of predefined rules or plans, formats, knowledge base, discourse parses, and more. Hovy (1985, 1990); Dalianis and Hovy (1996) integrated text planning in generation, where the plans are considered in knowledge, formatted rules and so forth. However, they are limited to small scale (i.e. few examples) and hand-written rules. Kang et al. (2017); Gardent et al. (2017); Kang et al. (2018b); Wang et al. (2018) used an external knowledge base to micro-planning for generating a corresponding text, while our work focuses on comparing two forms of relations from the text itself. Moore and Paris (1993); Young and Moore (1994) utilized discourse structures such as rhetorical structure theory (RST) (Mann and Thompson, 1988) for parsing a document. A script (Tomkins, 1978) is another structured representation that describes a typical sequence of events in a particular context. Zhang et al. (2016); Ji and Eisenstein (2014) proposed better discourse parsers using neural networks. The prior works, however, us"
D19-1589,N16-1174,1,0.750985,"concluding sentence. We call such a logical connection between sentences in a written paragraph as a flow. A coherent flow between sentences requires an understanding of various factors including tense, coreference, plans (Appelt, 1982; Hovy, 1991), scripts 1 https://github.com/dykang/flownet (Tomkins, 1978) and several others. We focus on the paragraph-level plan between sentences. In text planning, underlying relations in text are broadly categorized into two forms: an explicit human-defined relation (e.g., a discourse tree) (Reiter and Dale, 2000) or an implicitly learned latent relation (Yang et al., 2016). While the former is defined and manuallly annotated based on linguistic theories, the latter is simply determinable from how people in fact put sentences together. In this work, we provide an empirical comparison between a linguistically-informed and a latent form of relations in context of a paragraph generation. We compare the effectiveness of the two forms of relations using language modeling for paragraph generation. Due to the different characteristics of the two forms, we employ comparable but different components in addition to the base language model. For linguistic relations (e.g.,"
D19-1589,W94-0302,0,0.320001,"predefined rules or plans, formats, knowledge base, discourse parses, and more. Hovy (1985, 1990); Dalianis and Hovy (1996) integrated text planning in generation, where the plans are considered in knowledge, formatted rules and so forth. However, they are limited to small scale (i.e. few examples) and hand-written rules. Kang et al. (2017); Gardent et al. (2017); Kang et al. (2018b); Wang et al. (2018) used an external knowledge base to micro-planning for generating a corresponding text, while our work focuses on comparing two forms of relations from the text itself. Moore and Paris (1993); Young and Moore (1994) utilized discourse structures such as rhetorical structure theory (RST) (Mann and Thompson, 1988) for parsing a document. A script (Tomkins, 1978) is another structured representation that describes a typical sequence of events in a particular context. Zhang et al. (2016); Ji and Eisenstein (2014) proposed better discourse parsers using neural networks. The prior works, however, used the discourse representations to describe the structure of the paragraph, while we focus on applicability of the discourse relations to language generation. Latent relations use implicit information in a document"
D19-1589,D16-1037,0,0.0310165,"cale (i.e. few examples) and hand-written rules. Kang et al. (2017); Gardent et al. (2017); Kang et al. (2018b); Wang et al. (2018) used an external knowledge base to micro-planning for generating a corresponding text, while our work focuses on comparing two forms of relations from the text itself. Moore and Paris (1993); Young and Moore (1994) utilized discourse structures such as rhetorical structure theory (RST) (Mann and Thompson, 1988) for parsing a document. A script (Tomkins, 1978) is another structured representation that describes a typical sequence of events in a particular context. Zhang et al. (2016); Ji and Eisenstein (2014) proposed better discourse parsers using neural networks. The prior works, however, used the discourse representations to describe the structure of the paragraph, while we focus on applicability of the discourse relations to language generation. Latent relations use implicit information in a document such as hierarchical structure of the document: Lin et al. (2015); Chung et al. (2016) used hierarchical RNN for modeling a document. Similarly, the hierarchical model can be extended to other variants such as attention (Yang et al., 2016), encoder-decoder framework (Serb"
D19-5316,D14-1181,0,0.0141824,"aracteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources. Dataset and Baseline We use SLN: Satirical and Legitimate News Database (Rubin et al., 2016), RPN: Random Political News Dataset (Horne and Adali, 2017) and LUN: Labeled Unreliable News Dataset Rashkin et al. (2017) for our experiments. Table 1 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we implement the following neural baselines, • CNN: In this model, we apply a 1-d CNN (Convolutional Neural Network) layer (Kim, 2014) with filter size 3 over the word embeddings of the sentences within a document. This is followed by a max-pooling layer to get a single document vector which is passed to a fully connected projection layer to get the logits over output classes. Rashkin et al. (2017) extends Rubin et al. (2016)’s work by offering a quantitative study of linguistic differences found in articles of different types of fake news such as hoax, propaganda and satire. They also proposed predictive models for graded deception across multiple domains. Rashkin et al. (2017) found that neural methods didn’t perform well"
D19-5316,N19-1069,0,0.0188526,"sification using a graph neural network. 3 Satire, according to Simpson (2003), is complicated because it occupies more than one place in the framework for humor, proposed by Ziv (1988): it clearly has an aggressive and social function, and often expresses an intellectual aspect as well. Rubin et al. (2016) defines news satire as a genre of satire that mimics the format and style of journalistic reporting. Datasets created for the task of identifying satirical news articles from the trusted ones are often constructed by collecting documents from different online sources (Rubin et al., 2016). McHardy et al. (2019) hypothesized that this encourages the models to learn characteristics for different publication sources rather than characteristics of satire. In this work, we show that our proposed model generalizes to articles from unseen publication sources. Dataset and Baseline We use SLN: Satirical and Legitimate News Database (Rubin et al., 2016), RPN: Random Political News Dataset (Horne and Adali, 2017) and LUN: Labeled Unreliable News Dataset Rashkin et al. (2017) for our experiments. Table 1 shows the statistics. Since all of the previous methods on the aforementioned datasets are non-neural, we im"
D19-5316,D17-1317,0,0.311671,"of news articles. Specifically, satirical articles had a more coherent story and thus all the sentences in the document seemed similar to each other. On the other hand, the trusted news articles were also coherent but the similarity between sentences from different parts of the document was not that strong, as depicted in Figure 1. We believe that the reason for such kind of behaviour is the presence of factual jumps across sections in a trusted document. Introduction In today’s day and age of social media, there are ample opportunities for fake news production, dissemination and consumption. Rashkin et al. (2017) break down fake news into three categories, hoax, propaganda and satire. A hoax article typically tries to convince the reader about a cookedup story while propaganda ones usually mislead the reader into believing a false political or social agenda. Burfoot and Baldwin (2009) defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule. Previous works (Rubin et al., 2016; Rashkin et al., 2017) rely on various linguistic and handcrafted semantic features for differentiating between news articles. However, none of them try In thi"
D19-5316,P09-2041,0,0.0388083,"of the document was not that strong, as depicted in Figure 1. We believe that the reason for such kind of behaviour is the presence of factual jumps across sections in a trusted document. Introduction In today’s day and age of social media, there are ample opportunities for fake news production, dissemination and consumption. Rashkin et al. (2017) break down fake news into three categories, hoax, propaganda and satire. A hoax article typically tries to convince the reader about a cookedup story while propaganda ones usually mislead the reader into believing a false political or social agenda. Burfoot and Baldwin (2009) defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule. Previous works (Rubin et al., 2016; Rashkin et al., 2017) rely on various linguistic and handcrafted semantic features for differentiating between news articles. However, none of them try In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document. We present a series of experiments on News Corpus with Varying Reliability dataset (Rashkin et al., 2017) and Satirical Legitimate"
D19-5316,W16-0802,0,0.710561,"rusted document. Introduction In today’s day and age of social media, there are ample opportunities for fake news production, dissemination and consumption. Rashkin et al. (2017) break down fake news into three categories, hoax, propaganda and satire. A hoax article typically tries to convince the reader about a cookedup story while propaganda ones usually mislead the reader into believing a false political or social agenda. Burfoot and Baldwin (2009) defines a satirical article as the one which deliberately exposes real-world individuals, organisations and events to ridicule. Previous works (Rubin et al., 2016; Rashkin et al., 2017) rely on various linguistic and handcrafted semantic features for differentiating between news articles. However, none of them try In this work, we propose a graph neural network-based model to classify news articles while capturing the interaction of sentences across the document. We present a series of experiments on News Corpus with Varying Reliability dataset (Rashkin et al., 2017) and Satirical Legitimate News dataset (Rubin et al., 2016). Our results demonstrate that the proposed model achieves state-of-the-art performance on these datasets and provides interesting"
D19-5316,C18-1285,0,0.0369536,"Missing"
D19-5316,D17-1211,0,0.0353853,"Missing"
gerber-hovy-1998-improving,P95-1034,0,\N,Missing
gerber-hovy-1998-improving,W97-0704,1,\N,Missing
gerber-hovy-1998-improving,X98-1026,1,\N,Missing
H01-1069,A00-2004,0,0.0116859,"estion find desired semantic type • Engines: IdentiFinder (BBN) CONTEX (Hermjakob) Segment Parsing • Steps: parse segment sentences • Engines: CONTEX (Hermjakob) Match segments against question Rank and prepare answers QA typology • Categorize QA types in taxonomy (Gerber) Output answers Constraint patterns • Identify likely answers in relation to other parts of the sentence (Gerber) Figure 1. Webclopedia architecture. S e g m e n t a t i o n : To decrease the amount of text to be processed, the documents are broken into semantically coherent segments. Two text segmenter—TexTiling [5] and C99 [2]—were tried; the first is used; see [9]. Ranking s e g m e n t s : For each segment, each sentence i s scored using a formula that rewards word and phrase overlap with the question and its expanded query words. Segments are ranked. See [9] Parsing s e g m e n t s : CONTEX parses each sentence of the top-ranked 100 segments (Section 4). Pinpointing: For each sentence, three steps of matching are performed (Section 5); two compare the analyses of the question and the sentence; the third uses the window method t o compute a goodness score. Ranking of answers: The candidate answers’ scores are com"
H01-1069,A00-2016,1,0.869289,"Missing"
H01-1069,W01-1203,1,0.739956,"de (1) significantly more training data; (2) a few additional features, some more treebank cleaning, a bit more background knowledge etc.; and (3) the 251 test questions on Oct. 16 were probably a little bit harder on average, because a few of the TREC-9 questions initially treebanked (and included in the October figures) were selected for early treebanking because they represented particular challenges, hurting subsequent Qtarget processing. 4.2 Parsing Potential Answers The semantic type ontology in CONTEX was extended t o include 115 Qtarget types, plus some combined types; more details in [8]. Beside the Qtargets that refer to concepts i n CONTEX’s concept ontology (see first example below), Qtargets can also refer to part of speech labels (first example), to constituent roles or slots of parse trees (second and third examples), and to more abstract nodes in the QA Typology (later examples). For questions with the Qtargets Q-WHYFAMOUS, Q-WHY-FAMOUS-PERSON, Q-SYNONYM, and others, the parser also provides Qargs—information helpful for matching (final examples). Semantic ontology types (I-EN-CITY) and part of speech labels (S-PROPER-NAME): What is the capital of Uganda? QTARGET: (((I"
H01-1069,P00-1071,0,0.0567015,"For each sentence, three steps of matching are performed (Section 5); two compare the analyses of the question and the sentence; the third uses the window method t o compute a goodness score. Ranking of answers: The candidate answers’ scores are compared and the winner(s) are output. 3. THE QA TYPOLOGY In order to perform pinpointing deeper than the word level, the system has to produce a representation of what the user i s asking. Some previous work in automated question answering has categorized questions by question word or by a mixture of question word and the semantic class of the answer [11, 10]. To ensure full coverage of all forms of simple question and answer, and to be able to factor in deviations and special requirements, we are developing a QA Typology. We motivate the Typology (a taxonomy of QA types) as follows. There are many ways to ask the same thing: What is the age o f the Queen of Holland? How old is the Netherlands’ queen? How long has the ruler of Holland been alive? Likewise, there are many ways of delivering the same answer: about 60; 63 years old; since January 1938. Such variations form a sort of semantic equivalence class of both questions and answers. Since the"
H01-1069,A00-1023,0,0.0224804,"For each sentence, three steps of matching are performed (Section 5); two compare the analyses of the question and the sentence; the third uses the window method t o compute a goodness score. Ranking of answers: The candidate answers’ scores are compared and the winner(s) are output. 3. THE QA TYPOLOGY In order to perform pinpointing deeper than the word level, the system has to produce a representation of what the user i s asking. Some previous work in automated question answering has categorized questions by question word or by a mixture of question word and the semantic class of the answer [11, 10]. To ensure full coverage of all forms of simple question and answer, and to be able to factor in deviations and special requirements, we are developing a QA Typology. We motivate the Typology (a taxonomy of QA types) as follows. There are many ways to ask the same thing: What is the age o f the Queen of Holland? How old is the Netherlands’ queen? How long has the ruler of Holland been alive? Likewise, there are many ways of delivering the same answer: about 60; 63 years old; since January 1938. Such variations form a sort of semantic equivalence class of both questions and answers. Since the"
H01-1069,P94-1002,0,\N,Missing
H01-1069,P97-1062,1,\N,Missing
H05-1075,P05-1060,0,0.0137126,"d by Harabagiu (2001) to process questions in dialogue and implicatures. The agents are created based on pragmatic knowledge. Traditional answer extraction and answer fusion approaches assume the question is always correct and explicit answers do exist in the corpus. Reported work attempts to rank the candidate answer list to boost the correct one into top position. This is not enough when there may not be an answer for the question posed. For biographical fact extraction and generation, Zhou et al. (2004) and Schiffman et al. (2001) use summarization techniques to generate human biographies. Mann and Yarowsky (2005) propose fusing the extracted information across documents to return a consensus answer. In their approach, they did not consider multiple values or no values for biography facts, although multiple facts are common for some biography attributes, such as multiple occupations, children, books, places of residence, etc. In these cases a consensus answer is not adequate. Our work differs from theirs because we are not only working on information/answer extraction; the focus in this paper is the guidance for answer extraction of questions (or IE task for values) with implicatures. This work can be"
H05-1075,W04-3215,0,0.144478,"investigate questions with implicatures related to biography facts in a web-based QA system, PowerBio. We compare the performances of Decision Tree, Naïve Bayes, SVM (Support Vector Machine), and ME (Maximum Entropy) classification methods. The integration of the cascading guidance strategy can help extract answers for questions with implicatures and produce satisfactory results in our experiments. 1 Motivation Question Answering has emerged as a key area in natural language processing (NLP) to apply question parsing, information extraction, summarization, and language generation techniques (Clark et al., 2004; Fleischman et al., 2003; Echihabi et al., 2003; Yang et al., 2003; Hermjakob et al., 2002; Dumais et al., 2002). Traditional question answering systems adopt the framework of parsing questions, searching for relevant documents, and then pinpointing and generating answers. However, this framework includes potential dangers. For example, to answer the question “when did Beethoven get married?”, a typical QA system would identify the question target to be a “Date” and would apply techniques to identify the date Beethoven got married. Since Beethoven never married, this direct approach is likely"
H05-1075,N03-1022,0,0.0822041,"Missing"
H05-1075,P00-1071,0,0.0245308,"ntion from the areas of Natural Language Processing, Information Retrieval and Data Mining (Fleischman et al., 2003; Echihabi et al., 2003; Yang et al., 2003; Hermjakob et al., 2002; Dumais et al., 2002; Hermjakob et al., 2000). It is tested in several venues, including the TREC and CLEF Question Answering tracks (Voorhees, 2003; Magnini et al., 2003). Most research efforts in the Question Answering community have focused on factoid questions and successful Question Answering systems tend to have similar underlying pipelines structures (Prager et al., 2004; Xu et al., 2003; Hovy et al., 2000; Moldovan et al., 2000). Recently more techniques for answer extraction, answer selection, and answer validation have been proposed (Lita et al., 2004; Soricut and Brill, 2004; Clark et al., 2004). Prager et al. (2004) proposed applying constraint satisfaction obtained by asking auxiliary questions to improve system performance. This approach requires the creation of auxiliary questions, which may be complex to automate. Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. However, this approach will not work if no explicit answers exist in the source. The first r"
H05-1075,P03-1003,0,0.0805316,"Missing"
H05-1075,P03-1001,1,0.870112,"Missing"
H05-1075,C02-1042,1,0.908522,"Missing"
H05-1075,W04-3251,0,0.0290064,"Missing"
H05-1075,P02-1038,0,0.0219457,"Missing"
H05-1075,P04-1073,0,0.0240691,"2 Related Work Question Answering has attracted much attention from the areas of Natural Language Processing, Information Retrieval and Data Mining (Fleischman et al., 2003; Echihabi et al., 2003; Yang et al., 2003; Hermjakob et al., 2002; Dumais et al., 2002; Hermjakob et al., 2000). It is tested in several venues, including the TREC and CLEF Question Answering tracks (Voorhees, 2003; Magnini et al., 2003). Most research efforts in the Question Answering community have focused on factoid questions and successful Question Answering systems tend to have similar underlying pipelines structures (Prager et al., 2004; Xu et al., 2003; Hovy et al., 2000; Moldovan et al., 2000). Recently more techniques for answer extraction, answer selection, and answer validation have been proposed (Lita et al., 2004; Soricut and Brill, 2004; Clark et al., 2004). Prager et al. (2004) proposed applying constraint satisfaction obtained by asking auxiliary questions to improve system performance. This approach requires the creation of auxiliary questions, which may be complex to automate. Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. However, this approach will not"
H05-1075,P02-1006,1,0.727083,"d on factoid questions and successful Question Answering systems tend to have similar underlying pipelines structures (Prager et al., 2004; Xu et al., 2003; Hovy et al., 2000; Moldovan et al., 2000). Recently more techniques for answer extraction, answer selection, and answer validation have been proposed (Lita et al., 2004; Soricut and Brill, 2004; Clark et al., 2004). Prager et al. (2004) proposed applying constraint satisfaction obtained by asking auxiliary questions to improve system performance. This approach requires the creation of auxiliary questions, which may be complex to automate. Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. However, this approach will not work if no explicit answers exist in the source. The first reason is that in that situation the anchors to learn the patterns cannot be determined. Secondly, most of the facts without explicit values are not expressed with long patterns including anchors. For example, the phrase “the childless marriage” gives enough information that a person has no child. But it is almost impossible to learn such surface text patterns following (Ravichandran and Hovy, 2002). Reported work on question p"
H05-1075,P04-1072,0,0.0331439,"reason is that in that situation the anchors to learn the patterns cannot be determined. Secondly, most of the facts without explicit values are not expressed with long patterns including anchors. For example, the phrase “the childless marriage” gives enough information that a person has no child. But it is almost impossible to learn such surface text patterns following (Ravichandran and Hovy, 2002). Reported work on question processing focuses mainly on the problems of parsing questions, determining the question target for search subsystem (Pasca and Harabagiu, 2001; Hermjakob et al., 2000). Saquete et al. (2004) decompose complex temporal questions into simpler ones based on the temporal relationships in the question. To date, there has been little published work on handling implicatures in questions. Just-In-Time Information Seeking Agents (JITISA) was proposed by Harabagiu (2001) to process questions in dialogue and implicatures. The agents are created based on pragmatic knowledge. Traditional answer extraction and answer fusion approaches assume the question is always correct and explicit answers do exist in the corpus. Reported work attempts to rank the candidate answer list to boost the correct"
H05-1075,P01-1059,0,0.0525197,"Missing"
H05-1075,N04-1008,0,0.0127935,", 2003; Hermjakob et al., 2002; Dumais et al., 2002; Hermjakob et al., 2000). It is tested in several venues, including the TREC and CLEF Question Answering tracks (Voorhees, 2003; Magnini et al., 2003). Most research efforts in the Question Answering community have focused on factoid questions and successful Question Answering systems tend to have similar underlying pipelines structures (Prager et al., 2004; Xu et al., 2003; Hovy et al., 2000; Moldovan et al., 2000). Recently more techniques for answer extraction, answer selection, and answer validation have been proposed (Lita et al., 2004; Soricut and Brill, 2004; Clark et al., 2004). Prager et al. (2004) proposed applying constraint satisfaction obtained by asking auxiliary questions to improve system performance. This approach requires the creation of auxiliary questions, which may be complex to automate. Ravichandran and Hovy (2002) proposed automatically learning surface text patterns for answer extraction. However, this approach will not work if no explicit answers exist in the source. The first reason is that in that situation the anchors to learn the patterns cannot be determined. Secondly, most of the facts without explicit values are not expr"
H05-1075,W04-3256,1,0.768386,"ed work on handling implicatures in questions. Just-In-Time Information Seeking Agents (JITISA) was proposed by Harabagiu (2001) to process questions in dialogue and implicatures. The agents are created based on pragmatic knowledge. Traditional answer extraction and answer fusion approaches assume the question is always correct and explicit answers do exist in the corpus. Reported work attempts to rank the candidate answer list to boost the correct one into top position. This is not enough when there may not be an answer for the question posed. For biographical fact extraction and generation, Zhou et al. (2004) and Schiffman et al. (2001) use summarization techniques to generate human biographies. Mann and Yarowsky (2005) propose fusing the extracted information across documents to return a consensus answer. In their approach, they did not consider multiple values or no values for biography facts, although multiple facts are common for some biography attributes, such as multiple occupations, children, books, places of residence, etc. In these cases a consensus answer is not adequate. Our work differs from theirs because we are not only working on information/answer extraction; the focus in this pape"
H05-1075,N03-1004,0,\N,Missing
H05-2003,P05-1037,1,0.8298,"discussion board with an open source board that is currently used by selected classes. The board provides a platform for evaluating new teaching and learning technologies. Within the discussion board teachers and students post messages about course-related topics. The discussions are organized chronologically within topics and higher-level forums. These ‘live’ discussions are now enabling a new opportunity, the opportunity to apply and evaluate advanced natural language processing (NLP) technology. Recently we designed a summarization system for technical chats and emails on the Linux kernel (Zhou and Hovy, 2005). It clusters discussions according to subtopic structures on the sub-message level, identifies immediate responding pairs using machine-learning methods, and generates subtopicbased mini-summaries for each chat log. Incorporation of this system into the ISI Discussion Board framework, called Classummary, benefits both distance learning and NLP communities. Summaries are created periodically and sent to students and teachers via their preferred medium (emails, text messages on mobiles, web, etc). This relieves users of the burden of reading through a large volume of messages before participati"
H89-2015,E89-1038,0,0.0540475,"Missing"
H89-2015,P85-1008,0,0.0840782,"Missing"
H89-2015,P86-1035,0,0.0263733,"Missing"
H89-2015,1986.tc-1.15,0,0.116584,"Missing"
H89-2015,P87-1033,0,0.0374837,"Missing"
H89-2015,P88-1029,0,0.0246332,"Missing"
H89-2015,P84-1105,0,0.0610545,"Missing"
H89-2015,H86-1013,0,\N,Missing
H90-1011,P88-1035,0,0.0124436,"exponential time in the worst case [15]. The most common and obvious way to deal with disjunctive constraints is to expand the grammatical description to disjunctive normal form (DNF) during a pre-processing step, thereby eliminating disjunction from the rules that are actually used by the parser. Though this method works reasonably well for small grammars, it turns out to be unsatisfactory for larger grammars. It is possible to avoid exponential expansion for most practical grammars, and several unification algorithms for disjunctive feature descriptions have been developed in recent years: [6, 10, 4]. The latter two algorithms allow general disjunctive descriptions, and avoid expansion to DNF by exploiting logical equivalences between descriptions to produce normal forms that allow a more compact representation. Kasper&apos;s algorithm is based on a normal form that divides each description into definite and indefinite components. The definite component contains no disjunction, and the indefinite component contains a list of disjunctions that must be satisfied. The algorithm of Eisele and Doerre uses a different normal form that guarantees the detection of any inconsistencies during the normal"
H90-1011,H89-1022,1,0.707994,"d in the Penman system. In order to achieve greater portability, Penman contains a general taxonomic ontology of concepts called the Upper Model [ 1], under which the concepts from various application domains are subordinated. By inheriting information from the Upper Model, domain concepts can be handled appropriately by the Penman language generator without the generator ever having to be explicitly informed of their individual nature. Similarly, the parser can exploit inherited Upper Model information when trying to place words appropriately into structures. More information can be found in [7]. - - 1. a more specific type: that c is an active clause; to find all possible constituents, unless sufficientconstraints on constituent ordering can be applied early enoughin the parsingprocess. By performing a sha/lowstructuralparse before starting the deep classification-basedparse, one gains a largeimprovementin efficiency,becauseeven a skeletalcontextfree grammarcan providethe basic segmentationof the input sentenceinto its major constituents. Thus, a simple context-freeparsing componentwas used for this purposewith success in the prototypesystem. 57 Efficiency Considerations The classif"
H90-1011,P88-1029,1,0.811536,"definite component contains no disjunction, and the indefinite component contains a list of disjunctions that must be satisfied. The algorithm of Eisele and Doerre uses a different normal form that guarantees the detection of any inconsistencies during the normalization process by selectively expanding disjunctions that might possibly interact with other information in the description. The Kasper algorithm was first implemented as an extension to the unification algorithm of the PATR-II parser, and it has been further developed to handle conditional descriptions and a limited type of negation [8]. These extensions to PATR-II have been used to construct an experimental parser for systemic grammars [9], which has been tested with a large grammar of English called Nigel, which is part of the Penman language generation system [13]. Although these methods for processing complex feature constraints are generally much more efficient than expansion to DNF, they still have several significant sources of inefficiency: 1. a large amount of structure must be copied in order to guarantee correct unification; 2. consistency checks are required between components of a description that do not share a"
H90-1011,C88-1063,1,0.870345,"hat must be satisfied. The algorithm of Eisele and Doerre uses a different normal form that guarantees the detection of any inconsistencies during the normalization process by selectively expanding disjunctions that might possibly interact with other information in the description. The Kasper algorithm was first implemented as an extension to the unification algorithm of the PATR-II parser, and it has been further developed to handle conditional descriptions and a limited type of negation [8]. These extensions to PATR-II have been used to construct an experimental parser for systemic grammars [9], which has been tested with a large grammar of English called Nigel, which is part of the Penman language generation system [13]. Although these methods for processing complex feature constraints are generally much more efficient than expansion to DNF, they still have several significant sources of inefficiency: 1. a large amount of structure must be copied in order to guarantee correct unification; 2. consistency checks are required between components of a description that do not share any features in common, because unification cannot determine whether any dependencies exist between two str"
H90-1011,P87-1033,1,0.894725,"exponential time in the worst case [15]. The most common and obvious way to deal with disjunctive constraints is to expand the grammatical description to disjunctive normal form (DNF) during a pre-processing step, thereby eliminating disjunction from the rules that are actually used by the parser. Though this method works reasonably well for small grammars, it turns out to be unsatisfactory for larger grammars. It is possible to avoid exponential expansion for most practical grammars, and several unification algorithms for disjunctive feature descriptions have been developed in recent years: [6, 10, 4]. The latter two algorithms allow general disjunctive descriptions, and avoid expansion to DNF by exploiting logical equivalences between descriptions to produce normal forms that allow a more compact representation. Kasper&apos;s algorithm is based on a normal form that divides each description into definite and indefinite components. The definite component contains no disjunction, and the indefinite component contains a list of disjunctions that must be satisfied. The algorithm of Eisele and Doerre uses a different normal form that guarantees the detection of any inconsistencies during the normal"
H90-1011,P84-1075,0,0.022214,"discussed. Finally, the efficiency benefits of the new method are mentioned. Constraints in Unification-based Grammars A variety of current approaches to parsing in computational linguistics emphasize declarative representations of grammar with logical constraints stated in terms of feature and category structures. These approaches have collectively become known as the unification-based grammars, because unification is commonly used as the primary operation for building and combining feature structures. Some of the simplest of these grammatical frameworks, as exemplified by the PATR-II system [16], state constraints on features entirely in terms of sets of unifications that must be simultaneously satisfied whenever a grammatical rule is used. In such systems all constraints on a rule or lexical item are interpreted conjunctively. Many of the more recent frameworks also use other general logical connectives, such as disjunction, negation and implication, in their representation of constraints. The utility of such logical constraints is abundantly illustrated by linguistic models, including Systemic Grammar (SG) [5] and Head Driven Phrase Structure Grammar (HPSG) [14], and by computation"
H90-1011,P84-1024,0,0.0322021,"based upon logical properties that can be deduced from the definitions of objects and the facts known about them. In these systems, classification is the operation that places a new class or object into the lattice according to the subsumption order. A primary benefit of classification is that it organizes large collections of knowledge in such a way that properties shared by many objects need only be represented once, yet they can still be efficiently accessed by inheritance. KL-ONE and similar frameworks have been used for semantic interpretation in some natural language processing systems [18], but usually in a way that is quite separate from the grammatical parsing process (an exception is the aforementioned PSI-KLONE system). Recent research indicates that it may be advantageous to make use of a classification-based framework for processing grammatical knowledge as well. Many formal properties are shared by the feature descriptions used in unification-based grammars and the terminological definitions used in KL-ONE. Generally speaking, linguistic categories correspond to concepts, and their features (or attributes) correspond to binary relations in the knowledge representation sy"
H90-1011,P84-1008,0,\N,Missing
H92-1052,P88-1018,0,0.0503226,"Missing"
H92-1052,J89-3002,0,\N,Missing
H94-1025,H93-1036,0,0.214367,"hms during the process of development to suppress dispersions and inconsistencies, even if the final check should be entrusted to the human workers. Another motivation for the development of automated dictionary/ontology alignment algorithms is the increased availability of online lexical and semantic resources, such as lexicons, taxonomies, dictionaries and thesaiuri[Matsumoto et al., 1993b; Miller, 1990; Lenat and Guha, 1990; Carlson and Nirenburg, 1990; Collins, 1971; IPAL, 1987]. Making the best use of such resources leads to higher quality translation with lower development cost[Hovy and Knight, 1993; Knight, 1994; Hovy and Nirenburg, 1992]. For example, the JUMAN system provides a Japanese unilingual lexicon for analyzing Japanese texts[Matsumoto et al., 1993b]. The linkage of the unilingual lexicon to the ontology directly enables Japanese-English translation with lower development cost. From this viewpoint, automatic alignment algorithms represent a new paradigm for MT system building. The problem we focus on here is how to associate concepts in the ontology with Japanese lexicM entities by automatic methods, since it is too difficult to define adequately many concepts manually. We hav"
H94-1025,W90-0108,0,0.0245864,"Missing"
H94-1025,H92-1052,1,0.885976,"Missing"
H94-1025,H93-1054,0,0.0904573,"Missing"
hartholt-etal-2008-common,W05-1520,1,\N,Missing
hartholt-etal-2008-common,W08-0130,1,\N,Missing
hartholt-etal-2008-common,I05-7009,1,\N,Missing
hovy-etal-2006-automated,W03-0508,0,\N,Missing
hovy-etal-2006-automated,P02-1040,0,\N,Missing
hovy-etal-2006-automated,N04-1019,0,\N,Missing
hovy-etal-2006-automated,N03-1020,1,\N,Missing
hovy-etal-2006-automated,W02-0406,1,\N,Missing
I05-2011,W02-1011,0,0.0551679,"Missing"
I05-2011,W03-1014,0,0.477548,"Missing"
I05-2011,W03-0404,0,0.134144,"REC-2003 competition included a task of recognizing opinion-bearing sentences (see Section 5.2). Wilson and Wiebe (2003) developed an annotation scheme for so-called subjective sentences (opinions and other private states) as part of a U.S. government-sponsored project (ARDA AQUAINT NRRC) in 2002. They created a corpus, MPQA, containing news articles manually annotated. Several other approaches have been applied for learning words and phrases that signal subjectivity. Turney (2002) and Wiebe (2000) focused on learning adjectives and adjectival phrases and Wiebe et al. (2001) focused on nouns. Riloff et al. (2003) extracted nouns and Riloff and Wiebe (2003) extracted patterns for subjective expressions using a bootstrapping process. 3 S y n on y m set of O P OP N onO P w ord OP : O p in io n - b e a r in g w o r d s N o n O P : N o n - O p in io n - b e a r in g w ords S y n ony m set of a g iv e n w o r d Data Sources We developed several collections of opinionbearing and non-opinion-bearing words. One is accurate but small; another is large but relatively inaccurate. We combined them to obtain a more reliable list. We obtained an additional list from Columbia University. 3.1 S y n ony m set of N on O"
I05-2011,W03-2102,0,0.0240864,"n-opinion-bearing word list manually and produced related words for it using WordNet. To avoid collecting uncommon words, we started with a basic/common English word list compiled for foreign students preparing for the TOEFL test. From this we randomly selected 462 adjectives and 502 verbs for human annotation. Human1 and human2 annotated 462 adjectives and human3 and human2 annotated 502 verbs, labeling each word as either opinionbearing or non-opinion-bearing. (Soboroff and Harman, 2003) of the TREC-2003 competition included a task of recognizing opinion-bearing sentences (see Section 5.2). Wilson and Wiebe (2003) developed an annotation scheme for so-called subjective sentences (opinions and other private states) as part of a U.S. government-sponsored project (ARDA AQUAINT NRRC) in 2002. They created a corpus, MPQA, containing news articles manually annotated. Several other approaches have been applied for learning words and phrases that signal subjectivity. Turney (2002) and Wiebe (2000) focused on learning adjectives and adjectival phrases and Wiebe et al. (2001) focused on nouns. Riloff et al. (2003) extracted nouns and Riloff and Wiebe (2003) extracted patterns for subjective expressions using a b"
I05-2011,W03-1017,0,0.0689841,"as features we built a Naive bayesian classifier and we finally classified 32373 words. 3.2 Collection 2: WSJ Data Experiments with the above set did not provide very satisfactory results on arbitrary text. For one reason, WordNet’s synonym connections are simply not extensive enough. However, if we know the relative frequency of a word in opinion-bearing texts compared to non-opinionbearing text, we can use the statistical information instead of lexical information. For this, we collected a huge amount of data in order to make up for the limitations of collection 1. Following the insight of Yu and Hatzivassiloglou (2003), we made the basic and rough assumption that words that appear more often in newspaper editorials and letters to the editor than in non-editorial news articles could be potential opinion-bearing words (even though editorials contain sentences about factual events as well). We used the TREC collection to collect data, extracting and classifying all Wall Street Journal documents from it either as Editorial or nonEditorial based on the occurrence of the keywords “Letters to the Editor”, “Letter to the Editor” or “Editorial” present in its headline. This produced in total 7053 editorial documents"
I05-2011,P02-1053,0,\N,Missing
I05-7009,W04-2709,1,\N,Missing
I05-7009,reeder-etal-2004-interlingual,1,\N,Missing
I05-7009,agirre-de-lacalle-2004-publicly,0,\N,Missing
I05-7009,W03-1007,1,\N,Missing
I05-7009,J05-2004,0,\N,Missing
I05-7009,P05-1016,1,\N,Missing
I05-7009,P98-1013,0,\N,Missing
I05-7009,C98-1013,0,\N,Missing
I05-7009,magnini-cavaglia-2000-integrating,0,\N,Missing
I05-7009,W98-0713,0,\N,Missing
I05-7009,P03-1001,1,\N,Missing
I08-1048,J96-1002,0,0.162952,"Missing"
I08-1048,H92-1022,0,0.0353329,"Missing"
I08-1048,N06-1016,0,0.378872,"., 2007), part-of-speech tagging (Engelson and Dagan, 1999), information extraction (Thompson et al., 1999), statistical parsing (Steedman et al., 2003), and word sense disambiguation (Zhu and Hovy, 2007). Previous studies reported that active learning can help in reducing human labeling effort. With selective sampling techniques such as uncertainty sampling (Lewis and Gale, 1994) and committeebased sampling (McCallum and Nigam, 1998), the size of the training data can be significantly reduced for text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998), word sense disambiguation (Chen, et al. 2006; Zhu and Hovy, 2007), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007) tasks. Interestingly, deciding when to stop active learning is an issue seldom mentioned issue in these studies. However, it is an important practical topic, since it obviously makes no sense to continue the active learning procedure until the whole corpus has been labeled. How to define an adequate stopping criterion remains an unsolved problem in active learning. In principle, this is a problem of estimation of classifier effectiveness (Lewis and Gale, 1994). However, in real-world applications, it"
I08-1048,N06-2015,1,0.805293,"Missing"
I08-1048,W02-1006,0,0.194715,"Missing"
I08-1048,J93-2004,0,0.0496142,"Missing"
I08-1048,P04-1075,0,0.641095,"mount of human labeling effort by building an system that automatically selects the most informative unlabeled example for human annotation at each annotation cycle. In recent years active learning has attracted a lot of research interest, and has been studied in many natural language processing (NLP) tasks, such as text classification (TC) Eduard Hovy University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu (Lewis and Gale, 1994; McCallum and Nigam, 1998), chunking (Ngai and Yarowsky, 2000), named entity recognition (NER) (Shen et al., 2004; Tomanek et al., 2007), part-of-speech tagging (Engelson and Dagan, 1999), information extraction (Thompson et al., 1999), statistical parsing (Steedman et al., 2003), and word sense disambiguation (Zhu and Hovy, 2007). Previous studies reported that active learning can help in reducing human labeling effort. With selective sampling techniques such as uncertainty sampling (Lewis and Gale, 1994) and committeebased sampling (McCallum and Nigam, 1998), the size of the training data can be significantly reduced for text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998), word sense d"
I08-1048,N03-1031,0,0.00982727,"le. In recent years active learning has attracted a lot of research interest, and has been studied in many natural language processing (NLP) tasks, such as text classification (TC) Eduard Hovy University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu (Lewis and Gale, 1994; McCallum and Nigam, 1998), chunking (Ngai and Yarowsky, 2000), named entity recognition (NER) (Shen et al., 2004; Tomanek et al., 2007), part-of-speech tagging (Engelson and Dagan, 1999), information extraction (Thompson et al., 1999), statistical parsing (Steedman et al., 2003), and word sense disambiguation (Zhu and Hovy, 2007). Previous studies reported that active learning can help in reducing human labeling effort. With selective sampling techniques such as uncertainty sampling (Lewis and Gale, 1994) and committeebased sampling (McCallum and Nigam, 1998), the size of the training data can be significantly reduced for text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998), word sense disambiguation (Chen, et al. 2006; Zhu and Hovy, 2007), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007) tasks. Interestingly, deciding when to st"
I08-1048,D07-1051,0,0.575681,"ling effort by building an system that automatically selects the most informative unlabeled example for human annotation at each annotation cycle. In recent years active learning has attracted a lot of research interest, and has been studied in many natural language processing (NLP) tasks, such as text classification (TC) Eduard Hovy University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu (Lewis and Gale, 1994; McCallum and Nigam, 1998), chunking (Ngai and Yarowsky, 2000), named entity recognition (NER) (Shen et al., 2004; Tomanek et al., 2007), part-of-speech tagging (Engelson and Dagan, 1999), information extraction (Thompson et al., 1999), statistical parsing (Steedman et al., 2003), and word sense disambiguation (Zhu and Hovy, 2007). Previous studies reported that active learning can help in reducing human labeling effort. With selective sampling techniques such as uncertainty sampling (Lewis and Gale, 1994) and committeebased sampling (McCallum and Nigam, 1998), the size of the training data can be significantly reduced for text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998), word sense disambiguation (Chen, et"
I08-1048,D07-1082,1,0.941925,"of research interest, and has been studied in many natural language processing (NLP) tasks, such as text classification (TC) Eduard Hovy University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu (Lewis and Gale, 1994; McCallum and Nigam, 1998), chunking (Ngai and Yarowsky, 2000), named entity recognition (NER) (Shen et al., 2004; Tomanek et al., 2007), part-of-speech tagging (Engelson and Dagan, 1999), information extraction (Thompson et al., 1999), statistical parsing (Steedman et al., 2003), and word sense disambiguation (Zhu and Hovy, 2007). Previous studies reported that active learning can help in reducing human labeling effort. With selective sampling techniques such as uncertainty sampling (Lewis and Gale, 1994) and committeebased sampling (McCallum and Nigam, 1998), the size of the training data can be significantly reduced for text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998), word sense disambiguation (Chen, et al. 2006; Zhu and Hovy, 2007), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007) tasks. Interestingly, deciding when to stop active learning is an issue seldom mentioned issu"
I08-1048,P00-1016,0,\N,Missing
I08-1048,I05-7009,1,\N,Missing
I08-2124,P06-1009,0,0.0334763,"to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004); and Word Sense Disambiguation (Chen et al., 2006). In this paper, we use CRF models to perform a more complex task on the primary TTE experimental results and adapt"
I08-2124,N06-1016,0,0.0126479,"ta extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004); and Word Sense Disambiguation (Chen et al., 2006). In this paper, we use CRF models to perform a more complex task on the primary TTE experimental results and adapt it to process new biomedical data. 3 In order to construct the minimum information required to interpret a TTE, we consider a set of specific components as shown in Table 1. Figure 1 gives an example of description of a complete TTE in a single sentence. In the research articles, this information is usually spread over many such sentences. 3.2 CRF Labeling We use a plain text sentence for input and attempt to label each token with a field label. In addition to the four pre-define"
I08-2124,D07-1088,1,0.765065,"arily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004); and Word Sense Disambiguation (Chen et al., 2006). In this paper, we use CRF models to perform a more complex task on the primary TTE experimental results and adapt it to process new biomedical data. 3 In order to"
I08-2124,P06-1027,0,0.0294613,"equired fields values (e.g. Ravichandran and Hovy, 2002; Mann and Yarowsky, 2005; Feng et al., 2006). However, this only works if the data corpus is rich enough to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004);"
I08-2124,P05-1060,0,0.0141489,"on their own interaction with the research literature (Stephan et al., 2001; Burns and Cheng, 2006). But this process of data entry and curation is manual. Current approaches on biomedical text mining (e.g., Srinivas et al., 2005; OKanohara et al., 2006) tend to address the tasks of named entity recognition or relation extraction, and our goal is more complex: to extract computational representations of the minimum information in a given experiment type. Pattern-based IE approaches employ seed data to learn useful patterns to pinpoint required fields values (e.g. Ravichandran and Hovy, 2002; Mann and Yarowsky, 2005; Feng et al., 2006). However, this only works if the data corpus is rich enough to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2"
I08-2124,P06-1059,0,0.0503132,"Missing"
I08-2124,N04-1042,0,0.0259424,"ly works if the data corpus is rich enough to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004); and Word Sense Disambiguation (Chen et al., 2006). In this paper, we use CRF models to perform a more complex task on the p"
I08-2124,P02-1006,1,0.651966,"of knowledge statements based on their own interaction with the research literature (Stephan et al., 2001; Burns and Cheng, 2006). But this process of data entry and curation is manual. Current approaches on biomedical text mining (e.g., Srinivas et al., 2005; OKanohara et al., 2006) tend to address the tasks of named entity recognition or relation extraction, and our goal is more complex: to extract computational representations of the minimum information in a given experiment type. Pattern-based IE approaches employ seed data to learn useful patterns to pinpoint required fields values (e.g. Ravichandran and Hovy, 2002; Mann and Yarowsky, 2005; Feng et al., 2006). However, this only works if the data corpus is rich enough to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data ex"
I08-2124,P04-1075,0,0.0309927,"Missing"
I08-2124,P02-1016,0,0.0133651,"ved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004); and Word Sense Disambiguation (Chen et al., 2006). In this paper, we use CRF models to perform a more complex task on the primary TTE experimental results and adapt it to process new biomedical data. 3 In order to construct the minimum information required to interpret a TTE, we consider a set of specific components as shown in Table 1. Figure 1 gives an example of description of a complete TTE in a single sentence. In the research articles, this information is usually spread over many such sentences. 3.2 CRF Labeling We use a plain text sentence for input and attemp"
I08-2124,N04-4028,0,\N,Missing
I17-1007,W13-3520,0,0.059118,"Missing"
I17-1007,P16-1231,0,0.142845,"Missing"
I17-1007,de-marneffe-etal-2006-generating,0,0.063623,"Missing"
I17-1007,D15-1041,0,0.02274,"Missing"
I17-1007,P81-1022,0,0.787447,"Missing"
I17-1007,D13-1203,0,0.00849431,"can be computed efficiently, leading to a straightforward end-to-end model training procedure via back-propagation. We evaluate our model on 17 different datasets, across 14 different languages. Our parser achieves state-of-the-art parsing performance on nine datasets. 1 Introduction Dependency parsing is one of the first stages in deep language understanding and has gained interest in the natural language processing (NLP) community, due to its usefulness in a wide range of applications. Many NLP systems, such as machine translation (Xie et al., 2011), entity coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., 2016), low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014), and word sense disambiguation (Fauceglia et al., 2015), are becoming more sophisticated, in part because of utilizing syntactic knowledge such 59 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 59–69, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP abilistic parsing model to define a proper conditional distribution over non-projective trees for a given sentence. In this paper, we propose a probabilistic neural network-based model for non-p"
I17-1007,D16-1211,0,0.101247,"Missing"
I17-1007,P15-1033,0,0.0183364,"tation Figure 2: The convolution neural network for extracting character-level representations of words. Dashed arrows indicate a dropout layer applied before character embeddings are input to CNN. Maximum Spanning Tree Decoding. The decoding problem of this parsing model can be formulated as: BLSTM. Many linguistic structure prediction tasks can benefit from having access to both past (left) and future (right) contexts, while the LSTM’s hidden state ht takes information only from past, knowing nothing about the future. An elegant solution whose effectiveness has been proven by previous work (Dyer et al., 2015; Ma and Hovy, 2016) is bi-directional LSTM (BLSTM). The basic idea is to present each sequence forwards and backwards to two separate hidden states to capture past and future information, respectively. Then the two hidden states are concatenated to form the final output. As discussed in Dozat and Manning (2016), there are more than one advantages to apply a multilayer perceptron (MLP) to the output vectors of BLSTM before the score function, eg. reducing the dimensionality and overfitting of the model. We follow this work by using a one-layer perceptron with elu (Clevert et al., 2015) as acti"
I17-1007,D12-1133,0,0.0498876,"Missing"
I17-1007,W15-0802,1,0.840417,"nt datasets, across 14 different languages. Our parser achieves state-of-the-art parsing performance on nine datasets. 1 Introduction Dependency parsing is one of the first stages in deep language understanding and has gained interest in the natural language processing (NLP) community, due to its usefulness in a wide range of applications. Many NLP systems, such as machine translation (Xie et al., 2011), entity coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., 2016), low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014), and word sense disambiguation (Fauceglia et al., 2015), are becoming more sophisticated, in part because of utilizing syntactic knowledge such 59 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 59–69, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP abilistic parsing model to define a proper conditional distribution over non-projective trees for a given sentence. In this paper, we propose a probabilistic neural network-based model for non-projective dependency parsing. This parsing model uses bi-directional LSTM-CNNs (BLSTM-CNNs) as backbone to learn neural information representations, on"
I17-1007,W06-2920,0,0.0227648,"Missing"
I17-1007,D14-1082,0,0.398349,"Missing"
I17-1007,D16-1238,0,0.781113,"he hyper-parameters on the development sets by random search. We use the same hyper-parameters across the models on different treebanks and languages, due to time constrains. Note that we use 2-layer BLSTM followed with 1-layer MLP. We set the state size of LSTM to 256 and the dimension of MLP to 100. Tuning these two parameters did not significantly impact the performance of our model. 64 System Bohnet and Nivre (2012) Chen and Manning (2014) Ballesteros et al. (2015) Dyer et al. (2015) Kiperwasser and Goldberg (2016): graph Ballesteros et al. (2016) Wang and Chang (2016) Zhang et al. (2016) Cheng et al. (2016) Andor et al. (2016) Kuncoro et al. (2016) Dozat and Manning (2016) This work: Basic This work: +Char This work: +POS This work: Full English UAS LAS – – 91.8 89.6 91.6 89.4 93.1 90.9 93.1 91.0 93.6 91.4 94.1 91.8 94.1 91.9 94.1 91.5 94.6 92.8 94.3 92.1 95.7 94.1 94.6 92.5 94.7 92.8 94.8 93.0 94.9 93.0 Chinese UAS LAS 87.3 85.9 83.9 82.4 85.3 83.7 87.2 85.7 86.6 85.1 87.7 86.2 87.6 86.2 87.8 86.2 88.1 85.7 – – 88.9 87.3 89.3 88.2 84.4 81.6 85.2 82.5 89.1 87.7 88.8 87.5 German UAS LAS 91.4 89.4 – – 88.8 86.1 – – – – – – – – – – – – 90.9 89.2 91.6 89.2 93.5 91.4 90.7 88.4 92.2 90.2 92.2 90.1 92."
I17-1007,Q16-1026,0,0.017156,"posed, whose high performance heavily rely on hand-crafted features and task-specific resources that are costly to develop, making dependency parsing models difficult to adapt to new languages or new domains. Recently, non-linear neural networks, such as recurrent neural networks (RNNs) with long-short term memory (LSTM) and convolution neural networks (CNNs), with as input distributed word representations, also known as word embeddings, have been broadly applied, with great success, to NLP problems like part-of-speech (POS) tagging (Collobert et al., 2011) and named entity recognition (NER) (Chiu and Nichols, 2016). By utilizing distributed representations as inputs, these systems are capable of learning hidden information representations directly from data instead of manually designing hand-crafted features, yielding end-to-end models (Ma and Hovy, 2016). Previous studies explored the applicability of neural representations to traditional graph-based parsing models. Some work (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016) replaced the linear scoring function of each arc in traditional models with neural networks and used a margin-based objective (McDonald et al., 2005a) for model training. Othe"
I17-1007,D15-1154,1,0.866639,"2 [92.60] 92.71 [88.92] 86.73 [77.56] 89.20 [85.77] 91.22 [86.92] 77.71 [65.81] 89.95 [84.99] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 89.83 – Table 5: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), Ma and Hovy (2015), and Cheng et al. (2016). to learn sentence segment embedding based on an extra forward LSTM network. Both of these two parsers trained the parsing models by optimizing margin-based objectives. There are three main differences between their models and ours. First, they only used linear form score function, instead of using the bi-linear term between the vectors of heads and modifiers. Second, They did not employ CNNs to model character-level information. Third, we proposed a probabilistic model over non-projective trees on the top of neural representations, while they trained their models wit"
I17-1007,P16-1101,1,0.663911,"h as recurrent neural networks (RNNs) with long-short term memory (LSTM) and convolution neural networks (CNNs), with as input distributed word representations, also known as word embeddings, have been broadly applied, with great success, to NLP problems like part-of-speech (POS) tagging (Collobert et al., 2011) and named entity recognition (NER) (Chiu and Nichols, 2016). By utilizing distributed representations as inputs, these systems are capable of learning hidden information representations directly from data instead of manually designing hand-crafted features, yielding end-to-end models (Ma and Hovy, 2016). Previous studies explored the applicability of neural representations to traditional graph-based parsing models. Some work (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016) replaced the linear scoring function of each arc in traditional models with neural networks and used a margin-based objective (McDonald et al., 2005a) for model training. Other work (Zhang et al., 2016; Dozat and Manning, 2016) formalized dependency parsing as independently selecting the head of each word with cross-entropy objective, without the guarantee of a general non-projective tree structure output. Moreover,"
I17-1007,N16-1116,1,0.883631,"Missing"
I17-1007,Q16-1023,0,0.174032,"representations, also known as word embeddings, have been broadly applied, with great success, to NLP problems like part-of-speech (POS) tagging (Collobert et al., 2011) and named entity recognition (NER) (Chiu and Nichols, 2016). By utilizing distributed representations as inputs, these systems are capable of learning hidden information representations directly from data instead of manually designing hand-crafted features, yielding end-to-end models (Ma and Hovy, 2016). Previous studies explored the applicability of neural representations to traditional graph-based parsing models. Some work (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016) replaced the linear scoring function of each arc in traditional models with neural networks and used a margin-based objective (McDonald et al., 2005a) for model training. Other work (Zhang et al., 2016; Dozat and Manning, 2016) formalized dependency parsing as independently selecting the head of each word with cross-entropy objective, without the guarantee of a general non-projective tree structure output. Moreover, there have yet been no previous work on deriving a neural probIn this paper, we propose a probabilistic parsing model that defines a proper conditional prob"
I17-1007,P14-1126,1,0.621541,"k-propagation. We evaluate our model on 17 different datasets, across 14 different languages. Our parser achieves state-of-the-art parsing performance on nine datasets. 1 Introduction Dependency parsing is one of the first stages in deep language understanding and has gained interest in the natural language processing (NLP) community, due to its usefulness in a wide range of applications. Many NLP systems, such as machine translation (Xie et al., 2011), entity coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., 2016), low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014), and word sense disambiguation (Fauceglia et al., 2015), are becoming more sophisticated, in part because of utilizing syntactic knowledge such 59 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 59–69, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP abilistic parsing model to define a proper conditional distribution over non-projective trees for a given sentence. In this paper, we propose a probabilistic neural network-based model for non-projective dependency parsing. This parsing model uses bi-directional LSTM-CNNs (BLSTM-CNNs) as"
I17-1007,P10-1001,0,0.0416614,"Missing"
I17-1007,C12-2077,1,0.493961,"Missing"
I17-1007,D07-1015,0,0.793194,"represents a generic (possibly non-projective) dependency tree, which represents syntactic relationships through labeled directed edges between heads and their dependents. For example, Figure 1 shows a dependency tree for the sentence, “Economic news had 60 of the submatrix formed by deleting the hth row and mth column. The marginals can be computed by calculating the matrix inversion of the matrix corresponding to L(0,0) (Θ). The time complexity of computing the partition function and marginals is O(n3 ). Matrix-Tree Theorem. In order to train the probabilistic parsing model, as discussed in Koo et al. (2007), we have to compute the partition function and the marginals, requiring summation over the set T (x): P Q Z(x; Θ) = ψ(xh , xm ; Θ) y∈T (x) (xh ,xm )∈y P µh,m (x; Θ) = P (y|x; Θ) Labeled Parsing Model. Though it is originally designed for unlabeled parsing, our probabilistic parsing model is easily extended to include dependency labels. In labeled dependency trees, each edge is represented by a tuple (xh , xm , l), where xh and xm are the head word and modifier, respectively, and l is the label of dependency type of this edge. Then we can extend the original model for labeled dependency parsin"
I17-1007,J93-2004,0,0.0627508,"matrix by assigning the weight of a edge with the sum of weights over different dependency labels: A0 h,m (Θ) = L X ψ(xh , xm , l; Θ) l=1 The partition function and marginals over labeled dependency trees are obtained by operating on the new adjacency matrix A0 (Θ). The time complexity becomes O(n3 + Ln2 ). In practice, L is probably large. For English, the number of edge labels in Stanford Basic Dependencies (De Marneffe et al., 2006) is 45, and the number in the treebank of CoNLL-2008 shared task (Surdeanu et al., 2008) is 70. While, the average length of sentences in English Penn Treebank (Marcus et al., 1993) is around 23. Thus, L is not negligible comparing to n. It should be noticed that in our labeled model, for different dependency label l we use the same vector representation ϕ(xi ) for each word xi . The dependency labels are distinguished (only) by the parameters (weights and bias) corresponding to each of them. One advantage of this is that it significantly reduces the memory requirement comparing to the model in Dozat and Manning (2016) which distinguishes ϕl (xi ) for different label l. Ah,m (Θ) = ψ(xh , xm ; Θ) Based on the adjacency matrix A(Θ), we have the Laplacian matrix: L(Θ) = D(Θ"
I17-1007,D10-1125,0,0.504665,"rd, making their model require much more memory than ours. Baselines. We compare our model with the third-order Turbo parser (Martins et al., 2013), the low-rank tensor based model (Tensor) (Lei et al., 2014), the randomized greedy inference based (RGB) model (Zhang et al., 2014), the labeled dependency parser with inner-to-outer greedy decoding algorithm (In-Out) (Ma and Hovy, 2015), and the bi-direction attention based parser (BiAtt) (Cheng et al., 2016). We also compare our parser against the best published results for individual languages. This comparison includes four additional systems: Koo et al. (2010), Martins et al. (2011), Zhang and McDonald (2014) and Pitler and McDonald (2015). 1 65 http://ilk.uvt.nl/conll/software.html ar bg zh cs da nl en de ja pt sl es sv tr av Turbo UAS 79.64 93.10 89.98 90.32 91.48 86.19 93.22 92.41 93.52 92.69 86.01 85.59 91.14 76.90 88.73 Tensor UAS 79.95 93.50 92.68 90.50 91.39 86.41 93.02 91.97 93.71 91.92 86.24 88.00 91.00 76.84 89.08 RGB UAS 80.24 93.72 93.04 90.77 91.86 87.39 93.25 92.67 93.56 92.36 86.72 88.75 91.08 76.68 89.44 In-Out UAS [LAS] 79.60 [67.09] 92.68 [87.79] 92.58 [88.51] 88.01 [79.31] 91.44 [85.55] 84.45 [80.31] 92.45 [89.43] 90.79 [87.74] 9"
I17-1007,P13-2109,0,0.0889674,"] 80.80 [69.40] 94.28 [90.60] 93.40 [90.10] 91.18 [85.92] 91.86 [87.07] 87.85 [84.82] 94.66 [92.52] 93.62 [91.90] 94.02 [92.60] 92.71 [88.92] 86.73 [77.56] 89.20 [85.77] 91.22 [86.92] 77.71 [65.81] 89.95 [84.99] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 89.83 – Table 5: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), Ma and Hovy (2015), and Cheng et al. (2016). to learn sentence segment embedding based on an extra forward LSTM network. Both of these two parsers trained the parsing models by optimizing margin-based objectives. There are three main differences between their models and ours. First, they only used linear form score function, instead of using the bi-linear term between the vectors of heads and modifiers. Second, They did not employ CNNs to model character-level information. Third, we proposed a prob"
I17-1007,D16-1180,0,0.5185,"ects of “gradient exploding”, we use a gradient clipping of 5.0 (Pascanu et al., 2013). We explored other optimization algorithms such as stochastic gradient descent (SGD) with momentum, AdaDelta (Zeiler, 2012), or RMSProp (Dauphin et al., 2015), but none of them meaningfully improve upon Adam with learning rate annealing in our preliminary experiments. cross-entropy global-likelihood Test UAS LAS 93.77 91.57 94.88 92.98 Table 3: Parsing performance on PTB with different training objective functions. 4 4.1 Experiments Setup We evaluate our neural probabilistic parser on the same data setup as Kuncoro et al. (2016), namely the English Penn Treebank (PTB version 3.0) (Marcus et al., 1993), the Penn Chinese Treebank (CTB version 5.1) (Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009). Following previous work, all experiments are evaluated on the metrics of unlabeled attachment score (UAS) and Labeled attachment score (LAS). Dropout Training. To mitigate overfitting, we apply the dropout method (Srivastava et al., 2014; Ma et al., 2017) to regularize our model. As shown in Figure 2 and 3, we apply dropout on character embeddings before inputting to CNN, and on the input, hidden and"
I17-1007,D11-1022,0,0.483943,"del require much more memory than ours. Baselines. We compare our model with the third-order Turbo parser (Martins et al., 2013), the low-rank tensor based model (Tensor) (Lei et al., 2014), the randomized greedy inference based (RGB) model (Zhang et al., 2014), the labeled dependency parser with inner-to-outer greedy decoding algorithm (In-Out) (Ma and Hovy, 2015), and the bi-direction attention based parser (BiAtt) (Cheng et al., 2016). We also compare our parser against the best published results for individual languages. This comparison includes four additional systems: Koo et al. (2010), Martins et al. (2011), Zhang and McDonald (2014) and Pitler and McDonald (2015). 1 65 http://ilk.uvt.nl/conll/software.html ar bg zh cs da nl en de ja pt sl es sv tr av Turbo UAS 79.64 93.10 89.98 90.32 91.48 86.19 93.22 92.41 93.52 92.69 86.01 85.59 91.14 76.90 88.73 Tensor UAS 79.95 93.50 92.68 90.50 91.39 86.41 93.02 91.97 93.71 91.92 86.24 88.00 91.00 76.84 89.08 RGB UAS 80.24 93.72 93.04 90.77 91.86 87.39 93.25 92.67 93.56 92.36 86.72 88.75 91.08 76.68 89.44 In-Out UAS [LAS] 79.60 [67.09] 92.68 [87.79] 92.58 [88.51] 88.01 [79.31] 91.44 [85.55] 84.45 [80.31] 92.45 [89.43] 90.79 [87.74] 93.54 [91.80] 91.54 [87."
I17-1007,P14-1130,0,0.0540007,"90.60] 93.40 [90.10] 91.18 [85.92] 91.86 [87.07] 87.85 [84.82] 94.66 [92.52] 93.62 [91.90] 94.02 [92.60] 92.71 [88.92] 86.73 [77.56] 89.20 [85.77] 91.22 [86.92] 77.71 [65.81] 89.95 [84.99] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 89.83 – Table 5: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), Ma and Hovy (2015), and Cheng et al. (2016). to learn sentence segment embedding based on an extra forward LSTM network. Both of these two parsers trained the parsing models by optimizing margin-based objectives. There are three main differences between their models and ours. First, they only used linear form score function, instead of using the bi-linear term between the vectors of heads and modifiers. Second, They did not employ CNNs to model character-level information. Third, we proposed a probabilistic model ove"
I17-1007,P05-1012,0,0.952983,"ntity recognition (NER) (Chiu and Nichols, 2016). By utilizing distributed representations as inputs, these systems are capable of learning hidden information representations directly from data instead of manually designing hand-crafted features, yielding end-to-end models (Ma and Hovy, 2016). Previous studies explored the applicability of neural representations to traditional graph-based parsing models. Some work (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016) replaced the linear scoring function of each arc in traditional models with neural networks and used a margin-based objective (McDonald et al., 2005a) for model training. Other work (Zhang et al., 2016; Dozat and Manning, 2016) formalized dependency parsing as independently selecting the head of each word with cross-entropy objective, without the guarantee of a general non-projective tree structure output. Moreover, there have yet been no previous work on deriving a neural probIn this paper, we propose a probabilistic parsing model that defines a proper conditional probability distribution over nonprojective dependency trees for a given sentence, using neural representations as inputs. The neural network architecture is based on bi-direct"
I17-1007,N15-1142,0,0.0530678,"Missing"
I17-1007,D15-1166,0,0.0271961,"the score function of edge from xh to xm , and Z(x; Θ) = X  exp  y∈T (x) X  shm  (xh ,xm )∈y is the partition function. Bi-Linear Score Function. In our model, we adopt a bi-linear form score function: φ(xh , xm ; Θ) = ϕ(xh )T Wϕ(xm ) +UT ϕ(xh ) + VT ϕ(xm ) + b where Θ = {W, U, V, b}, ϕ(xi ) is the representation vector of xi , W, U, V denote the weight matrix of the bi-linear term and the two weight vectors of the linear terms in φ, and b denotes the bias vector. As discussed in Dozat and Manning (2016), the bi-linear form of score function is related to the bilinear attention mechanism (Luong et al., 2015). The bi-linear score function differs from the traditional score function proposed in Kiperwasser and Goldberg (2016) by adding the bi-linear term. A similar score function is proposed in Dozat and Manning (2016). The difference between their and our score function is that they only used the linear term for head words (UT ϕ(xh )) while use them for both heads and modifiers. Neural Probabilistic Parsing Model In this section, we describe the components (layers) of our neural parsing model. We introduce the neural layers in our neural network one-by-one from top to bottom. 2.1 dobj amod subj Ed"
I17-1007,H05-1066,0,0.943669,"ntity recognition (NER) (Chiu and Nichols, 2016). By utilizing distributed representations as inputs, these systems are capable of learning hidden information representations directly from data instead of manually designing hand-crafted features, yielding end-to-end models (Ma and Hovy, 2016). Previous studies explored the applicability of neural representations to traditional graph-based parsing models. Some work (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016) replaced the linear scoring function of each arc in traditional models with neural networks and used a margin-based objective (McDonald et al., 2005a) for model training. Other work (Zhang et al., 2016; Dozat and Manning, 2016) formalized dependency parsing as independently selecting the head of each word with cross-entropy objective, without the guarantee of a general non-projective tree structure output. Moreover, there have yet been no previous work on deriving a neural probIn this paper, we propose a probabilistic parsing model that defines a proper conditional probability distribution over nonprojective dependency trees for a given sentence, using neural representations as inputs. The neural network architecture is based on bi-direct"
I17-1007,P14-2107,0,0.110267,"emory than ours. Baselines. We compare our model with the third-order Turbo parser (Martins et al., 2013), the low-rank tensor based model (Tensor) (Lei et al., 2014), the randomized greedy inference based (RGB) model (Zhang et al., 2014), the labeled dependency parser with inner-to-outer greedy decoding algorithm (In-Out) (Ma and Hovy, 2015), and the bi-direction attention based parser (BiAtt) (Cheng et al., 2016). We also compare our parser against the best published results for individual languages. This comparison includes four additional systems: Koo et al. (2010), Martins et al. (2011), Zhang and McDonald (2014) and Pitler and McDonald (2015). 1 65 http://ilk.uvt.nl/conll/software.html ar bg zh cs da nl en de ja pt sl es sv tr av Turbo UAS 79.64 93.10 89.98 90.32 91.48 86.19 93.22 92.41 93.52 92.69 86.01 85.59 91.14 76.90 88.73 Tensor UAS 79.95 93.50 92.68 90.50 91.39 86.41 93.02 91.97 93.71 91.92 86.24 88.00 91.00 76.84 89.08 RGB UAS 80.24 93.72 93.04 90.77 91.86 87.39 93.25 92.67 93.56 92.36 86.72 88.75 91.08 76.68 89.44 In-Out UAS [LAS] 79.60 [67.09] 92.68 [87.79] 92.58 [88.51] 88.01 [79.31] 91.44 [85.55] 84.45 [80.31] 92.45 [89.43] 90.79 [87.74] 93.54 [91.80] 91.54 [87.68] 84.39 [73.74] 86.44 [83"
I17-1007,P10-1142,0,0.0288805,"marginals can be computed efficiently, leading to a straightforward end-to-end model training procedure via back-propagation. We evaluate our model on 17 different datasets, across 14 different languages. Our parser achieves state-of-the-art parsing performance on nine datasets. 1 Introduction Dependency parsing is one of the first stages in deep language understanding and has gained interest in the natural language processing (NLP) community, due to its usefulness in a wide range of applications. Many NLP systems, such as machine translation (Xie et al., 2011), entity coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., 2016), low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014), and word sense disambiguation (Fauceglia et al., 2015), are becoming more sophisticated, in part because of utilizing syntactic knowledge such 59 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 59–69, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP abilistic parsing model to define a proper conditional distribution over non-projective trees for a given sentence. In this paper, we propose a probabilistic neural netw"
I17-1007,D14-1109,0,0.0689882,"] 91.18 [85.92] 91.86 [87.07] 87.85 [84.82] 94.66 [92.52] 93.62 [91.90] 94.02 [92.60] 92.71 [88.92] 86.73 [77.56] 89.20 [85.77] 91.22 [86.92] 77.71 [65.81] 89.95 [84.99] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 89.83 – Table 5: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), Ma and Hovy (2015), and Cheng et al. (2016). to learn sentence segment embedding based on an extra forward LSTM network. Both of these two parsers trained the parsing models by optimizing margin-based objectives. There are three main differences between their models and ours. First, they only used linear form score function, instead of using the bi-linear term between the vectors of heads and modifiers. Second, They did not employ CNNs to model character-level information. Third, we proposed a probabilistic model over non-projective tree"
I17-1007,C04-1010,0,0.0253197,"Missing"
I17-1007,N15-1068,0,0.182446,"compare our model with the third-order Turbo parser (Martins et al., 2013), the low-rank tensor based model (Tensor) (Lei et al., 2014), the randomized greedy inference based (RGB) model (Zhang et al., 2014), the labeled dependency parser with inner-to-outer greedy decoding algorithm (In-Out) (Ma and Hovy, 2015), and the bi-direction attention based parser (BiAtt) (Cheng et al., 2016). We also compare our parser against the best published results for individual languages. This comparison includes four additional systems: Koo et al. (2010), Martins et al. (2011), Zhang and McDonald (2014) and Pitler and McDonald (2015). 1 65 http://ilk.uvt.nl/conll/software.html ar bg zh cs da nl en de ja pt sl es sv tr av Turbo UAS 79.64 93.10 89.98 90.32 91.48 86.19 93.22 92.41 93.52 92.69 86.01 85.59 91.14 76.90 88.73 Tensor UAS 79.95 93.50 92.68 90.50 91.39 86.41 93.02 91.97 93.71 91.92 86.24 88.00 91.00 76.84 89.08 RGB UAS 80.24 93.72 93.04 90.77 91.86 87.39 93.25 92.67 93.56 92.36 86.72 88.75 91.08 76.68 89.44 In-Out UAS [LAS] 79.60 [67.09] 92.68 [87.79] 92.58 [88.51] 88.01 [79.31] 91.44 [85.55] 84.45 [80.31] 92.45 [89.43] 90.79 [87.74] 93.54 [91.80] 91.54 [87.68] 84.39 [73.74] 86.44 [83.29] 89.94 [83.09] 75.32 [60.39"
I17-1007,D07-1014,0,0.138075,"ing model is easily extended to include dependency labels. In labeled dependency trees, each edge is represented by a tuple (xh , xm , l), where xh and xm are the head word and modifier, respectively, and l is the label of dependency type of this edge. Then we can extend the original model for labeled dependency parsing by extending the score function to include dependency labels: y∈T (x):(xh ,xm )∈y where ψ(xh , xm ; Θ) is the potential function: ψ(xh , xm ; Θ) = exp (φ(xh , xm ; Θ)) and µh,m (x; Θ) is the marginal for edge from hth word to mth word for x. Previous studies (Koo et al., 2007; Smith and Smith, 2007) have presented how a variant of Kirchhoff’s Matrix-Tree Theorem (Tutte, 1984) can be used to evaluate the partition function and marginals efficiently. In this section, we briefly revisit this method. For a sentence x with n words, we denote x = {x0 , x1 , . . . , xn }, where x0 is the root-symbol. We define a complete graph G on n + 1 nodes (including the root-symbol x0 ), where each node corresponds to a word in x and each edge corresponds to a dependency arc between two words. Then, we assign non-negative weights to the edges of this complete graph with n + 1 nodes, yielding the weighted a"
I17-1007,W08-2121,0,0.113449,"Missing"
I17-1007,P16-1218,0,0.275645,"ord embeddings, have been broadly applied, with great success, to NLP problems like part-of-speech (POS) tagging (Collobert et al., 2011) and named entity recognition (NER) (Chiu and Nichols, 2016). By utilizing distributed representations as inputs, these systems are capable of learning hidden information representations directly from data instead of manually designing hand-crafted features, yielding end-to-end models (Ma and Hovy, 2016). Previous studies explored the applicability of neural representations to traditional graph-based parsing models. Some work (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016) replaced the linear scoring function of each arc in traditional models with neural networks and used a margin-based objective (McDonald et al., 2005a) for model training. Other work (Zhang et al., 2016; Dozat and Manning, 2016) formalized dependency parsing as independently selecting the head of each word with cross-entropy objective, without the guarantee of a general non-projective tree structure output. Moreover, there have yet been no previous work on deriving a neural probIn this paper, we propose a probabilistic parsing model that defines a proper conditional probability distribution ov"
I17-1007,D11-1020,0,0.0267277,"heorem (Tutte, 1984), the partition functions and marginals can be computed efficiently, leading to a straightforward end-to-end model training procedure via back-propagation. We evaluate our model on 17 different datasets, across 14 different languages. Our parser achieves state-of-the-art parsing performance on nine datasets. 1 Introduction Dependency parsing is one of the first stages in deep language understanding and has gained interest in the natural language processing (NLP) community, due to its usefulness in a wide range of applications. Many NLP systems, such as machine translation (Xie et al., 2011), entity coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., 2016), low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014), and word sense disambiguation (Fauceglia et al., 2015), are becoming more sophisticated, in part because of utilizing syntactic knowledge such 59 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 59–69, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP abilistic parsing model to define a proper conditional distribution over non-projective trees for a given sentence. In this pape"
I17-1007,C02-1145,0,0.120649,"hastic gradient descent (SGD) with momentum, AdaDelta (Zeiler, 2012), or RMSProp (Dauphin et al., 2015), but none of them meaningfully improve upon Adam with learning rate annealing in our preliminary experiments. cross-entropy global-likelihood Test UAS LAS 93.77 91.57 94.88 92.98 Table 3: Parsing performance on PTB with different training objective functions. 4 4.1 Experiments Setup We evaluate our neural probabilistic parser on the same data setup as Kuncoro et al. (2016), namely the English Penn Treebank (PTB version 3.0) (Marcus et al., 1993), the Penn Chinese Treebank (CTB version 5.1) (Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009). Following previous work, all experiments are evaluated on the metrics of unlabeled attachment score (UAS) and Labeled attachment score (LAS). Dropout Training. To mitigate overfitting, we apply the dropout method (Srivastava et al., 2014; Ma et al., 2017) to regularize our model. As shown in Figure 2 and 3, we apply dropout on character embeddings before inputting to CNN, and on the input, hidden and output vectors of BLSTM. We apply dropout rate of 0.15 to all the embeddings. For BLSTM, we use the recurrent dropout (Gal and Ghahramani,"
I17-1007,W14-4012,0,\N,Missing
I17-3016,J90-2002,0,0.910129,"Missing"
I17-3016,2007.mtsummit-papers.13,0,0.0607379,"tem and OpenCC is a rule-based system. STCP outperforms OpenCC in terms of both accuracies and achieved comparable accuracy with XMUCC. Comparisons of these system are in section 5. Experimentation OpenCC XMUCC STCP 98.90 99.81 99.64 Overall Accuracy Ministry of Education of the P.R.C. and Chinese Information Processing Society of China held a competition on the Evaluation of Intelligent Conversion System of Simplified Chinese and Traditional Chinese 2 (MOE-CIPSC) in 2013. There are two core tasks: Character Conversion and Terminology Conversion. Few high-quality parallel corpus is available (Chang and Kung, 2007) and it is expensive to build one. Most websites that claim to have both Simplified Chinese and Traditional Chinese versions are using automatic systems without proofreading, thus are prone to errors. Our evaluation strategy adopts the task one of MOE evaluation. Table 4: Overall accuracies OpenCC XMUCC STCP 91.75 96.98 95.73 Macro-avg Acc. Table 5: Macro-average accuracies 5 Related Work There are several statistical approaches that have been proposed. Chen et al. (2011) integrates statistical features, including language models and lex2 http://www.moe.edu.cn/s78/A19/A19_ gggs/s8478/201302/t2"
I17-3016,1999.mtsummit-1.68,0,0.10675,"Missing"
I17-3016,W11-2123,0,0.0860487,"Missing"
I17-3016,P13-2121,0,0.024027,"Missing"
I17-3016,O10-3002,0,0.020109,"ions are using automatic systems without proofreading, thus are prone to errors. Our evaluation strategy adopts the task one of MOE evaluation. Table 4: Overall accuracies OpenCC XMUCC STCP 91.75 96.98 95.73 Macro-avg Acc. Table 5: Macro-average accuracies 5 Related Work There are several statistical approaches that have been proposed. Chen et al. (2011) integrates statistical features, including language models and lex2 http://www.moe.edu.cn/s78/A19/A19_ gggs/s8478/201302/t20130225_181150.html 3 http://bj.bcebos.com/cips-upload/dzb. txt 63 ical semantic consistencies, into log-linear models. Li et al. (2010) uses look-up tables retrieved from Wikipedia to perform word substitution and disambiguate characters through language model. We adopt this method to build our conversion model. We use different look-up tables and we use higher order language model while they only use bigram and unigram. The four most popular and publicly available systems are Google Translate, Microsoft Translator, Open Chinese Convert (OpenCC), and a system co-developed by Xiamen University, Ministry of Education of The People’s Republic of China, and Beijing Normal University (XMUCC). OpenCC 4 is an open-source project tha"
J02-4001,P99-1071,1,0.535805,"Missing"
J02-4001,J96-2004,0,0.0414546,"Missing"
J02-4001,W00-0408,0,0.00656006,"Missing"
J02-4001,J02-4006,0,0.00628879,"om the original document, Witbrock and Mittal (1999) extract a set of words from the input document and then order the words into sentences using a bigram language model. Jing and McKeown (1999) point out that human summaries are often constructed from the source document by a process of cutting and pasting document fragments that are then combined and regenerated as summary sentences. Hence a summarizer can be developed to extract sentences, reduce them by dropping unimportant fragments, and then use information fusion and generation to combine the remaining fragments. In this special issue, Jing (2002) reports on automated techniques to build a corpus representing the cut-and-paste process used by humans; such a corpus can then be used to train an automated summarizer. Other researchers focus on the reduction process. In an attempt to learn rules for reduction, Knight and Marcu (2000) use expectation maximization to train a system to compress the syntactic parse tree of a sentence in order to produce a shorter but 401 Computational Linguistics Volume 28, Number 4 still maximally grammatical version. Ultimately, this approach can likely be used for shortening two sentences into one, three in"
J02-4001,A97-1042,1,0.514115,"rely on machine learning to identify important features, on natural language analysis to identify key passages, or on relations between words rather than bags of words. The application of machine learning to summarization was pioneered by Kupiec, Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier to combine features from a corpus of scientific articles and their abstracts. Aone et al. (1999) and Lin (1999) experimented with other forms of machine learning and its effectiveness. Machine learning has also been applied to learning individual features; for example, Lin and Hovy (1997) applied machine learning to the problem of determining how sentence position affects the selection of sentences, and Witbrock and Mittal (1999) used statistical approaches to choose important words and phrases and their syntactic context. 400 Radev, Hovy, and McKeown Summarization: Introduction Approaches involving more sophisticated natural language analysis to identify key passages rely on analysis either of word relatedness or of discourse structure. Some research uses the degree of lexical connectedness between potential passages and the remainder of the text; connectedness may be measure"
J02-4001,W01-0100,0,0.107473,"ms that can automatically summarize one or more documents become increasingly desirable. Recent research has investigated types of summaries, methods to create them, and methods to evaluate them. Several evaluation competitions (in the style of the National Institute of Standards and Technology’s [NIST’s] Text Retrieval Conference [TREC]) have helped determine baseline performance levels and provide a limited set of training material. Frequent workshops and symposia reflect the ongoing interest of researchers around the world. The volume of papers edited by Mani and Maybury (1999) and a book (Mani 2001) provide good introductions to the state of the art in this rapidly evolving subfield. A summary can be loosely defined as a text that is produced from one or more texts, that conveys important information in the original text(s), and that is no longer than half of the original text(s) and usually significantly less than that. Text here is used rather loosely and can refer to speech, multimedia documents, hypertext, etc. The main goal of a summary is to present the main ideas in a document in less space. If all sentences in a text document were of equal importance, producing a summary would no"
J02-4001,P99-1072,0,0.0527337,"Missing"
J02-4001,W97-0713,0,0.0944504,"y the number of shared words, synonyms, or anaphora (e.g., Salton et al. 1997; Mani and Bloedorn 1997; Barzilay and Elhadad 1999). Other research rewards passages that include topic words, that is, words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure reliably, which is not possible in all genres. This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002), which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification. An open-source summarization environment, MEAD, was recently developed at the Johns Hopkins summer workshop (Radev et al. 2002). MEAD allows researchers to experiment with different features and methods for combination. Some re"
J02-4001,C94-1056,0,0.0181083,"Missing"
J02-4001,W02-0404,1,0.419938,"Missing"
J02-4001,2001.mtsummit-papers.68,0,0.0198146,"Missing"
J02-4001,W00-0403,1,0.391204,"Missing"
J02-4001,J98-3005,1,0.16809,"ems from different source documents. In an early approach to multidocument summarization, information extraction was used to facilitate the identification of similarities and differences (McKeown and Radev 1995). As for single-document summarization, this approach produces more of a briefing than a summary, as it contains only preidentified information types. Identity of slot values are used to determine when information is reliable enough to include in the summary. Later work merged information extraction approaches with regeneration of extracted text to improve summary generation (Radev and McKeown 1998). Important differences (e.g., updates, trends, direct contradictions) are identified through a set of discourse rules. Recent work also follows this approach, using enhanced information extraction and additional forms of contrasts (White and Cardie 2002). To identify redundancy in text documents, various similarity measures are used. A common approach is to measure similarity between all pairs of sentences and then use clustering to identify themes of common information (McKeown et al. 1999; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001). Alternatively, systems measure the similari"
J02-4001,J02-4005,0,0.0345746,"Missing"
J02-4001,J02-4004,0,0.00917955,"Missing"
J02-4001,J02-4002,0,0.0637939,", words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure reliably, which is not possible in all genres. This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002), which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification. An open-source summarization environment, MEAD, was recently developed at the Johns Hopkins summer workshop (Radev et al. 2002). MEAD allows researchers to experiment with different features and methods for combination. Some recent work (Conroy and O’Leary 2001) has turned to the use of hidden Markov models (HMMs) and pivoted QR decomposition to reflect the fact that the probability of inclusion of a sentence in an extract depends"
J02-4001,W02-0402,0,0.0221134,"Missing"
J02-4001,J02-4003,0,0.00735213,"Missing"
J02-4001,W97-0703,0,\N,Missing
J02-4001,E99-1011,0,\N,Missing
J02-4001,P02-1058,1,\N,Missing
J02-4001,P02-1040,0,\N,Missing
J02-4001,W97-0704,1,\N,Missing
J02-4001,W02-0406,1,\N,Missing
J02-4001,X98-1024,0,\N,Missing
J02-4001,H01-1065,1,\N,Missing
J02-4001,C67-1037,0,\N,Missing
J13-3001,J08-4005,0,0.073006,"Missing"
J13-3001,C04-1051,0,0.741508,"Missing"
J19-4002,H05-1071,0,0.0754747,"Missing"
J19-4002,D07-1087,0,0.00946961,"cally extracting structured information from unstructured and/or semi-structured documents. Although there has been a lot of work in IE on domains such as Web documents (Chang, Hsu, and Lui 2003; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is l"
J19-4002,W04-2504,0,0.561617,"; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into a number of sections with section subtitles. Parts of the text may be italicized or boldfaced to place appropriate emphasis wherever required. The text may contain itemized lists, footnotes,"
J19-4002,P08-1007,0,0.0604381,"Missing"
J19-4002,W12-3014,0,0.0254481,"; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our approach tackles these issues by making judicious use of typographical information, the redundancy of information, and ordering const"
J19-4002,J87-1002,0,0.504203,"try problems, making it more accurate as well as more explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 201"
J19-4002,C14-1206,0,0.049657,"Missing"
J19-4002,W16-1303,0,0.0164499,"04; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our approach tackles these issues by making judicious use of typographical information, the redundancy of information, and ordering constraints to improve th"
J19-4002,N10-1031,0,0.0263931,"Missing"
J19-4002,P09-1075,0,0.0225883,"component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Ch"
J19-4002,P12-1007,0,0.0226245,"c devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et"
J19-4002,P14-1048,0,0.380757,"to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse conside"
J19-4002,D14-1168,0,0.0268694,"ann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into"
J19-4002,W12-1622,0,0.0534271,"Missing"
J19-4002,J86-3001,0,0.739399,"isting solver for geometry problems, making it more accurate as well as more explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and"
J19-4002,P14-1092,0,0.0603686,"Missing"
J19-4002,P14-1002,0,0.0243145,"nicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without m"
J19-4002,P13-1127,0,0.012602,"Language to Programs: After harvesting axioms from textbooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-s"
J19-4002,P14-1003,0,0.0224181,"The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting."
J19-4002,D15-1193,0,0.0155235,"semi-structured documents. Although there has been a lot of work in IE on domains such as Web documents (Chang, Hsu, and Lui 2003; Etzioni et al. 2004; Cafarella et al. 2005; Chang et al. 2006; Banko et al. 2007; Etzioni et al. 2008; Mitchell et al. 2015) and scientific publication data (Shah et al. 2003; Peng and McCallum 2006; Saleem and Latif 2012), work on IE from educational material is much more sparse. Most of the research in IE from educational material deals with extracting simple educational concepts (Shah et al. 2003; Canisius and Sporleder 2007; Yang et al. 2015; Wang et al. 2015; Liang et al. 2015; Wu et al. 2015; Liu et al. 2016b; Wang et al. 2016) or binary relational tuples (Balasubramanian et al. 2002; Clark et al. 2012; Dalvi et al. 2016) using existing IE techniques. On the other hand, our approach extracts axioms and parses them to horn-clause rules. This is much more challenging. Raw application of rule mining or sequence labeling techniques used to extract information from Web documents and scientific publications to educational material usually leads to poor results as the amount of redundancy in educational material is lower and the amount of labeled data is sparse. Our appr"
J19-4002,P11-1060,0,0.017064,"Missing"
J19-4002,W04-1013,0,0.00883788,"iscourse elements in the two mentions. Alignment Scores We use an off-the-shelf monolingual word aligner—JACANA (Yao et al. 2013) pretrained on PPDB—and compute alignment score between axiom mentions as the feature. MT Metrics We use two common MT evaluation metrics METEOR (Denkowski and Lavie 2010) and MAXSIM (Chan and Ng 2008), and use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM performs bipartite graph matching and maps each word in one axiom to at most one word in the other. Summarization Metrics We also use Rouge-S (Lin 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on skip-grams. JSON structure Indicator matching the current (and parent) node of axiom mentions in respective JSON hierarchies; i.e., are both nodes mentioned as axioms, diagrams or bounding boxes? Equation Template Indicator feature that matches templates of equations detected in the axiom mentions. The template matcher is designed such that it identifies various rewritings of the same axiom equation, e.g., PA × PB = PT2 and PA × PB = PC2 could refer to the same axiom with point T in one axiom mention"
J19-4002,P16-1057,0,0.0602349,"Missing"
J19-4002,P17-1015,0,0.0126677,"ooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-scale corpus studies of multimedia text have been rare beca"
J19-4002,W10-4327,0,0.0608886,"Missing"
J19-4002,P14-5010,0,0.00542909,"Missing"
J19-4002,J96-3006,0,0.282068,"ore explainable. 1. Introduction The study of discourse focuses on the properties of text as a whole and how meaning is conveyed by making connections between component sentences. Writers often use certain linguistic devices to make a discourse structure that enables them to effectively communicate their narrative. The readers, too, comprehend text by picking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 20"
J19-4002,P15-1121,0,0.0128899,"nd Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is often richly formatted. Be it newsprint, textbooks, brochures, or even scientific articles, text is usually appropriately formatted and stylized. For example, the text may have a heading. It may be divided into a number of sections with section subtitles. Parts of the text may be italicized or boldfaced to place appropriate emphasis wherever required. The text may contain itemized lists, footnotes, indentations, or quotations. It may refer to asso"
J19-4002,P15-1085,0,0.0213097,"Missing"
J19-4002,P15-1024,1,0.90458,"Missing"
J19-4002,D15-1171,0,0.193157,"es in a multimedia document (Hovy 1998) for the various stages of information extraction. Our experiments show the usefulness of all the various typographical features over and above the various lexical semantic and discourse level features considered for the task. We use our model to extract and parse axiomatic knowledge from a novel data set of 20 publicly available math textbooks. We use this structured axiomatic knowledge to build a new axiomatic solver that performs logical inference to solve geometry problems. Our axiomatic solver outperforms GEOS on all existing test sets introduced in Seo et al. (2015) as well as a new test set of geometry questions collected from these textbooks. We also performed user studies on a number of school students studying geometry who found that our axiomatic solver is more interpretable and useful compared with GEOS. 2. Background and Related Work Discourse Analysis: Discourse analysis is the analysis of semantics conveyed by a coherent sequence of sentences, propositions, or speech. Discourse analysis is taken up in a variety of disciplines in the humanities and social sciences and a number of discourse theories have been proposed (Mann and Thompson 1988; Kamp"
J19-4002,N03-1030,0,0.361741,"Missing"
J19-4002,N09-1064,0,0.0330417,"Missing"
J19-4002,K15-2002,0,0.0155504,"icking up these linguistic devices and recognizing the discourse structure. There are a number of linguistic theories on discourse relations (Van Dijk 1972; Longacre 1983; Grosz and Sidner 1986; Cohen 1987; Mann and Thompson 1988; Polanyi 1988; Moser and Moore 1996) that specify relations between discourse units and how to represent the discourse structure of a piece of text (i.e., discourse parsing; Duverle and Prendinger 2009; Subba and Di Eugenio 2009; Feng and Hirst 2012; Gosh, Riccardi, and Johansson 2012; Feng and Hirst 2014; Ji and Eisenstein 2014; Li et al. 2014; Li, Ng, and Kan 2014; Wang and Lan 2015). These discourse features have been shown to be useful in a number of NLP applications such as summarization (Dijk 1979; Marcu 2000; Boguraev and Neff 2000; Louis, Joshi, and Nenkova 2010; Gerani et al. 2014), information retrieval (Wang et al. 2006; Lioma, Larsen, and Lu 2012), information extraction (Kitani, Eriguchi, and Hara 1994; Conrath et al. 2014), and question answering (Chai and Jin 2004; Sun and Chai 2007; Narasimhan and Barzilay 2015; Sachan et al. 2015). Most linguistic theories of discourse consider written text without much formatting. However, in this multimedia age, text is o"
J19-4002,P13-2123,0,0.0111238,"es (constants, predicates, and functions) across the two axioms. When comparing geometric entities, we include geometric entities derived from the associated diagrams when available. Longest Common Subsequence Real valued feature that computes the length of longest common subsequence of words between two axiom mentions normalized by the total number of words in the two mentions. Number of discourse elements Real valued feature that computes the absolute difference in the number of discourse elements in the two mentions. Alignment Scores We use an off-the-shelf monolingual word aligner—JACANA (Yao et al. 2013) pretrained on PPDB—and compute alignment score between axiom mentions as the feature. MT Metrics We use two common MT evaluation metrics METEOR (Denkowski and Lavie 2010) and MAXSIM (Chan and Ng 2008), and use the evaluation scores as features. While METEOR computes n-gram overlaps controlling on precision and recall, MAXSIM performs bipartite graph matching and maps each word in one axiom to at most one word in the other. Summarization Metrics We also use Rouge-S (Lin 2004), a text summarization metric, and use the evaluation score as a feature. Rouge-S is based on skip-grams. JSON structure"
J19-4002,P17-1041,0,0.0605138,"ing axioms from textbooks, we also parse the axiom mentions to horn-clause rules. This work is related to a large body of work on semantic parsing (Zelle and Mooney 1993, 1996; Kate et al. 2005; Zettlemoyer and Collins 2012, inter alia). Semantic parsers typically map natural language to formal programs such as database queries (Liang, Jordan, and Klein 2011; Berant et al. 2013; Yaghmazadeh et al. 2017, inter alia), commands to robots (Shimizu and Haas 2009; Matuszek, Fox, and Koscher 2010; Chen and Mooney 2011, inter alia), or even general purpose programs (Lei et al. 2013; Ling et al. 2016; Yin and Neubig 2017; Ling et al. 2017). More specifically, Liu et al. (2016a) and Quirk, Mooney, and Galley (2015) learn “If-Then” and “If-This-Then-That” rules, respectively. In theory, these works can be adapted to parse axiom mentions to horn-clause rules. However, this would require a large amount of supervision, which would be expensive to obtain. We mitigated this issue by using redundant axiom mention extractions from multiple textbooks and then combining the parses obtained from various textbooks to achieve a better final parse for each axiom. 3. Data Format Large-scale corpus studies of multimedia text"
J89-2012,P83-1001,0,0.0663519,"Missing"
K19-1033,D14-1059,0,0.0211661,", straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we nee"
K19-1033,W03-0906,0,0.209086,"Missing"
K19-1033,W17-5310,0,0.012966,"ide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy, antonymy (Glockner et al., 2018; Naik et"
K19-1033,D17-1070,0,0.0604839,"attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy, antonymy (Glockner e"
K19-1033,bentivogli-etal-2010-building,0,0.0745212,"Missing"
K19-1033,H05-1079,0,0.113473,"Missing"
K19-1033,P08-1118,0,0.109261,"Missing"
K19-1033,D15-1075,0,0.0536693,"ovides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge task"
K19-1033,P17-1152,0,0.282638,"ingent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy, antonymy (Glockner et al., 2018; Naik et al., 2018). In this"
K19-1033,N19-1246,0,0.152828,"Missing"
K19-1033,P14-1026,0,0.142336,"lenge area for textual entailment stands. 2 While to the best of our knowledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glic"
K19-1033,W07-1401,0,0.0672176,"focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the f"
K19-1033,P18-2103,0,0.0574716,"al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy, antonymy (Glockner et al., 2018; Naik et al., 2018). In this work, we specifically probe into quantitative reasoning. Recently, Dua et al. (2019) also recognize the importance of quantitative reasoning for text understanding. They propose DROP, a reading comprehension dataset focused on a limited set of discrete operations such as counting, comparison, sorting and arithmetic. In contrast, EQUATE features diverse phenomena that occur naturally in text, including reasoning with approximation, ordinals, implicit quantities and quantifiers, requiring NLI models to reason comprehensively about the interplay between quantities an"
K19-1033,N18-2017,0,0.0321211,"list (Pc ∪ Pr ∪ H ∪ O) TL Type list (set of types from Pc , Pr , H) N Length of symbol list K Index of first range quantity in symbol list M Index of first operator in symbol list OUTPUT ei Index of symbol assigned to ith position in postfix equation VARIABLES xi Main ILP variable for position i ci Indicator variable: is ei a single value? ri Indicator variable: is ei a range? oi Indicator variable: is ei an operator? di Stack depth of ei ti Type index for ei Figure 1: Overview of Q-REAS baseline. 2) Hypothesis-Only (HYP): FastText classifier (Mikolov et al., 2018) trained on only hypotheses (Gururangan et al., 2018). 3) ALIGN: A bag-of-words alignment model inspired by MacCartney (2009).10 4) CBOW: A simple bag-of-embeddings sentence representation model (wil). 5) BiLSTM: The simple BiLSTM model described by wil. 6) Chen (CH): Stacked BiLSTM-RNNs with shortcut connections and character word embeddings (Chen et al., 2017b). 7) InferSent: A single-layer BiLSTM-RNN model with max-pooling (Conneau et al., 2017). 8) SSEN: Stacked BiLSTM-RNNs with shortcut connections (Nie and Bansal, 2017). 9) ESIM: Sequential inference model proposed by Chen et al. (2017a) which uses BiLSTMs with an attention mechanism. 10)"
K19-1033,P17-1015,0,0.268598,"owledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006;"
K19-1033,H05-1049,0,0.0345695,"i et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task lik"
K19-1033,P06-1114,0,0.0668325,"y, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which"
K19-1033,W07-1407,0,0.0184018,"ment for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones"
K19-1033,marelli-etal-2014-sick,0,0.0373596,"f-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will"
K19-1033,D14-1058,0,0.0373675,"tween linguistic and numerical reasoning. The EQUATE evaluation framework makes it clear where this new challenge area for textual entailment stands. 2 While to the best of our knowledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted communit"
K19-1033,L18-1008,0,0.0338838,"⊆} L Length of equation to be generated SL Symbol list (Pc ∪ Pr ∪ H ∪ O) TL Type list (set of types from Pc , Pr , H) N Length of symbol list K Index of first range quantity in symbol list M Index of first operator in symbol list OUTPUT ei Index of symbol assigned to ith position in postfix equation VARIABLES xi Main ILP variable for position i ci Indicator variable: is ei a single value? ri Indicator variable: is ei a range? oi Indicator variable: is ei an operator? di Stack depth of ei ti Type index for ei Figure 1: Overview of Q-REAS baseline. 2) Hypothesis-Only (HYP): FastText classifier (Mikolov et al., 2018) trained on only hypotheses (Gururangan et al., 2018). 3) ALIGN: A bag-of-words alignment model inspired by MacCartney (2009).10 4) CBOW: A simple bag-of-embeddings sentence representation model (wil). 5) BiLSTM: The simple BiLSTM model described by wil. 6) Chen (CH): Stacked BiLSTM-RNNs with shortcut connections and character word embeddings (Chen et al., 2017b). 7) InferSent: A single-layer BiLSTM-RNN model with max-pooling (Conneau et al., 2017). 8) SSEN: Stacked BiLSTM-RNNs with shortcut connections (Nie and Bansal, 2017). 9) ESIM: Sequential inference model proposed by Chen et al. (2017a)"
K19-1033,D17-1084,0,0.0146358,"where this new challenge area for textual entailment stands. 2 While to the best of our knowledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al."
K19-1033,P16-1202,0,0.0313731,"merical reasoning. The EQUATE evaluation framework makes it clear where this new challenge area for textual entailment stands. 2 While to the best of our knowledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a st"
K19-1033,C18-1198,1,0.931097,"l., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy, antonymy (Glockner et al., 2018; Naik et al., 2018). In this work, we specifically probe into quantitative reasoning. Recently, Dua et al. (2019) also recognize the importance of quantitative reasoning for text understanding. They propose DROP, a reading comprehension dataset focused on a limited set of discrete operations such as counting, comparison, sorting and arithmetic. In contrast, EQUATE features diverse phenomena that occur naturally in text, including reasoning with approximation, ordinals, implicit quantities and quantifiers, requiring NLI models to reason comprehensively about the interplay between quantities and language. Ad350 di"
K19-1033,W17-5308,0,0.0711571,". Related Work NLI has attracted community-wide interest as a stringent test for natural language understanding (Cooper et al., 1996; Fyodorov; Glickman et al., 2005; Haghighi et al., 2005; Harabagiu and Hickl, 2006; Romano et al., 2006; Dagan et al., 2006; Giampiccolo et al., 2007; Zanzotto et al., 2006; Malakasiotis and Androutsopoulos, 2007; MacCartney, 2009; de2; Dagan et al., 2010; Angeli and Manning, 2014; Marelli et al., 2014). Recently, the creation of large-scale datasets (Bowman et al., 2015; wil; Khot et al., 2018) spurred the development of many neural models (Parikh et al., 2016; Nie and Bansal, 2017; Conneau et al., 2017; Balazs et al., 2017; Chen et al., 2017a; Radford et al., 2018; Devlin et al., 2018). However, state-of-the-art models for NLI treat the task like a matching problem, which appears to work in many cases, but breaks down in others. As the field moves past current models of the matching variety to ones that embody more of the reasoning we know is part of the task, we need benchmarks that will enable us to mark progress in the field. Prior work on challenge tasks has already made headway in defining tasks for subproblems such as lexical inference with hypernymy, co-hyponymy"
K19-1033,D16-1244,0,0.108172,"Missing"
K19-1033,P10-1122,0,0.0774486,"Missing"
K19-1033,D16-1029,0,0.0127033,"ramework makes it clear where this new challenge area for textual entailment stands. 2 While to the best of our knowledge, prior work has not studied quantitative reasoning in NLI, Roy (2017) propose a model for a related subtask called quantity entailment, which aims to determine if a given quantity can be inferred from a sentence. In contrast, our work is concerned with general-purpose textual entailment which considers if a given sentence can be inferred from another. Our work also relates to solving arithmetic word problems (Hosseini et al., 2014; Mitra and Baral, 2016; Zhou et al., 2015; Upadhyay et al., 2016; Huang et al., 2017; Kushman et al., 2014a; Koncel-Kedziorski et al., 2015; roy; Roy, 2017; Ling et al., 2017a). A key difference is that word problems focus on arithmetic reasoning, while the requirement for linguistic reasoning and world knowledge is limited as the text is concise, straightforward, and self-contained (Hosseini et al., 2014; Kushman et al., 2014b). Our work provides a testbed that evaluates basic arithmetic reasoning while incorporating the complexity of natural language. Related Work NLI has attracted community-wide interest as a stringent test for natural language understa"
K19-1033,D15-1096,0,0.0349587,"Missing"
K19-1033,E06-1052,0,\N,Missing
K19-1033,Q15-1001,0,\N,Missing
K19-1033,W09-2501,0,\N,Missing
K19-1033,Q15-1042,0,\N,Missing
K19-1033,W17-5301,0,\N,Missing
K19-1033,S18-2023,0,\N,Missing
L16-1206,C12-1044,0,0.327778,"te, but this information is not available for many active editors and is insufficient in explaining the nature of an editor’s work. While classification based on edit histories can be constructed for most active editors, current approaches focus on simple edit counts and access privileges fail to provide a finer grained description of the work actually performed in an edit. For example, it cannot tell the difference between an editor who copy-edits a paragraph and an editor who inserts markup or template to an article. In this work, we extend Daxenberger’s fine grained taxonomy of edit types (Daxenberger and Gurevych, 2012; Daxenberger and Gurevych, 2013) to differentiate edits and editors who occupy different editing roles. The edits are distinguished contextually in terms of the object being edited (e.g. information, template, reference, etc.) and functionally, in terms of the edit operation (e.g. insert, delete, etc.). The corpus construction will be described in detail later. Based on our corpus, we then described the development of methods for the automated measurement of these 24 edits categories revealed in users’ edits. These categories can be identified with a relatively reasonable performance by using"
L16-1206,D13-1055,0,0.215375,"available for many active editors and is insufficient in explaining the nature of an editor’s work. While classification based on edit histories can be constructed for most active editors, current approaches focus on simple edit counts and access privileges fail to provide a finer grained description of the work actually performed in an edit. For example, it cannot tell the difference between an editor who copy-edits a paragraph and an editor who inserts markup or template to an article. In this work, we extend Daxenberger’s fine grained taxonomy of edit types (Daxenberger and Gurevych, 2012; Daxenberger and Gurevych, 2013) to differentiate edits and editors who occupy different editing roles. The edits are distinguished contextually in terms of the object being edited (e.g. information, template, reference, etc.) and functionally, in terms of the edit operation (e.g. insert, delete, etc.). The corpus construction will be described in detail later. Based on our corpus, we then described the development of methods for the automated measurement of these 24 edits categories revealed in users’ edits. These categories can be identified with a relatively reasonable performance by using a multi-label classification alg"
L16-1206,P13-1071,0,0.0258381,"s essential to developing high quality articles(Kittur and Kraut, 2008). Thus, determining how contribution by different types of work and by different editors at distinct times in an article’s history influence changes in its quality is of great use to better understand the causes of quality variance in Wikipedia (De la Calzada and Dekhtyar, 2010). 3. Quality Flaw Detection: The detection and improvement of low quality information is an essential component in online production communities. Different from existing studies in quality flaw prediction (Anderka et al., 2012; Anderka et al., 2011; Ferschke et al., 2013), our taxonomy enables us to provide which specific aspects (information, reference, wikilink, template or markup, etc.) of an article needs improvement and what operations should be performed. 4. Domain Adaption: Even though the edit taxonomy introduced above is for English Wikipedia, it can be applied to other language versions of Wikipedia. For example, similar taxonomy, or even automated classification models can be transformed for another language. Beyond the context of Wikipedia, similar taxonomies can be designed for analyzing the collaboration and interaction happened in other online c"
liu-etal-2014-supervised,N10-1138,0,\N,Missing
liu-etal-2014-supervised,D11-1116,1,\N,Missing
liu-etal-2014-supervised,W09-3208,0,\N,Missing
liu-etal-2014-supervised,W99-0201,0,\N,Missing
liu-etal-2014-supervised,D12-1045,0,\N,Missing
liu-etal-2014-supervised,H05-1004,0,\N,Missing
liu-etal-2014-supervised,W11-1902,0,\N,Missing
liu-etal-2014-supervised,N04-3012,0,\N,Missing
liu-etal-2014-supervised,P02-1014,0,\N,Missing
liu-etal-2014-supervised,D10-1033,0,\N,Missing
liu-etal-2014-supervised,W12-4501,0,\N,Missing
liu-etal-2014-supervised,P10-1143,0,\N,Missing
liu-etal-2014-supervised,W13-1203,1,\N,Missing
liu-etal-2014-supervised,P13-2083,1,\N,Missing
liu-etal-2014-supervised,W09-4303,0,\N,Missing
liu-etal-2014-supervised,W06-0901,0,\N,Missing
liu-etal-2014-supervised,M93-1007,0,\N,Missing
liu-etal-2014-supervised,P05-1045,0,\N,Missing
N03-1020,W00-0408,0,0.0586575,"ent levels: all, most, some, hardly any, or none6. For example, as shown in Figure 1, an assessor marked system units 1.1 and 10.4 (red/dark underlines in the left pane) as sharing some content with the current model unit 2.2 (highlighted green/dark gray in the right). 2.3 Evaluation Metrics Recall at different compression ratios has been used in summarization research to measure how well an automatic system retains important content of original documents (Mani et al. 1998). However, the simple sentence recall measure cannot differentiate system performance appropriately, as is pointed out by Donaway et al. (2000). Therefore, instead of pure sentence recall score, we use coverage score C. We define it as follows7: (Number of MUs marked) • E (1) Total number of MUs in the model summary E, the ratio of completeness, ranges from 1 to 0: 1 for all, 3/4 for most, 1/2 for some, 1/4 for hardly any, and 0 for none. If we ignore E (set it to 1), we obtain simple sentence recall score. We use average coverage scores derived from human judgments as the references to evaluate various automatic scoring methods in the following sections. summary, the better it is. The question is: “Can we apply BLEU directly without"
N03-1020,W02-0406,1,0.24301,"Missing"
N03-1020,2001.mtsummit-papers.68,0,0.0500634,"ns to evaluate summaries as well?”. We first ran IBM’s BLEU evaluation script unmodified over the DUC 2001 model and peer summary set. The resulting Spearman rank order correlation coefficient (ρ) between BLEU and the human assessment for the single document task is 0.66 using one reference summary and 0.82 using three reference summaries; while Spearman ρ for the multidocument task is 0.67 using one reference and 0.70 using three. These numbers indicate that they positively correlate at α = 0.018. Therefore, BLEU seems a promising automatic scoring metric for summary evaluation. According to Papineni et al. (2001), BLEU is essentially a precision metric. It measures how well a machine translation overlaps with multiple human translations using n-gram co-occurrence statistics. N-gram precision in BLEU is computed as follows: C= 3 BLEU and N-gram Co-Occurrence To automatically evaluate machine translations the machine translation community recently adopted an n-gram co-occurrence scoring procedure BLEU (Papineni et al. 2001). The NIST (NIST 2002) scoring metric is based on BLEU. The main idea of BLEU is to measure the translation closeness between a candidate translation and a set of reference translatio"
N03-1020,H01-1056,0,0.0218636,"Missing"
N03-1020,W00-0400,0,0.088529,"Missing"
N03-1020,P02-1040,0,\N,Missing
N03-1037,P93-1001,0,0.0274139,"ary text, the sentences that are irrelevant to the summary sentence are deleted repeatedly, resulting in the preservation of sentences similar in meaning to the summary sentence. For each sentence in the summary, it is aligned with a number of nonsummary sentences to form (summary sentence, nonsummary sentences) pairs. This alignment is done for each sentence of the summary articles. Finally for each nonsummary we group together all the aligned sentences to form the pair (extract, text). similarity-based: inspired by sentence alignment for multilingual parallel corpora in Machine Translation (Church, 1993; Fung and Church, 1994; Melamed, 1999), we view the alignment between sentences from summaries and sentences from nonsummaries as the alignment of monolingual parallel texts at the sentence level. In every domain of the YFCC, each article is represented as a vector in a vector space where each dimension is a distinct non-stop word appearing in this domain. Measuring the cosinesimilarity between two articles, we can decide whether they are close semantically. This method has been widely used in Information Retrieval (Salton, 1975). To extend this idea, we measure the cosine-similarity between"
N03-1037,C94-2178,0,0.0138771,"sentences that are irrelevant to the summary sentence are deleted repeatedly, resulting in the preservation of sentences similar in meaning to the summary sentence. For each sentence in the summary, it is aligned with a number of nonsummary sentences to form (summary sentence, nonsummary sentences) pairs. This alignment is done for each sentence of the summary articles. Finally for each nonsummary we group together all the aligned sentences to form the pair (extract, text). similarity-based: inspired by sentence alignment for multilingual parallel corpora in Machine Translation (Church, 1993; Fung and Church, 1994; Melamed, 1999), we view the alignment between sentences from summaries and sentences from nonsummaries as the alignment of monolingual parallel texts at the sentence level. In every domain of the YFCC, each article is represented as a vector in a vector space where each dimension is a distinct non-stop word appearing in this domain. Measuring the cosinesimilarity between two articles, we can decide whether they are close semantically. This method has been widely used in Information Retrieval (Salton, 1975). To extend this idea, we measure the cosine-similarity between two sentences, one from"
N03-1037,C00-1072,1,0.723954,"ng the Training Domain There are 463 domains under the 8 categories of YFCC, meaning 463 paired summary-bigram and nonsummary-bigram tables. On average for each domain, the summary-bigram table contains 20000 entries; the nonsummary-bigram table contains 173000 entries. When an unknown text or a set of unknown texts come in to be summarized, the system needs to select the most appropriate pair of bigram tables to create the extract. The most desirable domain for an unknown text or texts contains articles focusing on the same issues as the unknown ones. Two methods are used: • topic signature (Lin and Hovy, 2000): a topic signature is a family of related terms {topic, signature}, where topic is the target concept and signature is a vector of related terms. The topic in the formula is assigned with the domain name. To construct the set of related words, we consider only nouns because we are only interested in the major issues discussed in the domain, not in how those issues evolved. Each noun in the domain receives a tf.idf score. 30 top-scoring nouns are selected to be the signature representing the domain. For each test text, its signature is computed with the same tf.idf method against each domain."
N03-1037,J99-1003,0,0.0118828,"levant to the summary sentence are deleted repeatedly, resulting in the preservation of sentences similar in meaning to the summary sentence. For each sentence in the summary, it is aligned with a number of nonsummary sentences to form (summary sentence, nonsummary sentences) pairs. This alignment is done for each sentence of the summary articles. Finally for each nonsummary we group together all the aligned sentences to form the pair (extract, text). similarity-based: inspired by sentence alignment for multilingual parallel corpora in Machine Translation (Church, 1993; Fung and Church, 1994; Melamed, 1999), we view the alignment between sentences from summaries and sentences from nonsummaries as the alignment of monolingual parallel texts at the sentence level. In every domain of the YFCC, each article is represented as a vector in a vector space where each dimension is a distinct non-stop word appearing in this domain. Measuring the cosinesimilarity between two articles, we can decide whether they are close semantically. This method has been widely used in Information Retrieval (Salton, 1975). To extend this idea, we measure the cosine-similarity between two sentences, one from a summary (week"
N03-2008,J96-1002,0,0.014147,"Missing"
N03-2008,A00-2031,0,0.0424978,"es extracted from parse trees, Human to models using features from FrameNet’s human annotations. 4 The models trained on features extracted from parse trees do not have access to rich grammatical information. Following Gildea and Jurafsky (2000), automatic extraction of grammatical information here is limited to the governing category of a Noun Phrase. The FrameNet annotations, however, are much richer and include information about complements, modifiers, etc. We are looking at ways to include such information either by using alternative parsers (Hermjakob, 1997) or as a post processing task (Blaheta and Charniak, 2000). In future work, we will extend the strategies outlined here to incorporate Frame Element identification into our model. By treating semantic classification as a single tagging problem, we hope to create a unified, practical, and high performance system for Frame Element tagging. Conclusion It is clear that using a tagging framework and syntactic patterns improves performance of the semantic classifier when features are extracted from either automatically generated parse trees or human annotations. The most striking result of these experiments, however, is the dramatic decrease in performance"
N03-2008,P97-1003,0,0.0333556,"Missing"
N03-2008,P00-1065,0,0.444504,"n each FrameNet sentence, a single target predicate is identified and all of its relevant Frame Elements are tagged with their element-type (e.g., Agent, Judge), their syntactic Phrase Type (e.g., NP, PP), and their Grammatical Function (e.g., External Argument, Object Argument). Figure 1 shows an example of an annotated sentence and its appropriate semantic frame. She clapped her hands in inspiration. -NP -NP -PP -Ext. -Obj. -Comp. Figure 1. Frame for lemma “clap” shown with three core Frame Elements and a sentence annotated with element type, phrase type, and grammatical function. We extend Gildea and Jurafsky (2000)’s initial effort in three ways. First, we adopt a Maximum Entropy (ME) framework to better learn the feature weights associated with the classification model. Second, we recast the classification task as a tagging problem in which an n-gram model of Frame Elements is applied to find the most probable tag sequence (as opposed to the most probable individual tags). Finally, we implement a re-ranking system that takes advantage of the sentence-level syntactic patterns of each sequence. We analyze our results using syntactic features extracted from a parse tree generated by Collins parser (Collin"
N03-2008,P97-1062,0,0.0213425,"Extracted refers to models trained using features extracted from parse trees, Human to models using features from FrameNet’s human annotations. 4 The models trained on features extracted from parse trees do not have access to rich grammatical information. Following Gildea and Jurafsky (2000), automatic extraction of grammatical information here is limited to the governing category of a Noun Phrase. The FrameNet annotations, however, are much richer and include information about complements, modifiers, etc. We are looking at ways to include such information either by using alternative parsers (Hermjakob, 1997) or as a post processing task (Blaheta and Charniak, 2000). In future work, we will extend the strategies outlined here to incorporate Frame Element identification into our model. By treating semantic classification as a single tagging problem, we hope to create a unified, practical, and high performance system for Frame Element tagging. Conclusion It is clear that using a tagging framework and syntactic patterns improves performance of the semantic classifier when features are extracted from either automatically generated parse trees or human annotations. The most striking result of these exp"
N03-2008,W00-0729,0,0.0369105,"Missing"
N03-2008,P02-1038,0,0.0689661,"Missing"
N03-2008,J02-3001,0,\N,Missing
N06-1026,J96-1002,0,0.00434684,"of “the criticism of T”, whereas A is the person who has an opinion that B’s criticism is wrong. Therefore, we define our task as finding an opinion holder, given an opinion expression. Our earlier work (ref suppressed) focused on identifying opinion expressions within text. We employ that system in tandem with the one described here. To learn opinion holders automatically, we use a Maximum Entropy model. Maximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible (Berger et al. 1996). There are two ways to model the problem with ME: classification and ranking. Classification allocates each holder candidate to one of a set of predefined classes while ranking selects a single candidate as answer. This means that classification modeling 3 can select many candidates as answers as long as they are marked as true, and does not select any candidate if every one is marked as false. In contrast, ranking always selects the most probable candidate as an answer, which suits our task better. Our earlier experiments showed poor performance with classification modeling, an experience al"
N06-1026,A00-2018,0,0.0218044,"Missing"
N06-1026,H05-1045,0,0.0274793,"1: Annotation example tant as “Is this an opinion?” or “What kind of opinion is expressed here?”. In this section, we describe the automated identification for opinion holders. We define an opinion holder as an entity (person, organization, country, or special group of people) who expresses explicitly or implicitly the opinion contained in the sentence. Previous work that is related to opinion holder identification is (Bethard et al. 2004) who identify opinion propositions and holders. However, their opinion is restricted to propositional opinion and mostly to verbs. Another related work is (Choi et al. 2005) who use the MPQA corpus2 to learn patterns of opinion sources using a graphical model and extraction pattern learning. However, they have a different task definition from ours. They define the task as identifying opinion sources (holders) given a sentence, whereas we define it as identifying opinion sources given an opinion expression in a sentence. We discussed their work in Section 1. Data: As training data, we used the MPQA corpus (Wilson and Wiebe, 2003), which contains news articles manually annotated by 5 trained annotators. They annotated 10657 sentences from 535 documents, in four dif"
N06-1026,P97-1023,0,0.128272,"ive or negative sentiment directly such as “good”, “bad”, “foolish”, “virtuous”, etc. In other words, this is the smallest unit of opinion that can thereafter be used as a clue for sentence-level or text-level opinion detection. We treat word sentiment classification into Positive, Negative, and Neutral as a three-way classification problem instead of a two-way classification problem of Positive and Negative. By adding the third class, Neutral, we can prevent the classifier from assigning either positive or negative sentiment to weak opinion-bearing words. For example, the word “central” that Hatzivassiloglou and McKeown (1997) included as a positive adjective is not classified as positive in our system. Instead we mark it as “neutral” since it is a weak clue for an opinion. If an unknown word has a strong relationship with the neutral class, we can therefore classify it as neutral even if it has some small connotation of Positive or Negative as well. Approach: We built a word sentiment classifier using WordNet and three sets of positive, negative, and neutral words tagged by hand. Our insight is that synonyms of positive words tend to have positive sentiment. We expanded those manually selected seed words of each s"
N06-1026,P00-1056,0,0.00683837,"es a system that can monitor and analyze citizens’ emails essential. The goal of our system is to classify emails as neutral or as bearing a positive or negative opinion. To generate opinion bearing words, we ran the word sentiment classifier from Section 2.1 on 8011 verbs to classify them into 807 positive, 785 negative, and 6149 neutral. For 19748 adjectives, the system classified them into 3254 positive, 303 negative, and 16191 neutral. Since our opinionbearing words are in English and our target system is in German, we also applied a statistical word alignment technique, GIZA++ 6 (Och and Ney 2000). Running it on version two of the European Parliament corpus, we obtained statistics for 678,340 German-English word pairs and 577,362 English-German word pairs. Obtaining these two lists of translation pairs allows us to convert English words to German, and German to English, without a full document translation system. To utilize our English opinion-bearing words in a German opinion analysis system, we developed two models, 5 6 outlined in Table 4, each of which is triggered at different points in the system. In both models, however, we still need to decide how to apply opinion-bearing words"
N06-1026,W02-1011,0,0.0332189,"mmersion of Internet users has come a proliferation of opinions available on the web. Not only do we read more opinions from the web, such as in daily news editorials, but also we post more opinions through mechanisms such as governmental web sites, product review sites, news group message boards and personal blogs. This phenomenon has opened the door for massive opinion collection, which has potential impact on various applications such as public opinion monitoring and product review summary systems. Although in its infancy, many researchers have worked in various facets of opinion analysis. Pang et al. (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. Wiebe et al. (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004) and news articles1. These researchers"
N06-1026,W03-1209,1,0.680996,"oblem with ME: classification and ranking. Classification allocates each holder candidate to one of a set of predefined classes while ranking selects a single candidate as answer. This means that classification modeling 3 can select many candidates as answers as long as they are marked as true, and does not select any candidate if every one is marked as false. In contrast, ranking always selects the most probable candidate as an answer, which suits our task better. Our earlier experiments showed poor performance with classification modeling, an experience also reported for Question Answering (Ravichandran et al. 2003). We modeled the problem to choose the most probable candidate that maximizes a given conditional probability distribution, given a set of holder candidates h 1 h 2 . . . h N and opinion expression e. The conditional probability P h h 1 h 2 . . . h N , e can be calculated based on K feature functions f k h , h1 h2 . .. h N , e . We write a decision rule for the ranking as follows: h = argmax [P(h |{h 1 h 2 ... h N }, e)] h = argmax h [ K ∑ λ k f k (h, {h 1 h 2 ... h N }, e) ] k=1 Each λ k is a model parameter indicating the weight of its feature function. Candidate holder selection … w2 ... w4"
N06-1026,W03-1014,0,0.0580543,"sites, product review sites, news group message boards and personal blogs. This phenomenon has opened the door for massive opinion collection, which has potential impact on various applications such as public opinion monitoring and product review summary systems. Although in its infancy, many researchers have worked in various facets of opinion analysis. Pang et al. (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. Wiebe et al. (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004) and news articles1. These researchers use lists of opinion-bearing clue words and phrases, and then apply various additional techniques and refinements. Along with many opinion researchers, we participated in a large pilot study, sponsored by NIST, which concluded that it is very dif"
N06-1026,P02-1053,0,0.015546,"ers has come a proliferation of opinions available on the web. Not only do we read more opinions from the web, such as in daily news editorials, but also we post more opinions through mechanisms such as governmental web sites, product review sites, news group message boards and personal blogs. This phenomenon has opened the door for massive opinion collection, which has potential impact on various applications such as public opinion monitoring and product review summary systems. Although in its infancy, many researchers have worked in various facets of opinion analysis. Pang et al. (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. Wiebe et al. (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004) and news articles1. These researchers use lists of opin"
N06-1026,P99-1032,0,0.242763,"Missing"
N06-1026,W03-2102,0,0.0165472,"opinion propositions and holders. However, their opinion is restricted to propositional opinion and mostly to verbs. Another related work is (Choi et al. 2005) who use the MPQA corpus2 to learn patterns of opinion sources using a graphical model and extraction pattern learning. However, they have a different task definition from ours. They define the task as identifying opinion sources (holders) given a sentence, whereas we define it as identifying opinion sources given an opinion expression in a sentence. We discussed their work in Section 1. Data: As training data, we used the MPQA corpus (Wilson and Wiebe, 2003), which contains news articles manually annotated by 5 trained annotators. They annotated 10657 sentences from 535 documents, in four different aspects: agent, expressive-subjectivity, on, and inside. Expressivesubjectivity marks words and phrases that indirectly express a private state that is defined as a term for opinions, evaluations, emotions, and speculations. The on annotation is used to mark speech events and direct expressions of private states. As for the holder, we use the agent of the selected private states or speech events. While there are many possible ways to define what opinio"
N06-1026,W03-1017,0,0.386889,"ive opinion collection, which has potential impact on various applications such as public opinion monitoring and product review summary systems. Although in its infancy, many researchers have worked in various facets of opinion analysis. Pang et al. (2002) and Turney (2002) classified sentiment polarity of reviews at the document level. Wiebe et al. (1999) classified sentence level subjectivity using syntactic classes such as adjectives, pronouns and modal verbs as features. Riloff and Wiebe (2003) extracted subjective expressions from sentences using a bootstrapping pattern learning process. Yu and Hatzivassiloglou (2003) identified the polarity of opinion sentences using semantically oriented words. These techniques were applied and examined in different domains, such as customer reviews (Hu and Liu 2004) and news articles1. These researchers use lists of opinion-bearing clue words and phrases, and then apply various additional techniques and refinements. Along with many opinion researchers, we participated in a large pilot study, sponsored by NIST, which concluded that it is very difficult to define what an opinion is in general. Moreover, an expression that is considered as an opinion in one domain might no"
N06-1027,P94-1002,0,0.0219972,"ains 1339 speech acts. Table 3 gives the frequencies and percentages of speech acts found in the data set. Each SA generates feature-oriented weighted links in the threaded graph accordingly as discussed previously. Number of best answers 1 2 3 4 tions that were each correct for a specified situation. Table 4 gives the statistics of the numbers of correct messages of our gold standard. We experimented with further segmenting the messages so as to narrow down the best-answer text, under the assumption that long messages probably include some less-than-useful information. We applied TextTiling (Hearst, 1994) to segment the messages, which is the technique used by Zhou and Hovy (2005) to summarize discussions. For our corpus, though, the ratio of segments to messages was only 1.03, which indicates that our messages are relatively short and coherent, and that segmenting them would not provide additional benefits. 5.2 Baseline System Number of threads 250 56 5 3 Table 4. Gold standard length distribution. We then read each thread and choose the message that contained the best answer to the initial query as the gold standard. If there are multiple best-answer messages, all of them will be ranked as b"
N06-1027,P04-3020,0,0.0108932,"ly applied in link analysis and for web searching in the IR community. Two of the most prominent algorithms are Page-Rank (Brin and Page, 1998) and the HITS algorithm (Kleinberg, 1999). Although they were 209 initially proposed for analyzing web pages, they proved useful for investigating and ranking structured objects. Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mihalcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005). However, until now there has not been any published work on its application to human conversation analysis specifically in the format of threaded discussions. In this paper, we focus on using HITS to detect conversation focus of threaded discussions. Rhetorical Structure Theory (Mann and Thomson, 1988) based discourse processing has attracted much attention with successful applications in sentence compression and"
N06-1027,H05-1052,0,0.00468348,"ty. Two of the most prominent algorithms are Page-Rank (Brin and Page, 1998) and the HITS algorithm (Kleinberg, 1999). Although they were 209 initially proposed for analyzing web pages, they proved useful for investigating and ranking structured objects. Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mihalcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005). However, until now there has not been any published work on its application to human conversation analysis specifically in the format of threaded discussions. In this paper, we focus on using HITS to detect conversation focus of threaded discussions. Rhetorical Structure Theory (Mann and Thomson, 1988) based discourse processing has attracted much attention with successful applications in sentence compression and summarization. Most of the current work on discourse processing fo"
N06-1027,C04-1162,0,0.0110605,"ching in the IR community. Two of the most prominent algorithms are Page-Rank (Brin and Page, 1998) and the HITS algorithm (Kleinberg, 1999). Although they were 209 initially proposed for analyzing web pages, they proved useful for investigating and ranking structured objects. Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mihalcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005). However, until now there has not been any published work on its application to human conversation analysis specifically in the format of threaded discussions. In this paper, we focus on using HITS to detect conversation focus of threaded discussions. Rhetorical Structure Theory (Mann and Thomson, 1988) based discourse processing has attracted much attention with successful applications in sentence compression and summarization. Most of the current work on discou"
N06-1027,H05-1115,0,0.00418474,"nberg, 1999). Although they were 209 initially proposed for analyzing web pages, they proved useful for investigating and ranking structured objects. Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mihalcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005). However, until now there has not been any published work on its application to human conversation analysis specifically in the format of threaded discussions. In this paper, we focus on using HITS to detect conversation focus of threaded discussions. Rhetorical Structure Theory (Mann and Thomson, 1988) based discourse processing has attracted much attention with successful applications in sentence compression and summarization. Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003) or the intermediate step (Sporleder and Lapata,"
N06-1027,P04-1035,0,0.0204035,"hms are Page-Rank (Brin and Page, 1998) and the HITS algorithm (Kleinberg, 1999). Although they were 209 initially proposed for analyzing web pages, they proved useful for investigating and ranking structured objects. Inspired by the idea of graph based algorithms to collectively rank and select the best candidate, research efforts in the natural language community have applied graph-based approaches on keyword selection (Mihalcea and Tarau, 2004), text summarization (Erkan and Radev, 2004; Mihalcea, 2004), word sense disambiguation (Mihalcea et al., 2004; Mihalcea, 2005), sentiment analysis (Pang and Lee, 2004), and sentence retrieval for question answering (Otterbacher et al., 2005). However, until now there has not been any published work on its application to human conversation analysis specifically in the format of threaded discussions. In this paper, we focus on using HITS to detect conversation focus of threaded discussions. Rhetorical Structure Theory (Mann and Thomson, 1988) based discourse processing has attracted much attention with successful applications in sentence compression and summarization. Most of the current work on discourse processing focuses on sentence-level text organization"
N06-1027,N03-1030,0,0.0231142,"and sentence retrieval for question answering (Otterbacher et al., 2005). However, until now there has not been any published work on its application to human conversation analysis specifically in the format of threaded discussions. In this paper, we focus on using HITS to detect conversation focus of threaded discussions. Rhetorical Structure Theory (Mann and Thomson, 1988) based discourse processing has attracted much attention with successful applications in sentence compression and summarization. Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003) or the intermediate step (Sporleder and Lapata, 2005). Analyzing and utilizing discourse information at a higher level, e.g., at the paragraph level, still remains a challenge to the natural language community. In our work, we utilize the discourse information at a message level. Zhou and Hovy (2005) proposed summarizing threaded discussions in a similar fashion to multidocument summarization; but then their work does not take into account the relative importance of different messages in a thread. Marom and Zukerman (2005) generated help-desk responses using clustering techniques, but their c"
N06-1027,H05-1033,0,0.0152712,"rbacher et al., 2005). However, until now there has not been any published work on its application to human conversation analysis specifically in the format of threaded discussions. In this paper, we focus on using HITS to detect conversation focus of threaded discussions. Rhetorical Structure Theory (Mann and Thomson, 1988) based discourse processing has attracted much attention with successful applications in sentence compression and summarization. Most of the current work on discourse processing focuses on sentence-level text organization (Soricut and Marcu, 2003) or the intermediate step (Sporleder and Lapata, 2005). Analyzing and utilizing discourse information at a higher level, e.g., at the paragraph level, still remains a challenge to the natural language community. In our work, we utilize the discourse information at a message level. Zhou and Hovy (2005) proposed summarizing threaded discussions in a similar fashion to multidocument summarization; but then their work does not take into account the relative importance of different messages in a thread. Marom and Zukerman (2005) generated help-desk responses using clustering techniques, but their corpus is composed of only two-party, two-turn, convers"
N06-1027,W04-3252,0,\N,Missing
N06-1027,P05-1037,1,\N,Missing
N06-1057,P05-1074,0,0.1557,"ry evaluation, a collection of less-strict paraphrases must be created and a matching strategy needs to be investigated. 4 Paraphrase Acquisition Paraphrases are alternative verbalizations for conveying the same information and are required by many Natural Language Processing (NLP) applications. In particular, summary creation and 449 evaluation methods need to recognize paraphrases and their semantic equivalence. Unfortunately, we have yet to incorporate into the evaluation framework previous findings in paraphrase identification and extraction (Barzilay and McKeown, 2001; Pang et al., 2003; Bannard and Callison-Burch, 2005). 4.1 Related Work on Paraphrasing Three major approaches in paraphrase collection are manual collection (domain-specific), collection utilizing existing lexical resources (i.e. WordNet), and derivation from corpora. Hermjakob et al. (2002) view paraphrase recognition as reformulation by pattern recognition. Pang et al. (2003) use word lattices as paraphrase representations from semantically equivalent translations sets. Using parallel corpora, Barzilay and McKeown (2001) identify paraphrases from multiple translations of classical novels, where as Bannard and Callison-Burch (2005) develop a p"
N06-1057,P01-1008,0,0.0474875,"found. To include paraphrase matching in summary evaluation, a collection of less-strict paraphrases must be created and a matching strategy needs to be investigated. 4 Paraphrase Acquisition Paraphrases are alternative verbalizations for conveying the same information and are required by many Natural Language Processing (NLP) applications. In particular, summary creation and 449 evaluation methods need to recognize paraphrases and their semantic equivalence. Unfortunately, we have yet to incorporate into the evaluation framework previous findings in paraphrase identification and extraction (Barzilay and McKeown, 2001; Pang et al., 2003; Bannard and Callison-Burch, 2005). 4.1 Related Work on Paraphrasing Three major approaches in paraphrase collection are manual collection (domain-specific), collection utilizing existing lexical resources (i.e. WordNet), and derivation from corpora. Hermjakob et al. (2002) view paraphrase recognition as reformulation by pattern recognition. Pang et al. (2003) use word lattices as paraphrase representations from semantically equivalent translations sets. Using parallel corpora, Barzilay and McKeown (2001) identify paraphrases from multiple translations of classical novels,"
N06-1057,J93-2003,0,0.0063894,"Missing"
N06-1057,P02-1033,0,0.0434437,"Missing"
N06-1057,N03-1020,1,0.713763,"sed. Naturally, there is a great amount of confidence in manual evaluation since humans can infer, paraphrase, and use world knowledge to relate text units with similar meanings, but which are worded differently. Human efforts are preferred if the evaluation task is easily conducted and managed, and does not need to be performed repeatedly. However, when resources are limited, automated evaluation methods become more desirable. For years, the summarization community has been actively seeking an automatic evaluation methodology that can be readily applied to various summarization tasks. ROUGE (Lin and Hovy, 2003) has gained popularity due to its simplicity and high correlation with human judgments. Even though validated by high correlations with human judgments gathered from previous Document Understanding Conference (DUC) experiments, current automatic procedures (Lin and Hovy, 2003; Hovy et al., 2005) only employ lexical n-gram matching. The lack of support for word or phrase matching that stretches beyond strict lexical matches has limited the expressiveness and utility of these methods. We need a mechanism that supplements literal matching—i.e. paraphrase and synonym—and approximates semantic clos"
N06-1057,N04-1019,0,0.182833,"way: Section 2 introduces previous work in summarization evaluation; Section 3 describes the motivation behind this work; paraphrase acquisition is discussed in Section 4; Section 5 explains in detail our summary comparison mechanism; Section 6 validates ParaEval with human summary judgments; and we conclude and discuss future work in Section 7. 2 Previous Work There has been considerable work in both manual and automatic summarization evaluations. Three most noticeable efforts in manual evaluation are SEE (Lin and Hovy, 2001), Factoid (Van Halteren and Teufel, 2003), and the Pyramid method (Nenkova and Passonneau, 2004). SEE provides a user-friendly environment in which human assessors evaluate the quality of system-produced peer summary by comparing it to a reference summary. Summaries are represented by a list of summary units (sentences, clauses, etc.). Assessors can assign full or partial content coverage score to peer summary units in comparison to the corresponding reference summary units. Grammaticality can also be graded unit-wise. The goal of the Factoid work is to compare the information content of different summaries of the same text and determine the minimum number of summaries, which was shown t"
N06-1057,J03-1002,0,0.00220072,"atistical Machine Translation (SMT) systems analyze large quantities of bilingual parallel texts in order to learn translational alignments between pairs of words and phrases in two languages (Och and Ney, 2004). The sentencebased translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory. This alignment process results in a corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other. We ran the alignment algorithm from (Och and Ney, 2003) on a Chinese-English parallel corpus of 218 million English words. Phrase pairs are extracted by following the method described in (Och and Ney, 2004) where all contiguous phrase pairs having consistent alignments are extraction candidates. The resulting phrase table is of high quality; both the alignment models and phrase extraction methFigure 2. An example of paraphrase extraction. ods have been shown to produce very good results for SMT. Using these pairs we build paraphrase sets by joining together all English phrases with the same Chinese translation. Figure 2 shows an example word/phras"
N06-1057,J04-4002,0,0.00402602,"nard and Callison-Burch (2005) develop a probabilistic representation for paraphrases extracted from large Machine Translation (MT) data sets. 4.2 Extracting Paraphrases Our method to automatically construct a large domain-independent paraphrase collection is based on the assumption that two different English phrases of the same meaning may have the same translation in a foreign language. Phrase-based Statistical Machine Translation (SMT) systems analyze large quantities of bilingual parallel texts in order to learn translational alignments between pairs of words and phrases in two languages (Och and Ney, 2004). The sentencebased translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory. This alignment process results in a corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other. We ran the alignment algorithm from (Och and Ney, 2003) on a Chinese-English parallel corpus of 218 million English words. Phrase pairs are extracted by following the method described in (Och and Ney, 2004) where all contiguous phrase pairs having"
N06-1057,N03-1024,0,0.16803,"Missing"
N06-1057,W03-0508,0,0.0210048,"Missing"
N06-1057,P02-1040,0,\N,Missing
N06-2015,I05-1081,1,0.260652,"Missing"
N06-2015,N06-1024,1,0.181596,"ed, allowing the entity mentions that are propositional arguments to be resolved in context. Annotation will cover multiple languages (English, Chinese, and Arabic) and multiple genres (newswire, broadcast news, news groups, weblogs, etc.), to create a resource that is broadly applicable. 2 Treebanking The Penn Treebank (Marcus et al., 1993) is annotated with information to make predicate-argument structure easy to decode, including function tags and markers of “empty” categories that represent displaced constituents. To expedite later stages of annotation, we have developed a parsing system (Gabbard et al., 2006) that recovers both of these latter annotations, the first we know of. A firststage parser matches the Collins (2003) parser on which it is based on the Parseval metric, while simultaneously achieving near state-of-the-art performance on recovering function tags (F-measure 89.0). A second stage, a seven stage pipeline of maximum entropy learners and voted perceptrons, achieves state-of-the-art performance (F-measure 74.7) on the recovery of empty categories by combining a linguistically-informed architecture and a rich feature set with the power of modern machine learning methods. * This work"
N06-2015,J93-2004,1,0.0841798,"with parse (TreeBank) and propositional (PropBank) structures, which provide normalization over predicates and their arguments. Word sense ambiguities are then resolved, with each word sense also linked to the appropriate node in the Omega ontology. Coreference is also annotated, allowing the entity mentions that are propositional arguments to be resolved in context. Annotation will cover multiple languages (English, Chinese, and Arabic) and multiple genres (newswire, broadcast news, news groups, weblogs, etc.), to create a resource that is broadly applicable. 2 Treebanking The Penn Treebank (Marcus et al., 1993) is annotated with information to make predicate-argument structure easy to decode, including function tags and markers of “empty” categories that represent displaced constituents. To expedite later stages of annotation, we have developed a parsing system (Gabbard et al., 2006) that recovers both of these latter annotations, the first we know of. A firststage parser matches the Collins (2003) parser on which it is based on the Parseval metric, while simultaneously achieving near state-of-the-art performance on recovering function tags (F-measure 89.0). A second stage, a seven stage pipeline of"
N06-2015,J05-1004,1,0.144555,"Missing"
N06-2015,I05-7009,1,0.218789,"e information extraction, summarization and machine translation. The subtle finegrained sense distinctions in WordNet have not lent themselves to high agreement between human annotators or high automatic tagging performance. Building on results in grouping fine-grained WordNet senses into more coarse-grained senses that led to improved inter-annotator agreement (ITA) and system performance (Palmer et al., 2004; Palmer et al., 2006), we have developed a process for rapid sense inventory creation and annotation that includes critical links between the grouped word senses and the Omega ontology (Philpot et al., 2005; see Section 5 below). This process is based on recognizing that sense distinctions can be represented by linguists in an hierarchical structure, similar to a decision tree, that is rooted in very coarse-grained distinctions which become increasingly fine-grained until reaching WordNet senses at the leaves. Sets of senses under specific nodes of the tree are grouped together into single entries, along with the syntacAnnotate test (2 people) not OK Results: agreement and confusion matrix OK Figure 1. Annotation Procedure As part of OntoNotes we are annotating the most frequent noun and verb se"
N06-2015,P98-1013,0,0.0785645,"her phenomena, including temporal and spatial relations, numerical expressions, deixis, etc. One of the principal aims of OntoNotes is to enable automated semantic analysis. The best current algorithm for semantic role labeling for PropBank style annotation (Pradhan et al., 2005) achieves an F-measure of 81.0 using an SVM. OntoNotes will provide a large amount of new training data for similar efforts. Existing work in the same realm falls into two classes: the development of resources for specific phenomena or the annotation of corpora. An example of the former is Berkeley’s FrameNet project (Baker et al., 1998), which produces rich semantic frames, annotating a set of examples for each predicator (including verbs, nouns and adjectives), and describing the network of relations among the semantic frames. An example of the latter type is the Salsa project (Burchardt et al., 2004), which produced a German lexicon based on the FrameNet semantic frames and annotated a large German newswire corpus. A second example, the Prague Dependency Treebank (Hajic et al., 2001), has annotated a large Czech corpus with several levels of (tectogrammatical) representation, including parts of speech, syntax, and topic/fo"
N06-2015,reeder-etal-2004-interlingual,1,0.282926,"Missing"
N06-2015,W04-2705,0,\N,Missing
N06-2015,W04-2704,1,\N,Missing
N06-2015,P05-1072,0,\N,Missing
N06-2015,C98-1013,0,\N,Missing
N07-1071,P01-1008,0,0.0721351,"automatically. Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither provides for the extracted in1 Some systems refer to inferences they extract as entailments; the two terms are sometimes used interchangeably. 565 ference rules to hold or fail based on SPs. Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences. Rather than examine the role of SPs in inferences, they use SPs of a particular type to derive inferences. For instance the preference of win for the subject player, a nomin"
N07-1071,P98-1013,0,0.0143219,"Light and Greiff 2002). Rather than venture into learning inferential SPs, much previous work has focused on learning SPs for simpler structures. Resnik (1996), the seminal paper on this topic, introduced a statistical model for learning SPs for predicates using an unsupervised method. Learning SPs often relies on an underlying set of semantic classes, as in both Resnik’s and our approach. Semantic classes can be specified manually or derived automatically. Manual collections of semantic classes include the hierarchies of WordNet (Fellbaum 1998), Levin verb classes (Levin 1993), and FrameNet (Baker et al. 1998). Automatic derivation of semantic classes can take a variety of approaches, but often uses corpus methods and the Distributional Hypothesis (Harris 1964) to automatically cluster similar entities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither p"
N07-1071,P06-1114,0,0.0274597,"tors for murder and conspiracy in the Oklahoma City bombing. and an inference rule such as: X is charged by Y ⇒ Y announced the arrest of X (1) Using this rule, we can infer that “federal prosecutors announced the arrest of Terry Nichols”. However, given the sentence: Fraud was suspected when accounts were charged by CCM telemarketers without obtaining consumer authorization. Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-intensiveness of generating exha"
N07-1071,P93-1016,0,0.279352,"Missing"
N07-1071,N03-1022,0,0.0131744,"ged by federal prosecutors for murder and conspiracy in the Oklahoma City bombing. and an inference rule such as: X is charged by Y ⇒ Y announced the arrest of X (1) Using this rule, we can infer that “federal prosecutors announced the arrest of Terry Nichols”. However, given the sentence: Fraud was suspected when accounts were charged by CCM telemarketers without obtaining consumer authorization. Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-inten"
N07-1071,E06-1052,0,0.0362033,"Missing"
N07-1071,W04-3206,1,0.918889,"charged by Y ⇒ Y announced the arrest of X (1) Using this rule, we can infer that “federal prosecutors announced the arrest of Terry Nichols”. However, given the sentence: Fraud was suspected when accounts were charged by CCM telemarketers without obtaining consumer authorization. Introduction Semantic inference is a key component for advanced natural language understanding. Several important applications are already relying heavily on inference, including question answering (Moldovan et al. 2003; Harabagiu and Hickl 2006), information extraction (Romano et al. 2006), and textual entailment (Szpektor et al. 2004). In response, several researchers have created resources for enabling semantic inference. Among manual resources used for this task are WordNet (Fellbaum 1998) and Cyc (Lenat 1995). Although important and useful, these resources primarily contain prescriptive inference rules such as “X divorces Y ⇒ X married Y”. In practical NLP applications, however, plausible inference rules such as “X married Y” ⇒ “X dated Y” are very useful. This, along with the difficulty and labor-intensiveness of generating exhaustive lists of rules, has led rethe plausible inference rule (1) would incorrectly infer th"
N07-1071,P06-1107,0,0.138975,"tities into classes, e.g. CBC (Pantel and Lin 2002). In this paper, we experiment with two sets of semantic classes, one from WordNet and one from CBC. Another thread related to our work includes extracting from text corpora paraphrases (Barzilay and McKeown 2001) and inference rules, e.g. TEASE1 (Szpektor et al. 2004) and DIRT (Lin and Pantel 2001). While these systems differ in their approaches, neither provides for the extracted in1 Some systems refer to inferences they extract as entailments; the two terms are sometimes used interchangeably. 565 ference rules to hold or fail based on SPs. Zanzotto et al. (2006) recently explored a different interplay between SPs and inferences. Rather than examine the role of SPs in inferences, they use SPs of a particular type to derive inferences. For instance the preference of win for the subject player, a nominalization of play, is used to derive that “win ⇒ play”. Our work can be viewed as complementary to the work on extracting semantic inferences and paraphrases, since we seek to refine when a given inference applies, filtering out incorrect inferences. 3 Selectional Preference Models The aim of this paper is to learn inferential selectional preferences for f"
N07-1071,C98-1013,0,\N,Missing
N07-2055,N03-1020,1,0.477478,"lace. This human involvement allows a much smaller subset of phrase segments, resulting from word enumeration, to be matched in summary comparisons. Without the human-created nuggets, text comparison falls back to its dependency on n-grams. Similarly, in question-answering (QA) evaluations, gold-standard answers use manually created nuggets and compare them against system-produced answers broken down into n-gram pieces, as shown in POURPRE (Lin and Demner-Fushman, 2005) and NUGGETEER (Marton and Radul, 2006). A serious problem in manual nugget creation is the inconsistency in human decisions (Lin and Hovy, 2003). The same nugget will not be marked consistently with the same words when sentences containing multiple instances of it are presented to human annotators. And if the annotation is performed over an extended period of time, the consistency is even lower. In recent exercises of the PYRAMID evaluation, inconsistent nuggets are flagged by a tracking program and returned back to the annotators, and resolved manually. Given these issues, we address two questions in this paper: First, how do we define nuggets so that they are consistent in definition? Secondly, how do 217 Proceedings of NAACL HLT 20"
N07-2055,H05-1117,0,0.0196562,"ible nuggets. Their automation process still requires nuggets to be manually created a priori for reference summaries before any summary comparison takes place. This human involvement allows a much smaller subset of phrase segments, resulting from word enumeration, to be matched in summary comparisons. Without the human-created nuggets, text comparison falls back to its dependency on n-grams. Similarly, in question-answering (QA) evaluations, gold-standard answers use manually created nuggets and compare them against system-produced answers broken down into n-gram pieces, as shown in POURPRE (Lin and Demner-Fushman, 2005) and NUGGETEER (Marton and Radul, 2006). A serious problem in manual nugget creation is the inconsistency in human decisions (Lin and Hovy, 2003). The same nugget will not be marked consistently with the same words when sentences containing multiple instances of it are presented to human annotators. And if the annotation is performed over an extended period of time, the consistency is even lower. In recent exercises of the PYRAMID evaluation, inconsistent nuggets are flagged by a tracking program and returned back to the annotators, and resolved manually. Given these issues, we address two que"
N07-2055,N04-1019,0,0.395886,"redundancy and/or relevancy judgments, without a precise and consistent breakdown of nuggets we can only rely on rudimentary n-gram segmentations of sentences to form nuggets and perform subsequent n-gram-wise text comparison. This is not satisfactory for a variety of reasons. For example, one n-gram window may contain several separate pieces of information, while another of the same length may not contain even one complete piece of information. Previous work shows that humans can create nuggets in a relatively straightforward fashion. In the PYRAMID scheme for manual evaluation of summaries (Nenkova and Passonneau, 2004), machine-generated summaries were compared with human-written ones at the nugget level. However, automatic creation of the nuggets is not trivial. Hamly et al. (2005) explore the enumeration and combination of all words in a sentence to create the set of all possible nuggets. Their automation process still requires nuggets to be manually created a priori for reference summaries before any summary comparison takes place. This human involvement allows a much smaller subset of phrase segments, resulting from word enumeration, to be matched in summary comparisons. Without the human-created nugget"
N07-2055,N06-1048,0,\N,Missing
N07-2055,J03-4003,0,\N,Missing
N10-1087,P07-1030,0,0.0186868,"Missing"
N10-1087,C96-1079,0,0.01598,"Missing"
N10-1087,C92-2082,0,0.177164,"Missing"
N10-1087,W09-1703,0,0.064246,"Missing"
N10-1087,P08-1119,1,0.423713,"Missing"
N10-1087,P08-1003,0,0.0363013,"Missing"
N10-1087,D08-1061,0,0.0387437,"Missing"
N10-1087,D09-1098,0,\N,Missing
N10-1087,P08-1000,0,\N,Missing
N10-2002,J97-1003,0,\N,Missing
N10-2002,P02-1058,1,\N,Missing
N10-2002,C00-1072,1,\N,Missing
N12-1083,N10-1021,0,0.0414233,"Missing"
N13-1132,W10-0701,0,0.033793,", both for predicted label accuracy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equally. Unfortunately, it is easy fo"
N13-1132,W10-1703,0,0.0107417,"ments over standard baselines, both for predicted label accuracy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equa"
N13-1132,W02-0102,0,0.0301667,"i=1 P (Ti ) · M Y P (Sij ; θj ) · P (Aij |Sij , Ti ; ξj ) i j=1 where A is the matrix of annotations, S is the matrix of competence indicators, and T is the vector of true labels. We maximize the marginal data likelihood using Expectation Maximization (EM) (Dempster et al., 1977), which has successfully been applied to similar problems (Dawid and Skene, 1979). We initialize EM randomly and run for 50 iterations. We perform 100 random restarts, and keep the model with the best marginal data likelihood. We smooth the M-step by adding a fixed value δ to the fractional counts before normalizing (Eisner, 2002). We find that smoothing improves accuracy, but, overall, learning is robust to varying δ, and set δ = num0.1 labels . 1122 3.1 Natural Data In order to evaluate our model, we use the datasets from (Snow et al., 2008) that use discrete label values (some tasks used continuous values, which we currently do not model). Since they compared AMT annotations to experts, gold annotations exist for these sets. We can thus evaluate the accuracy of the model as well as the proficiency of each annotator. We show results for word sense disambiguation (WSD: 177 items, 34 annotators), recognizing textual en"
N13-1132,W10-0702,0,0.17971,"cy and trustworthiness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equally. Unfortunately, it is easy for majority voting"
N13-1132,D07-1031,0,0.00707372,"Variational-Bayes (VB) training with symmetric Beta priors on θj and symmetric Dirichlet priors on the strategy parameters, ξj . Setting the shape parameters of the Beta distribution to 0.5 favors the extremes of the distribution, i.e., either an annotator tried to get the right answer, or simply did not care, but (almost) nobody tried “a little”. With VB training, we observe improved correlations over all test sets with no loss in accuracy. The hyper-parameters of the Dirichlet distribution on ξj were clamped to 10.0 for all our experiments with VB training. Our implementation is similar to Johnson (2007), which the reader can refer to for details. 3 Experiments We evaluate our method on existing annotated datasets from various AMT tasks. However, we also want to ensure that our model can handle adversarial conditions. Since we have no control over the factors in existing datasets, we create synthetic data for this purpose. P (A; θ, ξ) = N XhY T,S i=1 P (Ti ) · M Y P (Sij ; θj ) · P (Aij |Sij , Ti ; ξj ) i j=1 where A is the matrix of annotations, S is the matrix of competence indicators, and T is the vector of true labels. We maximize the marginal data likelihood using Expectation Maximizatio"
N13-1132,D08-1027,0,0.767307,"Missing"
N13-1132,P10-1070,1,0.275755,"78 0.70 0.76 0.87 0.91 Temporal 0.73 0.80 0.73 0.88 0.90 WSD 0.81 0.13 0.81 0.44 0.90 Table 1: Correlation with annotator proficiency: Pearson ρ of different methods for various data sets. MACE-VB’s trustworthiness parameter (trained with Variational Bayes with α = β = 0.5) correlates best with true annotator proficiency. It is natural to apply some form of weighting. One approach is to assume that reliable annotators agree more with others than random annotators. Inter-annotator agreement is thus a good candidate to weigh the answers. There are various measures for inter-annotator agreement. Tratz and Hovy (2010) compute the average agreement of each annotator and use it as a weight to identify reliable ones. Raw agreement can be directly computed from the data. It is related to majority voting, since it will produce high scores for all members of the majority class. Raw agreement is thus a very simple measure. In contrast, Cohen’s κ corrects the agreement between two annotators for chance agreement. It is widely used for inter-annotator agreement in annotation tasks. We also compute the κ values for each pair of annotators, and average them for each annotator (similar to the approach in Tratz and Hov"
N13-1132,P11-1122,0,0.0091563,"ness estimates. The latter can be further improved by introducing a prior on model parameters and using Variational Bayes inference. Additionally, we can achieve even higher accuracy by focusing on the instances our model is most confident in (trading in some recall), and by incorporating annotated control instances. Our system, MACE (Multi-Annotator Competence Estimation), is available for download1 . 1 Introduction Amazon’s MechanicalTurk (AMT) is frequently used to evaluate experiments and annotate data in NLP (Callison-Burch et al., 2010; Callison-Burch and Dredze, 2010; Jha et al., 2010; Zaidan and Callison-Burch, 2011). However, some turkers try to maximize their pay by supplying quick answers that have nothing to do with the correct label. We refer to 1 Available under http://www.isi.edu/ publications/licensed-sw/mace/index.html this type of annotator as a spammer. In order to mitigate the effect of spammers, researchers typically collect multiple annotations of the same instance so that they can, later, use de-noising methods to infer the best label. The simplest approach is majority voting, which weights all answers equally. Unfortunately, it is easy for majority voting to go wrong. A common and simple s"
N13-1132,P10-5004,1,\N,Missing
N15-1070,P11-1061,0,0.0146357,"graph, however their approach does not take distributional information from VSMs into account. Thus, to the best of our knowledge, our work presents the first attempt at producing sense grounded VSMs that are symbolically tied to lexical ontologies. From a modelling point of view, it is also the first to outline a unified, principled and extensible framework that effectively combines the symbolic and distributional paradigms of semantics. Both our models leverage the graph structure of ontologies to effectively ground the senses of a VSM. This ties into previous research (Das and Smith, 2011; Das and Petrov, 2011) that propagates information through a factor graph to perform tasks such as frame-semantic parsing and POStagging across languages. More generally, this approach can be viewed from the perspective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing"
N15-1070,P11-1144,0,0.0122982,"walks on the Wordnet graph, however their approach does not take distributional information from VSMs into account. Thus, to the best of our knowledge, our work presents the first attempt at producing sense grounded VSMs that are symbolically tied to lexical ontologies. From a modelling point of view, it is also the first to outline a unified, principled and extensible framework that effectively combines the symbolic and distributional paradigms of semantics. Both our models leverage the graph structure of ontologies to effectively ground the senses of a VSM. This ties into previous research (Das and Smith, 2011; Das and Petrov, 2011) that propagates information through a factor graph to perform tasks such as frame-semantic parsing and POStagging across languages. More generally, this approach can be viewed from the perspective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories"
N15-1070,D10-1113,0,0.00821942,"oothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-processing step while the second provides a framework to integrate ontological information with existing MLEbased predictive models. We presented an evaluation of 3 semantic tasks on 7 datasets. Our results show"
N15-1070,D08-1094,0,0.0228482,"rame-semantic parsing and POStagging across languages. More generally, this approach can be viewed from the perspective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-"
N15-1070,P10-2017,0,0.00736496,"pective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-processing step while the second provides a framework to integrate ontological information with existing MLEbase"
N15-1070,N15-1184,1,0.85529,"Missing"
N15-1070,N13-1092,0,0.0538894,"Missing"
N15-1070,C14-1048,0,0.0660298,"and help to distinguish the vectors from their single sense embeddings. 4 Related Work Since Reisinger and Mooney (2010) first proposed a simple context clustering technique to generate multi-prototype VSMs, a number of related efforts have worked on adaptations and improvements relying on the same clustering principle. Huang et al. (2012) train their vectors with a neural network and additionally take global context into account. Neelakantan et al. (2014) extend the popular skip-gram model (Mikolov et al., 2013a) in a non-parametric fashion to allow for different number of senses for words. Guo et al. (2014) exploit bilingual alignments to perform better context clustering during training. Tian et al. (2014) propose a probabilistic extension to skip-gram that treats the different prototypes as latent variables. This is similar to our second EM training framework, and turns out to be a special case of our general model. In all these papers, however, the multiple senses remain abstract and are not grounded in an ontology. Conceptually, our work is also similar to Yu and Dredze (2014) and Faruqui et al. (2014), who treat lexicons such as the paraphrase database (PPDB) (Ganitkevitch et al., 2013) or"
N15-1070,P12-1092,0,0.940798,"eanwhile, lexical ontologies, such as WordNet (Miller, 1995) specifically catalog sense inventories and provide typologies that link these senses to one another. These hand-curated ontologies provide a complementary source of information to distributional statistics. Recent research tries to leverage this information to train better VSMs (Yu and Dredze, 2014; Faruqui et al., 2014), but does not tackle the problem of polysemy. Parallely, work on polysemy for VSMs revolves primarily around techniques that cluster contexts to distinguish between different word senses (Reisinger and Mooney, 2010; Huang et al., 2012), but does not integrate ontologies in any way. In this paper we present two novel approaches to integrating ontological and distributional sources of information. Our focus is on allowing already existing, proven techniques to be adapted to produce ontologically grounded word sense embeddings. Our first technique is applicable to any sense-agnostic 683 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 683–693, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics VSM as a post-processing step that perfor"
N15-1070,D07-1031,0,0.00824306,"her than using the full posterior over senses. Also, given that the structured regularizer pΩ (θ) is essentially the retrofitting objective in equation 1, we run retrofitting periodically every k words (with α = 0 in equation 2) instead of lazy updates.2 The following decision rule is used in the “hard” E-step: (t) sij = arg max p(ci |sij ; θ(t) )πij sij (5) In the M-step we use Variational Bayes to update Π with:    (0) exp ψ c˜(wi , sij ) + λπij (t+1) πij ∝ (6) exp (ψ (˜ c(wi ) + λ)) where c˜(·) is the online expected count and ψ(·) is the digamma function. This approach is motivated by Johnson (2007) who found that naive EM leads to poor results, while Variational Bayes is consistently better and promotes faster convergence of the likelihood function. To update the parameters U and V , we use negative sampling (Mikolov et al., 2013a) which is an efficient approximation to the original skip-gram objective. Negative sampling attempts to distinguish between true word pairs in the data, relative to noise. Stochastic gradient descent on the following equation is used to update the model pa2 We find this gives slightly better performance. rameters U and V : L = log σ(ui · vij ) + + X m X log σ("
N15-1070,N13-1090,0,0.501745,"perform other models we compare against. 1 Introduction Vector space models (VSMs) of word meaning play a central role in computational semantics. These represent meanings of words as contextual feature vectors in a high-dimensional space (Deerwester et al., 1990) or some embedding thereof (Collobert and Weston, 2008) and are learned from unannotated corpora. Word vectors in these continuous space representations can be used for meaningful semantic operations such as computing word similarity (Turney, 2006), performing analogical reasoning (Turney, 2013) and discovering lexical relationships (Mikolov et al., 2013b). They have also proved useful in downstream NLP applications such as information retrieval (Manning et al., 2008) and question answering (Tellex et al., 2003), among others. However, VSMs remain flawed because they assign a single vector to every word, thus ignoring the possibility that words may have more than one meaning. For example, the word “bank” can either denote a financial institution or the shore of a river. The ability to model multiple meanings is an important component of any NLP system, given how common polysemy is in language. The lack of sense annotated corpora large enough"
N15-1070,P08-1028,0,0.0298665,"to perform tasks such as frame-semantic parsing and POStagging across languages. More generally, this approach can be viewed from the perspective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM a"
N15-1070,D14-1113,0,0.421675,"ors (i.e. synonyms, hypernyms and hyponyms), tended to have more clearly sense specific vectors. This is expected, since it is these neighborhoods that disambiguate and help to distinguish the vectors from their single sense embeddings. 4 Related Work Since Reisinger and Mooney (2010) first proposed a simple context clustering technique to generate multi-prototype VSMs, a number of related efforts have worked on adaptations and improvements relying on the same clustering principle. Huang et al. (2012) train their vectors with a neural network and additionally take global context into account. Neelakantan et al. (2014) extend the popular skip-gram model (Mikolov et al., 2013a) in a non-parametric fashion to allow for different number of senses for words. Guo et al. (2014) exploit bilingual alignments to perform better context clustering during training. Tian et al. (2014) propose a probabilistic extension to skip-gram that treats the different prototypes as latent variables. This is similar to our second EM training framework, and turns out to be a special case of our general model. In all these papers, however, the multiple senses remain abstract and are not grounded in an ontology. Conceptually, our work"
N15-1070,N09-5005,0,0.0139636,"cument level to further enhance the VSM. We distinguish three variants: the original single-sense vectors (SINGLE), a multi-prototype variant (MULTI), – both are available as pre-trained vectors for download4 – and a sense-based version obtained by running retrofitting on the original vectors (RETRO). Skip-gram Vectors (SG) (Mikolov et al., 2013a): We use the word vector tool Word2Vec5 to train skip-gram vectors. We define 6 variants: a single-sense version (SINGLE), two multi-sense variants that were trained by first sense disambiguating the entire corpus using WSD tools, – one unsupervised (Pedersen and Kolhatkar, 2009) (WSD) and the other supervised (Zhong and Ng, 2010) (IMS) – a retrofitted version obtained from the singlesense vectors (RETRO), an EM implementation of the skip-gram model with the structured regularizer as described in section 2.2 (EM+RETRO), and the same EM technique but ignoring the ontology (EM). All models were trained on publicly available WMT20116 English monolingual data. This corpus of 355 million words, although adequate in size, is smaller than typically used billion word corpora. We use this corpus because the WSD baseline involves preprocessing the corpus with sense disambiguati"
N15-1070,P13-1132,0,0.0148761,"probabilistic extension to skip-gram that treats the different prototypes as latent variables. This is similar to our second EM training framework, and turns out to be a special case of our general model. In all these papers, however, the multiple senses remain abstract and are not grounded in an ontology. Conceptually, our work is also similar to Yu and Dredze (2014) and Faruqui et al. (2014), who treat lexicons such as the paraphrase database (PPDB) (Ganitkevitch et al., 2013) or WordNet (Miller, 1995) as an auxiliary thesaurus to improve VSMs. However, they do not model senses in any way. Pilehvar et al. (2013) do model senses from an ontology by performing random-walks on the Wordnet graph, however their approach does not take distributional information from VSMs into account. Thus, to the best of our knowledge, our work presents the first attempt at producing sense grounded VSMs that are symbolically tied to lexical ontologies. From a modelling point of view, it is also the first to outline a unified, principled and extensible framework that effectively combines the symbolic and distributional paradigms of semantics. Both our models leverage the graph structure of ontologies to effectively ground"
N15-1070,I11-1079,0,0.0102795,"vised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-processing step while the second provides a framework to integrate ontological information with existing MLEbased predictive models."
N15-1070,N10-1013,0,0.762795,"ndling polysemy difficult. Meanwhile, lexical ontologies, such as WordNet (Miller, 1995) specifically catalog sense inventories and provide typologies that link these senses to one another. These hand-curated ontologies provide a complementary source of information to distributional statistics. Recent research tries to leverage this information to train better VSMs (Yu and Dredze, 2014; Faruqui et al., 2014), but does not tackle the problem of polysemy. Parallely, work on polysemy for VSMs revolves primarily around techniques that cluster contexts to distinguish between different word senses (Reisinger and Mooney, 2010; Huang et al., 2012), but does not integrate ontologies in any way. In this paper we present two novel approaches to integrating ontological and distributional sources of information. Our focus is on allowing already existing, proven techniques to be adapted to produce ontologically grounded word sense embeddings. Our first technique is applicable to any sense-agnostic 683 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 683–693, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics VSM as a post-proces"
N15-1070,D11-1097,0,0.0117779,"rduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-processing step while the second provides a framework to integrate ontological information with existing MLEbased predictive models. We presented an evaluation of 3 semantic tasks on 7 datasets. Our results show that our proposed methods are"
N15-1070,I11-1127,0,0.0110009,"g and POStagging across languages. More generally, this approach can be viewed from the perspective of semisupervised learning, with an optimization over a graph loss function defined on smoothness properties (Corduneanu and Jaakkola, 2002; Zhu et al., 2003; Subramanya and Bilmes, 2009). Related to the problem of polysemy is the issue of different shades of meaning a word assumes based on context. The space of research on this topic can be divided into three broad categories: models for computing contextual lexical semantics based on composition (Mitchell and Lapata, 2008; Erk and Padó, 2008; Thater et al., 2011), models that use fuzzy exemplar-based contexts without composing them (Erk and Padó, 2010; Reddy et al., 2011), and 691 models that propose latent variable techniques (Dinu and Lapata, 2010; Séaghdha and Korhonen, 2011; Van de Cruys et al., 2011). Our work, which tackles the stronger form of lexical ambiguity in polysemy falls into the latter two of three categories. 5 Conclusion and Future Work We have presented two general and flexible approaches to producing sense-specific VSMs grounded in an ontology. The first technique is applicable to any VSM as an efficient post-processing step while"
N15-1070,C14-1016,0,0.453061,"er and Mooney (2010) first proposed a simple context clustering technique to generate multi-prototype VSMs, a number of related efforts have worked on adaptations and improvements relying on the same clustering principle. Huang et al. (2012) train their vectors with a neural network and additionally take global context into account. Neelakantan et al. (2014) extend the popular skip-gram model (Mikolov et al., 2013a) in a non-parametric fashion to allow for different number of senses for words. Guo et al. (2014) exploit bilingual alignments to perform better context clustering during training. Tian et al. (2014) propose a probabilistic extension to skip-gram that treats the different prototypes as latent variables. This is similar to our second EM training framework, and turns out to be a special case of our general model. In all these papers, however, the multiple senses remain abstract and are not grounded in an ontology. Conceptually, our work is also similar to Yu and Dredze (2014) and Faruqui et al. (2014), who treat lexicons such as the paraphrase database (PPDB) (Ganitkevitch et al., 2013) or WordNet (Miller, 1995) as an auxiliary thesaurus to improve VSMs. However, they do not model senses in"
N15-1070,J06-3003,0,0.0185111,"oth the ontology and distributional statistics. Moreover, in most cases our sense-specific models outperform other models we compare against. 1 Introduction Vector space models (VSMs) of word meaning play a central role in computational semantics. These represent meanings of words as contextual feature vectors in a high-dimensional space (Deerwester et al., 1990) or some embedding thereof (Collobert and Weston, 2008) and are learned from unannotated corpora. Word vectors in these continuous space representations can be used for meaningful semantic operations such as computing word similarity (Turney, 2006), performing analogical reasoning (Turney, 2013) and discovering lexical relationships (Mikolov et al., 2013b). They have also proved useful in downstream NLP applications such as information retrieval (Manning et al., 2008) and question answering (Tellex et al., 2003), among others. However, VSMs remain flawed because they assign a single vector to every word, thus ignoring the possibility that words may have more than one meaning. For example, the word “bank” can either denote a financial institution or the shore of a river. The ability to model multiple meanings is an important component of"
N15-1070,Q13-1029,0,0.0113702,"Moreover, in most cases our sense-specific models outperform other models we compare against. 1 Introduction Vector space models (VSMs) of word meaning play a central role in computational semantics. These represent meanings of words as contextual feature vectors in a high-dimensional space (Deerwester et al., 1990) or some embedding thereof (Collobert and Weston, 2008) and are learned from unannotated corpora. Word vectors in these continuous space representations can be used for meaningful semantic operations such as computing word similarity (Turney, 2006), performing analogical reasoning (Turney, 2013) and discovering lexical relationships (Mikolov et al., 2013b). They have also proved useful in downstream NLP applications such as information retrieval (Manning et al., 2008) and question answering (Tellex et al., 2003), among others. However, VSMs remain flawed because they assign a single vector to every word, thus ignoring the possibility that words may have more than one meaning. For example, the word “bank” can either denote a financial institution or the shore of a river. The ability to model multiple meanings is an important component of any NLP system, given how common polysemy is in"
N15-1070,D11-1094,0,0.0272225,"Missing"
N15-1070,P14-2089,0,0.524461,"component of any NLP system, given how common polysemy is in language. The lack of sense annotated corpora large enough to robustly train VSMs, and the absence of fast, high quality word sense disambiguation (WSD) systems makes handling polysemy difficult. Meanwhile, lexical ontologies, such as WordNet (Miller, 1995) specifically catalog sense inventories and provide typologies that link these senses to one another. These hand-curated ontologies provide a complementary source of information to distributional statistics. Recent research tries to leverage this information to train better VSMs (Yu and Dredze, 2014; Faruqui et al., 2014), but does not tackle the problem of polysemy. Parallely, work on polysemy for VSMs revolves primarily around techniques that cluster contexts to distinguish between different word senses (Reisinger and Mooney, 2010; Huang et al., 2012), but does not integrate ontologies in any way. In this paper we present two novel approaches to integrating ontological and distributional sources of information. Our focus is on allowing already existing, proven techniques to be adapted to produce ontologically grounded word sense embeddings. Our first technique is applicable to any sens"
N15-1070,P10-4014,0,0.138375,"variants: the original single-sense vectors (SINGLE), a multi-prototype variant (MULTI), – both are available as pre-trained vectors for download4 – and a sense-based version obtained by running retrofitting on the original vectors (RETRO). Skip-gram Vectors (SG) (Mikolov et al., 2013a): We use the word vector tool Word2Vec5 to train skip-gram vectors. We define 6 variants: a single-sense version (SINGLE), two multi-sense variants that were trained by first sense disambiguating the entire corpus using WSD tools, – one unsupervised (Pedersen and Kolhatkar, 2009) (WSD) and the other supervised (Zhong and Ng, 2010) (IMS) – a retrofitted version obtained from the singlesense vectors (RETRO), an EM implementation of the skip-gram model with the structured regularizer as described in section 2.2 (EM+RETRO), and the same EM technique but ignoring the ontology (EM). All models were trained on publicly available WMT20116 English monolingual data. This corpus of 355 million words, although adequate in size, is smaller than typically used billion word corpora. We use this corpus because the WSD baseline involves preprocessing the corpus with sense disambiguation, which is slow enough that running it on corpora"
N15-1184,N09-1003,0,0.562483,"Missing"
N15-1184,N09-1014,0,0.00466202,"nalysis (Chang et al., 2013). The approach we propose is conceptually similar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector tr"
N15-1184,P98-1013,0,0.739801,"l., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-b"
N15-1184,P12-1015,0,0.139533,"cts of the representations along with an extrinsic sentiment analysis task. Word Similarity. We evaluate our word representations on a variety of different benchmarks that have been widely used to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001) containing 353 pairs of English words that have been assigned similarity ratings by humans. The second benchmark is the RG-65 (Rubenstein and Goodenough, 1965) dataset that contain 65 pairs of nouns. Since the commonly used word similarity datasets contain a small number of word pairs we also use the MEN dataset (Bruni et al., 2012) of 3,000 word pairs sampled from words that occur at least 700 times in a large web corpus. We calculate cosine similarity between the vectors of two words forming a test item, and report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Syntactic Relations (SYN-REL). Mikolov et al. (2013b) present a syntactic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common syntactic relation. For example, given walking and walked, the words are differently"
N15-1184,D13-1167,0,0.0169875,"luable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors (§2). This allows retrofitting to be used on pre-trained word vectors obtained using any ve"
N15-1184,P11-1061,0,0.0334295,"se to the observed value qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agnostic to the original v"
N15-1184,P11-1144,1,0.318338,"ue qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agnostic to the original vector training model o"
N15-1184,E14-1049,1,0.393831,"e use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3 Multilingual Vectors (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis (CCA) on pairs of vectors for words that align in parallel corpora. The monolingual vectors were trained on WMT-2011 news corpus for English, French, German and Spanish. We use the Enligsh word vectors projected in the common English–German space. The monolingual English WMT corpus had 360 million words and the trained vectors are of length 512.4 4 Semantic Lexicons We use three different semantic lexicons to evaluate their utility in improving the word vectors. We include both"
N15-1184,N13-1092,0,0.314995,"Missing"
N15-1184,W06-3808,0,0.0111527,"elief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector training method. Our experiments explored the method’s performance across tasks, semantic lexicons, and languages and showed that it outperforms existing alternatives. The retrofitting tool is available at: https:// g"
N15-1184,D14-1012,0,0.00727559,"cal semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations"
N15-1184,I05-1067,0,0.0116987,"Spanish. We used the Universal WordNet (de Melo and Weikum, 2009), an automatically constructed multilingual lexical knowledge base based on WordNet.12 It contains words connected via different lexical relations to other words both within and across languages. We construct separate graphs for different languages (i.e., only linking words to other words in the same language) and apply retrofitting to each. Since not many word similarity evaluation benchmarks are available for languages other than English, we tested our baseline and improved vectors on one benchmark per language. We used RG-65 (Gurevych, 2005), RG-65 (Joubarne and Inkpen, 2011) and MC-30 (Hassan and Mihalcea, 2009) for German, French and Spanish, respectively.13 We trained SG vectors for each language of length 300 on a corpus of 1 billion tokens, each extracted from Wikipedia, and evaluate them on word similarity on the benchmarks before and after retrofitting. Table 5 shows that we obtain high improvements which strongly indicates that our method generalizes across these languages. 7 Further Analysis Retrofitting vs. vector length. With more dimensions, word vectors might be able to capture higher orders of semantic information a"
N15-1184,D09-1124,0,0.0382359,"09), an automatically constructed multilingual lexical knowledge base based on WordNet.12 It contains words connected via different lexical relations to other words both within and across languages. We construct separate graphs for different languages (i.e., only linking words to other words in the same language) and apply retrofitting to each. Since not many word similarity evaluation benchmarks are available for languages other than English, we tested our baseline and improved vectors on one benchmark per language. We used RG-65 (Gurevych, 2005), RG-65 (Joubarne and Inkpen, 2011) and MC-30 (Hassan and Mihalcea, 2009) for German, French and Spanish, respectively.13 We trained SG vectors for each language of length 300 on a corpus of 1 billion tokens, each extracted from Wikipedia, and evaluate them on word similarity on the benchmarks before and after retrofitting. Table 5 shows that we obtain high improvements which strongly indicates that our method generalizes across these languages. 7 Further Analysis Retrofitting vs. vector length. With more dimensions, word vectors might be able to capture higher orders of semantic information and retrofitting might be less helpful. We train SG vec12 https://github.c"
N15-1184,P12-1092,0,0.106061,"btained from different lexicons. and are of length 300.1 Skip-Gram Vectors (SG). The word2vec tool (Mikolov et al., 2013a) is fast and currently in wide use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3 Multilingual Vectors (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis (CCA) on pairs of vectors for words that align in parallel corpora. The monolingual vectors were trained on WMT-2011 news corpus for English, French, German and Spanish. We use the Enligsh word vectors projected in the common English–German space. The monolingual English WMT corpus had 360 million words and the trained vector"
N15-1184,J03-1003,0,0.0119063,"Missing"
N15-1184,D11-1122,0,0.00432874,"ilar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector training method. Our experiments explored the method’s performan"
N15-1184,W14-1618,0,0.0675244,"rankings. Syntactic Relations (SYN-REL). Mikolov et al. (2013b) present a syntactic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common syntactic relation. For example, given walking and walked, the words are differently inflected forms of the same verb. There are nine different kinds of relations and overall there are 10,675 syntactic pairs of word tuples. The task is to find a word d that best fits the following relationship: “a is to b as c is to d,” given a, b, and c. We use the vector offset method (Mikolov et al., 2013a; Levy and Goldberg, 2014), computing q = qa − qb + qc and returning the vector from Q which has the highest cosine similarity to q. Synonym Selection (TOEFL). The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates (Landauer and Dumais, 1997). The dataset contains 80 such questions. An example is “rug → {sofa, ottoman, carpet, hallway}”, with carpet being the most synonym-like candidate to the target. Sentiment Analysis (SA). Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences f"
N15-1184,N13-1090,0,0.687964,"tors. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the word vector space. These vectors were trained on 6 billion words from Wikipedia and English Gigaword 1608 Lexicon PPDB WordNetsyn WordNetall FrameNet Words 102,902 148,730 148,730 10,822 Edges 374,555 304,856 934,705 417,456 Table 1: Approximate size of the graphs obtained from different lexicons. and are of length 300.1 Skip-Gram Vectors (SG). The word2vec tool (Mikolov et al., 2013a) is fast and currently in wide use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3"
N15-1184,D14-1162,0,0.120168,"Missing"
N15-1184,D13-1170,0,0.00650579,"nship: “a is to b as c is to d,” given a, b, and c. We use the vector offset method (Mikolov et al., 2013a; Levy and Goldberg, 2014), computing q = qa − qb + qc and returning the vector from Q which has the highest cosine similarity to q. Synonym Selection (TOEFL). The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates (Landauer and Dumais, 1997). The dataset contains 80 such questions. An example is “rug → {sofa, ottoman, carpet, hallway}”, with carpet being the most synonym-like candidate to the target. Sentiment Analysis (SA). Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarsegrained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We train an `2 -regularized logistic regression classifier on the average of the word vectors of a given sentence to predict the coarse-grained sentiment tag at the sentence level, and report the testset accuracy of the classifier. 6 Experiments We first show experiments measuring"
N15-1184,D10-1017,0,0.0133974,"red word vector to be close to the observed value qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agno"
N15-1184,P10-1149,0,0.00454798,"y induction (Yih et al., 2012) and multi-relational latent semantic analysis (Chang et al., 2013). The approach we propose is conceptually similar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information wh"
N15-1184,P10-1040,0,0.0887589,"tery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and pa"
N15-1184,J06-3003,0,0.0129744,", and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level inf"
N15-1184,D12-1111,0,0.0389385,"ons should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors (§2). This allows retrofitting to be used on pre-trained word vectors"
N15-1184,P14-2089,0,0.636299,"search on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-pr"
N15-1184,J90-1003,0,\N,Missing
N15-1184,J07-2002,0,\N,Missing
N15-1184,C98-1013,0,\N,Missing
N15-1184,D13-1141,0,\N,Missing
N16-1082,E14-1049,0,0.00880291,"tasets and the adopted neural models in Section 3. Different visualization strategies and correspondent analytical results are presented 681 Proceedings of NAACL-HLT 2016, pages 681–691, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet visualizing techniques consi"
N16-1082,P15-1144,0,0.130602,"2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet visualizing techniques consist mainly in mapping the different layers of the network (or other features like SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)) back to the initial image input, thus capturing the humaninterpretable information they represe"
N16-1082,N15-1004,0,0.0189816,", June 12-17, 2016. 2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet visualizing techniques consist mainly in mapping the different layers of the network (or other features like SIFT (Lowe, 2004) and HOG (Dalal and Triggs, 2005)) back to the initial image input, thus capturing the humaninterpretable i"
N16-1082,P14-1002,0,0.00988943,"this work. We describe datasets and the adopted neural models in Section 3. Different visualization strategies and correspondent analytical results are presented 681 Proceedings of NAACL-HLT 2016, pages 681–691, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics separately in Section 4,5,6, followed by a brief conclusion. 2 A Brief Review of Neural Visualization Similarity is commonly visualized graphically, generally by projecting the embedding space into two dimensions and observing that similar words tend to be clustered together (e.g., Elman (1989), Ji and Eisenstein (2014), Faruqui and Dyer (2014)). (Karpathy et al., 2015) attempts to interpret recurrent neural models from a statical point of view and does deeply touch compositionally of meanings. Other relevant attempts include (Fyshe et al., 2015; Faruqui et al., 2015). Methods for interpreting and visualizing neural models have been much more significantly explored in vision, especially for Convolutional Neural Networks (CNNs or ConvNets) (Krizhevsky et al., 2012), multi-layer neural networks in which the original matrix of image pixels is convolved and pooled as it is passed on to hidden layers. ConvNet vis"
N16-1082,C12-1118,0,0.0188047,"ire the work we present in this paper, there are fundamental differences between vision and NLP. In NLP words function as basic units, and hence (word) vectors rather than single pixels are the basic units. Sequences of words (e.g., phrases and sentences) are also presented in a more structured way than arrangements of pixels. In parallel to our research, independent researches (Karpathy et al., 2015) have been conducted to explore similar direction from an error-analysis point of view, by analyzing predictions and errors from a recurrent neural models. Other distantly relevant works include: Murphy et al. (2012; Fyshe et al. (2015) used an manual task to quantify the interpretability of semantic dimensions by presetting human users with a list of words and ask them to choose the one that does not belong to the list. Faruqui et al. (2015). Similar strategy is adopted in (Faruqui et al., 2015) by extracting top-ranked words in each vector dimension. 3 Datasets and Neural Models We explored two datasets on which neural models are trained, one of which is of relatively small scale and the other of large scale. 3.1 Stanford Sentiment Treebank Stanford Sentiment Treebank is a benchmark dataset widely used"
N16-1082,D13-1170,0,0.0729884,"atively small scale and the other of large scale. 3.1 Stanford Sentiment Treebank Stanford Sentiment Treebank is a benchmark dataset widely used for neural model evaluations. The dataset contains gold-standard sentiment labels for every parse tree constituent, from sentences to phrases to individual words, for 215,154 phrases in 11,855 sentences. The task is to perform both finegrained (very positive, positive, neutral, negative and very negative) and coarse-grained (positive vs negative) classification at both the phrase and sentence level. For more details about the dataset, please refer to Socher et al. (2013). While many studies on this dataset use recursive parse-tree models, in this work we employ only standard sequence models (RNNs and LSTMs) since these are the most widely used current neural models, and sequential visualization is more straightforward. We therefore first transform each parse tree node to a sequence of tokens. The sequence is first mapped to a phrase/sentence representation and fed into a softmax classifier. Phrase/sentence representations are built with the following three models: Standard Recurrent Sequence with TANH activation functions, LSTMs and Bidirectional LSTMs. For d"
N16-1082,P15-1002,0,\N,Missing
N16-1082,N16-1014,1,\N,Missing
N16-1116,P08-1090,0,0.0432637,"detect mentions are similar to those of Lee et al. (2013), except that their system uses a set of filtering rules designed to discard instances of pleonastic it, partitives, certain quantified noun phrases and other spurious mentions. Our system keeps partitives, quantified noun phrases and bare NP mentions, but discards pleonastic it and other spurious mentions. 3 3.1 Experiments Experimental Setup Datasets. Due to the availability of readily parsed data, we select the APW and NYT sections of Gigaword Corpus (years 1994-2010) (Parker et al., 2011) to train the model. Following previous work (Chambers and Jurafsky, 2008), we remove duplicated documents and the documents which include fewer than 3 sentences. The development and test data are the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The corpora statistics are shown in Table 2. Our system is evaluated with automatically extracted mentions on the version of the data with automatic preprocessing information (e.g., predicted parse trees). 1015 Evaluation Metrics. We evaluate our model on three measures widely used in the literature: MUC (Vilain et al., 1995), B3 (Bagga a"
N16-1116,P09-1068,0,0.0899005,"Missing"
N16-1116,E09-1018,0,0.0584167,"ng D as observed data and C as latent variables. We run EM with 10 iterations and select the parameters achieving the best performance on the development data. Each iteration takes around 12 hours with 10 CPUs parallelly. The best parameters appear at around the 5th iteration, according to our experiments.The detailed derivation of the learning algorithm is shown in Appendix A. The pseudo-code is shown is Algorithm 1. We use uniform initialization for all the parameters in our model. Several previous work has attempted to use EM for entity coreference resolution. Cherry and Bergsma (2005) and Charniak and Elsner (2009) applied EM for pronoun anaphora resolution. Ng (2008) probabilistically induced coreference partitions via EM clustering. Recently, Moosavi and Strube (2014) proposed an unsupervised model utiCoNLL’12 English development data CoNLL’12 English test data MUC B3 CEAFm CEAFe Blanc CoNLL MUC B3 CEAFm CEAFe Blanc CoNLL MIR 65.39 54.89 – 51.36 – 57.21 64.64 52.52 – 49.11 – 55.42 Stanford 64.96 54.49 59.39 51.24 56.03 56.90 64.71 52.26 56.01 49.32 53.92 55.43 Multigraph 66.22 56.41 60.87 52.61 58.15 58.41 65.41 54.38 58.60 50.21 56.03 56.67 Our Model 67.89 57.83 62.11 53.76 60.58 59.83 67.69 55.86 59"
N16-1116,W05-0612,0,0.0360696,"l., 1977) on our Model, treating D as observed data and C as latent variables. We run EM with 10 iterations and select the parameters achieving the best performance on the development data. Each iteration takes around 12 hours with 10 CPUs parallelly. The best parameters appear at around the 5th iteration, according to our experiments.The detailed derivation of the learning algorithm is shown in Appendix A. The pseudo-code is shown is Algorithm 1. We use uniform initialization for all the parameters in our model. Several previous work has attempted to use EM for entity coreference resolution. Cherry and Bergsma (2005) and Charniak and Elsner (2009) applied EM for pronoun anaphora resolution. Ng (2008) probabilistically induced coreference partitions via EM clustering. Recently, Moosavi and Strube (2014) proposed an unsupervised model utiCoNLL’12 English development data CoNLL’12 English test data MUC B3 CEAFm CEAFe Blanc CoNLL MUC B3 CEAFm CEAFe Blanc CoNLL MIR 65.39 54.89 – 51.36 – 57.21 64.64 52.52 – 49.11 – 55.42 Stanford 64.96 54.49 59.39 51.24 56.03 56.90 64.71 52.26 56.01 49.32 53.92 55.43 Multigraph 66.22 56.41 60.87 52.61 58.15 58.41 65.41 54.38 58.60 50.21 56.03 56.67 Our Model 67.89 57.83 62.11 5"
N16-1116,P15-1136,0,0.0347732,"Missing"
N16-1116,D13-1203,0,0.129904,"Missing"
N16-1116,Q14-1037,0,0.0318382,"Missing"
N16-1116,W12-4502,0,0.05507,"Missing"
N16-1116,P07-1107,0,0.190422,"Missing"
N16-1116,D09-1120,0,0.149835,"o that in the IBM 1 model on machine translation (Brown et al., 1993), where it assumes that given the corresponding English word, the aligned foreign word is independent with other English and foreign words. We do not make any independent assumptions among different features (see Section 2.4 for details). Inference in this model is efficient, because we can compute cj separately for each mention: The model is a so-called ranking model because it is able to identify the most probable candidate antecedent given a mention to be resolved. 2.3 Resolution Mode Variables According to previous work (Haghighi and Klein, 2009; Ratinov and Roth, 2012; Lee et al., 2013), antecedents are resolved by different categories of information for different mentions. For example, the Stanford system (Lee et al., 2013) uses stringmatching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym. Motivated by this, we introduce resolution mode variables Π = {π1 , . . . , πn }, where for each mention j the variable πj ∈ {str, prec, attr} indicates in which mode the mention should be resolved. In our model, we"
N16-1116,N10-1061,0,0.129937,"Missing"
N16-1116,N06-2015,1,0.602158,"tified noun phrases and bare NP mentions, but discards pleonastic it and other spurious mentions. 3 3.1 Experiments Experimental Setup Datasets. Due to the availability of readily parsed data, we select the APW and NYT sections of Gigaword Corpus (years 1994-2010) (Parker et al., 2011) to train the model. Following previous work (Chambers and Jurafsky, 2008), we remove duplicated documents and the documents which include fewer than 3 sentences. The development and test data are the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The corpora statistics are shown in Table 2. Our system is evaluated with automatically extracted mentions on the version of the data with automatic preprocessing information (e.g., predicted parse trees). 1015 Evaluation Metrics. We evaluate our model on three measures widely used in the literature: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFe ) (Luo, 2005). In addition, we also report results on another two popular metrics: Mention-based CEAF (CEAFm ) and BLANC (Recasens and Hovy, 2011). All the results are given by the latest version of CoNLL-2012"
N16-1116,Y09-1024,0,0.0307333,"r Gender Person Animacy Semantic Class Distance Description the type of a mention. We use three mention types: P roper, N ominal, P ronoun the same as the mention type feature under prec mode. boolean feature corresponding to String Match sieve in Stanford system. boolean feature corresponding to Relaxed String Match sieve in Stanford system. boolean feature corresponding to Strict Head Match A sieve in Stanford system. the same as the mention type feature under prec mode. the number of a mention similarly derived from Lee et al. (2013). the gender of a mention from Bergsma and Lin (2006) and Ji and Lin (2009). the person attribute from Lee et al. (2013). We assign person attributes to all mentions, not only pronouns. the animacy attribute same as Lee et al. (2013). semantic classes derived from WordNet (Soon et al., 2001). sentence distance between the two mentions. This feature is for parameter q(k|j, π) Table 1: Feature set for representing a mention under different resolution modes. The Distance feature is for parameter q, while all other features are for parameter t. Corpora Gigaword ON-Dev ON-Test Algorithm 1: Learning Model with EM 1 2 3 4 5 6 7 Initialization: Initialize θ0 = {t0 , q0 } for"
N16-1116,W11-1902,0,0.112102,"Missing"
N16-1116,J13-4004,0,0.550547,"Brown et al., 1993), where it assumes that given the corresponding English word, the aligned foreign word is independent with other English and foreign words. We do not make any independent assumptions among different features (see Section 2.4 for details). Inference in this model is efficient, because we can compute cj separately for each mention: The model is a so-called ranking model because it is able to identify the most probable candidate antecedent given a mention to be resolved. 2.3 Resolution Mode Variables According to previous work (Haghighi and Klein, 2009; Ratinov and Roth, 2012; Lee et al., 2013), antecedents are resolved by different categories of information for different mentions. For example, the Stanford system (Lee et al., 2013) uses stringmatching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym. Motivated by this, we introduce resolution mode variables Π = {π1 , . . . , πn }, where for each mention j the variable πj ∈ {str, prec, attr} indicates in which mode the mention should be resolved. In our model, we define three resolution modes — string-mat"
N16-1116,H05-1004,0,0.133704,"which include fewer than 3 sentences. The development and test data are the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The corpora statistics are shown in Table 2. Our system is evaluated with automatically extracted mentions on the version of the data with automatic preprocessing information (e.g., predicted parse trees). 1015 Evaluation Metrics. We evaluate our model on three measures widely used in the literature: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFe ) (Luo, 2005). In addition, we also report results on another two popular metrics: Mention-based CEAF (CEAFm ) and BLANC (Recasens and Hovy, 2011). All the results are given by the latest version of CoNLL-2012 scorer 1 3.2 Results and Comparison Table 3 illustrates the results of our model together as baseline with two deterministic systems, namely Stanford: the Stanford system (Lee et al., 2011) and Multigraph: the unsupervised multigraph system (Martschat, 2013), and one unsupervised system, namely MIR: the unsupervised system using most informative relations (Moosavi and Strube, 2014). Our model outperf"
N16-1116,P14-1126,1,0.850123,"Missing"
N16-1116,Q15-1029,0,0.023719,"Missing"
N16-1116,P13-3012,0,0.183093,"Missing"
N16-1116,C14-1061,0,0.0295196,"Each iteration takes around 12 hours with 10 CPUs parallelly. The best parameters appear at around the 5th iteration, according to our experiments.The detailed derivation of the learning algorithm is shown in Appendix A. The pseudo-code is shown is Algorithm 1. We use uniform initialization for all the parameters in our model. Several previous work has attempted to use EM for entity coreference resolution. Cherry and Bergsma (2005) and Charniak and Elsner (2009) applied EM for pronoun anaphora resolution. Ng (2008) probabilistically induced coreference partitions via EM clustering. Recently, Moosavi and Strube (2014) proposed an unsupervised model utiCoNLL’12 English development data CoNLL’12 English test data MUC B3 CEAFm CEAFe Blanc CoNLL MUC B3 CEAFm CEAFe Blanc CoNLL MIR 65.39 54.89 – 51.36 – 57.21 64.64 52.52 – 49.11 – 55.42 Stanford 64.96 54.49 59.39 51.24 56.03 56.90 64.71 52.26 56.01 49.32 53.92 55.43 Multigraph 66.22 56.41 60.87 52.61 58.15 58.41 65.41 54.38 58.60 50.21 56.03 56.67 Our Model 67.89 57.83 62.11 53.76 60.58 59.83 67.69 55.86 59.66 51.75 57.78 58.44 IMS 67.15 55.19 58.86 50.94 56.22 57.76 67.58 54.47 58.17 50.21 55.41 57.42 Latent-Tree 69.46 57.83 – 54.43 – 60.57 70.51 57.58 – 53.86"
N16-1116,D08-1067,0,0.121526,"ld entities. In recent years, several supervised entity coreference resolution systems have been proposed, which, according to Ng (2010), can be categorized into three classes — mention-pair models (McCarthy and Lehnert, 1995), entity-mention models (Yang et al., 2008a; Haghighi and Klein, 2010; Lee et al., 2011) and ranking models (Yang et al., 2008b; Several unsupervised learning algorithms have been applied to coreference resolution. Haghighi and Klein (2007) presented a mention-pair nonparametric fully-generative Bayesian model for unsupervised coreference resolution. Based on this model, Ng (2008) probabilistically induced coreference partitions via EM clustering. Poon and Domingos (2008) proposed an entity-mention model that is able to perform joint inference across mentions by using Markov Logic. Unfortunately, these unsupervised systems’ performance on accuracy significantly falls behind those of supervised systems, and are even worse than the deterministic rule-based systems. Furthermore, there is no previous work exploring the possibility of developing an unsupervised ranking model which achieved state-of-theart performance under supervised settings for entity coreference resoluti"
N16-1116,P10-1142,0,0.640591,"Missing"
N16-1116,D08-1068,0,0.0339165,"Missing"
N16-1116,W12-4501,0,0.0285803,"ases and other spurious mentions. Our system keeps partitives, quantified noun phrases and bare NP mentions, but discards pleonastic it and other spurious mentions. 3 3.1 Experiments Experimental Setup Datasets. Due to the availability of readily parsed data, we select the APW and NYT sections of Gigaword Corpus (years 1994-2010) (Parker et al., 2011) to train the model. Following previous work (Chambers and Jurafsky, 2008), we remove duplicated documents and the documents which include fewer than 3 sentences. The development and test data are the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The corpora statistics are shown in Table 2. Our system is evaluated with automatically extracted mentions on the version of the data with automatic preprocessing information (e.g., predicted parse trees). 1015 Evaluation Metrics. We evaluate our model on three measures widely used in the literature: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFe ) (Luo, 2005). In addition, we also report results on another two popular metrics: Mention-based CEAF (CEAFm ) and BLANC (Recasens and Hovy, 2011)"
N16-1116,D12-1113,0,0.0488591,"on machine translation (Brown et al., 1993), where it assumes that given the corresponding English word, the aligned foreign word is independent with other English and foreign words. We do not make any independent assumptions among different features (see Section 2.4 for details). Inference in this model is efficient, because we can compute cj separately for each mention: The model is a so-called ranking model because it is able to identify the most probable candidate antecedent given a mention to be resolved. 2.3 Resolution Mode Variables According to previous work (Haghighi and Klein, 2009; Ratinov and Roth, 2012; Lee et al., 2013), antecedents are resolved by different categories of information for different mentions. For example, the Stanford system (Lee et al., 2013) uses stringmatching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym. Motivated by this, we introduce resolution mode variables Π = {π1 , . . . , πn }, where for each mention j the variable πj ∈ {str, prec, attr} indicates in which mode the mention should be resolved. In our model, we define three resolution"
N16-1116,J01-4004,0,0.234164,"onding to String Match sieve in Stanford system. boolean feature corresponding to Relaxed String Match sieve in Stanford system. boolean feature corresponding to Strict Head Match A sieve in Stanford system. the same as the mention type feature under prec mode. the number of a mention similarly derived from Lee et al. (2013). the gender of a mention from Bergsma and Lin (2006) and Ji and Lin (2009). the person attribute from Lee et al. (2013). We assign person attributes to all mentions, not only pronouns. the animacy attribute same as Lee et al. (2013). semantic classes derived from WordNet (Soon et al., 2001). sentence distance between the two mentions. This feature is for parameter q(k|j, π) Table 1: Feature set for representing a mention under different resolution modes. The Distance feature is for parameter q, while all other features are for parameter t. Corpora Gigaword ON-Dev ON-Test Algorithm 1: Learning Model with EM 1 2 3 4 5 6 7 Initialization: Initialize θ0 = {t0 , q0 } for t = 0 to T do set all counts c(. . .) = 0 for each document D do for j = 1 to n do for k = 0 to j − 1 do t(mj |mk ,πj )q(k|πj ,j) Ljk = j−1 P i=0 9 10 11 2.4 t(mj |mi ,πj )q(i|πj ,j) 13 q(k, j, π) = # Entity 4,546 4,"
N16-1116,M95-1005,0,0.47658,"ork (Chambers and Jurafsky, 2008), we remove duplicated documents and the documents which include fewer than 3 sentences. The development and test data are the English data from the CoNLL-2012 shared task (Pradhan et al., 2012), which is derived from the OntoNotes corpus (Hovy et al., 2006). The corpora statistics are shown in Table 2. Our system is evaluated with automatically extracted mentions on the version of the data with automatic preprocessing information (e.g., predicted parse trees). 1015 Evaluation Metrics. We evaluate our model on three measures widely used in the literature: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and Entity-based CEAF (CEAFe ) (Luo, 2005). In addition, we also report results on another two popular metrics: Mention-based CEAF (CEAFm ) and BLANC (Recasens and Hovy, 2011). All the results are given by the latest version of CoNLL-2012 scorer 1 3.2 Results and Comparison Table 3 illustrates the results of our model together as baseline with two deterministic systems, namely Stanford: the Stanford system (Lee et al., 2011) and Multigraph: the unsupervised multigraph system (Martschat, 2013), and one unsupervised system, namely MIR: the unsupervised system usin"
N16-1116,P15-1137,0,0.0308544,"Missing"
N16-1116,J93-2003,0,\N,Missing
N16-1116,J08-3002,0,\N,Missing
N16-1116,P06-1005,0,\N,Missing
N16-1116,P08-1096,0,\N,Missing
N16-1116,W12-4503,0,\N,Missing
N16-1116,P14-1005,0,\N,Missing
N16-1174,P14-1062,0,0.152187,"ication is one of the fundamental task in Natural Language Processing. The goal is to assign labels to text. It has broad applications including topic labeling (Wang and Manning, 2012), sentiment classification (Maas et al., 2011; Pang and Lee, 2008), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have been quite effective (Kim, 2014; Zhang et al., 2015; Johnson and Zhang, 2014; Tang et al., 2015), in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the r"
N16-1174,D14-1002,1,0.0625345,"ig. 1, which is a short Yelp review where the task is to predict the rating on a scale from 1–5. Intuitively, the first and third sentence have stronger information in assisting the prediction of the rating; within these sentences, the word delicious, a-m-a-z-i-n-g contributes more in implying the positive attitude contained in this review. Attention serves two benefits: not only does it often result in better performance, but it also provides insight into which words and sentences contribute to the classification decision which can be of value in applications and analysis (Shen et al., 2014; Gao et al., 2014). The key difference to previous work is that our system uses context to discover when a sequence of tokens is relevant rather than simply filtering for (sequences of) tokens, taken out of context. To evaluate the performance of our model in comparison to other common classification architectures, we look at six data sets (§3). Our model outperforms previous approaches by a significant margin. 2 Hierarchical Attention Networks The overall architecture of the Hierarchical Attention Network (HAN) is shown in Fig. 2. It consists of several parts: a word sequence encoder, a word-level attention la"
N16-1174,D14-1181,0,0.347192,"8), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have been quite effective (Kim, 2014; Zhang et al., 2015; Johnson and Zhang, 2014; Tang et al., 2015), in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interactions of the words, not just their presence in isolation. Our primary contribution is a new neural architecture (§2), the Hierarchical Attention Network (HAN) that is designed to capture two"
N16-1174,P15-1107,0,0.642353,"Missing"
N16-1174,D15-1106,0,0.0757692,"nd Internet. explore the structure of a sentence and use a treestructured LSTMs for classification. There are also some works that combine LSTM and CNN structure to for sentence classification (Lai et al., 2015; Zhou et al., 2015). Tang et al. (2015) use hierarchical structure in sentiment classification. They first use a CNN or LSTM to get a sentence vector and then a bi-directional gated recurrent neural network to compose the sentence vectors to get a document vectors. There are some other works that use hierarchical structure in sequence generation (Li et al., 2015) and language modeling (Lin et al., 2015). The attention mechanism was proposed by (Bahdanau et al., 2014) in machine translation. The encoder decoder framework is used and an attention mechanism is used to select the reference words in original language for words in foreign language before translation. Xu et al. (2015) uses the attention mechanism in image caption generation to select the relevant image regions when generating words in the captions. Further uses of the attention mechanism include parsing (Vinyals et al., 2014), natural language question answering (Sukhbaatar et al., 2015; 1487 Kumar et al., 2015; Hermann et al., 201"
N16-1174,P11-1015,0,0.312116,"cocktails. ||next time I in Phoenix, I will go back here. ||Highly recommend. Figure 1: A simple example review from Yelp 2013 that consists of five sentences, delimited by period, question mark. The first and third sentence delivers stronger meaning and inside, the word delicious, a-m-a-z-i-n-g contributes the most in defining sentiment of the two sentences. Introduction Text classification is one of the fundamental task in Natural Language Processing. The goal is to assign labels to text. It has broad applications including topic labeling (Wang and Manning, 2012), sentiment classification (Maas et al., 2011; Pang and Lee, 2008), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have bee"
N16-1174,P14-5010,0,0.0519213,"words (average and maximum per document). LSTM takes the whole document as a single sequence and the average of the hidden states of all words is used as feature for classification. Conv-GRNN and LSTM-GRNN were proposed by (Tang et al., 2015). They also explore the hierarchical structure: a CNN or LSTM provides a sentence vector, and then a gated recurrent neural network (GRNN) combines the sentence vectors from a document level vector representation for classification. 3.3 Model configuration and training We split documents into sentences and tokenize each sentence using Stanford’s CoreNLP (Manning et al., 2014). We only retain words appearing more than 5 times in building the vocabulary and replace the words that appear 5 times with a special UNK token. We obtain the word embedding by training an unsupervised word2vec (Mikolov et al., 2013) model on the training and validation splits and then use the word embedding to initialize We . The hyper parameters of the models are tuned on the validation set. In our experiments, we set the word embedding dimension to be 200 and the GRU dimension to be 50. In this case a combination of forward and backward GRU gives us 100 dimensions for word/sentence annotat"
N16-1174,D13-1170,0,0.0565051,"ing sentences. Note that this happens in a multiclass setting, that is, detection happens before the selection of the topic! 4 Related Work Kim (2014) use neural networks for text classification. The architecture is a direct application of CNNs, as used in computer vision (LeCun et al., 1998), albeit with NLP interpretations. Johnson and Zhang (2014) explores the case of directly using a high-dimensional one hot vector as input. They find that it performs well. Unlike word level modelings, Zhang et al. (2015) apply a character-level CNN for text classification and achieve competitive results. Socher et al. (2013) use recursive neural networks for text classification. Tai et al. (2015) GT: 4 Prediction: 4 pork belly = delicious . scallops ? i do n’t . even . like . scallops , and these were a-m-a-z-i-n-g . fun and tasty cocktails . next time i ’m in phoenix , i will go back here . highly recommend . GT: 0 Prediction: 0 terrible value . ordered pasta entree . . $ 16.95 good taste but size was an appetizer size . . no salad , no bread no vegetable . this was . our and tasty cocktails . our second visit . i will not go back . Figure 5: Documents from Yelp 2013. Label 4 means star 5, label 0 means star 1."
N16-1174,P15-1150,0,0.146221,"ction happens before the selection of the topic! 4 Related Work Kim (2014) use neural networks for text classification. The architecture is a direct application of CNNs, as used in computer vision (LeCun et al., 1998), albeit with NLP interpretations. Johnson and Zhang (2014) explores the case of directly using a high-dimensional one hot vector as input. They find that it performs well. Unlike word level modelings, Zhang et al. (2015) apply a character-level CNN for text classification and achieve competitive results. Socher et al. (2013) use recursive neural networks for text classification. Tai et al. (2015) GT: 4 Prediction: 4 pork belly = delicious . scallops ? i do n’t . even . like . scallops , and these were a-m-a-z-i-n-g . fun and tasty cocktails . next time i ’m in phoenix , i will go back here . highly recommend . GT: 0 Prediction: 0 terrible value . ordered pasta entree . . $ 16.95 good taste but size was an appetizer size . . no salad , no bread no vegetable . this was . our and tasty cocktails . our second visit . i will not go back . Figure 5: Documents from Yelp 2013. Label 4 means star 5, label 0 means star 1. GT: 1 Prediction: 1 why does zebras have stripes ? what is the purpose or"
N16-1174,P14-1146,0,0.12484,"(Mikolov et al., 2013) is used as feature set. 3.2.2 SVMs SVMs-based methods are reported in (Tang et al., 2015), including SVM+Unigrams, Bigrams, Text Features, AverageSG, SSWE. In detail, Unigrams and Bigrams uses bag-of-unigrams and bagof-bigrams as features respectively. Text Features are constructed according to (Kiritchenko et al., 2014), including word and character n-grams, sentiment lexicon features etc. AverageSG constructs 200-dimensional word vectors using word2vec and the average word embeddings of each document are used. SSWE uses sentiment specific word embeddings according to (Tang et al., 2014). 3.2.3 Neural Network methods The neural network based methods are reported in (Tang et al., 2015) and (Zhang et al., 2015). CNN-word Word based CNN models like that of (Kim, 2014) are used. CNN-char Character level CNN models are reported in (Zhang et al., 2015). Data set Yelp 2013 Yelp 2014 Yelp 2015 IMDB review Yahoo Answer Amazon review classes documents average #s max #s average #w max #w vocabulary 5 5 5 10 10 5 335,018 1,125,457 1,569,264 348,415 1,450,000 3,650,000 8.9 9.2 9.0 14.0 6.4 4.9 151 151 151 148 515 99 151.6 156.9 151.9 325.6 108.4 91.9 1184 1199 1199 2802 4002 596 211,245 4"
N16-1174,D15-1167,0,0.706504,"l approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–based approaches to text classification have been quite effective (Kim, 2014; Zhang et al., 2015; Johnson and Zhang, 2014; Tang et al., 2015), in this paper we test the hypothesis that better representations can be obtained by incorporating knowledge of document structure in the model architecture. The intuition underlying our model is that not all parts of a document are equally relevant for answering a query and that determining the relevant sections involves modeling the interactions of the words, not just their presence in isolation. Our primary contribution is a new neural architecture (§2), the Hierarchical Attention Network (HAN) that is designed to capture two basic insights about document structure. First, since documents"
N16-1174,P12-2018,0,0.260108,"ops, and these were a-m-a-z-i-n-g . ||fun and tasty cocktails. ||next time I in Phoenix, I will go back here. ||Highly recommend. Figure 1: A simple example review from Yelp 2013 that consists of five sentences, delimited by period, question mark. The first and third sentence delivers stronger meaning and inside, the word delicious, a-m-a-z-i-n-g contributes the most in defining sentiment of the two sentences. Introduction Text classification is one of the fundamental task in Natural Language Processing. The goal is to assign labels to text. It has broad applications including topic labeling (Wang and Manning, 2012), sentiment classification (Maas et al., 2011; Pang and Lee, 2008), and spam detection (Sahami et al., 1998). Traditional approaches of text classification represent documents with sparse lexical features, such as n-grams, and then use a linear model or kernel methods on this representation (Wang and Manning, 2012; Joachims, 1998). More recent approaches used deep learning, such as convolutional neural networks (Blunsom et al., 2014) and recurrent neural networks based on long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn text representations. Although neural-network–bas"
N16-1174,D15-1176,1,\N,Missing
N16-1174,N15-1011,0,\N,Missing
N18-1149,D14-1162,0,0.0903683,"Missing"
N18-1149,D13-1181,0,\N,Missing
N18-1149,N18-1022,1,\N,Missing
N19-1186,W05-0909,0,0.493628,"between two word embeddings. The distance is obtained using cosine similarity and the difference of word position between the translation and reference. Results demonstrate that our proposed metric can evaluate translations also considering word order differences. We designate this new metric as Word Embedding-based automatic MT evaluation using Word Position Information (WE WPI). The experimentally obtained results based on the WMT16 metrics shared task (Bojar et al., 2016) demonstrated that our WE WPI achieves the highest correlation with human judgment among several metrics: BLEU, METEOR (Banerjee and Lavie, 2005), IMand PACT (Echizen-ya and Araki, 2007), RIBES (Isozaki et al., 2010). Moreover, the correlation of WE WPI is better than that of WE WPI without word position information (WE). Results therefore confirmed the effectiveness of WE WPI using word position information. 2 Related Work Kusner et al. (2016) proposed the Word Mover’s Distance (WMD) as a distance measure using word embedding and word alignment. This measure obtains the distance between two documents adjusting EMD to a document. However, it cannot accommodate differences of word order between the translation and reference. Matsuo et a"
N19-1186,W17-4755,0,0.0531052,"rd positions of the translation and reference, not the difference of lengths between the translation and reference. Therefore, it can sufficiently accommodate word order differences. Moreover, it can evaluate the translation efficiently using word embeddings of target languages without requiring large amounts of data or learning time. Our WE WPI requires no learning of bilingual knowledge or a relation between translation and reference. It needs only a model of word embeddings in advance to apply EMD to the automatic MT evaluation task. In a non-trained evaluation metric, MEANT 2.0 (Lo, 2017; Bojar et al., 2017) uses a distributional word vector model to evaluate lexical semantic similarity and shallow semantic parses to evaluate structural semantic similarity between the translation and reference. It is a new version of MEANT (Lo and Wu, 2011), which is a non-ensemble and untrained metric. Moreover, MEANT 2.0 - nosrl is a subversion of MEANT 2.0 to evaluate the translation for any output language by removing the dependence on semantic parsers for semantic role labeling (SRL). In that case, phrasal similarity is calculated using n-gram lexical similarities. However, MEANT 2.0 series do not specifical"
N19-1186,W16-2302,0,0.0500804,"Missing"
N19-1186,P15-2025,0,0.260814,"As described in that paper, Maximum Alignment Similarity (MAS) was found to have higher correlation with human evaluation than BLEU for European-to-English, which has similar grammar structures. For Japanese-toEnglish, which has different grammar structures, Average Alignment Similarity (AAS) showed better correlation with human evaluation than other metrics. However, neither MAS nor AAS uses word position information. Therefore, neither can sufficiently accommodate word order differences. Actually, WE WPI uses not only the word alignment but also word position information. One system, DREEM (Chen and Guo, 2015), learns distributed word representations from a neural network model and from distributed sentence representations computed with a recursive autoencoder. Moreover, it uses a penalty based on translation and reference lengths. By contrast, the WE WPI system specifically examines the difference between the word positions of the translation and reference, not the difference of lengths between the translation and reference. Therefore, it can sufficiently accommodate word order differences. Moreover, it can evaluate the translation efficiently using word embeddings of target languages without requ"
N19-1186,P05-1033,0,0.0661303,"position information are used to address wordorder differences. We designate this metric as Word Embedding-based automatic MT evaluation using Word Position Information (WE WPI). A meta-evaluation using WMT16 metrics shared task set indicates that our WE WPI achieves the highest correlation with human judgment among several representative metrics. 1 Introduction Recent advances in neural machine translation (NMT) (Sutskever et al., 2014; Luong et al., 2015) are remarkable. Results based on human evaluation have demonstrated that NMT outperforms statistical machine translations significantly (Chiang, 2005; Tufis¸ and Ceaus¸u, 2009). The NMT achieved especially high performance in terms of fluency. However, it tends to generate more omission errors than statistical machine translations generate. Unfortunately, it is difficult for automatic evaluation metrics to evaluate outputs with omission errors because those errors are not included as non-match words between the translation and reference. For such cases, the word embedding-based automatic MT evaluation metric, which is based on word position information, is effective. Various automatic evaluation metrics have been proposed for machine trans"
N19-1186,2007.mtsummit-papers.21,1,0.429922,"nce is obtained using cosine similarity and the difference of word position between the translation and reference. Results demonstrate that our proposed metric can evaluate translations also considering word order differences. We designate this new metric as Word Embedding-based automatic MT evaluation using Word Position Information (WE WPI). The experimentally obtained results based on the WMT16 metrics shared task (Bojar et al., 2016) demonstrated that our WE WPI achieves the highest correlation with human judgment among several metrics: BLEU, METEOR (Banerjee and Lavie, 2005), IMand PACT (Echizen-ya and Araki, 2007), RIBES (Isozaki et al., 2010). Moreover, the correlation of WE WPI is better than that of WE WPI without word position information (WE). Results therefore confirmed the effectiveness of WE WPI using word position information. 2 Related Work Kusner et al. (2016) proposed the Word Mover’s Distance (WMD) as a distance measure using word embedding and word alignment. This measure obtains the distance between two documents adjusting EMD to a document. However, it cannot accommodate differences of word order between the translation and reference. Matsuo et al. (2017) also proposed a word-alignment-"
N19-1186,L18-1550,0,0.032814,"different automatic MT evaluation metrics for comparison with our WE WPI: BLEU, METEOR, IMPACT, RIBES, and WE. Here, IMPACT and RIBES, which are surface-based metrics, are effective for language pairs with greatly different word order, such as English and Japanese. In addition, WE is an automatic MT evaluation metric that does not perform word alignment. It uses only dij = 1.0 − cos sim(ti , rj ) as the dij of Eq. (12) in the WE WPI. In both WE and WE WPI, the word vectors for seven languages (i.e., English, Czech, German, Finnish, Romanian, Russian, and Turkish) were obtained using fastText (Grave et al., 2018). 4.2 Experiment Results and Discussion Tables 4 and 5 respectively present the correlation coefficient of to-English and out-of-English at the system level. Tables 6 and 7 respectively present the correlation coefficients of to-English and outof-English at the segment level. In Tables 4–7, RR represents the correlation based on the relative ranking by human judgment to 5 translations at a time. The bold typeface shows the highest correlation coefficient among all correlation coefficients of metrics. Moreover, the coefficients of MEANT 2.0 described in (Lo, 2017) are added to Tables 4– 6. Here"
N19-1186,D10-1092,0,0.0338529,"ity and the difference of word position between the translation and reference. Results demonstrate that our proposed metric can evaluate translations also considering word order differences. We designate this new metric as Word Embedding-based automatic MT evaluation using Word Position Information (WE WPI). The experimentally obtained results based on the WMT16 metrics shared task (Bojar et al., 2016) demonstrated that our WE WPI achieves the highest correlation with human judgment among several metrics: BLEU, METEOR (Banerjee and Lavie, 2005), IMand PACT (Echizen-ya and Araki, 2007), RIBES (Isozaki et al., 2010). Moreover, the correlation of WE WPI is better than that of WE WPI without word position information (WE). Results therefore confirmed the effectiveness of WE WPI using word position information. 2 Related Work Kusner et al. (2016) proposed the Word Mover’s Distance (WMD) as a distance measure using word embedding and word alignment. This measure obtains the distance between two documents adjusting EMD to a document. However, it cannot accommodate differences of word order between the translation and reference. Matsuo et al. (2017) also proposed a word-alignment-based automatic evaluation met"
N19-1186,D15-1166,0,0.0232496,"Missing"
N19-1186,P02-1040,0,0.105581,"high performance in terms of fluency. However, it tends to generate more omission errors than statistical machine translations generate. Unfortunately, it is difficult for automatic evaluation metrics to evaluate outputs with omission errors because those errors are not included as non-match words between the translation and reference. For such cases, the word embedding-based automatic MT evaluation metric, which is based on word position information, is effective. Various automatic evaluation metrics have been proposed for machine translation, but none is sufficient for NMT. Actually, BLEU (Papineni et al., 2002) is the representative metric based on ngram matching. Unfortunately, because it is a surface-level metric, it is difficult to address word meaning during evaluation for MT outputs. The word-embedding-based distance measure for document (Kusner et al., 2016) and the word-alignment-based automatic evaluation metric using word embedding (Matsuo et al., 2017) are effective to address word meanings. Nevertheless, they can only ineffectively accommodate word order differences between the translation and reference. Given those circumstances, a new metric with word embedding-based automatic MT evalua"
N19-1186,P16-1159,0,0.0309751,"Missing"
N19-1186,D09-1117,0,0.0388943,"Missing"
N19-1186,W17-4767,0,0.286166,"een the word positions of the translation and reference, not the difference of lengths between the translation and reference. Therefore, it can sufficiently accommodate word order differences. Moreover, it can evaluate the translation efficiently using word embeddings of target languages without requiring large amounts of data or learning time. Our WE WPI requires no learning of bilingual knowledge or a relation between translation and reference. It needs only a model of word embeddings in advance to apply EMD to the automatic MT evaluation task. In a non-trained evaluation metric, MEANT 2.0 (Lo, 2017; Bojar et al., 2017) uses a distributional word vector model to evaluate lexical semantic similarity and shallow semantic parses to evaluate structural semantic similarity between the translation and reference. It is a new version of MEANT (Lo and Wu, 2011), which is a non-ensemble and untrained metric. Moreover, MEANT 2.0 - nosrl is a subversion of MEANT 2.0 to evaluate the translation for any output language by removing the dependence on semantic parsers for semantic role labeling (SRL). In that case, phrasal similarity is calculated using n-gram lexical similarities. However, MEANT 2.0 ser"
N19-1186,P11-1023,0,0.0223127,"ng word embeddings of target languages without requiring large amounts of data or learning time. Our WE WPI requires no learning of bilingual knowledge or a relation between translation and reference. It needs only a model of word embeddings in advance to apply EMD to the automatic MT evaluation task. In a non-trained evaluation metric, MEANT 2.0 (Lo, 2017; Bojar et al., 2017) uses a distributional word vector model to evaluate lexical semantic similarity and shallow semantic parses to evaluate structural semantic similarity between the translation and reference. It is a new version of MEANT (Lo and Wu, 2011), which is a non-ensemble and untrained metric. Moreover, MEANT 2.0 - nosrl is a subversion of MEANT 2.0 to evaluate the translation for any output language by removing the dependence on semantic parsers for semantic role labeling (SRL). In that case, phrasal similarity is calculated using n-gram lexical similarities. However, MEANT 2.0 series do not specifically examine the position of each word in the translation and reference. Results show that it is difficult to deal sufficiently with language pairs for which the grammar differs. In WE WPI, the evaluation score is calculated using the rela"
N19-1273,Q13-1005,1,0.929968,"Missing"
N19-1273,D13-1160,0,0.754253,"the previous best systems, on WTQ in a comparable setting, and on NLVR with significantly less supervision. 1 Introduction Semantic parsing is the task of translating natural language utterances into machine-executable meaning representations, often called programs or logical forms. These logical forms can be executed against some representation of the context in which the utterance occurs, to produce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive"
N19-1273,W18-2501,1,0.861033,"Missing"
N19-1273,P96-1024,0,0.380485,"logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where the supervision signal is binary—it is very hard for dynamic MML to bootstrap its way to finding good logical forms. To solve this problem, we interleave static MML, which has a consistent super"
N19-1273,P17-1097,0,0.348125,"tself defines a distribution over logical forms, however, not denotations, so this maximization must be recast as a marginalization over logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where the supervision signal is binary—it is very hard for dynamic M"
N19-1273,P17-1167,0,0.0510016,"ntic parsing model itself defines a distribution over logical forms, however, not denotations, so this maximization must be recast as a marginalization over logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where the supervision signal is binary—it is very"
N19-1273,P16-1002,0,0.0484511,"our contributions on the NLVR and W IKI TABLE Q UESTIONS datasets. Other work that evaluates on on these datasets include Goldman et al. (2018), Tan and Bansal (2018), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2"
N19-1273,D16-1032,1,0.829456,"ore details. In addition, we slightly modify the constrained decoding architecture from (Krishnamurthy et al., 2017) to bias the predicted actions towards those that would decrease the value of S(yi , xi ). This is done using a coverage vector, viS for each training instance that keeps track of the production rules triggered by xi , and gets updated whenever one of those desired productions is produced by the decoder. That is, viS is a vector of 1s and 0s, with 1s indicating the triggered productions that are yet to be produced by the decoder. This is similar to the idea of checklists used by Kiddon et al. (2016). The decoder in the original architecture scores output actions at each time step by computing a dot product of the predicted action representation with the embeddings of each of the actions. We add a weighted sum of all the actions that are yet to produced: sai = ea .(pi + γ ∗ viS .E) (4) where sai is the score of action a at time step i, ea is the embedding of that action, pi is the predicted action representation, E is the set of embeddings of all the actions, and γ is a learned parameter for regularizing the bias towards yet-to-be produced triggered actions. 6.2 Experimental setup NLVR We"
N19-1273,D17-1160,1,0.873567,"in frequent search failures early during training when model parameters are close to random, and in general may only yield spurious logical forms in the absence of any guidance. Since modern semantic parsers typically operate without a lexicon, new techniques are essential to provide guidance to the search procedure (Goldman et al., 2018). One way of providing this guidance during search is to perform some kind of heuristic search up front to find a set of logical forms that evaluate to the correct denotation, and use those logical forms to approximate the inner summation (Liang et al., 2011; Krishnamurthy et al., 2017). The particulars of the heuristic search can have a large impact on performance; a smaller candidate set has lower noise, while a larger set makes it more likely that the correct logical form is in it, and one needs to strike the right balance. In this paper, we refer to the MML that does search during training as dynamic MML, and the one that does an offline search as static MML. The main benefit of dynamic MML is that it adapts its training signal over time. As the model learns, it can increasingly focus its probability mass on a small set of very likely logical forms. The main benefit of s"
N19-1273,D11-1140,1,0.781683,"and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangle ((negate filter touch wall) all objects)))) There is a yellow block as the top of a tower with exactly three blocks. (object exists (yellow (top (object in box (member count equals all boxes 3))))) The tower with three blocks has a yellow block over a black block (object count greater equals (yellow (above (black (object in box (member count equals all boxes 3))))) 1) Table 7: Complexity of logical forms produced at"
N19-1273,P17-1003,0,0.0331307,"e denotation given the utterance. The semantic parsing model itself defines a distribution over logical forms, however, not denotations, so this maximization must be recast as a marginalization over logical forms that evaluate to the correct denotation: Y X max p(yi |xi ; θ) (1) θ xi ,di ∈D yi ∈Y |Jyi Kwi =di Reward-based methods When training weakly supervised semantic parsers, it is often desirable to inject some prior knowledge into the training procedure by defining arbitrary reward or cost functions. There exists prior work that use such methods, both in a reinforcement learning setting (Liang et al., 2017, 2018), and otherwise (Iyyer et al., 2017; Guu et al., 2017). In our work, we define a customized cost function that includes a coverage term, and use a Minimum Bayes Risk (MBR) (Goodman, 1996; Goel and Byrne, 2000; Smith and Eisner, 2670 2006) training scheme, which we describe in §3. 4 3 In this section we describe the iterative technique for refining the set of candidate logical forms associated with each training instance. As discussed in §2.2, most prior work on weakly-supervised training of semantic parsers uses dynamic MML. This is particularly problematic in domains like NLVR, where t"
N19-1273,P11-1060,0,0.732007,"aning representations, often called programs or logical forms. These logical forms can be executed against some representation of the context in which the utterance occurs, to produce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive at a complete set of operators required by the task. However, training semantic parsers with weak supervision requires not only searching over an exponentially large space of logical forms (Berant et al., 2013; Artzi an"
N19-1273,D18-1266,0,0.0200734,"), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangl"
N19-1273,P15-1142,0,0.14764,"Missing"
N19-1273,P16-1003,0,0.420486,"Missing"
N19-1273,P17-1105,0,0.0325794,"r work that evaluates on on these datasets include Goldman et al. (2018), Tan and Bansal (2018), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all box"
N19-1273,P17-1099,0,0.0230164,"esent modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangle ((negate filter touch wall) all objects)))) There is a yellow block as the top of a tower with exactly three blocks. (object exists"
N19-1273,P06-2101,0,0.125269,"Missing"
N19-1273,N18-1203,0,0.023553,"Missing"
N19-1273,P17-2034,0,0.113175,"Missing"
N19-1273,N18-2071,0,0.019839,"t al., 2011; Berant et al., 2013), or trying to automatically infer logical forms from denotations (Pasupat and Liang, 2016). However, matching the performance of a fully supervised semantic parser with only weak supervision remains a significant challenge (Yih et al., 2016). The main contributions of this work deal with training semantic parsers with weak supervision, and we gave a detailed discussion of related training methods in §2.2. We evaluate our contributions on the NLVR and W IKI TABLE Q UESTIONS datasets. Other work that evaluates on on these datasets include Goldman et al. (2018), Tan and Bansal (2018), Neelakantan et al. (2017), Krishnamurthy et al. (2017), Haug et al. (2018), and (Liang et al., 2018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include M"
N19-1273,P16-1008,0,0.0132738,"018). These prior works generally present modeling contributions that are orthogonal (and in some cases complementary) to the contributions of this paper. There has also been a lot of recent work on neural semantic parsing, most of which is also orthogonal to (and could probably benefit from) our contributions (Dong and Lapata, 2016; Jia and Liang, 2016; Yin and Neubig, 2017; Krishnamurthy et al., 2017; Rabinovich et al., 2017). Recent attempts at dealing with the problem of spuriousness include Misra et al. (2018) and Guu et al. (2017). Coverage has recently been used in machine translation (Tu et al., 2016) and summarization (See et al., 2017). There have also been many methods that use coverage-like mechanisms to give lexical cues to semantic parsers. Goldman et al. (2018)’s abstract examples is the most recent and related work, but the idea is also related to lexicons in pre-neural semantic parsers (Kwiatkowski et al., 2011). 2676 0 1 2 3 There is a tower with four blocks (box exists (member count equals all boxes 4)) Atleast one black triangle is not touching the edge (object exists (black (triangle ((negate filter touch wall) all objects)))) There is a yellow block as the top of a tower with"
N19-1273,P16-2033,0,0.0236319,"duce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive at a complete set of operators required by the task. However, training semantic parsers with weak supervision requires not only searching over an exponentially large space of logical forms (Berant et al., 2013; Artzi and Zettlemoyer, 2013; Pasupat and Liang, 2015; Guu et al., 2017, inter alia) but also dealing with spurious logical forms that evaluate to the correct denotation while not being s"
N19-1273,P17-1041,0,0.134843,"omparable setting, and on NLVR with significantly less supervision. 1 Introduction Semantic parsing is the task of translating natural language utterances into machine-executable meaning representations, often called programs or logical forms. These logical forms can be executed against some representation of the context in which the utterance occurs, to produce a denotation. This setup allows for complex reasoning over contextual knowledge, and it has been successfully used in several natural language understanding problems such as question answering (Berant et al., 2013), program synthesis (Yin and Neubig, 2017) and building natural language interfaces (Suhr et al., 2018). Recent work has focused on training semantic parses via weak supervision from denotations alone (Liang et al., 2011; Berant et al., 2013). This is because obtaining logical form annotations is generally expensive (although recent work has addressed this issue to some extent (Yih et al., 2016)), and not assuming full supervision lets us be agnostic about the logical form language. The second reason is more important in open-domain semantic parsing tasks where it may not be possible to arrive at a complete set of operators required b"
N19-1273,D17-1125,0,0.503711,"Missing"
N19-1364,P18-1058,0,0.0436454,"(2017) utilized the persuasive modes—ethos, logos, pathos—to model premises and the semantic types of argument components in an online persuasive forum. While most computational argumentation focuses on the relational support structures and factual evidence to make claims, persuasion focuses more on language cues aimed at shaping, reinforcing and changing people’s opinions and beliefs. How language changes people’s attitudes and behaviors have received less attention from the computational community than argumentation, although there have been important preliminary work (Persing and Ng, 2017; Carlile et al., 2018). Farra et al., (2015) built regression models to predict essay scores based on features extracted from opinion expressions and topical elements. Chatterjee et al., (2014) used verbal descriptors and para-verbal markers of hesitation to predict speakers’ persuasiveness on website housing videos of product reviews. When looking at persuasion in the context of online forum discussions (Wei et al., 2016), Tan et al., (2016) found that on the Change My View subreddit, interaction dynamics such as the language interplay between opinion holders and other participants provides highly predictive cues"
N19-1364,W14-5908,0,0.0359094,"putational argumentation focuses on the relational support structures and factual evidence to make claims, persuasion focuses more on language cues aimed at shaping, reinforcing and changing people’s opinions and beliefs. How language changes people’s attitudes and behaviors have received less attention from the computational community than argumentation, although there have been important preliminary work (Persing and Ng, 2017; Carlile et al., 2018). Farra et al., (2015) built regression models to predict essay scores based on features extracted from opinion expressions and topical elements. Chatterjee et al., (2014) used verbal descriptors and para-verbal markers of hesitation to predict speakers’ persuasiveness on website housing videos of product reviews. When looking at persuasion in the context of online forum discussions (Wei et al., 2016), Tan et al., (2016) found that on the Change My View subreddit, interaction dynamics such as the language interplay between opinion holders and other participants provides highly predictive cues for persuasiveness. Using the same dataset, Wel et al., (2016) extracted a set of textual information and social interaction features to identify persuasive posts. Recentl"
N19-1364,W14-2106,0,0.0327216,"or Computational Linguistics show that our semi-supervised model outperforms several baselines. We then apply this automated model to unseen requests from different domains and obtain nuanced findings of the importance of different strategies on persuasion success. Our model can be useful in any situation in which we have exogenous document-level supervision, but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and class"
N19-1364,P16-1150,0,0.0244722,"7; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and classified a set of argumentative actions associated with successful writing such as warrant/reasoning/backing, rebuttal/reservation, and claims/ideas. Habernal and Gurevych (2016) investigated the persuasiveness of arguments in any given argument pair using bidirectional LSTM. Hidey et al., (2017) utilized the persuasive modes—ethos, logos, pathos—to model premises and the semantic types of argument components in an online persuasive forum. While most computational argumentation focuses on the relational support structures and factual evidence to make claims, persuasion focuses more on language cues aimed at shaping, reinforcing and changing people’s opinions and beliefs. How language changes people’s attitudes and behaviors have received less attention from the comput"
N19-1364,J17-1004,0,0.0462079,"requests from different domains and obtain nuanced findings of the importance of different strategies on persuasion success. Our model can be useful in any situation in which we have exogenous document-level supervision, but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and classified a set of argumentative actions associated with successful writing such as warrant/reasoning/backing, rebuttal/reservation, and claims/ideas. H"
N19-1364,W17-5102,0,0.488347,"sing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and classified a set of argumentative actions associated with successful writing such as warrant/reasoning/backing, rebuttal/reservation, and claims/ideas. Habernal and Gurevych (2016) investigated the persuasiveness of arguments in any given argument pair using bidirectional LSTM. Hidey et al., (2017) utilized the persuasive modes—ethos, logos, pathos—to model premises and the semantic types of argument components in an online persuasive forum. While most computational argumentation focuses on the relational support structures and factual evidence to make claims, persuasion focuses more on language cues aimed at shaping, reinforcing and changing people’s opinions and beliefs. How language changes people’s attitudes and behaviors have received less attention from the computational community than argumentation, although there have been important preliminary work (Persing and Ng, 2017; Carlil"
N19-1364,P14-5010,0,0.00443789,"Missing"
N19-1364,W15-0608,0,0.0135574,"suasive modes—ethos, logos, pathos—to model premises and the semantic types of argument components in an online persuasive forum. While most computational argumentation focuses on the relational support structures and factual evidence to make claims, persuasion focuses more on language cues aimed at shaping, reinforcing and changing people’s opinions and beliefs. How language changes people’s attitudes and behaviors have received less attention from the computational community than argumentation, although there have been important preliminary work (Persing and Ng, 2017; Carlile et al., 2018). Farra et al., (2015) built regression models to predict essay scores based on features extracted from opinion expressions and topical elements. Chatterjee et al., (2014) used verbal descriptors and para-verbal markers of hesitation to predict speakers’ persuasiveness on website housing videos of product reviews. When looking at persuasion in the context of online forum discussions (Wei et al., 2016), Tan et al., (2016) found that on the Change My View subreddit, interaction dynamics such as the language interplay between opinion holders and other participants provides highly predictive cues for persuasiveness. Us"
N19-1364,P16-2089,0,0.0142165,"2019, pages 3620–3630 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics show that our semi-supervised model outperforms several baselines. We then apply this automated model to unseen requests from different domains and obtain nuanced findings of the importance of different strategies on persuasion success. Our model can be useful in any situation in which we have exogenous document-level supervision, but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim"
N19-1364,N18-1146,1,0.848177,"s highly predictive cues for persuasiveness. Using the same dataset, Wel et al., (2016) extracted a set of textual information and social interaction features to identify persuasive posts. Recently, Pryzant et al., (2017) introduced a neural network with an adversarial objective to select text features that are predictive of some outcomes but decorrelated with others and further analyzed the narratives highlighted by such text features. Further work extended the model to induce narrative persuasion lexicons predictive of enrollment from course descriptions and sales from product descriptions (Pryzant et al., 2018a), and the efficacy of search advertisements (Pryzant et al., 2018b). Similar to their settings, we use the outcomes of a persuasive description to supervise the learning of persuasion tactics, and our model can similarly induce lexicons associated with successful narrative persuasion by examining highly attentional words associated with persuasion outcomes. Our work differs both in our semisupervised method and also because we explicitly draw on the theoretical literature to model the persuasion strategy for each sentence in requests, allowing requests to have multiple persuasion strategies;"
N19-1364,P16-2032,0,0.0625096,"s people’s attitudes and behaviors have received less attention from the computational community than argumentation, although there have been important preliminary work (Persing and Ng, 2017; Carlile et al., 2018). Farra et al., (2015) built regression models to predict essay scores based on features extracted from opinion expressions and topical elements. Chatterjee et al., (2014) used verbal descriptors and para-verbal markers of hesitation to predict speakers’ persuasiveness on website housing videos of product reviews. When looking at persuasion in the context of online forum discussions (Wei et al., 2016), Tan et al., (2016) found that on the Change My View subreddit, interaction dynamics such as the language interplay between opinion holders and other participants provides highly predictive cues for persuasiveness. Using the same dataset, Wel et al., (2016) extracted a set of textual information and social interaction features to identify persuasive posts. Recently, Pryzant et al., (2017) introduced a neural network with an adversarial objective to select text features that are predictive of some outcomes but decorrelated with others and further analyzed the narratives highlighted by such tex"
N19-1364,W18-5415,0,0.0153708,"s highly predictive cues for persuasiveness. Using the same dataset, Wel et al., (2016) extracted a set of textual information and social interaction features to identify persuasive posts. Recently, Pryzant et al., (2017) introduced a neural network with an adversarial objective to select text features that are predictive of some outcomes but decorrelated with others and further analyzed the narratives highlighted by such text features. Further work extended the model to induce narrative persuasion lexicons predictive of enrollment from course descriptions and sales from product descriptions (Pryzant et al., 2018a), and the efficacy of search advertisements (Pryzant et al., 2018b). Similar to their settings, we use the outcomes of a persuasive description to supervise the learning of persuasion tactics, and our model can similarly induce lexicons associated with successful narrative persuasion by examining highly attentional words associated with persuasion outcomes. Our work differs both in our semisupervised method and also because we explicitly draw on the theoretical literature to model the persuasion strategy for each sentence in requests, allowing requests to have multiple persuasion strategies;"
N19-1364,N16-1174,1,0.694918,"Missing"
N19-1364,D14-1006,0,0.0449685,", but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and classified a set of argumentative actions associated with successful writing such as warrant/reasoning/backing, rebuttal/reservation, and claims/ideas. Habernal and Gurevych (2016) investigated the persuasiveness of arguments in any given argument pair using bidirectional LSTM. Hidey et al., (2017) utilized the persuasive modes—ethos, logos, pathos—to model premises a"
N19-1364,W15-0616,0,0.0135048,"fferent strategies on persuasion success. Our model can be useful in any situation in which we have exogenous document-level supervision, but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly, Zhang and Litman (2015) studied student essay revisions and classified a set of argumentative actions associated with successful writing such as warrant/reasoning/backing, rebuttal/reservation, and claims/ideas. Habernal and Gurevych (2016) investigated the persuasiveness of arguments in any"
N19-1364,C16-1246,0,0.0447825,"Missing"
N19-1364,J17-3005,0,0.021144,"30 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics show that our semi-supervised model outperforms several baselines. We then apply this automated model to unseen requests from different domains and obtain nuanced findings of the importance of different strategies on persuasion success. Our model can be useful in any situation in which we have exogenous document-level supervision, but only small amounts of expensive human-annotated sentence labels. 2 Related Work Computational argumentation has received much recent attention (Ghosh et al., 2016; Stab and Gurevych, 2017; Peldszus and Stede, 2013; Stab et al., 2018; Ghosh et al., 2014). Most work has either identified the arguments in news articles (Sardianos et al., 2015) or user-generated web content (Habernal and Gurevych, 2017; Musi et al., 2018), or classified argument components (Zhang and Litman, 2015) into claims and premises, supporting and opposing claims, or backings, rebuttals and refutations . For example, Stab and Gurevych (2014) proposed structural, lexical, syntactic and contextual features to identify convincing components of Web arguments including claim, major claim, and premise. Similarly,"
O08-6002,W06-2911,0,0.0394,"Missing"
O08-6002,P08-1077,0,0.0275794,"Missing"
O08-6002,D07-1108,0,0.0406125,"Missing"
O08-6002,W02-0817,0,0.0522505,"Missing"
O08-6002,N06-2015,1,0.801507,"Missing"
O08-6002,S01-1004,0,0.0257832,"Missing"
O08-6002,D07-1113,1,0.854748,"Missing"
O08-6002,W02-1006,0,0.0386323,"Missing"
O08-6002,W97-0207,0,0.201173,"Missing"
O08-6002,H93-1061,0,0.364807,"Missing"
O08-6002,P96-1006,0,0.176477,"Missing"
O08-6002,W04-2807,0,0.0764947,"Missing"
O08-6002,J05-1004,0,0.101837,"Missing"
O08-6002,S07-1016,0,0.0311514,"Missing"
O08-6002,D07-1107,0,0.0337662,"Missing"
O08-6002,P07-1006,0,0.0264734,"Missing"
O08-6002,S07-1057,0,0.0583238,"Missing"
O08-6002,C04-1164,1,0.900351,"Missing"
O08-6002,P08-1089,0,0.0257891,"Missing"
O08-6002,D07-1082,1,0.893155,"Missing"
O08-6002,N07-1025,0,\N,Missing
O08-6002,S07-1074,0,\N,Missing
P02-1006,C02-1042,1,0.27781,"proves to be very significant as it helps to eliminate dubious patterns, which may appear because the contents of two or more websites may be the same, or the same web document reappears in the search engine output for algorithms 1 and 2. Algorithm 1 does not explicitly specify any particular question type. Judicious choice of the QA example pair therefore allows it to be used for many question types without change. 3 Finding Answers Using the patterns to answer a new question we employ the following algorithm: 1. Determine the question type of the new question. We use our existing QA system (Hovy et al., 2002b; 2001) to do so. 2. The question term in the question is identified, also using our existing system. 3. Create a query from the question term and perform IR (by using a given answer document corpus such as the TREC-10 collection or web search otherwise). 4. Segment the documents obtained into sentences and smooth out white space variations and html and other tags, as before. 5. Replace the question term in each sentence by the question tag (“<NAME>”, in the case of BIRTHYEAR). 6. Using the pattern table developed for that particular question type, search for the presence of each pattern. Sel"
P02-1006,C02-1026,0,0.132728,"> &apos; s <NAME> . 1.0 1.0 1.0 1.0 1.0 1.0 0.96 0.92 0.92 regional : <ANSWER> : <NAME> to <ANSWER> &apos; s <NAME> , <ANSWER> &apos; s <NAME> in in <ANSWER> &apos; s <NAME> , of <ANSWER> &apos; s <NAME> , at the <NAME> in <ANSWER> the <NAME> in <ANSWER> , from <ANSWER> &apos; s <NAME> near <NAME> in <ANSWER> For each question type, we extracted the corresponding questions from the TREC-10 set. These questions were run through the testing phase of the algorithm. Two sets of experiments were performed. In the first case, the TREC corpus was used as the input source and IR was performed by the IR component of our QA system (Lin, 2002). In the second case, the web was the input source and the IR was performed by the AltaVista search engine. Results of the experiments, measured by Mean Reciprocal Rank (MRR) score (Voorhees, 01), are: TREC Corpus Question type BIRTHYEAR INVENTOR DISCOVERER DEFINITION WHY-FAMOUS LOCATION Number of questions 8 6 4 102 3 16 MRR on TREC docs 0.48 0.17 0.13 0.34 0.33 0.75 Number of questions 8 6 4 102 3 16 MRR on the Web 0.69 0.58 0.88 0.39 0.00 0.86 Web Question type BIRTHYEAR INVENTOR DISCOVERER DEFINITION WHY-FAMOUS LOCATION The results indicate that the system performs better on the Web data t"
P02-1006,A00-1023,0,0.00825422,"Missing"
P02-1058,J93-1003,0,0.0966277,"Missing"
P02-1058,C00-1072,1,0.481811,"EBCL -SIGN ATURE -KUCAN :L EX 0. 63636 36363 63636 ))))) )) Figure 1. Sample key concept structure. In a key step for locating important sentences, NeATS computes the likelihood ratio λ (Dunning, 1993) to identify key concepts in unigrams, bigrams, and trigrams1, using the on- topic document collection as the relevant set and the off-topic document collection as the irrelevant set. Figure 1 shows the top 5 concepts with their relevancy scores (-2λ) for the topic “Slovenia Secession from Yugoslavia ” in the DUC-2001 test collection. This is similar to the idea of topic signature introduced in (Lin and Hovy 2000). With the individual key concepts available, we proceed to cluster these concepts in order to identify major subtopics within the main topic. Clusters are formed through strict lexical connection. For example, Milan and Kucan are grouped as “Milan Kucan” since “Milan Kucan” is a key bigram concept; while Croatia, Yugoslavia, Slovenia, republic, and are joined due to the connections as follows: • Slovenia Croatia • Croatia Slovenia • Yugoslavia Slovenia • republic Slovenia 1 Closed class words (of, in, and, are, and so on) were ignored in constructing unigrams, bigrams and trigrams. • Croatia"
P02-1058,W01-0100,0,0.254104,"d in an effectiv e way? express all, most, some or hardly any of the content of the current model unit. 4 Evaluation Metrics One goal of DUC-2001 was to debug the evaluation procedures and identify stable metrics that could serve as common reference points. NIST did not define any official performance metric in DUC-2001. It released the raw evaluation results to DUC -2001 participants and encouraged them to propose metrics that would help progress the field. 4.1.1 Recall, Coverage, Retention and Weighted Retention Recall at different compression ratios has been used in summarization research (Mani 2001) to measure how well an automatic system retains important content of original documents. Assume we have a system summary Ss and a model summary Sm. The number of sentences occurring in both S s and S m is N a, the number of sentences in Ss is N s, and the number of sentences in Sm is N m. Recall is defined as Na/Nm. The Compression Ratio is defined as the length of a summary (by words or sentences) divided by the length of its original document. DUC-2001 set the compression lengths to 50, 100, 200, and 400 words for the multi-document summarization task. However, applying recall in DUC -2001"
P02-1058,J98-3005,0,\N,Missing
P03-1001,P99-1008,0,0.0615538,"Missing"
P03-1001,C02-1130,1,0.579111,"he TREC 11 Question Answering track (Hermjakob et al., 2002). Although both systems supply multiple possible answers for a question, evaluations were conducted on only one answer.4 For TextMap, this answer is just the output with highest confidence, i.e., the system’s first answer. For the extracted instances, the answer was that concept-instance pair that appeared most frequently in the list of extracted examples. If all pairs appear with equal frequency, a selection is made at random. Answers for both systems are then classified by hand into three categories based upon their “director.” See Fleischman and Hovy (2002) for techniques useful in disambiguating such instances. 4 Integration of multiple answers is an open research question and is not addressed in this work. information content. 5 Answers that unequivocally identify an instance’s celebrity (e.g., “Jennifer Capriati is a tennis star”) are marked correct. Answers that provide some, but insufficient, evidence to identify the instance’s celebrity (e.g., “Jennifer Capriati is a defending champion”) are marked partially correct. Answers that provide no information to identify the instance’s celebrity (e.g., “Jennifer Capriati is a daughter”) are marke"
P03-1001,C92-2082,0,0.0843486,"d sentence segmented, and part of speech tagged using Brill’s tagger (Brill, 1994). 3.2 Extraction Patterns Part of speech patterns were generated to take advantage of two syntactic constructions that often indicate concept-instance relationships: common noun/proper noun constructions (CN/PN) and appositions (APOS). Mann (2002) notes that concept-instance relationships are often expressed by a syntactic pattern in which a proper noun follows immediately after a common noun. Such patterns (e.g. “president George Bush”) are very productive and occur 40 times more often than patterns employed by Hearst (1992). Table 1 shows the regular expression used to extract such patterns along with examples of extracted patterns. ${NNP}*${VBG}*${JJ}*${NN}+${NNP}+ trainer/NN Victor/NNP Valle/NNP ABC/NN spokesman/NN Tom/NNP Mackin/NNP official/NN Radio/NNP Vilnius/NNP German/NNP expert/NN Rriedhart/NNP Dumez/NN Investment/NNP Table 1. The regular expression used to extract CN/PN patterns (common noun followed by proper noun). Examples of extracted text are presented below. Text in bold indicates that the example is judged illegitimate. ${NNP}+s*,/,s*${DT}*${JJ}*${NN}+(?:of/IN)* s*${NNP}*${NN}*${IN}*${DT}*$"
P03-1001,P02-1006,1,0.395611,"ate Question Answering, repositories of data much larger than that described here must be generated. We imagine huge data warehouses where each repository contains relations, such as birthplace-of, location-of, creator-of, etc. These repositories would be automatically filled by a system that continuously watches various online news sources, scouring them for useful information. Such a system would have a large library of extraction patterns for many different types of relations. These patterns could be manually generated, such as the ones described here, or learned from text, as described in Ravichandran and Hovy (2002). Each pattern would have a machine-learned filter in order to insure high precision output relations. These relations would then be stored in repositories that could be quickly and easily searched to answer user queries. 7 In this way, we envision a system similar to (Lin et al., 2002). However, instead of relying on costly structured databases and pain stakingly generated wrappers, repositories are automatically filled with information from many different patterns. Access to these repositories does not require wrapper generation, because all information is stored in easily accessible natural"
P03-1001,W02-1111,0,\N,Missing
P03-1001,P01-1005,0,\N,Missing
P03-2021,A97-1029,0,0.0135954,"ne and it is not used for summarization. Note also that the sentence that starts with “However,...” scored much lower than the selected two – its color is approximately half diluted into the background. There are quite a few sentences in the second part of the document that scored relatively high. However, these sentences are below the sentence position cutoff so they do not appear in the summary. We illustrate this by rendering such sentences in slanted style. 3.3 Alternative Summaries The bottom part of the summary panel is occupied by the map-based visualization. We use BBN’s IdentiFinder (Bikel et al., 1997) to detect the names of geographic locations in the document set. We then select the most frequently used location names and place them on world map. Each location is identified by a black dot followed by a frequency chart and the location name. The frequency chart is a bar chart where each bar corresponds to a document. The bar is painted using the document color and the length of the bar is proportional to the number of times the location name is used in the document. The document set we used in our example describes the progress of the hurricane Andrew and its effect on Florida, Louisiana,"
P03-2021,J93-1003,0,0.0305385,"ich addresses these three directions. The iNeATS system is built on top of the NeATS multi-document summarization system. In the following section we give a brief overview of the NeATS system and in Section 3 describe the interactive version. 2 NeATS NeATS (Lin and Hovy, 2002) is an extractionbased multi-document summarization system. It is among the top two performers in DUC 2001 and 2002 (Over, 2001). It consists of three main components: Content Selection The goal of content selection is to identify important concepts mentioned in a document collection. NeATS computes the likelihood ratio (Dunning, 1993) to identify key concepts in unigrams, bigrams, and trigrams and clusters these concepts in order to identify major subtopics within the main topic. Each sentence in the document set is then ranked, using the key concept structures. These n-gram key concepts are called topic signatures. Content Filtering NeATS uses three different filters: sentence position, stigma words, and redundancy filter. Sentence position has been used as a good important content filter since the late 60s (Edmundson, 1969). NeATS applies a simple sentence filter that only retains the N lead sentences. Some sentences sta"
P03-2021,P02-1058,1,\N,Missing
P03-2021,C94-2144,0,\N,Missing
P05-1037,J96-1002,0,0.0168857,"Missing"
P05-1037,P04-1085,0,0.0564718,"ents using preprocessed email messages and context information from previous emails in the thread. Rambow et al. (2004) show that sentence extraction techniques are applicable to summarizing email threads, but only with added email-specific features. Wan and McKeown (2004) introduce a system that creates overview summaries for ongoing decision-making email exchanges by first detecting the issue being discussed and then extracting the response to the issue. Both systems use a corpus that, on average, contains 190 words and 3.25 messages per thread, much shorter than the ones in our collection. Galley et al. (2004) describe a system that identifies agreement and disagreement occurring in human-to-human multi-party conversations. They utilize an important concept from conversational analysis, adjacent pairs (AP), which consists of initiating and responding utterances from different speakers. Identifying APs is also required by our research to find correspondences from different chat participants. In automatic summarization of spoken dialogues, Zechner (2001) presents an approach to obtain extractive summaries for multi-party dialogues in unrestricted domains by addressing in299 trinsic issues specific to"
P05-1037,P94-1002,0,0.437392,"Missing"
P05-1037,N04-4027,0,0.0639814,"ages into subtopic groups and extracting top-ranked sentences per subtopic group based on the intrinsic scores of position in the cluster and lexical centrality. Due to the technical nature of our working corpus, we had to handle intra-message topic shifts, in which the author of a message raises or responds to multiple issues in the same message. This requires that our clustering component be not message-based but submessage-based. Lam et al. (2002) employ an existing summarizer for single documents using preprocessed email messages and context information from previous emails in the thread. Rambow et al. (2004) show that sentence extraction techniques are applicable to summarizing email threads, but only with added email-specific features. Wan and McKeown (2004) introduce a system that creates overview summaries for ongoing decision-making email exchanges by first detecting the issue being discussed and then extracting the response to the issue. Both systems use a corpus that, on average, contains 190 words and 3.25 messages per thread, much shorter than the ones in our collection. Galley et al. (2004) describe a system that identifies agreement and disagreement occurring in human-to-human multi-par"
P05-1037,C04-1079,0,0.290243,"ntrality. Due to the technical nature of our working corpus, we had to handle intra-message topic shifts, in which the author of a message raises or responds to multiple issues in the same message. This requires that our clustering component be not message-based but submessage-based. Lam et al. (2002) employ an existing summarizer for single documents using preprocessed email messages and context information from previous emails in the thread. Rambow et al. (2004) show that sentence extraction techniques are applicable to summarizing email threads, but only with added email-specific features. Wan and McKeown (2004) introduce a system that creates overview summaries for ongoing decision-making email exchanges by first detecting the issue being discussed and then extracting the response to the issue. Both systems use a corpus that, on average, contains 190 words and 3.25 messages per thread, much shorter than the ones in our collection. Galley et al. (2004) describe a system that identifies agreement and disagreement occurring in human-to-human multi-party conversations. They utilize an important concept from conversational analysis, adjacent pairs (AP), which consists of initiating and responding utteran"
P05-1077,P89-1010,0,0.173444,"rally not easily web scalable. This kind of feature set does not seem to affect our results. Curran and Moens (2002) also report comparable results for Minipar features and simple word based proximity features. Table 1 gives the characteristics of both corpora. Since we use grammatical context, the feature set is considerably larger than the simple word based proximity feature set for the newspaper corpus. 4.3 Calculating Feature Vectors Having collected all nouns and their features, we now proceed to construct feature vectors (and values) for nouns from both corpora using mutual information (Church and Hanks, 1989). We first construct a frequency count vector C(e) = (ce1 , ce2 , ..., cek ), where k is the total number of features and cef is the frequency count of feature f occurring in word e. Here, cef is the number of times word e occurred in context f . We then construct a mutual information vector M I(e) = (mie1 , mie2 , ..., miek ) for each word e, where mief is the pointwise mutual information between word e and feature f , which is defined as: mief = log Pn cif i=1 N cef N × cej j=1 N Pk (6) where n is the number of words and N = 5 We perform this operation so that we can compare the performance"
P05-1077,P02-1030,0,0.00716698,"pus Corpus Size Unique Nouns Feature size Newspaper 6GB 65,547 940,154 Web 138GB 655,495 1,306,482 the left and right of each noun. We use the context words as features of the noun vector. 4.2 Newspaper Corpus We parse a 6 GB newspaper (TREC9 and TREC2002 collection) corpus using the dependency parser Minipar (Lin, 1994). We identify all nouns. For each noun we take the grammatical context of the noun as identified by Minipar5 . We do not use grammatical features in the web corpus since parsing is generally not easily web scalable. This kind of feature set does not seem to affect our results. Curran and Moens (2002) also report comparable results for Minipar features and simple word based proximity features. Table 1 gives the characteristics of both corpora. Since we use grammatical context, the feature set is considerably larger than the simple word based proximity feature set for the newspaper corpus. 4.3 Calculating Feature Vectors Having collected all nouns and their features, we now proceed to construct feature vectors (and values) for nouns from both corpora using mutual information (Church and Hanks, 1989). We first construct a frequency count vector C(e) = (ce1 , ce2 , ..., cek ), where k is the"
P05-1077,P90-1034,0,0.0927163,"er in the sorted lists. Overall, the algorithm takes O(nk + nlogn) time. However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k. (i.e., n &lt; k). This implies logn &lt;&lt; k and nlogn &lt;&lt; nk. Hence the time complexity of our algorithm is O(nk + nlogn) ≈ O(nk). This is a huge saving from the original O(n2 k) algorithm. In the next section, we proceed to apply this technique for generating noun similarity lists. 4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text (Hindle, 1990; Lin, 1998). The basic intuition is that words that are similar to each other tend to occur in similar contexts, thus linking the semantics of words with their lexical usage in text. One may ask why is clustering of words necessary in the first place? There may be several reasons for clustering, but generally it boils down to one basic reason: if the words that occur rarely in a corpus are found to be distributionally similar to more frequently occurring words, then one may be able to make better inferences on rare words. However, to unleash the real power of clustering one has to work with l"
P05-1077,P98-2127,0,0.058624,"ed lists. Overall, the algorithm takes O(nk + nlogn) time. However, for noun clustering, we generally have the number of nouns, n, smaller than the number of features, k. (i.e., n &lt; k). This implies logn &lt;&lt; k and nlogn &lt;&lt; nk. Hence the time complexity of our algorithm is O(nk + nlogn) ≈ O(nk). This is a huge saving from the original O(n2 k) algorithm. In the next section, we proceed to apply this technique for generating noun similarity lists. 4 Building Noun Similarity Lists A lot of work has been done in the NLP community on clustering words according to their meaning in text (Hindle, 1990; Lin, 1998). The basic intuition is that words that are similar to each other tend to occur in similar contexts, thus linking the semantics of words with their lexical usage in text. One may ask why is clustering of words necessary in the first place? There may be several reasons for clustering, but generally it boils down to one basic reason: if the words that occur rarely in a corpus are found to be distributionally similar to more frequently occurring words, then one may be able to make better inferences on rare words. However, to unleash the real power of clustering one has to work with large amounts"
P05-1077,C94-1079,0,0.00934932,"the total frequency count of all i=1 features of all words. Having thus obtained the feature representation of each noun we can apply the algorithm described in Section 3 to discover similarity lists. We report results in the next section for both corpora. Table 1: Corpus description Corpus Corpus Size Unique Nouns Feature size Newspaper 6GB 65,547 940,154 Web 138GB 655,495 1,306,482 the left and right of each noun. We use the context words as features of the noun vector. 4.2 Newspaper Corpus We parse a 6 GB newspaper (TREC9 and TREC2002 collection) corpus using the dependency parser Minipar (Lin, 1994). We identify all nouns. For each noun we take the grammatical context of the noun as identified by Minipar5 . We do not use grammatical features in the web corpus since parsing is generally not easily web scalable. This kind of feature set does not seem to affect our results. Curran and Moens (2002) also report comparable results for Minipar features and simple word based proximity features. Table 1 gives the characteristics of both corpora. Since we use grammatical context, the feature set is considerably larger than the simple word based proximity feature set for the newspaper corpus. 4.3 C"
P05-1077,J90-1003,0,\N,Missing
P05-1077,H01-1052,0,\N,Missing
P05-1077,C98-2122,0,\N,Missing
P06-2063,J96-1002,0,0.0226001,"llect sentences corresponding to those phrases. Figure 1 illustrates the automatic labeling process. Table 1: Classes defined for the classification tasks. Class Description symbol Sentences related to pros in a PR review Sentences related to cons in a CR review Sentences related to neither PR NR nor CR ral language processing, such as Semantic Role labeling, Question Answering, and Information Extraction. Maximum Entropy models implement the intuition that the best model is the one that is consistent with the set of constraints imposed by the evidence but otherwise is as uniform as possible (Berger et al., 1996). We modeled the conditional probability of a class c given a feature vector x as follows: p (c |x ) = 1 exp(∑ λi f i (c, x)) Zx i where Z x is a normalization factor which can be calculated by the following: Z x = ∑ exp(∑ λi f i (c, x)) Figure 1. The automatic labeling process of pros and cons sentences in a review. The system first extracts comma-delimited phrases from each pro and con field, generating two sets of phrases: {P1, P2, …, Pn} for pros and {C1, C2, …, Cm} for cons. In the example in Figure 1, “beautiful display” can be Pi and “not something you want to drop” can be Cj. Then the"
P06-2063,H05-1045,0,0.0228369,"ic orientation classification is a task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005). Sentiment of phrases and sentences has also been studied in (Kim and Hovy, 2004; Wilson et al., 2005). Document level sentiment classification is mostly applied to reviews, where systems assign a positive or negative sentiment for a whole review document (Pang et al., 2002; Turney, 2002). Building on this work, more sophisticated problems in the opinion domain have been studied by many researchers. (Bethard et al., 2004; Choi et al., 2005; Kim and Hovy, 2006) identified the holder (source) of opinions expressed in sentences using various techniques. (Wilson et al., 2004) focused on the strength of opinion clauses, finding strong and weak opinions. (Chklovski, 2006) presented a system that aggregates and quantifies degree assessment of opinions scattered throughout web pages. Beyond document level sentiment classification in online product reviews, (Hu and Liu, 2004; Popescu and Etzioni, 2005) concentrated on mining and summarizing reviews by extracting opinion sentences regarding product features. In this paper, we focus on an"
P06-2063,P97-1023,0,0.0303957,"e to express their opinions online. This trend has raised many interesting and challenging research topics such as subjectivity detection, semantic orientation classification, and review classification. Subjectivity detection is the task of identifying subjective words, expressions, and sentences. (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al, 2003). Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Semantic orientation classification is a task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005). Sentiment of phrases and sentences has also been studied in (Kim and Hovy, 2004; Wilson et al., 2005). Document level sentiment classification is mostly applied to reviews, where systems assign a positive or negative sentiment for a whole review document (Pang et al., 2002; Turney, 2002). Building on this work, more sophisticated problems in the opinion domain have been studied by many researchers. (Bethard et al., 2004; Choi et al., 2005; Kim and Hovy, 2006) identified the holder (source) of opinions expressed in sentences using various techniques."
P06-2063,C04-1200,1,0.095465,"emantic orientation classification, and review classification. Subjectivity detection is the task of identifying subjective words, expressions, and sentences. (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al, 2003). Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Semantic orientation classification is a task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005). Sentiment of phrases and sentences has also been studied in (Kim and Hovy, 2004; Wilson et al., 2005). Document level sentiment classification is mostly applied to reviews, where systems assign a positive or negative sentiment for a whole review document (Pang et al., 2002; Turney, 2002). Building on this work, more sophisticated problems in the opinion domain have been studied by many researchers. (Bethard et al., 2004; Choi et al., 2005; Kim and Hovy, 2006) identified the holder (source) of opinions expressed in sentences using various techniques. (Wilson et al., 2004) focused on the strength of opinion clauses, finding strong and weak opinions. (Chklovski, 2006) prese"
P06-2063,I05-2011,1,0.54828,"(Lin and Hovy, 1997), which may apply because reasons like pros and cons in a review document are most important sentences that summarize the whole point of the review. For opinion-bearing word features, we used pre-selected opinion-bearing words produced by a combination of two methods. The first method derived a list of opinion-bearing words from a large news corpus by separating opinion articles such as letters or editorials from news articles which simply reported news or events. The second method calculated semantic orientations of words based on WordNet2 synonyms. In our previous work (Kim and Hovy, 2005), we demonstrated that the list of words produced by a combination of those two methods performed very well in detecting opinion bearing sentences. Both algorithms are described in that paper. The motivation for including the list of opinion-bearing words as one of our features is that pro and con sentences are quite likely to contain opinion-bearing expressions (even though some of them are only facts), such as “The waiting time was horrible” and “Their portion size of food was extremely generous!” in restaurant reviews. We presumed pro and con sentences containing only facts, such as “The ba"
P06-2063,N06-1026,1,0.412605,"sification is a task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005). Sentiment of phrases and sentences has also been studied in (Kim and Hovy, 2004; Wilson et al., 2005). Document level sentiment classification is mostly applied to reviews, where systems assign a positive or negative sentiment for a whole review document (Pang et al., 2002; Turney, 2002). Building on this work, more sophisticated problems in the opinion domain have been studied by many researchers. (Bethard et al., 2004; Choi et al., 2005; Kim and Hovy, 2006) identified the holder (source) of opinions expressed in sentences using various techniques. (Wilson et al., 2004) focused on the strength of opinion clauses, finding strong and weak opinions. (Chklovski, 2006) presented a system that aggregates and quantifies degree assessment of opinions scattered throughout web pages. Beyond document level sentiment classification in online product reviews, (Hu and Liu, 2004; Popescu and Etzioni, 2005) concentrated on mining and summarizing reviews by extracting opinion sentences regarding product features. In this paper, we focus on another challenging yet"
P06-2063,H05-1043,0,0.194897,"Missing"
P06-2063,W03-0404,0,0.031881,"Missing"
P06-2063,P02-1053,0,0.00633327,"This trend has raised many interesting and challenging research topics such as subjectivity detection, semantic orientation classification, and review classification. Subjectivity detection is the task of identifying subjective words, expressions, and sentences. (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al, 2003). Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Semantic orientation classification is a task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005). Sentiment of phrases and sentences has also been studied in (Kim and Hovy, 2004; Wilson et al., 2005). Document level sentiment classification is mostly applied to reviews, where systems assign a positive or negative sentiment for a whole review document (Pang et al., 2002; Turney, 2002). Building on this work, more sophisticated problems in the opinion domain have been studied by many researchers. (Bethard et al., 2004; Choi et al., 2005; Kim and Hovy, 2006) identified the holder (source) of opinions expressed in sentences using various techniques. (Wilson et al"
P06-2063,H05-1044,0,0.0249858,"classification, and review classification. Subjectivity detection is the task of identifying subjective words, expressions, and sentences. (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al, 2003). Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Semantic orientation classification is a task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005). Sentiment of phrases and sentences has also been studied in (Kim and Hovy, 2004; Wilson et al., 2005). Document level sentiment classification is mostly applied to reviews, where systems assign a positive or negative sentiment for a whole review document (Pang et al., 2002; Turney, 2002). Building on this work, more sophisticated problems in the opinion domain have been studied by many researchers. (Bethard et al., 2004; Choi et al., 2005; Kim and Hovy, 2006) identified the holder (source) of opinions expressed in sentences using various techniques. (Wilson et al., 2004) focused on the strength of opinion clauses, finding strong and weak opinions. (Chklovski, 2006) presented a system that agg"
P06-2063,A97-1042,1,\N,Missing
P06-2063,C00-1044,0,\N,Missing
P06-2063,H05-2017,0,\N,Missing
P06-2063,P99-1032,0,\N,Missing
P06-2063,W02-1011,0,\N,Missing
P07-1129,H05-1121,0,0.0304208,"nts to identify the relevant documents is time consuming and tends to become overwhelming. Individuals need to be able to retrieve the relevant consultation documents efficiently and effectively. Therefore, this work presents a novel mechanism to automatically retrieve the relevant consultation documents with respect to users' problems. Traditional information retrieval systems represent queries and documents using a bag-of-words approach. Retrieval models, such as the vector space model (VSM) (Baeza-Yates and RibeiroNeto, 1999) and Okapi model (Robertson et al., 1995; Robertson et al., 1996; Okabe et al., 2005), are then adopted to estimate the relevance between queries and documents. The VSM represents each query and document as a vector of words, and adopts the cosine measure to estimate their relevance. The Okapi model, which has been used on the Text REtrieval Conference (TREC) collections, developed a family of word-weighting functions Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1024–1031, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Query: Consultation Document I broke up with my boyfriend. &lt;Depressed&gt; I o"
P07-1129,C04-1164,1,0.82514,"Missing"
P08-1119,P99-1008,0,0.881859,"Missing"
P08-1119,P99-1016,0,0.197953,"ar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic c"
P08-1119,P06-1038,0,0.114438,"d outperforming the results reported by others who have worked on the same classes. 2 Related Work A substantial amount of research has been done in the area of semantic class learning, under a variety of different names and with a variety of different goals. Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly sup"
P08-1119,C02-1130,1,0.642619,"ses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around the pattern. Pasca also developed a second technique (Pas¸ca, 2007b) that creates context vectors for a group of seed instances by searching web query logs, and uses them to learn similar instances. The"
P08-1119,N03-1011,0,0.750933,"c class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local an"
P08-1119,C92-2082,0,0.727441,"formation (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around the pattern. Pasca also developed a second technique (Pas¸ca, 2007b) that creates context vectors for a group of seed instances by searching web query logs, and uses them to learn similar instances. The work most closely related to ours is Hearst’s early work on hyponym learning (Hearst, 1992) and more recent work that has followed up on her idea. Hearst’s system exploited patterns that explicitly identify a hyponym relation between a semantic class and a word (e.g., “such authors as Shakespeare”). We will refer t"
P08-1119,C02-1144,0,0.168229,"ing high accuracies and outperforming the results reported by others who have worked on the same classes. 2 Related Work A substantial amount of research has been done in the area of semantic class learning, under a variety of different names and with a variety of different goals. Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH"
P08-1119,P98-2127,0,0.33935,"ses, achieving high accuracies and outperforming the results reported by others who have worked on the same classes. 2 Related Work A substantial amount of research has been done in the area of semantic class learning, under a variety of different names and with a variety of different goals. Given the great deal of similar work in information extraction and ontology learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic clas"
P08-1119,W02-1111,0,0.0186934,"ogy learning, we focus here only on techniques for weakly supervised or unsupervised semantic class (i.e., supertype-based) learning, since that is most related to the work in this paper. Fully unsupervised semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006)) has the disadvantage that it may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <"
P08-1119,N04-1041,0,0.490325,"create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups"
P08-1119,W02-1017,1,0.837817,"ed on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around"
P08-1119,W97-0313,1,0.630135,"sses desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web"
P08-1119,P98-2182,0,0.755248,"th just a class name and one seed instance and then automatically generate a ranked list of new class instances. We conducted experiments on four semantic classes and consistently achieved high accuracies. 1 Introduction Knowing the semantic classes of words (e.g., “trout” is a kind of FISH) can be extremely valuable for many natural language processing tasks. Although some semantic dictionaries do exist (e.g., WordNet (Miller, 1990)), they are rarely complete, especially for large open classes (e.g., classes of people and objects) and rapidly changing categories (e.g., computer technology). (Roark and Charniak, 1998) reported that 3 of every 5 terms generated by their semantic lexicon learner were not present in WordNet. Automatic semantic lexicon acquisition could be used to enhance existing resources such as WordNet, or to produce semantic lexicons for specialized categories or domains. A variety of methods have been developed for automatic semantic class identification, under the rubrics of lexical acquisition, hyponym acquisition, semantic lexicon induction, semantic class learning, and web-based information extraction. Many of these approaches employ surface-level patterns to identify words and their"
P08-1119,E06-1003,0,0.031708,"nstruction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)), and learning semantic relations such as meronymy (Berland and Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic cl"
P08-1119,W02-1028,1,0.802786,"nd Charniak, 1999; Girju et al., 2003). Our research focuses on semantic lexicon induction, which aims to generate lists of words that be1049 long to a given semantic class (e.g., lists of FISH or VEHICLE words). Weakly supervised learning methods for semantic lexicon generation have utilized co-occurrence statistics (Riloff and Shepherd, 1997; Roark and Charniak, 1998), syntactic information (Tanev and Magnini, 2006; Pantel and Ravichandran, 2004; Phillips and Riloff, 2002), lexico-syntactic contextual patterns (e.g., “resides in <location&gt;” or “moved to <location&gt;”) (Riloff and Jones, 1999; Thelen and Riloff, 2002), and local and global contexts (Fleischman and Hovy, 2002). These methods have been evaluated only on fixed corpora1 , although (Pantel et al., 2004) demonstrated how to scale up their algorithms for the web. Several techniques for semantic class induction have also been developed specifically for learning from the web. (Pas¸ca, 2004) uses Hearst’s patterns (Hearst, 1992) to learn semantic class instances and class groups by acquiring contexts around the pattern. Pasca also developed a second technique (Pas¸ca, 2007b) that creates context vectors for a group of seed instances by searching web"
P08-1119,C02-1114,0,0.733507,", 1992) and more recent work that has followed up on her idea. Hearst’s system exploited patterns that explicitly identify a hyponym relation between a semantic class and a word (e.g., “such authors as Shakespeare”). We will refer to these as hyponym patterns. Pasca’s previously mentioned system (Pas¸ca, 2004) applies hyponym patterns to the web and acquires contexts around them. The KnowItAll system (Etzioni et al., 2005) also uses hyponym patterns to extract class instances from the web and then evaluates them further by computing mutual information scores based on web queries. The work by (Widdows and Dorow, 2002) on lexical acquisition is similar to ours because they also use graph structures to learn semantic classes. However, their graph is based entirely on syntactic relations between words, while our graph captures the ability of instances to find each other in a hyponym pattern based on web querying, without any part-ofspeech tagging or parsing. 1 Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on web pages, but used a precompiled corpus of downloaded web pages. 3 Semantic Class Learning with Hyponym Pattern Linkage Graphs 3.1 A Doubly-Anchored Hyponym Pattern Our work was motivated by"
P08-1119,C98-2177,0,\N,Missing
P08-1119,C98-2122,0,\N,Missing
P10-1070,J96-1002,0,0.0127613,"Missing"
P10-1070,C08-1011,0,0.155692,"IAL + ARTEFACT, OBJ + PART) are generally less ambiguous than Levi’s. In contrast to studies that claim the existence of a relatively small number of semantic relations, Downing (1977) presents a strong case for the existence of an unbounded number of relations. While we agree with Downing’s belief that the number of relations is unbounded, we contend that the vast majority of noun compounds fits within a relatively small set of categories. The relations used in computational linguistics vary much along the same lines as those proposed earlier by linguists. Several lines of work (Finin, 1980; Butnariu and Veale, 2008; Nakov, 2008) assume the existence of an unbounded number of relations. Others use categories similar to Levi’s, such as Lauer’s (1995) set of prepositional paraphrases (i.e., OF, FOR , IN , ON , AT, FROM , WITH , ABOUT ) to analyze noun compounds. Some work (e.g., Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2005; Kim and Baldwin, 2005) use sets of categories that are somewhat more similar to those proposed by Warren (1978). While most of the noun compound research to date is not domain specific, Rosario and Hearst (2001) create and experiment with a taxonomy tail"
P10-1070,W09-2416,0,0.233441,"the hierarchical dependency structure of the NP in which they occur—is an important problem within a wide variety of natural language processing (NLP) applications, including machine translation (Baldwin and Tanaka, 2004) and question answering (Ahn et al., 2005). The interpretation of noun compounds is a difficult problem for various reasons (Spärck Jones, 1983). Among them is the fact that no set of relations proposed to date has been accepted as complete and appropriate for general-purpose text. Regardless, automatic noun compound interpretation is the focus of an upcoming S EM E VAL task (Butnariu et al., 2009). Leaving aside the problem of determining the dependency structure among strings of three or more nouns—a problem we do not address in this paper—automatic noun compound interpretation requires a taxonomy of noun-noun relations, an automatic method for accurately assigning the re2 2.1 Related Work Taxonomies The relations between the component nouns in noun compounds have been the subject of various linguistic studies performed throughout the years, including early work by Jespersen (1949). The taxonomies they created are varied. Lees created an early taxonomy based primarily upon grammar (Le"
P10-1070,P07-1072,0,0.538704,"th tone ⊃BNV:Possess*, ≈G:Property, ⊃L:Have2 , ≈W:Obj-Quality 4.51 fighter plane ≈BV:Equative, ⊃G:Type∪IS-A, ≈L:BEbcd , ≈N:Type∪Equality, ≈W:Copula 0.69 skeleton crew ≈W:Resemblance, ⊃G:Type 4.37 hour meeting ≈G:Measure, ⊂N:TimeThrough∪Measure, ≈W:Size-Whole 0.65 pig iron 1.67 contact lens Table 1: The semantic relations, their frequency in the dataset, examples, and approximate relation mappings to previous relation sets. ≈-approximately equivalent; ⊃/⊂-super/sub set; ∞-some overlap; ∪-union; initials BGLNVW refer respectively to the works of (Barker and Szpakowicz, 1998; Girju et al., 2005; Girju, 2007; Levi, 1978; Nastase and Szpakowicz, 2003; Vanderwende, 1994; Warren, 1978). 680 vious work including those of Barker and Szpakowicz (1998) and Girju et al. (2005). The results, shown in Table 1, demonstrate that our taxonomy is similar to several taxonomies used in other work. However, there are three main differences and several less important ones. The first major difference is the absence of a significant THEME or OBJECT category. The second main difference is that our taxonomy does not include a PURPOSE category and, instead, has several smaller categories. Finally, instead of possessing"
P10-1070,J09-2003,0,0.188564,"unds that also permits certain adjectival modifiers) are all derived either via nominalization or 678 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 678–687, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics a novel algorithm (i.e., semantic scattering). Nastase et al. (2006) experiment with a variety of classification methods including memory-based methods, SVMs, and decision trees. Ó Séaghdha and Copestake (2009) use SVMs and experiment with kernel methods on a dataset labeled using a relatively small taxonomy. Girju (2009) uses crosslinguistic information from parallel corpora to aid classification. by deleting one of nine predicates (i.e., CAUSE , HAVE , MAKE , USE , BE , IN , FOR , FROM , ABOUT ) from an underlying sentence construction (Levi, 1978). Of the taxonomies presented by purely linguistic studies, our categories are most similar to those proposed by Warren (1978), whose categories (e.g., MATERIAL + ARTEFACT, OBJ + PART) are generally less ambiguous than Levi’s. In contrast to studies that claim the existence of a relatively small number of semantic relations, Downing (1977) presents a strong case fo"
P10-1070,W04-0404,0,0.0994826,"data. In this paper, we present a novel taxonomy of relations that integrates previous relations, the largest publicly-available annotated dataset, and a supervised classification method for automatic noun compound interpretation. 1 Introduction Noun compounds (e.g., ‘maple leaf’) occur very frequently in text, and their interpretation— determining the relationships between adjacent nouns as well as the hierarchical dependency structure of the NP in which they occur—is an important problem within a wide variety of natural language processing (NLP) applications, including machine translation (Baldwin and Tanaka, 2004) and question answering (Ahn et al., 2005). The interpretation of noun compounds is a difficult problem for various reasons (Spärck Jones, 1983). Among them is the fact that no set of relations proposed to date has been accepted as complete and appropriate for general-purpose text. Regardless, automatic noun compound interpretation is the focus of an upcoming S EM E VAL task (Butnariu et al., 2009). Leaving aside the problem of determining the dependency structure among strings of three or more nouns—a problem we do not address in this paper—automatic noun compound interpretation requires a ta"
P10-1070,P98-1015,0,0.739412,"s belief that the number of relations is unbounded, we contend that the vast majority of noun compounds fits within a relatively small set of categories. The relations used in computational linguistics vary much along the same lines as those proposed earlier by linguists. Several lines of work (Finin, 1980; Butnariu and Veale, 2008; Nakov, 2008) assume the existence of an unbounded number of relations. Others use categories similar to Levi’s, such as Lauer’s (1995) set of prepositional paraphrases (i.e., OF, FOR , IN , ON , AT, FROM , WITH , ABOUT ) to analyze noun compounds. Some work (e.g., Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2005; Kim and Baldwin, 2005) use sets of categories that are somewhat more similar to those proposed by Warren (1978). While most of the noun compound research to date is not domain specific, Rosario and Hearst (2001) create and experiment with a taxonomy tailored to biomedical text. 2.2 3 3.1 Taxonomy Creation Given the heterogeneity of past work, we decided to start fresh and build a new taxonomy of relations using naturally occurring noun pairs, and then compare the result to earlier relation sets. We collected 17509 noun pairs and over a period"
P10-1070,C94-2125,0,0.862881,"variety of NLP research, including recent work on noun compounds by Nakov (2008) to collect short phrases for linking the nouns within noun compounds. For the Mechanical Turk annotation tests, we created five sets of 100 noun compounds from noun compounds automatically extracted from a random subset of New York Times articles written between 1987 and 2007 (Sandhaus, 2008). Each of these sets was used in a separate annotation round. For each round, a set of 100 noun compounds was uploaded along with category definiClassification The approaches used for automatic classification are also varied. Vanderwende (1994) presents one of the first systems for automatic classification, which extracted information from online sources and used a series of rules to rank a set of most likely interpretations. Lauer (1995) uses corpus statistics to select a prepositional paraphrase. Several lines of work, including that of Barker and Szpakowicz (1998), use memory-based methods. Kim and Baldwin (2005) and Turney (2006) use nearest neighbor approaches based upon WordNet (Fellbaum, 1998) and Turney’s Latent Relational Analysis, respectively. Rosario and Hearst (2001) utilize neural networks to classify compounds accordi"
P10-1070,S07-1051,0,0.0160375,"Missing"
P10-1070,W04-2609,0,0.334905,"c classification, which extracted information from online sources and used a series of rules to rank a set of most likely interpretations. Lauer (1995) uses corpus statistics to select a prepositional paraphrase. Several lines of work, including that of Barker and Szpakowicz (1998), use memory-based methods. Kim and Baldwin (2005) and Turney (2006) use nearest neighbor approaches based upon WordNet (Fellbaum, 1998) and Turney’s Latent Relational Analysis, respectively. Rosario and Hearst (2001) utilize neural networks to classify compounds according to their domain-specific relation taxonomy. Moldovan et al. (2004) use SVMs as well as 679 Category Name Causal Group C OMMUNICATOR OF C OMMUNICATION P ERFORMER OF ACT /ACTIVITY C REATOR /P ROVIDER /C AUSE O F Purpose/Activity Group P ERFORM /E NGAGE _I N C REATE /P ROVIDE /S ELL O BTAIN /ACCESS /S EEK M ODIFY /P ROCESS /C HANGE M ITIGATE /O PPOSE /D ESTROY O RGANIZE /S UPERVISE /AUTHORITY P ROPEL P ROTECT /C ONSERVE T RANSPORT /T RANSFER /T RADE T RAVERSE /V ISIT Ownership, Experience, Employment, and Use P OSSESSOR + OWNED /P OSSESSED E XPERIENCER + C OGINITION /M ENTAL E MPLOYER + E MPLOYEE /VOLUNTEER C ONSUMER + C ONSUMED U SER /R ECIPIENT + U SED /R ECE"
P10-1070,W05-0603,0,0.0324509,"Missing"
P10-1070,E09-1071,0,0.153443,"Missing"
P10-1070,P07-3013,0,0.46942,"Missing"
P10-1070,W01-0511,0,0.615364,"guists. Several lines of work (Finin, 1980; Butnariu and Veale, 2008; Nakov, 2008) assume the existence of an unbounded number of relations. Others use categories similar to Levi’s, such as Lauer’s (1995) set of prepositional paraphrases (i.e., OF, FOR , IN , ON , AT, FROM , WITH , ABOUT ) to analyze noun compounds. Some work (e.g., Barker and Szpakowicz, 1998; Nastase and Szpakowicz, 2003; Girju et al., 2005; Kim and Baldwin, 2005) use sets of categories that are somewhat more similar to those proposed by Warren (1978). While most of the noun compound research to date is not domain specific, Rosario and Hearst (2001) create and experiment with a taxonomy tailored to biomedical text. 2.2 3 3.1 Taxonomy Creation Given the heterogeneity of past work, we decided to start fresh and build a new taxonomy of relations using naturally occurring noun pairs, and then compare the result to earlier relation sets. We collected 17509 noun pairs and over a period of 10 months assigned one or more relations to each, gradually building and refining our taxonomy. More details regarding the dataset are provided in Section 4. The relations we produced were then compared to those present in other taxonomies (e.g., Levi, 1978;"
P10-1070,J06-3003,0,0.010662,"d in a separate annotation round. For each round, a set of 100 noun compounds was uploaded along with category definiClassification The approaches used for automatic classification are also varied. Vanderwende (1994) presents one of the first systems for automatic classification, which extracted information from online sources and used a series of rules to rank a set of most likely interpretations. Lauer (1995) uses corpus statistics to select a prepositional paraphrase. Several lines of work, including that of Barker and Szpakowicz (1998), use memory-based methods. Kim and Baldwin (2005) and Turney (2006) use nearest neighbor approaches based upon WordNet (Fellbaum, 1998) and Turney’s Latent Relational Analysis, respectively. Rosario and Hearst (2001) utilize neural networks to classify compounds according to their domain-specific relation taxonomy. Moldovan et al. (2004) use SVMs as well as 679 Category Name Causal Group C OMMUNICATOR OF C OMMUNICATION P ERFORMER OF ACT /ACTIVITY C REATOR /P ROVIDER /C AUSE O F Purpose/Activity Group P ERFORM /E NGAGE _I N C REATE /P ROVIDE /S ELL O BTAIN /ACCESS /S EEK M ODIFY /P ROCESS /C HANGE M ITIGATE /O PPOSE /D ESTROY O RGANIZE /S UPERVISE /AUTHORITY P"
P10-1070,I05-1082,0,\N,Missing
P10-1070,P95-1007,0,\N,Missing
P10-1070,S10-1007,0,\N,Missing
P10-1070,C98-1015,0,\N,Missing
P10-1144,D08-1031,0,0.26509,"ional Linguistics, pages 1423–1432, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pradhan et al., 2007; Hovy e"
P10-1144,A00-1031,0,0.112824,"wo corpora, which differ in the amount and correctness of such information. However, in this experiment, entity mapping was applied in the opposite direction: the OntoNotes entities were mapped onto the automatically preprocessed ACE dataset. This exposes the shortcomings of automated preprocessing in ACE for identifying all the mentions identified and linked in OntoNotes. Datasets The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). Syntactic chunks were obtained from YamCha 1.33, an SVM-based NPchunker (Kudoh and Matsumoto, 2000), and parse trees from Malt Parser 0.4, an SVM-based parser (Hall et al., 2007). Although the number of words in Tables 4 and 6 should in principle be the same, the latter contains fewer words as it lacks the null elements (traces, ellipsed material, etc.) manually annotated in OntoNotes. Missing parse tree nodes in the automatically parsed data account for the considerably lower number of OntoNotes mentions (approx. 5,700 fewer mention"
P10-1144,W00-0730,0,0.0526182,"direction: the OntoNotes entities were mapped onto the automatically preprocessed ACE dataset. This exposes the shortcomings of automated preprocessing in ACE for identifying all the mentions identified and linked in OntoNotes. Datasets The ACE data was morphologically annotated with a tokenizer based on manual rules adapted from the one used in CoNLL (Tjong Kim Sang and De Meulder, 2003), with TnT 2.2, a trigram POS tagger based on Markov models (Brants, 2000), and with the built-in WordNet lemmatizer (Fellbaum, 1998). Syntactic chunks were obtained from YamCha 1.33, an SVM-based NPchunker (Kudoh and Matsumoto, 2000), and parse trees from Malt Parser 0.4, an SVM-based parser (Hall et al., 2007). Although the number of words in Tables 4 and 6 should in principle be the same, the latter contains fewer words as it lacks the null elements (traces, ellipsed material, etc.) manually annotated in OntoNotes. Missing parse tree nodes in the automatically parsed data account for the considerably lower number of OntoNotes mentions (approx. 5,700 fewer mentions).14 However, the proportions of singleton:multi-mention entities as well as the average entity size do not vary. Results and Discussion The ACE scores for the"
P10-1144,H05-1083,0,0.0173296,"me of the same texts but annotated according to their respective guidelines, we can better isolate the effect of differences as well as add the additional dimension of gold preprocessing. Second, we evaluate not only with the MUC and B3 scoring metrics, but also with CEAF. Third, all our experiments use true mentions3 to avoid effects due to spurious system mentions. Finally, including different baselines and variations of the resolution model allows us to reveal biases of the metrics. Coreference resolution systems have been tested on languages other than English only within the ACE program (Luo and Zitouni, 2005), probably due to the fact that coreferentially annotated corpora for other languages are scarce. Thus there has been no discussion of the extent to which systems are portable across languages. This paper studies the case of English and Spanish.4 Several coreference systems have been developed in the past (Culotta et al., 2007; Finkel and Manning, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Ng, 2009). It is not our aim to compete with them. Rather, we conduct three experiments under a specific setup for comparison purposes. To this end, we use a different, neutral, system, and a d"
P10-1144,P04-1018,0,0.0279395,"n each experiment, we compute three baselines (1, 2, 3), and run CISTELL under four different models (4, 5, 6, 7). 1. A LL SINGLETONS. No coreference link is ever created. We include this baseline given the high number of singletons in the datasets, since some evaluation measures are affected by large numbers of singletons. 2. H EAD MATCH. All non-pronominal NPs that have the same head are clustered into the same entity. containing one single mention are referred to as singletons. 6 ‘Cistell’ is the Catalan word for ‘basket.’ 3.3 Features We follow Soon et al. (2001), Ng and Cardie (2002) and Luo et al. (2004) to generate most of the 29 features we use for the pairwise model. These include features that capture information from different linguistic levels: textual strings (head match, substring match, distance, frequency), morphology (mention type, coordination, possessive phrase, gender match, number match), syntax (nominal predicate, apposition, relative clause, grammatical function), and semantic match (named-entity type, is-a type, supertype). 7 The opposite search direction was also tried but gave worse results. 8 Taking the first mention classified as coreferent follows Soon et al. (2001)’s f"
P10-1144,N07-1011,0,0.273093,"sociation for Computational Linguistics, pages 1423–1432, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pra"
P10-1144,H05-1004,0,0.121203,"ere is no agreement on a standard. • MUC (Vilain et al., 1995). It computes the number of links common between the true and system partitions. Recall (R) and precision (P) result from dividing it by the minimum number of links required to specify the true and the system partitions, respectively. • B3 (Bagga and Baldwin, 1998). R and P are computed for each mention and averaged at the end. For each mention, the number of common mentions between the true and the system entity is divided by the number of mentions in the true entity or in the system entity to obtain R and P, respectively. • CEAF (Luo, 2005). It finds the best one-toone alignment between true and system entities. Using true mentions and the φ3 similarity function, R and P are the same and correspond to the number of common mentions between the aligned entities divided by the total number of mentions. 4 Parameter 1: Language The first experiment compared the performance of a coreference resolution system on a Germanic and a Romance language—English and Spanish— to explore to what extent language-specific issues such as zero subjects11 or grammatical gender might influence a system. Although OntoNotes and AnCora are two different c"
P10-1144,P02-1014,0,0.524071,"ment (m1 –m11 , m2 – m11 , etc.).7 When a pair (e.g., m3 –m11 ) is classified as coreferent, additional pairwise checks are performed with all the mentions contained in the (growing) entity basket (e.g., m7 –m11 ). Only if all the pairs are classified as coreferent is the mention under consideration attached to the existing growing entity. Otherwise, the search continues.8 5. S UPER STRONG MATCH. Similar to S TRONG MATCH but with a threshold. Coreference pairwise classifications are only accepted when TiMBL distance is smaller than 0.09.9 6. B EST MATCH. Similar to S TRONG MATCH but following Ng and Cardie (2002)’s best link approach. Thus, the mention under analysis is linked to the most confident mention among the previous ones, using TiMBL’s confidence score. 7. W EAK MATCH. A simplified version of S TRONG MATCH: not all mentions in the growing entity need to be classified as coreferent with the mention under analysis. A single positive pairwise decision suffices for the mention to be clustered into that entity.10 We have an update tonight on [this, the volcano in Mexico, they call El Popo]m3 . . . As the sun rises over [Mt. Popo]m7 tonight, the only hint of the fire storm inside, whiffs of smoke,"
P10-1144,N09-1065,0,0.074866,"e resolution model allows us to reveal biases of the metrics. Coreference resolution systems have been tested on languages other than English only within the ACE program (Luo and Zitouni, 2005), probably due to the fact that coreferentially annotated corpora for other languages are scarce. Thus there has been no discussion of the extent to which systems are portable across languages. This paper studies the case of English and Spanish.4 Several coreference systems have been developed in the past (Culotta et al., 2007; Finkel and Manning, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Ng, 2009). It is not our aim to compete with them. Rather, we conduct three experiments under a specific setup for comparison purposes. To this end, we use a different, neutral, system, and a dataset that is small and different from official ACE test sets despite the fact that it prevents our results from being compared directly with other systems. 3 3.1 Experimental Setup System Description The system architecture used in our experiments, CISTELL, is based on the incrementality of discourse. As a discourse evolves, it constructs a model that is updated with the new information gradually provided. A ke"
P10-1144,doddington-etal-2004-automatic,0,0.279097,"e compare the performance using goldstandard preprocessing information with that using automatic preprocessing tools. Throughout, we apply the three principal coreference evaluation measures in use today: MUC, B3 , and CEAF. We highlight the systematic preferences of each measure to reward different configurations. This raises the difficult question of why one should use one or another evaluation measure, and how one should interpret their differences in reporting changes of performance score due to ‘secondary’ factors like preprocessing information. To this end, we employ three corpora: ACE (Doddington et al., 2004), OntoNotes (Pradhan et al., 2007), and AnCora (Recasens and Mart´ı, 2009). In order to isolate the three parameters as far as possible, we benefit from a 100k-word portion (from the TDT collection) that is common to both ACE and OntoNotes. We apply the same coreference resolution system in all cases. The results show that a system’s score is not informative by itself, as different corpora or corpus parameters lead to different scores. Our goal is not to achieve the best performance to date, but rather to expose various issues raised by the choices of corpus preparation and evaluation measure"
P10-1144,D08-1068,0,0.0363577,"including different baselines and variations of the resolution model allows us to reveal biases of the metrics. Coreference resolution systems have been tested on languages other than English only within the ACE program (Luo and Zitouni, 2005), probably due to the fact that coreferentially annotated corpora for other languages are scarce. Thus there has been no discussion of the extent to which systems are portable across languages. This paper studies the case of English and Spanish.4 Several coreference systems have been developed in the past (Culotta et al., 2007; Finkel and Manning, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Ng, 2009). It is not our aim to compete with them. Rather, we conduct three experiments under a specific setup for comparison purposes. To this end, we use a different, neutral, system, and a dataset that is small and different from official ACE test sets despite the fact that it prevents our results from being compared directly with other systems. 3 3.1 Experimental Setup System Description The system architecture used in our experiments, CISTELL, is based on the incrementality of discourse. As a discourse evolves, it constructs a model that is updated with the new"
P10-1144,P08-2012,0,0.526619,"2010. 2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pradhan et al., 2007; Hovy et al., 2006). Unlike ACE, it is not application-oriented, so coref"
P10-1144,D09-1120,0,0.415688,"r Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the largest for English—is OntoNotes (Pradhan et al., 2007; Hovy et al., 2006). Unlike ACE, it is not application-oriented, so coreference relations between al"
P10-1144,D07-1097,0,0.0738498,"Missing"
P10-1144,S10-1001,1,0.899656,"Missing"
P10-1144,N06-2015,1,0.760547,"Missing"
P10-1144,J01-4004,0,0.892741,"23 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1423–1432, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially anno"
P10-1144,P09-1074,0,0.0837024,"emantic information. Since the MUC and ACE corpora are annotated with only coreference information,2 existing systems first preprocess the data using automatic tools (POS taggers, parsers, etc.) to obtain the information needed for coreference resolution. However, given that the output from automatic tools is far from perfect, it is hard to determine the level of performance of a coreference module acting on gold-standard preprocessing information. OntoNotes makes it possible to separate the coreference resolution problem from other tasks. Our study adds to the previously reported evidence by Stoyanov et al. (2009) that differences in corpora and in the task definitions need to be taken into account when comparing coreference resolution systems. We provide new insights as the current analysis differs in four ways. First, Stoyanov 1 The ACE-2004/05 semantic types are person, organization, geo-political entity, location, facility, vehicle, weapon. 2 ACE also specifies entity types and relations. et al. (2009) report on differences between MUC and ACE, while we contrast ACE and OntoNotes. Given that ACE and OntoNotes include some of the same texts but annotated according to their respective guidelines, we"
P10-1144,uryupina-2006-coreference,0,0.0172039,"he 48th Annual Meeting of the Association for Computational Linguistics, pages 1423–1432, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics and 6, we describe the experiments on three different datasets and discuss the results. We conclude in Section 7. 2 Background The bulk of research on automatic coreference resolution to date has been done for English and used two different types of corpus: MUC (Hirschman and Chinchor, 1997) and ACE (Doddington et al., 2004). A variety of learning-based systems have been trained and tested on the former (Soon et al., 2001; Uryupina, 2006), on the latter (Culotta et al., 2007; Bengtson and Roth, 2008; Denis and Baldridge, 2009), or on both (Finkel and Manning, 2008; Haghighi and Klein, 2009). Testing on both is needed given that the two annotation schemes differ in some aspects. For example, only ACE includes singletons (mentions that do not corefer) and ACE is restricted to seven semantic types.1 Also, despite a critical discussion in the MUC task definition (van Deemter and Kibble, 2000), the ACE scheme continues to treat nominal predicates and appositive phrases as coreferential. A third coreferentially annotated corpus—the"
P10-1144,J00-4005,0,0.116358,"Missing"
P10-1144,M95-1005,0,0.167295,"ink approach. 9 In TiMBL, being a memory-based learner, the closer the distance to an instance, the more confident the decision. We chose 0.09 because it appeared to offer the best results. 10 S TRONG and W EAK MATCH are similar to Luo et al. (2004)’s entity-mention and mention-pair models. 1425 For Spanish, we use 34 features as a few variations are needed for language-specific issues such as zero subjects (Recasens and Hovy, 2009). 3.4 Evaluation Since they sometimes provide quite different results, we evaluate using three coreference measures, as there is no agreement on a standard. • MUC (Vilain et al., 1995). It computes the number of links common between the true and system partitions. Recall (R) and precision (P) result from dividing it by the minimum number of links required to specify the true and the system partitions, respectively. • B3 (Bagga and Baldwin, 1998). R and P are computed for each mention and averaged at the end. For each mention, the number of common mentions between the true and the system entity is divided by the number of mentions in the true entity or in the system entity to obtain R and P, respectively. • CEAF (Luo, 2005). It finds the best one-toone alignment between true"
P10-1144,W03-0419,0,\N,Missing
P10-1144,M98-1029,0,\N,Missing
P10-1150,P08-1004,0,0.0481153,"actic patterns (Riloff and Jones, 1999; Snow et al., 2005; Etzioni et al., 2005). Some algorithms require ten seeds (Riloff and Jones, 1999; Igo and Riloff, 2009), while others use a variation of 5, 10, to even 25 seeds (Talukdar et al., 2008). Seeds may be chosen at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the most frequent terms of the desired class (Igo and Riloff, 2009), or by asking humans (Pantel et al., 2009). As (Pantel et al., 2009) show, picking seeds that yield high numbers of different terms is difficult. Thus, when dealing with unbounded sets of relations (Banko and Etzioni, 2008), providing many seeds becomes unrealistic. Interestingly, recent work reports a class of patterns that use only one seed to learn as much information with only one seed. (Kozareva et al., 2008; Hovy et al., 2009) introduce the so-called doublyanchored pattern (DAP) that has two anchor seed positions “htypei such as hseedi and *”, plus one open position for the terms to be learned. Learned terms can then be replaced into the seed position automatically, creating a recursive procedure that is reportedly much more accurate and has much higher final yield. (Kozareva et al., 2008; Hovy et al., 200"
P10-1150,P08-1119,1,0.663847,"he given semantic relation, indicating that living things like people can fly to cities and events, while non-living things like airlines fly mainly to cities. This is a significant improvement over systems that output a flat list of lexical semantic knowledge (Thelen and Riloff, 2002; Yates et al., 2007; Suchanek et al., 2007). 2 Related Work A substantial body of work has been done in attempts to harvest bits of semantic information, including: semantic lexicons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). When research focus"
P10-1150,C02-1144,0,0.0192029,"ty and event of the second. This information provides the selectional restrictions of the given semantic relation, indicating that living things like people can fly to cities and events, while non-living things like airlines fly mainly to cities. This is a significant improvement over systems that output a flat list of lexical semantic knowledge (Thelen and Riloff, 2002; Yates et al., 2007; Suchanek et al., 2007). 2 Related Work A substantial body of work has been done in attempts to harvest bits of semantic information, including: semantic lexicons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff a"
P10-1150,P07-1030,0,0.00963391,"r) and one open position for the term to be learned. Most researchers use singly-anchored patterns to harvest semantic relations. Unfortunately, these patterns run out of steam very quickly. To surmount this obstacle, a handful of seeds is generally used, and helps to guarantee diversity in the extraction of new lexicosyntactic patterns (Riloff and Jones, 1999; Snow et al., 2005; Etzioni et al., 2005). Some algorithms require ten seeds (Riloff and Jones, 1999; Igo and Riloff, 2009), while others use a variation of 5, 10, to even 25 seeds (Talukdar et al., 2008). Seeds may be chosen at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the most frequent terms of the desired class (Igo and Riloff, 2009), or by asking humans (Pantel et al., 2009). As (Pantel et al., 2009) show, picking seeds that yield high numbers of different terms is difficult. Thus, when dealing with unbounded sets of relations (Banko and Etzioni, 2008), providing many seeds becomes unrealistic. Interestingly, recent work reports a class of patterns that use only one seed to learn as much information with only one seed. (Kozareva et al., 2008; Hovy et al., 2009) introduce the so-called doublyanchored pattern (DAP) that"
P10-1150,N04-1041,0,0.05365,"icons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). When research focuses on a particular relation, careful attention is paid to the pattern(s) that express it in various ways (as in most of the work above, notably (Riloff and Jones, 1999)). But it Knowing the sectional restrictions of a semantic relation supports inference in many applications, for example enabling more accurate information extraction. (Igo and Riloff, 2009) report that patterns like “attack on hNPi” can learn undesirable words due to idiomatic expressions and parsing"
P10-1150,C02-1130,1,0.661444,"(Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). When research focuses on a particular relation, careful attention is paid to the pattern(s) that express it in various ways (as in most of the work above, notably (Riloff and Jones, 1999)). But it Knowing the sectional restrictions of a semantic relation supports inference in many applications, for example enabling more accurate information extraction. (Igo and Riloff, 2009) report that patterns like “attack on hNPi” can learn undesirable words due to idiomatic expressions and parsing errors. Over time this becomes problematic for the bootstrapping process and leads"
P10-1150,N03-1011,0,0.476956,"duction Building and maintaining knowledge-rich resources is of great importance to information extraction, question answering, and textual entailment. Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al., 2007), and concept lists (Katz et al., 2003). Researchers have also successfully harvested relations between entities, such as is-a (Hearst, 1992; Pasca, 2004) and part-of (Girju et al., 2003). The kinds of knowledge learned are generally of two kinds: ground instance facts (New York is-a city, Rome is the capital of Italy) and general relational types (city is-a location, engines are part-of cars). A variety of NLP tasks involving inference or entailment (Zanzotto et al., 2006), including QA (Katz and Lin, 2003) and MT (Mt et al., 1988), require a slightly different form of knowledge, derived from many more relations. This knowledge is usually used to support inference and is expressed as selectional restrictions (Wilks, 1975) (namely, the types of arguments that may fill a given"
P10-1150,C92-2082,0,0.524635,"sity of the harvested knowledge. 1 Introduction Building and maintaining knowledge-rich resources is of great importance to information extraction, question answering, and textual entailment. Given the endless amount of data we have at our disposal, many efforts have focused on mining knowledge from structured or unstructured text, including ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al., 2007), and concept lists (Katz et al., 2003). Researchers have also successfully harvested relations between entities, such as is-a (Hearst, 1992; Pasca, 2004) and part-of (Girju et al., 2003). The kinds of knowledge learned are generally of two kinds: ground instance facts (New York is-a city, Rome is the capital of Italy) and general relational types (city is-a location, engines are part-of cars). A variety of NLP tasks involving inference or entailment (Zanzotto et al., 2006), including QA (Katz and Lin, 2003) and MT (Mt et al., 1988), require a slightly different form of knowledge, derived from many more relations. This knowledge is usually used to support inference and is expressed as selectional restrictions (Wilks, 1975) (namely"
P10-1150,P06-1100,0,0.370756,"domain these approaches are both very promising, but when tackling an unbounded number of relations they are unrealistic. The quality of clustering decreases as the domain becomes more continuously varied and diverse, and it has proven difficult to create collections of effective patterns and high-yield seeds manually. In addition, the output of most harvesting systems is a flat list of lexical semantic expressions such as “New York is-a city” and “virus causes flu”. However, using this knowledge in inference requires it to be formulated appropriately and organized in a semantic repository. (Pennacchiotti and Pantel, 2006) proposed an algorithm for automatically ontologizing semantic relations into WordNet. However, despite its high precision entries, WordNet’s limited coverage makes it impossible for relations whose arguments are not present in WordNet to be incorporated. One would like a procedure that dynamically organizes and extends 1482 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1482–1491, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics its semantic repository in order to be able to accommodate all newly-harvested infor"
P10-1150,D09-1099,1,0.741071,"l, 2006) made an attempt to ontologize the harvested arguments of is-a, part-of, and cause relations. They mapped each argument of the relation into WordNet and identified the senses for which the relation holds. Unfortunately, despite its very high precision entries, WordNet is known to have limited coverage, which makes it impossible for algorithms to map the content of a relation whose arguments are not present in WordNet. To surmount this limitation, we do not use WordNet, but employ a different method of obtaining superclasses of a filler term: the inverse doubly-anchored patterns DAP−1 (Hovy et al., 2009), which, given two arguments, harvests its supertypes from the source corpus. (Hovy et al., 2009) show that DAP−1 is reliable and it enriches WordNet with additional hyponyms and hypernyms. 3 Recursive Patterns A singly-anchored pattern contains one example of the seed term (the anchor) and one open position for the term to be learned. Most researchers use singly-anchored patterns to harvest semantic relations. Unfortunately, these patterns run out of steam very quickly. To surmount this obstacle, a handful of seeds is generally used, and helps to guarantee diversity in the extraction of new l"
P10-1150,W09-1703,0,0.744377,"ng (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). When research focuses on a particular relation, careful attention is paid to the pattern(s) that express it in various ways (as in most of the work above, notably (Riloff and Jones, 1999)). But it Knowing the sectional restrictions of a semantic relation supports inference in many applications, for example enabling more accurate information extraction. (Igo and Riloff, 2009) report that patterns like “attack on hNPi” can learn undesirable words due to idiomatic expressions and parsing errors. Over time this becomes problematic for the bootstrapping process and leads to significant deterioration in performance. (Thelen and Riloff, 2002) address this problem by learning multiple semantic categories simultaneously, relying on the often unrealistic assumption that a word cannot belong to more than one semantic category. How1483 ever, if we have at our disposal a repository of semantic relations with their selectional restrictions, the problem addressed in (Igo and Ri"
P10-1150,W97-0313,0,0.0634208,"are supertypes of the first argument and city and event of the second. This information provides the selectional restrictions of the given semantic relation, indicating that living things like people can fly to cities and events, while non-living things like airlines fly mainly to cities. This is a significant improvement over systems that output a flat list of lexical semantic knowledge (Thelen and Riloff, 2002; Yates et al., 2007; Suchanek et al., 2007). 2 Related Work A substantial body of work has been done in attempts to harvest bits of semantic information, including: semantic lexicons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), a"
P10-1150,P98-2182,0,0.014557,"ts of semantic information, including: semantic lexicons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et al., 2007). A variety of techniques have been employed, including clustering (Lin and Pantel, 2002), co-occurrence statistics (Roark and Charniak, 1998), syntactic dependencies (Pantel and Ravichandran, 2004), and lexico-syntactic patterns (Riloff and Jones, 1999; Fleischman and Hovy, 2002; Thelen and Riloff, 2002). When research focuses on a particular relation, careful attention is paid to the pattern(s) that express it in various ways (as in most of the work above, notably (Riloff and Jones, 1999)). But it Knowing the sectional restrictions of a semantic relation supports inference in many applications, for example enabling more accurate information extraction. (Igo and Riloff, 2009) report that patterns like “attack on hNPi” can learn und"
P10-1150,D08-1061,0,0.0219601,"attern contains one example of the seed term (the anchor) and one open position for the term to be learned. Most researchers use singly-anchored patterns to harvest semantic relations. Unfortunately, these patterns run out of steam very quickly. To surmount this obstacle, a handful of seeds is generally used, and helps to guarantee diversity in the extraction of new lexicosyntactic patterns (Riloff and Jones, 1999; Snow et al., 2005; Etzioni et al., 2005). Some algorithms require ten seeds (Riloff and Jones, 1999; Igo and Riloff, 2009), while others use a variation of 5, 10, to even 25 seeds (Talukdar et al., 2008). Seeds may be chosen at random (Davidov et al., 2007; Kozareva et al., 2008), by picking the most frequent terms of the desired class (Igo and Riloff, 2009), or by asking humans (Pantel et al., 2009). As (Pantel et al., 2009) show, picking seeds that yield high numbers of different terms is difficult. Thus, when dealing with unbounded sets of relations (Banko and Etzioni, 2008), providing many seeds becomes unrealistic. Interestingly, recent work reports a class of patterns that use only one seed to learn as much information with only one seed. (Kozareva et al., 2008; Hovy et al., 2009) intro"
P10-1150,W02-1028,0,0.530436,"on and the pattern X flies to Y, automatically determining that John, Prague) and (John, conference) are two valid filler instance pairs, that (RyanAir, Prague) is another, as well as that person and airline are supertypes of the first argument and city and event of the second. This information provides the selectional restrictions of the given semantic relation, indicating that living things like people can fly to cities and events, while non-living things like airlines fly mainly to cities. This is a significant improvement over systems that output a flat list of lexical semantic knowledge (Thelen and Riloff, 2002; Yates et al., 2007; Suchanek et al., 2007). 2 Related Work A substantial body of work has been done in attempts to harvest bits of semantic information, including: semantic lexicons (Riloff and Shepherd, 1997), concept lists (Lin and Pantel, 2002), isa relations (Hearst, 1992; Etzioni et al., 2005; Pasca, 2004; Kozareva et al., 2008), part-of relations (Girju et al., 2003), and others. Knowledge has been harvested with varying success both from structured text such as Wikipedia’s infoboxes (Suchanek et al., 2007) or unstructured text such as the Web (Pennacchiotti and Pantel, 2006; Yates et"
P10-1150,N07-4013,0,0.553265,"ctional restrictions of semantic relations be learned automatically from the Web with minimal effort using lexico-syntactic recursive patterns? The contributions of the paper are as follows: • A novel representation of semantic relations using recursive lexico-syntactic patterns. • An automatic procedure to learn the selectional restrictions (arguments and supertypes) of semantic relations from Web data. has proven a difficult task to manually find effectively different variations and alternative patterns for each relation. In contrast, when research focuses on any relation, as in TextRunner (Yates et al., 2007), there is no standardized manner for re-using the pattern learned. TextRunner scans sentences to obtain relation-independent lexico-syntactic patterns to extract triples of the form (John, fly to, Prague). The middle string denotes some (unspecified) semantic relation while the first and third denote the learned arguments of this relation. But TextRunner does not seek specific semantic relations, and does not re-use the patterns it harvests with different arguments in order to extend their yields. • An exhaustive human-based evaluation of the harvested knowledge. • A comparison of the results"
P10-1150,P06-1107,0,0.0242775,"text, including ground facts (Etzioni et al., 2005), semantic lexicons (Thelen and Riloff, 2002), encyclopedic knowledge (Suchanek et al., 2007), and concept lists (Katz et al., 2003). Researchers have also successfully harvested relations between entities, such as is-a (Hearst, 1992; Pasca, 2004) and part-of (Girju et al., 2003). The kinds of knowledge learned are generally of two kinds: ground instance facts (New York is-a city, Rome is the capital of Italy) and general relational types (city is-a location, engines are part-of cars). A variety of NLP tasks involving inference or entailment (Zanzotto et al., 2006), including QA (Katz and Lin, 2003) and MT (Mt et al., 1988), require a slightly different form of knowledge, derived from many more relations. This knowledge is usually used to support inference and is expressed as selectional restrictions (Wilks, 1975) (namely, the types of arguments that may fill a given relation, such as person live-in city and airline fly-to location). Selectional restrictions constrain the possible fillers of a relation, and hence the possible contexts in which the patterns expressing that relation can participate in, thereby enabling sense disambiguation of both the fil"
P10-1150,C98-2177,0,\N,Missing
P10-1150,D09-1098,0,\N,Missing
P10-5004,W04-2703,0,\N,Missing
P10-5004,J08-3001,0,\N,Missing
P11-1147,P98-1013,0,0.0200753,"at expected agreement is a function of the number of choices rather than chance. It uses the same general formula as κ, (Pa − Pe ) (5) (1 − Pe ) where Pa is the actual raw agreement measured, and Pe is the expected agreement. The difference with κ is that Pe for the G-index is defined as Pe = 1/q, 1473 The approach we describe is similar in nature to unsupervised verb argument selection/selectional preferences and semantic role labeling, yet goes beyond it in several ways. For semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et al., 2003), classes have been derived from FrameNet (Baker et al., 1998). For verb argument detection, classes are either semi-manually derived from a repository like WordNet, or from NE taggers (Pardo et al., 2006; Fan et al., 2010). This allows for domain-independent systems, but limits the approach to a fixed set of oftentimes rather inappropriate classes. In contrast, we derive the level of granularity directly from the data. Pre-tagging the data with NE classes before training comes at a cost. It lumps entities together which can have very different classes (i.e., all people become labeled as PERSON), effectively allowing only one class per entity. Etzioni et"
P11-1147,P07-1057,0,0.0672416,"pieces. In a first step, we parse all documents with the Stanford dependency parser (De Marneffe et al., 2006) (see Figure 2, step 1). The output is lemmatized (collapsing “throws”, “threw”, etc., into “throw”), and marked for various dependencies (nsubj, amod, etc.). This enables us to extract the predicate argument structure, like subjectverb-object, or additional prepositional phrases (see Figure 2, step 2). These structures help to simplify the model by discarding additional words like modifiers, determiners, etc., which are not essential to the proposition. The same approach is used by (Brody, 2007). We also concatenate multiword names (identified by sequences of NNPs) with an underscore to form a single token (“Steve/NNP Young/NNP” → “Steve Young”). 1468 2.2 Deriving Classes To derive the classes used for entities, we do not restrict ourselves to a fixed set, but derive a domainspecific set directly from the data. This step is performed simultaneously with the corpus generation described above. We utilize three syntactic constructions to identify classes, namely nominal modifiers, copula verbs, and appositions, see below. This is similar in nature to Hearst’s lexico-syntactic patterns ("
P11-1147,de-marneffe-etal-2006-generating,0,0.0230078,"Missing"
P11-1147,W10-0915,0,0.06212,"textual enrichment. Teams beat teams Teams play teams Quarterbacks throw passes Teams win games Teams defeat teams Receivers catch passes Quarterbacks complete passes Quarterbacks throw passes to receivers Teams play games Teams lose games 2 Methods INPUT: Steve Young threw a pass to Michael Holt 1. PARSE INPUT: throw/VBD Figure 1: The ten most frequent propositions discovered by our system for the American football domain Our approach differs from verb-argument identification or Named Entity (NE) tagging in several respects. While previous work on verb-argument selection (Pardo et al., 2006; Fan et al., 2010) uses fixed sets of classes, we cannot know a priori how many and which classes we will encounter. We therefore provide a way to derive the appropriate classes automatically and include a probability distribution for each of them. Our approach is thus less restricted and can learn context-dependent, finegrained, domain-specific propositions. While a NEtagged corpus could produce a general proposition like “PERSON throws to PERSON”, our method enables us to distinguish the arguments and learn “quarterback throws to receiver” for American football and “outfielder throws to third base” for baseba"
P11-1147,W03-1007,1,0.748435,"ndex (Holley and Guilford, 1964), avoids the paradox. It assumes that expected agreement is a function of the number of choices rather than chance. It uses the same general formula as κ, (Pa − Pe ) (5) (1 − Pe ) where Pa is the actual raw agreement measured, and Pe is the expected agreement. The difference with κ is that Pe for the G-index is defined as Pe = 1/q, 1473 The approach we describe is similar in nature to unsupervised verb argument selection/selectional preferences and semantic role labeling, yet goes beyond it in several ways. For semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et al., 2003), classes have been derived from FrameNet (Baker et al., 1998). For verb argument detection, classes are either semi-manually derived from a repository like WordNet, or from NE taggers (Pardo et al., 2006; Fan et al., 2010). This allows for domain-independent systems, but limits the approach to a fixed set of oftentimes rather inappropriate classes. In contrast, we derive the level of granularity directly from the data. Pre-tagging the data with NE classes before training comes at a cost. It lumps entities together which can have very different classes (i.e., all people become labeled as PERSO"
P11-1147,J02-3001,0,0.101954,"Another statistic, the G-index (Holley and Guilford, 1964), avoids the paradox. It assumes that expected agreement is a function of the number of choices rather than chance. It uses the same general formula as κ, (Pa − Pe ) (5) (1 − Pe ) where Pa is the actual raw agreement measured, and Pe is the expected agreement. The difference with κ is that Pe for the G-index is defined as Pe = 1/q, 1473 The approach we describe is similar in nature to unsupervised verb argument selection/selectional preferences and semantic role labeling, yet goes beyond it in several ways. For semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et al., 2003), classes have been derived from FrameNet (Baker et al., 1998). For verb argument detection, classes are either semi-manually derived from a repository like WordNet, or from NE taggers (Pardo et al., 2006; Fan et al., 2010). This allows for domain-independent systems, but limits the approach to a fixed set of oftentimes rather inappropriate classes. In contrast, we derive the level of granularity directly from the data. Pre-tagging the data with NE classes before training comes at a cost. It lumps entities together which can have very different classes (i.e., all peop"
P11-1147,C92-2082,0,0.287696,". We also concatenate multiword names (identified by sequences of NNPs) with an underscore to form a single token (“Steve/NNP Young/NNP” → “Steve Young”). 1468 2.2 Deriving Classes To derive the classes used for entities, we do not restrict ourselves to a fixed set, but derive a domainspecific set directly from the data. This step is performed simultaneously with the corpus generation described above. We utilize three syntactic constructions to identify classes, namely nominal modifiers, copula verbs, and appositions, see below. This is similar in nature to Hearst’s lexico-syntactic patterns (Hearst, 1992) and other approaches that derive ISA relations from text. While we find it straightforward to collect classes for entities in this way, we did not find similar patterns for verbs. Given a suitable mechanism, however, these could be incorporated into our framework as well. Nominal modifier are common nouns (labeled NN) that precede proper nouns (labeled NNP), as in “quarterback/NN Steve/NNP Young/NNP”, where “quarterback” is the nominal modifier of “Steve Young”. Similar information can be gained from appositions (e.g., “Steve Young, the quarterback of his team, said...”), and copula verbs (“S"
P11-1147,W10-0903,1,0.885429,"Missing"
P11-1147,P10-1154,0,0.063488,"Missing"
P11-1147,P10-1044,0,0.296167,"position, given the input sentences, our system has to not only identify the classes, but also learn when to p abstract away from the lexical form to the appropriate class and when to keep it (cf. Figure 2, step 3). To facilitate extraction, we focus on propositions with the following predicate-argument structures: NOUN-VERB-NOUN (e.g., “quarterbacks throw passes”), or NOUN-VERB-NOUNPREPOSITION-NOUN (e.g., “quarterbacks throw passes to receivers”. There is nothing, though, that prevents the use of other types of structures as well. We do not restrict the verbs we consider (Pardo et al., 2006; Ritter et al., 2010)), which extracts a high number of hapax structures. Given a sentence, we want to find the most likely class for each word and thereby derive the most likely proposition. Similar to Pardo et al. (2006), we assume the observed data was produced by a process that generates the proposition and then transforms the classes into a sentence, possibly adding additional words. We model this as a Hidden Markov Model (HMM) with bigram transitions (see Section 2.3) and use the EM algorithm (Dempster et al., 1977) to train it on the observed data, with smoothing to prevent overfitting. 2.1 Data We use a co"
P11-1147,D08-1027,0,0.0664651,"Missing"
P11-1147,strassel-etal-2010-darpa,0,0.0270768,"oni et al. (2005) resolve the problem with a web-based approach that learns hierarchies of the NE classes in an unsupervised manner. We do not enforce a taxonomy, but include statistical knowledge about the distribution of possible classes over each entity by incorporating a prior distribution P (class, entity). This enables us to genPage 1 eralize from the lexical form without restricting ourselves to one class per entity, which helps to better fit the data. In addition, we can distinguish several classes for each entity, depending on the context (e.g., winner vs. quarterback). Ritter et al. (2010) also use an unsupervised model to derive selectional predicates from unlabeled text. They do not assign classes altogether, but group similar predicates and arguments into unlabeled clusters using LDA. Brody (2007) uses a very similar methodology to establish relations between clauses and sentences, by clustering simplified propositions. Pe˜nas and Hovy (2010) employ syntactic patterns to derive classes from unlabeled data in the context of LbR. They consider a wider range of syntactic structures, but do not include a probabilistic model to label new data. 5 Conclusion We use an unsupervised"
P11-1147,C98-1013,0,\N,Missing
P11-1147,W10-0900,0,\N,Missing
P11-1162,N03-1011,0,0.268649,"cially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and G"
P11-1162,C92-2082,0,0.0658644,"illustrative statistics. In Section 5 we discuss implications for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web documents. The latter has been shown to provide broader and more complete information. Researchers outside computational linguistics have studied complex networks such as the World Wide Web, th"
P11-1162,P10-1150,1,0.863755,"ic Networks in the Web Text mining algorithms such as those mentioned above raise certain questions, such as: Why are some seed terms more powerful (provide a greater yield) than others?, How can one find high-yield terms?, How many steps does one need, typically, to learn all terms for a given relation?, Can one estimate the total eventual yield of a given relation?, and so on. On the face of it, one would need to know the structure of the network a priori to be able to provide answers. But research has shown that some surprising regularities hold. For example, in the text mining community, (Kozareva and Hovy, 2010b) have shown that one can obtain a quite accurate estimate of the eventual yield of a pattern and seed after only five steps of harvesting. Why is this? They do not provide an answer, but research from the network community does. To illustrate the properties of networks of the kind induced by semantic relations, and to show the applicability of network research to text harvesting, we implemented a harvesting algorithm and applied it to a representative set of relations and seeds in two languages. Since the goal of this paper is not the development of a new text harvesting algorithm, we implem"
P11-1162,N10-1087,1,0.935523,"ic Networks in the Web Text mining algorithms such as those mentioned above raise certain questions, such as: Why are some seed terms more powerful (provide a greater yield) than others?, How can one find high-yield terms?, How many steps does one need, typically, to learn all terms for a given relation?, Can one estimate the total eventual yield of a given relation?, and so on. On the face of it, one would need to know the structure of the network a priori to be able to provide answers. But research has shown that some surprising regularities hold. For example, in the text mining community, (Kozareva and Hovy, 2010b) have shown that one can obtain a quite accurate estimate of the eventual yield of a pattern and seed after only five steps of harvesting. Why is this? They do not provide an answer, but research from the network community does. To illustrate the properties of networks of the kind induced by semantic relations, and to show the applicability of network research to text harvesting, we implemented a harvesting algorithm and applied it to a representative set of relations and seeds in two languages. Since the goal of this paper is not the development of a new text harvesting algorithm, we implem"
P11-1162,P08-1119,1,0.884429,"Missing"
P11-1162,C02-1144,0,0.0450675,"inking them are the edges. The results of computational network analysis, especially from the world wide web, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff an"
P11-1162,P06-1015,0,0.0244736,"eb, are thus applicable. Surprisingly, these results have not yet been broadly introduced into the computational linguistics community. In this paper we show how various results apply to text mining, how they explain some previously observed phenomena, and how they can be helpful for computational linguistics applications. 1 Introduction Text mining / harvesting algorithms have been applied in recent years for various uses, including learning of semantic constraints for verb participants (Lin and Pantel, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 200"
P11-1162,P02-1006,1,0.626575,"l and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et al., 2003). Generally, the harvesting procedure is recursive, in which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different network1 . Text mining is a process of network traversal, and faces the standard problems of handling"
P11-1162,W97-0313,0,0.256838,"el, 2002) related pairs in various relations, such as part-whole (Girju et al., 2003), cause (Pantel and Pennacchiotti, 2006), and other typical information extraction relations, large collections of entities (Soderland et al., 1999; Etzioni et al., 2005), features of objects (Pasca, 2004) and ontologies (Carlson et al., 2010). They generally start with one or more seed terms and employ patterns that specify the desired information as it relates to the seed(s). Several approaches have been developed specifically for learning patterns, including guided pattern collection with manual filtering (Riloff and Shepherd, 1997) automated surface-level pattern induction (Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) probabilistic methods for taxonomy relation learning (Snow et al., 2005) and kernel methods for relation learning (Zelenko et al., 2003). Generally, the harvesting procedure is recursive, in which data (terms or patterns) gathered in one step of a cycle are used as seeds in the following step, to gather more terms or patterns. This method treats the source text as a graph or network, consisting of terms (words) as nodes and inter-term relations as edges. Each relation type induces a different"
P11-1162,P06-1107,0,0.0139726,"describe the general harvesting procedure, and follow with an examination of the various statistical properties of implicit semantic networks in Section 4, using our implemented harvester to provide illustrative statistics. In Section 5 we discuss implications for computational linguistics research. 2 Related Work The Natural Language Processing knowledge harvesting community has developed a good understanding of how to harvests various kinds of semantic information and use this information to improve the performance of tasks such as information extraction (Riloff, 1993), textual entailment (Zanzotto et al., 2006), question answering (Katz et al., 2003), and ontology creation (Suchanek et al., 2007), among others. Researchers have focused on the automated extraction of semantic lexicons (Hearst, 1992; Riloff and Shepherd, 1997; Girju et al., 2003; Pasca, 2004; Etzioni et al., 2005; Kozareva et al., 2008). While clustering approaches tend to extract general facts, pattern based approaches have shown to produce more constrained but accurate lists of semantic terms. To extract this information, (Lin and Pantel, 2002) showed the effect of using different sizes and genres of corpora such as news and Web doc"
P11-1162,P10-1149,0,\N,Missing
P11-2056,J09-2001,0,0.0843537,"Missing"
P11-2056,N10-1083,0,0.0588888,"Missing"
P11-2056,P07-1005,1,0.835466,"sts an average of 9.76 senses for each of the 34 most frequent English prepositions, while nouns usually have around two (WordNet nouns average about 1.2 senses, 2.7 if monosemous nouns are excluded (Fellbaum, 1998)). Disambiguating prepositions is thus a challenging and interesting task in itself (as exemplified by the SemEval 2007 task, (Litkowski and Hargraves, 2007)), and holds promise for NLP applications such as Information Extraction or Machine Translation.1 Given a sentence such as the following: In the morning, he shopped in Rome we ultimately want to be able to annotate it as 1 See (Chan et al., 2007) for how using WSD can help MT. Here, the preposition in has two distinct meanings, namely a temporal and a locative one. These meanings are context-dependent. Ultimately, we want to disambiguate prepositions not by and for themselves, but in the context of sequential semantic labeling. This should also improve disambiguation of the words linked by the prepositions (here, morning, shopped, and Rome). We propose using unsupervised methods in order to leverage unlabeled data, since, to our knowledge, there are no annotated data sets that include both preposition and argument senses. In this pape"
P11-2056,W02-0102,0,0.418239,"Missing"
P11-2056,C10-2052,1,0.904044,"onstrain the argument senses, we construct a dictionary that lists for each word all the possible lexicographer senses according to WordNet. The set of lexicographer senses (45) is a higher level abstraction which is sufficiently coarse to allow for a good generalization. Unknown words are assumed to have all possible senses applicable to their 324 b) h! p! o! h p o h! p! o! h c) h! h o p! o! o Figure 1: Graphical Models. a) 1st order HMM. b) variant used in experiments (one model/preposition, thus no conditioning on p). c) incorporates further constraints on variables As shown by Hovy et al. (2010), preposition senses can be accurately disambiguated using only the head word and object of the PP. We exploit this property of prepositional constructions to represent the constraints between h, p, and o in a graphical model. We define a good model as one that reasonably constrains the choices, but is still tractable in terms of the number of parameters being estimated. As a starting point, we choose the standard firstorder Hidden Markov Model as depicted in Figure 1a. Since we train a separate model for each preposition, we can omit all arcs to p. This results in model 1b. The joint distribu"
P11-2056,D07-1031,0,0.0815467,"nitialized all models uniformly, and trained until the perplexity rate stopped increasing or a predefined number of iterations was reached. Note that MAPEM and Bayesian Inference require tuning of some hyper-parameters on held-out data, and are thus not fully unsupervised. 5.1 EM We use the EM algorithm (Dempster et al., 1977) as a baseline. It is relatively easy to implement with existing toolkits like Carmel (Graehl, 1997). However, EM has a tendency to assume equal importance for each parameter. It thus prefers “general” solutions, assigning part of the probability mass to unlikely states (Johnson, 2007). We ran EM on each model for 100 iterations, or until the perplexity stopped decreasing below a threshold of 10−6 . 5.2 2002). Smoothing helps to prevent overfitting. Repeated random restarts help escape unfavorable initializations that lead to local maxima. Carmel provides options for both smoothing and restarts. 5.3 MAP-EM with L0 Norm Since we want to encourage sparsity in our models, we use the MDL-inspired technique introduced by Vaswani et al. (2010). Here, the goal is to increase the data likelihood while keeping the number of parameters small. The authors use a smoothed L0 prior, whic"
P11-2056,S07-1005,0,0.363796,"Missing"
P11-2056,W03-0411,0,0.221531,"Missing"
P11-2056,J09-2002,0,0.428983,"Missing"
P11-2056,N09-3017,1,0.700377,"Missing"
P11-2056,P10-2039,1,0.79241,"Missing"
P11-2056,S07-1051,0,0.43851,"Missing"
P11-2056,W10-0900,0,\N,Missing
P11-2056,N10-1068,1,\N,Missing
P11-2096,P05-1074,0,0.935232,"pice of life”. As with life, variety also adds spice and appeal to language. Paraphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectrum are more complex approaches that require access to bilin"
P11-2096,N03-1003,0,0.572015,"s that “variety is the spice of life”. As with life, variety also adds spice and appeal to language. Paraphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectrum are more complex appro"
P11-2096,P01-1008,0,0.763036,"e, and low redundancy. 1 Introduction A popular idiom states that “variety is the spice of life”. As with life, variety also adds spice and appeal to language. Paraphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics."
P11-2096,P08-1077,0,0.755202,"roduction A popular idiom states that “variety is the spice of life”. As with life, variety also adds spice and appeal to language. Paraphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectru"
P11-2096,D08-1021,0,0.748208,"ty also adds spice and appeal to language. Paraphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectrum are more complex approaches that require access to bilingual parallel corpora"
P11-2096,N10-1017,0,0.386195,"ootball. Individual occurrences of verb phrases were sampled, which means that more common verb phrases were more likely to be selected and that a given phrase could be selected multiple times. This sampling strategy was used to evaluate the systems across a realistic sample of phrases. To obtain a richer class of phrases beyond basic verb groups, we defined verb phrases to be contiguous sequences of tokens that matched the following POS tag pattern: (TO |IN |RB |MD |VB)+. Following the methodology used in previous paraphrase evaluations (Bannard and CallisonBurch, 2005; Callison-Burch, 2008; Kok and Brockett, 2010), we presented annotators with two sentences. The first sentence was randomly selected from amongst all of the sentences in the evaluation corpus that contain the original phrase. The second sentence was the same as the first, except the original phrase is replaced with the system generated paraphrase. Annotators were given the following options, which were adopted from those described by Kok and Brockett (2010), for each sentence pair: 0) Different meaning; 1) Same meaning; revised is 1 Available at http://www.cs.jhu.edu/˜ccb/. 548 grammatically incorrect; and 2) Same meaning; revised is gram"
P11-2096,J10-3003,0,0.312664,"tyle patterns to generate paraphrases from a bilingual parallel corpus. The primary drawback of these type of approaches is that they require a considerable amount of resource engineering that may not be available for all languages, domains, or applications. Related Work 3 Instead of exhaustively covering the entire spectrum of previously proposed paraphrasing techniques, our evaluation focuses on two families of data-driven approaches that are widely studied and used. More comprehensive surveys of data-driven paraphrasing techniques can be found in Androutsopoulos and Malakasiotis (2010) and Madnani and Dorr (2010). The first family of approaches that we consider harvests paraphrases from monolingual corpora using distributional similarity. The DIRT algorithm, proposed by Lin and Pantel (2001), uses parse tree paths as contexts for computing distributional similarity. In this way, two phrases were considered similar if they occurred in similar contexts within many sentences. Although parse tree paths serve as rich representations, they are costly to construct and yield sparse representations. The approach proposed by Pasca and Dienes (2005) avoided the costs associated with parsing by using n-gram conte"
P11-2096,N03-1024,0,0.538282,"araphrases make it possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectrum are more complex approaches that require access to bilingual parallel corpora and may also rely on part-of-speech (POS)"
P11-2096,I05-1011,0,0.868971,"possible to express the same meaning in an almost unbounded number of ways. While variety prevents language from being overly rigid and boring, it also makes it difficult to algorithmically determine if two phrases or sentences express the same meaning. In an attempt to address this problem, a great deal of recent research has focused on identifying, generating, and harvesting phrase- and sentence-level paraphrases (Barzilay and McKeown, 2001; Bhagat and Ravichandran, 2008; Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; Callison-Burch, 2008; Lin and Pantel, 2001; Pang et al., 2003; Pasca and Dienes, 2005) Many data-driven approaches to the paraphrase problem have been proposed. The approaches vastly differ in their complexity and the amount of NLP resources that they rely on. At one end of the spectrum are approaches that generate paraphrases from a large monolingual corpus and minimally rely on NLP tools. Such approaches typically make use of statistical co-occurrences, which act as a rather crude proxy for semantics. At the other end of the spectrum are more complex approaches that require access to bilingual parallel corpora and may also rely on part-of-speech (POS) taggers, chunkers, parse"
P13-1037,J09-2003,0,0.0169694,"attention from the NLP community, there is a large body of work that focuses on similar problems involving noun-noun relation interpretation/paraphrasing, including interpreting the relations between the components of noun compounds (Butnariu et al., 2010), disambiguating preposition senses (Litkowski and Hargraves, 2007), or annotating the relation between nominals in more arbitrary constructions within the same sentence (Hendrickx et al., 2009). Whereas some of these lines of work use fixed inventories of semantic relations (Lauer, 1995; Nastase and Szpakowicz, 2003; Kim and Baldwin, 2005; Girju, 2009; Ó Séaghdha and Copestake, 2009; Tratz and Hovy, 2010), other work allows for a nearly infinite number of interpretations (Butnariu and Veale, 2008; Nakov, 2008). Recent SemEval tasks (Butnariu et al., 2009; Hendrickx et al., 2013) pursue this more open-ended strategy. In these tasks, participating systems recover the implicit predicate between the nouns in noun compounds by creating potentially unique paraphrases for each example. For instance, a system might generate the paraphrase made of for the noun com378 Feature Type Gloss Terms Hypernyms Synonyms Word Itself Lexnames Last Letters Lexn"
P13-1037,W09-2415,0,0.037595,"Missing"
P13-1037,C08-1011,0,0.0144183,"tation/paraphrasing, including interpreting the relations between the components of noun compounds (Butnariu et al., 2010), disambiguating preposition senses (Litkowski and Hargraves, 2007), or annotating the relation between nominals in more arbitrary constructions within the same sentence (Hendrickx et al., 2009). Whereas some of these lines of work use fixed inventories of semantic relations (Lauer, 1995; Nastase and Szpakowicz, 2003; Kim and Baldwin, 2005; Girju, 2009; Ó Séaghdha and Copestake, 2009; Tratz and Hovy, 2010), other work allows for a nearly infinite number of interpretations (Butnariu and Veale, 2008; Nakov, 2008). Recent SemEval tasks (Butnariu et al., 2009; Hendrickx et al., 2013) pursue this more open-ended strategy. In these tasks, participating systems recover the implicit predicate between the nouns in noun compounds by creating potentially unique paraphrases for each example. For instance, a system might generate the paraphrase made of for the noun com378 Feature Type Gloss Terms Hypernyms Synonyms Word Itself Lexnames Last Letters Lexnames Link types Link types Word Itself Last Letters Gloss Terms Hypernyms Word Itself Synonyms Lexnames Part-of-speech List Part-of-speech List Depe"
P13-1037,W09-2416,0,0.0165002,"een the components of noun compounds (Butnariu et al., 2010), disambiguating preposition senses (Litkowski and Hargraves, 2007), or annotating the relation between nominals in more arbitrary constructions within the same sentence (Hendrickx et al., 2009). Whereas some of these lines of work use fixed inventories of semantic relations (Lauer, 1995; Nastase and Szpakowicz, 2003; Kim and Baldwin, 2005; Girju, 2009; Ó Séaghdha and Copestake, 2009; Tratz and Hovy, 2010), other work allows for a nearly infinite number of interpretations (Butnariu and Veale, 2008; Nakov, 2008). Recent SemEval tasks (Butnariu et al., 2009; Hendrickx et al., 2013) pursue this more open-ended strategy. In these tasks, participating systems recover the implicit predicate between the nouns in noun compounds by creating potentially unique paraphrases for each example. For instance, a system might generate the paraphrase made of for the noun com378 Feature Type Gloss Terms Hypernyms Synonyms Word Itself Lexnames Last Letters Lexnames Link types Link types Word Itself Last Letters Gloss Terms Hypernyms Word Itself Synonyms Lexnames Part-of-speech List Part-of-speech List Dependency Part-of-speech List Link Types Word Itself Assigned"
P13-1037,S10-1007,0,0.0281136,"Missing"
P13-1037,P95-1007,0,0.136225,"ominals in the English possessive construction has received little attention from the NLP community, there is a large body of work that focuses on similar problems involving noun-noun relation interpretation/paraphrasing, including interpreting the relations between the components of noun compounds (Butnariu et al., 2010), disambiguating preposition senses (Litkowski and Hargraves, 2007), or annotating the relation between nominals in more arbitrary constructions within the same sentence (Hendrickx et al., 2009). Whereas some of these lines of work use fixed inventories of semantic relations (Lauer, 1995; Nastase and Szpakowicz, 2003; Kim and Baldwin, 2005; Girju, 2009; Ó Séaghdha and Copestake, 2009; Tratz and Hovy, 2010), other work allows for a nearly infinite number of interpretations (Butnariu and Veale, 2008; Nakov, 2008). Recent SemEval tasks (Butnariu et al., 2009; Hendrickx et al., 2013) pursue this more open-ended strategy. In these tasks, participating systems recover the implicit predicate between the nouns in noun compounds by creating potentially unique paraphrases for each example. For instance, a system might generate the paraphrase made of for the noun com378 Feature Type Glo"
P13-1037,S07-1005,0,0.101478,"definition, hold a specific relation to other real or conceptual things, and non-relational, or sortal nouns (Löbner, 1985), such as car. Vikner and Jensen’s (2002) approach for han6.2 Computational Linguistics Though the relation between nominals in the English possessive construction has received little attention from the NLP community, there is a large body of work that focuses on similar problems involving noun-noun relation interpretation/paraphrasing, including interpreting the relations between the components of noun compounds (Butnariu et al., 2010), disambiguating preposition senses (Litkowski and Hargraves, 2007), or annotating the relation between nominals in more arbitrary constructions within the same sentence (Hendrickx et al., 2009). Whereas some of these lines of work use fixed inventories of semantic relations (Lauer, 1995; Nastase and Szpakowicz, 2003; Kim and Baldwin, 2005; Girju, 2009; Ó Séaghdha and Copestake, 2009; Tratz and Hovy, 2010), other work allows for a nearly infinite number of interpretations (Butnariu and Veale, 2008; Nakov, 2008). Recent SemEval tasks (Butnariu et al., 2009; Hendrickx et al., 2013) pursue this more open-ended strategy. In these tasks, participating systems reco"
P13-1037,P10-1070,1,0.880225,"large body of work that focuses on similar problems involving noun-noun relation interpretation/paraphrasing, including interpreting the relations between the components of noun compounds (Butnariu et al., 2010), disambiguating preposition senses (Litkowski and Hargraves, 2007), or annotating the relation between nominals in more arbitrary constructions within the same sentence (Hendrickx et al., 2009). Whereas some of these lines of work use fixed inventories of semantic relations (Lauer, 1995; Nastase and Szpakowicz, 2003; Kim and Baldwin, 2005; Girju, 2009; Ó Séaghdha and Copestake, 2009; Tratz and Hovy, 2010), other work allows for a nearly infinite number of interpretations (Butnariu and Veale, 2008; Nakov, 2008). Recent SemEval tasks (Butnariu et al., 2009; Hendrickx et al., 2013) pursue this more open-ended strategy. In these tasks, participating systems recover the implicit predicate between the nouns in noun compounds by creating potentially unique paraphrases for each example. For instance, a system might generate the paraphrase made of for the noun com378 Feature Type Gloss Terms Hypernyms Synonyms Word Itself Lexnames Last Letters Lexnames Link types Link types Word Itself Last Letters Glo"
P13-1037,D11-1116,1,0.872452,"CATION / SPACE SOURCE / FROM TOPIC ACCOMPANIMENT EXPERIENCER RECIPIENT ASSOCIATED WITH MEASURE THEME RESULT OTHER 21,938 total examples, 15,330 come from sections 2–21 of the Penn Treebank (Marcus et al., 1993). Another 5,266 examples are from The History of the Decline and Fall of the Roman Empire (Gibbon, 1776), a non-fiction work, and 1,342 are from The Jungle Book (Kipling, 1894), a collection of fictional short stories. For the Penn Treebank, we extracted the examples using the provided gold standard parse trees, whereas, for the latter cases, we used the output of an open source parser (Tratz and Hovy, 2011). 4 The initial semantic relation inventory for possessives was created by first examining some of the relevant literature on possessives, including work by Badulescu and Moldovan (2009), Barker (1995), Quirk et al. (1985), Rosenbach (2002), and Taylor (1996), and then manually annotating the large dataset of examples. Similar examples were grouped together to form initial categories, and groups that were considered more difficult were later reexamined in greater detail. Once all the examples were assigned to initial categories, the process of refining the definitions and annotations began. In"
P13-1037,J93-2004,0,0.0473906,"ngeles Times news articles used in TREC-9. For the 960 extracted ’s-possessive examples, only 20 of their semantic relations are observed, including OTHER, with 8 of the observed relations occurring fewer than 10 times. They report a 0.82 Kappa agreement (Siegel and Castellan, 1988) for the two computational semantics graduates who annotate the data, stating that this strong result “can be explained by the instructions the annotators received Introduction The English ’s possessive construction occurs frequently in text—approximately 1.8 times for every 100 hundred words in the Penn Treebank1 (Marcus et al., 1993)—and can encode a number of different semantic relations including ownership (John’s car), part-of-whole (John’s arm), extent (6 hours’ drive), and location (America’s rivers). Accurate automatic possessive interpretation could aid many natural language processing (NLP) applications, especially those that build semantic representations for text understanding, text generation, question answering, or information extraction. These interpretations could be valuable for machine translation to or from languages that allow different semantic relations to be encoded by 1 Possessive pronouns such as hi"
P13-1037,W04-2705,0,0.046301,"essee, syntactic governor of possessee, all tokens between possessor and possessee, and the word next to the possessee (on the right), respectively. The C parameter value used to train the SVMs is shown in parentheses. pound pepperoni pizza. Computer-generated results are scored against a list of human-generated options in order to rank the participating systems. This approach could be applied to possessives interpretation as well. Concurrent with the lack of NLP research on the subject is the absence of available annotated datasets for training, evaluation, and analysis. The NomBank project (Meyers et al., 2004) provides coarse annotations for some of the possessive constructions in the Penn Treebank, but only those that meet their criteria. 7 largest available manually-annotated collection of possessives, and an effective method for automatically assigning the relations to unseen examples. We explain our methodology for building this inventory and dataset and report a strong level of inter-annotator agreement, reaching 0.78 Kappa overall. The resulting dataset is quite large, at 21,938 instances, and crosses multiple domains, including news, fiction, and historical non-fiction. It is the only large"
P13-1037,H05-1112,0,0.0601615,"Missing"
P13-1037,W09-3734,0,0.0304376,"(1914–1916). Curiously, there is not a single dominant semantic relation inventory for possessives. A representative example of semantic relation inventories for ’s-constructions is the one given by Quirk et al. (1985) (presented earlier in Section 2). Interestingly, the set of relations expressed by possessives varies by language. For example, Classical Greek permits a standard of comparison relation (e.g., “better than Plato”) (Nikiforidou, 1991), and, in Japanese, some relations are expressed in the opposite direction (e.g., “blue eye’s doll”) while others are not (e.g., “Tanaka’s face”) (Nishiguchi, 2009). To explain how and why such seemingly different relations as whole+part and cause+effect are expressed by the same linguistic phenomenon, Nikiforidou (1991) pursues an approach of metaphorical structuring in line with the work of Lakoff and Johnson (1980) and Lakoff (1987). She thus proposes a variety of such metaphors as THINGS THAT HAPPEN ( TO US ) ARE ( OUR ) POS SESSIONS and CAUSES ARE ORIGINS to explain how the different relations expressed by possessives extend from one another. Certainly, not all, or even most, of the linguistics literature on English possessives focuses on creating l"
P13-1037,E09-1071,0,0.0200511,"the NLP community, there is a large body of work that focuses on similar problems involving noun-noun relation interpretation/paraphrasing, including interpreting the relations between the components of noun compounds (Butnariu et al., 2010), disambiguating preposition senses (Litkowski and Hargraves, 2007), or annotating the relation between nominals in more arbitrary constructions within the same sentence (Hendrickx et al., 2009). Whereas some of these lines of work use fixed inventories of semantic relations (Lauer, 1995; Nastase and Szpakowicz, 2003; Kim and Baldwin, 2005; Girju, 2009; Ó Séaghdha and Copestake, 2009; Tratz and Hovy, 2010), other work allows for a nearly infinite number of interpretations (Butnariu and Veale, 2008; Nakov, 2008). Recent SemEval tasks (Butnariu et al., 2009; Hendrickx et al., 2013) pursue this more open-ended strategy. In these tasks, participating systems recover the implicit predicate between the nouns in noun compounds by creating potentially unique paraphrases for each example. For instance, a system might generate the paraphrase made of for the noun com378 Feature Type Gloss Terms Hypernyms Synonyms Word Itself Lexnames Last Letters Lexnames Link types Link types Word"
P13-1037,1996.amta-1.36,0,0.357552,"ine and Fall of the Roman Empire (Gibbon, 1776), a non-fiction work, and 1,342 are from The Jungle Book (Kipling, 1894), a collection of fictional short stories. For the Penn Treebank, we extracted the examples using the provided gold standard parse trees, whereas, for the latter cases, we used the output of an open source parser (Tratz and Hovy, 2011). 4 The initial semantic relation inventory for possessives was created by first examining some of the relevant literature on possessives, including work by Badulescu and Moldovan (2009), Barker (1995), Quirk et al. (1985), Rosenbach (2002), and Taylor (1996), and then manually annotating the large dataset of examples. Similar examples were grouped together to form initial categories, and groups that were considered more difficult were later reexamined in greater detail. Once all the examples were assigned to initial categories, the process of refining the definitions and annotations began. In total, 17 relations were created, not including OTHER . They are shown in Table 3 along with approximate (best guess) mappings to relations defined by others, specifically those of Quirk et al. (1985), whose relations are presented in Table 2, as well as Bad"
P13-1037,S13-2025,0,\N,Missing
P13-1037,I05-1082,0,\N,Missing
P13-1037,S10-1006,0,\N,Missing
P13-2083,W10-2805,0,0.0263389,"ix runs over all the relations and the other axis is over the distributional word vocabulary. The cells store word counts (or PMI scores, or other measures of word association). Note that collapsing the rows of the matrix provides the standard dependency based distributional representation. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. 3.1 Building Representation: The PropStore To build a lexicon of SDSM matrices for a given vocabulary we first construct a proposition knowledge base (the PropStore) created by parsing the Simple English Wikipedia. Dependency arcs are stored as 3-tuples of the form hw1 , r, w2 i, denoting an occurrence of words w1 , word w2 related by r. We also store sentence indices for triples as this allows us to achieve an in"
P13-2083,P10-1143,0,0.103716,"Missing"
P13-2083,D09-1120,0,0.0241477,"it keeps” was expressed by Firth ∗ *Equally contributing authors 467 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 467–473, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2.2 (1957). Several works have defined approaches to modelling context-word distributions anchored on a target word, topic, or sentence position. Collectively these approaches are called Distributional Semantic Models (DSMs). Event Co-reference Resolution While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. We, on the other hand, attempt a slightly different problem of making co-referentiality judgements on event-coreference candidate pairs. While DSMs have been very successful on a variety of tasks, they are not an effective model of semantics as they lack properties such as compositionality or the ability to handle operators su"
P13-2083,W06-1670,0,0.0196345,"addition to the words’ surface-forms, the PropStore also stores their POS tags, lemmas, and Wordnet supersenses. This helps to generalize our representation when surface-form distributions are sparse. The PropStore can be used to query for the expectations of words, supersenses, relations, etc., around a given word. In the example in Figure 1, the query (SST(W1 ) = verb.consumption, ?, dobj) i.e. “what is consumed” might return expectations [pasta:1, spaghetti:1, mice:1 . . . ]. Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas usPantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do"
P13-2083,W13-1203,1,0.695619,"simplified event constituents for comparison: Level 1: Full Composition: Mf ull = ComposePhrase(e, a, p). Level 2: Partial Composition: Mpart:EA = ComposePair(e, r, a) Mpart:EP = ComposePair(e, r, p). Level 3: No Composition: ME = queryM atrix(e) MA = queryM atrix(a) MP = queryM atrix(p) To judge coreference between events E1 and E2, we compute pair4 Experiments We evaluate our method on two datasets and compare it against four baselines, two of which use window based distributional vectors and two that employ weaker forms of composition. 4.1 Datasets IC Event Coreference Corpus: The dataset (Hovy et al., 2013), drawn from 100 news articles about violent events, contains manually created annotations for 2214 pairs of co-referent and noncoreferent events each. Where available, events’ semantic role-fillers for agent and patient are annotated as well. When missing, empirical substitutes were obtained by querying the PropStore for the preferred word attachments. EventCorefBank (ECB) corpus: This corpus (Bejan and Harabagiu, 2010) of 482 documents from Google News is clustered into 45 topics, with event coreference chains annotated over each topic. The event mentions are enriched with semantic roles to"
P13-2083,D12-1045,0,0.0495139,"inguistics, pages 467–473, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2.2 (1957). Several works have defined approaches to modelling context-word distributions anchored on a target word, topic, or sentence position. Collectively these approaches are called Distributional Semantic Models (DSMs). Event Co-reference Resolution While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. We, on the other hand, attempt a slightly different problem of making co-referentiality judgements on event-coreference candidate pairs. While DSMs have been very successful on a variety of tasks, they are not an effective model of semantics as they lack properties such as compositionality or the ability to handle operators such as negation. In order to model a stronger form of semantics, there has been a recent surge in studies that phrase the problem of DSM comp"
P13-2083,D10-1113,0,0.0183763,"opose an approach to incorporate structure into distributional semantics (more details in Goyal et al. (2013)). The word distributions drawn from the context defined by a set of relations anchored on the target word (or phrase) form a set of vectors, namely a matrix for the target word. One axis of the matrix runs over all the relations and the other axis is over the distributional word vocabulary. The cells store word counts (or PMI scores, or other measures of word association). Note that collapsing the rows of the matrix provides the standard dependency based distributional representation. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. 3.1 Building Representation: The PropStore To build a lexicon of SDSM matrices for a given vocabulary we first constr"
P13-2083,D08-1094,0,0.0565658,"ir POS tags, lemmas, and Wordnet supersenses. This helps to generalize our representation when surface-form distributions are sparse. The PropStore can be used to query for the expectations of words, supersenses, relations, etc., around a given word. In the example in Figure 1, the query (SST(W1 ) = verb.consumption, ?, dobj) i.e. “what is consumed” might return expectations [pasta:1, spaghetti:1, mice:1 . . . ]. Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas usPantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 468 Figure 1: Sample sente"
P13-2083,J03-4004,0,0.0214147,"positional semantics previously employed in the literature. 1 Introduction Distributional Semantic Models (DSM) are popular in computational semantics. DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the 2 Related Work Next, we relate and contrast our work to prior research in the fields of Distributional Vector Space Models, Semantic Compositionality and Event Co-reference Resolution. 2.1 DSMs and Compositionality The underlying idea that “a word is characterized by the company it keeps” was expressed by Firth ∗ *Equally contributing authors 467 Proceedings of the 51st Annual Meeting of the Association for"
P13-2083,P07-1028,0,0.058242,"roduction Distributional Semantic Models (DSM) are popular in computational semantics. DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the 2 Related Work Next, we relate and contrast our work to prior research in the fields of Distributional Vector Space Models, Semantic Compositionality and Event Co-reference Resolution. 2.1 DSMs and Compositionality The underlying idea that “a word is characterized by the company it keeps” was expressed by Firth ∗ *Equally contributing authors 467 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 467–473, c Sof"
P13-2083,P04-1036,0,0.0432119,"state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature. 1 Introduction Distributional Semantic Models (DSM) are popular in computational semantics. DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the 2 Related Work Next, we relate and contrast our work to prior research in the fields of Distributional Vector Space Models, Semantic Compositionality and Event Co-reference Resolution. 2.1 DSMs and Compositionality The underlying idea that “a word is characterized by the company it keeps” was expressed by Firth ∗ *Equally"
P13-2083,P08-1028,0,0.362694,"d, attempt a slightly different problem of making co-referentiality judgements on event-coreference candidate pairs. While DSMs have been very successful on a variety of tasks, they are not an effective model of semantics as they lack properties such as compositionality or the ability to handle operators such as negation. In order to model a stronger form of semantics, there has been a recent surge in studies that phrase the problem of DSM compositionality as one of vector composition. These techniques derive the meaning of the combination of two words a and b by a single vector c = f (a, b). Mitchell and Lapata (2008) propose a framework to define the composition c = f (a, b, r, K) where r is the relation between a and b, and K is the additional knowledge used to define composition. While this framework is quite general, the actual models considered in the literature tend to disregard K and r and mostly perform component-wise addition and multiplication, with slight variations, of the two vectors. To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. 3 Structured Distributional Semantics In this paper, we propose"
P13-2083,W13-3203,1,0.111648,"tion between a and b, and K is the additional knowledge used to define composition. While this framework is quite general, the actual models considered in the literature tend to disregard K and r and mostly perform component-wise addition and multiplication, with slight variations, of the two vectors. To the best of our knowledge the formulation of composition we propose is the first to account for both K and r within this compositional framework. 3 Structured Distributional Semantics In this paper, we propose an approach to incorporate structure into distributional semantics (more details in Goyal et al. (2013)). The word distributions drawn from the context defined by a set of relations anchored on the target word (or phrase) form a set of vectors, namely a matrix for the target word. One axis of the matrix runs over all the relations and the other axis is over the distributional word vocabulary. The cells store word counts (or PMI scores, or other measures of word association). Note that collapsing the rows of the matrix provides the standard dependency based distributional representation. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word me"
P13-2083,A00-2011,0,0.012312,"PropStore also stores their POS tags, lemmas, and Wordnet supersenses. This helps to generalize our representation when surface-form distributions are sparse. The PropStore can be used to query for the expectations of words, supersenses, relations, etc., around a given word. In the example in Figure 1, the query (SST(W1 ) = verb.consumption, ?, dobj) i.e. “what is consumed” might return expectations [pasta:1, spaghetti:1, mice:1 . . . ]. Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas usPantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 46"
P13-2083,W11-0114,0,0.0127933,"matrix for the target word. One axis of the matrix runs over all the relations and the other axis is over the distributional word vocabulary. The cells store word counts (or PMI scores, or other measures of word association). Note that collapsing the rows of the matrix provides the standard dependency based distributional representation. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. 3.1 Building Representation: The PropStore To build a lexicon of SDSM matrices for a given vocabulary we first construct a proposition knowledge base (the PropStore) created by parsing the Simple English Wikipedia. Dependency arcs are stored as 3-tuples of the form hw1 , r, w2 i, denoting an occurrence of words w1 , word w2 related by r. We also store sentence indices for tripl"
P13-2083,D10-1048,0,0.0169423,"ributing authors 467 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 467–473, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2.2 (1957). Several works have defined approaches to modelling context-word distributions anchored on a target word, topic, or sentence position. Collectively these approaches are called Distributional Semantic Models (DSMs). Event Co-reference Resolution While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. We, on the other hand, attempt a slightly different problem of making co-referentiality judgements on event-coreference candidate pairs. While DSMs have been very successful on a variety of tasks, they are not an effective model of semantics as they lack properties such as compositionality or the ability to handle operators such as negation. In order to model a stronger form"
P13-2083,P10-1093,0,0.0216476,"d on the target word (or phrase) form a set of vectors, namely a matrix for the target word. One axis of the matrix runs over all the relations and the other axis is over the distributional word vocabulary. The cells store word counts (or PMI scores, or other measures of word association). Note that collapsing the rows of the matrix provides the standard dependency based distributional representation. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. 3.1 Building Representation: The PropStore To build a lexicon of SDSM matrices for a given vocabulary we first construct a proposition knowledge base (the PropStore) created by parsing the Simple English Wikipedia. Dependency arcs are stored as 3-tuples of the form hw1 , r, w2 i, denoting an occurrence of words w1 , w"
P13-2083,J98-1004,0,0.174742,"ate that our model outperforms both state-of-the-art window-based word embeddings as well as simple approaches to compositional semantics previously employed in the literature. 1 Introduction Distributional Semantic Models (DSM) are popular in computational semantics. DSMs are based on the hypothesis that the meaning of a word or phrase can be effectively captured by the distribution of words in its neighborhood. They have been successfully used in a variety of NLP tasks including information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), wordsense discrimination (Schütze, 1998) and disambiguation (McCarthy et al., 2004), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003) and selectional preference modeling (Erk, 2007). A shortcoming of DSMs is that they ignore the syntax within the context, thereby reducing the distribution to a bag of words. Composing the 2 Related Work Next, we relate and contrast our work to prior research in the fields of Distributional Vector Space Models, Semantic Compositionality and Event Co-reference Resolution. 2.1 DSMs and Compositionality The underlying idea that “a word is characterized by the company"
P13-2083,D11-1097,0,0.0253663,"Missing"
P13-2083,D12-1110,0,0.0336103,"stributional word vocabulary. The cells store word counts (or PMI scores, or other measures of word association). Note that collapsing the rows of the matrix provides the standard dependency based distributional representation. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. However, these models do not efficiently account for structure. 3.1 Building Representation: The PropStore To build a lexicon of SDSM matrices for a given vocabulary we first construct a proposition knowledge base (the PropStore) created by parsing the Simple English Wikipedia. Dependency arcs are stored as 3-tuples of the form hw1 , r, w2 i, denoting an occurrence of words w1 , word w2 related by r. We also store sentence indices for triples as this allows us to achieve an intuitive technique to achieve compositionality. In addition to the word"
P13-2083,P09-1074,0,0.0125814,"y Firth ∗ *Equally contributing authors 467 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 467–473, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2.2 (1957). Several works have defined approaches to modelling context-word distributions anchored on a target word, topic, or sentence position. Collectively these approaches are called Distributional Semantic Models (DSMs). Event Co-reference Resolution While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the twoway feedback between events and their arguments. We, on the other hand, attempt a slightly different problem of making co-referentiality judgements on event-coreference candidate pairs. While DSMs have been very successful on a variety of tasks, they are not an effective model of semantics as they lack properties such as compositionality or the ability to handle operators such as negation. In orde"
P13-2083,P10-1097,0,0.0179552,"e. The PropStore can be used to query for the expectations of words, supersenses, relations, etc., around a given word. In the example in Figure 1, the query (SST(W1 ) = verb.consumption, ?, dobj) i.e. “what is consumed” might return expectations [pasta:1, spaghetti:1, mice:1 . . . ]. Relations and POS tags are obtained using a dependency parser Tratz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas usPantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 468 Figure 1: Sample sentences & triples ing Wordnet Fellbaum (1998). 3.2 (1) and (2) involve querying the PropStore for the individual tokens, noun.person an"
P13-2083,D12-1018,0,0.030136,"atz and Hovy (2011), supersense tags using sstlight Ciaramita and Altun (2006), and lemmas usPantel and Lin (2000) and Erk and Padó (2008) attempt to include syntactic context in distributional models. A quasi-compositional approach was attempted in Thater et al. (2010) by a combination of first and second order context vectors. But they do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. Furthermore, we also include structure into our method of composition. Prior work in structure aware methods to the best of our knowledge are (Weisman et al., 2012) and (Baroni and Lenci, 2010). However, these methods do not explicitly model composition. 468 Figure 1: Sample sentences & triples ing Wordnet Fellbaum (1998). 3.2 (1) and (2) involve querying the PropStore for the individual tokens, noun.person and eat. Let the resulting matrices be M1 and M2 , respectively. In step (3), SentIDs (sentences where the two words appear with the specified relation) are obtained by taking the intersection between the nsubj component vectors of the two matrices M1 and M2 . In step (4), the entries of the original matrices M1 and M2 are intersected with this list o"
P13-2083,D10-1115,0,\N,Missing
P13-2083,D11-1116,1,\N,Missing
P14-1016,P11-1055,0,0.0115094,"consuming to generate, distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Sec"
P14-1016,P11-1040,0,0.0339493,"emainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Section 3. The details of our model are presented in Section 4. We present experimental results in Section 5 and conclude in Section 6. 166 Figure 2: Example of fetching tweets containing entity USC mention from Miranda Cosgrove (an American actress and singer-songwriter)’s twitter stream. Figure 1: Illustration of Goolge Plus “knowledge base”. fields such as protein relation extraction (Craven et al., 1999; Ravikumar et al., 2012), event extraction from Twitter (Benson et al., 2011), sentiment analysis (Go et al., 2009) and Wikipedia infobox generation (Wu and Weld, 2007). Education/Job We first used the Google Plus API5 (shown in Figure 1) to obtain a seed set of users whose profiles contain both their education/job status and a link to their twitter account.6 Then, we fetched tweets containing the mention of the education/job entity from each correspondent user’s twitter stream using Twitter’s search API7 (shown in Figure 2) and used them to construct positive bags of tweets expressing the associated attribute, namely E DUCATION(Useri , Entityj ), or E MPLOYER(Useri ,"
P14-1016,P09-1113,0,0.688937,"annotations, which are expensive and time consuming to generate, distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The"
P14-1016,D13-1114,0,0.0432055,"N , HOMETOWN , LIVING LOCA TION , FAMILY MEMBERS and so on, where training data can be obtained by matching ground truth retrieved from multiple types of online social media such as Facebook, Google Plus, or LinkedIn. Our contributions are as follows: Related Work While user profile inference from social media has received considerable attention (Al Zamal et al., 2012; Rao and Yarowsky, 2010; Rao et al., 2010; Rao et al., 2011), most previous work has treated this as a classification task where the goal is to predict unary predicates describing attributes of the user. Examples include gender (Ciot et al., 2013; Liu and Ruths, 2013; Liu et al., 2012), age (Rao et al., 2010), or political polarity (Pennacchiotti and Popescu, 2011; Conover et al., 2011). A significant challenge that has limited previous efforts in this area is the lack of available training data. For example, researchers obtain training data by employing workers from Amazon Mechanical Turk to manually identify users’ gender from profile pictures (Ciot et al., 2013). This approach is appropriate for attributes such as gender with a small numbers of possible values (e.g., male or female), for which the values can be directly identified."
P14-1016,N13-1039,0,0.0107717,"s using Equ.6. k zi,e = argmax Ψ(z 0 , Xi , Fik ) z0 AFFINITY Job 14.5 Table 3: Affinity values for Education and Job. 5.1 Preprocessing and Experiment Setup Each tweet posting is tokenized using Twitter NLP tool introduced by Noah’s Ark14 with # and @ separated following tokens. We assume that attribute values should be either name entities or terms following @ and #. Name entities are extracted using Ritter et al.’s NER system (2011). Consecutive tokens with the same named entity tag are chunked (Mintz et al., 2009). Part-ofspeech tags are assigned based on Owoputi et al’s tweet POS system (Owoputi et al., 2013). Data is divided in halves. The first is used as training data and the other as testing data. (6) For NEIGH - LATENT setting, attributes for each node along the network are treated latent and user attribute prediction depends on attributes of his neighbors. The objective function for joint inference would be difficult to optimize exactly, and algorithms for doing so would be unlikely to scale to network of the size we consider. Instead, we use a sieve-based greedy search approach to inference (shown in Figure 3) inspired by recent work on coreference resolution (Raghunathan et al., 2010). Att"
P14-1016,D10-1048,0,0.0124034,"system (Owoputi et al., 2013). Data is divided in halves. The first is used as training data and the other as testing data. (6) For NEIGH - LATENT setting, attributes for each node along the network are treated latent and user attribute prediction depends on attributes of his neighbors. The objective function for joint inference would be difficult to optimize exactly, and algorithms for doing so would be unlikely to scale to network of the size we consider. Instead, we use a sieve-based greedy search approach to inference (shown in Figure 3) inspired by recent work on coreference resolution (Raghunathan et al., 2010). Attributes are initialized using only text features, maximizing Ψtext (e, Xi ), and ignoring network information. Then for each user we iteratively reestimate their profile given both their text features and network features (computed based on the current predictions made for their friends) which provide additional evidence. In this way, highly confident predictions will be made strictly from text in the first round, then the network can either support or contradict low confidence predictions as more decisions are made. This process continues until no changes are made at which point the algo"
P14-1016,D11-1141,1,0.441201,"Missing"
P14-1016,Q13-1030,1,0.223646,"re expensive and time consuming to generate, distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our data"
P14-1016,D12-1042,0,0.00990537,"distant supervision leverages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Section 3. The details of"
P14-1016,P12-1076,0,0.011214,"verages readily available structured data sources as a weak source of supervision for relation extraction from related text corpora (Craven et al., 1999). For example, suppose r(e1 , e2 ) = IsIn(P aris, F rance) is a ground tuple in the database and s =“Paris is the capital of France” contains synonyms for both “Paris” and “France”, then we assume that s may express the fact r(e1 , e2 ) in some way and can be used as positive training examples. In addition to the wide use in text entity relation extraction (Mintz et al., 2009; Ritter et al., 2013; Hoffmann et al., 2011; Surdeanu et al., 2012; Takamatsu et al., 2012), distant supervision has been applied to multiple • We present a large-scale dataset for this task gathered from various structured and unstructured social media sources. • We demonstrate the benefit of jointly reasoning about users’ social network structure when extracting their profiles from text. • We experimentally demonstrate the effectiveness of our approach on 3 relations: SPOUSE, J OB and EDUCATION. The remainder of this paper is organized as follows: We summarize related work in Section 2. The creation of our dataset is described in Section 3. The details of our model are presented i"
P14-1060,D10-1115,0,0.0862373,"Missing"
P14-1060,P02-1034,0,0.0765965,"current motifs such as ‘water table’ or ‘breaking a fall’ would be helpful in building more effective semantic representations. A significant advantage of a frequency driven view is that it makes the concern of compositionality of recurrent phrases immaterial. If a motif occurs frequently enough in common parlance, its semantics could be captured with distributional models irrespective of whether its associated semantics are compositional or acquired. Tree kernels Tree Kernel methods have gained popularity in the last decade for capturing syntactic information in the structure of parse trees (Collins and Duffy, 2002; Moschitti, 2006). Instead of procuring explicit representations, the kernel paradigm directly focuses on the larger goal of quantifying semantic similarity of larger linguistic units. Structural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels. These methods have been useful for eclectic tasks such as parsing, NER, semantic role labeling, and sentiment analysis. Recent approaches such as by Croce et al. (2011) and Srivastava et al. (2013) have attempted to provide formulations to incorporate semantics into tree kernels"
P14-1060,W02-1001,0,0.0617556,"res and penalties. We describe learning of the model parameters with fully annotated training data, as well as an approach for learning motif segmentation that requires only partial supervision. Supervised learning: In the supervised case, optimal state sequences y(k) are fully observed for the training set. For this purpose, we created a dataset of 1000 sentences from the Simple English Wikipedia and the Gigaword Corpus, and manually annotated it with motif boundaries using BRAT (Stenetorp et al., 2012). In this case, learning can follow the online structured perceptron learning procedure by Collins (2002), where weights updates for the k’th training example (x(k) , y(k) ) are given as: Algorithm 1 1: Input: Partially labeled data D = {(x, y)i } 2: Output: Weights w 3: Initialization: Set wi randomly, ∀i 4: for i : 1 to maxIter do 5: Decode D with current w to find optimal Viterbi paths that agree with (partial) ground truths. 6: Run Structured Perceptron algorithm with decoded tag-sequences to update weights w 7: end for 8: return w wi ← wi + α(fi (x(k) , y(k) ) − fi (x(k) , y0 )) Here y0 = Decode(x(k) , w) is the optimal Viterbi decoding using the current estimates of the weights. Updates are"
P14-1060,P05-1015,0,0.00592809,"ss of some of the other motifs). In particular, consider the second example, where the model picks ‘white elephant’ as a motif. In such cases, the disambiguating influence of context incorporated by the motif is apparent. Elephant tusks trunk african white indian baby 4.2 Distributional representations For evaluating distributional representations for motifs (in terms of other motifs) learnt by the framework, we test these representations in two downstream tasks: sentence polarity classification and metaphor detection. For sentence polarity, we consider the Cornell Sentence Polarity corpus by Pang and Lee (2005), where the task is to classify the polarity of a sentence as positive or negative. The data consists of 10662 sentences from movie reviews that have been annotated as either positive or negative. For composing the motifs representations to get judgments on semantic similarity of sentences, we use our recent Vector Tree Kernel approach The VTK approach defines a convolutional kernel over graphs defined by the dependency parses of sentences, using a vector representation at each graph node that representing a single lexical token. For our purposes, we modify the approach to merge the nodes of a"
P14-1060,D11-1096,0,0.0161042,"e gained popularity in the last decade for capturing syntactic information in the structure of parse trees (Collins and Duffy, 2002; Moschitti, 2006). Instead of procuring explicit representations, the kernel paradigm directly focuses on the larger goal of quantifying semantic similarity of larger linguistic units. Structural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels. These methods have been useful for eclectic tasks such as parsing, NER, semantic role labeling, and sentiment analysis. Recent approaches such as by Croce et al. (2011) and Srivastava et al. (2013) have attempted to provide formulations to incorporate semantics into tree kernels through the use of distributional word vectors at the individual word-nodes. While this framework is attractive in the lack of assumptions on representation that it makes, the use of distributional embeddings for individual tokens means 2.3 Identifying multi-word expressions Several approaches have focused on supervised identification of multi-word expressions (MWEs) through statistical (Pecina, 2008; Villavicencio et al., 2007) and linguistically motivated (Piao et al., 2005) techni"
P14-1060,P07-1028,0,0.0201153,"ervers. Second, the value of learning a very infrequent semantic mapping is likely marginal. This motivates the need for a frequency-driven view of lexical semantics. In particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below. Distributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics (Turney et al., 2010). Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al., 2008). However, while conventional DSMs consider collocaTraditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items. In this work, we present a frequencydriven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents, or motifs. The f"
P14-1060,D12-1110,0,0.016447,"entence into such motifs using a feature-set 635 that it suffers from the same shortcomings as described for the example in Table 1, and hence these methods model semantic relations between wordnodes very weakly. Figure 1 shows an example of the shortcomings of this general approach. Zamparelli’s (2010) model that differentially models content and function words for semantic composition, and Goyal et al.’s SDSM model (2013) that incorporates syntactic roles to model semantic composition. Notable among the most effective distributional representations are the recent deep-learning approaches by Socher et al. (2012), that model vector composition through non-linear transformations. While word embeddings and language models from such methods have been useful for tasks such as relation classification, polarity detection, event coreference and parsing; much of existing literature on composition is based on abstract linguistic theory and conjecture, and there is little evidence to support that learnt representations for larger linguistic units correspond to their semantic meanings. While works such as the SDSM model suffer from the problem of sparsity in composing structures beyond bigrams and trigrams, meth"
P14-1060,D11-1129,0,0.0315448,"Missing"
P14-1060,D13-1144,1,0.922077,"he last decade for capturing syntactic information in the structure of parse trees (Collins and Duffy, 2002; Moschitti, 2006). Instead of procuring explicit representations, the kernel paradigm directly focuses on the larger goal of quantifying semantic similarity of larger linguistic units. Structural kernels for NLP are based on matching substructures within two parse trees , consisting of word-nodes with similar labels. These methods have been useful for eclectic tasks such as parsing, NER, semantic role labeling, and sentiment analysis. Recent approaches such as by Croce et al. (2011) and Srivastava et al. (2013) have attempted to provide formulations to incorporate semantics into tree kernels through the use of distributional word vectors at the individual word-nodes. While this framework is attractive in the lack of assumptions on representation that it makes, the use of distributional embeddings for individual tokens means 2.3 Identifying multi-word expressions Several approaches have focused on supervised identification of multi-word expressions (MWEs) through statistical (Pecina, 2008; Villavicencio et al., 2007) and linguistically motivated (Piao et al., 2005) techniques. More recently, hybrid m"
P14-1060,W11-0114,0,0.0331225,"Missing"
P14-1060,E12-2021,0,0.0225111,"Missing"
P14-1060,W13-0907,1,0.646489,"lematic issue of differential compositional and non-compositional usage of words. Future work can focus on a more thorough quantitative evaluation of the paradigm, as well as extension to model non-contiguous motifs. proposed by Lapata et al. The model is competitive with the state-of-the-art VTK (Srivastava et al., 2013) that uses the SENNA neural embeddings by Collobert et al. (2011). CRF SVM+DSM VTK+ SENNA VTK+ MotifDSM P 0.74 0.63 0.67 0.70 R 0.50 0.80 0.87 0.87 F1 0.59 0.71 0.76 0.78 Table 5: Results for Metaphor identification On the metaphor detection task, we use the Metaphor dataset (Hovy et al., 2013). The data consists of sentences with defined phrases, and the task consists of identifying the linguistic use in these phrases as metaphorical or literal. For this task, the motif based model is expected to perform well as common metaphorical usage is generally through idiosyncratic MWEs, which the motif based models is specially geared to capture through the features of the segmentation model. For this task, we again use the VTK formalism for combining vector representations of the individual motifs. Table 5 shows that the motif-based DSM does better than discriminative models such as CRFs a"
P14-1060,E14-1051,0,0.0591811,"Missing"
P14-1060,D11-1077,0,0.013103,"through the use of distributional word vectors at the individual word-nodes. While this framework is attractive in the lack of assumptions on representation that it makes, the use of distributional embeddings for individual tokens means 2.3 Identifying multi-word expressions Several approaches have focused on supervised identification of multi-word expressions (MWEs) through statistical (Pecina, 2008; Villavicencio et al., 2007) and linguistically motivated (Piao et al., 2005) techniques. More recently, hybrid methods based on both statistical as well as linguistic features have been popular (Tsvetkov and Wintner, 2011). Ramisch et al. (2008) demonstrate that adding part-of-speech tags to frequency counts substantially improves performance. Other methods have attempted to exploit morphological, syntactic and semantic characteristics of MWEs. In 636 particular, approaches such as Bannard (2007) use syntactic rigidity to characterize MWEs. While existing work has focused on the classification task of categorizing a phrasal constituent as a MWE or a non-MWE, the general ideas of most of these works are in line with our current framework, and the feature-set for our motif segmentation model is designed to subsum"
P14-1060,J03-4004,0,0.00901572,"very infrequent semantic mapping is likely marginal. This motivates the need for a frequency-driven view of lexical semantics. In particular, such a perspective can be especially advantageous for distributional semantics for reasons we outline below. Distributional semantic models (DSMs) that represent words as distributions over neighbouring contexts have been particularly effective in capturing fine-grained lexical semantics (Turney et al., 2010). Such models have engendered improvements in diverse applications such as selectional preference modeling (Erk, 2007), word-sense discrimination (McCarthy and Carroll, 2003), automatic dictionary building (Curran, 2003), and information retrieval (Manning et al., 2008). However, while conventional DSMs consider collocaTraditional models of distributional semantics suffer from computational issues such as data sparsity for individual lexemes and complexities of modeling semantic composition when dealing with structures larger than single lexical items. In this work, we present a frequencydriven paradigm for robust distributional semantics in terms of semantically cohesive lineal constituents, or motifs. The framework subsumes issues such as differential compositio"
P14-1060,D07-1110,0,0.0149479,"labeling, and sentiment analysis. Recent approaches such as by Croce et al. (2011) and Srivastava et al. (2013) have attempted to provide formulations to incorporate semantics into tree kernels through the use of distributional word vectors at the individual word-nodes. While this framework is attractive in the lack of assumptions on representation that it makes, the use of distributional embeddings for individual tokens means 2.3 Identifying multi-word expressions Several approaches have focused on supervised identification of multi-word expressions (MWEs) through statistical (Pecina, 2008; Villavicencio et al., 2007) and linguistically motivated (Piao et al., 2005) techniques. More recently, hybrid methods based on both statistical as well as linguistic features have been popular (Tsvetkov and Wintner, 2011). Ramisch et al. (2008) demonstrate that adding part-of-speech tags to frequency counts substantially improves performance. Other methods have attempted to exploit morphological, syntactic and semantic characteristics of MWEs. In 636 particular, approaches such as Bannard (2007) use syntactic rigidity to characterize MWEs. While existing work has focused on the classification task of categorizing a phr"
P14-1060,P08-1028,0,0.317617,"et with only marginal success. While there is considerable variety in approaches and formulations, existing approaches for phrasal level and sentential semantics can broadly be partitioned into two categories. 2.1 Compositional approaches These have aimed at using semantic representations for individual words to learn semantic representations for larger linguistic structures. These methods implicitly make an assumption of compositionality, and often include explicit computational models of compositionality. Notable among such models are the additive and multiplicative models of composition by Mitchell and Lapata (2008), Grefenstette et al. (2010), Baroni and • We present a framework for extending distributional semantics to learn semantic representations of both words and phrases in terms of recurrent motifs, rather than arbitrary word tokens • We present a simple model to segment a sentence into such motifs using a feature-set 635 that it suffers from the same shortcomings as described for the example in Table 1, and hence these methods model semantic relations between wordnodes very weakly. Figure 1 shows an example of the shortcomings of this general approach. Zamparelli’s (2010) model that differentiall"
P14-1060,W07-1101,0,\N,Missing
P14-1060,W13-3203,1,\N,Missing
P14-1147,I13-1039,0,0.0487601,"on the review text, reviewer, and product to identify duplicate opinions, i.e., opinions that appear more than once in the corpus with similar contexts. Wu et al. (2010) propose an alternative strategy to detect deceptive opinion spam in the absence of a gold standard. Yoo and Gretzel (2009) gathered 40 truthful and 42 deceptive hotel reviews and manually compare the linguistic differences between them. Ott et al. created a gold-standard collection by employing Turkers to write fake reviews, and follow-up research was based on their data (Ott et al., 2012; Ott et al., 2013; Li et al., 2013b; Feng and Hirst, 2013). For example, Song et al. (2012) looked into syntactic features from Context Free Grammar parse trees to improve the classifier performance. A step further, Feng and Hirst (2013) make use of degree of compatibility between the personal experiment and a collection of reference reviews about the same product rather than simple textual features. In addition to exploring text or linguistic features in deception, some existing work looks into customers’ behavior to identify deception (Mukherjee et al., 2013a). For example, Mukherjee et al. (2011; 2012) delved into group behavior to identify group"
P14-1147,P12-2034,0,0.368025,"Missing"
P14-1147,P13-2039,1,0.838567,"on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012). However, recent studies show that deceptive opinion spam is not easily identified by human readers (Ott et al., 2011). An alternative approach, as introduced by Ott et al. (2011), crowdsourced deceptive reviews using Amazon Mechanical Turk.3 A couple of follow-up works have been introduced based on Ott et al.’s dataset, including estimating prevalence of deception in online reviews (Ott et al., 2012), identification of negative deceptive opinion spam (Ott et al., 2013), and identifying manipulated offerings (Li et al., 2013b). Despite the advantages of soliciting deceptive gold-standard material from Turkers (it is easy, large-scale, and affordable), it is unclear whether Turkers are representative of the general population that generate fake reviews, or in other words, Ott et al.’s data set may correspond to only one type of online deceptive opinion spam — fake reviews generated by people who have never been to offerings or experienced the entities. Specifically, according to their findings (Ott et al., 2011; has updated their guidelines on the use of endorsements and testimonials in advertising to suggest that"
P14-1147,D13-1199,1,0.866691,"on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012). However, recent studies show that deceptive opinion spam is not easily identified by human readers (Ott et al., 2011). An alternative approach, as introduced by Ott et al. (2011), crowdsourced deceptive reviews using Amazon Mechanical Turk.3 A couple of follow-up works have been introduced based on Ott et al.’s dataset, including estimating prevalence of deception in online reviews (Ott et al., 2012), identification of negative deceptive opinion spam (Ott et al., 2013), and identifying manipulated offerings (Li et al., 2013b). Despite the advantages of soliciting deceptive gold-standard material from Turkers (it is easy, large-scale, and affordable), it is unclear whether Turkers are representative of the general population that generate fake reviews, or in other words, Ott et al.’s data set may correspond to only one type of online deceptive opinion spam — fake reviews generated by people who have never been to offerings or experienced the entities. Specifically, according to their findings (Ott et al., 2011; has updated their guidelines on the use of endorsements and testimonials in advertising to suggest that"
P14-1147,P11-1032,1,0.703381,"used on developing supervised learningbased algorithms to help users identify deceptive opinion spam, which are highly dependent upon high-quality gold-standard labeled data (Jindal and Liu, 2008; Jindal et al., 2010; Lim et al., 2010; Wang et al., 2011; Wu et al., 2010). Studies in the literature rely on a couple of approaches for obtaining labeled data, which usually fall into two categories. The first relies on the judgements of human annotators (Jindal et al., 2010; Mukherjee et al., 2012). However, recent studies show that deceptive opinion spam is not easily identified by human readers (Ott et al., 2011). An alternative approach, as introduced by Ott et al. (2011), crowdsourced deceptive reviews using Amazon Mechanical Turk.3 A couple of follow-up works have been introduced based on Ott et al.’s dataset, including estimating prevalence of deception in online reviews (Ott et al., 2012), identification of negative deceptive opinion spam (Ott et al., 2013), and identifying manipulated offerings (Li et al., 2013b). Despite the advantages of soliciting deceptive gold-standard material from Turkers (it is easy, large-scale, and affordable), it is unclear whether Turkers are representative of the ge"
P14-1147,N13-1053,1,0.453732,"Missing"
P14-1147,D13-1113,0,0.00665599,"res from Context Free Grammar parse trees to improve the classifier performance. A step further, Feng and Hirst (2013) make use of degree of compatibility between the personal experiment and a collection of reference reviews about the same product rather than simple textual features. In addition to exploring text or linguistic features in deception, some existing work looks into customers’ behavior to identify deception (Mukherjee et al., 2013a). For example, Mukherjee et al. (2011; 2012) delved into group behavior to identify group of reviewers who work collaboratively to write fake reviews. Qian and Liu (2013) identified multiple user IDs that are generated by the same author, as these authors are more likely to generate deceptive reviews. In the psychological literature, researchers have looked into possible linguistic cues to deception (Newman et al., 2003), such as decreased spatial detail, which is consistent with theories of reality monitoring (Johnson and Raye, 1981), increased negative emotion terms (Newman et al., 2003), or the writing style difference between informative (truthful) and imaginative (deceptive) writings in (Rayson et al., 2001). The former typically consists of more nouns, a"
P14-2005,N06-2015,1,0.757794,"45.93 36.45 61.72 55.12 56.01 62.33 36.18 39.79 62.94 40.76 37.18 BLANC 58.75 55.04 55.42 53.86 52.87 52.65 54.42 52.11 46.47 50.44 46.04 45.10 34.80 36.54 31.85 40 ● ● ● 10 20 ●●●● ● ●● ● ● ● ●● ● ●● ● ● ● ● ● MUC ● 30 40 50 60 70 CEAF−m 0 10 ● 20 30 40 50 CEAF−e Figure 1: Correlation plot between the proposed BLANC and the other measures based on the CoNLL 2011/2012 results. All values are F1 scores. other measures along R, P and F1 (Table 3), showing that BLANC is able to capture most entity-based similarities measured by B-cubed and CEAF. However, the CoNLL data sets come from OntoNotes (Hovy et al., 2006), where singleton entities are not annotated, and BLANC has a wider dynamic range on data sets with singletons (Recasens and Hovy, 2011). So the correlations will likely be lower on data sets with singleton entities. 6 Conclusion The original BLANC-gold (Recasens and Hovy, 2011) requires that system mentions be identical to gold mentions, which limits the metric’s utility since detected system mentions often have missing key mentions or spurious mentions. The proposed BLANC is free from this assumption, and we have shown that it subsumes the original BLANCgold. Since BLANC works on imperfect s"
P14-2005,H05-1004,1,0.892092,"m mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data. 1 Introduction Coreference resolution aims at identifying natural language expressions (or mentions) that refer to the same entity. It entails partitioning (often imperfect) mentions into equivalence classes. A critically important problem is how to measure the quality of a coreference resolution system. Many evaluation metrics have been proposed in the past two decades, including the MUC measure (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and, more recently, BLANCgold (Recasens and Hovy, 2011). B-cubed and CEAF treat entities as sets of mentions and measure the agreement between key (or gold standard) entities and response (or system-generated) entities, while MUC and BLANC-gold are link-based. In particular, MUC measures the degree of agreement between key coreference links (i.e., links among mentions within entities) and response coreference links, while non-coreference links (i.e., links formed by mentions from different entities) are not explicitly taken into account. This leads to a phenomenon where coreference systems ou"
P14-2005,W11-1901,1,0.88997,"cd), (ce)}. Therefore, Ck ∩ Cr = {(bc)}, Nk ∩ Nr = {(bd), (cd)}, and Rc = 13 , Pc = 12 , Fc = 25 ; Rn = 2 2 4 17 3 , Pn = 4 , Fn = 7 . Finally, BLANC = 35 . Example 2. Key entity is {a}; response entity is {b}. This is boundary case (1): BLANC = 0. Example 3. Key entities are {a}{b}{c}; response entities are {a}{b}{d}. This is boundary case (2): there are no coreference links. Since Nk = {(ab), (bc), (ca)}, Results 5.1 CoNLL-2011/12 We have updated the publicly available CoNLL coreference scorer1 with the proposed BLANC, and used it to compute the proposed BLANC scores for all the CoNLL 2011 (Pradhan et al., 2011) and 2012 (Pradhan et al., 2012) participants in the official track, where participants had to automatically predict the mentions. Tables 1 and 2 report the updated results.2 5.2 Correlation with Other Measures Figure 1 shows how the proposed BLANC measure works when compared with existing metrics such as MUC, B-cubed and CEAF, using the BLANC and F1 scores. The proposed BLANC is highly positively correlated with the 1 http://code.google.com/p/reference-coreference-scorers The order is kept the same as in Pradhan et al. (2011) and Pradhan et al. (2012) for easy comparison. 2 27 30 R P F1 0.975"
P14-2005,W12-4501,1,0.869863,"{(bc)}, Nk ∩ Nr = {(bd), (cd)}, and Rc = 13 , Pc = 12 , Fc = 25 ; Rn = 2 2 4 17 3 , Pn = 4 , Fn = 7 . Finally, BLANC = 35 . Example 2. Key entity is {a}; response entity is {b}. This is boundary case (1): BLANC = 0. Example 3. Key entities are {a}{b}{c}; response entities are {a}{b}{d}. This is boundary case (2): there are no coreference links. Since Nk = {(ab), (bc), (ca)}, Results 5.1 CoNLL-2011/12 We have updated the publicly available CoNLL coreference scorer1 with the proposed BLANC, and used it to compute the proposed BLANC scores for all the CoNLL 2011 (Pradhan et al., 2011) and 2012 (Pradhan et al., 2012) participants in the official track, where participants had to automatically predict the mentions. Tables 1 and 2 report the updated results.2 5.2 Correlation with Other Measures Figure 1 shows how the proposed BLANC measure works when compared with existing metrics such as MUC, B-cubed and CEAF, using the BLANC and F1 scores. The proposed BLANC is highly positively correlated with the 1 http://code.google.com/p/reference-coreference-scorers The order is kept the same as in Pradhan et al. (2011) and Pradhan et al. (2012) for easy comparison. 2 27 30 R P F1 0.975 0.981 0.941 0.797 0.844 0.942 0"
P14-2005,P09-1074,0,0.0603219,"ll, precision and F-measure separately on coreference and noncoreference links in the usual way, and defines the overall recall, precision and F-measure as the mean of the respective measures for coreference and non-coreference links. The BLANC-gold metric was developed with the assumption that response mentions and key mentions are identical. In reality, however, mentions need to be detected from natural language text and the result is, more often than not, imperfect: some key mentions may be missing in the response, and some response mentions may be spurious—so-called “twinless” mentions by Stoyanov et al. (2009). Therefore, the identicalmention-set assumption limits BLANC-gold’s applicability when gold mentions are not available, or when one wants to have a single score measuring both the quality of mention detection and coreference resolution. The goal of this paper is to extend the BLANC-gold metric to imperfect response mentions. We first briefly review the original definition of BLANC, and rewrite its definition using set notation. We then argue that the gold-mention assumption in Recasens and Hovy (2011) can be lifted without changing the original definition. In fact, the proposed BLANC metric s"
P14-2005,M95-1005,0,0.704928,"oposed BLANC falls back seamlessly to the original one if system mentions are identical to gold mentions, and it is shown to strongly correlate with existing metrics on the 2011 and 2012 CoNLL data. 1 Introduction Coreference resolution aims at identifying natural language expressions (or mentions) that refer to the same entity. It entails partitioning (often imperfect) mentions into equivalence classes. A critically important problem is how to measure the quality of a coreference resolution system. Many evaluation metrics have been proposed in the past two decades, including the MUC measure (Vilain et al., 1995), B-cubed (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and, more recently, BLANCgold (Recasens and Hovy, 2011). B-cubed and CEAF treat entities as sets of mentions and measure the agreement between key (or gold standard) entities and response (or system-generated) entities, while MUC and BLANC-gold are link-based. In particular, MUC measures the degree of agreement between key coreference links (i.e., links among mentions within entities) and response coreference links, while non-coreference links (i.e., links formed by mentions from different entities) are not explicitly taken into account. Th"
P14-2006,W12-4501,1,0.648168,"ity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations"
P14-2006,W10-4305,1,0.648818,"ainst them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified with respect to predicted, as opposed to key (or gold) mentions. Several variations have been proposed that manipulate either, or both, the key and predicted mentions in order to get a one-to-one mapping. On the other hand, the metric BLANC was, until recently, limited to scoring partitions of key mentions. In this paper, we (i) argue that mention manipulation for scoring predicted mentions is unnecessary, and potentially h"
P14-2006,D09-1101,1,0.646014,"e mapping between the key and predicted mentions, assuming that the original measures cannot be applied to predicted mentions. Below we first provide an overview of these variations and then discuss the unnecessity of this assumption. Coining the term twinless mentions for those mentions that are either spurious or missing from the predicted mention set, Stoyanov et al. (2009) proposed two variations to B3 — B3all and B30 —to handle them. In the first variation, all predicted twinless mentions are retained, whereas the latter discards them and penalizes recall for twinless predicted mentions. Rahman and Ng (2009) proposed another variation by removing “all and only those twinless system mentions that are singletons before applying B3 and CEAF.” Following upon this line of research, Cai and Strube (2010) proposed a unified solution for both B3 and CEAF m , leaving the question of handling CEAF e as future work because “it produces unintuitive results.” The essence of their solution involves manipulating twinless key and predicted mentions by adding them either from the predicted partition to the key partition or vice versa, depending on whether one is computing precision or recall. The Cai and Strube ("
P14-2006,I13-1193,1,0.853507,"very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recas"
P14-2006,P11-1082,1,0.341706,"he OntoNotes corpus, and by the i2b2 2011 shared task on coreference resolution using an assortment of clinical notes corpora (Uzuner et al., 2012).1 It was later identified by Recasens et al. (2013) that there was a bug in the implementation of this variation in the scorer used for the CoNLL-2011/2012 tasks. We have not tested the correctness of this variation in the scoring package used for the i2b2 shared task. However, it turns out that the CEAF metric (Luo, 2005) was always intended to work seamlessly on predicted mentions, and so has been the case with the B3 metric.2 In a latter paper, Rahman and Ng (2011) correctly state that “CEAF can compare partitions with twinless mentions without any modification.” We will look at this further in Section 4.3. We argue that manipulations of key and response mentions/entities, as is done in the existing B3 variations, not only confound the evaluation process, but are also subject to abuse and can seriously jeopardize the fidelity of the evaluVariations of Scoring Measures Two commonly used coreference scoring metrics —B3 and CEAF—are underspecified in their application for scoring predicted, as opposed to key mentions. The examples in the papers describing"
P14-2006,W09-2411,1,0.899331,"Missing"
P14-2006,doddington-etal-2004-automatic,0,0.0166341,"ith this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms. 1 Introduction Coreference resolution is a key task in natural language processing (Jurafsky and Martin, 2008) aiming to detect the referential expressions (mentions) in a text that point to the same entity. Roughly over the past two decades, research in coreference (for the English language) had been plagued by individually crafted evaluations based on two central corpora—MUC (Hirschman and Chinchor, 1997; Chinchor and Sundheim, 2003; Chinchor, 2001) and ACE (Doddington et al., 2004). Experimental parameters ranged from using perfect (gold, or key) mentions as input for 30 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30–35, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics the BLANC metric for partitions of predicted mentions. Different interpretations as to how to compute B3 and CEAF scores for coreference systems when predicted mentions do not perfectly align with key mentions—which is usually the case— led to variations of these metrics that manipulate the gold st"
P14-2006,N13-1071,1,0.782346,"Missing"
P14-2006,P09-1074,0,0.498213,"arnegie Mellon University, Pittsburgh, PA 5 HLTRI, University of Texas at Dallas, Richardson, TX, 6 HITS, Heidelberg, Germany 3 sameer.pradhan@childrens.harvard.edu, {xql,recasens}@google.com, hovy@cmu.edu, vince@hlt.utdallas.edu, michael.strube@h-its.org Abstract purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et"
P14-2006,N13-2001,0,0.0493029,"pora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 200"
P14-2006,Q14-1012,1,0.042996,"Missing"
P14-2006,P14-2005,1,0.929886,"ne-to-one mention mapping (Stoyanov et al., 2009; Cai and Strube, 2010). Some of these variations arguably produce rather unintuitive results, while others are not faithful to the original measures. In this paper, we address the issues in scoring coreference partitions of predicted mentions. Specifically, we justify our decision to go back to the original scoring algorithms by arguing that manipulation of key or predicted mentions is unnecessary and could in fact produce unintuitive results. We demonstrate the use of our recent extension of BLANC that can seamlessly handle predicted mentions (Luo et al., 2014). We make available an open-source, thoroughly-tested reference implementation of the main coreference evaluation measures that do not involve mention manipulation and is faithful to the original intentions of the proposers of these metrics. We republish the CoNLL-2011/2012 results based on this scorer, so that future systems can use it for evaluation and have the CoNLL results available for comparison. The rest of the paper is organized as follows. Section 2 provides an overview of the variations of the existing measures. We present our newly updated coreference scoring package in Section 3 t"
P14-2006,M95-1005,0,0.887578,"proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified"
P14-2006,H05-1004,1,0.706057,"re informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified with respect to predicted, as opposed to key (or"
P14-2006,W11-1901,1,0.482275,"al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setbac"
P14-2006,J00-4006,0,\N,Missing
P14-2006,D08-1067,1,\N,Missing
P14-2006,S10-1001,1,\N,Missing
P16-1045,N13-1092,0,0.0194216,"Missing"
P16-1045,P11-1115,0,0.00973506,"interfaces for databases. Answering questions in this context refers to executing queries over relational databases (Cafarella et al., 2008; Pimplikar and Sarawagi, 2012). Yin et al. (2015a) consider databases where information is stored in n-tuples, which are essentially tables. Also, investigation of the relational structure of tables is connected with research on database schema analysis and induction (Venetis et al., 2011; Syed et al., 2010). Finally, unstructured text and structured formats links to work on open information extraction (Etzioni et al., 2008) and knowledge base population (Ji and Grishman, 2011). we crowd-source a collection of over 9000 MCQs with alignment annotations to table elements, using tables as guidelines in efficient data harvesting. Second, we develop a feature-driven model that uses these MCQs to perform QA, while factchecking and reasoning over tables. Others have used tables in the context of QA. Question bank creation for tables has been investigated (Pasupat and Liang, 2015), but without structural guidelines or the alignment information that we propose. Similarly, tables have been used in QA reasoning (Yin et al., 2015b; Neelakantan et al., 2015; Sun et al., 2016) bu"
P16-1045,N03-1022,0,0.177183,"Missing"
P16-1045,C04-1100,0,0.0514468,"Missing"
P16-1045,P15-1142,0,0.177719,"uction (Venetis et al., 2011; Syed et al., 2010). Finally, unstructured text and structured formats links to work on open information extraction (Etzioni et al., 2008) and knowledge base population (Ji and Grishman, 2011). we crowd-source a collection of over 9000 MCQs with alignment annotations to table elements, using tables as guidelines in efficient data harvesting. Second, we develop a feature-driven model that uses these MCQs to perform QA, while factchecking and reasoning over tables. Others have used tables in the context of QA. Question bank creation for tables has been investigated (Pasupat and Liang, 2015), but without structural guidelines or the alignment information that we propose. Similarly, tables have been used in QA reasoning (Yin et al., 2015b; Neelakantan et al., 2015; Sun et al., 2016) but have not explicitly attempted to encode all the semantics of table structure (see Section 3.1). To the best of our knowledge, no previous work uses tables for both creation and reasoning in a connected framework. We evaluate our model on MCQ answering for three benchmark datasets. Our results consistently and significantly outperform a strong retrieval baseline as well as a Markov Logic network mod"
P16-1045,D07-1002,0,0.089667,"Missing"
P16-1045,N15-1184,1,\N,Missing
P16-1045,D15-1080,0,\N,Missing
P16-1101,D14-1082,0,0.0211772,"Missing"
P16-1101,C02-1025,0,0.132496,"Missing"
P16-1101,W15-3904,0,0.182284,"Missing"
P16-1101,P15-1033,0,0.248945,"by0 ,y are the weight vector and bias corresponding to label pair (y 0 , y), respectively. For CRF training, we use the maximum conditional likelihood estimation. For a training set {(zi , y i )}, the logarithm of the likelihood (a.k.a. the log-likelihood) is given by: X L(W, b) = log p(y|z; W, b) i BLSTM For many sequence labeling tasks it is beneficial to have access to both past (left) and future (right) contexts. However, the LSTM’s hidden state ht takes information only from past, knowing nothing about the future. An elegant solution whose effectiveness has been proven by previous work (Dyer et al., 2015) is bi-directional LSTM (BLSTM). The basic idea is to present each sequence forwards and backwards to two separate hidden states to capture past and future information, respectively. Then the two hidden states are concatenated to form the final output. 2.3 vector of the ith word. y = {y1 , · · · , yn } represents a generic sequence of labels for z. Y(z) denotes the set of possible label sequences for z. The probabilistic model for sequence CRF defines a family of conditional probability p(y|z; W, b) over all possible label sequences y given z with the following form: CRF For sequence labeling"
P16-1101,W03-0425,0,0.506484,"Missing"
P16-1101,P10-1001,0,0.00859141,"Missing"
P16-1101,D15-1025,0,0.00995793,"Missing"
P16-1101,gimenez-marquez-2004-svmtool,0,0.0139906,"Missing"
P16-1101,P16-1228,1,0.172753,"hat independently classifies labels for each word by using contexts within a window with fixed size. Recently, recurrent neural networks (RNN) (Goller and Kuchler, 1996), together with its variants such as long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) and gated recurrent unit (GRU) (Cho et al., 2014), have shown great success in modeling sequential data. Several RNN-based neural network models have been proposed to solve sequence labeling tasks like speech recognition (Graves et al., 2013), POS tagging (Huang et al., 2015) and NER (Chiu and Nichols, 2015; Hu et al., 2016), achieving competitive performance against traditional models. However, even systems that have utilized distributed representations as inputs have used these to augment, rather than replace, hand-crafted features (e.g. word spelling and capitalization patterns). Their performance drops rapidly when the models solely depend on neural embeddings. 1064 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1064–1074, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics In this paper, we propose a neural network architecture"
P16-1101,N16-1030,0,0.674178,"rder to compare with previous work, we adopt the standard splits — section 0–18 as training data, section 19– 21 as development data and section 22–24 as test data (Manning, 2011; Søgaard, 2011). NER. For NER, We perform experiments on the English data from CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). This data set contains four different types of named entities: PERSON, LOCATION, ORGANIZATION, and MISC. We use the BIOES tagging scheme instead of standard BIO2, as previous studies have reported meaningful improvement with this scheme (Ratinov and Roth, 2009; Dai et al., 2015; Lample et al., 2016). The corpora statistics are shown in Table 2. We did not perform any pre-processing for data sets, leaving our system truly end-to-end. 4.2 Main Results We first run experiments to dissect the effectiveness of each component (layer) of our neural network architecture by ablation studies. We compare the performance with three baseline systems — BRNN, the bi-direction RNN; BLSTM, the bidirection LSTM, and BLSTM-CNNs, the combination of BLSTM with CNN to model characterlevel information. All these models are run using Stanford’s GloVe 100 dimensional word embeddings and the same hyper-parameters"
P16-1101,P09-1116,0,0.00808892,"Missing"
P16-1101,D15-1176,0,0.0235888,"Missing"
P16-1101,D15-1104,0,0.738804,"ral language processing (NLP) systems, like syntactic parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Koo and Collins, 2010; Ma and Zhao, 2012a; Ma and Zhao, 2012b; Chen and Manning, 2014; Ma and Hovy, 2015) and entity coreference resolution (Ng, 2010; Ma et al., 2016), are becoming more sophisticated, in part because of utilizing output information of POS tagging or NER systems. Most traditional high performance sequence labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015), which rely heavily on hand-crafted features and taskspecific resources. For example, English POS taggers benefit from carefully designed word spelling features; orthographic features and external resources such as gazetteers are widely used in NER. However, such task-specific knowledge is costly to develop (Ma and Xia, 2014), making sequence labeling models difficult to adapt to new tasks or new domains. In the past few years, non-linear neural networks with as input distributed word representations, also known as word embeddings, have been broadly applied to NLP problems with great success."
P16-1101,D15-1154,1,0.355802,"Missing"
P16-1101,P14-1126,1,0.0825262,"of utilizing output information of POS tagging or NER systems. Most traditional high performance sequence labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015), which rely heavily on hand-crafted features and taskspecific resources. For example, English POS taggers benefit from carefully designed word spelling features; orthographic features and external resources such as gazetteers are widely used in NER. However, such task-specific knowledge is costly to develop (Ma and Xia, 2014), making sequence labeling models difficult to adapt to new tasks or new domains. In the past few years, non-linear neural networks with as input distributed word representations, also known as word embeddings, have been broadly applied to NLP problems with great success. Collobert et al. (2011) proposed a simple but effective feed-forward neutral network that independently classifies labels for each word by using contexts within a window with fixed size. Recently, recurrent neural networks (RNN) (Goller and Kuchler, 1996), together with its variants such as long-short term memory (LSTM) (Hoch"
P16-1101,C12-2077,1,0.111018,"Missing"
P16-1101,N16-1116,1,0.272107,"Missing"
P16-1101,P07-1096,0,0.00910892,"Missing"
P16-1101,J93-2004,0,0.0791484,"ce of our model. For CNN, we use 30 filters with window length 3. 4 4.1 WSJ 38,219 912,344 5,527 131,768 5,462 129,654 CoNLL2003 14,987 204,567 3,466 51,578 3,684 46,666 Table 2: Corpora statistics. SENT and TOKEN refer to the number of sentences and tokens in each data set. Table 1: Hyper-parameters for all experiments. 3.3 SENT TOKEN SENT TOKEN SENT TOKEN Experiments Data Sets As mentioned before, we evaluate our neural network model on two sequence labeling tasks: POS tagging and NER. POS Tagging. For English POS tagging, we use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB) (Marcus et al., 1993), which contains 45 different POS tags. In order to compare with previous work, we adopt the standard splits — section 0–18 as training data, section 19– 21 as development data and section 22–24 as test data (Manning, 2011; Søgaard, 2011). NER. For NER, We perform experiments on the English data from CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). This data set contains four different types of named entities: PERSON, LOCATION, ORGANIZATION, and MISC. We use the BIOES tagging scheme instead of standard BIO2, as previous studies have reported meaningful improvement with this scheme"
P16-1101,P11-2009,0,0.00779009,"es and tokens in each data set. Table 1: Hyper-parameters for all experiments. 3.3 SENT TOKEN SENT TOKEN SENT TOKEN Experiments Data Sets As mentioned before, we evaluate our neural network model on two sequence labeling tasks: POS tagging and NER. POS Tagging. For English POS tagging, we use the Wall Street Journal (WSJ) portion of Penn Treebank (PTB) (Marcus et al., 1993), which contains 45 different POS tags. In order to compare with previous work, we adopt the standard splits — section 0–18 as training data, section 19– 21 as development data and section 22–24 as test data (Manning, 2011; Søgaard, 2011). NER. For NER, We perform experiments on the English data from CoNLL 2003 shared task (Tjong Kim Sang and De Meulder, 2003). This data set contains four different types of named entities: PERSON, LOCATION, ORGANIZATION, and MISC. We use the BIOES tagging scheme instead of standard BIO2, as previous studies have reported meaningful improvement with this scheme (Ratinov and Roth, 2009; Dai et al., 2015; Lample et al., 2016). The corpora statistics are shown in Table 2. We did not perform any pre-processing for data sets, leaving our system truly end-to-end. 4.2 Main Results We first run experim"
P16-1101,P05-1012,0,0.0168834,"Missing"
P16-1101,P10-1142,0,0.00734987,"Missing"
P16-1101,C04-1010,0,0.014872,"Missing"
P16-1101,E99-1023,0,0.674617,"abels for z. Y(z) denotes the set of possible label sequences for z. The probabilistic model for sequence CRF defines a family of conditional probability p(y|z; W, b) over all possible label sequences y given z with the following form: CRF For sequence labeling (or general structured prediction) tasks, it is beneficial to consider the correlations between labels in neighborhoods and jointly decode the best chain of labels for a given input sentence. For example, in POS tagging an adjective is more likely to be followed by a noun than a verb, and in NER with standard BIO2 annotation (Tjong Kim Sang and Veenstra, 1999) I-ORG cannot follow I-PER. Therefore, we model label sequence jointly using a conditional random field (CRF) (Lafferty et al., 2001), instead of decoding each label independently. Formally, we use z = {z1 , · · · , zn } to represent a generic input sequence where zi is the input Maximum likelihood training chooses parameters such that the log-likelihood L(W, b) is maximized. Decoding is to search for the label sequence y ∗ with the highest conditional probability: y ∗ = argmax p(y|z; W, b) y∈Y(z) For a sequence CRF model (only interactions between two successive labels are considered), traini"
P16-1101,W14-1609,0,0.00859019,"ssing community. Natural language processing (NLP) systems, like syntactic parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Koo and Collins, 2010; Ma and Zhao, 2012a; Ma and Zhao, 2012b; Chen and Manning, 2014; Ma and Hovy, 2015) and entity coreference resolution (Ng, 2010; Ma et al., 2016), are becoming more sophisticated, in part because of utilizing output information of POS tagging or NER systems. Most traditional high performance sequence labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015), which rely heavily on hand-crafted features and taskspecific resources. For example, English POS taggers benefit from carefully designed word spelling features; orthographic features and external resources such as gazetteers are widely used in NER. However, such task-specific knowledge is costly to develop (Ma and Xia, 2014), making sequence labeling models difficult to adapt to new tasks or new domains. In the past few years, non-linear neural networks with as input distributed word representations, also known as word embeddings, have been broadly applied to NLP problems"
P16-1101,D15-1064,0,0.40077,"3 1067 Layer CNN LSTM Dropout Hyper-parameter window size number of filters state size initial state peepholes dropout rate batch size initial learning rate decay rate gradient clipping POS 3 30 200 0.0 no 0.5 10 0.01 0.05 5.0 NER 3 30 200 0.0 no 0.5 10 0.015 0.05 5.0 Dataset Train Dev Test Fine Tuning. For each of the embeddings, we fine-tune initial embeddings, modifying them during gradient updates of the neural network model by back-propagating gradients. The effectiveness of this method has been previously explored in sequential and structured prediction problems (Collobert et al., 2011; Peng and Dredze, 2015). Dropout Training. To mitigate overfitting, we apply the dropout method (Srivastava et al., 2014) to regularize our model. As shown in Figure 1 and 3, we apply dropout on character embeddings before inputting to CNN, and on both the input and output vectors of BLSTM. We fix dropout rate at 0.5 for all dropout layers through all the experiments. We obtain significant improvements on model performance after using dropout (see Section 4.5). Tuning Hyper-Parameters Table 1 summarizes the chosen hyper-parameters for all experiments. We tune the hyper-parameters on the development sets by random se"
P16-1101,N03-1033,0,0.060525,"Missing"
P16-1101,P16-2025,0,0.0315629,"ing vanila RNN instead of LSTM. Another neural architecture employing 1071 CNN to model character-level information is the “CharWNN” architecture (Santos and Zadrozny, 2014) which is inspired by the feed-forward network (Collobert et al., 2011). CharWNN obtained near state-of-the-art accuracy on English POS tagging (see Section 4.3 for details). A similar model has also been applied to Spanish and Portuguese NER (dos Santos et al., 2015) Ling et al. (2015) and Yang et al. (2016) also used BSLTM to compose character embeddings to word’s representation, which is similar to Lample et al. (2016). Peng and Dredze (2016) Improved NER for Chinese Social Media with Word Segmentation. 6 Conclusion In this paper, we proposed a neural network architecture for sequence labeling. It is a truly end-toend model relying on no task-specific resources, feature engineering or data pre-processing. We achieved state-of-the-art performance on two linguistic sequence labeling tasks, comparing with previously state-of-the-art systems. There are several potential directions for future work. First, our model can be further improved by exploring multi-task learning approaches to combine more useful and correlated information. For"
P16-1101,D14-1162,0,0.129474,"Missing"
P16-1101,W09-1119,0,0.908891,"e natural language processing community. Natural language processing (NLP) systems, like syntactic parsing (Nivre and Scholz, 2004; McDonald et al., 2005; Koo and Collins, 2010; Ma and Zhao, 2012a; Ma and Zhao, 2012b; Chen and Manning, 2014; Ma and Hovy, 2015) and entity coreference resolution (Ng, 2010; Ma et al., 2016), are becoming more sophisticated, in part because of utilizing output information of POS tagging or NER systems. Most traditional high performance sequence labeling models are linear statistical models, including Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015), which rely heavily on hand-crafted features and taskspecific resources. For example, English POS taggers benefit from carefully designed word spelling features; orthographic features and external resources such as gazetteers are widely used in NER. However, such task-specific knowledge is costly to develop (Ma and Xia, 2014), making sequence labeling models difficult to adapt to new tasks or new domains. In the past few years, non-linear neural networks with as input distributed word representations, also known as word embeddings, have been broadly app"
P16-1101,W03-0419,0,\N,Missing
P16-1101,Q16-1026,0,\N,Missing
P16-1126,J96-2004,0,0.393999,"Missing"
P16-1126,W12-0515,0,0.100752,"eting legal documents, which may result in interpretation discrepancies compared to experts (Reidenberg et al., 2015a). Our policy annotation tool shares some common features with GATE (Bontcheva et al., 2013), although the interface for our tool is simpler to fit the specific requirements of the task. Few prior efforts, aside from those we cite above, have applied natural language processing to privacy policies or other legal documents purported for the general public to regularly read. More generally, legal text has a history of attention from natural language processing (Bach et al., 2013; Galgani et al., 2012; Francesconi et al., 2010) and from artificial intelligence (Sartor and Rotolo, 2013; Bench-Capon et al., 2012). Classifying legal text into categories has received some ˇ interest (Savelka and Ashley, 2015; Mickevicius et al., 2015), as well as making the contents of legal texts more accessible (Boella et al., 2015; Curtotti and McCreath, 2013). Compared to prior efforts, our data set is notable for its combination of size, input from experts (for the label scheme) and skilled annotators (for the annotation procedure), and fine-grained detail. 3 Corpus Creation and Structure In this section"
P16-1126,C14-1084,1,0.531147,"rity of privacy policies are unstructured and do not follow standardized formats. Costante et al. (2012) proposed a supervised learning approach to determine which data practice categories are covered in a privacy policy. Rule-based extraction techniques have been proposed to extract some of a website’s data collection practices from its privacy policy (Costante et al., 2013) or to answer certain binary questions about a privacy policy (Zimmeck and Bellovin, 2014). Other approaches leverage topic modeling (Chundi and Subramaniam, 2014; Stamey and Rossi, 2009) or sequence alignment techniques (Liu et al., 2014; Ramanath et al., 2014) to analyze privacy policies or identify similar policy sections and paragraphs. However, the complexity and vagueness of privacy policies makes it difficult to automatically extract complex data practices from privacy policies without substantial gold standard data. Crowdsourcing has been proposed as a potential approach to obtain annotations for privacy policies (Sadeh et al., 2013; Breaux and Schaub, 2014; Wilson et al., 2016). However, crowdworkers are not trained in understanding and interpreting legal documents, which may result in interpretation discrepancies com"
P16-1126,W15-5316,0,0.0227025,"face for our tool is simpler to fit the specific requirements of the task. Few prior efforts, aside from those we cite above, have applied natural language processing to privacy policies or other legal documents purported for the general public to regularly read. More generally, legal text has a history of attention from natural language processing (Bach et al., 2013; Galgani et al., 2012; Francesconi et al., 2010) and from artificial intelligence (Sartor and Rotolo, 2013; Bench-Capon et al., 2012). Classifying legal text into categories has received some ˇ interest (Savelka and Ashley, 2015; Mickevicius et al., 2015), as well as making the contents of legal texts more accessible (Boella et al., 2015; Curtotti and McCreath, 2013). Compared to prior efforts, our data set is notable for its combination of size, input from experts (for the label scheme) and skilled annotators (for the annotation procedure), and fine-grained detail. 3 Corpus Creation and Structure In this section we describe our procedure for selecting a diverse set of privacy policies, our annotation scheme, how we obtained annotations, and the structure of the corpus. 3.1 Privacy Policy Selection Privacy policies vary in length, complexity,"
P16-1126,P14-2099,1,0.421147,"licies are unstructured and do not follow standardized formats. Costante et al. (2012) proposed a supervised learning approach to determine which data practice categories are covered in a privacy policy. Rule-based extraction techniques have been proposed to extract some of a website’s data collection practices from its privacy policy (Costante et al., 2013) or to answer certain binary questions about a privacy policy (Zimmeck and Bellovin, 2014). Other approaches leverage topic modeling (Chundi and Subramaniam, 2014; Stamey and Rossi, 2009) or sequence alignment techniques (Liu et al., 2014; Ramanath et al., 2014) to analyze privacy policies or identify similar policy sections and paragraphs. However, the complexity and vagueness of privacy policies makes it difficult to automatically extract complex data practices from privacy policies without substantial gold standard data. Crowdsourcing has been proposed as a potential approach to obtain annotations for privacy policies (Sadeh et al., 2013; Breaux and Schaub, 2014; Wilson et al., 2016). However, crowdworkers are not trained in understanding and interpreting legal documents, which may result in interpretation discrepancies compared to experts (Reiden"
P16-1228,N16-1030,0,0.0602172,"-trained word vectors to capture character- and word-level information, respectively. These features are then fed into a bi-directional RNN with LSTM units for sequence tagging. Compared to (Chiu and Nichols, 2015) we omit the character type and capitalization features, as well as the additive transition matrix in the output layer. Figure 3 shows the network architecture. Logic Rules The base network largely makes independent tagging decisions at each position, ignoring the constraints on successive labels for a valid tag sequence (e.g., I-ORG cannot follow B-PER). In contrast to recent work (Lample et al., 2016) which adds a conditional random field (CRF) to capture bi-gram dependencies between outputs, we instead apply logic rules which does not introduce extra parameters to learn. An example rule is: equal(yi−1 , I-ORG) ⇒ ¬ equal(yi , B-PER) 1 NYC Char+Word Representation (6) Replacing ∧ with & in Eq.(5) leads to a probably more intuitive rule which takes the value σθ (B)+ when y = +, and 1 − σθ (B)+ otherwise. Figure 3: The architecture of the bidirectional LSTM recurrent network for NER. The CNN for extracting character representation is omitted. The confidence levels are set to ∞ to prevent any"
P16-1228,D15-1104,0,0.00591606,"based methods (Rows 4-7), including the BLSTM-CRF model (Lample et al., 2016) which applies a conditional random field (CRF) on top of a BLSTM in order to capture the transition patterns and encourage valid sequences. In contrast, our method implements the desired constraints in a more straightforward way by using the declarative logic rule language, and at the same time does not introduce extra model parameters to learn. Further integration of the list rule (Row 3) provides a second boost in performance, achieving an F1 score very close to the best-performing systems including Joint-NER-EL (Luo et al., 2015) (Row 8), a probabilistic graphical model optimizing NER and entity linking jointly with massive external resources, and BLSTM-CRF (Ma and Hovy, 2016), a combination of BLSTM and CRF with more parameters than our rule-enhanced neural networks. From the table we see that the accuracy gap between the joint teacher model q and the distilled student p is relatively larger than in the sentiment classification task (Table 1). This is because in the Discussion and Future Work We have developed a framework which combines deep neural networks with first-order logic rules to allow integrating human know"
P16-1228,P16-1101,1,0.0185807,"der to capture the transition patterns and encourage valid sequences. In contrast, our method implements the desired constraints in a more straightforward way by using the declarative logic rule language, and at the same time does not introduce extra model parameters to learn. Further integration of the list rule (Row 3) provides a second boost in performance, achieving an F1 score very close to the best-performing systems including Joint-NER-EL (Luo et al., 2015) (Row 8), a probabilistic graphical model optimizing NER and entity linking jointly with massive external resources, and BLSTM-CRF (Ma and Hovy, 2016), a combination of BLSTM and CRF with more parameters than our rule-enhanced neural networks. From the table we see that the accuracy gap between the joint teacher model q and the distilled student p is relatively larger than in the sentiment classification task (Table 1). This is because in the Discussion and Future Work We have developed a framework which combines deep neural networks with first-order logic rules to allow integrating human knowledge and intentions into the neural models. In particular, we proposed an iterative distillation procedure that transfers the structured information"
P16-1228,P05-1015,0,0.154071,"Missing"
P16-1228,D14-1162,0,0.117795,"Missing"
P16-1228,D14-1181,0,0.00636895,"NN architecture for sentence-level sentiment analysis. The sentence representation vector is followed by a fully-connected layer with softmax output activation, to output sentiment predictions. 4.1 Sentiment Classification Sentence-level sentiment analysis is to identify the sentiment (e.g., positive or negative) underlying an individual sentence. The task is crucial for many opinion mining applications. One challenging point of the task is to capture the contrastive sense (e.g., by conjunction “but”) within a sentence. Base Network We use the single-channel convolutional network proposed in (Kim, 2014). The simple model has achieved compelling performance on various sentiment classification benchmarks. The network contains a convolutional layer on top of word vectors of a given sentence, followed by a max-over-time pooling layer and then a fullyconnected layer with softmax output activation. A convolution operation is to apply a filter to word windows. Multiple filters with varying window sizes are used to obtain multiple features. Figure 2 shows the network architecture. Logic Rules One difficulty for the plain neural network is to identify contrastive sense in order to capture the dominan"
P16-1228,D13-1170,0,0.00539502,"Missing"
P16-1228,W03-0419,0,0.0329258,"Missing"
P16-1228,P14-1031,0,0.019169,"Missing"
P16-1228,K15-1021,0,0.032223,"Missing"
P16-1228,N16-1178,0,0.0164839,"Missing"
P16-1228,Q16-1026,0,\N,Missing
P17-1088,P15-1144,0,0.0309711,"se Dense + `1 Sparse MR 199 228 207 WN18 H10 Time 94.0 4m34s 94.2 4m25s 94.1 2m32s MR 69 131 67 FB15k H10 Time 79.4 4m30s 78.9 5m47s 79.6 1m52s Table 3: Performance of model with dense attention vectors or sparse attention vectors. MR, H10 and Time denotes mean rank, Hits@10 and training time per epoch respectively matrices into a 3-dimensional tensor X ∈ R2|R|×n×n , similar sparsity can be induced by solving an `1 -regularized tensor completion problem minA,D ||X − DA||22 + λkAk`1 . Basically, A plays the same role as the attention vectors in our model. For more details, we refer readers to (Faruqui et al., 2015). For completeness, we compare our model with the aforementioned approach4 . The comparison is summarized in table 4. On both benchmarks, ITransF achieves significant improvement against sparse encoding on pretrained model. This performance gap should be expected since the objective function of sparse encoding methods is to minimize the reconstruction loss rather than optimize the criterion for link prediction. Method Sparse Encoding ITransF WN18 MR H10 211 86.6 205 94.2 7 FB15k MR H10 66 79.1 65 81.0 Related Work In KBC, CTransR (Lin et al., 2015b) enables relation embedding sharing across si"
P17-1088,D13-1160,0,0.00795137,"er, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets— WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information. 1 Introduction Knowledge bases (KB), such as WordNet (Fellbaum, 1998), Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and DBpedia (Lehmann et al., 2015), are useful resources for many applications such as question answering (Berant et al., 2013; Yih et al., 2015; Dai et al., 2016) and information extraction (Mintz et al., 2009). However, knowledge bases suffer from incompleteness despite their formidable sizes (Socher et al., 2013; West et al., 2014), leading to a number of studies on automatic knowledge base completion (KBC) (Nickel et al., 2015) or link prediction. The fundamental motivation behind these studies is that there exist some statistical regularities under the intertwined facts stored in the multirelational knowledge base. By discovering gener950 Proceedings of the 55th Annual Meeting of the Association for Computationa"
P17-1088,D15-1034,0,0.117606,"Missing"
P17-1088,D16-1153,0,0.0114062,"our model can improve the performance on two benchmark datasets without external resources, over all previous models of the same kind. In the future, we plan to enable ITransF to perform multi-step inference, and extend the sharing mechanism to entity and relation embeddings, further enhancing the statistical binding across parameters. In addition, our framework can also be applied to multi-task learning, promoting a finer sharing among different tasks. Table 4: Different methods to obtain sparse representations 6 edge and statistical strengths across similar models or languages. For example, Bharadwaj et al. (2016) transfers models on resource-rich languages to low resource languages by parameter sharing through common phonological features in name entity recognition. Zoph et al. (2016) initialize from models trained by resource-rich languages to translate low-resource languages. Several works on obtaining a sparse attention (Martins and Astudillo, 2016; Makhzani and Frey, 2014; Shazeer et al., 2017) share a similar idea of sorting the values before softmax and only keeping the K largest values. However, the sorting operation in these works is not GPU-friendly. The block iterative optimization algorithm"
P17-1088,D15-1038,0,0.194269,"Missing"
P17-1088,P15-1067,0,0.672456,"ickel et al., 2011; Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016b). As a seminal work, Bordes et al. (2013) proposes the TransE, which models the statistical regularities with linear translations between entity embeddings operated by a relation embedding. Implicitly, TransE assumes both entity embeddings and relation embeddings dwell in the same vector space, posing an unnecessarily strong prior. To relax this requirement, a variety of models first project the entity embeddings to a relationdependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space. Typically, these relation-dependent spaces are characterized by the projection matrices unique to each relation. As a benefit, different aspects of the same entity can be temporarily emphasized or depressed as an effect of the projection. For instance, STransE (Nguyen et al., 2016b) utilizes two projection matrices per relation, one for the head entity and the other for the tail entity. Despite the superior performance of STransE compared to TransE, it is more prone to the data sparsity"
P17-1088,P16-1076,1,0.0201497,"lations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets— WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information. 1 Introduction Knowledge bases (KB), such as WordNet (Fellbaum, 1998), Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and DBpedia (Lehmann et al., 2015), are useful resources for many applications such as question answering (Berant et al., 2013; Yih et al., 2015; Dai et al., 2016) and information extraction (Mintz et al., 2009). However, knowledge bases suffer from incompleteness despite their formidable sizes (Socher et al., 2013; West et al., 2014), leading to a number of studies on automatic knowledge base completion (KBC) (Nickel et al., 2015) or link prediction. The fundamental motivation behind these studies is that there exist some statistical regularities under the intertwined facts stored in the multirelational knowledge base. By discovering gener950 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 950–962 c Vancou"
P17-1088,D15-1082,0,0.284007,"1; Bordes et al., 2011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016b). As a seminal work, Bordes et al. (2013) proposes the TransE, which models the statistical regularities with linear translations between entity embeddings operated by a relation embedding. Implicitly, TransE assumes both entity embeddings and relation embeddings dwell in the same vector space, posing an unnecessarily strong prior. To relax this requirement, a variety of models first project the entity embeddings to a relationdependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space. Typically, these relation-dependent spaces are characterized by the projection matrices unique to each relation. As a benefit, different aspects of the same entity can be temporarily emphasized or depressed as an effect of the projection. For instance, STransE (Nguyen et al., 2016b) utilizes two projection matrices per relation, one for the head entity and the other for the tail entity. Despite the superior performance of STransE compared to TransE, it is more prone to the data sparsity problem. Concretel"
P17-1088,P09-1113,0,0.0712785,"sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets— WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information. 1 Introduction Knowledge bases (KB), such as WordNet (Fellbaum, 1998), Freebase (Bollacker et al., 2008), YAGO (Suchanek et al., 2007) and DBpedia (Lehmann et al., 2015), are useful resources for many applications such as question answering (Berant et al., 2013; Yih et al., 2015; Dai et al., 2016) and information extraction (Mintz et al., 2009). However, knowledge bases suffer from incompleteness despite their formidable sizes (Socher et al., 2013; West et al., 2014), leading to a number of studies on automatic knowledge base completion (KBC) (Nickel et al., 2015) or link prediction. The fundamental motivation behind these studies is that there exist some statistical regularities under the intertwined facts stored in the multirelational knowledge base. By discovering gener950 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 950–962 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Asso"
P17-1088,W15-4007,0,0.47052,"y fr (h, t) of a plausible triple (h, r, t) and to maximize energy fr (h0 , t0 ) of an implausible triple (h0 , r, t0 ). Motivated by the linear translation phenomenon observed in well trained word embeddings (Mikolov et al., 2013), TransE (Bordes et al., 2013) represents the head entity h, the relation r and the tail entity t with vectors h, r and t ∈ Rn respectively, which were trained so that h + r ≈ t. They define the energy function as Previously, a line of research makes use of external information such as textual relations from web-scale corpus or node features (Toutanova et al., 2015; Toutanova and Chen, 2015; Nguyen et al., 2016a), alleviating the sparsity problem. In parallel, recent work has proposed to model regularities beyond local facts by considering multirelation paths (Garc´ıa-Dur´an et al., 2015; Lin et al., 2015a; Shen et al., 2016). Since the number of paths grows exponentially with its length, as a side effect, path-based models enjoy much more training cases, suffering less from the problem. In this paper, we propose an interpretable knowledge transfer model (ITransF), which encourages the sharing of statistic regularities between the projection matrices of relations and alleviates"
P17-1088,D15-1174,0,0.0350733,"earned to minimize energy fr (h, t) of a plausible triple (h, r, t) and to maximize energy fr (h0 , t0 ) of an implausible triple (h0 , r, t0 ). Motivated by the linear translation phenomenon observed in well trained word embeddings (Mikolov et al., 2013), TransE (Bordes et al., 2013) represents the head entity h, the relation r and the tail entity t with vectors h, r and t ∈ Rn respectively, which were trained so that h + r ≈ t. They define the energy function as Previously, a line of research makes use of external information such as textual relations from web-scale corpus or node features (Toutanova et al., 2015; Toutanova and Chen, 2015; Nguyen et al., 2016a), alleviating the sparsity problem. In parallel, recent work has proposed to model regularities beyond local facts by considering multirelation paths (Garc´ıa-Dur´an et al., 2015; Lin et al., 2015a; Shen et al., 2016). Since the number of paths grows exponentially with its length, as a side effect, path-based models enjoy much more training cases, suffering less from the problem. In this paper, we propose an interpretable knowledge transfer model (ITransF), which encourages the sharing of statistic regularities between the projection matrices of"
P17-1088,K16-1005,0,0.451923,"011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016b). As a seminal work, Bordes et al. (2013) proposes the TransE, which models the statistical regularities with linear translations between entity embeddings operated by a relation embedding. Implicitly, TransE assumes both entity embeddings and relation embeddings dwell in the same vector space, posing an unnecessarily strong prior. To relax this requirement, a variety of models first project the entity embeddings to a relationdependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space. Typically, these relation-dependent spaces are characterized by the projection matrices unique to each relation. As a benefit, different aspects of the same entity can be temporarily emphasized or depressed as an effect of the projection. For instance, STransE (Nguyen et al., 2016b) utilizes two projection matrices per relation, one for the head entity and the other for the tail entity. Despite the superior performance of STransE compared to TransE, it is more prone to the data sparsity problem. Concretely, since the projectio"
P17-1088,N16-1054,0,0.380976,"011, 2014, 2013; Socher et al., 2013; Wang et al., 2014; Guu et al., 2015; Nguyen et al., 2016b). As a seminal work, Bordes et al. (2013) proposes the TransE, which models the statistical regularities with linear translations between entity embeddings operated by a relation embedding. Implicitly, TransE assumes both entity embeddings and relation embeddings dwell in the same vector space, posing an unnecessarily strong prior. To relax this requirement, a variety of models first project the entity embeddings to a relationdependent space (Bordes et al., 2014; Ji et al., 2015; Lin et al., 2015b; Nguyen et al., 2016b), and then model the translation property in the projected space. Typically, these relation-dependent spaces are characterized by the projection matrices unique to each relation. As a benefit, different aspects of the same entity can be temporarily emphasized or depressed as an effect of the projection. For instance, STransE (Nguyen et al., 2016b) utilizes two projection matrices per relation, one for the head entity and the other for the tail entity. Despite the superior performance of STransE compared to TransE, it is more prone to the data sparsity problem. Concretely, since the projectio"
P17-1088,D16-1145,0,0.0345503,"Missing"
P17-1088,D16-1163,0,0.0121695,"erform multi-step inference, and extend the sharing mechanism to entity and relation embeddings, further enhancing the statistical binding across parameters. In addition, our framework can also be applied to multi-task learning, promoting a finer sharing among different tasks. Table 4: Different methods to obtain sparse representations 6 edge and statistical strengths across similar models or languages. For example, Bharadwaj et al. (2016) transfers models on resource-rich languages to low resource languages by parameter sharing through common phonological features in name entity recognition. Zoph et al. (2016) initialize from models trained by resource-rich languages to translate low-resource languages. Several works on obtaining a sparse attention (Martins and Astudillo, 2016; Makhzani and Frey, 2014; Shazeer et al., 2017) share a similar idea of sorting the values before softmax and only keeping the K largest values. However, the sorting operation in these works is not GPU-friendly. The block iterative optimization algorithm in our work is inspired by LightRNN (Li et al., 2016). They allocate every word in the vocabulary in a table. A word is represented by a row vector and a column vector depend"
P17-1088,P15-1128,0,\N,Missing
P17-1191,P08-1037,0,0.503666,"heir senses. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. Resnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al. (2013) applied models of selectional preferences automatically learned from WordNet and distributional information, to the problem of semantic role labeling. Resnik (1993); Brill and Resnik (1994); Agirre (2008) and others have used WordNet information towards improving prepositional phrase attachment predictions. 6 Conclusion In this paper, we proposed a grounding of lexical items which acknowledges the semantic ambiguity of word types using WordNet and a method to learn a context-sensitive distribution over their representations. We also showed how to integrate the proposed representation with recurrent neural networks for disambiguating prepositional phrase attachments, showing that the proposed WordNetgrounded context-sensitive token embeddings outperforms standard type-level embeddings for predi"
P17-1191,Q14-1043,0,0.601607,"mportance of each item in a sequence, it can also be applied to non-sequential items. 4 See Table 2 in §4 for detailed results. 2091 training set. At test time, we predict the candidate head with the highest probability according to the model in Eq. 3, i.e., kˆ = arg max p(hk is head = 1). k Figure 3: Two sentences illustrating the importance of lexicalization in PP attachment decisions. In the top sentence, the PP ‘with butter’ attaches to the noun ‘spaghetti’. In the bottom sentence, the PP ‘with chopsticks’ attaches to the verb ‘ate’. Note: This figure and caption have been reproduced from Belinkov et al. (2014). its direct dependent d in the prepositional phrase (PP), our goal is to predict the correct head word for the PP among an ordered list of candidate head words h. Each example in the train, validation, and test sets consists of an input tuple hh, p, di and an output index k to identify the correct head among the candidates in h. Note that the order of words that form each hh, p, di is the same as that in the corresponding original sentence. 3.2 Model Definition Both our proposed and baseline models for PP attachment use bidirectional RNN with LSTM cells (bi-LSTM) to encode the sequence t = hh"
P17-1191,C94-2195,0,0.771055,"convex combinations of their senses. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. Resnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al. (2013) applied models of selectional preferences automatically learned from WordNet and distributional information, to the problem of semantic role labeling. Resnik (1993); Brill and Resnik (1994); Agirre (2008) and others have used WordNet information towards improving prepositional phrase attachment predictions. 6 Conclusion In this paper, we proposed a grounding of lexical items which acknowledges the semantic ambiguity of word types using WordNet and a method to learn a context-sensitive distribution over their representations. We also showed how to integrate the proposed representation with recurrent neural networks for disambiguating prepositional phrase attachments, showing that the proposed WordNetgrounded context-sensitive token embeddings outperforms standard type-level embed"
P17-1191,D14-1082,0,0.00954565,"s define a single vector for each word type. However, a fundamental flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word ‘pool’ has different meanings, but the same representation is typically used for both of them. Similarly, the fact that ‘pool’ and ‘lake’ are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014; Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space. Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Sch¨utze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings. In this work, we represent a word token in a given context by estimating a context-sensi"
P17-1191,D14-1110,0,0.043953,"Missing"
P17-1191,N15-1184,1,0.896681,"xplicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014; Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space. Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Sch¨utze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings. In this work, we represent a word token in a given context by estimating a context-sensitive probability distribution over relevant concepts in WordNet (Miller, 1995) and use the expected value (i.e., weighted sum) of the concept embeddings as the token representation (see §2). We take a task-centric approach towards doing this, and learn the token representations jointly with the task-specific parameters. In addition to providing context-sensitive token embeddings, the proposed method"
P17-1191,P12-1092,0,0.0541338,"mation. Related Work This work is related to various lines of research within the NLP community: dealing with synonymy and homonymy in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distributional and knowledge based semantics; and selectional preferences and their relation with syntactic and semantic relations. The need for going beyond a single vector per word-type has been well established for a while, and many efforts were focused on building multi-prototype vector space models of meaning (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Jauhar et al., 2015; Neelakantan et al., 2015; Arora et al., 2016, etc.). However, the target of all these approaches is obtaining multisense word vector spaces, either by incorporating sense tagged information or other kinds of external context. The number of vectors learned is still fixed, based on the preset number of senses. In contrast, our focus is on learning a context dependent distribution over those concept representations. Other work not necessarily related to multisense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which prop"
P17-1191,N15-1070,1,0.904646,"ed to various lines of research within the NLP community: dealing with synonymy and homonymy in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distributional and knowledge based semantics; and selectional preferences and their relation with syntactic and semantic relations. The need for going beyond a single vector per word-type has been well established for a while, and many efforts were focused on building multi-prototype vector space models of meaning (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Jauhar et al., 2015; Neelakantan et al., 2015; Arora et al., 2016, etc.). However, the target of all these approaches is obtaining multisense word vector spaces, either by incorporating sense tagged information or other kinds of external context. The number of vectors learned is still fixed, based on the preset number of senses. In contrast, our focus is on learning a context dependent distribution over those concept representations. Other work not necessarily related to multisense vectors, but still related to our work includes Belanger and Kakade (2015)’s work which proposed a Gaussian linear dynamical system"
P17-1191,N15-1164,0,0.0199999,"o improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology. Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. Similarly, Johansson and Pina (2015) improved word embeddings by representing each sense of the word in a way that reflects the topology of the semantic network they belong to, and then representing the words as convex combinations of their senses. In contrast to previous work that was aimed at improving type level word representations, we propose an approach for obtaining context-sensitive embeddings at the token level, while jointly optimizing the model parameters for the NLP task of interest. Resnik (1993) showed the applicability of semantic classes and selectional preferences to resolving syntactic ambiguity. Zapirain et al"
P17-1191,N16-1030,1,0.409477,"for each word type. However, a fundamental flaw in this design is their inability to distinguish between different meanings and abstractions of the same word. In the two sentences shown above, the word ‘pool’ has different meanings, but the same representation is typically used for both of them. Similarly, the fact that ‘pool’ and ‘lake’ are both kinds of water bodies is not explicitly incorporated in most type-level embeddings. Furthermore, it has become a standard practice to tune pre-trained word embeddings as model parameters during training for an NLP task (e.g., Chen and Manning, 2014; Lample et al., 2016), potentially allowing the parameters of a frequent word in the labeled training data to drift away from related but rare words in the embedding space. Previous work partially addresses these problems by estimating concept embeddings in WordNet (e.g., Rothe and Sch¨utze, 2015), or improving word representations using information from knowledge graphs (e.g., Faruqui et al., 2015). However, it is still not clear how to use a lexical ontology to derive context-sensitive token embeddings. In this work, we represent a word token in a given context by estimating a context-sensitive probability distr"
P17-1191,P14-1130,0,0.026954,"Missing"
P17-1191,D14-1113,0,0.0593386,"Missing"
P17-1191,D14-1162,0,0.0848322,"Missing"
P17-1191,H94-1048,0,0.335356,"Missing"
P17-1191,N10-1013,0,0.0386834,"on with distributional information. Related Work This work is related to various lines of research within the NLP community: dealing with synonymy and homonymy in word representations both in the context of distributed embeddings and more traditional vector spaces; hybrid models of distributional and knowledge based semantics; and selectional preferences and their relation with syntactic and semantic relations. The need for going beyond a single vector per word-type has been well established for a while, and many efforts were focused on building multi-prototype vector space models of meaning (Reisinger and Mooney, 2010; Huang et al., 2012; Chen et al., 2014; Jauhar et al., 2015; Neelakantan et al., 2015; Arora et al., 2016, etc.). However, the target of all these approaches is obtaining multisense word vector spaces, either by incorporating sense tagged information or other kinds of external context. The number of vectors learned is still fixed, based on the preset number of senses. In contrast, our focus is on learning a context dependent distribution over those concept representations. Other work not necessarily related to multisense vectors, but still related to our work includes Belanger and Kakade (201"
P17-1191,H93-1054,0,0.640279,"wnstream task benefits from all the updates to related words which share one or more concept embeddings. 2089 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2089–2098 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1191 stractions. Among the labeled relations defined in WordNet between different synsets, we focus on the hypernymy relation to help model generalization and selectional preferences between words, which is especially important for predicting PP attachments (Resnik, 1993). To ground a word type, we identify the set of (direct and indirect) hypernyms of the WordNet senses of that word. A simplified grounding of the word ‘pool’ is illustrated in Figure 1. This grounding is key to our model of token embeddings, to be described in the following subsections. 2.2 Figure 1: An example grounding for the word ‘pool’. Solid arrows represent possible senses and dashed arrows represent hypernym relations. Note that the same set of concepts are used to ground the word ‘pool’ regardless of its context. Other WordNet senses for ‘pool’ were removed from the figure for simplic"
P17-1191,P15-1173,0,0.0797428,"Missing"
P17-1191,P14-2089,0,0.0232829,"ses on estimating token-level word embeddings as context sensitive distributions of concept em2095 Figure 5: Two examples from the test set where OntoLSTM-PP predicts the head correctly and LSTM-PP does not, along with weights by OntoLSTM-PP to synsets that contribute to token representations of infrequent word types. The prepositions are shown in bold, LSTM-PP’s predictions in red and OntoLSTMPP’s predictions in green. Words that are not candidate heads or dependents are shown in brackets. beddings. There is a large body of work that tried to improve word embeddings using external resources. Yu and Dredze (2014) extended the CBOW model (Mikolov et al., 2013) by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. Jauhar et al. (2015) extended the skipgram model (Mikolov et al., 2013) by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology. Faruqui et al. (2015) used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. Similarly, Johansson and Pina (2015) improved word embeddings by representing each"
P18-1130,P16-1231,0,0.345275,"rations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computati"
P18-1130,P15-1034,0,0.0202817,"ance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-t"
P18-1130,D15-1041,0,0.0343909,"eft-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting"
P18-1130,D16-1211,0,0.184897,"Missing"
P18-1130,D17-1209,0,0.0187661,"nt dependency annotation schemas, and achieve state-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-base"
P18-1130,D12-1133,0,0.0721718,"Missing"
P18-1130,W06-2920,0,0.707126,"in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the"
P18-1130,D14-1082,0,0.255712,"2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performan"
P18-1130,D16-1238,0,0.624302,"ith previous top-performing systems for comparison. Note that the results of S TACK P TR and our reimplementation of B I AF are the average of 5 repetitions instead of a single run. Our Full model significantly outperforms all the transition-based parsers on all three languages, and achieves better results than most graph-based parsers. Our 1408 System Chen and Manning (2014) Ballesteros et al. (2015) Dyer et al. (2015) Bohnet and Nivre (2012) Ballesteros et al. (2016) Kiperwasser and Goldberg (2016) Weiss et al. (2015) Andor et al. (2016) Kiperwasser and Goldberg (2016) Wang and Chang (2016) Cheng et al. (2016) Kuncoro et al. (2016) Ma and Hovy (2017) B I AF: Dozat and Manning (2017) B I AF: re-impl S TACK P TR: Org S TACK P TR: +gpar S TACK P TR: +sib S TACK P TR: Full T T T T T T T T G G G G G G G T T T T English UAS LAS 91.8 89.6 91.63 89.44 93.1 90.9 93.33 91.22 93.56 91.42 93.9 91.9 94.26 92.41 94.61 92.79 93.1 91.0 94.08 91.82 94.10 91.49 94.26 92.06 94.88 92.98 95.74 94.08 95.84 94.21 95.77 94.12 95.78 94.12 95.85 94.18 95.87 94.19 Chinese UAS LAS 83.9 82.4 85.30 83.72 87.2 85.7 87.3 85.9 87.65 86.21 87.6 86.1 – – – – 86.6 85.1 87.55 86.23 88.1 85.7 88.87 87.30 89.05 87.74 89.30 88.23 90.43 8"
P18-1130,Q16-1026,0,0.0270272,"to be introduced. The predefined order of children can have different alternatives, such as leftto-right or inside-out2 . In this paper, we adopt the inside-out order3 since it enables us to utilize second-order sibling information, which has been proven beneficial for parsing performance (McDonald and Pereira, 2006; Koo and Collins, 2010) (see § 3.4 for details). Figure 1 (b) depicts the architecture of S TACK P TR and the decoding procedure for the example sentence in Figure 1 (a). 3.2 Encoder The encoder of our parsing model is based on the bi-directional LSTM-CNN architecture (BLSTMCNNs) (Chiu and Nichols, 2016; Ma and Hovy, 2016) where CNNs encode character-level information of a word into its character-level repre2 Order the children by the distances to the head word on the left side, then the right side. 3 We also tried left-to-right order which obtained worse parsing accuracy than inside-out. 1405 sentation and BLSTM models context information of each word. Formally, for each word, the CNN, with character embeddings as inputs, encodes the character-level representation. Then the character-level representation vector is concate012nated 3456278 2965the 69 86 2embedding 5214 2523775 4395tofe"
P18-1130,P15-1033,0,0.0865232,"ly (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings o"
P18-1130,C96-1058,0,0.785418,"timent analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies"
P18-1130,W15-0802,1,0.779907,"s the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of mu"
P18-1130,Q16-1023,0,0.260785,"graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based arc"
P18-1130,P10-1001,0,0.638136,"ine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have"
P18-1130,D10-1125,0,0.0229124,"92±0.16] 93.57±0.12 [90.07±0.20] 87.59±0.36 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (201"
P18-1130,D16-1180,0,0.694907,"based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the corresponding asymptotic"
P18-1130,P81-1022,0,0.781098,"Missing"
P18-1130,P14-1130,0,0.0161117,"26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precisi"
P18-1130,D13-1203,0,0.0244038,"d parsers, yielding an efficient decoding algorithm with O(n2 ) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen"
P18-1130,N15-1142,0,0.0305836,"ned by certain head words. Our parser follows a similar kind of annotation process: starting from reading the whole sentence, and processing in a top-down manner by finding the main predicates first and only then search for sub-trees governed by them. When making latter decisions, the parser has access to the entire structure built in earlier steps. 3.8 Implementation Details Pre-trained Word Embeddings. For all the parsing models in different languages, we initialize word vectors with pretrained word embeddings. For Chinese, Dutch, English, German and Spanish, we use the structured-skipgram (Ling et al., 2015) embeddings. For other languages we use Polyglot embeddings (Al-Rfou et al., 2013). Optimization. Parameter optimization is performed with the Adam optimizer (Kingma and Ba, 2014) with β1 = β2 = 0.9. We choose an initial learning rate of η0 = 0.001. The learning rate η is annealed by multiplying a fixed decay rate ρ = 0.75 when parsing performance stops increasing on validation sets. To reduce the effects of “gradient exploding”, we use gradient clipping of 5.0 (Pascanu et al., 2013). Dropout Training. To mitigate overfitting, we apply dropout (Srivastava et al., 2014; Ma et al., 2017). For BL"
P18-1130,D15-1166,0,0.0166961,"e stack σ. Children: ch(wi ) denotes the list of all the children (modifiers) of word wi . 2.2 Pointer Networks Pointer Networks (P TR -N ET) (Vinyals et al., 2015) are a variety of neural network capable of learning the conditional probability of an output sequence with elements that are discrete tokens corresponding to positions in an input sequence. This model cannot be trivially expressed by standard sequence-to-sequence networks (Sutskever et al., 2014) due to the variable number of input positions in each sentence. P TR -N ET solves the problem by using attention (Bahdanau et al., 2015; Luong et al., 2015) as a pointer to select a member of the input sequence as the output. Formally, the words of the sentence x are fed one-by-one into the encoder (a multiple-layer bidirectional RNN), producing a sequence of encoder hidden states si . At each time step t, the decoder (a uni-directional RNN) receives the input from last step and outputs decoder hidden state ht . The attention vector at is calculated as follows: eti = score(ht , si ) at = softmax (et ) (1) where score(·, ·) is the attention scoring function, which has several variations such as dot-product, 1404 2 2 3 3 4 s1 s2 s3 s4 s5 s6 h1 $ Bu"
P18-1130,D15-1154,1,0.801978,"us et al., 1993), the Penn Chinese Treebank (CTB version 5.1) (Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009). We use the same experimental settings as Kuncoro et al. (2016). To make a thorough empirical comparison with previous studies, we also evaluate our system on treebanks from CoNLL shared task and the Universal Dependency (UD) Treebanks4 . For the CoNLL Treebanks, we use the English treebank from CoNLL-2008 shared task (Surdeanu et al., 2008) and all 13 treebanks from CoNLL-2006 shared task (Buchholz and Marsi, 2006). The experimental settings are the same as Ma and Hovy (2015). For UD Treebanks, we select 12 languages. The details of the treebanks and experimental settings are in § 4.5 and Appendix B. Evaluation Metrics Parsing performance is measured with five metrics: unlabeled attachment score (UAS), labeled attachment score (LAS), unlabeled complete match (UCM), labeled complete match (LCM), and root accuracy (RA). Following previous work (Kuncoro et al., 2016; Dozat and Manning, 2017), we report results excluding punctuations for Chinese and English. For each experiment, we report the mean values with corresponding standard deviations over 5 repetitions. 1407"
P18-1130,P16-1101,1,0.832214,"edefined order of children can have different alternatives, such as leftto-right or inside-out2 . In this paper, we adopt the inside-out order3 since it enables us to utilize second-order sibling information, which has been proven beneficial for parsing performance (McDonald and Pereira, 2006; Koo and Collins, 2010) (see § 3.4 for details). Figure 1 (b) depicts the architecture of S TACK P TR and the decoding procedure for the example sentence in Figure 1 (a). 3.2 Encoder The encoder of our parsing model is based on the bi-directional LSTM-CNN architecture (BLSTMCNNs) (Chiu and Nichols, 2016; Ma and Hovy, 2016) where CNNs encode character-level information of a word into its character-level repre2 Order the children by the distances to the head word on the left side, then the right side. 3 We also tried left-to-right order which obtained worse parsing accuracy than inside-out. 1405 sentation and BLSTM models context information of each word. Formally, for each word, the CNN, with character embeddings as inputs, encodes the character-level representation. Then the character-level representation vector is concate012nated 3456278 2965the 69 86 2embedding 5214 2523775 4395tofeed into with word ve"
P18-1130,I17-1007,1,0.717476,"0.21] 90.10±0.27 [87.05±0.26] 93.25±0.05 [93.17±0.05] 94.77±0.05 [93.21±0.10] 93.38±0.08 [91.92±0.16] 93.57±0.12 [90.07±0.20] 87.59±0.36 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arc"
P18-1130,N16-1116,1,0.871211,"Missing"
P18-1130,P14-1126,1,0.849815,"tep towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of op"
P18-1130,C12-2077,1,0.903676,"l Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the corresponding asymptotic efficiency, but still maintains a global view of the sentence that proves essential for achieving competitive accuracy. Our S TACK P TR parser has a pointer network (Vinyals et al., 2015) as its backbone, and is equipped with an internal stack to maintain the order of head words in tree structures. The S TACK P TR parser performs parsing in an incremental, topdown, depth-first fashion;"
P18-1130,J93-2004,0,0.0662464,"idden states and 0.33 between layers. Following Dozat and Manning (2017), we also use embedding dropout with a rate of 0.33 on all word, character, and POS embeddings. Hyper-Parameters. Some parameters are chosen from those reported in Dozat and Manning (2017). We use the same hyper-parameters across the models on different treebanks and languages, due to time constraints. The details of the chosen hyper-parameters for all experiments are summarized in Appendix A. 4 Experiments 4.1 Setup We evaluate our S TACK P TR parser mainly on three treebanks: the English Penn Treebank (PTB version 3.0) (Marcus et al., 1993), the Penn Chinese Treebank (CTB version 5.1) (Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009). We use the same experimental settings as Kuncoro et al. (2016). To make a thorough empirical comparison with previous studies, we also evaluate our system on treebanks from CoNLL shared task and the Universal Dependency (UD) Treebanks4 . For the CoNLL Treebanks, we use the English treebank from CoNLL-2008 shared task (Surdeanu et al., 2008) and all 13 treebanks from CoNLL-2006 shared task (Buchholz and Marsi, 2006). The experimental settings are the same as Ma and Hovy (201"
P18-1130,P13-2109,0,0.0287562,"6 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advan"
P18-1130,J11-1007,0,0.635811,"McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 2011). Previous studies have explored solutions to address this challenge. Stack LSTMs (Dyer et al., 2015; Ballesteros et al., 2015, 2016) are capable of learning representations of the parser state that are sensitive to the complete contents of the parser’s state. Andor et al. (2016) proposed a globally normalized transition model to replace the locally normalized classifier. However, the parsing accuracy is still behind state-of-the-art graph-based parsers (Dozat and Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive s"
P18-1130,E06-1011,0,0.718934,"ly 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the corresponding asymptotic efficiency, but still maintains a global view of the sentence that proves essential for achieving competitive accuracy. Our S TACK P TR parser has a pointer network (Vinyals et al., 2015) as its backbone, and is equipped with an internal stack to maintain the order of head words in tree structures. The S TACK P TR parser performs parsing in an"
P18-1130,H05-1066,0,0.819927,"s (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 20"
P18-1130,P10-1142,0,0.0278987,"ition-based parsers, yielding an efficient decoding algorithm with O(n2 ) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zh"
P18-1130,D09-1143,0,0.011773,"ate-of-theart performance on 21 of them. 1 Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially"
P18-1130,C04-1010,0,0.0584363,"rence resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information,"
P18-1130,Q17-1008,1,0.818078,"Introduction Dependency parsing, which predicts the existence and type of linguistic dependency relations between words, is a first step towards deep language understanding. Its importance is widely recognized in the natural language processing (NLP) community, with it benefiting a wide range of NLP applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build d"
P18-1130,D11-1022,0,0.029926,"[90.07±0.20] 87.59±0.36 [78.85±0.53] 90.87±0.26 [87.80±0.31] 92.49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does no"
P18-1130,petrov-etal-2012-universal,0,0.10217,"Missing"
P18-1130,P05-1012,0,0.857049,"s (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propagation and worse performance compared to graph-based parsers on root and long dependencies (McDonald and Nivre, 20"
P18-1130,N15-1068,0,0.019183,"t Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precision for arcs further away from the root. Furthermore, the S TACK P TR parser"
P18-1130,W08-2121,0,0.156465,"Missing"
P18-1130,P15-1150,0,0.0877003,"Missing"
P18-1130,P16-1218,0,0.102311,"Manning, 2017). Graph-based dependency parsers, on the other hand, learn scoring functions for parse trees and perform exhaustive search over all possible trees for a sentence to find the globally highest scoring 1403 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1403–1414 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tree. Incorporating this global search algorithm with distributed representations learned from neural networks, neural graph-based parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017) have achieved the state-of-the-art accuracies on a number of treebanks in different languages. Nevertheless, these models, while accurate, are usually slow (e.g. decoding is O(n3 ) time complexity for first-order models (McDonald et al., 2005a,b) and higher polynomials for higherorder models (McDonald and Pereira, 2006; Koo and Collins, 2010; Ma and Zhao, 2012b,a)). In this paper, we propose a novel neural network architecture for dependency parsing, stackpointer networks (S TACK P TR). S TACK P TR is a transition-based architecture, with the co"
P18-1130,P15-1032,0,0.190239,"Missing"
P18-1130,C02-1145,0,0.226098,"2017), we also use embedding dropout with a rate of 0.33 on all word, character, and POS embeddings. Hyper-Parameters. Some parameters are chosen from those reported in Dozat and Manning (2017). We use the same hyper-parameters across the models on different treebanks and languages, due to time constraints. The details of the chosen hyper-parameters for all experiments are summarized in Appendix A. 4 Experiments 4.1 Setup We evaluate our S TACK P TR parser mainly on three treebanks: the English Penn Treebank (PTB version 3.0) (Marcus et al., 1993), the Penn Chinese Treebank (CTB version 5.1) (Xue et al., 2002), and the German CoNLL 2009 corpus (Hajiˇc et al., 2009). We use the same experimental settings as Kuncoro et al. (2016). To make a thorough empirical comparison with previous studies, we also evaluate our system on treebanks from CoNLL shared task and the Universal Dependency (UD) Treebanks4 . For the CoNLL Treebanks, we use the English treebank from CoNLL-2008 shared task (Surdeanu et al., 2008) and all 13 treebanks from CoNLL-2006 shared task (Buchholz and Marsi, 2006). The experimental settings are the same as Ma and Hovy (2015). For UD Treebanks, we select 12 languages. The details of the"
P18-1130,W03-3023,0,0.337352,"applications, such as coreference resolution (Ng, 2010; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is base"
P18-1130,P14-2107,0,0.015528,"79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precision for arcs further away from the root. Furtherm"
P18-1130,D14-1109,0,0.0152966,"49±0.21 [89.01±0.22] 79.56±0.22 [68.03±0.15] Best Published UAS LAS 81.12 – 94.02 – 93.04 – 91.16 85.14 92.00 – 87.39 – 93.25 – 92.71 89.80 93.80 – 93.03 – 87.06 – 88.75 84.03 91.85 85.26 78.43 66.16 Table 3: UAS and LAS on 14 treebanks from CoNLL shared tasks, together with several state-of-the-art parsers. Bi-Att is the bi-directional attention based parser (Cheng et al., 2016), and NeuroMST is the neural MST parser (Ma and Hovy, 2017). “Best Published” includes the most accurate parsers in term of UAS among Koo et al. (2010), Martins et al. (2011), Martins et al. (2013), Lei et al. (2014), Zhang et al. (2014), Zhang and McDonald (2014), Pitler and McDonald (2015), and Cheng et al. (2016). in McDonald and Nivre (2011). One possible reason is that, unlike traditional transition-based parsers that scan the sentence from left to right, S TACK P TR processes in a top-down manner, thus sometimes unnecessarily creating shorter dependency arcs first. Root Distance. Figure 3 (c) plots the precision and recall of each system for arcs of varying distance to the root. Different from the observation in McDonald and Nivre (2011), S TACK P TR does not show an obvious advantage on the precision for arcs further a"
P18-1130,P11-2033,0,0.0696926,"10; Durrett and Klein, 2013; Ma et al., ∗ Work done while at Carnegie Mellon University. 2016), sentiment analysis (Tai et al., 2015), machine translation (Bastings et al., 2017), information extraction (Nguyen et al., 2009; Angeli et al., 2015; Peng et al., 2017), word sense disambiguation (Fauceglia et al., 2015), and low-resource languages processing (McDonald et al., 2013; Ma and Xia, 2014). There are two dominant approaches to dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007): local and greedy transitionbased algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Zhang and Nivre, 2011; Chen and Manning, 2014), and the globally optimized graph-based algorithms (Eisner, 1996; McDonald et al., 2005a,b; Koo and Collins, 2010). Transition-based dependency parsers read words sequentially (commonly from left-to-right) and build dependency trees incrementally by making series of multiple choice decisions. The advantage of this formalism is that the number of operations required to build any projective parse tree is linear with respect to the length of the sentence. The challenge, however, is that the decision made at each step is based on local information, leading to error propag"
P18-1130,W13-3520,0,\N,Missing
P18-1154,D16-1032,0,0.195865,"Missing"
P18-1154,D16-1128,0,0.139786,"Missing"
P18-1154,P09-1011,0,0.0572073,"we have introduced a new largescale data for game commentary generation. The commentaries cover a variety of aspects like move description, quality of move, and alternative moves. This leads to a content selection challenge, similar to that noted in Wiseman et al. (2017). Unlike Wiseman et al. (2017), our focus is on generating commentary for individual moves in a game, as opposed to game summaries from aggregate statistics as in their task. One of the first NLG datasets was the SUMTIME-METEO (Reiter et al., 2005) corpus with ≈ 500 record-text pairs for technical weather forecast generation. Liang et al (2009) worked on common weather forecast generation using the WEATHERGOV dataset, which has ≈ 10K record-text pairs. A criticism of WEATHERGOV dataset (Reiter, 2017) is that weather records themselves may have used templates and rules with optional human post-editing. There have been prior works on generating commentary for ROBOCUP matches (Chen and Mooney, 2008; Mei et al., 2015). The ROBOCUP dataset, however, is collected from 4 games and contains about 1K events in total. Our dataset is two orders of magnitude larger than the ROBOCUP dataset, and we hope that it provides a promising setting for f"
P18-1154,O90-1011,0,0.648433,"blocked by that move. Both descriptions are true, but the latter is most salient given the player’s goal. However, sometimes, none of the aspects may stand out as being most salient, and the most salient aspect may even change from commentator to commentator. Moreover, a human commentator may introduce variations in the aspects he or she chooses to talk about, in order to reduce monotony in the commentary. This makes the dataset a useful testbed not only for NLG but also for related work on modeling pragmatics in language (Liu et al., 2016). Prior work has explored game commentary generation. Liao and Chang (1990); Sadikov et al. (2006) have explored chess commentary generation, but for lack of large-scale training data their methods have been mainly rule-based. Kameko et al. (2015) have explored commentary generation for the game of Shogi, proposing a twostep process where salient terms are generated from the game state and then composed in a language model. In contrast, given the larger amount of training data available to us, our proposed model uses an end-to-end trainable neural architecture to predict commentaries given the game state. Our model conditions on semantic and pragmatic information abo"
P18-1154,D16-1230,0,0.0252276,"imply that the pawn was moved, or one may comment on how the check was blocked by that move. Both descriptions are true, but the latter is most salient given the player’s goal. However, sometimes, none of the aspects may stand out as being most salient, and the most salient aspect may even change from commentator to commentator. Moreover, a human commentator may introduce variations in the aspects he or she chooses to talk about, in order to reduce monotony in the commentary. This makes the dataset a useful testbed not only for NLG but also for related work on modeling pragmatics in language (Liu et al., 2016). Prior work has explored game commentary generation. Liao and Chang (1990); Sadikov et al. (2006) have explored chess commentary generation, but for lack of large-scale training data their methods have been mainly rule-based. Kameko et al. (2015) have explored commentary generation for the game of Shogi, proposing a twostep process where salient terms are generated from the game state and then composed in a language model. In contrast, given the larger amount of training data available to us, our proposed model uses an end-to-end trainable neural architecture to predict commentaries given the"
P18-1154,P02-1040,0,0.102226,"ross-entropy loss over the decoding outputs to train the model. 4 Experiments We split each of the data subsets in a 70:10:20 ratio into train, validation and test. All our models are implemented in Pytorch version 0.3.1 (Paszke et al., 2017). We use the ADAM optimizer (Kingma and Ba, 2014) with its default parameters and a mini-batch size of 32. Validation set perplexity is used for early-stopping. At test-time, we use greedy search to generate the model output. We observed that beam decoding does not lead to any significant improvement in terms of validation BLEU score. We observe the BLEU (Papineni et al., 2002) and BLEU-2 (Vedantam et al., 2015) scores to measure the performance of the models. Addi1665 Dataset Features TEMP NN (M+T+S) RAW MoveDesc GAC-sparse GAC (M+T) TEMP NN (M+T) RAW Quality GAC-sparse GAC(M+T+S) NN (M) RAW Comparative GAC-sparse GAC(M+T) tionally, we consider a measure to quantify the diversity in the generated outputs. Finally, we also conduct a human evaluation study. In the remainder of this section, we discuss baselines along with various experiments and results. 4.1 Baselines In this subsection we discuss the various baseline methods. Manually-defined template (TEMP) We devi"
P18-1154,J09-4008,0,0.159794,"Missing"
P18-1154,W12-1516,0,0.0199831,"and previous board (Ri ) information from the game. P (Si |Mi , Gi ) = P (Si |Mi , Ci , Ri ). We model this using an end-to-end trainable neural model, which models conjunctions of features using feature encoders. Our model employs a selection mechanism to select the salient features for a given chess move. Finally a LSTM recurrent neural network (Hochreiter and Schmidhuber, 1997) is used to generate the commentary text based on selected features from encoder. 3.1 Incorporating Domain Knowledge Past work shows that acquiring domain knowledge is critical for NLG systems (Reiter et al., 2003b; Mahamood and Reiter, 2012). Commentary texts cover a range of perspectives, including criticism or goodness of current move, possible alternate moves, quality of alternate moves, etc. To be able to make such comments, the model must learn about the quality of moves, as well as the set of valid moves for a given chess board state. We consider the following features to provide our model with necessary information to generate commentary texts (Figure 3): Move features fmove (Mi , Ci , Ri ) encode the current move information such as which piece moved, the position of the moved piece before and after the move was made, the"
P18-1155,D15-1166,0,0.0309628,"ne translation and image captioning. Due to the space limit, we only present the information necessary to compare the empirical results at this moment. For a more detailed description, we refer readers to Appendix B and the code6 . Machine Translation Following Ranzato et al. (2015), we evaluate on IWSLT 2014 German-toEnglish dataset (Mauro et al., 2012). The corpus contains approximately 153K sentence pairs in the training set. We follow the pre-processing procedure used in (Ranzato et al., 2015). Architecture wise, we employ a seq2seq model with dot-product attention (Bahdanau et al., 2014; Luong et al., 2015), where the encoder is a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with each direction being size 128, and the decoder is another LSTM of size 256. Moreover, we consider two variants of the decoder, one using the input feeding technique (Luong et al., 2015) and the other not. For all algorithms, the sequence-level BLEU score is employed as the pay-off function R, while the corpus-level BLEU score (Papineni et al., 2002) is used for the final evaluation. The sequence-level BLEU score is scaled up by the sentence length so that the scale of the immediate reward at each step is invari"
P18-1155,2012.eamt-1.60,0,0.011215,"continuous control and employs the advantage actor critic variant as in (Mnih et al., 2016), while ERAC follows the Q actor critic as in (Bahdanau et al., 2016). 6 Experiments 6.1 Experiment Settings In this work, we focus on two sequence prediction tasks: machine translation and image captioning. Due to the space limit, we only present the information necessary to compare the empirical results at this moment. For a more detailed description, we refer readers to Appendix B and the code6 . Machine Translation Following Ranzato et al. (2015), we evaluate on IWSLT 2014 German-toEnglish dataset (Mauro et al., 2012). The corpus contains approximately 153K sentence pairs in the training set. We follow the pre-processing procedure used in (Ranzato et al., 2015). Architecture wise, we employ a seq2seq model with dot-product attention (Bahdanau et al., 2014; Luong et al., 2015), where the encoder is a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with each direction being size 128, and the decoder is another LSTM of size 256. Moreover, we consider two variants of the decoder, one using the input feeding technique (Luong et al., 2015) and the other not. For all algorithms, the sequence-level BLEU scor"
P18-1155,P02-1040,0,0.106462,"We follow the pre-processing procedure used in (Ranzato et al., 2015). Architecture wise, we employ a seq2seq model with dot-product attention (Bahdanau et al., 2014; Luong et al., 2015), where the encoder is a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) with each direction being size 128, and the decoder is another LSTM of size 256. Moreover, we consider two variants of the decoder, one using the input feeding technique (Luong et al., 2015) and the other not. For all algorithms, the sequence-level BLEU score is employed as the pay-off function R, while the corpus-level BLEU score (Papineni et al., 2002) is used for the final evaluation. The sequence-level BLEU score is scaled up by the sentence length so that the scale of the immediate reward at each step is invariant to the length. Image Captioning For image captioning, we consider the MSCOCO dataset (Lin et al., 2014). We adapt the same preprocessing procedure and the train/dev/test split used by Karpathy and FeiFei (2015). The NIC (Vinyals et al., 2015) is employed as the baseline model, where a feature vector of the image is extracted by a pre-trained CNN and then used to initialize the LSTM decoder. Different from the original NIC model"
P18-1155,D15-1044,0,0.0359675,"ets, we show the proposed algorithms outperform RAML and Actor-Critic respectively, providing new alternatives to sequence prediction. 1 Introduction Modeling and predicting discrete sequences is the central problem to many natural language processing tasks. In the last few years, the adaption of recurrent neural networks (RNNs) and the sequenceto-sequence model (seq2seq) (Sutskever et al., 2014; Bahdanau et al., 2014) has led to a wide range of successes in conditional sequence prediction, including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014), automatic summarization (Rush et al., 2015), image captioning (Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015) and speech recognition (Chan et al., 2016). Despite the distinct evaluation metrics for the aforementioned tasks, the standard training algorithm has been the same for all of them. Specifically, the algorithm is based on maximum likelihood estimation (MLE), which maximizes the log∗ Equal contribution. • A widely explored idea is to directly optimize the task metric for sequences produced by the model, with the specific approaches ranging from minimum risk training (MRT) (Shen et al., 2015) and learning as se"
P18-1155,D16-1137,0,0.0479988,"Missing"
P18-1225,D14-1059,0,0.0299562,"ations available in a knowledge-base:  := r(x; y) where the relation r (such as synonym, hypernym, etc.) may differ across knowledge bases. We use a simple (partial) transformation function, f (s):=“Replace x in s with y”, as described in an earlier example. In some cases, when part-of-speech (POS) tags are available, the partial function requires the tags for x in s and in r(x; y) to match. The entailment label g for the resulting examples is also defined based on the relation r, as summarized in Table 2. This idea is similar to Natural Logic Inference or NLI (Lakoff, 1970; Sommers, 1982; Angeli and Manning, 2014) where words in a sentence can be replaced by their hypernym/hyponym to produce entailing/neutral sentences, depending on their context. We propose a context-agnostic use of lexical resources that, despite its simplicity, already results in significant gains. We use three sources for generators: WordNet (Miller, 1995) is a large, handcurated, semantic lexicon with synonymous words grouped into synsets. Synsets are connected by many semantic relations, from which we use hyponym and synonym relations to generate entailing sentences, and antonym relations to generate contradicting sentences2 . Gi"
P18-1225,J16-4007,0,0.0227354,"t al., 2013) is a large resource of lexical, phrasal, and syntactic paraphrases. We use 24,273 lexical paraphrases in their smallest set, PPDB-S (Pavlick et al., 2015), as equivalence relations, x  y. The (partial) transformation function f for this generator is POS-tagged matched replacement of x in s with y, and the label g is entails. SICK (Marelli et al., 2014) is dataset with entailment examples of the form (p; h; c), created to evaluate an entailment model’s ability to capture compositional knowledge via hand-authored rules. We use the 12,508 patterns of the form c(x; y) extracted by Beltagy et al. (2016) by comparing sentences in this dataset, with the property that for each SICK example (p; h; c), replacing (when applicable) x with y in p produces h. For simplicity, we ignore positional information in these patterns. The (partial) transformation function f is replacement of x in s with y, and the label g is c. 3.2 Hand-Defined Generators Even very large entailment datasets have no or very few examples of certain otherwise common linguistic constructs such as negation,3 causing models trained on them to struggle with these constructs. A simple model-agnostic way to alleviate this issue is v"
P18-1225,D15-1075,0,0.144757,"Missing"
P18-1225,N15-1184,1,0.885486,"and WordNet (Miller, 1995). These resources could help them generalize beyond specific words observed during training. For instance, while the SNLI dataset contains the pattern two men  people, it does not contain the analogous pattern two dogs  animals found easily in WordNet. Effectively integrating simple rules or linguistic resources in a deep learning model, however, is challenging. Doing so directly by substantially adapting the model architecture (Sha et al., 2016; Chen et al., 2018) can be cumbersome and limiting. Incorporating such knowledge indirectly via modified word embeddings (Faruqui et al., 2015; Mrkˇsi´c et al., 2016), as we show, can have little positive impact and can even be detrimental. Our proposed method, which is task-specific but model-independent, is inspired by dataaugmentation techniques. We generate new training examples by applying knowledge-guided rules, via only a handful of rule templates, to the original training examples. Simultaneously, we also use a sequence-to-sequence or seq2seq model for each entailment class to generate new hypotheses from a given premise, adaptively creating new adversarial examples. These can be used with any entailment model without constr"
P18-1225,N13-1092,0,0.113536,"Missing"
P18-1225,P18-2103,0,0.0762172,"Missing"
P18-1225,N18-2017,0,0.0885405,"Missing"
P18-1225,H05-1049,0,0.0755534,"scriminator. The generator uses explicit knowledge or hand written rules, and is trained in a end-to-end fashion along with the discriminator. Incorporating external rules or linguistic resources in a deep learning model generally requires substantially adapting the model architecture (Sha et al., 2016; Liang et al., 2017; Kang et al., 2017). This is a model-dependent approach, which can be cumbersome and constraining. Similarly non-neural textual entailment models have been developed that incorporate knowledge bases. However, these also require model-specific engineering (Raina et al., 2005; Haghighi et al., 2005; Silva et al., 2018). An alternative is the model- and taskindependent route of incorporating linguistic resources via word embeddings that are retro-fitted (Faruqui et al., 2015) or counterfitted (Mrkˇsi´c et al., 2016) to such resources. We demonstrate, however, that this has little positive impact in our setting and can even be detrimental. Further, it is unclear how to incorporate knowledge sources into advanced representations such as contextual embeddings (McCann et al., 2419 2017; Peters et al., 2018). We thus focus on a task-specific but model-independent approach. Logical rules have"
P18-1225,D17-1215,0,0.123622,"Missing"
P18-1225,D17-1292,1,0.809478,"ork systems trained on a large corpus can easily break when faced with carefully designed unseen adversarial patterns at test time. Our motivation is different. We use adversarial examples at training time, in a data augmentation setting, to train a more robust entailment discriminator. The generator uses explicit knowledge or hand written rules, and is trained in a end-to-end fashion along with the discriminator. Incorporating external rules or linguistic resources in a deep learning model generally requires substantially adapting the model architecture (Sha et al., 2016; Liang et al., 2017; Kang et al., 2017). This is a model-dependent approach, which can be cumbersome and constraining. Similarly non-neural textual entailment models have been developed that incorporate knowledge bases. However, these also require model-specific engineering (Raina et al., 2005; Haghighi et al., 2005; Silva et al., 2018). An alternative is the model- and taskindependent route of incorporating linguistic resources via word embeddings that are retro-fitted (Faruqui et al., 2015) or counterfitted (Mrkˇsi´c et al., 2016) to such resources. We demonstrate, however, that this has little positive impact in our setting and"
P18-1225,P17-1003,0,0.0233465,"veal how neural network systems trained on a large corpus can easily break when faced with carefully designed unseen adversarial patterns at test time. Our motivation is different. We use adversarial examples at training time, in a data augmentation setting, to train a more robust entailment discriminator. The generator uses explicit knowledge or hand written rules, and is trained in a end-to-end fashion along with the discriminator. Incorporating external rules or linguistic resources in a deep learning model generally requires substantially adapting the model architecture (Sha et al., 2016; Liang et al., 2017; Kang et al., 2017). This is a model-dependent approach, which can be cumbersome and constraining. Similarly non-neural textual entailment models have been developed that incorporate knowledge bases. However, these also require model-specific engineering (Raina et al., 2005; Haghighi et al., 2005; Silva et al., 2018). An alternative is the model- and taskindependent route of incorporating linguistic resources via word embeddings that are retro-fitted (Faruqui et al., 2015) or counterfitted (Mrkˇsi´c et al., 2016) to such resources. We demonstrate, however, that this has little positive impact"
P18-1225,D15-1166,0,0.0315292,"Missing"
P18-1225,marelli-etal-2014-sick,0,0.0657437,"transformation function f is the POS-tag matched replacement of x in s with y, and requires the POS tag to be noun or verb. NLI provides a more robust way of using these relations based on context, which we leave for future work. PPDB (Ganitkevitch et al., 2013) is a large resource of lexical, phrasal, and syntactic paraphrases. We use 24,273 lexical paraphrases in their smallest set, PPDB-S (Pavlick et al., 2015), as equivalence relations, x  y. The (partial) transformation function f for this generator is POS-tagged matched replacement of x in s with y, and the label g is entails. SICK (Marelli et al., 2014) is dataset with entailment examples of the form (p; h; c), created to evaluate an entailment model’s ability to capture compositional knowledge via hand-authored rules. We use the 12,508 patterns of the form c(x; y) extracted by Beltagy et al. (2016) by comparing sentences in this dataset, with the property that for each SICK example (p; h; c), replacing (when applicable) x with y in p produces h. For simplicity, we ignore positional information in these patterns. The (partial) transformation function f is replacement of x in s with y, and the label g is c. 3.2 Hand-Defined Generators Even"
P18-1225,D16-1244,0,0.206479,"Missing"
P18-1225,P15-2070,0,0.0469679,"Missing"
P18-1225,2014.lilt-9.7,0,0.0334983,"Missing"
P18-1225,D14-1162,0,0.0798199,"Missing"
P18-1225,N18-1170,0,0.0596279,"Missing"
P18-1225,N18-1202,0,0.0432733,"e bases. However, these also require model-specific engineering (Raina et al., 2005; Haghighi et al., 2005; Silva et al., 2018). An alternative is the model- and taskindependent route of incorporating linguistic resources via word embeddings that are retro-fitted (Faruqui et al., 2015) or counterfitted (Mrkˇsi´c et al., 2016) to such resources. We demonstrate, however, that this has little positive impact in our setting and can even be detrimental. Further, it is unclear how to incorporate knowledge sources into advanced representations such as contextual embeddings (McCann et al., 2419 2017; Peters et al., 2018). We thus focus on a task-specific but model-independent approach. Logical rules have also been defined to label existing examples based on external resources (Hu et al., 2016). Our focus here is on generating new training examples. Our use of the GAN framework to create a better discriminator is related to CatGANs (Wang and Zhang, 2017) and TripleGANs (Chongxuan et al., 2017) where the discriminator is trained to classify the original training image classes as well as a new ‘fake’ image class. We, on the other hand, generate examples belonging to the same classes as the training examples. Fur"
P18-1225,D16-1264,0,0.0722131,"Missing"
P19-1329,N13-1090,0,0.268214,"embeddings capture two essential properties of numbers: magnitude (e.g. 3&lt;4) and numeration (e.g. 3=three). Our experiments reveal that most models capture an approximate notion of magnitude, but are inadequate at capturing numeration. We hope that our observations provide a starting point for the development of methods which better capture numeracy in NLP systems. 1 Introduction Word embeddings operationalize the distributional hypothesis, where a word is characterized by “the company it keeps” (Harris, 1954; Firth, 1957), and have been shown to capture semantic regularities in vector space (Mikolov et al., 2013c). They have been a driving force in NLP in recent years, and enjoy widespread use in a variety of semantic tasks (Rumelhart et al.; Mikolov et al., 2013a,b; Collobert and Weston, 2008; Glorot et al., 2011; Turney and Pantel, 2010; Turney, 2013). However, to what extent do these word representations capture numeric properties? Numbers often need to be dealt with precisely, and understanding the meaning of text also requires a careful understanding of the quantities involved. They have been identified to play an important role in textual entailment, a benchmark natural language ∗ *The first tw"
P19-1329,D09-1045,0,0.204085,"Missing"
P19-1329,C18-1198,1,0.895984,"Missing"
P19-1329,D14-1162,0,0.0838474,"Missing"
P19-1329,P18-2100,0,0.0591297,"Missing"
P19-1461,P08-1090,0,0.0126612,"e scene: we identify which goals are aimed at to fulfil a given motive, which plans are taken to 4675 achieve the goals, which actions and conditions appear in the plans, and how well they are actually performed. These intermediate components would relate what we call aspects in aspect-based sentiment analysis. Although we focused on sentiment analysis in this study, detection of motives can benefit other NLP applications such as in-depth machine reading. For example, underlying motives will be a strong clue for modeling a sequence of actions that share the same actor (a.k.a narrative chains (Chambers and Jurafsky, 2008)). 7 Conclusion We aimed at understanding why a writer of a text holds a particular sentiment and proposed a task of human motive detection as an essential building block to this end. We presented a taxonomy of motives derived from a psychology study and annotated 1,600 restaurant and laptop reviews with six motives. We evaluated the performance of baseline predictive models on this dataset.4 One interesting property is that the same underlying motives can appear in different domains even though their distribution may differ. We empirically verified this by transferring learned parameters acro"
P19-1461,N13-1132,1,0.765746,"crowd workers agreed moderately on annotations: Krippendorffs α was 0.48 and 0.59 in the restaurant and laptop domains, respectively. We found that S ELF FULFILLMENT and E MBRACE & E XPLORE L IFE are often hard to distinguish. We, therefore, collapsed these categories, and Krippendorffs α increased to 0.51 and 0.61. For reference, three graduate students studying language technology annotated 150 sentences in the restaurant domain. Their Krippendorffs α was 0.72 on the original annotation scheme and 0.74 on the collapsed scheme. Analysis: We next aggregated crowd workers responses using MACE (Hovy et al., 2013), where a response was regarded as a binary value of a combination of a text and a human motive. We set the prior probability of a positive class to 1/6 (i.e., one text is likely to have one of the six motives). This prior fits the responses better than a uniform prior. Table 2 shows the distributions of human motive labels. There is a clear difference between domains: the restaurant domain has a variety of motives relevant to hedonic motives (i.e. pleasure seeking) like S ELF -F ULFILLMENT (S F ) and S O CIAL R ELATION (S R ), while the laptop domain tends to have utilitarian motives (i.e. pr"
P19-1461,D14-1181,0,0.00240072,"g to BoNG vectors to emphasize topic words (BoNGtfidf ). 4.1.2 Multi-layer Perceptron (MLP) We build an MLP classifier with one hidden layer on top of word embedding-based sentence representations. We compress a variable-sized sequence of word embeddings into a fixed-sized sentence embedding before feeding them into MLPs using three standard encoders below. Simple word-embeddings model (SWEM): We calculate element-wise average and max-pooling of word embeddings in a sequence and concatenate them (Shen et al., 2018). CNN: A CNN aggregates adjacent word units in a hierarchical manner. We follow Kim (2014) and use filter windows of 3, 4, and 5. Training We simply treat our multi-label classification task as a set of binary classification tasks, where MLP classifiers share parameters except for those of an output layer over motive categories. To handle highly skewed class distributions, we minimize a weighted loss function to train a model. For example, MLP classifier minimizes a weighted crossentropy loss: X X L=− [wc yc log MLPc (x) (x,y)∈D c∈C +(1 − yc ) log(1 − MLPc (x))] , (1) where (x, y) is a pair of a sentence and a label in dataset D, C is a set of categories, and MLPc is an output func"
P19-1461,P14-5010,0,0.00395855,"the prior probability of a positive class to 1/6 (i.e., one text is likely to have one of the six motives). This prior fits the responses better than a uniform prior. Table 2 shows the distributions of human motive labels. There is a clear difference between domains: the restaurant domain has a variety of motives relevant to hedonic motives (i.e. pleasure seeking) like S ELF -F ULFILLMENT (S F ) and S O CIAL R ELATION (S R ), while the laptop domain tends to have utilitarian motives (i.e. practical needs) such as A MBITION &A BILITY (A A ) and F INANCE (F). 2 We use Stanford CoreNLP v.3.9.2 (Manning et al., 2014) to tokenize sentences. 4673 S ELF - FULFILLMENT (S F ) Finding meaning in life. Feeling satisfied with one’s life. “Ess-A-Bagel is by far the best bagel in NY.” *E MBRACE &E XPLORE L IFE (E E ) Being entertained. Exploring a new thing. “The wine list is extensive.” A PPRECIATING B EAUTY (BA ) Enjoying fine design/natural beauty. Being creative. “A beautifully designed dreamy restaurant.” S OCIAL R ELATION (S R ) Being treated well by others. Belonging to a social group. “Everyone was cheerfully cooperative.” H EALTH (H) Being physically healthy. “The fish was not fresh and the rice tasted old"
P19-1461,D14-1162,0,0.0821192,"ut-of-domain loss. 5 5.1 Experiments Setup We use macro-averaging of F1 measures over motive categories as the primary evaluation metrics. We conduct three-fold cross-validation, where the dataset is divided evenly into training, validation, and test sets. In each fold, we conduct a grid search of hyperparameters based on the validation set. We then use a training and validation set to train a model and test on a test split. We report the average scores over test splits as the final score. We use pretrained 100-D GloVe embeddings trained on 6 billion tokens from Wikipedia and Gigaword corpus (Pennington et al., 2014).3 We provide the implementation details in the appendix. 5.2 Results Table 3 shows that the MLP classifiers performed better or on par with the SVM classifier in terms of F1 measure. The low recall scores of SVM classifiers indicate that surface-level features are insufficient to detect various realizations of human motives. 3 https://nlp.stanford.edu/projects/ glove/ Interestingly, adding out-of-domain data improved F1 of all classifiers except SVM-BoNGtfidf . Particularly, the precision of the MLP classifiers increased by transfer learning. For the MLP-CNN classifier, the boost from laptop"
P19-1461,P18-1213,0,0.160708,"ervice is terrible because one’s basic motive for social behavior is not met. What and how many motives do we have? Decades of effort have been devoted to this question in research areas such as psychology, for example, (Maslow, 1943). A recent study by Talevich et al. (2017) defines a taxonomy of motives, including S ELF - FULFILLMENT, A PPRECIATING B EAUTY, S OCIAL R ELATION, H EALTH, A MBI TION &A BILITY , and F INANCE . We use their comprehensive taxonomy for understanding sentiments. Our work is in line with studies attempting to identify relevant motives in texts (Ding and Riloff, 2018; Rashkin et al., 2018), aiming to equip machines with the ability to understand a more complete description of a situation and justify human decisions and actions. While Ding and Riloff (2018) and Rashkin et al. (2018) specifically focus 4672 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4672–4677 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics on predicate-argument tuples and artificial texts, respectively, our work analyzes real review sentences. As an initial step, we conduct a task of human motive detection. We manually"
P19-1461,P18-1041,0,0.0125086,"rare words, we discard n-grams that occur only once in a training set. We also apply TF-IDF scaling to BoNG vectors to emphasize topic words (BoNGtfidf ). 4.1.2 Multi-layer Perceptron (MLP) We build an MLP classifier with one hidden layer on top of word embedding-based sentence representations. We compress a variable-sized sequence of word embeddings into a fixed-sized sentence embedding before feeding them into MLPs using three standard encoders below. Simple word-embeddings model (SWEM): We calculate element-wise average and max-pooling of word embeddings in a sequence and concatenate them (Shen et al., 2018). CNN: A CNN aggregates adjacent word units in a hierarchical manner. We follow Kim (2014) and use filter windows of 3, 4, and 5. Training We simply treat our multi-label classification task as a set of binary classification tasks, where MLP classifiers share parameters except for those of an output layer over motive categories. To handle highly skewed class distributions, we minimize a weighted loss function to train a model. For example, MLP classifier minimizes a weighted crossentropy loss: X X L=− [wc yc log MLPc (x) (x,y)∈D c∈C +(1 − yc ) log(1 − MLPc (x))] , (1) where (x, y) is a pair of"
P19-1562,P15-1033,0,0.0168124,"d neural dependency parser (Dozat and Manning, 2017). With evaluations on 14 treebanks, we empirically show that global output-structured models can generally obtain better performance, especially on the metric of sentence-level Complete Match. However, probably because neural models already learn good global views of the inputs, the improvement brought by structured output modeling is modest. 1 Introduction In the past few years, dependency parsers, equipped with neural network models, have led to impressive empirical successes on parsing accuracy (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017; Ma et al., 2018). Among them, the deep-biaffine attentional parser (BiAF) (Dozat and Manning, 2017) has stood out for its simplicity and effectiveness. BiAF adopts a simple bi-directional LSTM neural architecture (Ma and Hovy, 2016; Kiperwasser and Goldberg, 2016) with the first-order graph parsing algorithm (McDonald et al., 2005a,b). Simple as it appears to be, BiAF has led to several recordbreaking performences in multiple treebanks and languages (Dozat et al., 2017). In their pioneering wor"
P19-1562,C96-1058,0,0.735368,"Linguistics 2.1 Factorization The output structure of dependency parsing is a collection of dependency edges forming a singlerooted tree. Graph-based dependency parsers factorize the outputs into specifically-shaped subtrees (factors). Based on the assumption that the sub-trees are independent to each other, the score of the output tree structure (T ) is the combination of the scores of individual sub-trees in the tree. In the simplest case, the sub-trees are the individual dependency edges connecting each modifier and its head word ((m, h)). This is referred to as first-order factorization (Eisner, 1996; McDonald et al., 2005a), which is adopted in (Dozat and Manning, 2017) and the neural parsing models in this work. There are further extensions to higherorder factors, considering more complex sub-trees with multiple edges (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012). We leave the exploration of these higher-order graph models to future work. 2.2 Normalization After obtaining the individual scores of the substructures, we need to compute the score of the whole output structure. The main question is on what scale to normalize the output scores. For gr"
P19-1562,W15-1508,0,0.0403021,"Missing"
P19-1562,Q16-1023,0,0.570477,"anning, 2017). With evaluations on 14 treebanks, we empirically show that global output-structured models can generally obtain better performance, especially on the metric of sentence-level Complete Match. However, probably because neural models already learn good global views of the inputs, the improvement brought by structured output modeling is modest. 1 Introduction In the past few years, dependency parsers, equipped with neural network models, have led to impressive empirical successes on parsing accuracy (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017; Ma et al., 2018). Among them, the deep-biaffine attentional parser (BiAF) (Dozat and Manning, 2017) has stood out for its simplicity and effectiveness. BiAF adopts a simple bi-directional LSTM neural architecture (Ma and Hovy, 2016; Kiperwasser and Goldberg, 2016) with the first-order graph parsing algorithm (McDonald et al., 2005a,b). Simple as it appears to be, BiAF has led to several recordbreaking performences in multiple treebanks and languages (Dozat et al., 2017). In their pioneering work, besides the neural architecture, Dozat and Mannin"
P19-1562,P10-1001,0,0.296269,"e independent to each other, the score of the output tree structure (T ) is the combination of the scores of individual sub-trees in the tree. In the simplest case, the sub-trees are the individual dependency edges connecting each modifier and its head word ((m, h)). This is referred to as first-order factorization (Eisner, 1996; McDonald et al., 2005a), which is adopted in (Dozat and Manning, 2017) and the neural parsing models in this work. There are further extensions to higherorder factors, considering more complex sub-trees with multiple edges (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012). We leave the exploration of these higher-order graph models to future work. 2.2 Normalization After obtaining the individual scores of the substructures, we need to compute the score of the whole output structure. The main question is on what scale to normalize the output scores. For graph-based parsing, there can be mainly three options: Global, Local or Single, following different structured output constraints and corresponding to different loss functions. Global Global models directly normalize at the level of overall tree structures, whose scores are obtained by direc"
P19-1562,D07-1015,0,0.126333,"typical ones: probabilistic MaximumLikelihood loss (Prob), which requires actual normalization over the output space, and Max-Margin Hinge loss (Hinge), which only requires lossaugmented decoding in the same output space. Table 1 summarizes the methods (normalization and loss function) that we investigate in our experiments. For global models, we consider both Projective (Proj) and Non-Projective (NProj) constraints. Specific algorithms are required for probabilistic loss (a variation of Inside-Outside algorithm for projective (Paskin, 2001) and MatrixTree Theorem for non-projective parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007)) and hinge loss (Eisner’s algorithm for projective (Eisner, 1996) and ChuLiu-Edmonds’ algorithm for non-projective parsing (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b)). For Single and Local models, we only utilize probabilistic loss, since in preliminary experiments we found hinge loss performed worse. No special algorithms other than simple enumeration are needed for them in training. In testing, we adopt non-projective algorithms for the non-global models unless otherwise noted. 3 3.1 Experiments Settings We evaluate the parse"
P19-1562,D16-1180,0,0.0722769,"on 14 treebanks, we empirically show that global output-structured models can generally obtain better performance, especially on the metric of sentence-level Complete Match. However, probably because neural models already learn good global views of the inputs, the improvement brought by structured output modeling is modest. 1 Introduction In the past few years, dependency parsers, equipped with neural network models, have led to impressive empirical successes on parsing accuracy (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017; Ma et al., 2018). Among them, the deep-biaffine attentional parser (BiAF) (Dozat and Manning, 2017) has stood out for its simplicity and effectiveness. BiAF adopts a simple bi-directional LSTM neural architecture (Ma and Hovy, 2016; Kiperwasser and Goldberg, 2016) with the first-order graph parsing algorithm (McDonald et al., 2005a,b). Simple as it appears to be, BiAF has led to several recordbreaking performences in multiple treebanks and languages (Dozat et al., 2017). In their pioneering work, besides the neural architecture, Dozat and Manning (2017) adopt a simpl"
P19-1562,P16-1101,1,0.714912,"the improvement brought by structured output modeling is modest. 1 Introduction In the past few years, dependency parsers, equipped with neural network models, have led to impressive empirical successes on parsing accuracy (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017; Ma et al., 2018). Among them, the deep-biaffine attentional parser (BiAF) (Dozat and Manning, 2017) has stood out for its simplicity and effectiveness. BiAF adopts a simple bi-directional LSTM neural architecture (Ma and Hovy, 2016; Kiperwasser and Goldberg, 2016) with the first-order graph parsing algorithm (McDonald et al., 2005a,b). Simple as it appears to be, BiAF has led to several recordbreaking performences in multiple treebanks and languages (Dozat et al., 2017). In their pioneering work, besides the neural architecture, Dozat and Manning (2017) adopt a simple head-selection training object (Zhang et al., 2017) by regarding the original structured prediction task as an head-classification task in training. Although practically this simplification works well, there are still problems with it. Due to local normali"
P19-1562,I17-1007,1,0.943914,"ll, there are still problems with it. Due to local normalization in the training objective (see §2.2), no global tree-structured information can be back-propagated during training. This can lead to the discrepancy between training and testing, since during testing, the MST (Maximum Spanning Tree) algorithm (McDonald et al., 2005b) is used to ensure valid tree structures. This problem raises concerns about the structured output layer. Several previous neural graph parsers utilized structured techniques (Pei et al., 2015; Kiperwasser and Goldberg, 2016; Zhang et al., 2016; Wang and Chang, 2016; Ma and Hovy, 2017), but their neural architectures might not be competitive to the current state-of-the-art BiAF parsing model. In this paper, building upon the BiAF based neural architecture, we empirically investigate the effectiveness of utilizing classical structured prediction techniques of output modeling for graph-based neural dependency parsing. We empirically show that structured output modeling can obtain better performance, especially on the the sentence-level metrics. However, the improvements are modest, probably because neural models make the problem easier to solve locally. 2 Output Modeling In s"
P19-1562,P18-1130,1,0.912298,"l output-structured models can generally obtain better performance, especially on the metric of sentence-level Complete Match. However, probably because neural models already learn good global views of the inputs, the improvement brought by structured output modeling is modest. 1 Introduction In the past few years, dependency parsers, equipped with neural network models, have led to impressive empirical successes on parsing accuracy (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017; Ma et al., 2018). Among them, the deep-biaffine attentional parser (BiAF) (Dozat and Manning, 2017) has stood out for its simplicity and effectiveness. BiAF adopts a simple bi-directional LSTM neural architecture (Ma and Hovy, 2016; Kiperwasser and Goldberg, 2016) with the first-order graph parsing algorithm (McDonald et al., 2005a,b). Simple as it appears to be, BiAF has led to several recordbreaking performences in multiple treebanks and languages (Dozat et al., 2017). In their pioneering work, besides the neural architecture, Dozat and Manning (2017) adopt a simple head-selection training object (Zhang et"
P19-1562,C12-2077,1,0.580419,"ther, the score of the output tree structure (T ) is the combination of the scores of individual sub-trees in the tree. In the simplest case, the sub-trees are the individual dependency edges connecting each modifier and its head word ((m, h)). This is referred to as first-order factorization (Eisner, 1996; McDonald et al., 2005a), which is adopted in (Dozat and Manning, 2017) and the neural parsing models in this work. There are further extensions to higherorder factors, considering more complex sub-trees with multiple edges (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012). We leave the exploration of these higher-order graph models to future work. 2.2 Normalization After obtaining the individual scores of the substructures, we need to compute the score of the whole output structure. The main question is on what scale to normalize the output scores. For graph-based parsing, there can be mainly three options: Global, Local or Single, following different structured output constraints and corresponding to different loss functions. Global Global models directly normalize at the level of overall tree structures, whose scores are obtained by directly summing the raw"
P19-1562,P05-1012,0,0.802266,"ears, dependency parsers, equipped with neural network models, have led to impressive empirical successes on parsing accuracy (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017; Ma et al., 2018). Among them, the deep-biaffine attentional parser (BiAF) (Dozat and Manning, 2017) has stood out for its simplicity and effectiveness. BiAF adopts a simple bi-directional LSTM neural architecture (Ma and Hovy, 2016; Kiperwasser and Goldberg, 2016) with the first-order graph parsing algorithm (McDonald et al., 2005a,b). Simple as it appears to be, BiAF has led to several recordbreaking performences in multiple treebanks and languages (Dozat et al., 2017). In their pioneering work, besides the neural architecture, Dozat and Manning (2017) adopt a simple head-selection training object (Zhang et al., 2017) by regarding the original structured prediction task as an head-classification task in training. Although practically this simplification works well, there are still problems with it. Due to local normalization in the training objective (see §2.2), no global tree-structured information can be back-propag"
P19-1562,E06-1011,0,0.263218,"ased on the assumption that the sub-trees are independent to each other, the score of the output tree structure (T ) is the combination of the scores of individual sub-trees in the tree. In the simplest case, the sub-trees are the individual dependency edges connecting each modifier and its head word ((m, h)). This is referred to as first-order factorization (Eisner, 1996; McDonald et al., 2005a), which is adopted in (Dozat and Manning, 2017) and the neural parsing models in this work. There are further extensions to higherorder factors, considering more complex sub-trees with multiple edges (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Ma and Zhao, 2012). We leave the exploration of these higher-order graph models to future work. 2.2 Normalization After obtaining the individual scores of the substructures, we need to compute the score of the whole output structure. The main question is on what scale to normalize the output scores. For graph-based parsing, there can be mainly three options: Global, Local or Single, following different structured output constraints and corresponding to different loss functions. Global Global models directly normalize at the level of overall tree structu"
P19-1562,H05-1066,0,0.699637,"ears, dependency parsers, equipped with neural network models, have led to impressive empirical successes on parsing accuracy (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017; Ma et al., 2018). Among them, the deep-biaffine attentional parser (BiAF) (Dozat and Manning, 2017) has stood out for its simplicity and effectiveness. BiAF adopts a simple bi-directional LSTM neural architecture (Ma and Hovy, 2016; Kiperwasser and Goldberg, 2016) with the first-order graph parsing algorithm (McDonald et al., 2005a,b). Simple as it appears to be, BiAF has led to several recordbreaking performences in multiple treebanks and languages (Dozat et al., 2017). In their pioneering work, besides the neural architecture, Dozat and Manning (2017) adopt a simple head-selection training object (Zhang et al., 2017) by regarding the original structured prediction task as an head-classification task in training. Although practically this simplification works well, there are still problems with it. Due to local normalization in the training objective (see §2.2), no global tree-structured information can be back-propag"
P19-1562,W07-2216,0,0.0324092,"ihood loss (Prob), which requires actual normalization over the output space, and Max-Margin Hinge loss (Hinge), which only requires lossaugmented decoding in the same output space. Table 1 summarizes the methods (normalization and loss function) that we investigate in our experiments. For global models, we consider both Projective (Proj) and Non-Projective (NProj) constraints. Specific algorithms are required for probabilistic loss (a variation of Inside-Outside algorithm for projective (Paskin, 2001) and MatrixTree Theorem for non-projective parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007)) and hinge loss (Eisner’s algorithm for projective (Eisner, 1996) and ChuLiu-Edmonds’ algorithm for non-projective parsing (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b)). For Single and Local models, we only utilize probabilistic loss, since in preliminary experiments we found hinge loss performed worse. No special algorithms other than simple enumeration are needed for them in training. In testing, we adopt non-projective algorithms for the non-global models unless otherwise noted. 3 3.1 Experiments Settings We evaluate the parsers on 14 treebanks: English Penn Treebank (PTB), P"
P19-1562,P15-1031,0,0.401782,"s an head-classification task in training. Although practically this simplification works well, there are still problems with it. Due to local normalization in the training objective (see §2.2), no global tree-structured information can be back-propagated during training. This can lead to the discrepancy between training and testing, since during testing, the MST (Maximum Spanning Tree) algorithm (McDonald et al., 2005b) is used to ensure valid tree structures. This problem raises concerns about the structured output layer. Several previous neural graph parsers utilized structured techniques (Pei et al., 2015; Kiperwasser and Goldberg, 2016; Zhang et al., 2016; Wang and Chang, 2016; Ma and Hovy, 2017), but their neural architectures might not be competitive to the current state-of-the-art BiAF parsing model. In this paper, building upon the BiAF based neural architecture, we empirically investigate the effectiveness of utilizing classical structured prediction techniques of output modeling for graph-based neural dependency parsing. We empirically show that structured output modeling can obtain better performance, especially on the the sentence-level metrics. However, the improvements are modest, p"
P19-1562,N18-1202,0,0.0362599,"Missing"
P19-1562,D07-1014,0,0.235618,"babilistic MaximumLikelihood loss (Prob), which requires actual normalization over the output space, and Max-Margin Hinge loss (Hinge), which only requires lossaugmented decoding in the same output space. Table 1 summarizes the methods (normalization and loss function) that we investigate in our experiments. For global models, we consider both Projective (Proj) and Non-Projective (NProj) constraints. Specific algorithms are required for probabilistic loss (a variation of Inside-Outside algorithm for projective (Paskin, 2001) and MatrixTree Theorem for non-projective parsing (Koo et al., 2007; Smith and Smith, 2007; McDonald and Satta, 2007)) and hinge loss (Eisner’s algorithm for projective (Eisner, 1996) and ChuLiu-Edmonds’ algorithm for non-projective parsing (Chu and Liu, 1965; Edmonds, 1967; McDonald et al., 2005b)). For Single and Local models, we only utilize probabilistic loss, since in preliminary experiments we found hinge loss performed worse. No special algorithms other than simple enumeration are needed for them in training. In testing, we adopt non-projective algorithms for the non-global models unless otherwise noted. 3 3.1 Experiments Settings We evaluate the parsers on 14 treebanks: Eng"
P19-1562,W04-3201,0,0.125075,"d output constraints and corresponding to different loss functions. Global Global models directly normalize at the level of overall tree structures, whose scores are obtained by directly summing the raw scores of the sub-trees without any local normalization. This can be shown clearly if further taking a probabilistic CRF-like treatment, where a final normalization is performed over all possible trees: P exp (m,h)2T Score(m, h) P Scoreg (T ) = log P T 0 exp (m,h)2T 0 Score(m, h) Here, the normalization is carried out in the exact output space of all legal trees (T 0 ). MaxMargin (Hinge) loss (Taskar et al., 2004) adopts the similar idea, though there is no explicit normalization in its formulation. The output space can be further constrained by requiring the projectivity of the trees (Kubler et al., 2009). Several manual-feature-based (McDonald et al., 2005b; Koo and Collins, 2010) and neural-based dependency parsers (Pei et al., 2015; Kiperwasser and Goldberg, 2016; Zhang et al., 2016; Ma and Hovy, 2017) utilize global normalization. Local Local models, in contrast, ignore the global tree constraints and view the problem as a head-selection classification problem (Fonseca and Alu´ısio, 2015; Zhang et"
P19-1562,N03-1033,0,0.0625349,"Missing"
P19-1562,P16-1218,0,0.143857,"implification works well, there are still problems with it. Due to local normalization in the training objective (see §2.2), no global tree-structured information can be back-propagated during training. This can lead to the discrepancy between training and testing, since during testing, the MST (Maximum Spanning Tree) algorithm (McDonald et al., 2005b) is used to ensure valid tree structures. This problem raises concerns about the structured output layer. Several previous neural graph parsers utilized structured techniques (Pei et al., 2015; Kiperwasser and Goldberg, 2016; Zhang et al., 2016; Wang and Chang, 2016; Ma and Hovy, 2017), but their neural architectures might not be competitive to the current state-of-the-art BiAF parsing model. In this paper, building upon the BiAF based neural architecture, we empirically investigate the effectiveness of utilizing classical structured prediction techniques of output modeling for graph-based neural dependency parsing. We empirically show that structured output modeling can obtain better performance, especially on the the sentence-level metrics. However, the improvements are modest, probably because neural models make the problem easier to solve locally. 2"
P19-1562,P15-1032,0,0.022948,"ofthe-art graph-based neural dependency parser (Dozat and Manning, 2017). With evaluations on 14 treebanks, we empirically show that global output-structured models can generally obtain better performance, especially on the metric of sentence-level Complete Match. However, probably because neural models already learn good global views of the inputs, the improvement brought by structured output modeling is modest. 1 Introduction In the past few years, dependency parsers, equipped with neural network models, have led to impressive empirical successes on parsing accuracy (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Dozat and Manning, 2017; Ma et al., 2018). Among them, the deep-biaffine attentional parser (BiAF) (Dozat and Manning, 2017) has stood out for its simplicity and effectiveness. BiAF adopts a simple bi-directional LSTM neural architecture (Ma and Hovy, 2016; Kiperwasser and Goldberg, 2016) with the first-order graph parsing algorithm (McDonald et al., 2005a,b). Simple as it appears to be, BiAF has led to several recordbreaking performences in multiple treebanks and languages (Dozat et al., 2017). In t"
P19-1562,E17-1063,0,0.365584,"l., 2018). Among them, the deep-biaffine attentional parser (BiAF) (Dozat and Manning, 2017) has stood out for its simplicity and effectiveness. BiAF adopts a simple bi-directional LSTM neural architecture (Ma and Hovy, 2016; Kiperwasser and Goldberg, 2016) with the first-order graph parsing algorithm (McDonald et al., 2005a,b). Simple as it appears to be, BiAF has led to several recordbreaking performences in multiple treebanks and languages (Dozat et al., 2017). In their pioneering work, besides the neural architecture, Dozat and Manning (2017) adopt a simple head-selection training object (Zhang et al., 2017) by regarding the original structured prediction task as an head-classification task in training. Although practically this simplification works well, there are still problems with it. Due to local normalization in the training objective (see §2.2), no global tree-structured information can be back-propagated during training. This can lead to the discrepancy between training and testing, since during testing, the MST (Maximum Spanning Tree) algorithm (McDonald et al., 2005b) is used to ensure valid tree structures. This problem raises concerns about the structured output layer. Several previou"
P19-1562,D08-1059,0,0.0955202,"Missing"
P19-1562,P16-1131,1,0.887137,"Missing"
P19-1562,K18-2001,0,0.0596233,"Missing"
P88-1020,P85-1007,0,0.22217,"Missing"
P88-1020,1986.tc-1.15,0,0.0641828,"Missing"
P88-1020,P83-1012,0,0.0604873,"Missing"
P88-1020,E83-1027,0,\N,Missing
P88-1022,P83-1011,0,0.240591,"Missing"
P88-1022,P84-1044,0,0.0274499,"Missing"
penas-etal-2012-evaluating,P11-1142,1,\N,Missing
R13-1030,W10-1754,0,0.01556,"rics, METEOR and IMPACT are based on the f-measure, which combines precision and recall between the reference and candidate texts. The metrics’ simple f-measure (P. Koehn, 2010) obtains precision and recall using Eqs. (1)–(3): matching words length of candidate (1) recall = matching words length of ref erence (2) Then f-measure is calculated using Eq. (3): Introduction Various automatic evaluation metrics for machine translation have been proposed through the metrics task on the Workshop on statistical Machine Translation (WMT). One can identify three kinds of automatic evaluation metrics (C. Liu et al., 2010): the heavyweight linguistic approach, which corresponds to RTE (S. Pad´o et al., 2009) and ULC (J. Gim´enez and L. M´ arquez, 2007); the lightweight linguistic approach, which corresponds to METEOR precision = f-measure = 2 × precision × recall precision + recall (3) For example, in the reference “doctor cured a patient” and candidate “doctor treated a patient”, the precision and the recall are respectively 0.75 (= 34 ). Therefore, the f-measure is 0.75 (= 2×0.75×0.75 0.75+0.75 ), even though there is only one non-matching word. This is because the denominator is so small, since the sentences"
R13-1030,W09-0404,0,0.0382904,"Missing"
R13-1030,W07-0734,0,0.0998113,"Missing"
R13-1030,P08-1007,0,0.0438942,"Missing"
R13-1030,P02-1040,0,0.111222,"ort sentences even when only one non-matching word exists. In our metric, the weight of each mismatched word is kept small even in short sentences. We designate our metric as Automatic Evaluation Metric that is Independent of Sentence Length (AILE). Experimental results indicate that AILE has the highest correlation with human judgments among some leading metrics. 1 Eduard Hovy Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 USA hovy@cmu.edu (A. Lavie and A. Agarwal, 2007) and MaxSim (Y. Seng Chan and H. Tou Ng, 2008) and the non-linguistic approach, which includes BLEU (K. Papineni et al., 2002), TER (M. Snover et al., 2006), RIBES (H. Isozaki et al., 2010) and IMPACT (H. Echizen-ya and K. Araki, 2007)(H. Echizen-ya et al., 2012). In this paper, we speciﬁcally examine a metric that corresponds to the lightweight linguistic and nonlinguistic approaches because they are useful and are very easily built. Among these metrics, METEOR and IMPACT are based on the f-measure, which combines precision and recall between the reference and candidate texts. The metrics’ simple f-measure (P. Koehn, 2010) obtains precision and recall using Eqs. (1)–(3): matching words length of candidate (1) recall"
R13-1030,2006.amta-papers.25,0,0.0881626,"non-matching word exists. In our metric, the weight of each mismatched word is kept small even in short sentences. We designate our metric as Automatic Evaluation Metric that is Independent of Sentence Length (AILE). Experimental results indicate that AILE has the highest correlation with human judgments among some leading metrics. 1 Eduard Hovy Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 USA hovy@cmu.edu (A. Lavie and A. Agarwal, 2007) and MaxSim (Y. Seng Chan and H. Tou Ng, 2008) and the non-linguistic approach, which includes BLEU (K. Papineni et al., 2002), TER (M. Snover et al., 2006), RIBES (H. Isozaki et al., 2010) and IMPACT (H. Echizen-ya and K. Araki, 2007)(H. Echizen-ya et al., 2012). In this paper, we speciﬁcally examine a metric that corresponds to the lightweight linguistic and nonlinguistic approaches because they are useful and are very easily built. Among these metrics, METEOR and IMPACT are based on the f-measure, which combines precision and recall between the reference and candidate texts. The metrics’ simple f-measure (P. Koehn, 2010) obtains precision and recall using Eqs. (1)–(3): matching words length of candidate (1) recall = matching words length of re"
R13-1030,D10-1092,0,0.0207844,"r metric, the weight of each mismatched word is kept small even in short sentences. We designate our metric as Automatic Evaluation Metric that is Independent of Sentence Length (AILE). Experimental results indicate that AILE has the highest correlation with human judgments among some leading metrics. 1 Eduard Hovy Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 USA hovy@cmu.edu (A. Lavie and A. Agarwal, 2007) and MaxSim (Y. Seng Chan and H. Tou Ng, 2008) and the non-linguistic approach, which includes BLEU (K. Papineni et al., 2002), TER (M. Snover et al., 2006), RIBES (H. Isozaki et al., 2010) and IMPACT (H. Echizen-ya and K. Araki, 2007)(H. Echizen-ya et al., 2012). In this paper, we speciﬁcally examine a metric that corresponds to the lightweight linguistic and nonlinguistic approaches because they are useful and are very easily built. Among these metrics, METEOR and IMPACT are based on the f-measure, which combines precision and recall between the reference and candidate texts. The metrics’ simple f-measure (P. Koehn, 2010) obtains precision and recall using Eqs. (1)–(3): matching words length of candidate (1) recall = matching words length of ref erence (2) Then f-measure is ca"
R13-1030,2009.mtsummit-wpt.2,1,0.789174,"Missing"
R13-1030,2007.mtsummit-papers.21,1,0.818296,"Missing"
R13-1030,W12-6102,1,0.868937,"Missing"
R13-1030,J10-4005,0,0.0257559,"g Chan and H. Tou Ng, 2008) and the non-linguistic approach, which includes BLEU (K. Papineni et al., 2002), TER (M. Snover et al., 2006), RIBES (H. Isozaki et al., 2010) and IMPACT (H. Echizen-ya and K. Araki, 2007)(H. Echizen-ya et al., 2012). In this paper, we speciﬁcally examine a metric that corresponds to the lightweight linguistic and nonlinguistic approaches because they are useful and are very easily built. Among these metrics, METEOR and IMPACT are based on the f-measure, which combines precision and recall between the reference and candidate texts. The metrics’ simple f-measure (P. Koehn, 2010) obtains precision and recall using Eqs. (1)–(3): matching words length of candidate (1) recall = matching words length of ref erence (2) Then f-measure is calculated using Eq. (3): Introduction Various automatic evaluation metrics for machine translation have been proposed through the metrics task on the Workshop on statistical Machine Translation (WMT). One can identify three kinds of automatic evaluation metrics (C. Liu et al., 2010): the heavyweight linguistic approach, which corresponds to RTE (S. Pad´o et al., 2009) and ULC (J. Gim´enez and L. M´ arquez, 2007); the lightweight linguistic"
R13-1030,W10-1703,0,\N,Missing
R13-1030,W11-2103,0,\N,Missing
R13-1030,I08-1042,0,\N,Missing
R13-1030,W11-2100,0,\N,Missing
rambow-etal-2006-parallel,W04-2709,1,\N,Missing
rambow-etal-2006-parallel,passonneau-etal-2006-inter,1,\N,Missing
rambow-etal-2006-parallel,J93-2004,0,\N,Missing
rambow-etal-2006-parallel,W00-0204,0,\N,Missing
rambow-etal-2006-parallel,W02-1503,0,\N,Missing
rambow-etal-2006-parallel,rambow-etal-2002-dependency,1,\N,Missing
recasens-etal-2010-typology,poesio-artstein-2008-anaphoric,0,\N,Missing
recasens-etal-2010-typology,N09-1065,0,\N,Missing
recasens-etal-2010-typology,W09-3017,0,\N,Missing
recasens-etal-2010-typology,W05-0311,0,\N,Missing
recasens-etal-2010-typology,P08-2012,0,\N,Missing
recasens-etal-2010-typology,doddington-etal-2004-automatic,0,\N,Missing
recasens-etal-2010-typology,C69-7001,0,\N,Missing
recasens-etal-2010-typology,C69-6902,0,\N,Missing
reeder-etal-2004-interlingual,W04-2709,1,\N,Missing
reeder-etal-2004-interlingual,W03-1601,0,\N,Missing
reeder-etal-2004-interlingual,W03-1604,0,\N,Missing
reeder-etal-2004-interlingual,P98-1013,0,\N,Missing
reeder-etal-2004-interlingual,C98-1013,0,\N,Missing
reeder-etal-2004-interlingual,1991.mtsummit-papers.9,1,\N,Missing
reeder-etal-2004-interlingual,J96-2004,0,\N,Missing
reeder-etal-2004-interlingual,A97-1011,0,\N,Missing
S10-1049,W04-0404,0,0.0795937,"e understanding applications and is an area of increasing interest. In this paper, we present the system we used to participate in the S EM E VAL 2010 Task 8 Multi-Way Classification of Semantic Relations between Pairs of Nominals. Our system, based upon a Maximum Entropy classifier trained using a large number of boolean features, received the third highest score. 1 Introduction 3 Semantic interpretation of the relations between nominals in text is an area of growing interest within natural language processing (NLP). It has potential uses for a variety of tasks including machine translation (Baldwin and Tanaka, 2004) and question answering (Ahn et al., 2005). The related and more narrowly-focused problem of automatic interpretation of noun compounds is the focus of another S EM E VAL task (Butnariu et al., 2009). In this paper, we discuss the overall setup of S EM E VAL 2010 Task 8 (Hendrickx et al., 2010), present the system we used to participate, and discuss our system’s performance. Our system, which consists of a Maximum Entropy classifier trained using a large variety of boolean features, received the third highest official score of all the entries. 2 Task Overview The task is, given a pair of nomin"
S10-1049,J96-1002,0,0.0105048,"2010, instead of providing a binary out222 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 222–225, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics 3.2 matching, WordNet (Fellbaum, 1998), and Roget’s Thesaurus. Data The training and testing datasets consist of 8000 and 2717 examples respectively. Each example consists of a single sentence with two of its nominals marked as being the nominals of interest. The training data also provides the correct relation for each example. Orthographic Features We use a Maximum Entropy (Berger et al., 1996) classifier trained using a large number of boolean features. Maximum Entropy classifiers have proven effective for a variety of NLP problems including word sense disambiguation (Tratz et al., 2007; Ye and Baldwin, 2007). We use the implementation provided in the MALLET machine learning toolkit (McCallum, 2002). We used the default Gaussian prior parameter value of 1.0. • Capitalization indicator • The {first, last} {two, three} letters of each word • Indicator if the first letter of the word is a/A. • Indicator for the overall form of the word (e.g. jump -> a, Mr. -> Aa., SemEval2 -> AaAa0) •"
S10-1049,E09-1071,0,0.0218981,"Missing"
S10-1049,S07-1057,1,0.831234,"utational Linguistics 3.2 matching, WordNet (Fellbaum, 1998), and Roget’s Thesaurus. Data The training and testing datasets consist of 8000 and 2717 examples respectively. Each example consists of a single sentence with two of its nominals marked as being the nominals of interest. The training data also provides the correct relation for each example. Orthographic Features We use a Maximum Entropy (Berger et al., 1996) classifier trained using a large number of boolean features. Maximum Entropy classifiers have proven effective for a variety of NLP problems including word sense disambiguation (Tratz et al., 2007; Ye and Baldwin, 2007). We use the implementation provided in the MALLET machine learning toolkit (McCallum, 2002). We used the default Gaussian prior parameter value of 1.0. • Capitalization indicator • The {first, last} {two, three} letters of each word • Indicator if the first letter of the word is a/A. • Indicator for the overall form of the word (e.g. jump -> a, Mr. -> Aa., SemEval2 -> AaAa0) • Indicators for the suffix types (e.g., deadjectival, de-nominal [non]agentive, deverbal [non]agentive) • Indicators for a wide variety of affixes including those related to degree, number, order,"
S10-1049,C08-1011,0,0.0240812,"Missing"
S10-1049,W09-2416,0,0.0116599,"y Information Sciences Institute University of Southern California Marina del Rey, CA 90292 {stratz,hovy}@isi.edu Abstract put for a single class, participants were required to perform multi-way classification, that is, select the most appropriate relation from a set of 10 relations including the OTHER relation. The selection of a semantic relation for a pair of nominals within a sentence is somewhat similar to the task of noun compound interpretation, which is a more restricted problem focused only upon the nouns within noun compounds. Some of the recent work on this problem includes that of Butnariu et al. (2009), Girju (2007), Girju et al. (2005), Kim and Baldwin (2005), Nakov (2008), Nastase et al. (2006), Turney (2006), and Ó Séaghdha and Copestake (2009). The automatic interpretation of semantic relations between nominals is an important subproblem within natural language understanding applications and is an area of increasing interest. In this paper, we present the system we used to participate in the S EM E VAL 2010 Task 8 Multi-Way Classification of Semantic Relations between Pairs of Nominals. Our system, based upon a Maximum Entropy classifier trained using a large number of boolean features,"
S10-1049,J06-3003,0,0.0378299,"ct put for a single class, participants were required to perform multi-way classification, that is, select the most appropriate relation from a set of 10 relations including the OTHER relation. The selection of a semantic relation for a pair of nominals within a sentence is somewhat similar to the task of noun compound interpretation, which is a more restricted problem focused only upon the nouns within noun compounds. Some of the recent work on this problem includes that of Butnariu et al. (2009), Girju (2007), Girju et al. (2005), Kim and Baldwin (2005), Nakov (2008), Nastase et al. (2006), Turney (2006), and Ó Séaghdha and Copestake (2009). The automatic interpretation of semantic relations between nominals is an important subproblem within natural language understanding applications and is an area of increasing interest. In this paper, we present the system we used to participate in the S EM E VAL 2010 Task 8 Multi-Way Classification of Semantic Relations between Pairs of Nominals. Our system, based upon a Maximum Entropy classifier trained using a large number of boolean features, received the third highest score. 1 Introduction 3 Semantic interpretation of the relations between nominals i"
S10-1049,S07-1051,0,0.147543,"s 3.2 matching, WordNet (Fellbaum, 1998), and Roget’s Thesaurus. Data The training and testing datasets consist of 8000 and 2717 examples respectively. Each example consists of a single sentence with two of its nominals marked as being the nominals of interest. The training data also provides the correct relation for each example. Orthographic Features We use a Maximum Entropy (Berger et al., 1996) classifier trained using a large number of boolean features. Maximum Entropy classifiers have proven effective for a variety of NLP problems including word sense disambiguation (Tratz et al., 2007; Ye and Baldwin, 2007). We use the implementation provided in the MALLET machine learning toolkit (McCallum, 2002). We used the default Gaussian prior parameter value of 1.0. • Capitalization indicator • The {first, last} {two, three} letters of each word • Indicator if the first letter of the word is a/A. • Indicator for the overall form of the word (e.g. jump -> a, Mr. -> Aa., SemEval2 -> AaAa0) • Indicators for the suffix types (e.g., deadjectival, de-nominal [non]agentive, deverbal [non]agentive) • Indicators for a wide variety of affixes including those related to degree, number, order, etc. (e.g., ultra-, pol"
S10-1049,I05-1082,0,\N,Missing
S10-1049,S07-1003,0,\N,Missing
S10-1049,P07-1072,0,\N,Missing
S17-1025,P09-1004,0,0.0168773,"our evaluations. In this work, we only focus on verbal predicates. Our training data gives us a vocabulary of 4449 predicates, after pruning verbs that occur fewer than 5 times. Then, from the training data we extract all predicate-argument pairs using gold standard argument annotations, for the sake of simplicity. Note that previous unsupervised frame induction work also uses gold argument mentions (Lang and Lapata, 2011a; Titov and Klementiev, 2012b). Our method, however, does not depend on this, or any other annotation, and we could as easily use the output from an automated system such as Abend et al. (2009) instead. In this manner, we obtain a total of approximately 3.35 million predicate-argument word pairs on which to train. Using this data we train a total of 4 distinct models: a base model and a relational variant (see Section 3.4), both of which are trained with two different IBP hyperparameters of α = 0.35 and α = 0.7. The hyperparameter controls the avidity of the model for latent slots (a higher α implies a greater number of induced slots). Experiments and Evaluation In what follows, we detail experimental results on two quantitative evaluation tasks: at the local and global levels of pr"
S17-1025,N13-1104,0,0.0631864,"our research is the first to attempt using selectional preference as a basis for directly inducing semantic frames. At the global level, frame induction subsumes selectional preference by attempting to group arguments of predicates into coherent and cohesive clusters. While work in this area has included diverse approaches, such as leveraging examplebased representations (Kawahara et al., 2014) and cross-lingual resources (Fung and Chen, 2004; Titov and Klementiev, 2012b), most attempts have focussed on two broad categories. These are latent variable driven models (Grenager and Manning, 2006; Cheung et al., 2013) and similarity driven clustering models (Lang and Lapata, 2011a,b), Our work includes elements of both major categories, since we use latent slots to represent arguments, but an Indian Buffet process induces these latent slots in the first place. The work of Titov and Klementiev (2012a) and Woodsend and Lapata (2015) are particularly relevant to our research. The former use another non-parametric Bayesian model (a Chinese Restaurant process) in their work, while the latter embed predicateargument structures before performing clustering. Related Work The work in this paper relates to research"
S17-1025,C00-1028,0,0.0642081,"ls are able to outperform baselines on both the local and global level of frame knowledge. At the local level we score higher than a standard predictive embedding model on selectional preference, while at the global level we outperform a syntactic baseline on lexicon overlap with PropBank. Finally, our analysis on the induced latent slots yields insight into some interesting generalities that we are able to capture from unlabeled predicate-argument pairs. 2 Previous work has sought to acquire these preferences using various means, including ontological resources such as WordNet (Resnik, 1997; Ciaramita and Johnson, 2000), latent variable models (Rooth et al., 1999; Séaghdha, 2010; Ritter et al., 2010) and distributional similarity metrics (Erk, 2007). Most closely related to our contribution is the work by Van de Cruys (2014) who use a predictive neural network to capture predicateargument associations. To the best of our knowledge, our research is the first to attempt using selectional preference as a basis for directly inducing semantic frames. At the global level, frame induction subsumes selectional preference by attempting to group arguments of predicates into coherent and cohesive clusters. While work i"
S17-1025,P07-1028,0,0.0188342,"ve embedding model on selectional preference, while at the global level we outperform a syntactic baseline on lexicon overlap with PropBank. Finally, our analysis on the induced latent slots yields insight into some interesting generalities that we are able to capture from unlabeled predicate-argument pairs. 2 Previous work has sought to acquire these preferences using various means, including ontological resources such as WordNet (Resnik, 1997; Ciaramita and Johnson, 2000), latent variable models (Rooth et al., 1999; Séaghdha, 2010; Ritter et al., 2010) and distributional similarity metrics (Erk, 2007). Most closely related to our contribution is the work by Van de Cruys (2014) who use a predictive neural network to capture predicateargument associations. To the best of our knowledge, our research is the first to attempt using selectional preference as a basis for directly inducing semantic frames. At the global level, frame induction subsumes selectional preference by attempting to group arguments of predicates into coherent and cohesive clusters. While work in this area has included diverse approaches, such as leveraging examplebased representations (Kawahara et al., 2014) and cross-lingu"
S17-1025,C04-1134,0,0.0599971,"related to our contribution is the work by Van de Cruys (2014) who use a predictive neural network to capture predicateargument associations. To the best of our knowledge, our research is the first to attempt using selectional preference as a basis for directly inducing semantic frames. At the global level, frame induction subsumes selectional preference by attempting to group arguments of predicates into coherent and cohesive clusters. While work in this area has included diverse approaches, such as leveraging examplebased representations (Kawahara et al., 2014) and cross-lingual resources (Fung and Chen, 2004; Titov and Klementiev, 2012b), most attempts have focussed on two broad categories. These are latent variable driven models (Grenager and Manning, 2006; Cheung et al., 2013) and similarity driven clustering models (Lang and Lapata, 2011a,b), Our work includes elements of both major categories, since we use latent slots to represent arguments, but an Indian Buffet process induces these latent slots in the first place. The work of Titov and Klementiev (2012a) and Woodsend and Lapata (2015) are particularly relevant to our research. The former use another non-parametric Bayesian model (a Chinese"
S17-1025,J02-3001,0,0.702433,"PropBank specifies frames in the following manner: • eat → [agent]0 , [patient]1 • give → [agent]0 , [theme]1 , [recipient]2 These frames provide semantic information such as the fact that “eat” is transitive, while “give” is 209 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 209–219, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics ture the semantic preference of predicates for certain arguments in local contexts. These preferences are useful for many tasks, including unsupervised semantic role labelling (Gildea and Jurafsky, 2002) among others. Their lack of portable features or model parameters unfortunately means they cannot be used to solve other applications or problems that require lexicon-level information – such as information extraction or machine translation. Another limitation is that they always depend on high-level linguistic annotation, such as syntactic dependencies, which may not exist in resource-poor settings. Thus, in this paper we propose to combine the two approaches to induce a frame semantic lexicon in a minimally supervised fashion with nothing more than unlabeled predicate-argument word pairs. A"
S17-1025,W06-1601,0,0.0369288,"the best of our knowledge, our research is the first to attempt using selectional preference as a basis for directly inducing semantic frames. At the global level, frame induction subsumes selectional preference by attempting to group arguments of predicates into coherent and cohesive clusters. While work in this area has included diverse approaches, such as leveraging examplebased representations (Kawahara et al., 2014) and cross-lingual resources (Fung and Chen, 2004; Titov and Klementiev, 2012b), most attempts have focussed on two broad categories. These are latent variable driven models (Grenager and Manning, 2006; Cheung et al., 2013) and similarity driven clustering models (Lang and Lapata, 2011a,b), Our work includes elements of both major categories, since we use latent slots to represent arguments, but an Indian Buffet process induces these latent slots in the first place. The work of Titov and Klementiev (2012a) and Woodsend and Lapata (2015) are particularly relevant to our research. The former use another non-parametric Bayesian model (a Chinese Restaurant process) in their work, while the latter embed predicateargument structures before performing clustering. Related Work The work in this pape"
S17-1025,J05-1004,0,0.841184,"ment word pairs. We hypothesize that aggregating such pair selectional preferences across training leads us to a global understanding that captures predicate-argument frame structure. Our approach revolves around a novel integration between a predictive embedding model and an Indian Buffet Process posterior regularizer. We show, through our experimental evaluation, that we outperform baselines on two tasks and can learn an embedded frame lexicon that is able to capture some interesting generalities in relation to hand-crafted semantic frames. 1 Introduction Semantic lexicons such as PropBank (Palmer et al., 2005) and FrameNet (Baker et al., 1998) contain information about predicate-argument frame structure. These frames capture knowledge about the affinity of predicates for certain types of arguments, their number and their semantic nature, regardless of syntactic realization. For example, PropBank specifies frames in the following manner: • eat → [agent]0 , [patient]1 • give → [agent]0 , [theme]1 , [recipient]2 These frames provide semantic information such as the fact that “eat” is transitive, while “give” is 209 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 20"
S17-1025,W97-0209,0,0.743346,"prove diverse tasks such as information extraction (Surdeanu et al., 2003), semantic parsing (Das et al., 2010) and question answering (Shen and Lapata, 2007), among others. However, building these frame lexicons is very expensive and time consuming. Thus, it remains difficult to port applications from resource-rich languages or domains to data impoverished ones. The NLP community has tackled this issue along two different lines of unsupervised work. At the local token level, researchers have attempted to model frame structure by the selectional preference of predicates for certain arguments (Resnik, 1997; Séaghdha, 2010). For example, on this problem a good model might assign a high probability to the word “pasta” occurring as an argument of the word “eat”. Contrastingly, at the global type level, work has focussed on inducing frames by clustering predicates and arguments in a joint framework (Lang and Lapata, 2011a; Titov and Klementiev, 2012b). In this case, one is interested in associating predicates such as “eat”, “consume”, “devour”, with a joint clustering of arguments such as “pasta”, “chicken”, “burger”. While these methods have been useful for several problems, they also have shortco"
S17-1025,1993.eamt-1.1,0,0.604439,"Missing"
S17-1025,P10-1044,0,0.0644538,"At the local level we score higher than a standard predictive embedding model on selectional preference, while at the global level we outperform a syntactic baseline on lexicon overlap with PropBank. Finally, our analysis on the induced latent slots yields insight into some interesting generalities that we are able to capture from unlabeled predicate-argument pairs. 2 Previous work has sought to acquire these preferences using various means, including ontological resources such as WordNet (Resnik, 1997; Ciaramita and Johnson, 2000), latent variable models (Rooth et al., 1999; Séaghdha, 2010; Ritter et al., 2010) and distributional similarity metrics (Erk, 2007). Most closely related to our contribution is the work by Van de Cruys (2014) who use a predictive neural network to capture predicateargument associations. To the best of our knowledge, our research is the first to attempt using selectional preference as a basis for directly inducing semantic frames. At the global level, frame induction subsumes selectional preference by attempting to group arguments of predicates into coherent and cohesive clusters. While work in this area has included diverse approaches, such as leveraging examplebased repre"
S17-1025,N06-2015,1,0.481758,"tion immediately preceding the argument phrase (or None if there isn’t one). Thus, for example, we have relational indicators such as “L-on”, “R-before”, “L-because”, “R-None”, etc. We obtain a total of 146 such relations. Note, that in keeping with the goals of this work, these relation indicators still require no annotation (prepositions are closed-class words than can be enumerated). 4 For training our models, we use a combination of the training data released for the CoNLL 2008 shared task (Surdeanu et al., 2008) and the extended PropBank release which covers annotations of the Ontonotes (Hovy et al., 2006) and English Web Treebank (Bies et al., 2012) corpora. We reserve the test portion of the CoNLL 2008 shared task data for one of our evaluations. In this work, we only focus on verbal predicates. Our training data gives us a vocabulary of 4449 predicates, after pruning verbs that occur fewer than 5 times. Then, from the training data we extract all predicate-argument pairs using gold standard argument annotations, for the sake of simplicity. Note that previous unsupervised frame induction work also uses gold argument mentions (Lang and Lapata, 2011a; Titov and Klementiev, 2012b). Our method, h"
S17-1025,P99-1014,0,0.546665,"and global level of frame knowledge. At the local level we score higher than a standard predictive embedding model on selectional preference, while at the global level we outperform a syntactic baseline on lexicon overlap with PropBank. Finally, our analysis on the induced latent slots yields insight into some interesting generalities that we are able to capture from unlabeled predicate-argument pairs. 2 Previous work has sought to acquire these preferences using various means, including ontological resources such as WordNet (Resnik, 1997; Ciaramita and Johnson, 2000), latent variable models (Rooth et al., 1999; Séaghdha, 2010; Ritter et al., 2010) and distributional similarity metrics (Erk, 2007). Most closely related to our contribution is the work by Van de Cruys (2014) who use a predictive neural network to capture predicateargument associations. To the best of our knowledge, our research is the first to attempt using selectional preference as a basis for directly inducing semantic frames. At the global level, frame induction subsumes selectional preference by attempting to group arguments of predicates into coherent and cohesive clusters. While work in this area has included diverse approaches,"
S17-1025,P10-1045,0,0.149522,"tasks such as information extraction (Surdeanu et al., 2003), semantic parsing (Das et al., 2010) and question answering (Shen and Lapata, 2007), among others. However, building these frame lexicons is very expensive and time consuming. Thus, it remains difficult to port applications from resource-rich languages or domains to data impoverished ones. The NLP community has tackled this issue along two different lines of unsupervised work. At the local token level, researchers have attempted to model frame structure by the selectional preference of predicates for certain arguments (Resnik, 1997; Séaghdha, 2010). For example, on this problem a good model might assign a high probability to the word “pasta” occurring as an argument of the word “eat”. Contrastingly, at the global type level, work has focussed on inducing frames by clustering predicates and arguments in a joint framework (Lang and Lapata, 2011a; Titov and Klementiev, 2012b). In this case, one is interested in associating predicates such as “eat”, “consume”, “devour”, with a joint clustering of arguments such as “pasta”, “chicken”, “burger”. While these methods have been useful for several problems, they also have shortcomings. Selectiona"
S17-1025,N15-1070,1,0.836646,"(K → ∞) of a beta-Bernoulli model. πk ∼ Beta(α/K, 1) zik ∼ Bernoulli(πk ) 1 In this work, we assume argument chunks are broken down into individual words, – to increase training data size – but the model remains agnostic to this decision. (3) Given a suitable likelihood function and some 211 In practise, the likelihood component is optimized using negative sampling with EM for the latent slots. In particular we use hard EM, to select a single slot before taking gradient steps with respect to the model parameters. This was shown to work well for Skip-gram style models with latent variables by Jauhar et al. (2015). In the E-Step we find the best latent slot for a particular predicate-argument pair: data, inference in an IBP computes a posterior that yields an optimal finite binary matrix with respect to regularities in the data. Setting the data, in our case, to be the embedding matrix of predicates V , this gives us precisely what we are seeking. It allows us to find regularities in the embeddings, while factorizing them according to these consistencies. The model also automatically optimizes the number of and relationship between latent slots, rather than setting these a priori. Other desiderata are"
S17-1025,D07-1002,0,0.312051,"vy Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA hovy@cs.cmu.edu Sujay Kumar Jauhar Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA sjauhar@cs.cmu.edu Abstract ditransitive, or that the beneficiary of one action is a “patient”, while the other is a “recipient”. This structural knowledge is crucial for a number of NLP applications. Information about frames has been successfully used to drive and improve diverse tasks such as information extraction (Surdeanu et al., 2003), semantic parsing (Das et al., 2010) and question answering (Shen and Lapata, 2007), among others. However, building these frame lexicons is very expensive and time consuming. Thus, it remains difficult to port applications from resource-rich languages or domains to data impoverished ones. The NLP community has tackled this issue along two different lines of unsupervised work. At the local token level, researchers have attempted to model frame structure by the selectional preference of predicates for certain arguments (Resnik, 1997; Séaghdha, 2010). For example, on this problem a good model might assign a high probability to the word “pasta” occurring as an argument of the w"
S17-1025,E14-1007,0,0.0141928,"utional similarity metrics (Erk, 2007). Most closely related to our contribution is the work by Van de Cruys (2014) who use a predictive neural network to capture predicateargument associations. To the best of our knowledge, our research is the first to attempt using selectional preference as a basis for directly inducing semantic frames. At the global level, frame induction subsumes selectional preference by attempting to group arguments of predicates into coherent and cohesive clusters. While work in this area has included diverse approaches, such as leveraging examplebased representations (Kawahara et al., 2014) and cross-lingual resources (Fung and Chen, 2004; Titov and Klementiev, 2012b), most attempts have focussed on two broad categories. These are latent variable driven models (Grenager and Manning, 2006; Cheung et al., 2013) and similarity driven clustering models (Lang and Lapata, 2011a,b), Our work includes elements of both major categories, since we use latent slots to represent arguments, but an Indian Buffet process induces these latent slots in the first place. The work of Titov and Klementiev (2012a) and Woodsend and Lapata (2015) are particularly relevant to our research. The former use"
S17-1025,P03-1002,0,0.108037,"bedded Semantic Lexicon Induction with Joint Global and Local Optimization Eduard Hovy Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA hovy@cs.cmu.edu Sujay Kumar Jauhar Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, USA sjauhar@cs.cmu.edu Abstract ditransitive, or that the beneficiary of one action is a “patient”, while the other is a “recipient”. This structural knowledge is crucial for a number of NLP applications. Information about frames has been successfully used to drive and improve diverse tasks such as information extraction (Surdeanu et al., 2003), semantic parsing (Das et al., 2010) and question answering (Shen and Lapata, 2007), among others. However, building these frame lexicons is very expensive and time consuming. Thus, it remains difficult to port applications from resource-rich languages or domains to data impoverished ones. The NLP community has tackled this issue along two different lines of unsupervised work. At the local token level, researchers have attempted to model frame structure by the selectional preference of predicates for certain arguments (Resnik, 1997; Séaghdha, 2010). For example, on this problem a good model m"
S17-1025,P11-1112,0,0.281745,"urce-rich languages or domains to data impoverished ones. The NLP community has tackled this issue along two different lines of unsupervised work. At the local token level, researchers have attempted to model frame structure by the selectional preference of predicates for certain arguments (Resnik, 1997; Séaghdha, 2010). For example, on this problem a good model might assign a high probability to the word “pasta” occurring as an argument of the word “eat”. Contrastingly, at the global type level, work has focussed on inducing frames by clustering predicates and arguments in a joint framework (Lang and Lapata, 2011a; Titov and Klementiev, 2012b). In this case, one is interested in associating predicates such as “eat”, “consume”, “devour”, with a joint clustering of arguments such as “pasta”, “chicken”, “burger”. While these methods have been useful for several problems, they also have shortcomings. Selectional preference modelling only captures local predicate-argument affinities, but does not aggregate these associations to arrive at a structural understanding of frames. Meanwhile, frame induction performs clustering at a global level. But most approaches tend to be algorithmic methods (or some extensi"
S17-1025,W08-2121,0,0.0446627,"nation of the directionality of the argument with respect to the predicate (L or R), and the preposition immediately preceding the argument phrase (or None if there isn’t one). Thus, for example, we have relational indicators such as “L-on”, “R-before”, “L-because”, “R-None”, etc. We obtain a total of 146 such relations. Note, that in keeping with the goals of this work, these relation indicators still require no annotation (prepositions are closed-class words than can be enumerated). 4 For training our models, we use a combination of the training data released for the CoNLL 2008 shared task (Surdeanu et al., 2008) and the extended PropBank release which covers annotations of the Ontonotes (Hovy et al., 2006) and English Web Treebank (Bies et al., 2012) corpora. We reserve the test portion of the CoNLL 2008 shared task data for one of our evaluations. In this work, we only focus on verbal predicates. Our training data gives us a vocabulary of 4449 predicates, after pruning verbs that occur fewer than 5 times. Then, from the training data we extract all predicate-argument pairs using gold standard argument annotations, for the sake of simplicity. Note that previous unsupervised frame induction work also"
S17-1025,D11-1122,0,0.23029,"urce-rich languages or domains to data impoverished ones. The NLP community has tackled this issue along two different lines of unsupervised work. At the local token level, researchers have attempted to model frame structure by the selectional preference of predicates for certain arguments (Resnik, 1997; Séaghdha, 2010). For example, on this problem a good model might assign a high probability to the word “pasta” occurring as an argument of the word “eat”. Contrastingly, at the global type level, work has focussed on inducing frames by clustering predicates and arguments in a joint framework (Lang and Lapata, 2011a; Titov and Klementiev, 2012b). In this case, one is interested in associating predicates such as “eat”, “consume”, “devour”, with a joint clustering of arguments such as “pasta”, “chicken”, “burger”. While these methods have been useful for several problems, they also have shortcomings. Selectional preference modelling only captures local predicate-argument affinities, but does not aggregate these associations to arrive at a structural understanding of frames. Meanwhile, frame induction performs clustering at a global level. But most approaches tend to be algorithmic methods (or some extensi"
S17-1025,E12-1003,0,0.724689,"omains to data impoverished ones. The NLP community has tackled this issue along two different lines of unsupervised work. At the local token level, researchers have attempted to model frame structure by the selectional preference of predicates for certain arguments (Resnik, 1997; Séaghdha, 2010). For example, on this problem a good model might assign a high probability to the word “pasta” occurring as an argument of the word “eat”. Contrastingly, at the global type level, work has focussed on inducing frames by clustering predicates and arguments in a joint framework (Lang and Lapata, 2011a; Titov and Klementiev, 2012b). In this case, one is interested in associating predicates such as “eat”, “consume”, “devour”, with a joint clustering of arguments such as “pasta”, “chicken”, “burger”. While these methods have been useful for several problems, they also have shortcomings. Selectional preference modelling only captures local predicate-argument affinities, but does not aggregate these associations to arrive at a structural understanding of frames. Meanwhile, frame induction performs clustering at a global level. But most approaches tend to be algorithmic methods (or some extension thereof) that focus on sem"
S17-1025,J14-3006,0,0.0591215,"ced frame embeddings without any reliance on annotated data. 210 3 Joint Local and Global Frame Lexicon Induction In this section we present our approach to induce a frame lexicon with latent slots. Following prior work on frame induction (Lang and Lapata, 2011a; Titov and Klementiev, 2012a), the procedural pipeline can be split into two distinct phases: argument identification and argument clustering. As with previous work, we focus on the latter stage, and assume that we have unlabeled predicate-argument structure pairs – given to us from gold standard annotation or through heuristic means (Lang and Lapata, 2014). We begin with preliminary notation. Given a vocabulary of predicate types P = {p1 , ..., pn } and contextual argument types A = {a1 , ..., am }. Let C = {(p1 , a1 ), ..., (pN , aN )} be a corpus of predicate-argument word token pairs1 . Given this corpus, we will attempt to learn an optimal set of model parameters θ that maximizes a regularized likelihood over the corpus. The model parameters include V = {vi |∀pi ∈ P } an n × d embedding matrix for the predicates and U = {ui |∀ai ∈ A} an m × d embedding matrix for the arguments. Additionally, assuming K latent frame slots we define Z = {zik"
S17-1025,P12-1068,0,0.188214,"omains to data impoverished ones. The NLP community has tackled this issue along two different lines of unsupervised work. At the local token level, researchers have attempted to model frame structure by the selectional preference of predicates for certain arguments (Resnik, 1997; Séaghdha, 2010). For example, on this problem a good model might assign a high probability to the word “pasta” occurring as an argument of the word “eat”. Contrastingly, at the global type level, work has focussed on inducing frames by clustering predicates and arguments in a joint framework (Lang and Lapata, 2011a; Titov and Klementiev, 2012b). In this case, one is interested in associating predicates such as “eat”, “consume”, “devour”, with a joint clustering of arguments such as “pasta”, “chicken”, “burger”. While these methods have been useful for several problems, they also have shortcomings. Selectional preference modelling only captures local predicate-argument affinities, but does not aggregate these associations to arrive at a structural understanding of frames. Meanwhile, frame induction performs clustering at a global level. But most approaches tend to be algorithmic methods (or some extension thereof) that focus on sem"
S17-1025,D14-1004,0,0.0286994,"Missing"
S17-1025,D15-1295,0,0.0175825,"approaches, such as leveraging examplebased representations (Kawahara et al., 2014) and cross-lingual resources (Fung and Chen, 2004; Titov and Klementiev, 2012b), most attempts have focussed on two broad categories. These are latent variable driven models (Grenager and Manning, 2006; Cheung et al., 2013) and similarity driven clustering models (Lang and Lapata, 2011a,b), Our work includes elements of both major categories, since we use latent slots to represent arguments, but an Indian Buffet process induces these latent slots in the first place. The work of Titov and Klementiev (2012a) and Woodsend and Lapata (2015) are particularly relevant to our research. The former use another non-parametric Bayesian model (a Chinese Restaurant process) in their work, while the latter embed predicateargument structures before performing clustering. Related Work The work in this paper relates to research on identifying predicate-argument structure in both local and global contexts. These related areas of research correspond to the NLP community’s work respectively on selectional preference modelling and semantic frame induction (which is also known variously as unsupervised semantic role labelling or role induction)."
S17-1025,P98-1013,0,\N,Missing
S17-1025,C98-1013,0,\N,Missing
W01-1313,A97-1007,0,0.00951897,"nt to include into the final summary not only the most important information but also the most recent one. The output of the presented system gives the information about the timeorder of the events described in several documents. 6 Related work Several linguistic and psycholinguistic studies deal with the problem of time-arrangement of different texts. The research presented in these studies highlights many problems but does not solve them. As for computational applications of time theories, most work was done on temporal expressions that appear in scheduling dialogues (Busemann et al., 1997; Alexandresson et al., 1997). There are many constraints on temporal expressions in this domain. The most relevant prior work is (Mani and Wilson, 2000), who implemented their system on news stories, introduced rules spreading time-stamps obtained with the help of explicit temporal expressions throughout the whole article, and invented machine learning rules for disambiguating between specific and generic use of temporal expressions (for example, whether Christmas is used to denote the 25th of December or to denote some period of time around the 25th of December). They also mention a problem of disambiguating between tem"
W01-1313,A97-1006,0,0.0159839,"on, where it is important to include into the final summary not only the most important information but also the most recent one. The output of the presented system gives the information about the timeorder of the events described in several documents. 6 Related work Several linguistic and psycholinguistic studies deal with the problem of time-arrangement of different texts. The research presented in these studies highlights many problems but does not solve them. As for computational applications of time theories, most work was done on temporal expressions that appear in scheduling dialogues (Busemann et al., 1997; Alexandresson et al., 1997). There are many constraints on temporal expressions in this domain. The most relevant prior work is (Mani and Wilson, 2000), who implemented their system on news stories, introduced rules spreading time-stamps obtained with the help of explicit temporal expressions throughout the whole article, and invented machine learning rules for disambiguating between specific and generic use of temporal expressions (for example, whether Christmas is used to denote the 25th of December or to denote some period of time around the 25th of December). They also mention a problem"
W01-1313,P00-1010,0,0.816758,"area rocked three months earlier by another massive quake 5.2.4 that <another massive quake> claimed some 2,300 victims. 5.3.1 On Monday and Tuesday, U.N. helicopters evacuated 50 of the most seriously injured to emergency medical centers. The last time interval assigned for sentence 5.2 is {1998:53:0}---{1998:71:0}, which gives an approximate range of days when the previous earthquake happened. But the information in sentence 5.3 is about the recent earthquake and not about the previous one of 3 months earlier, which is why it would be a mistake to point Monday and Tuesday within that range. Mani and Wilson (2000) point out “over half of the errors [made by his time-stamper] were due to propagation of spreading of an incorrect event time to neighboring events”. The rule of dropping the most recently assigned date as an anchor point when proceeding to the next sentence very often helps us to avoid this problem. There are however cases where dropping the most recent time as an anchor when proceeding to the next sentence causes errors: 4.8.1 But in February a devastating earthquake in the same region killed 2,300 people and left thousands of people homeless. 4.9.1 At the time international aid workers suf"
W01-1313,setzer-gaizauskas-2000-annotating,0,\N,Missing
W01-1313,P97-1062,0,\N,Missing
W01-1313,L00-1000,0,\N,Missing
W02-0406,2001.mtsummit-papers.68,0,0.0282352,"imum retention score of x (row) is higher than the maximum retention score of y (column), a ‘-’ indicates the maximum retention score of x is lower than the minimum retention score of y, and a ‘~’ means x and y are indistinguishable. Table 2 shows relative system performance in the multi-document summarization task. Despite the instability of the manual evaluation, we discuss automatic summary evaluation in an attempt to approximate the human evaluation results in the next section. 5 Automatic Summary Evaluation Inspired by recent progress in automatic evaluation of machine translation (BLEU; Papineni et al. 2001), we would like to apply the same idea in the evaluation of summaries. Following BLEU, we used the automatically computed accumulative n-gram matching scores (NAMS) between a model unit (MU) and a system summary (S)4 as performance indicator, considering multi-document summaries. Only content words were used in forming n-grams. NAMS is defined as follows: a1·NAM1 + a2·NAM2 + a3·NAM3 + a4·NAM4 NAMn is n-gram hit ratio defined as: # of matched n - grams between MU and S total # of n - grams in MU We tested three different configurations of ai: 4 The whole system summary was used to compute NAMS"
W02-0406,P02-1040,0,\N,Missing
W02-2108,P95-1034,0,0.019098,"ntains in it a pointer to the frame <patient&gt; from Figure 1(b). By keeping semantic content localized in the tree, we allow the gesture and speech synthesis modules convenient access to needed semantic information. This strategy is particularly convenient in a setting such as the MRE, where modules require increasing amounts of information as research continues. For any given state and event, there are a number of theoretically valid realizations available in the lexicon. Instead of attempting to decide which is most appropriate at any stage, we adopt a strategy similar to that introduced by (Knight & Hatzivassiloglou, 1995), which puts off the decision until realization is complete. We realize all possible valid trees that correspond to a given semantic input, and store the fully constructed trees in a forest structure. After all such trees are constructed we move on to the final stage. In this stage we examine all the trees in the forest structure and decide which tree will be propagated further down the NLP pipeline. Each tree is given a rank score based upon the tree’s information content and emotional quality. The score of each tree is calculated by recursively summing the scores of the nodes along the front"
W02-2108,J88-3006,0,0.0523586,"le work has been done on the effects of emotion on the verbal behavior of embodied agents. Most previous work focuses on intonation and nonverbal communication. With respect to content and phrasing, the most relevant work is over 10 years old. In his thesis, Hovy (1988) implemented a 3-valued (positive, negative, neutral) system of emotional shades with a simple sign multiplication calculus to control affect laden text generation. The three values provided little flexibility to accommodate the more subtle nuances associated with different shades of affect. Work by Bateman and Paris (1989) and Paris (1988) focus on variations of expert system output based on the reader’s knowledge. Also here, the rules for combining ratings of sentence constituents was fairly simple and not easily extensible. Papers by Walker et al (1996) and Loyall and Bates (1997) explore aspects of style and emotion, but do not focus on the particulars of natural language generation. In this paper we describe an integrated framework for modeling emotion in the speechbased natural language generation of embodied agents. It incorporates a distance calculus that adds flexibility and allows us to extend the emotional input from"
W03-0510,W00-0408,0,0.105548,"Missing"
W03-0510,J98-3005,0,0.0416223,"am co-occurrence scoring metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The evaluation results of the single document summarization task in DUC 2001 and 2002 (DUC 2002, Paul & Liggett 2002) indicate that most systems are as good as the baseline lead-based system and that humans are significantly better, though not by much. This leads to the be"
W03-0510,W02-0406,1,0.599598,"ndpipe to help her breathe.&lt;/S&gt; &lt;/TEXT&gt; &lt;/DOC&gt; Figure 3. A 150-word oracle extract for document AP900424-0035. good summary. It also makes system and human performance approach average since it is more likely to include some good sentences but not all of them. Empirical results shown in Section 5 confirm this and that leads us to the question of how to construct a corpus to evaluate summarization systems. We discuss this issue in the conclusion section. 4.3 Inter-Human Agreement and Its Effect on System Performance In this section we study how inter-human agreement affects system performance. Lin and Hovy (2002) reported that, compared to a manually created ideal, humans scored about 0.40 in average coverage score and the best system scored about 0.35. According to these numbers, we might assume that humans cannot agree to each other on what is important and the best system is almost as good as humans. If this is true then estimating an upper bound using oracle extracts is meaningless. No matter how high the estimated upper bounds may be, we probably would never be able to achieve that performance due to lack of agreement between humans: the oracle approximating one human would fail miserably with an"
W03-0510,H01-1054,0,0.0371477,"metric since all possible words are contained in the full text. Three manual summaries are used. 3 Oracle extracts are the best scoring extracts generated by exhaustive search of all possible sentence combinations of 100±5 words. popular (Edmundson 1969, Luhn 1969, Kupiec et al. 1995, Goldstein et al. 1999, Hovy and Lin 1999). The majority of systems participating in the past Document Understanding Conference (DUC 2002), a large scale summarization evaluation effort sponsored by the US government, are extraction based. Although systems based on information extraction (Radev and McKeown 1998, White et al. 2001, McKeown et al. 2002) and discourse analysis (Marcu 1999b, Strzalkowski et al. 1999) also exist, we focus our study on the potential and limitations of sentence extraction systems with the hope that our results will further progress in most of the automatic text summarization systems and evaluation setup. The evaluation results of the single document summarization task in DUC 2001 and 2002 (DUC 2002, Paul & Liggett 2002) indicate that most systems are as good as the baseline lead-based system and that humans are significantly better, though not by much. This leads to the belief that lead-base"
W03-0510,N03-1020,1,0.607198,"pants were required to create a generic 100-word summary. There were 30 test sets in DUC 2001 and each test set contained about 10 documents. For each document, one summary was created manually as the ëidealí model summary at approximately 100 words. We will refer to this manual summary as H1. Two other manual summaries were also created at about that length. We will refer to these two additional human summaries as H2 and H3. In addition, baseline summaries were created automatically by taking the first n sentences up to 100 words. We will refer this baseline extract as B1. In a recent study (Lin and Hovy 2003), we showed that the recall-based unigram co-occurrence automatic scoring metric correlated highly with human evaluation and has high recall and precision in predicting statistical significance of results comparing with its human counterpart. The idea is to measure the content similarity between a system extract and a manual summary using simple n-gram overlap. A similar idea called IBM BLEU score has proved successful in automatic machine translation evaluation (Papineni et al. 2001, NIST 2002). For summarization, we can express the degree of content overlap in terms of n-gram matches as the"
W03-0510,P02-1040,0,\N,Missing
W03-1007,J96-1002,0,0.0536596,"Missing"
W03-1007,P97-1003,0,0.0240947,"Missing"
W03-1007,J02-3001,0,0.826241,"e in June 2002, FrameNet has made available 49,000 annotated sentences. The release contains 99,000 annotated frame elements for 1462 distinct lexical predicates (927 verbs, 339 nouns, and 175 adjectives). While considerable in scale, the FrameNet database does not yet approach the magnitude of resources available for other NLP tasks. Each target predicate, for example, has on average only 30 sentences tagged. This data sparsity makes the task of learning a semantic classifier formidable, and increases the importance of the modeling framework that is employed. 2 Related Work To our knowledge, Gildea and Jurafsky (2002) is the only work to use FrameNet to build a statistically based semantic classifier. They split the problem into two distinct sub-tasks: frame element identification and frame element classification. In the identification phase, syntactic information is extracted from a parse tree to learn the boundaries of the frame elements in a sentence. In the classification phase, similar syntactic information is used to classify those elements into their semantic roles. In both phases Gildea and Jurafsky (2002) build a model of the conditional probabilities of the classification given a vector of syntac"
W03-1209,P97-1062,0,0.0280039,"Missing"
W03-1209,P02-1054,0,0.0636082,"Missing"
W04-0701,P98-1012,0,0.781691,"Missing"
W04-0701,J96-1002,0,0.00597831,"n held out test data compared to baseline (i.e., always same referent). Aside from the noise introduced by the assumption described above, another problem with these features arises when the derived probabilities are based on very low frequency counts. Thus, when adding these features to the model, we bin each feature according to the number of counts that the score was based on. 3.3 Model Maximum Entropy (Max. Ent.) models implement the intuition that the best model will be the one that is consistent with the set of constrains imposed by the evidence, but otherwise is as uniform as possible (Berger et al., 1996). We model the probability of two instances having the same referent (r=[1,0]) given a vector of features x according to the Max. Ent. formulation below: p( r |x ) = 1 n Zx exp[ ∑ λi f i ( r, x )] i =0 Here Zx is a normalization constant, fi(r,x) is a feature function over values of r and vector elements, n is the total number of feature functions, and λi is the weight for a given feature function. The final output of the model is the probability given a feature vector that r=1; i.e., the probability that the referents are the same. We train the Max. Ent. model using the YASMET Max. Ent. packa"
W04-0701,P03-1001,1,0.77803,"Missing"
W04-0701,C98-1012,0,\N,Missing
W04-0701,P02-1014,0,\N,Missing
W04-0701,W03-0405,0,\N,Missing
W04-0832,J81-4005,0,0.613718,"Missing"
W04-0832,J96-1002,0,0.0075566,"Missing"
W04-0832,A00-2018,0,0.135813,"Missing"
W04-0832,W03-1007,1,0.825946,"Missing"
W04-0832,J02-3001,0,0.347545,"Missing"
W04-1010,P00-1041,0,0.207827,"dline. In this paper, we describe in Section 2 previous work. Section 3 describes a study on the use of headline templates. A discussion on the process of selecting and expanding key headline phrases is in Section 4. And Section 5 goes back to the idea of templates but with the help of headline phrases. Future work is discussed in Section 6. 2 Related Work Several previous systems were developed to address the need for headline-style summaries. A lossy summarizer that ‘translates’ news stories into target summaries using the ‘IBM-style’ statistical machine translation (MT) model was shown in (Banko, et al., 2000). Conditional probabilities for a limited vocabulary and bigram transition probabilities as headline syntax approximation were incorporated into the translation model. It was shown to have worked surprisingly well with a stand-alone evaluation of quantitative analysis on content coverage. The use of a noisy-channel model and a Viterbi search was shown in another MT-inspired headline summarization system (Zajic, et al., 2002). The method was automatically evaluated by BiLingual Evaluation Understudy (Bleu) (Papineni, et al., 2001) and scored 0.1886 with its limited length model. A nonstatistica"
W04-1010,N03-1020,1,0.740393,"Missing"
W04-1010,W03-0501,0,\N,Missing
W04-1010,P02-1040,0,\N,Missing
W04-2709,P98-1013,0,0.473703,"dependency parser (details in section 6) and is a useful starting point for semantic annotation at IL1, since it allows annotators to see how textual units relate syntactically when making semantic judgments. 4.1.3 IL2 IL2 is intended to be an interlingua, a representation of meaning that is reasonably independent of language. IL2 is intended to capture similarities in meaning across languages and across different lexical/syntactic realizations within a language. For example, IL2 is expected to normalize over conversives (e.g. X bought a book from Y vs. Y sold a book to X) (as does FrameNet (Baker et al 1998)) and non-literal language usage (e.g. X started its business vs. X opened its doors to customers). The exact definition of IL2 will be the major research contribution of this project. 4.2 The Omega Ontology In progressing from IL0 to IL1, annotators have to select semantic terms (concepts) to represent the nouns, verbs, adjectives, and adverbs present in each sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998)"
W04-2709,J96-2004,0,0.0756983,"Missing"
W04-2709,2003.mtsummit-eval.3,1,0.726095,"al content, modality, speech acts, etc. At the same time, while incorporating these items, vagueness and redundancy must be eliminated from the annotation language. Many inter-event relations would need to be captured such as entity reference, time reference, place reference, causal relationships, associative relationships, etc. Finally, to incorporate these, crosssentence phenomena remain a challenge. From an MT perspective, issues include evaluating the consistency in the use of an annotation language given that any source text can result in multiple, different, legitimate translations (see Farwell and Helmreich, 2003) for discussion of evaluation in this light. Along these lines, there is the problem of annotating texts for translation without including in the annotations inferences from the source text. 9 Conclusions This is a radically different annotation project from those that have focused on morphology, syntax or even certain types of semantic content (e.g., for word sense disambiguation competitions). It is most similar to PropBank (Kingsbury et al 2002) and FrameNet (Baker et al 1998). However, it is novel in its emphasis on: (1) a more abstract level of mark-up (interpretation); (2) the assignment"
W04-2709,P03-1001,1,0.790541,"sentence. These terms are represented in the 110,000-node ontology Omega (Philpot et al., 2003), under construction at ISI. Omega has been built semi-automatically from a variety of sources, including Princeton's WordNet (Fellbaum, 1998), NMSU’s Mikrokosmos (Mahesh and Nirenburg, 1995), ISI's Upper Model (Bateman et al., 1989) and ISI's SENSUS (Knight and Luk, 1994). After the uppermost region of Omega was created by hand, these various resources’ contents were incorporated and, to some extent, reconciled. After that, several million instances of people, locations, and other facts were added (Fleischman et al., 2003). The ontology, which has been used in several projects in recent years (Hovy et al., 2001), can be browsed using the DINO browser at http://blombos.isi.edu:8000/dino; this browser forms a part of the annotation environment. Omega remains under continued development and extension. 4.1.2 IL1 IL1 is an intermediate semantic representation. It associates semantic concepts with lexical units like nouns, adjectives, adverbs and verbs (details of the ontology in section 4.2). It also replaces the syntactic relations in IL0, like subject and object, with thematic roles, like agent, theme and goal (de"
W04-2709,C18-2019,0,0.0664532,"Missing"
W04-2709,A97-1011,0,0.0452745,"first present the annotation process and tools used with it as well as the annotation manuals. Finally, setup issues relating to negotiating multi-site annotations are discussed. 6.1 Annotation process The annotation process was identical for each text. For the initial testing period, only English texts were annotated, and the process described here is for English text. The process for non-English texts will be, mutatis mutandis, the same. Each sentence of the text is parsed into a dependency tree structure. For English texts, these trees were first provided by the Connexor parser at UMIACS (Tapanainen and Jarvinen, 1997), and then corrected by one of the team PIs. For the initial testing period, annotators were not permitted to alter these structures. Already at this stage, some of the lexical items are replaced by features (e.g., tense), morphological forms are replaced by features on the citation form, and certain constructions are regularized (e.g., passive) and empty arguments inserted. It is this dependency structure that is loaded into the annotation tool and which each annotator then marks up. The annotator was instructed to annotate all nouns, verbs, adjectives, and adverbs. This involves annotating e"
W04-2709,1994.amta-1.25,0,0.0933693,"Missing"
W04-2709,C98-1013,0,\N,Missing
W04-3256,P99-1071,0,0.0311501,"l to achieve grammaticality and information capture, and push a step beyond sentence extraction. Many systems use machinelearning methods to learn from readily aligned corpora of scientific articles and their corresponding abstracts. Zhou and Hovy (2003) show a summarization system trained from automatically obtained text-summary alignments obeying the chronological occurrences of news events. MDS poses more challenges in assessing similarities and differences among the set of documents. The simple idea of extract-andconcatenate does not respond to problems arisen from coherence and cohesion. Barzilay et al. (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. Lin and Hovy (2002) apply a collection of known single-document summarization techniques, cooperating positional and topical information, clustering, etc., and extend them to perform MDS. While many have suggested that conventional MDS systems can be applied to biography generation directly, Mani (2001) illustrates that the added functionality of biographical MDS comes at the expense of a substantial increase in system complexity and is somewhat beyond the capabilities of present day MDS syst"
W04-3256,J95-4004,0,0.0254209,"Classification The Naïve Bayes classifier was used to perform the 10-Class task. Table 1 shows its performance with various features. P(F1,F2 ,...F j |S ∈ C) • P(S ∈ C) P(F1,F2 ,...Fk ) Assuming statistical independence of the features: P(S ∈ C |F1,F2 ,...Fk ) = ∏ k j=1 P(F j |S ∈ C) • P(S ∈ C) ∏ k j=1 P(F j ) Since P(Fj) has no role in selecting C: € k It can be expressed using Bayes’ rule, as (Kupiec et al., 1995): P(S ∈ C |F1,F2 ,...Fk ) = € P(S ∈ C |F1,F2 ,...Fk ) = ∏ Table 1. Performance of 10-Class sentence classification, using Naïve Bayes Classifier. Part-of-speech (POS) information (Brill, 1995) and word stems (Lovins, 1968) were used in some feature sets. We bootstrapped 10395 more biographyindicating words by recording the immediate hypernyms, using WordNet (Fellbaum, 1998), of the words collected from the controlled biography corpus described in Section 3. These words are called Expanded Unigrams and their frequency scores are reduced to a fraction of the original word’s frequency score. Some sentences in the testing set were labeled with multiple biography classes due to the fact that the original corpus was annotated at clause level. Since the classification was done at sentence"
W04-3256,W00-0405,0,0.0510221,"1). We discuss the other modules next. 5.1 Name-filter A filter scans through all documents in the set, eliminating sentences that are direct quotes, dialogues, and too short (under 5 words). Personoriented sentences containing any variation (first name only, last name only, and the full name) of the person’s name are kept for subsequent steps. Sentences classified as biography-worthy are merged with the name-filtered sentences with duplicates eliminated. 5.2 Sentence Ranking An essential capability of a multi-document summarizer is to combine text passages in a useful manner for the reader (Goldstein et al., 2000). This includes a sentence ordering parameter (Mani, 2001). Each of the sentences selected by the name-filter and the biography classifier is either related to the person-in-question via some news event or referred to as part of this person’s biographical profile, or both. We need a mechanism that will select sentences that are of informative significance within the source document set. Using inverse-term-frequency (ITF), i.e. an estimation of information value, words with high information value (low ITF) are distinguished from those with low value (high ITF). A sorted list of words along with"
W04-3256,N03-1020,1,0.649199,"Missing"
W04-3256,J98-3005,0,0.0697651,"the true quality of machinegenerated summaries. There is no automated tool for this purpose currently. We plan to incorporate one for the future development of this work. 6.3 Discussion N-gram recall scores are computed by ROUGE, in addition to ROUGE-L shown here. While cosine similarity and unigram and bigram overlap demonstrate a sufficient measure on content coverage, they are not sensitive on how information is sequenced in the text (Saggion et al., 2002). In evaluating and analyzing MDS results, metrics, such as ROUGE-L, that consider linguistic sequence are essential. Radev and McKeown (1998) point out when summarizing interesting news events from multiple sources, one can expect reports with contradictory and redundant information. An intelligent summarizer should attain as much information as possible, combine it, and present it in the most concise form to the user. When we look at the different attributes in a person’s life reported in news articles, a person is described by the job positions that he/she has held, by education institutions that he/she has attended, and etc. Those data are confirmed biographical information and do not bear the necessary contradiction associated"
W04-3256,C02-1073,0,0.0290082,"ongst the peer systems. Clearly, humans still perform at a level much superior to any system. Measuring fluency and coherence is also important in reflecting the true quality of machinegenerated summaries. There is no automated tool for this purpose currently. We plan to incorporate one for the future development of this work. 6.3 Discussion N-gram recall scores are computed by ROUGE, in addition to ROUGE-L shown here. While cosine similarity and unigram and bigram overlap demonstrate a sufficient measure on content coverage, they are not sensitive on how information is sequenced in the text (Saggion et al., 2002). In evaluating and analyzing MDS results, metrics, such as ROUGE-L, that consider linguistic sequence are essential. Radev and McKeown (1998) point out when summarizing interesting news events from multiple sources, one can expect reports with contradictory and redundant information. An intelligent summarizer should attain as much information as possible, combine it, and present it in the most concise form to the user. When we look at the different attributes in a person’s life reported in news articles, a person is described by the job positions that he/she has held, by education institution"
W04-3256,P01-1059,0,0.413504,"ugh sentence generation. Lin and Hovy (2002) apply a collection of known single-document summarization techniques, cooperating positional and topical information, clustering, etc., and extend them to perform MDS. While many have suggested that conventional MDS systems can be applied to biography generation directly, Mani (2001) illustrates that the added functionality of biographical MDS comes at the expense of a substantial increase in system complexity and is somewhat beyond the capabilities of present day MDS systems. The discussion was based in part on the only known MDS biography system (Schiffman et al., 2001) that uses corpus statistics along with linguistic knowledge to select and merge description of people in news. The focus of this work was on synthesizing succinct descriptions of people by merging appositives from semantic processing using WordNet (Miller, 1995). 3 Corpus Description In order to extract information that is related to a person from a large set of news texts written not exclusively about this person, we need to identify attributes shared among biographies. Biographies share certain standard components. We annotated a corpus of 130 biographies of 12 people (activists, artists, l"
W04-3256,J98-2013,0,0.0763768,"Missing"
W04-3256,N03-1037,1,0.819389,"nerating summaries by extraction, which is finding a subset of the document that is indicative of its contents (Kupiec et al., 1995) using “shallow” linguistic analysis and statistics. The other influence is the exploration of “deeper” knowledge-based methods for condensing information. Knight and Marcu (2000) equate summarization with compression at sentence level to achieve grammaticality and information capture, and push a step beyond sentence extraction. Many systems use machinelearning methods to learn from readily aligned corpora of scientific articles and their corresponding abstracts. Zhou and Hovy (2003) show a summarization system trained from automatically obtained text-summary alignments obeying the chronological occurrences of news events. MDS poses more challenges in assessing similarities and differences among the set of documents. The simple idea of extract-andconcatenate does not respond to problems arisen from coherence and cohesion. Barzilay et al. (1999) introduce a combination of extracted similar phrases and a reformulation through sentence generation. Lin and Hovy (2002) apply a collection of known single-document summarization techniques, cooperating positional and topical info"
W05-1520,P98-1013,0,0.018584,"rsity of Southern California Marina del Rey, CA, 90292, USA leuski@ict.usc.edu Eduard Hovy Information Sciences Institute University of Southern California Marina del Rey, CA, 90292, USA hovy@isi.edu 2 Parsing Methods Introduction and Related Work Natural language understanding is an essential module in any dialogue system. To obtain satisfactory performance levels, a dialogue system needs a semantic parser/natural language understanding system (NLU) that produces accurate and detailed dialogue oriented semantic output. Recently, a number of semantic parsers trained using either the FrameNet (Baker et al., 1998) or the PropBank (Kingsbury et al., 2002) have been reported. Despite their reasonable performances on general tasks, these parsers do not work so well in specific domains. Also, where these general purpose parsers tend to provide case-frame structures, that include the standard core case roles (Agent, Patient, Instrument, etc.), dialogue oriented domains tend to require additional information about addressees, modality, speech acts, etc. Where general-purpose resources such as PropBank and Framenet provide invaluable training data for general case, it tends to be a problem to obtain enough tr"
W05-1520,J96-1002,0,0.00571078,"slot-value pairs, for the dialogue module. 2.1 Voting Model We use a simple conditional probability model P (f |W ) for parsing. The model represents the probability of producing slot-value pair f as an output given that we have seen a particular word or n-gram W as input. Our two-stage procedure for generating a frame for a given input sentence is: (1) Find a set of all slot-value that correspond with each word/ngram (2) Select the top portion of these candidates to form the final frame (Bhagat et al., 2005; Feng and Hovy, 2003). 2.2 Maximum Entropy Our next approach is the Maximum Entropy (Berger et al., 1996) classification approach. Here, we cast our problem as a problem of ranking using a classifier where each slot-value pair in the training data is considered a class and feature set consists of the unigrams, bigrams and trigrams in the sentences (Bhagat et al., 2005). 2.3 Support Vector Machines We use another commonly used classifier, Support Vector Machine (Burges, 1998), to perform the same task (Bhagat et al., 2005). Approach is similar to Section 2.2. 2.4 Language Model As a fourth approach to the problem, we use the Statistical Language Model (Ponte and Croft, 1997). We estimate the langu"
W05-1520,W05-1520,1,0.0522675,"the sentence strings produced by the speech recognizer into internal shallow semantic frames composed of slot-value pairs, for the dialogue module. 2.1 Voting Model We use a simple conditional probability model P (f |W ) for parsing. The model represents the probability of producing slot-value pair f as an output given that we have seen a particular word or n-gram W as input. Our two-stage procedure for generating a frame for a given input sentence is: (1) Find a set of all slot-value that correspond with each word/ngram (2) Select the top portion of these candidates to form the final frame (Bhagat et al., 2005; Feng and Hovy, 2003). 2.2 Maximum Entropy Our next approach is the Maximum Entropy (Berger et al., 1996) classification approach. Here, we cast our problem as a problem of ranking using a classifier where each slot-value pair in the training data is considered a class and feature set consists of the unigrams, bigrams and trigrams in the sentences (Bhagat et al., 2005). 2.3 Support Vector Machines We use another commonly used classifier, Support Vector Machine (Burges, 1998), to perform the same task (Bhagat et al., 2005). Approach is similar to Section 2.2. 2.4 Language Model As a fourth app"
W05-1520,W96-0213,0,\N,Missing
W05-1520,P97-1003,0,\N,Missing
W05-1520,C98-1013,0,\N,Missing
W05-1520,J02-3001,0,\N,Missing
W05-1520,N04-1030,0,\N,Missing
W06-0301,W03-1007,1,0.619445,"Missing"
W06-0301,J02-3001,0,0.418669,"opinion holder and topic identification. This paper is organized as follows: Section 2 briefly introduces related work both in sentiment analysis and semantic role labeling. Section 3 describes our approach for identifying opinions and labeling holders and topics by utilizing FrameNet1 data for our task. Section 4 reports our experiments and results with discussions and finally Section 5 concludes. 2 2.2 Semantic role labeling is the task of identifying semantic roles such as Agent, Patient, Speaker, or Topic, in a sentence. A statistical approach for semantic role labeling was introduced by (Gildea and Jurafsky, 2002). Their system learned semantic relationship among constituents in a sentence from FrameNet, a large corpus of semantically hand-annotated data. The FrameNet annotation scheme is based on Frame Semantics (Fillmore, 1976). Frames are defined as “schematic representations of situations involving various frame elements such as participants, props, and other conceptual roles.” For example, given a sentence “Jack built a new house out of bricks”, a semantic role labeling system should identify the roles for the verb built such as “[Agent Jack] built [Created_entity a new house] [Component out of br"
W06-0301,P97-1023,0,0.264402,"ing it on opinion-bearing frames and their frame elements in FrameNet. Related Work This section reviews previous works in both sentiment detection and semantic role labeling. 2.1 Subjectivity and Sentiment Detection Subjectivity detection is the task of identifying subjective words, expressions, and sentences (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al., 2003). Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005), phrases and sentences (Kim and Hovy, 2004; Wilson et al., 2005), or documents (Pang et al., 2002; Turney, 2002). Building on this work, more sophisticated problems such as opinion holder identification have also been studied. (Bethard et al., 2004) identify opinion propositions and holders. Their 1 Semantic Role Labeling 3 Finding Opinions and Their Holders and Topics For the goal of this study, extracting opinions from news media texts with their holders and topics, we utilize FrameNet data. The basic idea of our approach is to explore how an opini"
W06-0301,C00-1044,0,0.362202,"Missing"
W06-0301,C04-1200,1,0.427141,"Missing"
W06-0301,W02-1011,0,0.0416026,"ic role labeling. 2.1 Subjectivity and Sentiment Detection Subjectivity detection is the task of identifying subjective words, expressions, and sentences (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al., 2003). Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005), phrases and sentences (Kim and Hovy, 2004; Wilson et al., 2005), or documents (Pang et al., 2002; Turney, 2002). Building on this work, more sophisticated problems such as opinion holder identification have also been studied. (Bethard et al., 2004) identify opinion propositions and holders. Their 1 Semantic Role Labeling 3 Finding Opinions and Their Holders and Topics For the goal of this study, extracting opinions from news media texts with their holders and topics, we utilize FrameNet data. The basic idea of our approach is to explore how an opinion holder and a topic are semantically related to an opinion bearing word in a sentence. Given a sentence and an opinion bearing word, our me"
W06-0301,H05-1043,0,0.390443,"Missing"
W06-0301,W03-0404,0,0.0654235,"oles for the verb built such as “[Agent Jack] built [Created_entity a new house] [Component out of bricks]”3. In our study, we build a semantic role labeling system as an intermediate step to label opinion holders and topics by training it on opinion-bearing frames and their frame elements in FrameNet. Related Work This section reviews previous works in both sentiment detection and semantic role labeling. 2.1 Subjectivity and Sentiment Detection Subjectivity detection is the task of identifying subjective words, expressions, and sentences (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al., 2003). Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005), phrases and sentences (Kim and Hovy, 2004; Wilson et al., 2005), or documents (Pang et al., 2002; Turney, 2002). Building on this work, more sophisticated problems such as opinion holder identification have also been studied. (Bethard et al., 2004) identify opinion propositions and holders. Their 1 Semantic"
W06-0301,P02-1053,0,0.0234823,"their frame elements in FrameNet. Related Work This section reviews previous works in both sentiment detection and semantic role labeling. 2.1 Subjectivity and Sentiment Detection Subjectivity detection is the task of identifying subjective words, expressions, and sentences (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al., 2003). Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005), phrases and sentences (Kim and Hovy, 2004; Wilson et al., 2005), or documents (Pang et al., 2002; Turney, 2002). Building on this work, more sophisticated problems such as opinion holder identification have also been studied. (Bethard et al., 2004) identify opinion propositions and holders. Their 1 Semantic Role Labeling 3 Finding Opinions and Their Holders and Topics For the goal of this study, extracting opinions from news media texts with their holders and topics, we utilize FrameNet data. The basic idea of our approach is to explore how an opinion holder and"
W06-0301,P99-1032,0,0.21623,"Missing"
W06-0301,H05-1044,0,0.0624379,"both sentiment detection and semantic role labeling. 2.1 Subjectivity and Sentiment Detection Subjectivity detection is the task of identifying subjective words, expressions, and sentences (Wiebe et al., 1999; Hatzivassiloglou and Wiebe, 2000; Riloff et al., 2003). Identifying subjectivity helps separate opinions from fact, which may be useful in question answering, summarization, etc. Sentiment detection is the task of determining positive or negative sentiment of words (Hatzivassiloglou and McKeown, 1997; Turney, 2002; Esuli and Sebastiani, 2005), phrases and sentences (Kim and Hovy, 2004; Wilson et al., 2005), or documents (Pang et al., 2002; Turney, 2002). Building on this work, more sophisticated problems such as opinion holder identification have also been studied. (Bethard et al., 2004) identify opinion propositions and holders. Their 1 Semantic Role Labeling 3 Finding Opinions and Their Holders and Topics For the goal of this study, extracting opinions from news media texts with their holders and topics, we utilize FrameNet data. The basic idea of our approach is to explore how an opinion holder and a topic are semantically related to an opinion bearing word in a sentence. Given a sentence an"
W06-0301,H05-2017,0,\N,Missing
W06-0301,J96-1002,0,\N,Missing
W06-0301,H05-1045,0,\N,Missing
W06-0301,P03-2030,0,\N,Missing
W06-1610,N03-1020,1,0.0666167,"at uses paraphrases to improve the quality of machine translation evaluations. Previous work has focused on fixed n-gram evaluation metrics coupled with lexical identity matching. ParaEval addresses three important issues: support for paraphrase/synonym matching, recall measurement, and correlation with human judgments. We show that ParaEval correlates significantly better than BLEU with human assessment in measurements for both fluency and adequacy. 1 Introduction The introduction of automated evaluation procedures, such as BLEU (Papineni et al., 2001) for machine translation (MT) and ROUGE (Lin and Hovy, 2003) for summarization, have prompted much progress and development in both of these areas of research in Natural Language Processing (NLP). Both evaluation tasks employ a comparison strategy for comparing textual units from machine-generated and gold-standard texts. Ideally, this comparison process would be performed manually, because of humans’ abilities to infer, paraphrase, and use world knowledge to relate differently worded pieces of equivalent information. However, manual evaluations are time consuming and expensive, thus making them a bottleneck in system development cycles. BLEU measures"
W06-1610,P04-1077,1,0.647219,"Missing"
W06-1610,J03-1002,0,0.00239997,"tistical Machine Translation (SMT) systems analyze large quantities of bilingual parallel texts in order to learn translational alignments between pairs of words and phrases in two languages (Och and Ney, 2004). The sentence-based translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory. This alignment process results in a corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other. We ran the alignment algorithm from (Och and Ney, 2003) on a Chinese-English parallel corpus of 218 million English words, available from the Linguistic Data Consortium (LDC). Phrase pairs are extracted by following the method described in (Och and Ney, 2004) where all contiguous phrase pairs having consistent alignments are extraction candidates. Using these pairs we build paraphrase sets by joining together all English phrases that have the same Chinese translation. Figure 3 shows an example word/phrase alignment for two parallel sentence pairs from our corpus where the phrases “blowing up” and “bombing” have the same Chinese translation. On the"
W06-1610,J04-4002,0,0.00709496,"involving Chinese) and has determined it to be a direct substitute of the phrase “bombing attack”, then the Chinese translation of “bombing attack” would be used in place of the translation for “bombing”. This substitution technique has shown some improvement in translation quality (Callison-Burch et al., 2006). Figure 3. An example of the paraphrase extraction process. foreign language. Phrase-based Statistical Machine Translation (SMT) systems analyze large quantities of bilingual parallel texts in order to learn translational alignments between pairs of words and phrases in two languages (Och and Ney, 2004). The sentence-based translation model makes word/phrase alignment decisions probabilistically by computing the optimal model parameters with application of the statistical estimation theory. This alignment process results in a corpus of word/phrase-aligned parallel sentences from which we can extract phrase pairs that are translations of each other. We ran the alignment algorithm from (Och and Ney, 2003) on a Chinese-English parallel corpus of 218 million English words, available from the Linguistic Data Consortium (LDC). Phrase pairs are extracted by following the method described in (Och an"
W06-1610,2003.mtsummit-papers.51,0,0.128697,"overall system and human ranking. In the upper left corner, human translators are grouped together, significantly separated from the automatic MT systems clustered into the lower right corner. Evaluating ParaEval To be effective in MT evaluations, an automated procedure should be capable of distinguishing good translation systems from bad ones, human translations from systems’, and human translations of differing quality. For a particular evaluation exercise, an evaluation system produces a ranking for system and human translations, and compares this ranking with one created by human judges (Turian et al., 2003). The closer a system’s ranking is to the human’s, the better the evaluation system is. 4.1 Validating ParaEval To test ParaEval’s ability, NIST 2003 Chinese MT evaluation results were used (NIST 2003). This collection consists of 100 source documents in Chinese, translations from eight individual translation systems, reference translations from four humans, and human assessments (on fluency and adequacy). The Spearman rank-order coefficient is computed as an indicator of how close a system ranking is to gold-standard human ranking. It should be noted that the 2003 MT data is separate from the"
W06-1610,P05-1074,0,0.454537,"uce domain-specific collections that are used for text generation and are application-specific. But operating in multiple domains and for multiple tasks translates into multiple manual collection efforts, which could be very time-consuming and costly. In order to facilitate smooth paraphrase utilization across a variety of NLP applications, we need an unsupervised paraphrase collection mechanism that can be easily conducted, and produces paraphrases that are of adequate quality and can be readily used with minimal amount of adaptation effort. Our method (Anonymous, 2006), also illustrated in (Bannard and Callison-Burch, 2005), to automatically construct a large domainindependent paraphrase collection is based on the assumption that two different phrases of the same meaning may have the same translation in a With the introduction of ParaEval, we will address two of these three issues, namely the paraphrasing problem and providing a recall measure. 79 in Figure 3, the paraphrase table would contain only “bombing” and “bombing attack”. Paraphrases that are direct substitutes of one another are useful when translating unknown phrases. For instance, if a MT system does not have the Chinese translation for the word “bom"
W06-1610,N06-1003,0,0.158728,"Missing"
W06-1610,E06-1032,0,0.0432685,"Missing"
W06-1610,2003.mtsummit-papers.32,0,0.110898,"Missing"
W06-1610,P02-1040,0,\N,Missing
W08-0628,D07-1088,1,0.698464,"xperiment. In this experiment, when the tracer was injected into the injection location “the contralateral AVCN”, “no labeled cells” was found in the labeling location “the DCN”. For sentence level fields labeling, the performance of F1 score is around 0.79 (Feng et al., 2008). 1 http://www.neuroscholar.org/ 120 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 120–121, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics We here show how the adaptive information extraction framework is applied to labeling individual sentences. Please see Feng et al. (2007) for the details of segmenting data records. 2.1 Choosing Learning Approach via F1 A natural way to label sentences is to obtain (by hand or learning) patterns characterizing each field (Feng et al., 2006; Ravichandran and Hovy, 2002). We tried to annotate field values for the biomedical data, but we found few intuitive clues that rich surface text patterns could be learned with this corpus. This insight, Feedback F1, caused us to give up the idea of learning surface text patterns as usual, and switch to the Conditional Random Fields (CRF) (Lafferty et al., 2001) for labeling sentences instead"
W08-0628,I08-2124,1,0.840382,"ingle research article --- anywhere from one to many. Table 1. An example tract-tracing experiment. Figure 1. Adaptive information extraction. For more complex tasks requiring iterative cycles, an adaptive and extended IE framework has not yet been fully defined although variants have been exTable 1 provides an example of a tract-tracing experiment. In this experiment, when the tracer was injected into the injection location “the contralateral AVCN”, “no labeled cells” was found in the labeling location “the DCN”. For sentence level fields labeling, the performance of F1 score is around 0.79 (Feng et al., 2008). 1 http://www.neuroscholar.org/ 120 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 120–121, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics We here show how the adaptive information extraction framework is applied to labeling individual sentences. Please see Feng et al. (2007) for the details of segmenting data records. 2.1 Choosing Learning Approach via F1 A natural way to label sentences is to obtain (by hand or learning) patterns characterizing each field (Feng et al., 2006; Ravichandran and Hovy, 2002). We tried to annotate fi"
W08-0628,P02-1006,1,0.535292,"rmance of F1 score is around 0.79 (Feng et al., 2008). 1 http://www.neuroscholar.org/ 120 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 120–121, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics We here show how the adaptive information extraction framework is applied to labeling individual sentences. Please see Feng et al. (2007) for the details of segmenting data records. 2.1 Choosing Learning Approach via F1 A natural way to label sentences is to obtain (by hand or learning) patterns characterizing each field (Feng et al., 2006; Ravichandran and Hovy, 2002). We tried to annotate field values for the biomedical data, but we found few intuitive clues that rich surface text patterns could be learned with this corpus. This insight, Feedback F1, caused us to give up the idea of learning surface text patterns as usual, and switch to the Conditional Random Fields (CRF) (Lafferty et al., 2001) for labeling sentences instead. In contrast to fixed-order patterns, the CRF model provides a compact way to integrate different types of features for sequential labeling problems and can reach state-of-the-art level performance. 2.2 Determining Knowledge Schema v"
W10-0903,P03-1054,0,0.00641668,"Missing"
W10-0903,W08-1301,0,0.0661378,"Missing"
W10-0903,W08-2219,0,0.0302828,"Missing"
W11-0143,pan-etal-2006-annotated,1,0.781094,"and are very complex structures in language. 3 Evaluation of the Granularity Model in Natural Language We conducted an evaluation study to judge the “goodness” of the granularity model proposed. In this study the annotators were asked to annotate granularity relations between two given paragraphs. Paragraph-based analysis was preferred to event-word-based analysis because people reason much more easily with paragraph descriptions than with individual event mentions 2 . The annotation set consisted of paragraph pairs from three domains: travel articles (confluence.org), Timebank annotated data Pan et al. (2006), and Wikipedia articles on games. We selected a total of 37 articles: 10 articles about travel, 10 about games, and 17 from Timebank. Both paragraphs of a given question were selected from the same article and referred to the same overall concept. 3.1 Annotation Task The articles were uploaded to Mechanical Turk and were annotated by non-expert annotators (regular Turkers). The entire set of 37 articles was annotated by 5 people. The annotators were given a pair of paragraphs and were asked four questions about the relations between them: (i) Is one paragraph a subevent of the other paragraph"
W11-0206,D10-1035,0,0.0291008,"Missing"
W11-0206,W03-0404,1,0.797083,"Missing"
W11-0206,W02-1028,1,0.675165,"Missing"
W11-0206,P09-1045,0,\N,Missing
W11-0704,P06-2005,0,0.174701,"n this model by adapting the channel noise according to several predefined word formations such as stylistic variation, word clipping, etc. However, spelling correction is traditionally conducted in media with relatively high percentages of well-formed text where one can perform word boundary detection and thus tokenization to a high degree of accuracy. The main drawback is 21 the strong confidence this approach places on word boundaries (Beaufort et al., 2010), since detecting word boundaries in noisy text is not a trivial problem. In the machine translation approach (Bangalore et al., 2002; Aw et al., 2006), normalizing noisy text is considered as a translation task from a source language (the noisy text) to a target language (the cleansed text). Since noisy- and clean text typically vary wildly, it satisfies the notion of translating between two languages. However, since these transformations can be highly creative, they usually need a wide context (more than one word) to be resolved adequately. Kobus (2008) also points out that despite the fairly good results achieved with this system, such a purely phrase-based translation model cannot adequately handle the wide level of lexical creativity fo"
W11-0704,C02-1134,0,0.0825419,"evenson (2009) improve on this model by adapting the channel noise according to several predefined word formations such as stylistic variation, word clipping, etc. However, spelling correction is traditionally conducted in media with relatively high percentages of well-formed text where one can perform word boundary detection and thus tokenization to a high degree of accuracy. The main drawback is 21 the strong confidence this approach places on word boundaries (Beaufort et al., 2010), since detecting word boundaries in noisy text is not a trivial problem. In the machine translation approach (Bangalore et al., 2002; Aw et al., 2006), normalizing noisy text is considered as a translation task from a source language (the noisy text) to a target language (the cleansed text). Since noisy- and clean text typically vary wildly, it satisfies the notion of translating between two languages. However, since these transformations can be highly creative, they usually need a wide context (more than one word) to be resolved adequately. Kobus (2008) also points out that despite the fairly good results achieved with this system, such a purely phrase-based translation model cannot adequately handle the wide level of lex"
W11-0704,P10-1079,0,0.024912,"hannel model (Shannon and Weaver, 1948) using a hidden Markov model to handle both graphemic and phonemic variations, and Cook and Stevenson (2009) improve on this model by adapting the channel noise according to several predefined word formations such as stylistic variation, word clipping, etc. However, spelling correction is traditionally conducted in media with relatively high percentages of well-formed text where one can perform word boundary detection and thus tokenization to a high degree of accuracy. The main drawback is 21 the strong confidence this approach places on word boundaries (Beaufort et al., 2010), since detecting word boundaries in noisy text is not a trivial problem. In the machine translation approach (Bangalore et al., 2002; Aw et al., 2006), normalizing noisy text is considered as a translation task from a source language (the noisy text) to a target language (the cleansed text). Since noisy- and clean text typically vary wildly, it satisfies the notion of translating between two languages. However, since these transformations can be highly creative, they usually need a wide context (more than one word) to be resolved adequately. Kobus (2008) also points out that despite the fairl"
W11-0704,C10-2022,0,0.0579168,"’, ‘12:44’, etc. At this stage, all out-of-vocabulary (OOV) terms represent the terms that we are uncertain about, and hence candidate terms for cleansing. First, for each OOV term, we enumerate each possibly ambiguous character into all its possible interpretations with the transliteration table shown in Table 1. This expands, for example, ‘t0day’ → [‘t0day’, ‘today’], and also ‘2day’ → [‘2day’, ‘twoday’, ‘today’], etc. Each transliterated candidate word in each confusion set produced this way is then scored with the original word and ranked using the heuristic function (sim()) described in (Contractor et al., 2010)2 . We also evaluated a purely phonetic editdistance similarity function, based on the Double Metaphone algorithm (Philips, 2000), but found the string-similarity-based function to give more reliable results. Each confusion set produced this way (see Algorithm 2) is joined to its previous set to form a growing confusion lattice. Finally this lattice is decoded by converting it into the probabilistic finitestate grammar format, and by using the SRI-LM toolkit’s (Stolcke, 2002) lattice-tool command to find the best path through the lattice by 2 The longest common subsequence between the two word"
W11-0704,W09-2010,0,0.484723,"kes strong use of techniques for normalizing ‘noisy text’ such as SMS-messages and Twitter messages into standard English. Normalizing text can traditionally be approached using three well-known NLP metaphors, namely that of spell-checking, machine translation (MT) and automatic speech recognition (ASR) (Kobus et al., 2008). In the spell-checking approach, corrections from ‘noisy’ words to ‘clean’ words proceed on a wordby-word basis. Choudhury (2007) implements the noisy channel model (Shannon and Weaver, 1948) using a hidden Markov model to handle both graphemic and phonemic variations, and Cook and Stevenson (2009) improve on this model by adapting the channel noise according to several predefined word formations such as stylistic variation, word clipping, etc. However, spelling correction is traditionally conducted in media with relatively high percentages of well-formed text where one can perform word boundary detection and thus tokenization to a high degree of accuracy. The main drawback is 21 the strong confidence this approach places on word boundaries (Beaufort et al., 2010), since detecting word boundaries in noisy text is not a trivial problem. In the machine translation approach (Bangalore et a"
W11-0704,P11-1038,0,0.330593,"Missing"
W11-0704,C08-1056,0,0.492818,"Section 5 describes our experimental results. Finally, Section 6 concludes the paper and describes possible directions for future work. 2 Related Work Although our work is primarily focused on analyzing the lexical variation in language found in online social media, our analysis methodology makes strong use of techniques for normalizing ‘noisy text’ such as SMS-messages and Twitter messages into standard English. Normalizing text can traditionally be approached using three well-known NLP metaphors, namely that of spell-checking, machine translation (MT) and automatic speech recognition (ASR) (Kobus et al., 2008). In the spell-checking approach, corrections from ‘noisy’ words to ‘clean’ words proceed on a wordby-word basis. Choudhury (2007) implements the noisy channel model (Shannon and Weaver, 1948) using a hidden Markov model to handle both graphemic and phonemic variations, and Cook and Stevenson (2009) improve on this model by adapting the channel noise according to several predefined word formations such as stylistic variation, word clipping, etc. However, spelling correction is traditionally conducted in media with relatively high percentages of well-formed text where one can perform word bound"
W11-0704,W02-0109,0,0.0376284,"using pre-processing techniques discussed in (Kaufmann and Kalita, 2010). It works as follows: For each input message, we replace @-usernames with “*USR*” and urls with “*URL*”. Hash tags can either be part of the sentence (‘just got a #droid today’) or be peripheral to the sentence (‘what a loooong day! #wasted’). Following Kaufmann (2010) we remove hashtags at the end of messages when they are preceded by typical end-of-sentence punctuation marks. Hash tags in the middle of messages are retained, and the hash sign removed. Next we tokenize this preprocessed message using the NLTK tokenizer (Loper and Bird, 2002). As noted earlier, standard NLP tools do not perform well on noisy text out-of-the-box. Based on inspection of incorrectly tokenized output, we therefore include a post-tokenization phase where we split all tokens that include a punctuation symbol into the individual one or two alphanumeric tokens (on either side of the punctuation symbol), and the punctuation symbol1 . This heuristic catches most cases of run-on sentences. Given a set of input tokens, we process these one by one, by comparing each token to the words in the lexicon L and constructing a confusion network CN. Each in-vocabulary"
W12-1905,P07-1036,0,0.200301,"si.edu Abstract (Johnson, 2007). Running repeated restarts with random initialization can help escape local maxima, but in order to find the global optimum, we need to run a great number (100 or more) of them (Ravi and Knight, 2009; Hovy et al., 2011). However, there is another solution. Various papers have shown that the inclusion of some knowledge greatly enhances performance of unsupervised systems. They introduce constraints on the initial model and the parameters. This directs the learning algorithm towards a better parameter configuration. Types of constraints include ILP-based methods (Chang et al., 2007; Chang et al., 2008; Ravi and Knight, 2009), and posterior regularization (Grac¸a et al., 2007; Ganchev et al., 2010). While those approaches are powerful and yield good results, they require us to reformulate the constraints in a certain language, and either use an external solver, or re-design parts of the maximization step. This is time-consuming and requires a certain expertise. For many NLP tasks, EM-trained HMMs are the common models. However, in order to escape local maxima and find the best model, we need to start with a good initial model. Researchers suggested repeated random restar"
W12-1905,W02-0102,0,0.12242,"position as last element For POS-tagging, we use a standard bigram HMM without back-off. For PSD, we use a trigram HMM, but move the preposition at the end of the observed sequence, to condition it on the previous words (see Figure 2). Since not all prepositions have the same set of labels, we train individual models for each preposition. We can thus learn different parameter settings for the different prepositions. We use EM with smoothing and random restarts to train our models. For smoothing,  is added to each fractional count before normalization at each iteration to prevent overfitting (Eisner, 2002a). We set  to 0.01. We stop training after 40 iterations, or if the perplexity change between iterations was less than 0.0001. We experimented with different numbers of random restarts (none, 10, 50, and 100). 3.3 Dealing with Partial Annotations The most direct way to constrain a specific word to only one label is to substitute it for a special token that has only that label. If we have a partially annotated example “walk on-sense5 water” as input (see Figure 1), we add an emission probability P (word = label |tag = label ) to our model. However, this is problematic in two ways. Firstly, we"
W12-1905,P02-1001,0,0.265279,"position as last element For POS-tagging, we use a standard bigram HMM without back-off. For PSD, we use a trigram HMM, but move the preposition at the end of the observed sequence, to condition it on the previous words (see Figure 2). Since not all prepositions have the same set of labels, we train individual models for each preposition. We can thus learn different parameter settings for the different prepositions. We use EM with smoothing and random restarts to train our models. For smoothing,  is added to each fractional count before normalization at each iteration to prevent overfitting (Eisner, 2002a). We set  to 0.01. We stop training after 40 iterations, or if the perplexity change between iterations was less than 0.0001. We experimented with different numbers of random restarts (none, 10, 50, and 100). 3.3 Dealing with Partial Annotations The most direct way to constrain a specific word to only one label is to substitute it for a special token that has only that label. If we have a partially annotated example “walk on-sense5 water” as input (see Figure 1), we add an emission probability P (word = label |tag = label ) to our model. However, this is problematic in two ways. Firstly, we"
W12-1905,W10-1701,0,0.0264235,"ances. Rehbein et al. (2009) study the utility of partial annotations as precursor to further, human annotation. Their experiments do not extend to unsupervised training. Tsuboi et al. (2008) used data that was not full annotated. However, their setting is in principle supervised, only few words are missing. Instead of no labels, those words have a limited number of possible alternatives. This works well for tasks with a small label alphabet or data where annotators left multiple options for some words. In contrast, we start out with unannotated data and assume that some words can be labeled. Gao et al. (2010) present a successful word alignment approach that uses partial annotations. These are derived from human annotation or heuristics. Their method improves BLEU, but requires some modification of the EM framework. 7 Conclusion and Future Work It is obvious, and common knowledge, that providing some annotation to an unsupervised algorithm will improve accuracy and learning speed. Surprisingly, however, our literature search did not turn up any papers stating exactly how and to what degree the improvements appear. We therefore selected a very general training method, EM, and a simple approach to i"
W12-1905,P08-1085,0,0.0180592,"traints. Careful selection of the right heuristics and the tradeoff between false positives they in36 Unsupervised methods have great appeal for resource-poor languages and new tasks. They have been applied to a wide variety of sequential labeling tasks, such as POS tagging, NE recognition, etc. The most common training technique is forwardbackward EM. While EM is guaranteed to improve the data likelihood, it can get stuck in local maxima. Merialdo (1994) showed how the the initialized model influences the outcome after a fixed number of iterations. The importance is underscored succinctly by Goldberg et al. (2008). They experiment with various constraints. The idea of using partial annotations has been explored in various settings. Druck et al. (2008) present an approach to label features instead of instances for discriminative probabilistic models, yielding substantial improvements. They also study the effectiveness of labeling features vs. labeling instances. Rehbein et al. (2009) study the utility of partial annotations as precursor to further, human annotation. Their experiments do not extend to unsupervised training. Tsuboi et al. (2008) used data that was not full annotated. However, their settin"
W12-1905,P07-1094,0,0.0716994,"Missing"
W12-1905,C10-2052,1,0.826539,"ein et al. (2009) study the utility of partial annotations as precursor to further, human annotation. Their experiments do not extend to unsupervised training. Tsuboi et al. (2008) used data that was not full annotated. However, their setting is in principle supervised, only few words are missing. Instead of no labels, those words have a limited number of possible alternatives. This works well for tasks with a small label alphabet or data where annotators left multiple options for some words. In contrast, we start out with unannotated data and assume that some words can be labeled. Gao et al. (2010) present a successful word alignment approach that uses partial annotations. These are derived from human annotation or heuristics. Their method improves BLEU, but requires some modification of the EM framework. 7 Conclusion and Future Work It is obvious, and common knowledge, that providing some annotation to an unsupervised algorithm will improve accuracy and learning speed. Surprisingly, however, our literature search did not turn up any papers stating exactly how and to what degree the improvements appear. We therefore selected a very general training method, EM, and a simple approach to i"
W12-1905,P11-2056,1,0.851936,"emEval 2007 task, (Litkowski and Hargraves, 2007)). There are three elements in the syntactic structure of prepositional phrases, namely the head word h (usually a noun, verb, or adjective), the preposition p, and the object of the preposition, o. The triple (h, p, o) forms a syntactically and semantically constrained structure. This structure is reflected in dependency parses as a common construction. Tratz and Hovy (2009) show how to use the dependency structure to solve it. Their method outperformed the previous state-of-the-art (which used a window-based approach) by a significant margin. Hovy et al. (2011) showed how the sequential nature of the problem can be exploited in unsupervised learning. They present various sequential models and training options. They compare a standard bigram HMM and a very complex model that is designed to capture mutual constraints. In contrast to them, we use a trigram HMM, but move the preposition at the end of the observed sequence, to condition it on the previous words. As suggested there, we use EM with smoothing and random restarts. WordNet lexicographer senses as labels for the arguments. It has 45 labels for nouns, verbs, and adjectives and is thus roughly c"
W12-1905,D07-1031,0,0.0206166,"m with another token: there are now fewer instances from which we collect C(on|sense5 ). The fractional counts for our transition parameters are not affected by this, but the counts for emission parameter are skewed. We thus essentially siphon probability mass from P (on|sense5 ) and move it to P (on : sense5 |sense5 ). Since the test data never contains labels such as sense5 , our partial annotations have moved a large amount of probability mass to a useless parameter: we are never going to use P (on : sense5 |sense5 ) during inference! Secondly, since EM tends to find uniform distributions (Johnson, 2007), other, rarer labels will also have to receive some probability. The counts for labels with partial annotations are fixed, so in order to use the rare labels (for which we have no partial annotations), their emission counts need to come from unlabeled instances. Say sense1 is a label for which we have no partial annotations. Every time EM collects emission counts from a word “on” (and not a labeled version “on:sensen ”), it assigns some of it to P (on|sense1 ). Effectively, we thus assign too much probability mass to the emission of the word from rare labels. The result of these two effects i"
W12-1905,S07-1005,0,0.0197228,"e use, we often also know which labels are applicable to which 32 While the technique is mainly useful for problems where only few labeled examples are available, we make use of a corpus of annotated data. This allows us to control the effect of the amount and type of annotated data on accuracy. We evaluate the impact of partial annotations on two tasks: preposition sense disambiguation and POS tagging. 2.2 Preposition Sense Disambiguation Prepositions are ubiquitous and highly ambiguous. Disambiguating prepositions is thus a challenging and interesting task in itself (see SemEval 2007 task, (Litkowski and Hargraves, 2007)). There are three elements in the syntactic structure of prepositional phrases, namely the head word h (usually a noun, verb, or adjective), the preposition p, and the object of the preposition, o. The triple (h, p, o) forms a syntactically and semantically constrained structure. This structure is reflected in dependency parses as a common construction. Tratz and Hovy (2009) show how to use the dependency structure to solve it. Their method outperformed the previous state-of-the-art (which used a window-based approach) by a significant margin. Hovy et al. (2011) showed how the sequential natu"
W12-1905,J94-2001,0,0.305076,"dom restarts. This indicates that the same accuracy can be reached with fewer restarts, which speeds up training time considerably. Our contributions are: • we show how to include partial annotations in EM training via parameter tying • we show how the amounts and distribution of partial annotations influence accuracy • we evaluate our method on an existing data set, comparing to both supervised and unsupervised methods on two tasks 2 2.1 observations. This is encoded in a dictionary. For POS-tagging, it narrows the possible tags for each word–irrespective of context–down to a manageable set. Merialdo (1994) showed how the amount of available dictionary information is correlated with performance. However, dictionaries list all applicable labels per word, regardless of context. We can often restrict the applicable label for an observation in a specific context even more. We extend this to include constraints applied to some, but not all instances. This allows us to restrict the choice for an observation to one label. We substitute the word in case by a special token with just one label. Based on simple heuristics, we can annotate individual words in the training data with their label. For example,"
W12-1905,H94-1048,0,0.226441,"Missing"
W12-1905,P09-1057,0,0.0204689,"repeated restarts with random initialization can help escape local maxima, but in order to find the global optimum, we need to run a great number (100 or more) of them (Ravi and Knight, 2009; Hovy et al., 2011). However, there is another solution. Various papers have shown that the inclusion of some knowledge greatly enhances performance of unsupervised systems. They introduce constraints on the initial model and the parameters. This directs the learning algorithm towards a better parameter configuration. Types of constraints include ILP-based methods (Chang et al., 2007; Chang et al., 2008; Ravi and Knight, 2009), and posterior regularization (Grac¸a et al., 2007; Ganchev et al., 2010). While those approaches are powerful and yield good results, they require us to reformulate the constraints in a certain language, and either use an external solver, or re-design parts of the maximization step. This is time-consuming and requires a certain expertise. For many NLP tasks, EM-trained HMMs are the common models. However, in order to escape local maxima and find the best model, we need to start with a good initial model. Researchers suggested repeated random restarts or constraints that guide the model evolu"
W12-1905,W09-3003,0,0.0303741,"to improve the data likelihood, it can get stuck in local maxima. Merialdo (1994) showed how the the initialized model influences the outcome after a fixed number of iterations. The importance is underscored succinctly by Goldberg et al. (2008). They experiment with various constraints. The idea of using partial annotations has been explored in various settings. Druck et al. (2008) present an approach to label features instead of instances for discriminative probabilistic models, yielding substantial improvements. They also study the effectiveness of labeling features vs. labeling instances. Rehbein et al. (2009) study the utility of partial annotations as precursor to further, human annotation. Their experiments do not extend to unsupervised training. Tsuboi et al. (2008) used data that was not full annotated. However, their setting is in principle supervised, only few words are missing. Instead of no labels, those words have a limited number of possible alternatives. This works well for tasks with a small label alphabet or data where annotators left multiple options for some words. In contrast, we start out with unannotated data and assume that some words can be labeled. Gao et al. (2010) present a"
W12-1905,P05-1044,0,0.0677564,"Missing"
W12-1905,N09-3017,1,0.799359,"on and POS tagging. 2.2 Preposition Sense Disambiguation Prepositions are ubiquitous and highly ambiguous. Disambiguating prepositions is thus a challenging and interesting task in itself (see SemEval 2007 task, (Litkowski and Hargraves, 2007)). There are three elements in the syntactic structure of prepositional phrases, namely the head word h (usually a noun, verb, or adjective), the preposition p, and the object of the preposition, o. The triple (h, p, o) forms a syntactically and semantically constrained structure. This structure is reflected in dependency parses as a common construction. Tratz and Hovy (2009) show how to use the dependency structure to solve it. Their method outperformed the previous state-of-the-art (which used a window-based approach) by a significant margin. Hovy et al. (2011) showed how the sequential nature of the problem can be exploited in unsupervised learning. They present various sequential models and training options. They compare a standard bigram HMM and a very complex model that is designed to capture mutual constraints. In contrast to them, we use a trigram HMM, but move the preposition at the end of the observed sequence, to condition it on the previous words. As s"
W12-1905,C08-1113,0,0.126176,"of iterations. The importance is underscored succinctly by Goldberg et al. (2008). They experiment with various constraints. The idea of using partial annotations has been explored in various settings. Druck et al. (2008) present an approach to label features instead of instances for discriminative probabilistic models, yielding substantial improvements. They also study the effectiveness of labeling features vs. labeling instances. Rehbein et al. (2009) study the utility of partial annotations as precursor to further, human annotation. Their experiments do not extend to unsupervised training. Tsuboi et al. (2008) used data that was not full annotated. However, their setting is in principle supervised, only few words are missing. Instead of no labels, those words have a limited number of possible alternatives. This works well for tasks with a small label alphabet or data where annotators left multiple options for some words. In contrast, we start out with unannotated data and assume that some words can be labeled. Gao et al. (2010) present a successful word alignment approach that uses partial annotations. These are derived from human annotation or heuristics. Their method improves BLEU, but requires s"
W12-1905,P10-2039,0,0.026539,"Missing"
W12-1905,C08-1142,1,0.889626,"Missing"
W13-0907,W10-0303,0,0.0373687,"ly concerned with the former. 52 Proceedings of the First Workshop on Metaphor in NLP, pages 52–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics in certain syntactic relations, they are usually considered semantic preference violations; e.g., in the metaphorical “You will have to eat your words”, the food-related verb heads a noun of communication. In contrast, with the literal sense of “eat” in “You will have to eat your peas”, it heads a food noun. This intuition is the basis of the approaches in (Iverson and Helmreich, 1991; Krishnakumaran and Zhu, 2007; Baumer et al., 2010; Turney et al., 2011).3 We generalize this intuition beyond preference selections of verbs and relational nouns. Given enough labeled examples of a word, we expect to find distinctive differences in the compositional behavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syntactically related wo"
W13-0907,E06-1042,0,0.0651925,"Missing"
W13-0907,W07-0104,0,0.196907,"Missing"
W13-0907,P04-1054,0,0.358462,"istinctive differences in the compositional behavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syntactically related words makes dependency tree kernels a natural fit for the problem. Tree kernels have been successfully applied to a wide range of NLP tasks that involve (syntactic) relations (Culotta and Sorensen, 2004; Moschitti, 2006; Qian et al., 2008; Giuliano et al., 2009; Mirroshandel et al., 2011). Our contributions in this paper are: whether a particular instance is used metaphorically, we set up an annotation task on Amazon Mechanical Turk (AMT). Annotators were asked to decide whether a highlighted expression in a sentence was used metaphorically or not (see Figure 2 for a screenshot). They were prompted to think about whether the expression was used in its original meaning.5 In some cases, it is not clear whether an expression is used metaphorically or not (usually in short sentences such as “Tha"
W13-0907,J09-1005,0,0.0130978,"d of 3872 instances, 1749 of them labeled as metaphors. • we annotate and release a corpus of 3872 instances for supervised metaphor classification • we are the first to use tree kernels for metaphor identification • our approach achieves an F1-score of 0.75, the best score of of all systems tested. 2 Data 2.1 Annotation We downloaded a list of 329 metaphor examples from the web4 . For each expression, we extracted sentences from the Brown corpus that contained the seed (see Figure 1 for an example). To decide 3 A similar assumption can be used to detect the literal/nonliteral uses of idioms (Fazly et al., 2009). 4 http://www.metaphorlist.com and http:// www.macmillandictionaryblog.com 53 Figure 2: Screenshot of the annotation interface on Amazon’s Mechanical Turk We divided the data into training, dev, and test sets, using a 80-10-10 split. All results reported here were obtained on the test set. Tuning and development was only carried out on the dev set. 2.2 Vector Representation of Words The same word may occur in a literal and a metaphorical usage. Lexical information alone is 5 While this is somewhat imprecise and not always easy to decide, it proved to be a viable strategy for untrained annotat"
W13-0907,W06-3506,0,0.322276,"Missing"
W13-0907,J09-4007,0,0.0284105,"ral and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syntactically related words makes dependency tree kernels a natural fit for the problem. Tree kernels have been successfully applied to a wide range of NLP tasks that involve (syntactic) relations (Culotta and Sorensen, 2004; Moschitti, 2006; Qian et al., 2008; Giuliano et al., 2009; Mirroshandel et al., 2011). Our contributions in this paper are: whether a particular instance is used metaphorically, we set up an annotation task on Amazon Mechanical Turk (AMT). Annotators were asked to decide whether a highlighted expression in a sentence was used metaphorically or not (see Figure 2 for a screenshot). They were prompted to think about whether the expression was used in its original meaning.5 In some cases, it is not clear whether an expression is used metaphorically or not (usually in short sentences such as “That’s sweet”), so annotators could state that it was not poss"
W13-0907,E12-1019,1,0.900764,"the input sentence with the FANSE parser (Tratz and Hovy, 2011)6 . It provides the dependency structure, POS tags, and other information. To construct the different tree representations, we replace each node in the tree with its word, lemma, POS tag, dependency label, or supersense (the WordNet lexicographer name of the word’s first sense (Fellbaum, 1998)), and mark the word in question with a special node. See Figure 3 for a graphical representation. These trees are used in addition to the vectors. This approach is similar to the ones described in (Moschitti et al., 2006; Qian et al., 2008; Hovy et al., 2012). 2.4 Classification Models the sweet in Boston DT JJ IN NNP O adj.all O n.location Figure 3: Graphic demonstration of our approach. a) dependency tree over words, with node of interest labeled. b) as POS representation. c) as supersense representation The intuition behind our approach is that metaphorical use differs from literal use in certain syntactic relations. For example, the only difference between the two sentences “I like the sweet people in Boston” and “I like the sweet pies in Boston” is the head of “sweet”. Our assumption is that—given enough examples—certain patterns emerge (e.g."
W13-0907,N13-1132,1,0.381717,"on was used in its original meaning.5 In some cases, it is not clear whether an expression is used metaphorically or not (usually in short sentences such as “That’s sweet”), so annotators could state that it was not possible to decide. We paid $0.09 for each set of 10 instances. Each instance was annotated by 7 annotators. Instances where the annotators agreed that it was impossible to tell whether it is a metaphor or not were discarded. Inter-annotator agreement was 0.57, indicating a difficult task. In order to get the label for each instance, we weighted the annotator’s answers using MACE (Hovy et al., 2013), an implementation of an unsupervised item-response model. This weighted voting produces more reliable estimates than simple majority voting, since it is capable of sorting out unreliable annotators. The final corpus consisted of 3872 instances, 1749 of them labeled as metaphors. • we annotate and release a corpus of 3872 instances for supervised metaphor classification • we are the first to use tree kernels for metaphor identification • our approach achieves an F1-score of 0.75, the best score of of all systems tested. 2 Data 2.1 Annotation We downloaded a list of 329 metaphor examples from"
W13-0907,P91-1049,0,0.13536,"hich she calls recognition) and interpretation. We are solely concerned with the former. 52 Proceedings of the First Workshop on Metaphor in NLP, pages 52–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics in certain syntactic relations, they are usually considered semantic preference violations; e.g., in the metaphorical “You will have to eat your words”, the food-related verb heads a noun of communication. In contrast, with the literal sense of “eat” in “You will have to eat your peas”, it heads a food noun. This intuition is the basis of the approaches in (Iverson and Helmreich, 1991; Krishnakumaran and Zhu, 2007; Baumer et al., 2010; Turney et al., 2011).3 We generalize this intuition beyond preference selections of verbs and relational nouns. Given enough labeled examples of a word, we expect to find distinctive differences in the compositional behavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anom"
W13-0907,W07-0103,0,0.0363077,"nd interpretation. We are solely concerned with the former. 52 Proceedings of the First Workshop on Metaphor in NLP, pages 52–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics in certain syntactic relations, they are usually considered semantic preference violations; e.g., in the metaphorical “You will have to eat your words”, the food-related verb heads a noun of communication. In contrast, with the literal sense of “eat” in “You will have to eat your peas”, it heads a food noun. This intuition is the basis of the approaches in (Iverson and Helmreich, 1991; Krishnakumaran and Zhu, 2007; Baumer et al., 2010; Turney et al., 2011).3 We generalize this intuition beyond preference selections of verbs and relational nouns. Given enough labeled examples of a word, we expect to find distinctive differences in the compositional behavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syn"
W13-0907,J04-1002,0,0.0959412,"as new metaphors, approaches that try to identify them based on lexical features alone are bound to be unsuccessful. Some approaches have therefore suggested considering distributional properties and “abstractness” of the phrase (Turney et al., 2011). This nicely captures the contextual nature of metaphors, but their ubiquity makes it impossible to find truly “clean” data to learn the separate distributions of metaphorical and literal use for each word. Other approaches have used pre-defined mappings from a source to a target domain, as in “X is like Y”, e.g., “emotions are like temperature” (Mason, 2004). These approaches tend to do well on the defined mappings, but they do not generalize to new, creative metaphors. It is doubtful that it is feasible to list all possible mappings, so these approaches remain brittle. In contrast, we do not assume any predefined mappings. We hypothesize instead that if we interpreted every word literally, metaphors will manifest themselves as unusual semantic compositions. Since these compositions most frequently occur 2 Shutova (2010) distinguishes between metaphor identification (which she calls recognition) and interpretation. We are solely concerned with th"
W13-0907,E06-1015,0,0.117392,"e compositional behavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syntactically related words makes dependency tree kernels a natural fit for the problem. Tree kernels have been successfully applied to a wide range of NLP tasks that involve (syntactic) relations (Culotta and Sorensen, 2004; Moschitti, 2006; Qian et al., 2008; Giuliano et al., 2009; Mirroshandel et al., 2011). Our contributions in this paper are: whether a particular instance is used metaphorically, we set up an annotation task on Amazon Mechanical Turk (AMT). Annotators were asked to decide whether a highlighted expression in a sentence was used metaphorically or not (see Figure 2 for a screenshot). They were prompted to think about whether the expression was used in its original meaning.5 In some cases, it is not clear whether an expression is used metaphorically or not (usually in short sentences such as “That’s sweet”), so a"
W13-0907,R11-2018,0,0.0408968,"Missing"
W13-0907,C08-1088,0,0.208095,"ehavior of its literal and metaphorical uses in certain preferred syntactic relationships. If we can learn to detect such differences/anomalies, we can reliably identify metaphors. Since we expect these patterns in levels other than the lexical level, the approach expands well to creative metaphors. The observation that the anomaly tends to occur between syntactically related words makes dependency tree kernels a natural fit for the problem. Tree kernels have been successfully applied to a wide range of NLP tasks that involve (syntactic) relations (Culotta and Sorensen, 2004; Moschitti, 2006; Qian et al., 2008; Giuliano et al., 2009; Mirroshandel et al., 2011). Our contributions in this paper are: whether a particular instance is used metaphorically, we set up an annotation task on Amazon Mechanical Turk (AMT). Annotators were asked to decide whether a highlighted expression in a sentence was used metaphorically or not (see Figure 2 for a screenshot). They were prompted to think about whether the expression was used in its original meaning.5 In some cases, it is not clear whether an expression is used metaphorically or not (usually in short sentences such as “That’s sweet”), so annotators could sta"
W13-0907,C10-1113,0,0.0811424,"Missing"
W13-0907,P10-1071,0,0.0229229,"ches have used pre-defined mappings from a source to a target domain, as in “X is like Y”, e.g., “emotions are like temperature” (Mason, 2004). These approaches tend to do well on the defined mappings, but they do not generalize to new, creative metaphors. It is doubtful that it is feasible to list all possible mappings, so these approaches remain brittle. In contrast, we do not assume any predefined mappings. We hypothesize instead that if we interpreted every word literally, metaphors will manifest themselves as unusual semantic compositions. Since these compositions most frequently occur 2 Shutova (2010) distinguishes between metaphor identification (which she calls recognition) and interpretation. We are solely concerned with the former. 52 Proceedings of the First Workshop on Metaphor in NLP, pages 52–57, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics in certain syntactic relations, they are usually considered semantic preference violations; e.g., in the metaphorical “You will have to eat your words”, the food-related verb heads a noun of communication. In contrast, with the literal sense of “eat” in “You will have to eat your peas”, it heads a food noun. T"
W13-0907,D11-1116,1,0.806458,"essive way. We use the existing vector representation SENNA (Collobert et al., 2011) which is derived from contextual similarity. In it, semantically similar words are represented by similar vectors, without us having to define similarity or looking at the word itself. In initial tests, these vectors performed better than binary vectors straightforwardly derived from features of the word in context. 2.3 Constructing Trees a) b) VB like I people PRP c) v.emotion O NNS n.group dependency trees, and the different levels from various annotations. We parse the input sentence with the FANSE parser (Tratz and Hovy, 2011)6 . It provides the dependency structure, POS tags, and other information. To construct the different tree representations, we replace each node in the tree with its word, lemma, POS tag, dependency label, or supersense (the WordNet lexicographer name of the word’s first sense (Fellbaum, 1998)), and mark the word in question with a special node. See Figure 3 for a graphical representation. These trees are used in addition to the vectors. This approach is similar to the ones described in (Moschitti et al., 2006; Qian et al., 2008; Hovy et al., 2012). 2.4 Classification Models the sweet in Bosto"
W13-0907,D11-1063,0,0.704648,"fixed expression with the metaphorical meaning now accepted as just another sense, no longer recognized as metaphorical at all. This gradient makes it hard to determine a boundary between literal and metaphorical use of some expressions. Identifying metaphors is thus a difficult but important step in language understanding.2 Since many words can be productively used as new metaphors, approaches that try to identify them based on lexical features alone are bound to be unsuccessful. Some approaches have therefore suggested considering distributional properties and “abstractness” of the phrase (Turney et al., 2011). This nicely captures the contextual nature of metaphors, but their ubiquity makes it impossible to find truly “clean” data to learn the separate distributions of metaphorical and literal use for each word. Other approaches have used pre-defined mappings from a source to a target domain, as in “X is like Y”, e.g., “emotions are like temperature” (Mason, 2004). These approaches tend to do well on the defined mappings, but they do not generalize to new, creative metaphors. It is doubtful that it is feasible to list all possible mappings, so these approaches remain brittle. In contrast, we do no"
W13-0907,W06-2607,0,\N,Missing
W13-1203,bejan-harabagiu-2008-linguistic,0,0.549061,"M. Felisa Vedejo from UNED Madrid. Since nobody has complete knowledge, the author’s mental image of the entity or event in question might differ from the reader’s, and from the truth. Specifically, the properties the author assumes for the event or entity might not be the ones the reader assumes. This difference has deep consequences for the treatment of the semantic meaning of a text. In particular, it fundamentally affects how one must perform coreference among entities or events. As discussed in Section 6, events have been the focus of study in both Linguistics and NLP (Chen and Ji, 2009; Bejan and Harabagiu, 2008, 2010; Humphreys et al., 1997). Determining when two event mentions in text corefer is, however, an unsolved problem2. Past work in NLP has avoided some of the more complex problems by considering only certain types of coreference, or by simply ignoring the major problems. The results have been partial, or inconsistent, annotations. In this paper we describe our approach to the problem of coreference among events. In order to build a corpus containing event coreference links that is annotated with high enough inter-annotator agreement to be useful for machine learning, it has proven necessary"
W13-1203,P10-1143,0,0.495771,"Missing"
W13-1203,W09-3208,0,0.346808,"Missing"
W13-1203,N07-1011,0,0.0222776,"o so-called Identity Criteria. Guarino (1999) outlines several ‘dimensions’ along which entities can remain identical or change under transformations; for example, a glass before and after it is crushed is identical with respect to its matter but not its shape; the ACL now and one hundred years hence is (probably) identical as an organization but not in its membership. There has not been much theoretical work on semantic identity in the NLP community. But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Culotta et al., 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009). Focusing on event coreference are (Humphries et al., 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010). Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al., 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Kučová and Hajičová. 2004), the ACE corpus (Walker et al., 2006), and"
W13-1203,P08-2012,0,0.0229614,"99) outlines several ‘dimensions’ along which entities can remain identical or change under transformations; for example, a glass before and after it is crushed is identical with respect to its matter but not its shape; the ACL now and one hundred years hence is (probably) identical as an organization but not in its membership. There has not been much theoretical work on semantic identity in the NLP community. But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Culotta et al., 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009). Focusing on event coreference are (Humphries et al., 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010). Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al., 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Kučová and Hajičová. 2004), the ACE corpus (Walker et al., 2006), and OntoNotes (Pradhan et al., 2007). Most simila"
W13-1203,D10-1033,0,0.123314,"Missing"
W13-1203,hasler-etal-2006-nps,0,0.0775551,"” are identical. In “she was elected President” / “she took office as President”, it is more difficult to decide. Does being elected automatically entail taking office? In some political systems it may, and in others it may not. When in doubt, we treat the case as only quasiidentical. Thus, comparing to examples from FullIdentity: Paraphrase, the following are only quasiidentical because of additional information: “she sold the book” / “she sold Peter the book”; “she sold Peter the book” / “Peter got [not bought] the book from her”. Quasi-identity has been considered in coreference before in (Hasler et al., 2006) but not as extensively, and in (Recasens and Hovy, 2010a; 2011) but applied only to entities. When applied to events, the issue becomes more complex. 3 Two Problems 3.1 Domain and Reporting Events As described above, inconsistent reporting occurs when a DE stated in reported text contains significant differences from the author’s description of the same DE. To handle such cases we have found it necessary to additionally identify communication events, which we call Reportings, during annotation because they provide a context in which a DE is stated. We identify two principal types of Reporting"
W13-1203,W97-1311,0,0.756043,". Since nobody has complete knowledge, the author’s mental image of the entity or event in question might differ from the reader’s, and from the truth. Specifically, the properties the author assumes for the event or entity might not be the ones the reader assumes. This difference has deep consequences for the treatment of the semantic meaning of a text. In particular, it fundamentally affects how one must perform coreference among entities or events. As discussed in Section 6, events have been the focus of study in both Linguistics and NLP (Chen and Ji, 2009; Bejan and Harabagiu, 2008, 2010; Humphreys et al., 1997). Determining when two event mentions in text corefer is, however, an unsolved problem2. Past work in NLP has avoided some of the more complex problems by considering only certain types of coreference, or by simply ignoring the major problems. The results have been partial, or inconsistent, annotations. In this paper we describe our approach to the problem of coreference among events. In order to build a corpus containing event coreference links that is annotated with high enough inter-annotator agreement to be useful for machine learning, it has proven necessary to create a model of event ide"
W13-1203,W04-0208,0,0.037375,"dentical as an organization but not in its membership. There has not been much theoretical work on semantic identity in the NLP community. But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Culotta et al., 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009). Focusing on event coreference are (Humphries et al., 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010). Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al., 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Kučová and Hajičová. 2004), the ACE corpus (Walker et al., 2006), and OntoNotes (Pradhan et al., 2007). Most similar to our work is that of (Hasler et al., 2006). In that study, coreferential events and their arguments (also coreference between the arguments) were annotated for the terrorism/security domain, considering five event categories (attack, defend, injure, die, contact), and five"
W13-1203,N09-1065,0,0.0251365,"uarino (1999) outlines several ‘dimensions’ along which entities can remain identical or change under transformations; for example, a glass before and after it is crushed is identical with respect to its matter but not its shape; the ACL now and one hundred years hence is (probably) identical as an organization but not in its membership. There has not been much theoretical work on semantic identity in the NLP community. But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Culotta et al., 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009). Focusing on event coreference are (Humphries et al., 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010). Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al., 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Kučová and Hajičová. 2004), the ACE corpus (Walker et al., 2006), and OntoNotes (Pradhan"
W13-1203,W05-0311,0,0.0347599,"r but not its shape; the ACL now and one hundred years hence is (probably) identical as an organization but not in its membership. There has not been much theoretical work on semantic identity in the NLP community. But there has been a considerable amount of work on the problem of coreference. Focusing on entity coreference are (McCarthy and Lehnert, 1995; Culotta et al., 2007; Ng, 2007; Ng, 2009; Finkel and Manning, 2008; Ng, 2009). Focusing on event coreference are (Humphries et al., 1997; Chen and Zi, 2009; Bejan and Harabagiu, 2008; 2010). Anaphora and bridging reference are discussed in (Poesio and Artstein, 2005; 2007). Relevant to events is the TIME-ML corpus (Mani and Pustejovsky, 2004; Pustejovsky et al., 2003), which provides a specification notation for events and temporal expressions. Several corpora contain annotations for entity coreference, including the Prague Dependency Treebank (Kučová and Hajičová. 2004), the ACE corpus (Walker et al., 2006), and OntoNotes (Pradhan et al., 2007). Most similar to our work is that of (Hasler et al., 2006). In that study, coreferential events and their arguments (also coreference between the arguments) were annotated for the terrorism/security domain, consi"
W13-1203,poesio-artstein-2008-anaphoric,0,0.0612666,"Missing"
W13-1203,P10-1144,1,0.848137,"e took office as President”, it is more difficult to decide. Does being elected automatically entail taking office? In some political systems it may, and in others it may not. When in doubt, we treat the case as only quasiidentical. Thus, comparing to examples from FullIdentity: Paraphrase, the following are only quasiidentical because of additional information: “she sold the book” / “she sold Peter the book”; “she sold Peter the book” / “Peter got [not bought] the book from her”. Quasi-identity has been considered in coreference before in (Hasler et al., 2006) but not as extensively, and in (Recasens and Hovy, 2010a; 2011) but applied only to entities. When applied to events, the issue becomes more complex. 3 Two Problems 3.1 Domain and Reporting Events As described above, inconsistent reporting occurs when a DE stated in reported text contains significant differences from the author’s description of the same DE. To handle such cases we have found it necessary to additionally identify communication events, which we call Reportings, during annotation because they provide a context in which a DE is stated. We identify two principal types of Reporting verbs: locutionary verbs “say”, “report”, “announce”, e"
W13-1203,taule-etal-2008-ancora,0,0.0458449,"Missing"
W13-3203,W11-0114,0,0.236415,"disambiguation (McCarthy et al., 2004; ∗ *Equally contributing authors 20 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20–29, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics addition and multiplication on the vectors to be composed, with slight variations. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. Pantel and Lin (2000) and Erk and Padó (2008) attempted to include syntactic context in distributional models. However, their approaches do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. A quasi-compositional approach was also attempted in Thater et al. (2010) by a systematic combination of first and second order context vectors. To the best of our knowledge the formulation"
W13-3203,D10-1115,0,0.136199,"), word-sense discrimination and disambiguation (McCarthy et al., 2004; ∗ *Equally contributing authors 20 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20–29, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics addition and multiplication on the vectors to be composed, with slight variations. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. Pantel and Lin (2000) and Erk and Padó (2008) attempted to include syntactic context in distributional models. However, their approaches do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. A quasi-compositional approach was also attempted in Thater et al. (2010) by a systematic combination of first and second order context vectors. To the best of"
W13-3203,W10-2805,0,0.0517608,"contributing authors 20 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20–29, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics addition and multiplication on the vectors to be composed, with slight variations. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. Pantel and Lin (2000) and Erk and Padó (2008) attempted to include syntactic context in distributional models. However, their approaches do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. A quasi-compositional approach was also attempted in Thater et al. (2010) by a systematic combination of first and second order context vectors. To the best of our knowledge the formulation of composition we propose is the firs"
W13-3203,P10-1143,0,0.126779,"Missing"
W13-3203,D09-1120,0,0.033666,"p). Event Coreference Judgment Given the SDSM formulation and assuming no sparsity constraints, it is possible to calculate SDSM matrices for composed concepts. However, are these correct? Intuitively, if they truly capture semantics, the two SDSM matrix representations for “Booth assassinated Lincoln” and “Booth shot Lincoln with a gun&quot; should be (almost) the same. To test this hypothesis we turn to the task of predicting whether two event mentions are coreferent or not, even if their surface forms differ. While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the two-way feedback between events and their arguments. In this paper, however, we only consider coreferentiality between pairs of events. Formally, two event mentions generally refer to the same event when their respective actions, agents, patients, locations, and times are (almost) the same. Given the non-compositional nature of determining equality of locations and times, we re"
W13-3203,W06-1670,0,0.0260257,"bed later. In addition to the words’ surface-forms, the PropStore also stores their POS tags, lemmas, and Wordnet supersenses. The PropStore can be used to query for preferred expectations of words, supersenses, relations, etc., around a given word. In the example in Figure 1, the query (SST(W1 ) 21 Figure 1: Sample sentences & triples = verb.consumption, ?, dobj) i.e., “what is consumed”, might return expectations [pasta:1, spaghetti:1, mice:1 . . . ]. In our implementation, the relations and POS tags are obtained using the Fanseparser (Tratz and Hovy, 2011), supersense tags using sst-light (Ciaramita and Altun, 2006), and lemmas are obtained from Wordnet (Miller, 1995). 3.2 Figure 2: Mimicking composition of two words Building the Representation Next, we describe a method to represent lexical entries as structured distributional matrices using the PropStore. The canonical form of a concept C (word, phrase etc.) in the SDSM framework is a matrix M C , whose entry MijC is a list of sentence identifiers obtained by querying the PropStore for contexts in which C appears in the syntactic neighborhood of the word j linked by the dependency relation i. As with other distributional models in the literature, the c"
W13-3203,W07-2415,0,0.0259648,"Missing"
W13-3203,D10-1113,0,0.0409168,"question answering (Tellex et al., 2003), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003), automated dictionary building (Curran, 2003), automated essay grading (Landauer and Dutnais, 1997), word-sense discrimination and disambiguation (McCarthy et al., 2004; ∗ *Equally contributing authors 20 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20–29, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics addition and multiplication on the vectors to be composed, with slight variations. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. Pantel and Lin (2000) and Erk and Padó (2008) attempted to include syntactic context in distributional models. However, their approaches do not explicitly construct phrase-level mea"
W13-3203,D12-1045,0,0.0328327,"sed concepts. However, are these correct? Intuitively, if they truly capture semantics, the two SDSM matrix representations for “Booth assassinated Lincoln” and “Booth shot Lincoln with a gun&quot; should be (almost) the same. To test this hypothesis we turn to the task of predicting whether two event mentions are coreferent or not, even if their surface forms differ. While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the two-way feedback between events and their arguments. In this paper, however, we only consider coreferentiality between pairs of events. Formally, two event mentions generally refer to the same event when their respective actions, agents, patients, locations, and times are (almost) the same. Given the non-compositional nature of determining equality of locations and times, we represent each event mention by a triple E = (e, a, p) for the event, agent, and patient. While linguistic theory of argument realization is a"
W13-3203,D08-1094,0,0.29447,"tly disregards syntactic structure. For instance, the distributions for “Lincoln”, “Booth”, and “killed” when merged produce the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. As suggested by Pantel and Lin (2000) and others, modeling the distribution over preferential attachments for each syntactic relation separately can yield greater expressive power. Attempts have been made to model linguistic composition of individual word vectors (Mitchell and Lapata, 2009), as well as remedy the inherent failings of the standard distributional approach (Erk and Padó, 2008). The results show varying degrees of efficacy, but have largely failed to model deeper lexical semantics or compositional expectations of words and word combinations. In this paper we propose an extension to the traditional DSM model that explicitly preserves structural information and permits the approximation of distributional expectation over dependency relations. We extend the generic DSM model by representing a word as distributions over relationspecific syntactic neighborhoods. One can think of the Structured DSM (SDSM) representation of a word/phrase as several vectors defined over the"
W13-3203,P07-1028,0,0.351904,"Missing"
W13-3203,J03-4004,0,0.175022,"ns on an artificial task of verb sense disambiguation and a real-world application of judging event coreference. 1 Introduction With the advent of statistical methods for NLP, Distributional Semantic Models (DSMs) have emerged as powerful method for representing word semantics. In particular, the distributional vector formalism, which represents meaning by a distribution over neighboring words, has gained the most popularity. DSMs are widely used in information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003), automated dictionary building (Curran, 2003), automated essay grading (Landauer and Dutnais, 1997), word-sense discrimination and disambiguation (McCarthy et al., 2004; ∗ *Equally contributing authors 20 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20–29, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics addition and multiplication on the vectors to be composed, with slight variations. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent"
W13-3203,P04-1036,0,0.031433,"Distributional Semantic Models (DSMs) have emerged as powerful method for representing word semantics. In particular, the distributional vector formalism, which represents meaning by a distribution over neighboring words, has gained the most popularity. DSMs are widely used in information retrieval (Manning et al., 2008), question answering (Tellex et al., 2003), semantic similarity computation (Wong and Raghavan, 1984; McCarthy and Carroll, 2003), automated dictionary building (Curran, 2003), automated essay grading (Landauer and Dutnais, 1997), word-sense discrimination and disambiguation (McCarthy et al., 2004; ∗ *Equally contributing authors 20 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20–29, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics addition and multiplication on the vectors to be composed, with slight variations. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regressio"
W13-3203,P09-1074,0,0.019613,"ment Given the SDSM formulation and assuming no sparsity constraints, it is possible to calculate SDSM matrices for composed concepts. However, are these correct? Intuitively, if they truly capture semantics, the two SDSM matrix representations for “Booth assassinated Lincoln” and “Booth shot Lincoln with a gun&quot; should be (almost) the same. To test this hypothesis we turn to the task of predicting whether two event mentions are coreferent or not, even if their surface forms differ. While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the two-way feedback between events and their arguments. In this paper, however, we only consider coreferentiality between pairs of events. Formally, two event mentions generally refer to the same event when their respective actions, agents, patients, locations, and times are (almost) the same. Given the non-compositional nature of determining equality of locations and times, we represent each event ment"
W13-3203,P08-1028,0,0.823335,"of composed units. The overarching theme of our framework of evaluation is to explore the semantic space of the SDSM. We do this by measuring its ability to discriminate between varying surface forms of the same underlying concept. We perform the following set of experiments to evaluate its expressive power, and conclude the following: 1. Experiments with single words on similarity scoring and substitute selection: SDSM performs at par with window-based distributional vectors. 2. Experiments with phrasal units on two-word composition: state-of-the-art results are produced on the dataset from Mitchell and Lapata (2008) in terms of correlation with human judgment. 3. Experiments with larger structures on the task of judging event coreferentiality: SDSM shows superior performance over state-ofthe-art window-based word embeddings, and simple models for composing distributional semantic representations. 2 3 Structured Distributional Semantics In this section, we describe our Structured Distributional Semantic framework in detail. We first build a large knowledge base from sample english texts and use it to represent basic lexical units. Next, we describe a technique to obtain the representation for larger units"
W13-3203,D09-1045,0,0.0152133,"f lexical constituents. It follows that the DSM formalism lends itself poorly to composition since it implicitly disregards syntactic structure. For instance, the distributions for “Lincoln”, “Booth”, and “killed” when merged produce the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. As suggested by Pantel and Lin (2000) and others, modeling the distribution over preferential attachments for each syntactic relation separately can yield greater expressive power. Attempts have been made to model linguistic composition of individual word vectors (Mitchell and Lapata, 2009), as well as remedy the inherent failings of the standard distributional approach (Erk and Padó, 2008). The results show varying degrees of efficacy, but have largely failed to model deeper lexical semantics or compositional expectations of words and word combinations. In this paper we propose an extension to the traditional DSM model that explicitly preserves structural information and permits the approximation of distributional expectation over dependency relations. We extend the generic DSM model by representing a word as distributions over relationspecific syntactic neighborhoods. One can"
W13-3203,P10-1097,0,0.0974385,"equently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. Pantel and Lin (2000) and Erk and Padó (2008) attempted to include syntactic context in distributional models. However, their approaches do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. A quasi-compositional approach was also attempted in Thater et al. (2010) by a systematic combination of first and second order context vectors. To the best of our knowledge the formulation of composition we propose is the first to account for K and r within the general framework of composition c = f (a, b, r, K). word’s selectional preferences for a different syntactic argument. We argue that this representation captures individual word semantics effectively, and is better able to express the semantics of composed units. The overarching theme of our framework of evaluation is to explore the semantic space of the SDSM. We do this by measuring its ability to discrim"
W13-3203,J88-2003,0,0.354452,"r arguments. In this paper, however, we only consider coreferentiality between pairs of events. Formally, two event mentions generally refer to the same event when their respective actions, agents, patients, locations, and times are (almost) the same. Given the non-compositional nature of determining equality of locations and times, we represent each event mention by a triple E = (e, a, p) for the event, agent, and patient. While linguistic theory of argument realization is a debated research area (Levin and Rappaport Hovav, 2005; Goldberg, 2005), it is commonly believed that event structure (Moens and Steedman, 1988) centralizes on the predicate, which governs and selects its role arguments (Jackendoff, 1987). In the corpora we use for our experiments, most event mentions are verbs. However, when nominalized events are encountered, we replace them by their verbal forms. We use SRL Collobert et al. (2011) to determine the agent and patient arguments of an event mention. When SRL fails to determine either role, its empirical substitutes are obtained by querying the PropStore for the most likely word expectations for the To judge coreference between events E1 and E2, we compute pairwise similarities Sim(M 1f"
W13-3203,A00-2011,0,0.513262,"ssumption: that the meaning of a phrase can be reasonably estimated from the meaning of its constituents. However, semantics in natural language is a compositional phenomenon, encompassing interactions between syntactic structures, and the meaning of lexical constituents. It follows that the DSM formalism lends itself poorly to composition since it implicitly disregards syntactic structure. For instance, the distributions for “Lincoln”, “Booth”, and “killed” when merged produce the same result regardless of whether the input is “Booth killed Lincoln” or “Lincoln killed Booth”. As suggested by Pantel and Lin (2000) and others, modeling the distribution over preferential attachments for each syntactic relation separately can yield greater expressive power. Attempts have been made to model linguistic composition of individual word vectors (Mitchell and Lapata, 2009), as well as remedy the inherent failings of the standard distributional approach (Erk and Padó, 2008). The results show varying degrees of efficacy, but have largely failed to model deeper lexical semantics or compositional expectations of words and word combinations. In this paper we propose an extension to the traditional DSM model that expl"
W13-3203,D10-1048,0,0.0399698,"mulation and assuming no sparsity constraints, it is possible to calculate SDSM matrices for composed concepts. However, are these correct? Intuitively, if they truly capture semantics, the two SDSM matrix representations for “Booth assassinated Lincoln” and “Booth shot Lincoln with a gun&quot; should be (almost) the same. To test this hypothesis we turn to the task of predicting whether two event mentions are coreferent or not, even if their surface forms differ. While automated resolution of entity coreference has been an actively researched area (Haghighi and Klein, 2009; Stoyanov et al., 2009; Raghunathan et al., 2010), there has been relatively little work on event coreference resolution. Lee et al. (2012) perform joint cross-document entity and event coreference resolution using the two-way feedback between events and their arguments. In this paper, however, we only consider coreferentiality between pairs of events. Formally, two event mentions generally refer to the same event when their respective actions, agents, patients, locations, and times are (almost) the same. Given the non-compositional nature of determining equality of locations and times, we represent each event mention by a triple E = (e, a,"
W13-3203,P10-1093,0,0.0699981,"ing (Landauer and Dutnais, 1997), word-sense discrimination and disambiguation (McCarthy et al., 2004; ∗ *Equally contributing authors 20 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 20–29, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics addition and multiplication on the vectors to be composed, with slight variations. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. Pantel and Lin (2000) and Erk and Padó (2008) attempted to include syntactic context in distributional models. However, their approaches do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. A quasi-compositional approach was also attempted in Thater et al. (2010) by a systematic combination of first and second order c"
W13-3203,J98-1004,0,0.648114,"Missing"
W13-3203,D11-1097,0,0.204556,"Missing"
W13-3203,D12-1110,0,0.0703411,"us Vector Space Models and their Compositionality, pages 20–29, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics addition and multiplication on the vectors to be composed, with slight variations. Dinu and Lapata (2010) and Séaghdha and Korhonen (2011) introduced a probabilistic model to represent word meanings by a latent variable model. Subsequently, other high-dimensional extensions by Rudolph and Giesbrecht (2010), Baroni and Zamparelli (2010) and Grefenstette et al. (2011), regression models by Guevara (2010), and recursive neural network based solutions by Socher et al. (2012) and Collobert et al. (2011) have been proposed. Pantel and Lin (2000) and Erk and Padó (2008) attempted to include syntactic context in distributional models. However, their approaches do not explicitly construct phrase-level meaning from words which limits their applicability to real world problems. A quasi-compositional approach was also attempted in Thater et al. (2010) by a systematic combination of first and second order context vectors. To the best of our knowledge the formulation of composition we propose is the first to account for K and r within the general framework of composition c"
W13-3203,D11-1116,1,\N,Missing
W13-3203,S07-1009,0,\N,Missing
W14-2303,N10-1039,0,0.0984093,"Missing"
W14-2303,W02-0109,0,0.020937,"Missing"
W14-2303,W13-0904,0,0.236062,"rences) and the involved conceptual domains is required. Especially in the case of conceptual mappings, this makes it very difficult for automated systems to achieve appropriate coverage of metaphors. Even when limited to a single target domain, detecting all metaphors would require knowledge of many metaphoric source domains to cover all relevant mappings (which themselves have to be known, too). As a result of this, many systems attempt to achieve high precision for specific mappings, rather than provide general coverage. Many approaches (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Mohler et al., 2013; Tsvetkov et al., 2013, and more) make use of manually crafted knowledge bases such as WordNet or FrameNet to establish concept domains. Other recent works establish domains via topic modeling (Shutova et al., 2010; Heintz et al., 2013), ad-hoc clustering (Strzalkowski et al., 2013) or by using semantic similarity vectors (Hovy et al., 2013). We introduce term relevance as a measure for how “out of place” a word is in a given conMost computational approaches to metaphor detection try to leverage either conceptual metaphor mappings or selectional preferences. Both require extensive knowledge o"
W14-2303,P10-1044,0,0.0358614,"Missing"
W14-2303,P10-1045,0,0.0195087,"Missing"
W14-2303,C10-1113,0,0.304638,"Missing"
W14-2303,W06-3506,0,0.117936,"Missing"
W14-2303,P10-1071,0,0.0175148,"Missing"
W14-2303,E09-1086,0,0.0808377,"Missing"
W14-2303,W13-0908,0,0.0913364,"Missing"
W14-2303,W13-0909,0,0.0616927,"equire knowledge of many metaphoric source domains to cover all relevant mappings (which themselves have to be known, too). As a result of this, many systems attempt to achieve high precision for specific mappings, rather than provide general coverage. Many approaches (Gedigian et al., 2006; Krishnakumaran and Zhu, 2007; Mohler et al., 2013; Tsvetkov et al., 2013, and more) make use of manually crafted knowledge bases such as WordNet or FrameNet to establish concept domains. Other recent works establish domains via topic modeling (Shutova et al., 2010; Heintz et al., 2013), ad-hoc clustering (Strzalkowski et al., 2013) or by using semantic similarity vectors (Hovy et al., 2013). We introduce term relevance as a measure for how “out of place” a word is in a given conMost computational approaches to metaphor detection try to leverage either conceptual metaphor mappings or selectional preferences. Both require extensive knowledge of the mappings/preferences in question, as well as sufficient data for all involved conceptual domains. Creating these resources is expensive and often limits the scope of these systems. We propose a statistical approach to metaphor detection that utilizes the rarity of novel metapho"
W14-2303,W13-0907,1,0.673414,"Missing"
W14-2303,W13-0906,0,0.219985,"Missing"
W14-2303,W07-0103,0,0.253158,"Missing"
W14-2303,P06-4018,0,\N,Missing
W14-2910,W10-4305,0,0.0133783,"ht produce them in the process of detecting mentions. The assumption regarding twinless mentions has been investigated in research on entity coreference resolution. Cluster-based metrics such as B-CUBED and CEAF assume that a system is given true mentions without any twinless mentions in the gold standard, and then resolves full coreference on them. Researchers have made different assumptions about this issue. Early work such as (Ji et al., 2005) and (Bengtson and Roth, 2008) simply ignored such mentions. Rahman and Ng (2009) removed twinless mentions that are singletons in a system response. Cai and Strube (2010) proposed two variants of B-CUBED and CEAF that can deal with twinless mentions in order to make the evaluation of end-to-end coreference resolution system consistent. In evaluation of partial coreference where twinless mentions can also exist, we believe that the value of making evaluation consistent and comparable is the most important, and hypothesize that it is possible to effectively create a metric to measure the performance of partial coreference while dealing with twinless mentions. A potential problem of making a single metric handle twinless mentions is that the metric would not be i"
W14-2910,P08-1090,0,0.0401423,"tion extraction, question answering, textual entailment, and contradiction detection. A key challenge for event coreference resolution is that one can define several relations between two events, where some of them exhibit subtle deviation from perfect event identity. For clarification, we refer to perfect event identity as full (event) coreference in this paper. To address the subtlety in event identity, Hovy et al. (2013) focused on two types of partial event identity: subevent and membership. Subevent relations form a stereotypical sequence of events, or a script (Schank and Abelson, 1977; Chambers and Jurafsky, 2008). Membership relations represent instances of an event collection. We refer to both as partial (event) coreference in this paper. Figure 1 shows some examples of the subevent and Figure 1: Examples of subevent and membership relations. Solid and dashed arrows represent subevent and membership relations respectively, with the direction from a parent to its subevent or member. For example, we say that E4 is a subevent of E6. Solid lines without any arrow heads represent full coreference. In this paper, we address the problem of evalu68 Proceedings of the 2nd Workshop on EVENTS: Definition, Detec"
W14-2910,D11-1096,0,0.0177431,"ees is fairly complex. Another tree similarity metric is Simple Tree Matching (STM) (Yang, 1991). STM measures the similarity of two trees by counting the maximum match with dynamic programming. Although this algorithm was also originally developed for ordered trees, the underlying idea of the algorithm is simple, making it relatively easy to extend the algorithm for unordered trees. Tree kernels have been also widely studied and applied to NLP tasks, more specifically, to capture the similarity between parse trees (Collins and Duffy, 2001; Moschitti et al., 2008) or between dependency trees (Croce et al., 2011; Srivastava et al., 2013). This method is based on a supervised learning model with training data; hence we need a number of pairs of trees and associated numeric similarity values between these trees as input. Thus, it is not appropriate for an evaluation setting. ating the performance of a system that detects partial coreference in the context of event coreference resolution. This problem is important because, as with other tasks, a good evaluation method for partial coreference will facilitate future research on the task in a consistent and comparable manner. When one introduces a certain"
W14-2910,P13-1012,0,0.0128099,"erarchy that simplifies the evaluation process for partial event coreference. • We present a way to extend MUC, BLANC, and STM for the case of unordered trees. Those metrics are generic and flexible enough to be used in evaluations involving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric should satisfy. In this section, we first d"
W14-2910,N10-1145,0,0.0191369,"unclear how to define a cluster. The latter is not readily applicable to the evaluation because it is unclear how to penalize incorrect directions of links. We discuss these aspects in Section 4.1 and Section 4.2. Tree Edit Distance (TED) is one of the traditional algorithms for measuring tree similarity. It has a long history of theoretical studies (Tai, 1979; Zhang and Shasha, 1989; Klein, 1998; Bille, 2005; Demaine et al., 2009; Pawlik and Augsten, 2011). It is also widely studied in many applications, including Natural Language Processing (NLP) tasks (Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Yao et al., 2013). However, TED has a disadvantage: we need to predefine appropriate costs for basic tree-edit operations. In addition, an implementation of TED for unordered trees is fairly complex. Another tree similarity metric is Simple Tree Matching (STM) (Yang, 1991). STM measures the similarity of two trees by counting the maximum match with dynamic programming. Although this algorithm was also originally developed for ordered trees, the underlying idea of the algorithm is simple, making it relatively easy to extend the algorithm for unordered trees. Tree kernels have been also widely"
W14-2910,W13-1203,1,0.802376,"determine whether two event mentions refer to the same event. This task is important since resolved event coreference is useful in various tasks such as topic detection and tracking, information extraction, question answering, textual entailment, and contradiction detection. A key challenge for event coreference resolution is that one can define several relations between two events, where some of them exhibit subtle deviation from perfect event identity. For clarification, we refer to perfect event identity as full (event) coreference in this paper. To address the subtlety in event identity, Hovy et al. (2013) focused on two types of partial event identity: subevent and membership. Subevent relations form a stereotypical sequence of events, or a script (Schank and Abelson, 1977; Chambers and Jurafsky, 2008). Membership relations represent instances of an event collection. We refer to both as partial (event) coreference in this paper. Figure 1 shows some examples of the subevent and Figure 1: Examples of subevent and membership relations. Solid and dashed arrows represent subevent and membership relations respectively, with the direction from a parent to its subevent or member. For example, we say t"
W14-2910,H05-1003,0,0.0723793,"Missing"
W14-2910,P13-1049,0,0.0112315,"s the evaluation process for partial event coreference. • We present a way to extend MUC, BLANC, and STM for the case of unordered trees. Those metrics are generic and flexible enough to be used in evaluations involving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric should satisfy. In this section, we first describe assumptions for par"
W14-2910,P10-1143,0,0.0271402,"follows: • We introduce a conceptual tree hierarchy that simplifies the evaluation process for partial event coreference. • We present a way to extend MUC, BLANC, and STM for the case of unordered trees. Those metrics are generic and flexible enough to be used in evaluations involving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric sho"
W14-2910,D12-1045,0,0.01697,"conceptual tree hierarchy that simplifies the evaluation process for partial event coreference. • We present a way to extend MUC, BLANC, and STM for the case of unordered trees. Those metrics are generic and flexible enough to be used in evaluations involving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric should satisfy. In th"
W14-2910,D08-1031,0,0.0183008,"t in the gold standard but do not in a system response, or vice versa. In reality, twinless mentions often happen since an end-to-end system might produce them in the process of detecting mentions. The assumption regarding twinless mentions has been investigated in research on entity coreference resolution. Cluster-based metrics such as B-CUBED and CEAF assume that a system is given true mentions without any twinless mentions in the gold standard, and then resolves full coreference on them. Researchers have made different assumptions about this issue. Early work such as (Ji et al., 2005) and (Bengtson and Roth, 2008) simply ignored such mentions. Rahman and Ng (2009) removed twinless mentions that are singletons in a system response. Cai and Strube (2010) proposed two variants of B-CUBED and CEAF that can deal with twinless mentions in order to make the evaluation of end-to-end coreference resolution system consistent. In evaluation of partial coreference where twinless mentions can also exist, we believe that the value of making evaluation consistent and comparable is the most important, and hypothesize that it is possible to effectively create a metric to measure the performance of partial coreference w"
W14-2910,H05-1004,0,0.0455022,"ving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric should satisfy. In this section, we first describe assumptions for partial coreference evaluation, and introduce the notion of conceptual event hierarchy to address the challenge posed by one of the assumptions. We then enumerate the desiderata for a metric. 69 3.1 E9 i"
W14-2910,P09-2073,0,0.0272352,"ate partial coreference because it is unclear how to define a cluster. The latter is not readily applicable to the evaluation because it is unclear how to penalize incorrect directions of links. We discuss these aspects in Section 4.1 and Section 4.2. Tree Edit Distance (TED) is one of the traditional algorithms for measuring tree similarity. It has a long history of theoretical studies (Tai, 1979; Zhang and Shasha, 1989; Klein, 1998; Bille, 2005; Demaine et al., 2009; Pawlik and Augsten, 2011). It is also widely studied in many applications, including Natural Language Processing (NLP) tasks (Mehdad, 2009; Wang and Manning, 2010; Heilman and Smith, 2010; Yao et al., 2013). However, TED has a disadvantage: we need to predefine appropriate costs for basic tree-edit operations. In addition, an implementation of TED for unordered trees is fairly complex. Another tree similarity metric is Simple Tree Matching (STM) (Yang, 1991). STM measures the similarity of two trees by counting the maximum match with dynamic programming. Although this algorithm was also originally developed for ordered trees, the underlying idea of the algorithm is simple, making it relatively easy to extend the algorithm for un"
W14-2910,N13-1106,0,0.0253555,"Missing"
W14-2910,J08-2003,0,0.0187277,"n addition, an implementation of TED for unordered trees is fairly complex. Another tree similarity metric is Simple Tree Matching (STM) (Yang, 1991). STM measures the similarity of two trees by counting the maximum match with dynamic programming. Although this algorithm was also originally developed for ordered trees, the underlying idea of the algorithm is simple, making it relatively easy to extend the algorithm for unordered trees. Tree kernels have been also widely studied and applied to NLP tasks, more specifically, to capture the similarity between parse trees (Collins and Duffy, 2001; Moschitti et al., 2008) or between dependency trees (Croce et al., 2011; Srivastava et al., 2013). This method is based on a supervised learning model with training data; hence we need a number of pairs of trees and associated numeric similarity values between these trees as input. Thus, it is not appropriate for an evaluation setting. ating the performance of a system that detects partial coreference in the context of event coreference resolution. This problem is important because, as with other tasks, a good evaluation method for partial coreference will facilitate future research on the task in a consistent and c"
W14-2910,W11-1901,0,0.0507955,"Missing"
W14-2910,D09-1101,0,0.0212462,"or vice versa. In reality, twinless mentions often happen since an end-to-end system might produce them in the process of detecting mentions. The assumption regarding twinless mentions has been investigated in research on entity coreference resolution. Cluster-based metrics such as B-CUBED and CEAF assume that a system is given true mentions without any twinless mentions in the gold standard, and then resolves full coreference on them. Researchers have made different assumptions about this issue. Early work such as (Ji et al., 2005) and (Bengtson and Roth, 2008) simply ignored such mentions. Rahman and Ng (2009) removed twinless mentions that are singletons in a system response. Cai and Strube (2010) proposed two variants of B-CUBED and CEAF that can deal with twinless mentions in order to make the evaluation of end-to-end coreference resolution system consistent. In evaluation of partial coreference where twinless mentions can also exist, we believe that the value of making evaluation consistent and comparable is the most important, and hypothesize that it is possible to effectively create a metric to measure the performance of partial coreference while dealing with twinless mentions. A potential pr"
W14-2910,D13-1144,1,0.839184,"x. Another tree similarity metric is Simple Tree Matching (STM) (Yang, 1991). STM measures the similarity of two trees by counting the maximum match with dynamic programming. Although this algorithm was also originally developed for ordered trees, the underlying idea of the algorithm is simple, making it relatively easy to extend the algorithm for unordered trees. Tree kernels have been also widely studied and applied to NLP tasks, more specifically, to capture the similarity between parse trees (Collins and Duffy, 2001; Moschitti et al., 2008) or between dependency trees (Croce et al., 2011; Srivastava et al., 2013). This method is based on a supervised learning model with training data; hence we need a number of pairs of trees and associated numeric similarity values between these trees as input. Thus, it is not appropriate for an evaluation setting. ating the performance of a system that detects partial coreference in the context of event coreference resolution. This problem is important because, as with other tasks, a good evaluation method for partial coreference will facilitate future research on the task in a consistent and comparable manner. When one introduces a certain evaluation metric to such"
W14-2910,P09-1074,0,0.0130005,"omenon, let ei ⇔ ej denote full coreference between ei and ej . In Figure 1, we s have E6 ⇔ E7 and E7 → − E8. In this case, E8 s is also a subevent of E6, i.e., E6 → − E8. The rationale behind this assumption is that if a syss s tem identifies E6 → − E8 instead of E7 → − E8, then there is no reason to argue that the identified subevent relation is incorrect given that E6 ⇔ E7 s and E7 → − E8. The discussion here also applies to membership relations. Assumptions on Partial Coreference We make the following three assumptions to evaluate partial coreference. Twinless mentions: Twinless mentions (Stoyanov et al., 2009) are the mentions that exist in the gold standard but do not in a system response, or vice versa. In reality, twinless mentions often happen since an end-to-end system might produce them in the process of detecting mentions. The assumption regarding twinless mentions has been investigated in research on entity coreference resolution. Cluster-based metrics such as B-CUBED and CEAF assume that a system is given true mentions without any twinless mentions in the gold standard, and then resolves full coreference on them. Researchers have made different assumptions about this issue. Early work such"
W14-2910,M95-1005,0,0.682808,"unordered trees. Those metrics are generic and flexible enough to be used in evaluations involving data structures based on unordered trees. • Our experimental results indicate that the extended MUC and BLANC are better than Simple Tree Matching for evaluating partial coreference. 2 Related Work Recent studies on both entity and event coreference resolution use several metrics to evaluate system performance (Bejan and Harabagiu, 2010; Lee et al., 2012; Durrett et al., 2013; Lassalle and Denis, 2013) since there is no agreement on a single metric. Currently, five metrics are widely used: MUC (Vilain et al., 1995), B-CUBED (Bagga and Baldwin, 1998), two CEAF metrics CEAF-φ3 and CEAF-φ4 (Luo, 2005), and BLANC (Recasens and Hovy, 2011). We can divide these metrics into two groups: cluster-based metrics, e.g., BCUBED and CEAF, and link-based metrics, e.g., 3 Evaluation Scheme When one formulates an evaluation scheme for a new task, it is important to define assumptions for the evaluation and desiderata that an ideal metric should satisfy. In this section, we first describe assumptions for partial coreference evaluation, and introduce the notion of conceptual event hierarchy to address the challenge posed"
W14-2910,C10-1131,0,0.020493,"in the process of detecting mentions. The assumption regarding twinless mentions has been investigated in research on entity coreference resolution. Cluster-based metrics such as B-CUBED and CEAF assume that a system is given true mentions without any twinless mentions in the gold standard, and then resolves full coreference on them. Researchers have made different assumptions about this issue. Early work such as (Ji et al., 2005) and (Bengtson and Roth, 2008) simply ignored such mentions. Rahman and Ng (2009) removed twinless mentions that are singletons in a system response. Cai and Strube (2010) proposed two variants of B-CUBED and CEAF that can deal with twinless mentions in order to make the evaluation of end-to-end coreference resolution system consistent. In evaluation of partial coreference where twinless mentions can also exist, we believe that the value of making evaluation consistent and comparable is the most important, and hypothesize that it is possible to effectively create a metric to measure the performance of partial coreference while dealing with twinless mentions. A potential problem of making a single metric handle twinless mentions is that the metric would not be i"
W14-3349,D10-1092,0,0.0243437,"Automatic Evaluation of Machine Translation in which the Prize is Applied to a Chunkbased metric (APAC). Through metaevaluation experiments and comparison with several metrics, we conﬁrmed that our metric shows stable correlation with human judgment. 1 Introduction In the ﬁeld of machine translation, various automatic evaluation metrics have been proposed. Among them, chunk-based metrics such as METEOR(A. Lavie and A. Agarwal, 2007), ROUGE-L(Lin and Och, 2004), and IMPACT(H. Echizen-ya and K. Araki, 2007) are eﬀective. In general, BLEU(K. Papineni et al., 2002), NIST(NIST, 2002), and RIBES(H. Isozaki et al., 2010) use a penalty for calculation of scores because the high score is often given extremely when the candidate translation is short. Therefore, the penalty is eﬀective to obtain high correlation with human judgment. On the other hand, almost all chunkbased metrics use the F -measure based on a precision by candidate translation and a recall by reference. Moreover, they assign a 2 Score calculation in APAC The APAC score is calculated in two phases. In the ﬁrst phase, the chunk sequence is determined between a candidate translation and the reference. The chunk sequence 381 Proceedings of the Ninth"
W14-3349,W07-0734,0,0.103887,"Missing"
W14-3349,P04-1077,0,0.03671,"te translation. Moreover, we apply a prize based on sentence-length to the metric, dissimilar from penalties in BLEU or NIST. We designate this metric as Automatic Evaluation of Machine Translation in which the Prize is Applied to a Chunkbased metric (APAC). Through metaevaluation experiments and comparison with several metrics, we conﬁrmed that our metric shows stable correlation with human judgment. 1 Introduction In the ﬁeld of machine translation, various automatic evaluation metrics have been proposed. Among them, chunk-based metrics such as METEOR(A. Lavie and A. Agarwal, 2007), ROUGE-L(Lin and Och, 2004), and IMPACT(H. Echizen-ya and K. Araki, 2007) are eﬀective. In general, BLEU(K. Papineni et al., 2002), NIST(NIST, 2002), and RIBES(H. Isozaki et al., 2010) use a penalty for calculation of scores because the high score is often given extremely when the candidate translation is short. Therefore, the penalty is eﬀective to obtain high correlation with human judgment. On the other hand, almost all chunkbased metrics use the F -measure based on a precision by candidate translation and a recall by reference. Moreover, they assign a 2 Score calculation in APAC The APAC score is calculated in two p"
W14-3349,W12-3102,0,\N,Missing
W14-3349,P02-1040,0,\N,Missing
W14-3349,W13-2201,0,\N,Missing
W15-0802,E09-1005,0,0.0373921,"y a word sense dictionary, without any annotated sentences or structures and relations between different senses (as in WordNet). Our approach can be extended to disambiguate senses of words for parts of speech besides verbs. 1 1.1 Introduction The task of Word Sense Disambiguation (WSD) is to identify the correct meaning or sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007) and information retrieval (Stokoe et al., 2003). Most of the top performance WSD systems (Agirre and Soroa, 2009; Zhong and Ng, 2010), however, rely on manually annotated data or on lexical knowledge bases (e.g., WordNet), which are highly expensive to create. In this paper, we propose a novel approach for Word Sense Disambiguation of verbs using the PropStore. With the help of PropStore, our approach can utilize information about verbs’ appearance in syntactic contexts similar to the target sentence. This information significantly enriches Event Mention Detection Task Event understanding has recently attracted a lot of attention1 . A fundamental task in event understanding is to conduct Event Mention D"
W15-0802,P13-2083,1,0.854711,"re siblings of a shared head; or more complicated configurations, with their frequencies. amod They express these packages to corporate headquarters. PRP The PropStore is a proposition knowledge base, essentially a triple store implemented as a database of relations between words, created using Wikipedia articles. Each relation is represented in the form of a triple of two words connected by a relation: adpobj det express packages to VBP NNS IN Figure 2: Sig(express, X) Previous work us=ed a similar PropStore approach to build a Structured Distributional Semantics Model for event coreference (Goyal et al., 2013). 3 Our Approach for WSD In the following, we use X = x1 , x2 , . . . , xn to denote a generic sentence. For a given sentence X and a word x ∈ X which we want to disambiguate, we define the signature of x in X, Sig(x, X), as a “small part” of the dependency parse of X including x. For example, the sentence They express these packages to corporate headquarters is shown in Figure 1 along with its dependency tree, and Figure 2 gives the signature of express, Sig(express, X), in this sentence. Suppose x has m different senses in a dictionary (e.g., OntoNotes), v1 , . . . , vm , our task is to pred"
W15-0802,P13-1008,0,0.0339191,"approach for Word Sense Disambiguation of verbs using the PropStore. With the help of PropStore, our approach can utilize information about verbs’ appearance in syntactic contexts similar to the target sentence. This information significantly enriches Event Mention Detection Task Event understanding has recently attracted a lot of attention1 . A fundamental task in event understanding is to conduct Event Mention Detection (EMD). The EMD task requires a system to identify text spans in which events are mentioned, and to provide their attributes. The major attribute studied in recent EMD tasks (Li et al., 2013; Li et al., 2014) is event mention type, which is one of the most salient attributes relating to its semantics. In this paper, we propose a novel method on identifying event mention types. In particular, we focus on one major source of event mentions: verb-based mentions. Given a list of possible candidates, the event mention detection task consists in identifying the type of each candidate mention (being one of the predefined event types or other). In this paper, we simply regard all verbs as mention candidates. In this setting, event mention detection can be cast as a verb sense disambiguat"
W15-0802,D14-1198,0,0.04429,"Missing"
W15-0802,D11-1116,1,0.785406,"and language. Each triple can occur one or thousands of times in the corpus. For each occurrence of a triple in a sentence, we store a new entry in the PropStore with that information. The PropStore supports different types of relations, including dependency, semantic role, etc., and for each type, many values, including nsubj, dobj, etc. The current implementation of the PropStore uses just a single type of relation: dependency. The source of the triples is every Wikipedia article available for each supported language. Each article is parsed and POS tagged using the Fanse Dependency Parser (Tratz and Hovy, 2011). For each triple occurrence in the corpus, we store the source article title, the sentence number, and the position of the child word in the sentence. This way, for every occurrence of a triple within a sentence, we can re-build the sentence, and also we can distinguish between two occurrences of the same triple in a sentence, allowing us to chain two or more triples in a tree configuration. With this structure we can query the database to retrieve, for example, all sentences with a particular relation configuration; all verbs which have a particular dobj; all subjects of a given verb; two or"
W15-0802,P10-4014,0,0.0237715,"y, without any annotated sentences or structures and relations between different senses (as in WordNet). Our approach can be extended to disambiguate senses of words for parts of speech besides verbs. 1 1.1 Introduction The task of Word Sense Disambiguation (WSD) is to identify the correct meaning or sense of an ambiguous word in a given context. As a fundamental task in natural language processing (NLP), WSD can benefit applications such as machine translation (Chan et al., 2007) and information retrieval (Stokoe et al., 2003). Most of the top performance WSD systems (Agirre and Soroa, 2009; Zhong and Ng, 2010), however, rely on manually annotated data or on lexical knowledge bases (e.g., WordNet), which are highly expensive to create. In this paper, we propose a novel approach for Word Sense Disambiguation of verbs using the PropStore. With the help of PropStore, our approach can utilize information about verbs’ appearance in syntactic contexts similar to the target sentence. This information significantly enriches Event Mention Detection Task Event understanding has recently attracted a lot of attention1 . A fundamental task in event understanding is to conduct Event Mention Detection (EMD). The E"
W15-0802,P07-1005,0,\N,Missing
W15-0807,P08-1090,0,0.0420243,"e analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building robust event mention detection system can help promote research in these areas and enable researchers to produce end-to-end systems. In this paper, we discuss our recent eff"
W15-0807,P09-1068,0,0.0274299,"luating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building robust event mention detection system can help promote research in these areas and enable researchers to produce end-to-end systems. In this paper, we discuss our recent effort in providing a proper eval"
W15-0807,W09-3208,0,0.0363768,"r, we present our evaluation algorithm and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building robust event mention detection system can help promote researc"
W15-0807,I13-1100,0,0.0157917,"m and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building robust event mention detection system can help promote research in these areas and enable researche"
W15-0807,D12-1045,0,0.0354797,"valuation algorithm and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building robust event mention detection system can help promote research in these areas a"
W15-0807,P13-1008,0,0.299474,"ion is important for modern natural language processing tasks. In this paper, we present our evaluation algorithm and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotat"
W15-0807,D14-1198,0,0.0987897,"for modern natural language processing tasks. In this paper, we present our evaluation algorithm and results during the Event Mention Evaluation pilot study. We analyze the problems of evaluating multiple event mention attributes and discontinuous event mention spans. In addition, we identify a few limitations in the evaluation algorithm used for the pilot task and propose some potential improvements. 1 Introduction Textual event understanding has attracted a lot of attention in the community. Recent work has covered several areas about events, such as event mention detection(Li et al., 2013; Li et al., 2014) , event coreference (Bejan et al., 2005; Chen and Ji, 2009; Lee et al., 2012; Chen and Ng, 2013; Liu et al., 2013), and script understanding (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). Event Mention detection is the fundamental preprocessing step for these tasks. However, downstream event researches often make minimal effort for event mention detection. For example, in event coreference work, Lee et al. (2012) do not make clear distinction between event and entity mentions. Bejan et al. (2005) and Liu et al. (2013) use oracle event mentions from human annotations. Building rob"
W16-6005,N07-1030,0,0.0498993,"Missing"
W16-6005,D09-1120,0,0.0276697,"baseline systems using the traditional Bagof-Words model and a Neural Network architecture that outperforms the baseline models. 3. We define a cloze-test evaluation method that requires no annotation. Our procedure stems from the following insight. Instead of starting with the coreference trigger word/phrase and asking “which clauses can refer to this?”, we 23 Table 2: A sample cloze-test evaluation task 2 Related Work Entity coreference has been studied quite extensively. There are primarily two complementary approaches. The first focuses mainly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has"
W16-6005,N10-1061,0,0.027033,"ly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplored. Some earlier work in this area includes (Humphreys et al., 1997) but the work was very specific to selected events. More recently, there have been approaches to model event coreferences separately (Liu et al., 2014) as well as jointly with entities (Lee et al., 2012). All this work makes the limiting assumption of word/phrase to word/phrase coreference (levels 1 and 2 described earlier). Our work aligns with the event coreference literature but assumes longer spans of text and tackles the more challenging problem of"
W16-6005,W97-1311,0,0.188687,"g, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplored. Some earlier work in this area includes (Humphreys et al., 1997) but the work was very specific to selected events. More recently, there have been approaches to model event coreferences separately (Liu et al., 2014) as well as jointly with entities (Lee et al., 2012). All this work makes the limiting assumption of word/phrase to word/phrase coreference (levels 1 and 2 described earlier). Our work aligns with the event coreference literature but assumes longer spans of text and tackles the more challenging problem of abstract multievent/clause coreference. 3 Model vectors are trained using gradient descent, with gradients are obtained though back-propagatio"
W16-6005,W11-1902,0,0.0530903,"Missing"
W16-6005,D12-1045,0,0.0845507,"a better vacation. In this paper we generalize the idea of coreference to 3 levels based on the degree of abstraction of the coreference trigger: 3. Level 3 – Multiple Clauses: The trigger is quite generic and refers to a particular instance of an event that is described over multiple clauses or sentences (either contiguous or noncontiguous). Typically, the abstract event refers to a set of [sub]events, each of them with its own own participants or arguments. See Table 1 for examples. We use PubMed1 as our primary corpus. Almost all work on event coreference (for example, (Liu et al., 2014) (Lee et al., 2012)) applies to levels 1 or 2. In this paper, we propose a generalized coreference classification scheme and address the challenges related to resolving level-3 coreferences. Creating gold-standard training and evaluation materials for such coreferences is an uphill challenge. First, there is a significant annotation overhead and, depending on the nature of the corpus, the annotator might require significant domain knowledge. Each annotation instance might require multiple labels depending the number of abstract events mentioned in the corpus. Second, the vocabulary of the corpus is rather large"
W16-6005,liu-etal-2014-supervised,1,0.910295,"couldn’t have been a better vacation. In this paper we generalize the idea of coreference to 3 levels based on the degree of abstraction of the coreference trigger: 3. Level 3 – Multiple Clauses: The trigger is quite generic and refers to a particular instance of an event that is described over multiple clauses or sentences (either contiguous or noncontiguous). Typically, the abstract event refers to a set of [sub]events, each of them with its own own participants or arguments. See Table 1 for examples. We use PubMed1 as our primary corpus. Almost all work on event coreference (for example, (Liu et al., 2014) (Lee et al., 2012)) applies to levels 1 or 2. In this paper, we propose a generalized coreference classification scheme and address the challenges related to resolving level-3 coreferences. Creating gold-standard training and evaluation materials for such coreferences is an uphill challenge. First, there is a significant annotation overhead and, depending on the nature of the corpus, the annotator might require significant domain knowledge. Each annotation instance might require multiple labels depending the number of abstract events mentioned in the corpus. Second, the vocabulary of the corp"
W16-6005,N06-1025,0,0.0528501,"l and a Neural Network architecture that outperforms the baseline models. 3. We define a cloze-test evaluation method that requires no annotation. Our procedure stems from the following insight. Instead of starting with the coreference trigger word/phrase and asking “which clauses can refer to this?”, we 23 Table 2: A sample cloze-test evaluation task 2 Related Work Entity coreference has been studied quite extensively. There are primarily two complementary approaches. The first focuses mainly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplored. Some earlier work in this"
W16-6005,D08-1068,0,0.0347864,"we 23 Table 2: A sample cloze-test evaluation task 2 Related Work Entity coreference has been studied quite extensively. There are primarily two complementary approaches. The first focuses mainly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplored. Some earlier work in this area includes (Humphreys et al., 1997) but the work was very specific to selected events. More recently, there have been approaches to model event coreferences separately (Liu et al., 2014) as well as jointly with entities (Lee et al., 2012). All this work makes the limiting assumption of word/phrase"
W16-6005,D10-1048,0,0.0175362,"traditional Bagof-Words model and a Neural Network architecture that outperforms the baseline models. 3. We define a cloze-test evaluation method that requires no annotation. Our procedure stems from the following insight. Instead of starting with the coreference trigger word/phrase and asking “which clauses can refer to this?”, we 23 Table 2: A sample cloze-test evaluation task 2 Related Work Entity coreference has been studied quite extensively. There are primarily two complementary approaches. The first focuses mainly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplore"
W16-6005,P11-1082,0,0.0134691,"ecture that outperforms the baseline models. 3. We define a cloze-test evaluation method that requires no annotation. Our procedure stems from the following insight. Instead of starting with the coreference trigger word/phrase and asking “which clauses can refer to this?”, we 23 Table 2: A sample cloze-test evaluation task 2 Related Work Entity coreference has been studied quite extensively. There are primarily two complementary approaches. The first focuses mainly on identifying entity mention clusters (see (Haghighi and Klein, 2009), (Raghunathan et al., 2010), (Ponzetto and Strube, 2006), (Rahman and Ng, 2011), (Ponzetto and Strube, 2006)). These models employ featurerich approaches to improve the clustering models and are limited to noun pairs. The second focuses on jointly modeling mentions across all the entries in the document (see (Denis et al., 2007), (Poon and Domingos, 2008), (Wick et al., 2008) and (Lee et al., 2011)). Some more recent work uses event argument information to assist entity coreference; this includes (Rahman and Ng, 2011), (Haghighi and Klein, 2010). The distinct problem of Event Coreference has been relatively underexplored. Some earlier work in this area includes (Humphrey"
W17-2703,P98-1013,0,0.371895,"tection and its classification to one type/subtype pair, as defined by the ACE guidelines. 15 Proceedings of the Events and Stories in the News Workshop, pages 15–20, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics 2.2 der to construct an out-domain ANN model. Peng et al. (2016) showed that it is feasible to achieve state-of-the-art results with minimal supervision. In their approach, they use only a few examples and the SRL of a candidate event in order to construct a structured vector representation, which maps the event to an ontology. FrameNet FrameNet (Baker et al., 1998) is a taxonomy of more than 1,200 manually identified semantic frames, deriving from a corpus of 200,000 annotated sentences. The aim of the FrameNet semantic frames is to capture information about the type of a linguistic structure, which can be an event, entity or relation, and its participants. This type is called Frame and the participants are called Frame Elements. Each Frame is linked to a set of words that may trigger the Frame (Lexical Units). Following the definition of FrameNet semantic frames and the ACE 2005 guidelines, it seems natural to assume a good correspondence between the t"
W17-2703,P15-1017,0,0.0330511,"Nugget, and it involves a set of participants, the Event Arguments. The term Event Nugget (TAC, 2014) refers to a semantically meaningful unit of text that denotes some action (event), while the Event Arguments are Entity mentions or temporal expressions related to the Event Nugget. In this work, we focus on the task of Event Nugget Detection and its classification to types and subtypes of Events, according to the ACE 2005 guidelines. Current Event Detection methods that achieve state-of-the-art results are based on Deep Learning techniques using shallow lexical features and word embeddings (Chen et al., 2015), (Nguyen and Gr2 2.1 Background The ACE Dataset According to the ACE 2005 Evaluations (ACE, 2005), an Event contains two spans: the Event Nugget and the Event Arguments. Although there are several types of events, the ACE annotations include only events that can be defined under a certain ontological structure. This structure contains 8 event types followed by a total of 33 event subtypes. The event types are: LIFE, MOVEMENT, TRANSACTION, BUSINESS, CONFLICT, CONTACT, PERSONNEL and JUSTICE. In this work, we focus on the Event Nugget detection and its classification to one type/subtype pair, as"
W17-2703,P16-2060,0,0.027452,"Missing"
W17-2703,P16-1201,0,0.01258,"ameNet semantic frames is to capture information about the type of a linguistic structure, which can be an event, entity or relation, and its participants. This type is called Frame and the participants are called Frame Elements. Each Frame is linked to a set of words that may trigger the Frame (Lexical Units). Following the definition of FrameNet semantic frames and the ACE 2005 guidelines, it seems natural to assume a good correspondence between the two resources. This property implies that a mapping from FrameNet Frames to ACE types and subtypes can be extremely helpful in Event Detection (Liu et al., 2016). 2.3 3 In this paper, we present a system that uses a semantic-frame parser in order to generate event candidates, which are then filtered according to a mapping between ontologies. The main motivation behind this approach is that most systems based on deep learning methods do not exploit rich semantic information and therefore miss nonsurface-level equivalences, which results in low recall. Furthermore, we claim that a combination of a semantically rich system with a deep learning approach can result in better overall performance than both traditional semantic-based approaches and pure deep"
W17-2703,P14-5010,0,0.00447561,"munication means, Text creation, Request Personnel Take place of, Get a job, Hiring, Appointing, Removing, Firing, Quitting, Choosing, Becoming a member, Change of leadership Justice Arrest, Imprisonment, Detaining, Extradition, Breaking out captive, Try defendant, Pardon, Appeal, Verdict, Sentencing, Fining, Execution, Releasing, Notification of charges Table 1: Mapping of FrameNet verbs to ACE Ontology. 3.3 System Architecture We first use Semafor to generate a set of candidate Event Nuggets, their FrameNet frame and their 17 Frame Elements. Then we use the POS tagger from Stanford CoreNLP (Manning et al., 2014) in order to distinguish the candidate events to verbal events and nominal events. For every trigger in the candidate events, we use the output FrameNet Frame in order to decide whether it is an event or not. If the Frame is in the domain of the FrameNet to ACE mapping, then it means that it corresponds to some subtype of the ACE Ontology and, thus, we accept it as an event. Furthermore, according to the mapping, we assign the type and subtype of the event. In Figure 1 we see an example output of the system for one article. The events are represented with green, red and black color if they are"
W17-2703,P15-2060,0,0.0646177,"Missing"
W17-2703,D16-1038,0,0.0600801,"lude only events that can be defined under a certain ontological structure. This structure contains 8 event types followed by a total of 33 event subtypes. The event types are: LIFE, MOVEMENT, TRANSACTION, BUSINESS, CONFLICT, CONTACT, PERSONNEL and JUSTICE. In this work, we focus on the Event Nugget detection and its classification to one type/subtype pair, as defined by the ACE guidelines. 15 Proceedings of the Events and Stories in the News Workshop, pages 15–20, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics 2.2 der to construct an out-domain ANN model. Peng et al. (2016) showed that it is feasible to achieve state-of-the-art results with minimal supervision. In their approach, they use only a few examples and the SRL of a candidate event in order to construct a structured vector representation, which maps the event to an ontology. FrameNet FrameNet (Baker et al., 1998) is a taxonomy of more than 1,200 manually identified semantic frames, deriving from a corpus of 200,000 annotated sentences. The aim of the FrameNet semantic frames is to capture information about the type of a linguistic structure, which can be an event, entity or relation, and its participant"
W17-2703,C98-1013,0,\N,Missing
W17-4415,D10-1124,0,0.802123,"Missing"
W17-4415,C12-1064,0,0.796157,"easure the geographic distribution of each NE type, and measure their entropy. In the second experiment, we conduct feature selection via randomized logistic regression, and, in the third experiment, we establish a baseline by using majority classes for all types. Geographic diversity We first measure the geographic distribution of each type. We extract all NEs in the WORLD training set and use the Tweet corpus to measure entropy and mean pairwise distance (in kilometers) between tweets that contain the same NEs. We compute unpredictability as entropy: Resources Data We use the WORLD dataset (Han et al., 2012), which covers 3,709 cities worldwide and consists of tweets from 1.4M users. Han et al. (2012) hold out 10,000 users as development and 10,000 as test set. For each user with at least 10 geotagged tweets, the user’s location is set to be the city in which the majority of their tweets are from. We also use Han et al. (2012)’s method to extract the nearest city to a given latitudelongitude coordinate. H(x) = − n X P(xi ) log P(xi ) i=1 1 We map the latitude and longitude coordinates to cities/regions based on Han et al. (2012). 117 GEO-LOC FACILITY SPORT-TEAM MOVIE TV-SHOW PERSON BAND PRODUCT C"
W17-4415,D15-1256,0,0.115835,"Missing"
W17-4415,P15-2104,0,0.726545,"33.6 53.6 17.7 82.2 29.9 83.3 33.7 83.6 34.0 – 10.3 – – 31.0 24.1 Distance Median↓ Mean↓ 515 1727 2186 5317 612 1885 520 1769 495 1735 509 646 1669 1953 Table 4: Accuracy and distance results for various methods. – indicates no report in respective paper NE. To divide the world into regions with roughly the same number of users, we use a k-d tree approach proposed by Roller et al. (2012). As a result, we will cover larger regions when the population density is low and vice versa. Each region is then considered as a label to train the classifiers. The approach of using k-d tree is also used in Rahimi et al. (2015); Han et al. (2012) and Wing and Baldridge (2014). See Table 3 for an example of the following methods. All use logistic regression as classifier, following Rahimi et al. (2015). repetition is used to put more emphasis/weight based on frequency.3 In order to measure the effectiveness of the three top NE types discovered in Section 4, we experiment with (1) considering all NE types (shown as Our MethodallN Es in Table 4), and (2) the three most useful types (shown as Our Methodtop3 ). Evaluation metrics We use the same evaluation metrics as previous studies: accuracy depending on location granu"
W17-4415,D11-1141,0,0.168968,"Missing"
W17-4415,D12-1137,0,0.675438,"ent detection, recommender systems, sentiment analysis, and knowledge base population. Since tweets contain at most 140 characters, geolocation of individual tweets is rarely feasible. Instead, most studies focus on predicting the primary location of a user by concatenating their entire tweet history. While this provides more context, it is still a noisy source with features of varying informativeness. 2 Related Work Most previous studies use textual features as input. Some use KL divergence between the distribution of a users words and the words used in each region (Wing and Baldridge, 2011; Roller et al., 2012), regional topic distributions (Eisenstein et al., 2010; Ahmed et al., 2013; Hong et al., 2012), or feature selection/weighting to find words indicative of location (Priedhorsky et al., 2014; Han et al., 2012, 2014; Wing and Baldridge, 2014). All these studies require relatively large training sets to fit the models, and can be heavily biased by 116 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 116–121 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics Type PERSON GEO-LOC FACILITY COMPANY MOVIE BAND PRODUCT TV-SHOW SPORT-TEAM All Exam"
W17-4415,W17-4409,1,0.339601,"ce location names can refer to multiple locations (Brunsting et al., 2016). Chi et al. (2016) explicitly study the contributions of city and country names, hashtags, and user mentionings, to geolocation. Their results suggested that a combination of city and country names, as well as hashtags, are good location predictors. Pavalanathan and Eisenstein (2015) suggest that non-standard words are more locationspecific, and also, more likely to occur in geotagged tweets. In contrast to this paper, none of the previous works study how much various NE types reveal about the user location. Similarly, Salehi and Søgaard (2017) evaluate common hypotheses about language and location. However, they do not explicitly study named entities. 3 4 NE types and Geolocation In Table 1, we have seen the general distribution of NE types, with PERSON, GEO-LOC and FACILITY as top three. In this section, we focus on the predictiveness of NEs (as features) for geolocation. Later, in Section 5, we will propose a method to improve geolocation by putting more emphasis on the top NEs and their hidden location information. We conduct three experiments to quantify predictiveness of NEs. In the first, we measure the geographic distributio"
W17-4415,W16-3930,0,0.0544298,"or events during the time of collection, such as an election or a disaster. In contrast to our work, most do not consider multi-word NEs. Only few text-based studies consider NEs, and if so, focus on location names using gazetteers like GeoNames, limiting the methods to the completeness of these gazetteers. Since they usually also use other text-based models, it is hard to determine how much location names contribute. These approaches depend on a namedisambiguation phase, using Wikipedia, DBPedia, or OpenStreetMap, since location names can refer to multiple locations (Brunsting et al., 2016). Chi et al. (2016) explicitly study the contributions of city and country names, hashtags, and user mentionings, to geolocation. Their results suggested that a combination of city and country names, as well as hashtags, are good location predictors. Pavalanathan and Eisenstein (2015) suggest that non-standard words are more locationspecific, and also, more likely to occur in geotagged tweets. In contrast to this paper, none of the previous works study how much various NE types reveal about the user location. Similarly, Salehi and Søgaard (2017) evaluate common hypotheses about language and location. However, th"
W17-4415,D14-1039,0,0.532042,"he impact of NEs and their hidden location information for geolocation. To extract the hidden location information of each NE, we collect the locations of all tweets in our tweet corpus that contain that 118 Example baseline Only NE Baseline without NE Our Method Me, my friend and the Eiffel tower Me my friend and the Eiffel tower [Eiffel tower] ME my friend and the Me my friend and the Eiffel tower Paris Paris Paris Paris [Las Vegas] [Las Vegas] Table 3: Examples and features of methods in Section 5 Method Baseline Only NE Baseline without NE Our MethodallN Es Our Methodtop3 Previous studies Wing and Baldridge (2014) Han et al. (2012) city↑ 17.6 9.3 14.8 17.5 17.8 Accuracy country↑ @161↑ 83.6 33.6 53.6 17.7 82.2 29.9 83.3 33.7 83.6 34.0 – 10.3 – – 31.0 24.1 Distance Median↓ Mean↓ 515 1727 2186 5317 612 1885 520 1769 495 1735 509 646 1669 1953 Table 4: Accuracy and distance results for various methods. – indicates no report in respective paper NE. To divide the world into regions with roughly the same number of users, we use a k-d tree approach proposed by Roller et al. (2012). As a result, we will cover larger regions when the population density is low and vice versa. Each region is then considered as a l"
W17-4415,P11-1096,0,0.256809,"as technologies such as event detection, recommender systems, sentiment analysis, and knowledge base population. Since tweets contain at most 140 characters, geolocation of individual tweets is rarely feasible. Instead, most studies focus on predicting the primary location of a user by concatenating their entire tweet history. While this provides more context, it is still a noisy source with features of varying informativeness. 2 Related Work Most previous studies use textual features as input. Some use KL divergence between the distribution of a users words and the words used in each region (Wing and Baldridge, 2011; Roller et al., 2012), regional topic distributions (Eisenstein et al., 2010; Ahmed et al., 2013; Hong et al., 2012), or feature selection/weighting to find words indicative of location (Priedhorsky et al., 2014; Han et al., 2012, 2014; Wing and Baldridge, 2014). All these studies require relatively large training sets to fit the models, and can be heavily biased by 116 Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 116–121 c Copenhagen, Denmark, September 7, 2017. 2017 Association for Computational Linguistics Type PERSON GEO-LOC FACILITY COMPANY MOVIE BAND PRODUCT TV-SH"
W17-4902,P03-1021,0,0.0263479,"S (128-128), M E (192-192) and L (256-256). Across models, we find that the M E configuration performs better in terms of highest validation BLEU. We also find that larger configurations (384-384 & 512-512) fail to converge or perform very poorly 8 . Here, we report results only for the M E configuration for all the models. For all our models, we picked the best saved model over 15 epochs which has the highest validation BLEU. To train statistical machine translation (SMT) baselines, we use publicly available open-source toolkit MOSES (Koehn et al., 2007), along with the GIZA++ word aligner (Och, 2003), as was done in (Xu et al., 2012). For training the targetside LM component, we use the lmplz toolkit within MOSES to train a 4-gram LM. We also use MERT (Och, 2003), available as part of MOSES, to tune on the validation set. For fairness of comparison, it is necessary to use the pairwise dictionary and PTB while training the SMT models as well - the most obvious way for this is to use the dictionary and PTB as additional training data for the alignment component and the target-side LM respectively. We experiment with several SMT models, ablating for the use of both PTB and dictionary. In 8,"
W17-4902,P11-1020,0,0.00999304,"test-time we use greedy decoding to find the most likely target sentence9 . We also experiment with a post-processing strategy which replaces UNKs in the target output with the highest aligned (maximum attention) source word. We find that this gives a small jump in BLEU of about 0.1-0.2 for all neural models 10 . Our best model, for instance, gets a jump of 0.14 to reach a BLEU of 31.26 from 31.12. Evaluation Our primary evaluation metric is BLEU (Papineni et al., 2002) . We compute BLEU using the freely available and very widely used perl script7 from the MOSES decoder. We also report PINC (Chen and Dolan, 2011), which originates from paraphrase evaluation liter6 Training and Parameters We use a minibatch-size of 32 and the ADAM optimizer (Kingma and Ba, 2014) with learning rate 0.001, momentum parameters 0.9 and 0.999, and  = 10−8 . All our implementations are written in Python using Tensorflow 1.1.0 framework. For every model, we experimented with two configurations of embedding and LSTM size S (128-128), M E (192-192) and L (256-256). Across models, we find that the M E configuration performs better in terms of highest validation BLEU. We also find that larger configurations (384-384 & 512-512) f"
W17-4902,P02-1040,0,0.0978547,"eover, a dictionary cannot easily capture one-to-many mappings as well as long-range dependencies 6 . 7.2.3 7.4 Off-the-shelf SMT 7.5 7 Decoding At test-time we use greedy decoding to find the most likely target sentence9 . We also experiment with a post-processing strategy which replaces UNKs in the target output with the highest aligned (maximum attention) source word. We find that this gives a small jump in BLEU of about 0.1-0.2 for all neural models 10 . Our best model, for instance, gets a jump of 0.14 to reach a BLEU of 31.26 from 31.12. Evaluation Our primary evaluation metric is BLEU (Papineni et al., 2002) . We compute BLEU using the freely available and very widely used perl script7 from the MOSES decoder. We also report PINC (Chen and Dolan, 2011), which originates from paraphrase evaluation liter6 Training and Parameters We use a minibatch-size of 32 and the ADAM optimizer (Kingma and Ba, 2014) with learning rate 0.001, momentum parameters 0.9 and 0.999, and  = 10−8 . All our implementations are written in Python using Tensorflow 1.1.0 framework. For every model, we experimented with two configurations of embedding and LSTM size S (128-128), M E (192-192) and L (256-256). Across models, we"
W17-4902,D14-1162,0,0.0865762,"s on all training sentences. We also experiment with adding additional data from PTB (Marcus et al., 1993) for better learning of embeddings. Additionally we leverage a dictionary mapping tokens from Shakespearean English to modern English. We consider four distinct strategies to train the embeddings. In the cases where we use external text data, we first train the embeddings using both the external data and training data, and then for the same number of iterations on training data alone, to ensure adaptation. Note that we do not directly use off-the-shelf pretrained embeddings such as Glove (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013) since we need to learn embeddings for novel word forms (and also different word senses for extant word forms) on the Original side. Table 2 shows some statistics from the training split of the dataset. In general, the Original side has longer sentences and a larger vocabulary. The slightly higher entropy of the Original side’s frequency distribution indicates that the frequencies are more spread out over words. Intuitively, the large number of shared word types indicates that sharing the representation between Original and Modern sides could provide some be"
W17-4902,D17-1315,1,0.814755,"ine translation (MT), , summarization (Rush et al., 2015), etc. In the context of MT, various settings such as multisource MT (Zoph and Knight, 2016) and MT with external information (Sennrich et al., 2016) have been explored. Distinct from all of these, our work attempts to solve a Modern English → Shakespearean English style transformation task. Although closely related to both paraphrasing and MT, our task has some differentiating characteristics such as considerable source-target overlap in vocabulary and grammar (unlike MT), and different source and target language (unlike paraphrasing). Gangal et al. (2017) have proposed a neural sequence-to-sequence solution for generating a portmanteau given two English root-words. Though their task also involves large overlap in target and input, they do not employ any special copying mechanism. Unlike text simplification and summarization, our task does not involve shortening content length. Figure 2: Attention matrices from a Copy (top) and a simple S2S (bottom) model respectively on the input sentence “Holy Saint Francis, this is a drastic change!” . < s &gt; and < /s &gt; are start and stop characters. Darker cells are higher-valued. semble the ground truth mor"
W17-4902,D15-1044,0,0.0389034,"life . then she is just a life of life, let me life out of life . then the window will let day in, and life out . Table 1: Examples from dataset showing modern paraphrases (M ODERN) of few sentences from Shakespeare’s plays (O RIGINAL). We also show transformation of modern text to Shakespearean text from our models (C OPY, S IMPLE S2S and S TAT). ∗ * denotes equal contribution https://github.com/harsh19/Shakespearizing-ModernEnglish 1 10 Proceedings of the Workshop on Stylistic Variation, pages 10–19 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics (Rush et al., 2015) , brand naming (Hiranandani et al., 2017), text expansion (Srinivasan et al., 2017), etc. However, there is a dearth of automated solutions for adapting text quickly to different styles. We consider the problem of transforming text written in modern English text to Shakepearean style English. For the sake of brevity and clarity of exposition, we henceforth refer to the Shakespearean sentences/side as Original and the modern English paraphrases as Modern. Unlike traditional domain or style transfer, our task is made more challenging by the fact that the two styles employ diachronically dispara"
W17-4902,P07-2045,0,0.0118972,"mented with two configurations of embedding and LSTM size S (128-128), M E (192-192) and L (256-256). Across models, we find that the M E configuration performs better in terms of highest validation BLEU. We also find that larger configurations (384-384 & 512-512) fail to converge or perform very poorly 8 . Here, we report results only for the M E configuration for all the models. For all our models, we picked the best saved model over 15 epochs which has the highest validation BLEU. To train statistical machine translation (SMT) baselines, we use publicly available open-source toolkit MOSES (Koehn et al., 2007), along with the GIZA++ word aligner (Och, 2003), as was done in (Xu et al., 2012). For training the targetside LM component, we use the lmplz toolkit within MOSES to train a 4-gram LM. We also use MERT (Och, 2003), available as part of MOSES, to tune on the validation set. For fairness of comparison, it is necessary to use the pairwise dictionary and PTB while training the SMT models as well - the most obvious way for this is to use the dictionary and PTB as additional training data for the alignment component and the target-side LM respectively. We experiment with several SMT models, ablatin"
W17-4902,P17-1099,0,0.0260369,"stical machine translation to transform text to target style. On the other hand our method is an endto-end trainable neural network. Saha Roy et al (2015) leverage different language models based on geolocation and occupation to align a text to specific style. However, their work is limited to addition of adjectives and adverbs. Our method can handle more generic transformations including addition and deletion of words. Pointer networks (Vinyals et al., 2015) allow the use of input-side words directly as output in a neural S2S model, and have been used for tasks like extractive summarization (See et al., 2017) (Zeng et al., 2016) and question answering (Wang and Jiang, 2016). However, pointer networks cannot generate words not present in the input. A mixture model of recurrent neural network and pointer We have demonstrated the transformation to Shakespearean style English only. Methods have to be explored to achieve other stylistic variations corresponding to formality and politeness of text, usage of fancier words and expressions, etc. We release our code publicly to foster further research on stylistic transformations on text. 12 . 12 https://github.com/harsh19/Shakespearizing-ModernEnglish 17 A"
W17-4902,N16-1005,0,0.11106,"se due to our preprocessing. Although this slightly affects BLEU, it helps prevent token occurrences getting split due to capitalization. 16 network has been shown to achieve good performance on language modeling task (Merity et al., 2016). S2S neural models, first proposed by Sutskever et al. (2014), and enhanced with a attention mechanism by Bahdanau et al. (2014), have yielded state-of-the-art results for machine translation (MT), , summarization (Rush et al., 2015), etc. In the context of MT, various settings such as multisource MT (Zoph and Knight, 2016) and MT with external information (Sennrich et al., 2016) have been explored. Distinct from all of these, our work attempts to solve a Modern English → Shakespearean English style transformation task. Although closely related to both paraphrasing and MT, our task has some differentiating characteristics such as considerable source-target overlap in vocabulary and grammar (unlike MT), and different source and target language (unlike paraphrasing). Gangal et al. (2017) have proposed a neural sequence-to-sequence solution for generating a portmanteau given two English root-words. Though their task also involves large overlap in target and input, they d"
W17-4902,C12-1177,0,0.469547,"wn and belike), novel grammatical constructions (two second person forms - thou (informal) and you (formal) (Brown et al., 1960)), semantically drifted senses (e.g fetches is a synonym of excuses) and non-standard orthography (Rayson et al., 2007). Additionally, there is a domain difference since the Shakespearean play sentences are from a dramatic screenplay whereas the parallel modern English sentences are meant to be simplified explanation for high-school students. Prior works in this field leverage a language model for the target style, achieving transformation either using phrase tables (Xu et al., 2012), or by inserting relevant adjectives and adverbs (Saha Roy et al., 2015). Such works have limited scope in the type of transformations that can be achieved. Moreover, statistical and rule MT based systems do not provide a direct mechanism to a) share word representation information between source and target sides b) incorporating constraints between words into word representations in end-to-end fashion. Neural sequence-tosequence models, on the other hand, provide such flexibility. Our main contributions are as follows: # Word Tokens # Word Types Average Sentence Length Entropy (Type.Dist) ∩"
W17-4902,N16-1004,0,0.0346923,"Copy+SL configurations. 11 All neural outputs are lowercase due to our preprocessing. Although this slightly affects BLEU, it helps prevent token occurrences getting split due to capitalization. 16 network has been shown to achieve good performance on language modeling task (Merity et al., 2016). S2S neural models, first proposed by Sutskever et al. (2014), and enhanced with a attention mechanism by Bahdanau et al. (2014), have yielded state-of-the-art results for machine translation (MT), , summarization (Rush et al., 2015), etc. In the context of MT, various settings such as multisource MT (Zoph and Knight, 2016) and MT with external information (Sennrich et al., 2016) have been explored. Distinct from all of these, our work attempts to solve a Modern English → Shakespearean English style transformation task. Although closely related to both paraphrasing and MT, our task has some differentiating characteristics such as considerable source-target overlap in vocabulary and grammar (unlike MT), and different source and target language (unlike paraphrasing). Gangal et al. (2017) have proposed a neural sequence-to-sequence solution for generating a portmanteau given two English root-words. Though their tas"
W17-4902,J93-2004,0,\N,Missing
W17-5538,D14-1162,0,0.0809725,"each pattern cluster are facet instance candidates. Although we have clusters of similar facet instance candidates, there are many noisy instances in each cluster. To determine which instances are most reliable, we score each instance based on how far its generating patterns are from the center of the cluster. Specifically, an instance is scored high if it is found in more patterns in the cluster, and in patterns with higher within-cluster scores. We also take into account how semantically close each instance is to the other words in the same cluster. We use the GloVe vector representations (Pennington et al., 2014) to compute cosine similarity between two words. The scoring formula is shown below, where Ni is the number of different patterns that extracted wordi , Sim is the average cosine similarity with all other words in the same cluster, score patternk is within-cluster score computed by NMF. Cluster Lexico-Grammar Patterns score(wordi ) = Sim∗ Using the idea that lexico-grammar patterns can approximate semantic relations, we first cluster collected lexico-grammar patterns so that each cluster may represent a different relation (facet slot). The feature representation of each pattern is based on all"
W17-5538,W13-0907,1,0.881605,"ce concept occur in similar lexicosyntactic settings. They cluster nouns (target domain) and verbs (source domain), and search the corpus for metaphors that use the verbs in the source domain lexicon to represent the target domain concepts. Extending Shutova et al. (2010), (Shutova and Sun, 2013) find metaphorical mappings by building and traversing a graph of concepts. Then, they generate lists of salient features for the metaphorically connected clusters, and search the corpus for metaphors that use the verbs in the salient features to represent the target domain concepts. Another approach, Hovy et al. (2013) detected metaphors using certain semantic patterns appearing in metaphor manifestations. For example, “sweet” with food is literal, but is metaphorical with people. By finding these patterns on different levels, they extended the application of this mapping information from a narrow focus on verb relations to other syntactic relations. Along the same lines, Mohler et al. (2013) presented a domain-aware semantic signature to capture source and target domains for a text. A semantic signature represents the placement of a text on a semantic space by using a set of related WordNet senses, and it"
W17-5538,W13-1602,0,0.023732,"e her status (EX(7)) and her wish to the other person with the extension of get back on after you fall. Although falling off the wagon and on the wagon are metaphorical idioms, get back on after you fall is a novel metaphor created by the following speaker. This novel metaphor is drawn from the wagon frame that has been brought into this conversation. In this way, a metaphor that is taken up by multiple speakers may increase empathetic understanding as well as add creative opportunities (e.g., for “fun”) to the conversation. 323 Figure 1: System flow diagram. 1. as (Riloff et al., 1999, 2003; Qadir and Riloff, 2013). In our model, we assume that a sentence tends to contain more than one important facet of a metaphor frame. In other words, if a sentence contains one facet of a metaphor frame, the sentence is likely to contain additional facets. Additionally, we assume that facets and dependency relations have some relationship. There are certain grammatical patterns that represent semantic relations that connect facets in context. Note that we disregard frame facet instances that do not cooccur with a keyword (e.g., journey) within the same sentence. This can be considered as a limitation of this approach"
W17-5538,O14-3001,0,0.0117225,"ism through which the frame and topic information enable the more accurate metaphor detection. 1 Introduction Computational work on metaphor has largely focused on metaphor detection within individual sentences, for the purpose of identification of literal meaning, with an eye towards improvement of downstream applications like Machine Translation. This limited conceptualization of metaphor within these restricted contexts has allowed prior work to leverage local indicators to identify metaphorical language, such as the violation of selectional preferences (Martin, 1996; Shutova et al., 2010; Huang, 2014) or the use of abstract vs concrete descriptors (Turney et al., 2011; Brysbaert et al., 2014; Tsvetkov et al., 2013). When detecting metaphor in an extended discourse, and especially for the purpose of modeling the use of metaphor in interaction, however, a broader conceptualization of metaphor is needed in order to accommodate the many places where these simplifying assumptions break down (Jang 320 Proceedings of the SIGDIAL 2017 Conference, pages 320–330, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics metaphorically, or what target and source domai"
W17-5538,N16-1146,0,0.0226168,"“All the world’s a stage and men and women merely players.” (Shakespeare, Twelfth Night) EX(3) “Bobby Holloway says my imagination is a three-hundred-ring circus. Currently I was in ring two hundred and ninety-nine, with elephants dancing and clowns cart wheeling and tigers leaping through rings of fire. The time had come to step back, leave the main tent, go buy some popcorn and a Coke, bliss out, cool down.” (Dean Koontz, Seize the Night. Bantam, 1999) Extraction of Properties So far very little computational work has focused on facets, or properties, of metaphor specifically. However, the Qadir et al. (2016) approach auto322 EX(5) “falling off the wagon is no big thing in my opinion, the psychological good feelings of enjoyment weigh in big for feeling good.” EX(6) “Tina falling off is part of this journey, it is stupid to deny yourself everything.” EX(7) “I am on the wagon so far today . . . ongoing battle.” EX(8) “Tina — hope you stay on the wagon, or at least get back on after you fall!” In the breast cancer discussion forum we use in our work, community participants frequently bring in journey and battle frames when talking about their cancer experience. Depending on what aspects of the cance"
W17-5538,P16-1021,1,0.863785,"Missing"
W17-5538,W03-0404,0,0.0365829,"Missing"
W17-5538,W15-4650,1,0.851491,"Missing"
W17-5538,P14-5010,0,0.00267755,"ontain the frame (e.g. journey) and at least one example seed facet instance. Note that the sentences in the corpus are not annotated metaphorical or literal. Since we are building a frame that can be used either metaphorically or literally, we do not require sen324 As a solution, we propose using lexicogrammatical patterns generated from dependency paths between a domain word and facet words via the ROOT. The lexico-grammatical patterns are defined as the shortest path that passes through the ROOT in dependencies between the domain name and seed facet instances. For example, StanfordCoreNLP (Manning et al., 2014) outputs the dependencies in Table 2 for the sentence She resumed her journey through the city. The lexicogrammatical pattern that connects journey with other candidate property words such as she and city is defined as the reverse path from journey to ROOT combined with the path from ROOT to journey. The paths for the example are shown in Table 3. Words are lemmatized to reduce sparsity. This lexico-grammatical pattern representation has advantages. First, it allows representing patterns connecting pairs of words in a position invariant manner. For example, in our baseline bootstrapping model,"
W17-5538,J15-4002,0,0.0133813,"opting the concept of a frame may be useful for studying metaphor in discourse from a social perspective. Section 4 explains our semi-supervised approach of template induction to model a metaphor frame in detail. Section 5 presents the effectiveness of the frame information through metaphor detection experiments. Section 6 analyzes the results and identifies when the frame information is beneficial. Section 7 concludes the paper. 2 Relation to Prior Work In this section, we discuss previous computational work on metaphor that is most relevant to our study. (For more thorough review, refer to (Shutova, 2015).) Next, Section 2.1 introduces approaches to metaphor detection by modeling metaphorical mapping patterns instead of relying on the idea of violation of linguistic expectations. Section 2.2 reviews work that specifically aims to address problems of metaphor detection in discourse. As a direction related to metaphor detection, Section 2.3 introduces computational work that extracts properties of similes, which provides inspiration for our template induction approach used to induce properties (facets) of a metaphor frame. 2.1 Modeling Metaphorical Mapping There are many different types of metap"
W17-5538,C10-1113,0,0.103882,"illustrate the mechanism through which the frame and topic information enable the more accurate metaphor detection. 1 Introduction Computational work on metaphor has largely focused on metaphor detection within individual sentences, for the purpose of identification of literal meaning, with an eye towards improvement of downstream applications like Machine Translation. This limited conceptualization of metaphor within these restricted contexts has allowed prior work to leverage local indicators to identify metaphorical language, such as the violation of selectional preferences (Martin, 1996; Shutova et al., 2010; Huang, 2014) or the use of abstract vs concrete descriptors (Turney et al., 2011; Brysbaert et al., 2014; Tsvetkov et al., 2013). When detecting metaphor in an extended discourse, and especially for the purpose of modeling the use of metaphor in interaction, however, a broader conceptualization of metaphor is needed in order to accommodate the many places where these simplifying assumptions break down (Jang 320 Proceedings of the SIGDIAL 2017 Conference, pages 320–330, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics metaphorically, or what target an"
W17-5538,N13-1118,0,0.0207064,"or what target and source domains are frequently used together in metaphors. Within these approaches that model frequent target and source domain mappings, Shutova et al. (2010) identified new metaphors by expanding seed metaphors. The idea in this approach is that target concepts that are frequently used with the same source concept occur in similar lexicosyntactic settings. They cluster nouns (target domain) and verbs (source domain), and search the corpus for metaphors that use the verbs in the source domain lexicon to represent the target domain concepts. Extending Shutova et al. (2010), (Shutova and Sun, 2013) find metaphorical mappings by building and traversing a graph of concepts. Then, they generate lists of salient features for the metaphorically connected clusters, and search the corpus for metaphors that use the verbs in the salient features to represent the target domain concepts. Another approach, Hovy et al. (2013) detected metaphors using certain semantic patterns appearing in metaphor manifestations. For example, “sweet” with food is literal, but is metaphorical with people. By finding these patterns on different levels, they extended the application of this mapping information from a n"
W17-5538,N10-2007,0,0.0296799,"Missing"
W17-5538,W13-0904,0,0.260283,"they generate lists of salient features for the metaphorically connected clusters, and search the corpus for metaphors that use the verbs in the salient features to represent the target domain concepts. Another approach, Hovy et al. (2013) detected metaphors using certain semantic patterns appearing in metaphor manifestations. For example, “sweet” with food is literal, but is metaphorical with people. By finding these patterns on different levels, they extended the application of this mapping information from a narrow focus on verb relations to other syntactic relations. Along the same lines, Mohler et al. (2013) presented a domain-aware semantic signature to capture source and target domains for a text. A semantic signature represents the placement of a text on a semantic space by using a set of related WordNet senses, and it includes source concept dimensions and target concept dimensions. The primary idea is that the signature of a known metaphor is used to detect the same conceptual metaphor. These approaches are effective for capturing frequent domain specific metaphorical mappings, and in appropriate contexts are helpful for metaphor detection. They also provided valuable insight to our approach"
W17-5538,W13-0906,0,0.0994871,"ion Computational work on metaphor has largely focused on metaphor detection within individual sentences, for the purpose of identification of literal meaning, with an eye towards improvement of downstream applications like Machine Translation. This limited conceptualization of metaphor within these restricted contexts has allowed prior work to leverage local indicators to identify metaphorical language, such as the violation of selectional preferences (Martin, 1996; Shutova et al., 2010; Huang, 2014) or the use of abstract vs concrete descriptors (Turney et al., 2011; Brysbaert et al., 2014; Tsvetkov et al., 2013). When detecting metaphor in an extended discourse, and especially for the purpose of modeling the use of metaphor in interaction, however, a broader conceptualization of metaphor is needed in order to accommodate the many places where these simplifying assumptions break down (Jang 320 Proceedings of the SIGDIAL 2017 Conference, pages 320–330, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics metaphorically, or what target and source domains are frequently used together in metaphors. Within these approaches that model frequent target and source domain m"
W17-5538,D11-1063,0,0.106048,"more accurate metaphor detection. 1 Introduction Computational work on metaphor has largely focused on metaphor detection within individual sentences, for the purpose of identification of literal meaning, with an eye towards improvement of downstream applications like Machine Translation. This limited conceptualization of metaphor within these restricted contexts has allowed prior work to leverage local indicators to identify metaphorical language, such as the violation of selectional preferences (Martin, 1996; Shutova et al., 2010; Huang, 2014) or the use of abstract vs concrete descriptors (Turney et al., 2011; Brysbaert et al., 2014; Tsvetkov et al., 2013). When detecting metaphor in an extended discourse, and especially for the purpose of modeling the use of metaphor in interaction, however, a broader conceptualization of metaphor is needed in order to accommodate the many places where these simplifying assumptions break down (Jang 320 Proceedings of the SIGDIAL 2017 Conference, pages 320–330, c Saarbr¨ucken, Germany, 15-17 August 2017. 2017 Association for Computational Linguistics metaphorically, or what target and source domains are frequently used together in metaphors. Within these approache"
W17-5538,P10-1072,0,\N,Missing
W19-4502,P16-1154,0,0.0532062,"Missing"
W19-4502,C14-1142,0,0.0550184,"T) (Reed and Budzynska, 2011) explains how propositional contents and the argumentative relations between them are anchored in the expressed locutions by means of illocutionary connections. IAT has been applied to annotate argumentative dialogues of various kinds, including the corpus used in this paper (Section 3). IAT annotation comprises, amongst other things, segmenting the original text into locutions1 , identifying the illoFrom Text to ADUs In most studies, ADUs are obtained via text segmentation. While some studies leave the choice of the boundary of an ADU to the annotator’s judgment (Stab and Gurevych, 2014), many studies employ a set of syntactic rules as a basis. For instance, an ADU can be as fine-grained as a phrase that plays a discrete argumentative function (Stede et al., 2016). In other cases, an ADU may be a clause (Peldszus and Stede, 2015) or a series of clauses that must include a subject, a verb, and an object if necessary (Al Khatib et al., 2016). Based on annotated ADUs, some studies have 1 12 Analogous to ADUs. We use the terms interchangeably. Alice: Bob stopped by my oﬃce and complained, ``Why is the company not launching the new service?&apos;&apos; I think I have explained to him alread"
W19-4502,L16-1167,0,0.360639,"Most argument mining models for identifying the argumentative structure of a text build upon elementary text spans that serve argumentative functions, such as premise and conclusion. In argumentation theory, it is commonly accepted that these building blocks are propositions (Blackburn, 2016), i.e., statements that are either true or false. Despite the foundational role of propositions, however, proposition extraction from text has been little studied in computational argumentation. Instead, most models rely on argumentative discourse units (ADUs) obtained by surface-level text segmentation (Stede et al., 2016; Al Khatib et al., 2016). In what follows, we discuss limitations of ADUs that potentially impinge upon subsequent argument mining processes, and then describe our approach. One limitation of ADUs is that they may lack important semantic information, such as the ref11 Proceedings of the 6th Workshop on Argument Mining, pages 11–24 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics If I&apos;m our nominee, how is Hillary Clinton gonna lecture me about living paycheck to paycheck? I was raised paycheck to paycheck. from argumentative dialogues, with important semantic"
W19-4502,D15-1166,0,0.0231761,"Missing"
W19-5009,D14-1159,0,0.257885,"methods are typically trained on large-scale datasets, but their performances do not directly transfer to lowresource domain-specific settings. In this paper, we propose two approaches for domain adaptation in biological domain that involve pre-training LSTM-CRF based on existing large-scale datasets and adapting it for a low-resource corpus of biological processes. Our first approach defines a mapping between the source labels and the target labels, and the other approach modifies the final CRF layer in sequence-labeling neural network architecture. We perform our experiments on ProcessBank (Berant et al., 2014) dataset which contains less than 200 paragraphs on biological processes. We improve over the previous state-of-the-art system on this dataset by 21 F1 points. We also show that, by incorporating event-event relationship in ProcessBank, we are able to achieve an additional 2.6 F1 gain, giving us possible insights into how to improve SRL systems for biological process using richer annotations. 1 Introduction Semantic Role Labeling (SRL) is shallow semantic representation of a sentence, that allows us to capture the roles of arguments that anchor around an event. Despite significant recent progr"
W90-0117,P89-1025,0,0.0258373,"Missing"
W90-0117,P88-1020,1,0.740701,"Missing"
W90-0117,P88-1022,1,0.858609,"Missing"
W90-0117,C86-1043,0,0.0532748,"Missing"
W90-0117,E89-1023,0,0.0269687,"Missing"
W90-0117,J86-3001,0,\N,Missing
W93-0210,J86-3001,0,0.00862524,"Missing"
W93-0210,P89-1025,0,0.0366832,"Missing"
W93-0210,J92-4007,0,0.0826051,"Missing"
W93-0210,A92-1006,0,0.0658277,"Missing"
W96-0401,W90-0108,0,0.0476712,"Missing"
W96-0401,W96-0508,1,0.869369,"Missing"
W96-0401,A92-1006,0,0.0776024,"Missing"
W96-0401,T78-1009,0,0.345926,"Missing"
W96-0401,W94-0315,0,0.165643,"Missing"
W96-0401,W96-0415,0,0.0597035,"Missing"
W96-0401,C92-2114,0,0.0534274,"Missing"
W96-0401,J95-1002,0,0.206089,"Missing"
W96-0401,W90-0105,1,0.854162,"Missing"
W96-0401,W94-0316,1,0.877546,"Missing"
W96-0401,J93-3002,0,\N,Missing
W96-0401,J78-3015,0,\N,Missing
W96-0508,P84-1002,0,0.109148,"Missing"
W97-0704,W97-0711,0,\N,Missing
W97-0704,A97-1042,1,\N,Missing
W97-0704,C92-2070,0,\N,Missing
W97-0704,P95-1034,0,\N,Missing
X98-1026,P98-1012,0,0.0398847,"Missing"
X98-1026,P98-1116,0,0.0136715,"Missing"
X98-1026,W97-0707,0,0.0146187,"Missing"
X98-1026,A97-1042,1,0.788819,"Missing"
X98-1026,A92-1006,0,0.0422787,"Missing"
X98-1026,W98-1124,0,0.0146725,"Missing"
X98-1026,W97-0711,0,\N,Missing
X98-1026,C98-1112,0,\N,Missing
X98-1026,C98-1012,0,\N,Missing
X98-1026,P95-1046,1,\N,Missing
X98-1026,A97-1039,0,\N,Missing
zhou-etal-2006-summarizing,W03-0508,0,\N,Missing
zhou-etal-2006-summarizing,A97-1042,1,\N,Missing
zhou-etal-2006-summarizing,N04-1019,0,\N,Missing
zhou-etal-2006-summarizing,P99-1072,0,\N,Missing
zhou-etal-2006-summarizing,P99-1071,0,\N,Missing
zhou-etal-2006-summarizing,W04-1013,1,\N,Missing
