2007.iwslt-1.12,2006.iwslt-evaluation.5,1,0.846504,"d on Chinese to English translation, but we also report results on the Arabic to English, Italian to English, and Japanese to English tasks. 1. Introduction We describe experiments conducted at HKUST during the IWSLT 2007 evaluation campaign on spoken language translation. For our second participation in the IWSLT evaluation, our focus was on experimenting with Moses (Koehn et al., 2007), the new open-source toolkit for phrase-based Statistical Machine Translation (SMT), and on comparing it against its closed-source predecessor Pharaoh (Koehn, 2004) which we used in our IWSLT 2006 submission (Carpuat et al., 2006). Our main focus was on the Chinese-English task, which, this year, used clean text as opposed to the other tasks where speech transcriptions were to be translated. We also report results on all the language pairs, although we did not do any tuning or any language-specific processing for the Arabic to English, Japanese to English and Italian to English tasks.  This material is based upon work supported in part by the Defense Advanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC625"
2007.iwslt-1.12,2006.iwslt-evaluation.19,0,0.0290995,"under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 2. Machine translation engine We focus on phrase-based statistical machine translation since this approach has been shown to achieve competitive translation quality in many state-of-the-art systems (e.g., Zens et al. (2005), Shen et al. (2006), Eck et al. (2006)), and therefore is a good benchmark to evaluate decoder architectures. 2.1. Decoder We use the Moses decoder (Koehn et al., 2007), which is an open source toolkit for statistical machine translation. Just like the Pharaoh decoder, Moses uses a log-linear model, which combines several knowledge sources in translation decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based model to incorporate richer linguistic i"
2007.iwslt-1.12,P07-2045,0,0.015333,"he HKUST experiments in the IWSLT 2007 evaluation campaign on spoken language translation. Our primary objective was to compare the open-source phrase-based statistical machine translation toolkit Moses against Pharaoh. We focused on Chinese to English translation, but we also report results on the Arabic to English, Italian to English, and Japanese to English tasks. 1. Introduction We describe experiments conducted at HKUST during the IWSLT 2007 evaluation campaign on spoken language translation. For our second participation in the IWSLT evaluation, our focus was on experimenting with Moses (Koehn et al., 2007), the new open-source toolkit for phrase-based Statistical Machine Translation (SMT), and on comparing it against its closed-source predecessor Pharaoh (Koehn, 2004) which we used in our IWSLT 2006 submission (Carpuat et al., 2006). Our main focus was on the Chinese-English task, which, this year, used clean text as opposed to the other tasks where speech transcriptions were to be translated. We also report results on all the language pairs, although we did not do any tuning or any language-specific processing for the Arabic to English, Japanese to English and Italian to English tasks.  This"
2007.iwslt-1.12,E06-1031,0,0.0938417,"(DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 2. Machine translation engine We focus on phrase-based statistical machine translation since this approach has been shown to achieve competitive translation quality in many state-of-the-art systems (e.g., Zens et al. (2005), Shen et al. (2006), Eck et al. (2006)), and therefore is a good benchmark to evaluate decoder architectures. 2.1. Decoder We use the Moses decoder (Koehn et al., 2007), which is an open source toolkit for statistical machine translation. Just like the Pharaoh decoder, Moses uses a log-linear model, which combines several knowledge sources in translation decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based model to incorporate"
2007.iwslt-1.12,P02-1038,0,0.0434894,"n decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based model to incorporate richer linguistic information. However, we do not use the factored representation in this first set of experiments, and use the surface form of words, just like in Pharaoh. 2.2. Phrasal bilexicon The core phrasal bilexicon is obtained by collecting phrase pairs that are consistent with the IBM model 4 alignments obtained with GIZA++ (Och and Ney, 2002). During phrase extraction, we tried two different methods to get the final word alignment from the bi-directional GIZA++ alignments: (1) intersect and (2) grow-diagfinal. Intersect uses the strict intersection of the bidirectional word alignments, while grow-diag-final expands the alignment by adding directly neighboring alignment points, and alignment points in the diagonal neighborhood. We found that using grow-diag-final improves IWSLT-07 data set CE devtest1 CE devtest2 CE devtest3 Table 1: Resegmenting test sentences improves BLEU score. # original # sentences after BLEU with original BL"
2007.iwslt-1.12,P02-1040,0,0.0935189,"not optimal, as Italian presents more morphological inflexions than English, as suggested by the larger vocabulary size on the Italian side of the training data than on the English side (Table 2.) Japanese: We used the provided word segmentation and did not perform any additional processing. 5. Experimental results The official BLEU scores for HKUST’s submitted runs, which were buggy due to accidental errors in combining the models and parameters used in the experiments, are shown in Table 3 for all four language pairs. The official results were only automatically evaluated using BLEU score (Papineni et al., 2002). We achieved a BLEU score of 34.26 on Chinese to English read speech translation. There were 9 primary submissions to that task, with BLEU scores ranging from 19.34 to 40.77. Our subsequent debugged runs yielded higher translation accuracy. Updated results for our debugged runs on the development sets are reported in Table 4 for the Chinese-English task. For running the submitted buggy model on the official IWSLT-07 test set, there is a slight difference between the official BLEU score of 34.26 and our own measurement of 34.04. This difference appears to be caused by slight differences betwee"
2007.iwslt-1.12,2006.amta-papers.25,0,0.0178306,"on the development sets are reported in Table 4 for the Chinese-English task. For running the submitted buggy model on the official IWSLT-07 test set, there is a slight difference between the official BLEU score of 34.26 and our own measurement of 34.04. This difference appears to be caused by slight differences between BLEU scoring tools and settings (the tool we are using appears to give lower scores).We also computed the other most commonly used automatic evaluation metrics for translation quality: NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) and Translation Edit Rate (TER) (Snover et al., 2006), Word Error Rate (WER), Position-Independent Word Error Rate (PER) and CDER (Leusch et al., 2006). 6. Comparing Moses results with Pharaoh Using the same phrasal bilexicon and language model as with Moses, we performed several contrastive runs using Pharaoh, all other settings being identical. Results are reported in Table 5 for three different baseline experimental settings. We performed many experimental runs in which we vary the experimental settings and pre or post processing steps, e.g. phrase tables, language models, to compare the translation quality produced by Pharoah and Moses. The"
2007.iwslt-1.12,P96-1021,1,0.764787,"Missing"
2007.iwslt-1.12,J97-3002,1,0.656199,"Missing"
2007.iwslt-1.12,2005.iwslt-1.20,0,0.0197801,"vanced Research Projects Agency (DARPA) under GALE Contract No. HR0011-06-C-0023, and by the Hong Kong Research Grants Council (RGC) research grants RGC6083/99E, RGC6256/00E, and DAG03/04.EG09. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the Defense Advanced Research Projects Agency. 2. Machine translation engine We focus on phrase-based statistical machine translation since this approach has been shown to achieve competitive translation quality in many state-of-the-art systems (e.g., Zens et al. (2005), Shen et al. (2006), Eck et al. (2006)), and therefore is a good benchmark to evaluate decoder architectures. 2.1. Decoder We use the Moses decoder (Koehn et al., 2007), which is an open source toolkit for statistical machine translation. Just like the Pharaoh decoder, Moses uses a log-linear model, which combines several knowledge sources in translation decisions. Moses differs from Pharaoh by its represention of each input word as a factor as opposed to the word suface form only. Factors can include additional information such as part-ofspeech, class, morphology and allow the phrase-based m"
2007.iwslt-1.12,2006.iwslt-evaluation.20,0,0.0201845,"previous evaluations where manual and automatic transcriptions of speech had to be translated. The IWSLT 2007 test set therefore matches more closely with the training data and the first three development test data, as opposed to the more recent Chinese-English tasks where automatic transcriptions of read and spontaneous speech were used. 3.2. Training data preprocessing For the training data, we used the same basic preprocessing as in our IWSLT 2006 submission, which consists in performing tokenization and case normalization. The case normalization method is the same as the one described in Zollmann et al. (2006), where the first word of the sentence is normalized to its most frequent form. English: The English was simply tokenized and case-normalized in the same manner for all languages. Chinese: We use the LDC segmenter to re-segment the Chinese side of the corpus to get a better segmentation. 3.3. English text normalization For all language pairs, in addition to training data normalization, we use simple heuristics to normalize punctuation, capitalization and contractions in the English output. 3.4. Improved sentence segmentation Since the training data is drawn from clean text as opposed to speech"
2007.iwslt-1.12,koen-2004-pharaoh,0,\N,Missing
2007.iwslt-1.12,W05-0909,0,\N,Missing
2007.iwslt-1.12,D07-1007,1,\N,Missing
2007.iwslt-1.12,D08-1064,0,\N,Missing
2007.iwslt-1.12,C08-1141,0,\N,Missing
2007.iwslt-1.12,2005.iwslt-1.16,0,\N,Missing
2012.eamt-1.64,W07-0718,0,0.2635,"Missing"
2012.eamt-1.64,2011.eamt-1.3,0,0.0287249,"Missing"
2012.eamt-1.64,W08-0309,0,0.234232,"Missing"
2012.eamt-1.64,W07-0738,0,0.354497,"Missing"
2012.eamt-1.64,W08-0332,0,0.246741,"Missing"
2012.eamt-1.64,J09-4009,0,0.0233322,"Huang et al., 2009). The linear time skeleton algorithm builds canonical binarization trees by reducing greedily but such an approach would not work for a LTG. For example, the permutation [3, 2, 0, 1] which can be parsed by an LTG reduces to 2-3, 0-1 on the stack which cannot be further reduced. 296 We propose an algorithm that makes use of a technique similar to top-down parsing of bisentences using linear transduction grammars. The algorithm is as shown in the procedure parsable. In order to prove the correctness of the algorithm, we use the deﬁnition of permuted sequence from Huang et al. (2009) but we redeﬁne proper split in the context of BLTGs. The proof is as follows: Deﬁnition 1. A permuted sequence is a permutation of consecutive integers. If a permuted sequence of sequence a can be split into the concatenation of a permuted sequence b and a single element of permutation α such that a = (b; α) or a = (α; b), then the corresponding split is called the proper split of a. The deﬁnition of a proper split implicitly imposes the constraints of a linear transduction grammar. Restricting one of the elements in a split to be a single element in the permutation is equivalent to allowing"
2012.eamt-1.64,P11-1023,1,0.855519,"Missing"
2012.eamt-1.64,J05-1004,0,0.0113465,"labels, we noticed that there were some inconsistencies in the annotation. The sentence pairs were manually annotated with the frame sets deﬁned for Chinese and English Zh/En alt. patterns [arg0:action:arg1] [arg0:action] [action:arg1] [arg1:action] [action:arg2] [arg0:action:arg2] [action:arg4] [arg1:action:arg2] [arg1:action:arg4] Sum [arg0:action:arg1] 88 0 0 0 0 3 0 3 3 97 [arg0:action 0 11 3 12 1 0 1 0 0 28 [action:arg1] 0 0 39 6 5 0 1 0 0 51 [arg1:action] 0 0 1 3 0 0 0 0 0 4 Sum 88 11 43 21 6 3 2 3 3 Table 1: Frequency of source and target alternation pattern occurrence in the Propbank (Palmer et al., 2005). We argue that it is due to the limitation of frame set deﬁnitions as they were deﬁned to be consistent within one language but not across languages. For example, in the frame set deﬁnition of 死于 (died of), the arg0 is the entity who dies, while in the frame set deﬁnition of its translation die, the deceased is arg1 and there is no arg0 deﬁned. Similar observations could be made for most of the sentence pairs which diﬀered in source and target alternation labels. As our initial analysis of cross-lingual verb frame alternation patterns suggests that patterns in one language align with only a r"
2012.eamt-1.64,2011.mtsummit-papers.49,1,0.897227,"Missing"
2012.eamt-1.64,N09-2004,1,0.901535,"Missing"
2012.eamt-1.64,J97-3002,1,0.916868,"nd Linear Transduction Grammars (Saers, 2011). As a part of our evaluation, we discuss the reordering of semantic roles within a frame and across frames within a sentence. We also present a novel algorithm to determine whether there exists a canonical parse for an alignment under Linear Transduction Grammars. © 2012 European Association for Machine Translation. 295 To fulﬁll the above requirements, we evaluate two well known syntax-based machine translation formalisms: Inversion Transduction Grammars or ITGs (Wu, 1997) and Linear Transduction Grammars or LTGs (Saers, 2011). As discussed in Wu (1997), ITGs allow nearly all possible reorderings (22 out of 24) given up to four semantic role labels within a semantic frame. Further, various forms of empirical conﬁrmation for the eﬀectiveness of ITG expressivity constraints (Zens and Ney, 2003; Zhang and Gildea, 2005, 2004) motivate us to choose it as a likely candidate. Though ITGs are far more constraining than other higher order syntax directed transduction grammars and IBM models, it would be interesting to see how far an even more constrained model is able to handle reorderings of semantic role ﬁllers. For this purpose, we choose LTGs whi"
2012.eamt-1.64,P03-1019,0,0.745834,"Missing"
2012.eamt-1.64,C04-1060,0,0.0688712,"Missing"
2012.eamt-1.64,P05-1059,0,0.658455,"Missing"
2012.eamt-1.64,H93-1040,0,\N,Missing
2012.eamt-1.64,1993.mtsummit-1.24,0,\N,Missing
2012.eamt-1.64,C00-2108,0,\N,Missing
2012.eamt-1.64,2009.eamt-1.30,1,\N,Missing
2012.eamt-1.64,W09-2300,0,\N,Missing
2012.eamt-1.64,N04-1030,0,\N,Missing
2013.iwslt-evaluation.2,P11-1023,1,0.791624,"13 German-English MT and SLT tracks which show that HMEANT provides a perspective which is different from BLEU and TER in evaluating the performance of the MT systems. The IWSLT evaluation campaign has offered a variety of speech translation tasks over the past decade but none of them included evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should"
2013.iwslt-evaluation.2,W11-1002,1,0.754734,"13 German-English MT and SLT tracks which show that HMEANT provides a perspective which is different from BLEU and TER in evaluating the performance of the MT systems. The IWSLT evaluation campaign has offered a variety of speech translation tasks over the past decade but none of them included evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should"
2013.iwslt-evaluation.2,2006.amta-papers.25,0,0.0441286,"mpaign has offered a variety of speech translation tasks over the past decade but none of them included evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tunin"
2013.iwslt-evaluation.2,P02-1040,0,0.0974466,"r the past decade but none of them included evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, sup"
2013.iwslt-evaluation.2,W05-0909,0,0.0320103,"none of them included evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper"
2013.iwslt-evaluation.2,E06-1031,0,0.0200686,"ed evaluation of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the re"
2013.iwslt-evaluation.2,niessen-etal-2000-evaluation,0,0.0515966,"tion of system performance using a semantic MT evaluation metric because of the inherent cost in evaluation in terms of both the (a) amount of time, and (b) the level of expertise needed by the human annotators. We choose HMEANT as a way around these challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of"
2013.iwslt-evaluation.2,E06-1032,0,0.0187295,"challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of one of the largest semantic MT evaluations to date, in terms of both the number of systems and the number of translations evaluated, using HMEANT as the evaluation metric. The aims of this evaluation campaign are two-fold: (1) to demonstrate feasibility o"
2013.iwslt-evaluation.2,W06-3114,0,0.0277223,"challenges given substantial empirical evidence [1, 2] that HMEANT is a inexpensive, simple, and representationally transparent semantic MT evaluation metric that correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of one of the largest semantic MT evaluations to date, in terms of both the number of systems and the number of translations evaluated, using HMEANT as the evaluation metric. The aims of this evaluation campaign are two-fold: (1) to demonstrate feasibility o"
2013.iwslt-evaluation.2,P13-2067,1,0.759382,"t correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of one of the largest semantic MT evaluations to date, in terms of both the number of systems and the number of translations evaluated, using HMEANT as the evaluation metric. The aims of this evaluation campaign are two-fold: (1) to demonstrate feasibility of running a large-scale semantic MT evaluation campaign using humans, and (2) to provide fine-grained statistics over a large number of systems that enable a fair com"
2013.iwslt-evaluation.2,2013.mtsummit-papers.12,1,0.783852,"t correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of one of the largest semantic MT evaluations to date, in terms of both the number of systems and the number of translations evaluated, using HMEANT as the evaluation metric. The aims of this evaluation campaign are two-fold: (1) to demonstrate feasibility of running a large-scale semantic MT evaluation campaign using humans, and (2) to provide fine-grained statistics over a large number of systems that enable a fair com"
2013.iwslt-evaluation.2,2013.iwslt-evaluation.5,1,0.716509,"t correlates with human translation adequacy judgements more highly than HTER [3] and other automatic MT evaluation metrics, such as BLEU [4], NIST [5], METEOR [6], PER [7], CDER [8], WER [9], and TER [3]. Although fast and inexpensive lexical n-gram based objective functions like BLEU have driven MT system development over the past decade, these metrics do not enforce translation utility adequately and often fail to preserve meaning [10, 11]. We believe that the system development should also be driven by semantic MT evaluation metrics which focus on getting the meaning right. Recent results [12, 13, 14] which indicate that more adequate translations are produced by tuning MT systems using the semantic evaluation metric MEANT, support us. In this paper, we present the results of one of the largest semantic MT evaluations to date, in terms of both the number of systems and the number of translations evaluated, using HMEANT as the evaluation metric. The aims of this evaluation campaign are two-fold: (1) to demonstrate feasibility of running a large-scale semantic MT evaluation campaign using humans, and (2) to provide fine-grained statistics over a large number of systems that enable a fair com"
2013.iwslt-evaluation.2,W12-3129,1,0.677229,"rences between metrics gauging semantic similarity and surface based metrics, but also quantifies the robustness of HMEANT as an MT evaluation metric. In the rest of the paper, we discuss the details of the evaluation campaign and provide results on the interannotator agreement on the tasks of semantic role annotation and alignment. We also provide an analysis of the time taken for annotation and the alignment of the semantic roles. We also report the results of different participating systems according to the criterion of our semantic evaluation metric HMEANT and its automatic variant, MEANT [15]. 2. Human judges align the semantic frames between the references and the MT output by judging the correctness of the predicates. 3. For each pair of aligned semantic frames, (a) Human judges determine the translation correctness of the semantic role fillers. (b) Human judges align the semantic role fillers between the reference and the MT output according to the correctness of the semantic role fillers. 2. Participating tracks and systems To perform a full-scale semantic MT evaluation, all the systems which participated in IWSLT 2013 GermanEnglish MT and SLT tracks were evaluated. There were"
2013.iwslt-evaluation.2,W12-4206,1,0.732282,"Missing"
2013.iwslt-evaluation.5,P02-1040,0,0.101561,"nglish and English-Chinese translation with that of the baseline SMT systems tuned against BLEU. We show that the improvement of tuning a MT system against MEANT on Chinese translation output is more significant because of the nature of ambiguous word boundaries in Chinese. Our encouraging results show that using MEANT is a promising alternative to BLEU in evaluating and tuning MT systems to drive the progress of MT research across different languages. In the past decade, the progress of MT research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU [1], which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning[2, 3]. Unlike BLEU, or other n-gram based MT evaluation metrics, the MEANT family of metrics [4, 5, 6] adopt at outset the principle that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, wher"
2013.iwslt-evaluation.5,P11-1023,1,0.938243,"valuating and tuning MT systems to drive the progress of MT research across different languages. In the past decade, the progress of MT research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU [1], which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning[2, 3]. Unlike BLEU, or other n-gram based MT evaluation metrics, the MEANT family of metrics [4, 5, 6] adopt at outset the principle that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU"
2013.iwslt-evaluation.5,W12-4206,1,0.886653,"valuating and tuning MT systems to drive the progress of MT research across different languages. In the past decade, the progress of MT research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU [1], which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning[2, 3]. Unlike BLEU, or other n-gram based MT evaluation metrics, the MEANT family of metrics [4, 5, 6] adopt at outset the principle that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU"
2013.iwslt-evaluation.5,W12-3129,1,0.854369,"valuating and tuning MT systems to drive the progress of MT research across different languages. In the past decade, the progress of MT research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU [1], which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning[2, 3]. Unlike BLEU, or other n-gram based MT evaluation metrics, the MEANT family of metrics [4, 5, 6] adopt at outset the principle that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU"
2013.iwslt-evaluation.5,N04-1030,0,0.345627,"sume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning[2, 3]. Unlike BLEU, or other n-gram based MT evaluation metrics, the MEANT family of metrics [4, 5, 6] adopt at outset the principle that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or"
2013.iwslt-evaluation.5,W05-0909,0,0.788856,"inciple that a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or TER with a semantic frame based objective function, MEANT, when tuning the parameters of MT systems [13, 14]. However, the question of whether the same approach of tuning MT systems against a semantic objective function might improve translation adequacy when translating into other languages, such as Chinese, is left unanswered. Although there exists no studies on correlation"
2013.iwslt-evaluation.5,E06-1031,0,0.811225,"at a good translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or TER with a semantic frame based objective function, MEANT, when tuning the parameters of MT systems [13, 14]. However, the question of whether the same approach of tuning MT systems against a semantic objective function might improve translation adequacy when translating into other languages, such as Chinese, is left unanswered. Although there exists no studies on correlation between hu"
2013.iwslt-evaluation.5,niessen-etal-2000-evaluation,0,0.911651,"translation is one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or TER with a semantic frame based objective function, MEANT, when tuning the parameters of MT systems [13, 14]. However, the question of whether the same approach of tuning MT systems against a semantic objective function might improve translation adequacy when translating into other languages, such as Chinese, is left unanswered. Although there exists no studies on correlation between human adequa"
2013.iwslt-evaluation.5,2006.amta-papers.25,0,0.717561,"one from which humans can successfully understand at least the central meaning of the input sentence as captured by the basic event structure— “who did what to whom, when, where and why”[7]. [6]MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or TER with a semantic frame based objective function, MEANT, when tuning the parameters of MT systems [13, 14]. However, the question of whether the same approach of tuning MT systems against a semantic objective function might improve translation adequacy when translating into other languages, such as Chinese, is left unanswered. Although there exists no studies on correlation between human adequacy judgement a"
2013.iwslt-evaluation.5,2013.mtsummit-papers.12,1,0.711088,"similarities between the semantic frame structures of output and reference translations. For evaluating English translations, we have shown that MEANT correlates better with human adequacy judgment than commonly used MT evaluation metrics, such as BLEU [1], NIST [8], METEOR [9], CDER [10], WER [11], and TER [12]. We recently showed that the translation adequacy across different genres (ranging from formal news to informal web forum) is improved by replacing surface oriented metrics like BLEU or TER with a semantic frame based objective function, MEANT, when tuning the parameters of MT systems [13, 14]. However, the question of whether the same approach of tuning MT systems against a semantic objective function might improve translation adequacy when translating into other languages, such as Chinese, is left unanswered. Although there exists no studies on correlation between human adequacy judgement and MEANT scores on Chinese output, we hypothesize that the benefits of tuning against MEANT that we see for English: better adequacy and fluency carries over into Chinese. It is because a high MEANT score is contingent on correct lexical choices as well as getting the syntactic and semantic str"
2013.iwslt-evaluation.5,W07-0738,0,0.119299,"nd TER [12] do not correctly reflect the similarity of the basic event structure— “who did what to whom, when, where and why”— of the input sentence. In fact, a number of large scale meta-evaluations [2, 3] report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work on developing MT evaluation metrics that would outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticate"
2013.iwslt-evaluation.5,W08-0332,0,0.0857312,"nd TER [12] do not correctly reflect the similarity of the basic event structure— “who did what to whom, when, where and why”— of the input sentence. In fact, a number of large scale meta-evaluations [2, 3] report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work on developing MT evaluation metrics that would outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticate"
2013.iwslt-evaluation.5,W07-0718,0,0.086525,"umber of large scale meta-evaluations [2, 3] report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work on developing MT evaluation metrics that would outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these me"
2013.iwslt-evaluation.5,W08-0309,0,0.0746276,"umber of large scale meta-evaluations [2, 3] report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work on developing MT evaluation metrics that would outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these me"
2013.iwslt-evaluation.5,W12-3107,0,0.0975052,"ecent surge of work on developing MT evaluation metrics that would outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these metrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive"
2013.iwslt-evaluation.5,W12-3103,0,0.067404,"with human adequacy judgment [16], however, it is very hard to interpret and indicate what errors the MT systems are making. ULC [17, 18] is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18] but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE [21] is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these metrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters that need to be estimated. ROSE [23] is a weighted linear model of shallow linguistic features"
2013.iwslt-evaluation.5,W11-2113,0,0.0119603,"ce needed for the MT output to match the reference. Sagan [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these metrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters that need to be estimated. ROSE [23] is a weighted linear model of shallow linguistic features which is cheaper in run time but still contains several dozens of weights that need to be tuned, which makes it hard to port the metric to different domains. TINE [24] is an automatic recall-oriented evaluation metric which aims to preserve the basic event structure. However, it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, there is very little work on designing MT evaluation metrics for evaluating Chinese or other languages with ambiguous word boundaries. For instance, stud"
2013.iwslt-evaluation.5,W11-2112,0,0.151131,"ntain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase tables. Like ULC, these metrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters that need to be estimated. ROSE [23] is a weighted linear model of shallow linguistic features which is cheaper in run time but still contains several dozens of weights that need to be tuned, which makes it hard to port the metric to different domains. TINE [24] is an automatic recall-oriented evaluation metric which aims to preserve the basic event structure. However, it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, there is very little work on designing MT evaluation metrics for evaluating Chinese or other languages with ambiguous word boundaries. For instance, studies show that simply adapting the commonly used MT evaluation metrics to evaluate Chinese on characterlevel showed a higher correlation with human judgment than the original word-level evaluation metrics [25]. Later, TESLA-CEL"
2013.iwslt-evaluation.5,P11-2028,0,0.018332,"ent domains. TINE [24] is an automatic recall-oriented evaluation metric which aims to preserve the basic event structure. However, it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, there is very little work on designing MT evaluation metrics for evaluating Chinese or other languages with ambiguous word boundaries. For instance, studies show that simply adapting the commonly used MT evaluation metrics to evaluate Chinese on characterlevel showed a higher correlation with human judgment than the original word-level evaluation metrics [25]. Later, TESLA-CELAB is introduced as a hybrid character-level and word-level MT evaluation metric for evaluating Chinese [26]. Although TESLA-CELAB correlates significantly better with human judgment for evaluating Chinese than BLEU, no work has been done towards tuning an SMT system for translating into Chinese using it. 2.2. The MEANT family of metrics MEANT [6], which is the weighted f-score over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlating with human adequacy judgment. MEANT"
2013.iwslt-evaluation.5,P12-1097,0,0.0126949,"ever, it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, there is very little work on designing MT evaluation metrics for evaluating Chinese or other languages with ambiguous word boundaries. For instance, studies show that simply adapting the commonly used MT evaluation metrics to evaluate Chinese on characterlevel showed a higher correlation with human judgment than the original word-level evaluation metrics [25]. Later, TESLA-CELAB is introduced as a hybrid character-level and word-level MT evaluation metric for evaluating Chinese [26]. Although TESLA-CELAB correlates significantly better with human judgment for evaluating Chinese than BLEU, no work has been done towards tuning an SMT system for translating into Chinese using it. 2.2. The MEANT family of metrics MEANT [6], which is the weighted f-score over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlating with human adequacy judgment. MEANT is easily portable to other languages requiring only an automatic semantic parser and a large monolingual corpus in the outpu"
2013.iwslt-evaluation.5,D11-1035,0,0.0282041,"labels in MEANT as defined in [27]. For MEANT, wpred and wj are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments [4]. For UMEANT, wpred and wj are estimated in an unsupervised manner using relative frequency of each semantic role label in the reference translations. UMEANT can thus be used when when human judgments on adequacy of the development set are unavailable [5]. 2.3. Tuning against better evaluation metrics Previous works show that tuning MT system against better evaluation metrics improve the translation quality [28, 29]. Recent studies [13, 14] also shows that tuning MT system against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. Therefore, we believe that tuning MT systems against MEANT would improve the adequacy on Chinese MT output. 3. Experimental setup In this section, we describe the details of our systems for the English-Chinese and Chinese-English TED talk MT tasks in terms of data, preprocesing, SMT pipeline and MEANT settings. 3."
2013.iwslt-evaluation.5,P03-1021,0,0.0739366,"was used as a part of development set other than the officially released development set. In order to test the consistency of the experimental results the test sets of IWSLT 2011 and 2012 were used in addition to the IWSLT 2013 test set. We perform minimal preprocessing on the training data running a maximum entropy Chinese segmenter [30] along with numex/timex segmenter on the Chinese data and punctuation tokenization and true casing on the English data. 3.2. SMT pipeline With the goal of improving MT utility by using MEANT as an objective function to drive minimum error rate training (MERT) [31] of state-of-the-art MT systems, we setup our baseline using Moses [32], an off-the-shelf translation toolkit. In this paper we have two baselines: a flat phrase-based MT and a hierarchical phrase-based MT [33]. This allows us to use Moses to compare the performance of MEANT-tuned systems in these two different MT paradigms. The language models are trained using the SRI language model toolkit [34]. For both translation tasks, we used a 6-gram language model. We use ZMERT [35] to tune the baseline because it is a widely used, highly competitive, robust, and reliable implementation of MERT that"
2013.iwslt-evaluation.5,P07-2045,0,0.0071028,"d development set. In order to test the consistency of the experimental results the test sets of IWSLT 2011 and 2012 were used in addition to the IWSLT 2013 test set. We perform minimal preprocessing on the training data running a maximum entropy Chinese segmenter [30] along with numex/timex segmenter on the Chinese data and punctuation tokenization and true casing on the English data. 3.2. SMT pipeline With the goal of improving MT utility by using MEANT as an objective function to drive minimum error rate training (MERT) [31] of state-of-the-art MT systems, we setup our baseline using Moses [32], an off-the-shelf translation toolkit. In this paper we have two baselines: a flat phrase-based MT and a hierarchical phrase-based MT [33]. This allows us to use Moses to compare the performance of MEANT-tuned systems in these two different MT paradigms. The language models are trained using the SRI language model toolkit [34]. For both translation tasks, we used a 6-gram language model. We use ZMERT [35] to tune the baseline because it is a widely used, highly competitive, robust, and reliable implementation of MERT that is also fully configurable and extensible with regard to incorporating"
2013.iwslt-evaluation.5,J07-2003,0,0.129562,"to the IWSLT 2013 test set. We perform minimal preprocessing on the training data running a maximum entropy Chinese segmenter [30] along with numex/timex segmenter on the Chinese data and punctuation tokenization and true casing on the English data. 3.2. SMT pipeline With the goal of improving MT utility by using MEANT as an objective function to drive minimum error rate training (MERT) [31] of state-of-the-art MT systems, we setup our baseline using Moses [32], an off-the-shelf translation toolkit. In this paper we have two baselines: a flat phrase-based MT and a hierarchical phrase-based MT [33]. This allows us to use Moses to compare the performance of MEANT-tuned systems in these two different MT paradigms. The language models are trained using the SRI language model toolkit [34]. For both translation tasks, we used a 6-gram language model. We use ZMERT [35] to tune the baseline because it is a widely used, highly competitive, robust, and reliable implementation of MERT that is also fully configurable and extensible with regard to incorporating new evaluation metrics. 3.3. MEANT for evaluating Chinese Since UMEANT is shown to be more stable when evaluating translations across diffe"
2013.iwslt-evaluation.5,W13-2254,1,0.365957,"n these two different MT paradigms. The language models are trained using the SRI language model toolkit [34]. For both translation tasks, we used a 6-gram language model. We use ZMERT [35] to tune the baseline because it is a widely used, highly competitive, robust, and reliable implementation of MERT that is also fully configurable and extensible with regard to incorporating new evaluation metrics. 3.3. MEANT for evaluating Chinese Since UMEANT is shown to be more stable when evaluating translations across different language pairs [36], we use a UMEANT framework along the lines described in [37] for evaluating both English and Chinese. However, for evaluating Chinese, MEANT has to be equipped with a Chinese shallow semantic parser in order to capture the semantic frames in the Chinese translation output. For this purpose, we used C-ASSERT [38] because of its high accuracy. Since the primary objective in this experiment is studying the feasibility of tuning MT systems against Chinese MEANT, we limited ourselves to using a window-size3 context vector model trained on the word segmented monolingual Chinese gigaword corpus, for estimating the phrasal similarity of the semantic role fille"
2013.iwslt-evaluation.5,E06-1032,0,\N,Missing
2013.iwslt-evaluation.5,W12-3102,0,\N,Missing
2013.iwslt-evaluation.5,W06-3114,0,\N,Missing
2013.iwslt-evaluation.5,P13-2067,1,\N,Missing
2013.iwslt-evaluation.5,W12-3104,0,\N,Missing
2013.mtsummit-papers.12,W11-2136,0,0.0199318,"difficulty of semantic parsing. Below, we describe some of the attempts to (a) improve informal text translation quality using domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semantics into various stages of the SMT pipeline, for example, preprocessing the input (Komachi et al., 2006; Wu et al., 2011), training treeto-string MT models (Liu and Gildea, 2010; Aziz et al., 2011), training reordering model (Xiong et al., 2012) and reordering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and (2) only small volu"
2013.mtsummit-papers.12,W05-0909,0,0.40075,"of the discourse, through contextual modelling. Again, all these approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translati"
2013.mtsummit-papers.12,2011.mtsummit-papers.32,0,0.0267266,"Missing"
2013.mtsummit-papers.12,E06-1032,0,0.321911,"Missing"
2013.mtsummit-papers.12,W07-0718,0,0.310907,"ch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008). Lambert et al. (2006) tuned on QUEEN, a simplified version of ULC, that discards the semantic features of ULC and bases on pure lexical similarity. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to a semantic objective function. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but only performs"
2013.mtsummit-papers.12,W08-0309,0,0.535531,"Missing"
2013.mtsummit-papers.12,N10-1080,0,0.0197458,"ion metrics. In this experiment, we use a MEANT implementation along the lines described in Lo et al. (2012) and Tumuluru et al. (2012) but we incorporate a variant of the aggregation function proposed in Mihalcea et al. (2006) for phrasal similarity of role fillers because it normalizes the phrase length better than geometric mean. 4 Results Of course, tuning against any metric would maximize the performance of the SMT system on that particular metric; it would be overfitting. In the following, we avoid comparing on metrics too similar to the one that the system was tuned on. This is because Cer et al. (2010) showed that tuning on METEOR ↑ 34.63 30.85 33.08 WER ↓ 80.09 76.15 77.32 CDER ↓ 64.54 57.96 61.01 TER ↓ 76.12 74.73 74.64 MEANT ↑ 17.11 15.39 17.27 METEOR, TER and their variations would do well on metrics similar to what they were tuned on but perform particularly poorly on the other metrics. Therefore, it is less meaningful to evaluate a system on metrics similar to what they were tuned on. A far more worthwhile goal would be to bias the SMT system to produce adequate translations while achieving the best scores across all the metrics. In addition, we believe a good translation is one from"
2013.mtsummit-papers.12,W07-0738,0,0.55035,"es not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008). Lambert et al. (2006) tuned on QUEEN, a simplified version of ULC, that discards the semantic features of ULC and bases on pure lexical similarity. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any seman"
2013.mtsummit-papers.12,W08-0332,0,0.497374,"ailing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008). Lambert et al. (2006) tuned on QUEEN, a simplified version of ULC, that discards the semantic features of ULC and bases on pure lexical similarity. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to a semantic objective function. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but only performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment"
2013.mtsummit-papers.12,W06-3114,0,0.488604,"Missing"
2013.mtsummit-papers.12,2006.iwslt-evaluation.11,0,0.163443,"ility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008). Lambert et al. (2006) tuned on QUEEN, a simplified version of ULC, that discards the semantic features of ULC and bases on pure lexical similarity. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to a semantic objective function. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but only performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, Lo et al"
2013.mtsummit-papers.12,E06-1031,0,0.570378,"tual modelling. Again, all these approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch e"
2013.mtsummit-papers.12,C10-1081,0,0.015891,"ormal text due to the difficulty of semantic parsing. Below, we describe some of the attempts to (a) improve informal text translation quality using domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semantics into various stages of the SMT pipeline, for example, preprocessing the input (Komachi et al., 2006; Wu et al., 2011), training treeto-string MT models (Liu and Gildea, 2010; Aziz et al., 2011), training reordering model (Xiong et al., 2012) and reordering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and"
2013.mtsummit-papers.12,C10-1079,0,0.0625948,"Missing"
2013.mtsummit-papers.12,P11-1023,1,0.86082,"where mi and ri are the weights for frame i that estimate the degree of contribution of the frame to the overall meaning of the sentence in the MT/REF respectively. Mi,j and Ri,j are the total counts of argument of type j in frame i in the MT and REF respectively. Si,pred and Si,j are the lexical/phrasal similarities of the predicates and role fillers of the arguments of type j between the MT and REF. wpred and wj are the weights of the the predicates and role fillers of the arguments of type j in the MT and REF. There are 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b) and they are determined by optimizing the correlation with human adequacy judgments using grid search (Lo and Wu, 2011a). 3 Tuning SMT against MEANT We now show that using MEANT as an objective function to drive minimum error rate training (MERT) of state-of-the-art MT systems improves MT utility in the informal genres. Aiming at improving SMT adequacy of informal genres, we set up two experiments on public speech TED talk data and web forum data. The TED talk MT system is trained on the IWSLT2012 ChineseEnglish parallel TED talk training data consisting of over 130k sentences pairs. The de"
2013.mtsummit-papers.12,W12-3129,1,0.849085,"Missing"
2013.mtsummit-papers.12,P13-2067,1,0.225645,"hereas He and Deng (2011) proposed to classify the training data into in-domain or out-of-domain for training two independent translation model and then combine the two models using a system combination approach. Mei and Kirchhoff (2010) incorporated document-level semantics, such as topic of the discourse, through contextual modelling. Again, all these approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more l"
2013.mtsummit-papers.12,C10-1138,0,0.0226934,"ering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and (2) only small volumes of high-quality parallel training data are available (Mei and Kirchhoff, 2010). Rao et al. (2007) and Wang et al. (2010) proposed to remove disfluency in preprocessing stage; Bertoldi et al. (2010) introduced a model to recover the misspelled words before translation; Banerjee et al. (2011) addressed the data sparsity problem by mixing data from comparable domain into the training of both the translation model and the language model whereas He and Deng (2011) proposed to classify the training data into in-domain or out-of-domain for training two independent translation model and then combine the two models using a system combination approach. Mei and Kirchhoff (2010) in"
2013.mtsummit-papers.12,niessen-etal-2000-evaluation,0,0.849474,"se approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates a large set of linguistic features, including several semantic features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrqu"
2013.mtsummit-papers.12,J05-1004,0,0.0591182,"Missing"
2013.mtsummit-papers.12,P02-1040,0,0.0951864,"training data into in-domain or out-of-domain for training two independent translation model and then combine the two models using a system combination approach. Mei and Kirchhoff (2010) incorporated document-level semantics, such as topic of the discourse, through contextual modelling. Again, all these approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of"
2013.mtsummit-papers.12,N04-1030,0,0.608726,"ms from different SMT approaches (such as hierarchical, phrase based, or synchronous/transduction grammar based) or those applying other techniques for informal data (such as domain adaptation from formal to informal text, or integration of linguistic features) could also benefit from the semantic information incorporated through our approach. 2 Related work Relatively little work has been done toward addressing the problem of biasing the translation decisions of an SMT system to produce adequate translations for informal text that correctly preserve who did what to whom, when, where and why (Pradhan et al., 2004). There has been a recent surge of work aimed at incorporating semantics into the SMT pipeline; however, none attempts to improve translation quality on informal text due to the difficulty of semantic parsing. Below, we describe some of the attempts to (a) improve informal text translation quality using domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semanti"
2013.mtsummit-papers.12,2007.mtsummit-papers.51,0,0.0212218,"stprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and (2) only small volumes of high-quality parallel training data are available (Mei and Kirchhoff, 2010). Rao et al. (2007) and Wang et al. (2010) proposed to remove disfluency in preprocessing stage; Bertoldi et al. (2010) introduced a model to recover the misspelled words before translation; Banerjee et al. (2011) addressed the data sparsity problem by mixing data from comparable domain into the training of both the translation model and the language model whereas He and Deng (2011) proposed to classify the training data into in-domain or out-of-domain for training two independent translation model and then combine the two models using a system combination approach. Mei and Kirchhoff (2010) incorporated document"
2013.mtsummit-papers.12,W11-2112,0,0.232616,"c features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008). Lambert et al. (2006) tuned on QUEEN, a simplified version of ULC, that discards the semantic features of ULC and bases on pure lexical similarity. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to a semantic objective function. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but only performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. In contrast, Lo et al. (2013) show that a MT system tuned against MEANT (Lo et al., 2012) produces more adequate translations in formal news genre as evaluated both quantitatively and qualitatively. Precisely, MEANT is computed as follows: 1. Apply an automatic shallow semantic parser on both the references and MT output. 2. Apply maximum weighted bipartite matching algorithm to align the semantic frames between th"
2013.mtsummit-papers.12,2006.amta-papers.25,0,0.274257,"r out-of-domain for training two independent translation model and then combine the two models using a system combination approach. Mei and Kirchhoff (2010) incorporated document-level semantics, such as topic of the discourse, through contextual modelling. Again, all these approaches are orthogonal to our approach of incorporating semantics into SMT by tuning against a semantic objective function and any of the 94 above models could potentially benefit from tuning with semantic metrics. 2.3 MT evaluation metrics Lo et al. (2013) showed that tuning against BLEU (Papineni et al., 2002) or TER (Snover et al., 2006) does not sufficiently drive SMT into making decisions to produce adequate translations. Other similar n-gram based or edit distance based metrics, such as NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) also suffer from the same problem of failing to adequately reflect translation utility and correctly bias translation model for producing adequate translation. On the other hand, no work has been done towards tuning SMT systems against more linguistically motivated MT evaluation metrics because of expensive run time costs. UL"
2013.mtsummit-papers.12,N09-2004,1,0.841443,"sing domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semantics into various stages of the SMT pipeline, for example, preprocessing the input (Komachi et al., 2006; Wu et al., 2011), training treeto-string MT models (Liu and Gildea, 2010; Aziz et al., 2011), training reordering model (Xiong et al., 2012) and reordering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and (2) only small volumes of high-quality parallel training data are available (Mei and Kirchhoff, 2010). Rao et al. (2007) and Wang et al. (201"
2013.mtsummit-papers.12,I11-1004,0,0.0126835,"none attempts to improve translation quality on informal text due to the difficulty of semantic parsing. Below, we describe some of the attempts to (a) improve informal text translation quality using domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semantics into various stages of the SMT pipeline, for example, preprocessing the input (Komachi et al., 2006; Wu et al., 2011), training treeto-string MT models (Liu and Gildea, 2010; Aziz et al., 2011), training reordering model (Xiong et al., 2012) and reordering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as"
2013.mtsummit-papers.12,P12-1095,0,0.0121004,"ibe some of the attempts to (a) improve informal text translation quality using domain adaptation techniques and (b) incorporate semantic role labeling information into the SMT pipeline and present a brief survey of evaluation metrics that focus on rewarding semantically valid translations. 2.1 Semantics in SMT There is a rising trend of work aimed at incorporating semantics into various stages of the SMT pipeline, for example, preprocessing the input (Komachi et al., 2006; Wu et al., 2011), training treeto-string MT models (Liu and Gildea, 2010; Aziz et al., 2011), training reordering model (Xiong et al., 2012) and reordering the output in the postprocessing stage (Wu and Fung, 2009). All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. In fact, any of the above models could potentially benefit from the proposed approach. 2.2 Adapting SMT for formal genres to informal genres The major challenges for machine translation in the informal genres are (1) the data demonstrates a large variety of grammar issues, such as disfluencies, incomplete sentences and misspellings; and (2) only small volumes of high-quality parallel training data are a"
2014.iwslt-evaluation.4,N04-1030,0,0.357453,"s similar lexical n-grams as the reference translation. Although such metrics tend to enforce fluency, it has been shown that these metrics generally do not emphasize meaning preservation, and thus are weak at enforcing translation adequacy (Callison-Burch et al. [7]; Koehn and Monz [8]). Unlike BLEU, or other n-gram based metrics, the MEANT family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation metrics such as BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14]. 34 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and T"
2014.iwslt-evaluation.4,W05-0909,0,0.2511,"s, the MEANT family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation metrics such as BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14]. 34 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14] do not correctly reflect the meaning similarities of the basic event structure “who did what to whom, when, where and why” of the input sentence. In fact, many studies (Callison-Bursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of w"
2014.iwslt-evaluation.4,W14-4003,1,0.71406,"gments, resulting also in translation performance gains compared to the system tuned against our previous version of MEANT from the IWSLT 2013 evaluation campaign (Lo et al. [4]). This improved variant of MEANT uses f-score to aggregate lexical similarities within role filler phrases instead of linear average. We also introduced several changes to last year’s baseline, including improved Chinese word segmentation, improved Chinese named entity recognition combined with dedicated proper name translation, and number expression handling. We also experimented with tuning against IMEANT (Wu et al. [5]), a new inversion transduction grammar (ITG) version of MEANT, that was shown this year to correlate with human adequacy judgements more closely than MEANT. Despite this fact, we observed that tuning to IMEANT is statistically indistinguishable from tuning to MEANT.In the past few years, MT research has mainly focused on evaluation using fast and cheap ngram based MT evaluation metrics such as BLEU [6] which assume that a good translation is one that has similar lexical n-grams as the reference translation. Although such metrics tend to enforce fluency, it has been shown that these metrics ge"
2014.iwslt-evaluation.4,E06-1031,0,0.383768,"T family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation metrics such as BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14]. 34 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14] do not correctly reflect the meaning similarities of the basic event structure “who did what to whom, when, where and why” of the input sentence. In fact, many studies (Callison-Bursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on deve"
2014.iwslt-evaluation.4,P02-1040,0,0.0900717,"gmentation, improved Chinese named entity recognition combined with dedicated proper name translation, and number expression handling. We also experimented with tuning against IMEANT (Wu et al. [5]), a new inversion transduction grammar (ITG) version of MEANT, that was shown this year to correlate with human adequacy judgements more closely than MEANT. Despite this fact, we observed that tuning to IMEANT is statistically indistinguishable from tuning to MEANT.In the past few years, MT research has mainly focused on evaluation using fast and cheap ngram based MT evaluation metrics such as BLEU [6] which assume that a good translation is one that has similar lexical n-grams as the reference translation. Although such metrics tend to enforce fluency, it has been shown that these metrics generally do not emphasize meaning preservation, and thus are weak at enforcing translation adequacy (Callison-Burch et al. [7]; Koehn and Monz [8]). Unlike BLEU, or other n-gram based metrics, the MEANT family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did"
2014.iwslt-evaluation.4,niessen-etal-2000-evaluation,0,0.401668,"f metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation metrics such as BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14]. 34 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14] do not correctly reflect the meaning similarities of the basic event structure “who did what to whom, when, where and why” of the input sentence. In fact, many studies (Callison-Bursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on developing MT"
2014.iwslt-evaluation.4,E06-1032,0,0.0481031,"judgements more closely than MEANT. Despite this fact, we observed that tuning to IMEANT is statistically indistinguishable from tuning to MEANT.In the past few years, MT research has mainly focused on evaluation using fast and cheap ngram based MT evaluation metrics such as BLEU [6] which assume that a good translation is one that has similar lexical n-grams as the reference translation. Although such metrics tend to enforce fluency, it has been shown that these metrics generally do not emphasize meaning preservation, and thus are weak at enforcing translation adequacy (Callison-Burch et al. [7]; Koehn and Monz [8]). Unlike BLEU, or other n-gram based metrics, the MEANT family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly"
2014.iwslt-evaluation.4,2006.amta-papers.25,0,0.114241,"t the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation metrics such as BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14]. 34 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14] do not correctly reflect the meaning similarities of the basic event structure “who did what to whom, when, where and why” of the input sentence. In fact, many studies (Callison-Bursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on developing MT evaluation met"
2014.iwslt-evaluation.4,W06-3114,0,0.0342993,"ely than MEANT. Despite this fact, we observed that tuning to IMEANT is statistically indistinguishable from tuning to MEANT.In the past few years, MT research has mainly focused on evaluation using fast and cheap ngram based MT evaluation metrics such as BLEU [6] which assume that a good translation is one that has similar lexical n-grams as the reference translation. Although such metrics tend to enforce fluency, it has been shown that these metrics generally do not emphasize meaning preservation, and thus are weak at enforcing translation adequacy (Callison-Burch et al. [7]; Koehn and Monz [8]). Unlike BLEU, or other n-gram based metrics, the MEANT family of metrics adopt the principle that a good translation is one in which humans can successfully understand the central meaning of the input sentence as captured by the basic event structure “who did what to whom, when, where and why” (Pradhan et al. [9]). MEANT measures similarity between an MT output and a reference translation by comparing the similarities between the semantic frame structures of the MT output and reference. We have shown that MEANT correlates better with human adequacy judgments than commonly used MT evaluation"
2014.iwslt-evaluation.4,W12-3104,0,0.0199397,"e Translation Lake Tahoe, December 4th and 5th, 2014 2. Related work Surface-form oriented metrics like BLEU [6], NIST [10], METEOR [11], CDER [12], WER [13], and TER [14] do not correctly reflect the meaning similarities of the basic event structure “who did what to whom, when, where and why” of the input sentence. In fact, many studies (Callison-Bursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on developing MT evaluation metrics that outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and"
2014.iwslt-evaluation.4,W07-0738,0,0.0244469,"ursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on developing MT evaluation metrics that outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive"
2014.iwslt-evaluation.4,W08-0332,0,0.0172516,"ursh et al. [7]; Koehn and Monz [8]) report cases where BLEU strongly disagrees with human adequacy judgment. This has caused a recent surge of work on developing MT evaluation metrics that outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive"
2014.iwslt-evaluation.4,W07-0718,0,0.0182655,"n metrics that outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive linguistic resources, like WordNet and paraphrase tables. These metrics themselves are expensive in training and tuning due to the large number of parameters that need to be estimated"
2014.iwslt-evaluation.4,W08-0309,0,0.0220211,"n metrics that outperforms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive linguistic resources, like WordNet and paraphrase tables. These metrics themselves are expensive in training and tuning due to the large number of parameters that need to be estimated"
2014.iwslt-evaluation.4,W12-3107,0,0.0136485,"ms BLEU in correlation with human judgment. AMBER [15] shows a high correlation with human adequacy judgment (Callison-Burch et al. [16]); however, it is very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive linguistic resources, like WordNet and paraphrase tables. These metrics themselves are expensive in training and tuning due to the large number of parameters that need to be estimated, thus to tu"
2014.iwslt-evaluation.4,W12-3103,0,0.0168022,"s very hard to indicate what errors the MT systems are making. Many automatic metrics that aggregate semantic similarity have been introduced, but no tuning has been done using these metrics, because of their expensive run time. Gimenez and Marquez [17, 18] introduced ULC, an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality [19, 17, 20, 18]. SPEDE [21] is a metric that integrats probabilistic FSM and PDA models that predicts the edit sequence needed for the MT output to match the reference. SAGAN [22] is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; furthermore, they typically rely on several dozens of parameters to tune and use expensive linguistic resources, like WordNet and paraphrase tables. These metrics themselves are expensive in training and tuning due to the large number of parameters that need to be estimated, thus to tune against these metrics can be extremely expensive. the semantic role fillers of the reference and translation. More precisely, MEANT is computed as fo"
2014.iwslt-evaluation.4,2013.mtsummit-papers.12,1,0.792327,"arameters that need to be estimated, thus to tune against these metrics can be extremely expensive. the semantic role fillers of the reference and translation. More precisely, MEANT is computed as follows: 1. Apply an automatic shallow semantic parser to both the reference and machine translations. (Figure 1 shows examples of automatic shallow semantic parses on both reference and machine translations.) 2. Apply the maximum weighted bipartite matching algorithm to align the semantic frames between the reference and machine translations according to the lexical similarities of the predicates. ([23] proposed a backoff algorithm that evaluates the entire sentence of the MT output using the lexical similarity based on the context vector model, if the automatic shallow semantic parser fails to parse the reference or machine translations.) 3. For each pair of the aligned frames, apply the maximum weighted bipartite matching algorithm to align the arguments between the reference and machine translations according to the lexical similarity of role fillers. 4. Compute the weighted f-score over the matching role labels of these aligned predicates and role fillers as follow : 0 qi,j 1 qi,j ≡ ≡ wi"
2014.iwslt-evaluation.4,J97-3002,1,0.587069,"illers of the arguments of type j of all frame between the reference translations and the machine translations. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu [24]. For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu [1]). For UMEANT (Lo and Wu [2]), they are estimated in an unsupervised manner using relative freIMEANT (Wu et al. [5]) is an inversion transduction grammar based variant of MEANT. IMEANT uses a a length-normalized weighted BITG [25, 26, 27, 28] to constrain permissible token alignment patterns between aligned role filler phrases. More precisely, IMEANT differs from MEANT in the definition of si,pred and si,j , as follows: G R ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } ≡ p ([AA] |A) p (e/f |A) = = si,pred = si,j = p (⟨AA⟩|A) = 1 s(e, f ) ))   ( ( ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |) 36 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 where G is a brac"
2014.iwslt-evaluation.4,P03-1019,0,0.127937,"illers of the arguments of type j of all frame between the reference translations and the machine translations. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu [24]. For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu [1]). For UMEANT (Lo and Wu [2]), they are estimated in an unsupervised manner using relative freIMEANT (Wu et al. [5]) is an inversion transduction grammar based variant of MEANT. IMEANT uses a a length-normalized weighted BITG [25, 26, 27, 28] to constrain permissible token alignment patterns between aligned role filler phrases. More precisely, IMEANT differs from MEANT in the definition of si,pred and si,j , as follows: G R ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } ≡ p ([AA] |A) p (e/f |A) = = si,pred = si,j = p (⟨AA⟩|A) = 1 s(e, f ) ))   ( ( ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |) 36 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 where G is a brac"
2014.iwslt-evaluation.4,W09-2304,1,0.82476,"illers of the arguments of type j of all frame between the reference translations and the machine translations. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu [24]. For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu [1]). For UMEANT (Lo and Wu [2]), they are estimated in an unsupervised manner using relative freIMEANT (Wu et al. [5]) is an inversion transduction grammar based variant of MEANT. IMEANT uses a a length-normalized weighted BITG [25, 26, 27, 28] to constrain permissible token alignment patterns between aligned role filler phrases. More precisely, IMEANT differs from MEANT in the definition of si,pred and si,j , as follows: G R ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } ≡ p ([AA] |A) p (e/f |A) = = si,pred = si,j = p (⟨AA⟩|A) = 1 s(e, f ) ))   ( ( ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |) 36 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 where G is a brac"
2014.iwslt-evaluation.4,2012.eamt-1.64,1,0.776757,"illers of the arguments of type j of all frame between the reference translations and the machine translations. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu [24]. For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu [1]). For UMEANT (Lo and Wu [2]), they are estimated in an unsupervised manner using relative freIMEANT (Wu et al. [5]) is an inversion transduction grammar based variant of MEANT. IMEANT uses a a length-normalized weighted BITG [25, 26, 27, 28] to constrain permissible token alignment patterns between aligned role filler phrases. More precisely, IMEANT differs from MEANT in the definition of si,pred and si,j , as follows: G R ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } ≡ p ([AA] |A) p (e/f |A) = = si,pred = si,j = p (⟨AA⟩|A) = 1 s(e, f ) ))   ( ( ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |) 36 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 where G is a brac"
2014.iwslt-evaluation.4,W09-3804,1,0.906381,",j |, |fi,j |) 36 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 where G is a bracketing ITG whose only non terminal is A, and R is a set of transduction rules with e ∈ W 0 ∪ {ϵ} denoting a token in the MT output (or the null token) and f ∈ W 1 ∪ {ϵ} denoting a token in the reference translation (or the null token). The rule weight function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is defined using MEANT’s context vector model based lexical similarity measure. The Saers et al. [29] algorithm is used to (compute the ) inside probability of a pair ∗ of segments, P A ⇒ e/f|G . Given this, si,pred and si,j now represent the length normalized BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and machine translations. 4. Baseline In this section, we describe in detail our systems for the Chinese-English and English-Chinese TED talk MT tasks in terms of data, preprocessing, SMT pipeline and MEANT settings. 4.1. Data Our main goal for 2014 was to improve our MEANT tuned system and compare the results to our 2013 system. For th"
2014.iwslt-evaluation.4,P03-1021,0,0.0983889,"focus was to test our performance in comparison to 2013, we purposely targeted the IWSLT 2013 set more than the IWSLT 2014 set. However, we do present IWSLT 2014 results for our BLEU tuned system for both EnglishChinese and Chinese-English. The English sentences were normalized for punctuation, tokenization, and truecasing. Obviously, higher scores could have been obtained by training on the IWSLT 2014 data set instead of 2013. 4.2. SMT pipeline With the goal of improving MT utility by using our new improved version of MEANT as an objective function to drive minimum error rate training (MERT) [30] of state-of-the-art MT systems, we set up our baseline using the translation toolkit Moses [31]. In our experiments, we are using the flat phrase-based MT. The language models are trained using the SRI language model toolkit [32]. For both translation tasks, we used a 6gram language model. We use ZMERT [33] to tune the baseline since it is a reliable implementation of MERT and is fully configurable and extensible allowing us to easily incorporate our new evaluation metrics. 5. Experiments 5.1. MEANT improvements This year’s system incorporated new improvements to the MEANT metric, consisting"
2014.iwslt-evaluation.4,W13-2254,1,0.832613,"ince it is a reliable implementation of MERT and is fully configurable and extensible allowing us to easily incorporate our new evaluation metrics. 5. Experiments 5.1. MEANT improvements This year’s system incorporated new improvements to the MEANT metric, consisting of using f-score in order to aggregate lexical similarities within semantic role filler phrases instead of Mihalcea’s [34] method used in our last year system. We also tried to extend the window-size from 3 to 5 for the context vector model trained on the word segmented monolingual English gigaword corpus. Since UMEANT (Lo and Wu [35]) has been shown to be more stable when evaluating translations across different language pairs (Machacek and Bojar [36]), we use UMEANT for evaluating our output. 5.2. Tuning to IMEANT In this paper, we also ran preliminary experiments on tuning to IMEANT [5], the new inversion transduction grammar based variant of MEANT, that achieves higher correlation with human adequacy judgments of MT output quality than MEANT and its variants. Addanki et al. [28] showed empirically that the semantic role reordering that MEANT uses is covered by ITG constraints. 5.3. Word segmentation improvements For Ch"
2014.iwslt-evaluation.4,W13-2202,0,0.0679049,"our new evaluation metrics. 5. Experiments 5.1. MEANT improvements This year’s system incorporated new improvements to the MEANT metric, consisting of using f-score in order to aggregate lexical similarities within semantic role filler phrases instead of Mihalcea’s [34] method used in our last year system. We also tried to extend the window-size from 3 to 5 for the context vector model trained on the word segmented monolingual English gigaword corpus. Since UMEANT (Lo and Wu [35]) has been shown to be more stable when evaluating translations across different language pairs (Machacek and Bojar [36]), we use UMEANT for evaluating our output. 5.2. Tuning to IMEANT In this paper, we also ran preliminary experiments on tuning to IMEANT [5], the new inversion transduction grammar based variant of MEANT, that achieves higher correlation with human adequacy judgments of MT output quality than MEANT and its variants. Addanki et al. [28] showed empirically that the semantic role reordering that MEANT uses is covered by ITG constraints. 5.3. Word segmentation improvements For Chinese sentences, we improved the segmentation of Chinese words. We performed extensive comparisons between four word seg"
2014.iwslt-evaluation.4,W03-1730,0,0.0159284,"nary experiments on tuning to IMEANT [5], the new inversion transduction grammar based variant of MEANT, that achieves higher correlation with human adequacy judgments of MT output quality than MEANT and its variants. Addanki et al. [28] showed empirically that the semantic role reordering that MEANT uses is covered by ITG constraints. 5.3. Word segmentation improvements For Chinese sentences, we improved the segmentation of Chinese words. We performed extensive comparisons between four word segmentation approaches. The results reported this year were obtained using the ICTCLAS word segmenter [37]. 5.4. Named entity translation improvements We also used our own new implementation of Chinese named entity recognition and a dedicated proper name translation, where we use our own library translator based on Wikipedia data. We implemented an adequate library generator for our new named entity recognizer. 37 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 Table 1: Translation quality of the participated Chinese-English MT systems on the IWSLT 2013 test set: (a) 2013 MEANT-tuned system, (b) 2014 improved MEANT-tuned system."
2020.lrec-1.312,W18-1810,0,0.0358313,"Missing"
2020.lrec-1.312,C10-2010,0,0.0386339,"gorithm. An improved release, NH 1.1, was one of three corpora used as experimental data for a 2005 shared task on word alignment for languages with scarce resources (Martin et al., 2005). Version 2.0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through November 8, 2007 (excluding 2003). NH 2.0 contained 5,589,323 English words and and 2,651,414 Inuktitut words. Sentence alignment was carried out with Moore’s aligner (Moore, 2002).4 For this work, we use our in-house sentence aligner, based on Moore (2002) with some enhancements. Like Braune and Fraser (2010) and most sentence aligners, we allow 1– many alignments, and like Yu et al. (2012) we use a multipass approach to refine alignments. A recent approach to sentence alignment (Thompson and Koehn, 2019) is reported to significantly improve alignment quality by incorporating bilingual word embeddings trained on large pre-existing parallel corpora. We experimented with this approach as well (see §3.3.). 3. Alignment of the Nunavut Hansard Inuktitut–English Parallel Corpus 3.0 The Nunavut Hansard Inuktitut–English Parallel Corpus 3.0 (NH 3.0) consists of 17,330,271 English words in 1,452,347 senten"
2020.lrec-1.312,J93-2003,0,0.18769,"nd chrF on dev and had the best BLEU and second-best chrF on devtest, with very close performance to systems ranging from 500 to 10,000 merges. These results by BPE merges are in line with the low-resource Transformer results described by Ding et al. (2019), though NH 3.0 falls between their low-resource and high-resource settings. 18 github.com/rsennrich/subword-nmt 4.3.2. Statistical Machine Translation We trained our statistical machine translation system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 word"
2020.lrec-1.312,N12-1047,0,0.0144963,"hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tuning runs with different random seeds. We ran SMT experiments with the same BPE vocabularies as used in NMT. On the basis of BLEU and chrF results on dev and devtest for NH 3.0, we selected 30,000 merges for all reported English–Inuktitut SMT experiments, and 5,000 merges for Inuktitut–English SMT experiments. 4.4. Results and Evaluation We experiment with variations on the corpus to understand the effects of data size, recency, and alignment quality o"
2020.lrec-1.312,N13-1003,0,0.0124969,"near phrase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tuning runs with different random seeds. We ran SMT experiments with the same BPE"
2020.lrec-1.312,P14-1129,0,0.0349202,"gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tuning runs with different random seeds. We ran SMT experiments with the same BPE vocabularies as used in NMT. On the basis of BLEU and chrF results on dev and devtest for NH 3.0, we selected 30,000 merges for all reported English–Inuktitut SMT experiments, and 5,00"
2020.lrec-1.312,W19-6620,0,0.0346965,"U and chrF (Popović, 2015) results on dev and devtest, we selected 2,000 merges for all reported English–Inuktitut NMT experiments, and 5,000 merges for Inuktitut–English NMT experiments. Translating into English, 5,000 merges produced the best results (or tied for best) on both metrics. Into Inuktitut, 2,000 merges tied for the best result in terms of BLEU and chrF on dev and had the best BLEU and second-best chrF on devtest, with very close performance to systems ranging from 500 to 10,000 merges. These results by BPE merges are in line with the low-resource Transformer results described by Ding et al. (2019), though NH 3.0 falls between their low-resource and high-resource settings. 18 github.com/rsennrich/subword-nmt 4.3.2. Statistical Machine Translation We trained our statistical machine translation system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. T"
2020.lrec-1.312,J93-1004,0,0.919827,"nal sentence into the target language. 2 assembly.nu.ca/hansard We are unable to confirm the exact date at which this change in production occurred, or whether it happened gradually. 3 2.3. Prior Releases and Work on Alignment The first Nunavut Hansard parallel corpus was released to the research community in 2003 (Martin et al., 2003). Nunavut Hansard 1.0 (NH 1.0), covered 155 days of proceedings of the Nunavut Assembly, from April 1, 1999 to November 1, 2002; it comprised 3,432,212 English tokens and 1,586,423 Inuktitut tokens. Sentence alignment was carried out by a modified version of the Gale and Church (1993) algorithm. An improved release, NH 1.1, was one of three corpora used as experimental data for a 2005 shared task on word alignment for languages with scarce resources (Martin et al., 2005). Version 2.0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through November 8, 2007 (excluding 2003). NH 2.0 contained 5,589,323 English words and and 2,651,414 Inuktitut words. Sentence alignment was carried out with Moore’s aligner (Moore, 2002).4 For this work, we use our in-house sentence aligner, based on Moore (2002) with some enhancements. Li"
2020.lrec-1.312,E17-3017,0,0.0318897,"e translation using this corpus. BLEU scores (Papineni et al., 2002) are computed with lowercase, v13a tokenization, using sacrebleu (Post, 2018). It should be emphasized that these scores are as high as they are in large part due to the formulaic and self-similar nature of the parliamentary genre, and should not be taken as representative of general-domain MT performance. 4.3.1. Neural Machine Translation Our baseline neural machine translation system uses the Transformer architecture (Vaswani et al., 2017) implemented in Sockeye and following similar parameter settings to those described in Hieber et al. (2017): a 6-layer encoder, 6-layer decoder, a model dimension of 512 and 2048 hidden units in the feed-forward networks. As our byte-pair vocabularies were disjoint, we did not use weight-tying. The network was optimized using Adam (Kingma and Ba, 2015), with an initial learning rate of 10−4 , decreasing by a factor of 0.7 each time the development set BLEU did not improve for 8,000 updates, and stopping early when BLEU did not improve for 32,000 updates. We experimented with BPE vocabularies with 0.5, 1, 2, 5, 10, 15, 20, 25, 30, and 60 thousand merges. The maximum sentence length was set to 200, a"
2020.lrec-1.312,D11-1125,0,0.00975563,"machine translation system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tunin"
2020.lrec-1.312,P07-1019,0,0.0204863,"a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tuning runs with different random seeds. We ran SMT experiments with the same BPE vocabularies as used in NMT. On the basis of BLEU and chrF results on dev and devtest for NH 3.0, we selected 30,000 merges for all reported English–Inuktitut SMT experiments, and 5,000 merges for Inuktitut–English SMT experiments. 4.4. Results and Evaluation We experiment with variations on the corpus to understand the effects of data size, recency, and alignment quality on machine translation output. In keeping with prior work, most of our"
2020.lrec-1.312,W18-2709,0,0.0526136,"Missing"
2020.lrec-1.312,W06-3114,0,0.103089,"he NMT systems outperform SMT on the NH 2.0 size (approximately 6.7 million English tokens).19 The size comparison is not entirely fair on its own: it conflates data size and recency. To examine recency effects, we also selected subsets from the end of the NH 3.0 training corpus that matched the corresponding “(older)” subsets in size; we denote these “(recent)” in the table. For both NMT and SMT, we observe large (often 10+ BLEU) recency effects, with more recent data performing better. BLEU has long been criticized for not correlating well enough with human judgments on translation quality (Koehn and Monz, 2006; Ma et al., 2019). This problem is more apparent when evaluating translation into morphologically complex languages because BLEU does not account for morphological and lexical variation between the reference and the MT output. Minor differences in morpheme choice are scored as badly as mistranslating a whole word. Thus, in addition to BLEU, we have evaluated the MT systems into Inuktitut using YiSi-0 (Lo, 2019), a word-level metric that incorporates character-level information, which has been shown to correlate better with human judgment than BLEU on translation quality into agglutinative lan"
2020.lrec-1.312,N03-1017,0,0.0335596,"very close performance to systems ranging from 500 to 10,000 merges. These results by BPE merges are in line with the low-resource Transformer results described by Ding et al. (2019), though NH 3.0 falls between their low-resource and high-resource settings. 18 github.com/rsennrich/subword-nmt 4.3.2. Statistical Machine Translation We trained our statistical machine translation system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al.,"
2020.lrec-1.312,P07-2045,0,0.0139792,"nonbreaking characters on the Inuktitut side of the data (and thus not tokenized). 16 uniconv is distributed with Yudit: www.yudit.org Note that this romanization pipeline does not fully conform to all spelling and romanization conventions described in the Nunavut Utilities plugins for Microsoft Word (www.gov.nu.ca/ culture-and-heritage/information/computer-tools); we use the pipeline described here solely for MT experiments. 17 2567 Following conversion to romanized script, we ran identical preprocessing with English defaults on both the Inuktitut and English sides of the corpus using Moses (Koehn et al., 2007) scripts: punctuation normalization, tokenization (with aggressive hyphen splitting), cleaned the training corpus (sentence length ratio 15, minimum sentence length 1, maximum 200), trained a truecaser on the training data and then applied it to all data. We trained byte-pair encoding models on the training data using subword-nmt,18 with disjoint vocabularies ranging from 500 to 60,000 operations, which were then applied to all data. Future work may wish to consider promising morphologically informed approaches to segmentation for machine translation or alignment (Ataman et al., 2017; Ataman a"
2020.lrec-1.312,D18-2012,0,0.0306536,"e underlying form of many morphemes (Micher, 2017). Here is a typical example sentence from this corpus: ᐅᖃᖅᑏ, ᒐᕙᒪᒃᑯᑦ ᐊᐅᓚᔾᔭᐃᒋᐊᓪᓚᑦᑖᕋᓱᒻᒪᑕ ᐅᖃᐅᓯᖅ ᐆᒻᒪᖅᑎᑕᐅᒃᑲᓐᓂᕋᓱᓪᓗᓂ ᐊᒻᒪᓗ ᐃᓕᓐᓂᐊᖅᑏᑦ ᐃᓕᓐᓂᐊᖅᑎᑕᐅᓗᑎᒃ. uqaqtii, gavamakkut aulajjaigiallattaarasummata uqausiq uummaqtitaukkannirasulluni ammalu ilinniaqtiit ilinniaqtitaulutik. Mr. Speaker, this government is taking concrete action on language revitalization and student outcomes. This complexity contributes to the scientific interest of this corpus, raising questions like whether contemporary subword methods in natural language processing (Sennrich et al., 2015; Kudo and Richardson, 2018, among others) are sufficient to process one of the world’s most morphologically complex languages, and if not, how to integrate prior knowledge of Inuktitut morphology in a robust way. The work described in this paper can also be viewed in the context of a wide range of language technologies currently being developed for Indigenous languages spoken in Canada, as surveyed by Littell et al. (2018). 2.2. Legislative Assembly of Nunavut and Hansard The Legislative Assembly of Nunavut conducts business in both Inuktut and English, with the choice of language influenced by both Member preference a"
2020.lrec-1.312,W10-1717,1,0.866261,"es,6 with some heuristics to correct errors.7 The output is plain text in UTF-8 with one paragraph per line. For the other sessions and for all English documents, we used AlignFactory8 (AF) to extract the collection of paragraphs out of the Word documents.9 There are many opensource tools that can do the same task with similar results, but we find this commercial tool reliable and easy to use. AF creates a TMX file from which we extract the text to plain text files in UTF-8 with one paragraph per line. Segmentation of the paragraphs in both languages into sentences was done using the Portage (Larkin et al., 2010) sentence splitter, which is part of the Portage tokenizer. To remove parts of the corpus that were clearly not parallel, we searched for outliers using various heuristics: documents with unusually large or small paragraph or text length ratios between the languages; with similar imbalances in the appendices; or where a first pass of the baseline alignment methodology (see §3.3.) had unusually low alignment probability. These outliers were visually inspected and nonparallel text was manually removed. Examples of problems found include: an appendix occurring only in one language; an appendix oc"
2020.lrec-1.312,li-etal-2010-enriching,0,0.0497906,"Missing"
2020.lrec-1.312,C18-1222,1,0.771688,"outcomes. This complexity contributes to the scientific interest of this corpus, raising questions like whether contemporary subword methods in natural language processing (Sennrich et al., 2015; Kudo and Richardson, 2018, among others) are sufficient to process one of the world’s most morphologically complex languages, and if not, how to integrate prior knowledge of Inuktitut morphology in a robust way. The work described in this paper can also be viewed in the context of a wide range of language technologies currently being developed for Indigenous languages spoken in Canada, as surveyed by Littell et al. (2018). 2.2. Legislative Assembly of Nunavut and Hansard The Legislative Assembly of Nunavut conducts business in both Inuktut and English, with the choice of language influenced by both Member preference and the topic at hand (Okalik, 2011), and its proceedings, the Nunavut Hansard, are published in both Inuktitut and English. “The policy of Nunavut’s Hansard editors is to provide a verbatim transcript with a minimum of editing and without any alteration of the meaning of a Member’s speech.”2 Prior to approximately 2005–2006,3 the English version of the Hansard was a transcription of the English sp"
2020.lrec-1.312,W19-5358,1,0.847114,"0+ BLEU) recency effects, with more recent data performing better. BLEU has long been criticized for not correlating well enough with human judgments on translation quality (Koehn and Monz, 2006; Ma et al., 2019). This problem is more apparent when evaluating translation into morphologically complex languages because BLEU does not account for morphological and lexical variation between the reference and the MT output. Minor differences in morpheme choice are scored as badly as mistranslating a whole word. Thus, in addition to BLEU, we have evaluated the MT systems into Inuktitut using YiSi-0 (Lo, 2019), a word-level metric that incorporates character-level information, which has been shown to correlate better with human judgment than BLEU on translation quality into agglutinative languages, such as Finnish and Turkish, at both sentence level and document level (Ma et al., 2018; Ma et al., 2019). In general, the YiSi-0 results follow the same trends as BLEU: (1) better aligned training data improves translation quality; (2) larger training data improves translation quality; and (3) more recent training data improves translation quality. 19 For NMT, we use the settings found to be optimal for"
2020.lrec-1.312,W15-1521,0,0.0139794,"t/llab/www/ 13 In an ideal world, we would have wanted a dev set to tune the BPE meta-parameters, but the desired output of this work is the best corpus possible and we only have a small amount of manually annotated data, so we simply retain the BPE settings that yield the best results over our gold standard. Shortly before submission, Vecalign (Thompson and Koehn, 2019) reported a new state of the art in alignment, using on high-quality pre-trained bilingual word vectors (“bivectors”). Since pretrained bivectors for Inuktitut– English do not exist, to the best of our knowledge, we use bivec (Luong et al., 2015) to train bivectors on our best system output (BPE 10k joint), and then use the results to realign the whole corpus with vecalign.14 The embeddings, of dimension 1024, are trained for 5 iterations using skip-gram, a maximum skip length between words of 11, 10 negative samples, a hierarchical softmax, and without discarding any words based on frequency counts. We create Vecalign overlaps using overlaps.py and an overlap window of 15 for the source and the target separately. Using bivec’s word embeddings, we average the word embeddings for each sentence in the overlap sets to produce their sente"
2020.lrec-1.312,W18-6450,0,0.029608,"to morphologically complex languages because BLEU does not account for morphological and lexical variation between the reference and the MT output. Minor differences in morpheme choice are scored as badly as mistranslating a whole word. Thus, in addition to BLEU, we have evaluated the MT systems into Inuktitut using YiSi-0 (Lo, 2019), a word-level metric that incorporates character-level information, which has been shown to correlate better with human judgment than BLEU on translation quality into agglutinative languages, such as Finnish and Turkish, at both sentence level and document level (Ma et al., 2018; Ma et al., 2019). In general, the YiSi-0 results follow the same trends as BLEU: (1) better aligned training data improves translation quality; (2) larger training data improves translation quality; and (3) more recent training data improves translation quality. 19 For NMT, we use the settings found to be optimal for NH 3.0 throughout our remaining experiments; this may be suboptimal, as noted in Sennrich and Zhang (2019), but we still would not expect the smaller corpora to outperform the larger in this scenario. However, as we do not yet have human judgments on MT into Inuktitut, we urge c"
2020.lrec-1.312,W19-5302,0,0.0363237,"orm SMT on the NH 2.0 size (approximately 6.7 million English tokens).19 The size comparison is not entirely fair on its own: it conflates data size and recency. To examine recency effects, we also selected subsets from the end of the NH 3.0 training corpus that matched the corresponding “(older)” subsets in size; we denote these “(recent)” in the table. For both NMT and SMT, we observe large (often 10+ BLEU) recency effects, with more recent data performing better. BLEU has long been criticized for not correlating well enough with human judgments on translation quality (Koehn and Monz, 2006; Ma et al., 2019). This problem is more apparent when evaluating translation into morphologically complex languages because BLEU does not account for morphological and lexical variation between the reference and the MT output. Minor differences in morpheme choice are scored as badly as mistranslating a whole word. Thus, in addition to BLEU, we have evaluated the MT systems into Inuktitut using YiSi-0 (Lo, 2019), a word-level metric that incorporates character-level information, which has been shown to correlate better with human judgment than BLEU on translation quality into agglutinative languages, such as Fi"
2020.lrec-1.312,W03-0320,0,0.447846,"the Hansard. The Hansard contains parenthetical annotations of when interpretation occurs (e.g., “interpretation begins”, “interpretation ends”). When the transcriber considers the interpretation to contain an error (e.g., missing information), they will instead translate the original sentence into the target language. 2 assembly.nu.ca/hansard We are unable to confirm the exact date at which this change in production occurred, or whether it happened gradually. 3 2.3. Prior Releases and Work on Alignment The first Nunavut Hansard parallel corpus was released to the research community in 2003 (Martin et al., 2003). Nunavut Hansard 1.0 (NH 1.0), covered 155 days of proceedings of the Nunavut Assembly, from April 1, 1999 to November 1, 2002; it comprised 3,432,212 English tokens and 1,586,423 Inuktitut tokens. Sentence alignment was carried out by a modified version of the Gale and Church (1993) algorithm. An improved release, NH 1.1, was one of three corpora used as experimental data for a 2005 shared task on word alignment for languages with scarce resources (Martin et al., 2005). Version 2.0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through"
2020.lrec-1.312,W05-0809,0,0.105721,"ior Releases and Work on Alignment The first Nunavut Hansard parallel corpus was released to the research community in 2003 (Martin et al., 2003). Nunavut Hansard 1.0 (NH 1.0), covered 155 days of proceedings of the Nunavut Assembly, from April 1, 1999 to November 1, 2002; it comprised 3,432,212 English tokens and 1,586,423 Inuktitut tokens. Sentence alignment was carried out by a modified version of the Gale and Church (1993) algorithm. An improved release, NH 1.1, was one of three corpora used as experimental data for a 2005 shared task on word alignment for languages with scarce resources (Martin et al., 2005). Version 2.0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through November 8, 2007 (excluding 2003). NH 2.0 contained 5,589,323 English words and and 2,651,414 Inuktitut words. Sentence alignment was carried out with Moore’s aligner (Moore, 2002).4 For this work, we use our in-house sentence aligner, based on Moore (2002) with some enhancements. Like Braune and Fraser (2010) and most sentence aligners, we allow 1– many alignments, and like Yu et al. (2012) we use a multipass approach to refine alignments. A recent approach to sentence"
2020.lrec-1.312,W17-0114,1,0.9037,"sembly (as well as their interpreters and transcribers) have diverse linguistic backgrounds, users of this corpus should not necessarily take any given word or sentence to be representative of any particular variety of Inuktitut. Inuktut is known for having considerable morphological complexity. Words are typically multi-morphemic, with roots followed by multiple suffixes/enclitics from an inventory of hundreds of possible elements. Moreover, these el2562 ements undergo complex (albeit regular) morphophonemic changes in contact with each other, obscuring the underlying form of many morphemes (Micher, 2017). Here is a typical example sentence from this corpus: ᐅᖃᖅᑏ, ᒐᕙᒪᒃᑯᑦ ᐊᐅᓚᔾᔭᐃᒋᐊᓪᓚᑦᑖᕋᓱᒻᒪᑕ ᐅᖃᐅᓯᖅ ᐆᒻᒪᖅᑎᑕᐅᒃᑲᓐᓂᕋᓱᓪᓗᓂ ᐊᒻᒪᓗ ᐃᓕᓐᓂᐊᖅᑏᑦ ᐃᓕᓐᓂᐊᖅᑎᑕᐅᓗᑎᒃ. uqaqtii, gavamakkut aulajjaigiallattaarasummata uqausiq uummaqtitaukkannirasulluni ammalu ilinniaqtiit ilinniaqtitaulutik. Mr. Speaker, this government is taking concrete action on language revitalization and student outcomes. This complexity contributes to the scientific interest of this corpus, raising questions like whether contemporary subword methods in natural language processing (Sennrich et al., 2015; Kudo and Richardson, 2018, among others) are suffi"
2020.lrec-1.312,W18-4807,1,0.88702,"oming data sparsity for alignment, we perform experiments on subword and morphological segmentations of the text. The morphological experiments (Morpho surface and Morpho deep) use morphological segmentation, replacing Inuktitut words with either sequences of surface forms (simple morphological word segmentation) or sequences of deep forms (segmentation with substitution). The morphological analysis was done on romanized words using Uqailaut (Farley, 2009). Words that had no analysis from the Uqailaut analyzer were subsequently processed with the neural analyzer described by Micher (2017) and Micher (2018). While the Uqailaut analyzer produced multiple analyses per word, only the first analysis was used in these experiments. Furthermore, as the neural analyzer only produces a 1-best 2565 analysis for each word, each word in the full corpus has a single, 1-best analysis.12 We follow the same 4-pass alignment methodology as in the baseline, but train and apply the IBM-HMM model on sequences of surface or deep forms instead of words. The morphological segmentation experiments rely on the existence of a morphological analyzer, a tool that is not available for all languages. To demonstrate that simp"
2020.lrec-1.312,moore-2002-fast,0,0.364503,"d 1,586,423 Inuktitut tokens. Sentence alignment was carried out by a modified version of the Gale and Church (1993) algorithm. An improved release, NH 1.1, was one of three corpora used as experimental data for a 2005 shared task on word alignment for languages with scarce resources (Martin et al., 2005). Version 2.0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through November 8, 2007 (excluding 2003). NH 2.0 contained 5,589,323 English words and and 2,651,414 Inuktitut words. Sentence alignment was carried out with Moore’s aligner (Moore, 2002).4 For this work, we use our in-house sentence aligner, based on Moore (2002) with some enhancements. Like Braune and Fraser (2010) and most sentence aligners, we allow 1– many alignments, and like Yu et al. (2012) we use a multipass approach to refine alignments. A recent approach to sentence alignment (Thompson and Koehn, 2019) is reported to significantly improve alignment quality by incorporating bilingual word embeddings trained on large pre-existing parallel corpora. We experimented with this approach as well (see §3.3.). 3. Alignment of the Nunavut Hansard Inuktitut–English Parallel Cor"
2020.lrec-1.312,N12-1040,0,0.227577,"n the full corpus has a single, 1-best analysis.12 We follow the same 4-pass alignment methodology as in the baseline, but train and apply the IBM-HMM model on sequences of surface or deep forms instead of words. The morphological segmentation experiments rely on the existence of a morphological analyzer, a tool that is not available for all languages. To demonstrate that simple heuristics can also perform well in the absence of morphological analysis tools, we performed experiments using the Prefix of each word as a stand-in for more complicated lemmatization (Simard et al., 1992; Och, 2005; Nicholson et al., 2012). As a polysynthetic language, Inuktitut has many very-low frequency words, and even English words have some morphology which a stemmer could normalize. Since syllabic characters represent whole syllables, we chose a prefix of three characters to approximate the stem of an Inuktitut word and we use a five letter prefix for English words. We follow the same alignment methodology as in the baseline, but train and apply the IBM-HMM model on sequences of prefixes instead of words. Byte-pair encoding (BPE) is another approach to word segmentation that does not rely on existing language-specific too"
2020.lrec-1.312,J03-1002,0,0.0685157,"Missing"
2020.lrec-1.312,P02-1040,0,0.109109,"te-pair encoding models on the training data using subword-nmt,18 with disjoint vocabularies ranging from 500 to 60,000 operations, which were then applied to all data. Future work may wish to consider promising morphologically informed approaches to segmentation for machine translation or alignment (Ataman et al., 2017; Ataman and Federico, 2018; Micher, 2018; Toral et al., 2019). 4.3. Baseline Experiments We present baseline NMT and SMT results, over several versions of the data, in the hopes of encouraging future work on English–Inuktitut machine translation using this corpus. BLEU scores (Papineni et al., 2002) are computed with lowercase, v13a tokenization, using sacrebleu (Post, 2018). It should be emphasized that these scores are as high as they are in large part due to the formulaic and self-similar nature of the parliamentary genre, and should not be taken as representative of general-domain MT performance. 4.3.1. Neural Machine Translation Our baseline neural machine translation system uses the Transformer architecture (Vaswani et al., 2017) implemented in Sockeye and following similar parameter settings to those described in Hieber et al. (2017): a 6-layer encoder, 6-layer decoder, a model di"
2020.lrec-1.312,W15-3049,0,0.0303091,"id not use weight-tying. The network was optimized using Adam (Kingma and Ba, 2015), with an initial learning rate of 10−4 , decreasing by a factor of 0.7 each time the development set BLEU did not improve for 8,000 updates, and stopping early when BLEU did not improve for 32,000 updates. We experimented with BPE vocabularies with 0.5, 1, 2, 5, 10, 15, 20, 25, 30, and 60 thousand merges. The maximum sentence length was set to 200, allowing us to compare the vocabularies without filtering out large numbers of training lines for the smaller vocabulary experiments. On the basis of BLEU and chrF (Popović, 2015) results on dev and devtest, we selected 2,000 merges for all reported English–Inuktitut NMT experiments, and 5,000 merges for Inuktitut–English NMT experiments. Translating into English, 5,000 merges produced the best results (or tied for best) on both metrics. Into Inuktitut, 2,000 merges tied for the best result in terms of BLEU and chrF on dev and had the best BLEU and second-best chrF on devtest, with very close performance to systems ranging from 500 to 10,000 merges. These results by BPE merges are in line with the low-resource Transformer results described by Ding et al. (2019), though"
2020.lrec-1.312,W18-6319,0,0.016305,"ries ranging from 500 to 60,000 operations, which were then applied to all data. Future work may wish to consider promising morphologically informed approaches to segmentation for machine translation or alignment (Ataman et al., 2017; Ataman and Federico, 2018; Micher, 2018; Toral et al., 2019). 4.3. Baseline Experiments We present baseline NMT and SMT results, over several versions of the data, in the hopes of encouraging future work on English–Inuktitut machine translation using this corpus. BLEU scores (Papineni et al., 2002) are computed with lowercase, v13a tokenization, using sacrebleu (Post, 2018). It should be emphasized that these scores are as high as they are in large part due to the formulaic and self-similar nature of the parliamentary genre, and should not be taken as representative of general-domain MT performance. 4.3.1. Neural Machine Translation Our baseline neural machine translation system uses the Transformer architecture (Vaswani et al., 2017) implemented in Sockeye and following similar parameter settings to those described in Hieber et al. (2017): a 6-layer encoder, 6-layer decoder, a model dimension of 512 and 2048 hidden units in the feed-forward networks. As our byt"
2020.lrec-1.312,P19-1021,0,0.0161047,"n shown to correlate better with human judgment than BLEU on translation quality into agglutinative languages, such as Finnish and Turkish, at both sentence level and document level (Ma et al., 2018; Ma et al., 2019). In general, the YiSi-0 results follow the same trends as BLEU: (1) better aligned training data improves translation quality; (2) larger training data improves translation quality; and (3) more recent training data improves translation quality. 19 For NMT, we use the settings found to be optimal for NH 3.0 throughout our remaining experiments; this may be suboptimal, as noted in Sennrich and Zhang (2019), but we still would not expect the smaller corpora to outperform the larger in this scenario. However, as we do not yet have human judgments on MT into Inuktitut, we urge caution in the interpretation of any automatic metric. 5. Conclusion The main contribution of the work described in this paper is the release of a corpus of approximately 1.3 million aligned Inuktitut–English sentence pairs drawn from the proceedings of the Legislative Assembly of Nunavut. Care was taken to ensure that the sentence alignment was as accurate as possible: the performance of different sentence alignment algorit"
2020.lrec-1.312,1992.tmi-1.7,0,0.653436,"lysis for each word, each word in the full corpus has a single, 1-best analysis.12 We follow the same 4-pass alignment methodology as in the baseline, but train and apply the IBM-HMM model on sequences of surface or deep forms instead of words. The morphological segmentation experiments rely on the existence of a morphological analyzer, a tool that is not available for all languages. To demonstrate that simple heuristics can also perform well in the absence of morphological analysis tools, we performed experiments using the Prefix of each word as a stand-in for more complicated lemmatization (Simard et al., 1992; Och, 2005; Nicholson et al., 2012). As a polysynthetic language, Inuktitut has many very-low frequency words, and even English words have some morphology which a stemmer could normalize. Since syllabic characters represent whole syllables, we chose a prefix of three characters to approximate the stem of an Inuktitut word and we use a five letter prefix for English words. We follow the same alignment methodology as in the baseline, but train and apply the IBM-HMM model on sequences of prefixes instead of words. Byte-pair encoding (BPE) is another approach to word segmentation that does not re"
2020.lrec-1.312,D19-1136,0,0.0737348,"0 of the corpus (NH 2.0) was released in January 2008. It covered the proceedings of the Nunavut Assembly through November 8, 2007 (excluding 2003). NH 2.0 contained 5,589,323 English words and and 2,651,414 Inuktitut words. Sentence alignment was carried out with Moore’s aligner (Moore, 2002).4 For this work, we use our in-house sentence aligner, based on Moore (2002) with some enhancements. Like Braune and Fraser (2010) and most sentence aligners, we allow 1– many alignments, and like Yu et al. (2012) we use a multipass approach to refine alignments. A recent approach to sentence alignment (Thompson and Koehn, 2019) is reported to significantly improve alignment quality by incorporating bilingual word embeddings trained on large pre-existing parallel corpora. We experimented with this approach as well (see §3.3.). 3. Alignment of the Nunavut Hansard Inuktitut–English Parallel Corpus 3.0 The Nunavut Hansard Inuktitut–English Parallel Corpus 3.0 (NH 3.0) consists of 17,330,271 English words in 1,452,347 sentences, and 8,068,977 Inuktitut words in 1,450,094 sentences, yielding approximately 1.3 million aligned sentence pairs.5 It covers the proceedings through June 8, 2017. 3.1. Text Extraction from Word Do"
2020.lrec-1.312,W19-5343,0,0.0197083,"on, tokenization (with aggressive hyphen splitting), cleaned the training corpus (sentence length ratio 15, minimum sentence length 1, maximum 200), trained a truecaser on the training data and then applied it to all data. We trained byte-pair encoding models on the training data using subword-nmt,18 with disjoint vocabularies ranging from 500 to 60,000 operations, which were then applied to all data. Future work may wish to consider promising morphologically informed approaches to segmentation for machine translation or alignment (Ataman et al., 2017; Ataman and Federico, 2018; Micher, 2018; Toral et al., 2019). 4.3. Baseline Experiments We present baseline NMT and SMT results, over several versions of the data, in the hopes of encouraging future work on English–Inuktitut machine translation using this corpus. BLEU scores (Papineni et al., 2002) are computed with lowercase, v13a tokenization, using sacrebleu (Post, 2018). It should be emphasized that these scores are as high as they are in large part due to the formulaic and self-similar nature of the parliamentary genre, and should not be taken as representative of general-domain MT performance. 4.3.1. Neural Machine Translation Our baseline neural"
2020.lrec-1.312,D13-1140,0,0.0347578,"rase-based SMT system. The translation model uses IBM4 word alignments (Brown et al., 1993) with growdiag-final-and phrase extraction heuristics (Koehn et al., 2003). A 5-gram language model was trained on the target-side of the corpus using SRILM (Stolcke, 2002) with Kneser–Ney interpolated smoothing, modified when possible. The SMT system also includes a hierarchical distortion model with a maximum phrase length of 7 words, a sparse feature model consisting of the standard sparse features proposed by Hopkins and May (2011) and sparse hierarchical distortion model features proposed by Cherry (2013), and a neural network joint model (NNJM), with 3 words of target context and 11 words of source context, effectively a 15gram language model (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7-word distortion limit. We report the average results from five tuning runs with different random seeds. We ran SMT experiments with the same BPE"
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2020.wmt-1.100,D16-1250,0,0.0411131,"(Conneau et al., 2020) (XLMR) is also a massive multilingual pretrained language model. Similar to BERT, XLM-R is also In the WMT 2019 metrics shared task (Ma et al., 2019), we saw a very significant performance degradation between YiSi-1 and YiSi-2. This shows that current multilingual language models construct a shared multilingual space in an unsupervised manner without any direct bilingual signal, in which representations of context in the same language are likely to cluster together in part of the subspace and there is a language segregation in the shared multilingual space. Inspired by Artetxe et al. (2016) and Zhao et al. (2020), we obtain subword token pairs from the news translation task development set for each language (each contains around 1k to 3k sentence pairs) aligned by maximum alignment of their semantic similarities. We then train a cross-lingual linear projection (Zhao et al., 2020) that transforms the source embeddings into the target embeddings subspace. 904 Figure 1: Segment-level Kendall’s τ correlation with human direct assessment averaged over all WMT 2019 news translation test sets of YiSi-2 using contextual embeddings extracted from different layers of the multilingual pret"
2020.wmt-1.100,2020.acl-main.747,0,0.196713,"ems for WMT20’s QE Task 1 have access to the MT system that generate the translations while the referenceless metrics for the latter task have no information on the MT systems being evaluated. In WMT 2019 metrics shared task, pretrained multilingual BERT (Devlin et al., 2018) was used in YiSi for both MT reference-based (YiSi1) and reference-less (YiSi-2) evaluation in all tested translation directions where monolingual pretrained BERT model was not available for the target language (such as Czech, German, etc.). Since then, another massive multilingual pretrained language model, XLM-RoBERTa (Conneau et al., 2020), has been published. We evaluate the use of contextual embeddings extracted from each of the intermediate layers of the two models in MT reference-less evaluation. In addition, despite using the same pretrained embedding model of last year, YiSi-2 showed a significant performance degradation when comparing to YiSi-1. For example, segment-level correlation with human direct assessment for evaluating English→Czech drops from 0.475 (YiSi-1) to 0.069 (YiSi-2). This shows that the cross-lingual semantic representation in pretrained multilingual BERT is not as accurate as the monolingual semantic r"
2020.wmt-1.100,2020.lrec-1.312,1,0.732132,"LM-Rlarge for the best performance on crosslingual semantic similarity. As suggested by Devlin et al. (2018); Peters et al. (2018); Zhang et al. (2020), we experimented using contextual embeddings extracted from different layers of the multilingual language encoder to find out the layer that best represents the semantic space of the language. 2.2 Inuktitut-English Cross-lingual Language Model Since Inuktitut is neither covered by pretrained multilingual BERT nor XLM-RoBERTa, we trained our own Inuktitut-English XLM (Lample and Conneau, 2019) using the Nunavut Hansard 3.0 (NH) parallel corpus (Joanis et al., 2020). The model was trained with masked language model and translation language model tasks. The Inuktitut-English XLM model has 12 layers with 8 heads and embedding size of 512. 2.3 Cross-lingual Linear Projection YiSi-2 relies on a cross-lingual language representation to evaluate the cross-lingual lexical semantic similarity. Previously, it used pretrained multilingual BERT (Devlin et al., 2018) for this purpose. BERT captures the sentence context in the embeddings, such that the embedding of the same subword unit in different sentences would be different from each other and be better represent"
2020.wmt-1.100,W19-5358,1,0.735733,"velopment set. 1 Introduction The machine translation quality estimation as a metric (QE as a metric) task was first introduced in WMT 2019 (Ma et al., 2019; Fonseca et al., 2019) to encourage the exploration of reference-less evaluation metrics. QE as a metric task shifts the use case of the QE systems from assisting professional translators to estimate post-editing efforts to assisting MT developers or general MT users to discriminate the translation quality of different MT systems without the presence of a human reference translation. YiSi-2, the reference-less variants of the YiSi metric (Lo, 2019), was the only metric who participated in evaluating all the translation directions in WMT 2019 QE as a metric shared task. The QE as a metric task is very similar to Task 1 (Sentence-level direct assessment) of WMT20’s quality estimation shared task where metric performance is evaluated in terms of correlation at the sentence-level with human direct assessment scores on translation quality. The subtle but crucial difference between the WMT20 QE Task 1 and the QE as a metric task is that QE systems for the former task is trained specifically to estimate the quality of a single MT system wherea"
2020.wmt-1.100,2020.wmt-1.99,1,0.725361,"om the last) and projecting the source embeddings into the target embedding space using a cross-lingual linear projection matrix learnt from a small development set. 2 YiSi-2 YiSi (Lo, 2019) is a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources. YiSi1 measures the similarity between a machine translation and human references by aggregating weighted distributional (lexical) semantic similarities, and optionally incorporating shallow semantic structures. Improvements in YiSi-1 for WMT 2020 metrics shared task is detailed in (Lo, 2020). YiSi-2 is the bilingual, reference-less version, which uses bilingual word embeddings to evaluate cross-lingual lexical semantic similarity between the input and MT output. 2.1 Massive Multilingual Pretrained Language Models trained with a masked language model task on the concatenation of non-parallel data. The differences between XLM-R and BERT are 1) XLM-R is trained on the CommonCrawl corpus which is significantly larger than the Wikipedia training data used by BERT; 2) instead of a uniform data sampling rate used in BERT, XLM-R uses a language sampling rate that is proportional to the a"
2020.wmt-1.100,K19-1020,1,0.738886,"ross-lingual language representation to evaluate the cross-lingual lexical semantic similarity. Previously, it used pretrained multilingual BERT (Devlin et al., 2018) for this purpose. BERT captures the sentence context in the embeddings, such that the embedding of the same subword unit in different sentences would be different from each other and be better represented in the embedding space. Since multilingual BERT is trained on the concatenation of nonparallel data from each language, the circular dependency deadlock between parallel resource and cross-lingual semantic similarity is broken (Lo and Simard, 2019). Multilingual BERT covers the 104 largest languages in Wikipedia. XLM-RoBERTa (Conneau et al., 2020) (XLMR) is also a massive multilingual pretrained language model. Similar to BERT, XLM-R is also In the WMT 2019 metrics shared task (Ma et al., 2019), we saw a very significant performance degradation between YiSi-1 and YiSi-2. This shows that current multilingual language models construct a shared multilingual space in an unsupervised manner without any direct bilingual signal, in which representations of context in the same language are likely to cluster together in part of the subspace and"
2020.wmt-1.100,W19-5302,0,0.296145,"language to be closer to that of the target language in the pretrained model to obtain more accurate cross-lingual semantic similarity representations. Our results show that YiSi-2’s correlation with human direct assessment on translation quality is greatly improved by replacing multilingual BERT with XLM-RoBERTa and projecting the source embeddings into the target embedding space using a cross-lingual linear projection (CLP) matrix learnt from a small development set. 1 Introduction The machine translation quality estimation as a metric (QE as a metric) task was first introduced in WMT 2019 (Ma et al., 2019; Fonseca et al., 2019) to encourage the exploration of reference-less evaluation metrics. QE as a metric task shifts the use case of the QE systems from assisting professional translators to estimate post-editing efforts to assisting MT developers or general MT users to discriminate the translation quality of different MT systems without the presence of a human reference translation. YiSi-2, the reference-less variants of the YiSi metric (Lo, 2019), was the only metric who participated in evaluating all the translation directions in WMT 2019 QE as a metric shared task. The QE as a metric task"
2020.wmt-1.100,N18-1202,0,0.0459137,"data. The differences between XLM-R and BERT are 1) XLM-R is trained on the CommonCrawl corpus which is significantly larger than the Wikipedia training data used by BERT; 2) instead of a uniform data sampling rate used in BERT, XLM-R uses a language sampling rate that is proportional to the amount of data available in the training set. Because of these differences, XLM-R performs better on low resource languages than multilingual BERT. XLMR covers 100 languages. In this work, we use XLM-Rlarge for the best performance on crosslingual semantic similarity. As suggested by Devlin et al. (2018); Peters et al. (2018); Zhang et al. (2020), we experimented using contextual embeddings extracted from different layers of the multilingual language encoder to find out the layer that best represents the semantic space of the language. 2.2 Inuktitut-English Cross-lingual Language Model Since Inuktitut is neither covered by pretrained multilingual BERT nor XLM-RoBERTa, we trained our own Inuktitut-English XLM (Lample and Conneau, 2019) using the Nunavut Hansard 3.0 (NH) parallel corpus (Joanis et al., 2020). The model was trained with masked language model and translation language model tasks. The Inuktitut-English"
2020.wmt-1.100,2020.acl-main.151,0,0.191781,"ent for evaluating English→Czech drops from 0.475 (YiSi-1) to 0.069 (YiSi-2). This shows that the cross-lingual semantic representation in pretrained multilingual BERT is not as accurate as the monolingual semantic representation for each language. In other words, we observed the language clustering effect where a clear segregation of vector subspace among different languages in the multilingual contextual em903 Proceedings of the 5th Conference on Machine Translation (WMT), pages 903–910 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics bedding model. Inspired by Zhao et al. (2020), we employ a weakly-supervised bilingual mapping learnt from a small development set that transforms the contextual embeddings of the source sentence to the target subspace for better cross-lingual semantic similarity evaluation. In this paper, we show that YiSi-2’s correlation with human direct assessment on translation quality is greatly improved by replacing multilingual BERT with XLM-RoBERTalarge using the optimal intermediate layer (7th layer count from the last) and projecting the source embeddings into the target embedding space using a cross-lingual linear projection matrix learnt fro"
2020.wmt-1.110,D16-1250,0,0.160531,", Lo et al. (2018)’s supervised submissions were developed for the same MT evaluation pipeline using a new semantic MT metric, YiSi-1 (Lo, 2019) (see also section 2.3). At the WMT19 parallel corpus filtering task, Bernier972 Proceedings of the 5th Conference on Machine Translation (WMT), pages 972–978 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Colborne and Lo (2019) exploited the quality estimation metric YiSi-2 using bilingual word embeddings learnt in a supervised manner (Luong et al., 2015) from clean parallel training data or a weakly supervised manner (Artetxe et al., 2016) from bilingual dictionary. Lo and Simard (2019) further showed that using YiSi-2 with multilingual BERT (Devlin et al., 2019) on fully unsupervised parallel corpus filtering (i.e. without access of any parallel training data) achieved similar results to those in Bernier-Colborne and Lo (2019). This year, the National Research Council of Canada (NRC) team submitted one system to the parallel corpus filtering task and one to the alignment task. The two systems share the same components in scoring the parallelism of the noisy sentence pairs, i.e., the pre-filtering rules and the quality estimati"
2020.wmt-1.110,W19-5434,1,0.869483,"T), pages 972–978 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Colborne and Lo (2019) exploited the quality estimation metric YiSi-2 using bilingual word embeddings learnt in a supervised manner (Luong et al., 2015) from clean parallel training data or a weakly supervised manner (Artetxe et al., 2016) from bilingual dictionary. Lo and Simard (2019) further showed that using YiSi-2 with multilingual BERT (Devlin et al., 2019) on fully unsupervised parallel corpus filtering (i.e. without access of any parallel training data) achieved similar results to those in Bernier-Colborne and Lo (2019). This year, the National Research Council of Canada (NRC) team submitted one system to the parallel corpus filtering task and one to the alignment task. The two systems share the same components in scoring the parallelism of the noisy sentence pairs, i.e., the pre-filtering rules and the quality estimation metric YiSi-2. For the parallel corpus aligning task, we use an iterative statistical alignment method to align sentences from the given document pairs before passing the aligned sentences to the scoring pipeline. Our internal results show that MT systems trained on pre-aligned sentences fi"
2020.wmt-1.110,W19-5435,0,0.0360302,"lel corpus filtering task and one to the alignment task. The two systems share the same components in scoring the parallelism of the noisy sentence pairs, i.e., the pre-filtering rules and the quality estimation metric YiSi-2. For the parallel corpus aligning task, we use an iterative statistical alignment method to align sentences from the given document pairs before passing the aligned sentences to the scoring pipeline. Our internal results show that MT systems trained on pre-aligned sentences filtered by our scoring pipeline outperform those trained on the organizers’ LASER-based baseline (Chaudhary et al., 2019) by 0.2–1.4 BLEU. Training MT systems on re-aligned sentences using our iterative statistical alignment method achieve further gains of 0.3–1.8 BLEU. 2 2.1 Iterative statistical sentence alignment Our iterative statistical sentence alignment method as detailed in Joanis et al. (2020) uses ssal, a reimplementation and extension of Moore (2002) which is part of the Portage statistical machine translation toolkit (Larkin et al., 2010). First, we train an IBM-HMM model (Och and Ney, 2003) on the clean parallel training data and the subsampled noisy corpora (see Table 1 for statistics) and use it t"
2020.wmt-1.110,2020.acl-main.747,0,0.0267588,"eighted distributional (lexical) semantic similarities, and optionally incorporating shallow semantic structures. YiSi-2 is the bilingual, referenceless version, which uses bilingual word embeddings to evaluate cross-lingual lexical semantic similarity between the input and MT output or, in this task, between the source and target sentences. YiSi-2 relies on a crosslingal language representation to evaluate the crosslingual lexical semantic similarity. Previously, it used pre-trained multilingual BERT (Devlin et al., 2019) for this purpose. In this work, we instead experiment with XLMRoBERTa (Conneau et al., 2020) because (1) at the time this work was done, it was the only pretrained multilingual language encoder that covers both Khmer, Pashto and English; and (2) it shows better performance with lower-resource languages than BERT. As suggested by Devlin et al. (2019); Peters et al. (2018); Zhang et al. (2020), we experiment with using contextual embeddings extracted from different layers of the multilingual language encoder to find out the layer that best represents the semantic space of the language. YiSi is semantic oriented. In the past, we noticed that YiSi-based scoring functions failed to filter"
2020.wmt-1.110,N19-1423,0,0.0540883,"YiSi-1 (Lo, 2019) (see also section 2.3). At the WMT19 parallel corpus filtering task, Bernier972 Proceedings of the 5th Conference on Machine Translation (WMT), pages 972–978 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Colborne and Lo (2019) exploited the quality estimation metric YiSi-2 using bilingual word embeddings learnt in a supervised manner (Luong et al., 2015) from clean parallel training data or a weakly supervised manner (Artetxe et al., 2016) from bilingual dictionary. Lo and Simard (2019) further showed that using YiSi-2 with multilingual BERT (Devlin et al., 2019) on fully unsupervised parallel corpus filtering (i.e. without access of any parallel training data) achieved similar results to those in Bernier-Colborne and Lo (2019). This year, the National Research Council of Canada (NRC) team submitted one system to the parallel corpus filtering task and one to the alignment task. The two systems share the same components in scoring the parallelism of the noisy sentence pairs, i.e., the pre-filtering rules and the quality estimation metric YiSi-2. For the parallel corpus aligning task, we use an iterative statistical alignment method to align sentences f"
2020.wmt-1.110,2012.amta-papers.7,0,0.0522598,"ll sources of parallel data, covering different domains, were provided for each of the two low-resource languages. Much larger monolingual corpora were also provided for each language (en, km and ps). In addition to the task of computing quality scores for the purpose of filtering, there is also a sub-task of re-aligning the sentence pairs from the original crawled document pairs. Cleanliness or quality of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality in different ways. Crosslingual semantic textual similarity is one of the most important properties of high-quality sentence pairs. Lo et al. (2016) scored cross-lingual semantic textual similarity in two ways, either using a semantic MT quality estimation metric, or by first translating one of the sentences using MT, and then comparing the result to the other sentence, using a semantic MT evaluation metric. At the WMT18 parallel corpus filtering task, Lo et al. (2018)’s supervised submission"
2020.wmt-1.110,D19-1632,0,0.0130362,"pair. Then samples containing the top-scoring 5M words are used to train MT systems. While using the filtered parallel data to train a FAIRseq (Ott et al., 2019) neural machine translation (NMT) system remains the same as last year, the organisers are no longer building statistical machine translation (SMT) systems as part of the task evaluation. Instead, as an alternative evaluation, the filtered parallel corpus is used to fine-tune an MBART (Liu et al., 2020) pretrained NMT system. Participants were ranked based on the performance of these MT systems on a test set of Wikipedia translations (Guzmán et al., 2019), as measured by BLEU (Papineni et al., 2002). A few small sources of parallel data, covering different domains, were provided for each of the two low-resource languages. Much larger monolingual corpora were also provided for each language (en, km and ps). In addition to the task of computing quality scores for the purpose of filtering, there is also a sub-task of re-aligning the sentence pairs from the original crawled document pairs. Cleanliness or quality of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of th"
2020.wmt-1.110,W19-5404,0,0.0390844,"Missing"
2020.wmt-1.110,W10-1717,1,0.697494,"results show that MT systems trained on pre-aligned sentences filtered by our scoring pipeline outperform those trained on the organizers’ LASER-based baseline (Chaudhary et al., 2019) by 0.2–1.4 BLEU. Training MT systems on re-aligned sentences using our iterative statistical alignment method achieve further gains of 0.3–1.8 BLEU. 2 2.1 Iterative statistical sentence alignment Our iterative statistical sentence alignment method as detailed in Joanis et al. (2020) uses ssal, a reimplementation and extension of Moore (2002) which is part of the Portage statistical machine translation toolkit (Larkin et al., 2010). First, we train an IBM-HMM model (Och and Ney, 2003) on the clean parallel training data and the subsampled noisy corpora (see Table 1 for statistics) and use it to align paragraphs in the given document pairs, as Moore (2002) does. The subsampled noisy corpora are those obtained by applying our filtering baseline as described in sections 2.2 and 2.3 (and denoted as “nrc.baseline” in table 2). Then, we segment the paragraphs in both languages into sentences using the Portage sentence splitter. Finally, we align sentences within aligned paragraphs using the IBM model again. In this process, b"
2020.wmt-1.110,2020.tacl-1.47,0,0.0171199,"to– English (ps–en). Specifically, participating systems must produce a score for each sentence pair in the test corpora indicating the quality of that pair. Then samples containing the top-scoring 5M words are used to train MT systems. While using the filtered parallel data to train a FAIRseq (Ott et al., 2019) neural machine translation (NMT) system remains the same as last year, the organisers are no longer building statistical machine translation (SMT) systems as part of the task evaluation. Instead, as an alternative evaluation, the filtered parallel corpus is used to fine-tune an MBART (Liu et al., 2020) pretrained NMT system. Participants were ranked based on the performance of these MT systems on a test set of Wikipedia translations (Guzmán et al., 2019), as measured by BLEU (Papineni et al., 2002). A few small sources of parallel data, covering different domains, were provided for each of the two low-resource languages. Much larger monolingual corpora were also provided for each language (en, km and ps). In addition to the task of computing quality scores for the purpose of filtering, there is also a sub-task of re-aligning the sentence pairs from the original crawled document pairs. Clean"
2020.wmt-1.110,W19-5358,1,0.896999,"T quality in different ways. Crosslingual semantic textual similarity is one of the most important properties of high-quality sentence pairs. Lo et al. (2016) scored cross-lingual semantic textual similarity in two ways, either using a semantic MT quality estimation metric, or by first translating one of the sentences using MT, and then comparing the result to the other sentence, using a semantic MT evaluation metric. At the WMT18 parallel corpus filtering task, Lo et al. (2018)’s supervised submissions were developed for the same MT evaluation pipeline using a new semantic MT metric, YiSi-1 (Lo, 2019) (see also section 2.3). At the WMT19 parallel corpus filtering task, Bernier972 Proceedings of the 5th Conference on Machine Translation (WMT), pages 972–978 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Colborne and Lo (2019) exploited the quality estimation metric YiSi-2 using bilingual word embeddings learnt in a supervised manner (Luong et al., 2015) from clean parallel training data or a weakly supervised manner (Artetxe et al., 2016) from bilingual dictionary. Lo and Simard (2019) further showed that using YiSi-2 with multilingual BERT (Devlin et al., 20"
2020.wmt-1.110,S16-1102,1,0.892927,"Missing"
2020.wmt-1.110,2020.wmt-1.100,1,0.768633,"Missing"
2020.wmt-1.110,K19-1020,1,0.82923,"developed for the same MT evaluation pipeline using a new semantic MT metric, YiSi-1 (Lo, 2019) (see also section 2.3). At the WMT19 parallel corpus filtering task, Bernier972 Proceedings of the 5th Conference on Machine Translation (WMT), pages 972–978 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Colborne and Lo (2019) exploited the quality estimation metric YiSi-2 using bilingual word embeddings learnt in a supervised manner (Luong et al., 2015) from clean parallel training data or a weakly supervised manner (Artetxe et al., 2016) from bilingual dictionary. Lo and Simard (2019) further showed that using YiSi-2 with multilingual BERT (Devlin et al., 2019) on fully unsupervised parallel corpus filtering (i.e. without access of any parallel training data) achieved similar results to those in Bernier-Colborne and Lo (2019). This year, the National Research Council of Canada (NRC) team submitted one system to the parallel corpus filtering task and one to the alignment task. The two systems share the same components in scoring the parallelism of the noisy sentence pairs, i.e., the pre-filtering rules and the quality estimation metric YiSi-2. For the parallel corpus aligni"
2020.wmt-1.110,W18-6481,1,0.850833,"etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality in different ways. Crosslingual semantic textual similarity is one of the most important properties of high-quality sentence pairs. Lo et al. (2016) scored cross-lingual semantic textual similarity in two ways, either using a semantic MT quality estimation metric, or by first translating one of the sentences using MT, and then comparing the result to the other sentence, using a semantic MT evaluation metric. At the WMT18 parallel corpus filtering task, Lo et al. (2018)’s supervised submissions were developed for the same MT evaluation pipeline using a new semantic MT metric, YiSi-1 (Lo, 2019) (see also section 2.3). At the WMT19 parallel corpus filtering task, Bernier972 Proceedings of the 5th Conference on Machine Translation (WMT), pages 972–978 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Colborne and Lo (2019) exploited the quality estimation metric YiSi-2 using bilingual word embeddings learnt in a supervised manner (Luong et al., 2015) from clean parallel training data or a weakly supervised manner (Artetxe et al., 20"
2020.wmt-1.110,W15-1521,0,0.0221661,"e, using a semantic MT evaluation metric. At the WMT18 parallel corpus filtering task, Lo et al. (2018)’s supervised submissions were developed for the same MT evaluation pipeline using a new semantic MT metric, YiSi-1 (Lo, 2019) (see also section 2.3). At the WMT19 parallel corpus filtering task, Bernier972 Proceedings of the 5th Conference on Machine Translation (WMT), pages 972–978 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Colborne and Lo (2019) exploited the quality estimation metric YiSi-2 using bilingual word embeddings learnt in a supervised manner (Luong et al., 2015) from clean parallel training data or a weakly supervised manner (Artetxe et al., 2016) from bilingual dictionary. Lo and Simard (2019) further showed that using YiSi-2 with multilingual BERT (Devlin et al., 2019) on fully unsupervised parallel corpus filtering (i.e. without access of any parallel training data) achieved similar results to those in Bernier-Colborne and Lo (2019). This year, the National Research Council of Canada (NRC) team submitted one system to the parallel corpus filtering task and one to the alignment task. The two systems share the same components in scoring the parallel"
2020.wmt-1.110,W19-5302,0,0.0170054,"al language encoder that covers both Khmer, Pashto and English; and (2) it shows better performance with lower-resource languages than BERT. As suggested by Devlin et al. (2019); Peters et al. (2018); Zhang et al. (2020), we experiment with using contextual embeddings extracted from different layers of the multilingual language encoder to find out the layer that best represents the semantic space of the language. YiSi is semantic oriented. In the past, we noticed that YiSi-based scoring functions failed to filter out sentence pairs with disfluent target text. In the WMT19 metrics shared task (Ma et al., 2019), we saw a very significant performance degradation between YiSi-1 and YiSi-2. This suggests that current multilingual language models construct a shared multilingual space in an unsupervised manner without any direct bilingual signal, in which representations of context in the same language are likely to cluster together in part of the subspace and there is a language segregation in the shared multilingual space. Inspired by Artetxe et al. (2016) and Zhao et al. (2020), we sample 5k clean sentence pairs and use the token pairs aligned by maximum alignment of their semantic similarity to train"
2020.wmt-1.110,moore-2002-fast,0,0.123683,"nt pairs before passing the aligned sentences to the scoring pipeline. Our internal results show that MT systems trained on pre-aligned sentences filtered by our scoring pipeline outperform those trained on the organizers’ LASER-based baseline (Chaudhary et al., 2019) by 0.2–1.4 BLEU. Training MT systems on re-aligned sentences using our iterative statistical alignment method achieve further gains of 0.3–1.8 BLEU. 2 2.1 Iterative statistical sentence alignment Our iterative statistical sentence alignment method as detailed in Joanis et al. (2020) uses ssal, a reimplementation and extension of Moore (2002) which is part of the Portage statistical machine translation toolkit (Larkin et al., 2010). First, we train an IBM-HMM model (Och and Ney, 2003) on the clean parallel training data and the subsampled noisy corpora (see Table 1 for statistics) and use it to align paragraphs in the given document pairs, as Moore (2002) does. The subsampled noisy corpora are those obtained by applying our filtering baseline as described in sections 2.2 and 2.3 (and denoted as “nrc.baseline” in table 2). Then, we segment the paragraphs in both languages into sentences using the Portage sentence splitter. Finally,"
2020.wmt-1.110,J03-1002,0,0.0474675,"tences filtered by our scoring pipeline outperform those trained on the organizers’ LASER-based baseline (Chaudhary et al., 2019) by 0.2–1.4 BLEU. Training MT systems on re-aligned sentences using our iterative statistical alignment method achieve further gains of 0.3–1.8 BLEU. 2 2.1 Iterative statistical sentence alignment Our iterative statistical sentence alignment method as detailed in Joanis et al. (2020) uses ssal, a reimplementation and extension of Moore (2002) which is part of the Portage statistical machine translation toolkit (Larkin et al., 2010). First, we train an IBM-HMM model (Och and Ney, 2003) on the clean parallel training data and the subsampled noisy corpora (see Table 1 for statistics) and use it to align paragraphs in the given document pairs, as Moore (2002) does. The subsampled noisy corpora are those obtained by applying our filtering baseline as described in sections 2.2 and 2.3 (and denoted as “nrc.baseline” in table 2). Then, we segment the paragraphs in both languages into sentences using the Portage sentence splitter. Finally, we align sentences within aligned paragraphs using the IBM model again. In this process, both the data used in training the IBM-HMM model and th"
2020.wmt-1.110,N19-4009,0,0.0284869,"ehn et al., 2018b, 2019): identifying high-quality sentence pairs in a noisy corpus crawled from the web using ParaCrawl (Koehn et al., 2018a), in order to train machine translation (MT) systems on the clean data. This year, the low-resource language pairs being tested are Khmer–English (km–en) and Pashto– English (ps–en). Specifically, participating systems must produce a score for each sentence pair in the test corpora indicating the quality of that pair. Then samples containing the top-scoring 5M words are used to train MT systems. While using the filtered parallel data to train a FAIRseq (Ott et al., 2019) neural machine translation (NMT) system remains the same as last year, the organisers are no longer building statistical machine translation (SMT) systems as part of the task evaluation. Instead, as an alternative evaluation, the filtered parallel corpus is used to fine-tune an MBART (Liu et al., 2020) pretrained NMT system. Participants were ranked based on the performance of these MT systems on a test set of Wikipedia translations (Guzmán et al., 2019), as measured by BLEU (Papineni et al., 2002). A few small sources of parallel data, covering different domains, were provided for each of th"
2020.wmt-1.110,P02-1040,0,0.106641,"ng 5M words are used to train MT systems. While using the filtered parallel data to train a FAIRseq (Ott et al., 2019) neural machine translation (NMT) system remains the same as last year, the organisers are no longer building statistical machine translation (SMT) systems as part of the task evaluation. Instead, as an alternative evaluation, the filtered parallel corpus is used to fine-tune an MBART (Liu et al., 2020) pretrained NMT system. Participants were ranked based on the performance of these MT systems on a test set of Wikipedia translations (Guzmán et al., 2019), as measured by BLEU (Papineni et al., 2002). A few small sources of parallel data, covering different domains, were provided for each of the two low-resource languages. Much larger monolingual corpora were also provided for each language (en, km and ps). In addition to the task of computing quality scores for the purpose of filtering, there is also a sub-task of re-aligning the sentence pairs from the original crawled document pairs. Cleanliness or quality of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Prev"
2020.wmt-1.110,N18-1202,0,0.00825476,"ut or, in this task, between the source and target sentences. YiSi-2 relies on a crosslingal language representation to evaluate the crosslingual lexical semantic similarity. Previously, it used pre-trained multilingual BERT (Devlin et al., 2019) for this purpose. In this work, we instead experiment with XLMRoBERTa (Conneau et al., 2020) because (1) at the time this work was done, it was the only pretrained multilingual language encoder that covers both Khmer, Pashto and English; and (2) it shows better performance with lower-resource languages than BERT. As suggested by Devlin et al. (2019); Peters et al. (2018); Zhang et al. (2020), we experiment with using contextual embeddings extracted from different layers of the multilingual language encoder to find out the layer that best represents the semantic space of the language. YiSi is semantic oriented. In the past, we noticed that YiSi-based scoring functions failed to filter out sentence pairs with disfluent target text. In the WMT19 metrics shared task (Ma et al., 2019), we saw a very significant performance degradation between YiSi-1 and YiSi-2. This suggests that current multilingual language models construct a shared multilingual space in an unsu"
2020.wmt-1.110,2014.amta-researchers.6,0,0.01357,"l data, covering different domains, were provided for each of the two low-resource languages. Much larger monolingual corpora were also provided for each language (en, km and ps). In addition to the task of computing quality scores for the purpose of filtering, there is also a sub-task of re-aligning the sentence pairs from the original crawled document pairs. Cleanliness or quality of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality in different ways. Crosslingual semantic textual similarity is one of the most important properties of high-quality sentence pairs. Lo et al. (2016) scored cross-lingual semantic textual similarity in two ways, either using a semantic MT quality estimation metric, or by first translating one of the sentences using MT, and then comparing the result to the other sentence, using a semantic MT evaluation metric. At the WMT18 parallel corpus filtering task, Lo et al. (2018)’s supervised submissions were develope"
2020.wmt-1.110,2020.acl-main.151,0,0.0110477,"YiSi-based scoring functions failed to filter out sentence pairs with disfluent target text. In the WMT19 metrics shared task (Ma et al., 2019), we saw a very significant performance degradation between YiSi-1 and YiSi-2. This suggests that current multilingual language models construct a shared multilingual space in an unsupervised manner without any direct bilingual signal, in which representations of context in the same language are likely to cluster together in part of the subspace and there is a language segregation in the shared multilingual space. Inspired by Artetxe et al. (2016) and Zhao et al. (2020), we sample 5k clean sentence pairs and use the token pairs aligned by maximum alignment of their semantic similarity to train a cross-lingual linear projection that would transform the source embeddings into the target embeddings subspace. 974 Lo and Larkin (2020) provide a detailed correlation analysis of YiSi-2 with all the improvements mentioned above and human judgment on (a) km–en FAIRseq (b) km–en MBART (c) ps–en FAIRseq (d) ps–en MBART Figure 1: BLEU scores on the Khmer–English dev set for (a) FAIRseq and (b) MBART and the Pashto–English dev set for (c) FAIRseq and (d) MBART trained on"
2020.wmt-1.99,2020.acl-main.645,0,0.0816945,"Missing"
2020.wmt-1.99,P19-1269,0,0.104287,"Missing"
2020.wmt-1.99,2020.acl-main.703,0,0.0678964,"Missing"
2020.wmt-1.99,2020.acl-main.448,0,0.104918,"valuation. In this paper, we study YiSi-1’s correlation with human translation quality judgment by varying three major attributes (which architecture; which intermediate layer; whether it is monolingual or multilingual) of the pretrained language models. Results of the study show further improvements over YiSi-1 on the WMT 2019 Metrics shared task. We also describe the pretrained language model we trained for evaluating Inuktitut machine translation output. 1 Introduction Recent research on large-scale evaluation of automatic machine translation (MT) evaluation metrics (Ma et al., 2018, 2019; Mathur et al., 2020) showed that the newly proposed contextual embedding based metrics, like YiSi-1, BERTscore (Zhang et al., 2020) and ESIM (Mathur et al., 2019), significantly outperform BLEU (Papineni et al., 2002) and other metrics in correlating with human judgment on translation quality. YiSi-1 and BERTscore use contextual embeddings extracted from the pretrained language model, Devlin et al. (2018), as-is without further fine-tuning or fitting to existing labeled data predictions. Although finetuning the pretrained language models for specific downstream tasks show improvements in many cases, using the pre"
2020.wmt-1.99,2021.ccl-1.108,0,0.0301619,"Missing"
2020.wmt-1.99,P02-1040,0,0.106867,"lingual or multilingual) of the pretrained language models. Results of the study show further improvements over YiSi-1 on the WMT 2019 Metrics shared task. We also describe the pretrained language model we trained for evaluating Inuktitut machine translation output. 1 Introduction Recent research on large-scale evaluation of automatic machine translation (MT) evaluation metrics (Ma et al., 2018, 2019; Mathur et al., 2020) showed that the newly proposed contextual embedding based metrics, like YiSi-1, BERTscore (Zhang et al., 2020) and ESIM (Mathur et al., 2019), significantly outperform BLEU (Papineni et al., 2002) and other metrics in correlating with human judgment on translation quality. YiSi-1 and BERTscore use contextual embeddings extracted from the pretrained language model, Devlin et al. (2018), as-is without further fine-tuning or fitting to existing labeled data predictions. Although finetuning the pretrained language models for specific downstream tasks show improvements in many cases, using the pretrained language models without fine-tuning makes the MT evaluation metrics more portable to languages without labeled data 2 YiSi YiSi (Lo, 2019) is a unified semantic MT quality evaluation and es"
2020.wmt-1.99,W19-5358,1,0.898353,"2019), significantly outperform BLEU (Papineni et al., 2002) and other metrics in correlating with human judgment on translation quality. YiSi-1 and BERTscore use contextual embeddings extracted from the pretrained language model, Devlin et al. (2018), as-is without further fine-tuning or fitting to existing labeled data predictions. Although finetuning the pretrained language models for specific downstream tasks show improvements in many cases, using the pretrained language models without fine-tuning makes the MT evaluation metrics more portable to languages without labeled data 2 YiSi YiSi (Lo, 2019) is a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources. YiSi1 measures the similarity between a machine 895 Proceedings of the 5th Conference on Machine Translation (WMT), pages 895–902 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics translation and human references by aggregating weighted distributional (lexical) semantic similarities, and optionally incorporating shallow semantic structures. YiSi-0 is the degenerate version of YiSi-1 that is ready-to-deploy to any languages by using longest"
2020.wmt-1.99,N18-1202,0,0.0443701,"as German (Chan et al., 2019), Finnish (Virtanen et al., 2019), French (Martin et al., 2020), Japanese Inui Laboratory (2019), Dutch (de Vries et al., 2019). In our experiments, we compare the performance of YiSi-1 using these monolingual models against that using multilingual language models. 2.3 Model size and intermediate layers In this study, we are interested in achieving the best performance using the pretrained language models. Thus, if different sizes of the same model architecture are released, we only evaluate the largest one in out experiment. As suggested by Devlin et al. (2018); Peters et al. (2018); Zhang et al. (2020), we experimented using contextual embeddings extracted from different layers of the multilingual language encoder to find out the layer that Other monolingual models in English A number of modifications to BERT have been proposed to optimize the pretrained language models. Lan et al. (2020) proposed ALBERT to reduce the amount of parameters in BERT for lower memory consumption and faster training speed. BART (Lewis et al., 2019) is effective when fine tuned 896 Figure 1: Average segment-level Kendall’s τ correlation with human direct assessment on WMT19 *-en news translat"
2020.wmt-1.99,2020.wmt-1.100,1,0.725361,"semantic similarities, and optionally incorporating shallow semantic structures. YiSi-0 is the degenerate version of YiSi-1 that is ready-to-deploy to any languages by using longest common character substring, instead of cosine similarity of contextual embeddings, to measure lexical similarity. YiSi-2 is the bilingual, reference-less version, which uses bilingual word embeddings to evaluate cross-lingual lexical semantic similarity between the input and MT output, and optionally incorporating shallow semantic structures. Improvements in YiSi-2 for WMT 2020 metrics shared task is detailed in (Lo and Larkin, 2020). 2.1 for text generation tasks. RoBERTa (Liu et al., 2019) is a more robustly trained version of BERT where the key hyperparameters are empirically chosen. XLNET (Yang et al., 2019) an autoregressive model that maximizes the expected likelihood over all permutations of the input sequence factorization order. We use these models in YiSi-1 for correlation analysis with human judgment on translation quality. 2.1.2 Multilingual models In addition to multilingual BERT used in Lo (2019), XLM-RoBERTa (Conneau et al., 2020) (XLM-R) is also a massive multilingual pretrained language model. Similar to"
2020.wmt-1.99,W18-6450,0,0.0670595,"r machine translation evaluation. In this paper, we study YiSi-1’s correlation with human translation quality judgment by varying three major attributes (which architecture; which intermediate layer; whether it is monolingual or multilingual) of the pretrained language models. Results of the study show further improvements over YiSi-1 on the WMT 2019 Metrics shared task. We also describe the pretrained language model we trained for evaluating Inuktitut machine translation output. 1 Introduction Recent research on large-scale evaluation of automatic machine translation (MT) evaluation metrics (Ma et al., 2018, 2019; Mathur et al., 2020) showed that the newly proposed contextual embedding based metrics, like YiSi-1, BERTscore (Zhang et al., 2020) and ESIM (Mathur et al., 2019), significantly outperform BLEU (Papineni et al., 2002) and other metrics in correlating with human judgment on translation quality. YiSi-1 and BERTscore use contextual embeddings extracted from the pretrained language model, Devlin et al. (2018), as-is without further fine-tuning or fitting to existing labeled data predictions. Although finetuning the pretrained language models for specific downstream tasks show improvements"
2020.wmt-1.99,W19-5302,0,0.1276,"-en news translation test set of YiSi-1 using different pretrained language representation models. Solid lines represent the use of pretrained monolingual models. Dotted line represents the use of pretrained XLM-R and dashed line represents the use of pretrained multilingual BERT. best represents the semantic space of the language. 3 and XLM-RoBERTalarge . Therefore, in WMT 2020 metrics shared task *-English MT output evaluation, we submit YiSi-1 scores based on embeddings extracted from the layer −6 of RoBERTalarge . Experiments and Results We use WMT 2019 metrics shared task evaluation set (Ma et al., 2019) for our development experiments. The official human judgments for translation quality of WMT 2019 were collected using reference-based direct assessment. Since we use exactly the same correlation analysis as the official metrics shared evaluation and the 2019 version of YiSi performed consistently well among participants in WMT 2019, we only compare our results with the 2019 version of YiSi and BLEU. Our results are directly comparable with those reported in Ma et al. (2019). 3.1 3.2 Monolingual models vs. multilingual models In Figure 1 and 2, we identify a common pattern that for evaluating"
I11-1047,1998.amta-tutorials.5,0,0.198481,"Missing"
I11-1047,C10-1054,0,0.0292649,"n the target language, the system simply returns &lt;not-found&gt;. 2.1 Representing Source Document We cannot enter documents with thousands of words directly into an online search engine. We need to convert full text into keywords to perform automated queries. A keyword may exist in multiple articles. However, several keywords cam uniquely identify a document if they are grouped together as a keyword set (Jiang et al., 2009). We then translate each keyword to target language to form the initial query. There are several reasons why using the translated keyword set as query directly, as proposed by Hong et al. (2010), does not always yields the desired target document: 1) Keyword translation might not correspond to the actual words in the target document; 2) Certain keywords in the target document might have been removed by content editors; 3) There are errors in keyword translation or selection. It is essential to select appropriate keywords to find the desired target document in a search engine. Two conditions that an appropriate keyword set should satisfy are: (1) they should represent the document exclusively (Jiang et al., 2009) (2) they should have unique or common translation in both languages. We"
I11-1047,W06-1710,0,0.0758253,"Missing"
I11-1047,ma-2006-champollion,0,0.0653889,"Missing"
I11-1047,J05-4003,0,0.0870527,"Missing"
I11-1047,J03-3002,0,0.0532487,"cuments from the web. We suggest that parallel documents can be mined with high precision from web sites that are not necessarily parallel to each other. Parallel resources reside on a diverse range of websites which can be classified into the following categories: Parallel websites: single website with structurally aligned bilingual pages. Typically they are websites of institutions, governments and commercial companies. (e.g. Financial Times Chinese/English, Wall Street Journal Chinese/English). Structure based methods were previously proposed to mine parallel documents from these websites: Resnik and Smith (2003) used (1) parent pages containing links to versions of one document in different languages and (2) sibling pages contains link to translation of the current documents. They also rely on the URL and anchor text to spot language specific version of documents. A structural alignment using DOM tree representation was proposed by Shi et al. (2006) to align parallel documents by using HTML structure. They identify the translational equivalent texts and hyperlinks between two parallel DOM trees to find parallel documents. However, the web is a heterogeneous collection of documents that extend far bey"
I11-1047,P06-1062,0,0.0236433,"ly they are websites of institutions, governments and commercial companies. (e.g. Financial Times Chinese/English, Wall Street Journal Chinese/English). Structure based methods were previously proposed to mine parallel documents from these websites: Resnik and Smith (2003) used (1) parent pages containing links to versions of one document in different languages and (2) sibling pages contains link to translation of the current documents. They also rely on the URL and anchor text to spot language specific version of documents. A structural alignment using DOM tree representation was proposed by Shi et al. (2006) to align parallel documents by using HTML structure. They identify the translational equivalent texts and hyperlinks between two parallel DOM trees to find parallel documents. However, the web is a heterogeneous collection of documents that extend far beyond bilingual and comparable pages with obvious structural features, such as similar URLs or common titles. Structural features only work for bilingual websites or document pairs that are already linked by editors. Comparable websites: websites that contain parallel content in different languages without any structural relation between docume"
I11-1047,C10-1124,0,0.0345992,"Missing"
I11-1047,P11-1133,1,0.874666,"Missing"
I11-1047,P06-1011,0,0.0608623,"Missing"
I11-1047,E09-1003,0,\N,Missing
I11-1047,P02-1040,0,\N,Missing
K19-1020,W11-4533,0,0.0508847,"Missing"
K19-1020,S16-1089,0,0.0696589,"Missing"
K19-1020,S17-2001,0,0.0331208,"ation models such as BERT, which are trained on the concatenation of non-parallel data from each language, we show that the deadlock around parallel resources can be broken. We perform intrinsic evaluations on crosslingual STS data sets and extrinsic evaluations on parallel corpus filtering and human translation equivalence assessment tasks. Our results show that the unsupervised crosslingual STS metric using BERT without fine-tuning achieves performance on par with supervised or weakly supervised approaches. 1 Introduction Crosslingual semantic textual similarity (STS) (Agirre et al., 2016a; Cer et al., 2017) aims at measuring the degree of meaning overlap between two texts written in different languages. It is a key task in crosslingual natural language understanding (XLU), with applications in crosslingual information retrieval (Franco-Salvador et al., 2014; Vuli´c and Moens, 2015), crosslingual plagiarism detection (Franco-Salvador et al., 2016a,b), etc. It 206 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 206–215 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics w(f ) for lexical units e and f in each language directly"
K19-1020,S16-1081,0,0.308764,"gual context representation models such as BERT, which are trained on the concatenation of non-parallel data from each language, we show that the deadlock around parallel resources can be broken. We perform intrinsic evaluations on crosslingual STS data sets and extrinsic evaluations on parallel corpus filtering and human translation equivalence assessment tasks. Our results show that the unsupervised crosslingual STS metric using BERT without fine-tuning achieves performance on par with supervised or weakly supervised approaches. 1 Introduction Crosslingual semantic textual similarity (STS) (Agirre et al., 2016a; Cer et al., 2017) aims at measuring the degree of meaning overlap between two texts written in different languages. It is a key task in crosslingual natural language understanding (XLU), with applications in crosslingual information retrieval (Franco-Salvador et al., 2014; Vuli´c and Moens, 2015), crosslingual plagiarism detection (Franco-Salvador et al., 2016a,b), etc. It 206 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 206–215 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics w(f ) for lexical units e and f in ea"
K19-1020,N19-1423,0,0.239773,"vily on existing parallel resources to first build a machine translation (MT) system and translate one of the test sentences into the other language for applying monolingual STS methods (Brychc´ın and Svoboda, 2016). Methods that do not rely explicitly on MT, such as that in Lo et al. (2018), still require parallel resources to build bilingual word representations for evaluating crosslingual lexical semantic similarity. It is clear that there is a circular dependency problem on parallel resources. Massively multilingual context representation models, such as MUSE (Conneau et al., 2017), BERT (Devlin et al., 2019), and XLM (Lample and Conneau, 2019), that are trained in an unsupervised manner with non-parallel data from each We present a fully unsupervised crosslingual semantic textual similarity (STS) metric, based on contextual embeddings extracted from BERT – Bidirectional Encoder Representations from Transformers (Devlin et al., 2019). The goal of crosslingual STS is to measure to what degree two segments of text in different languages express the same meaning. Not only is it a key task in crosslingual natural language understanding (XLU), it is also particularly useful for identifying parallel res"
K19-1020,W18-6481,1,0.939811,"ngual STS, neither the direction nor the origin (human or machine) of the translation is taken into account. Furthermore, MTQE also typically considers the fluency and grammaticality of the target text; these aspects are usually not perceived as relevant for crosslingual STS. Many previous crosslingual STS methods rely heavily on existing parallel resources to first build a machine translation (MT) system and translate one of the test sentences into the other language for applying monolingual STS methods (Brychc´ın and Svoboda, 2016). Methods that do not rely explicitly on MT, such as that in Lo et al. (2018), still require parallel resources to build bilingual word representations for evaluating crosslingual lexical semantic similarity. It is clear that there is a circular dependency problem on parallel resources. Massively multilingual context representation models, such as MUSE (Conneau et al., 2017), BERT (Devlin et al., 2019), and XLM (Lample and Conneau, 2019), that are trained in an unsupervised manner with non-parallel data from each We present a fully unsupervised crosslingual semantic textual similarity (STS) metric, based on contextual embeddings extracted from BERT – Bidirectional Enco"
K19-1020,E14-1044,0,0.0184522,"rinsic evaluations on parallel corpus filtering and human translation equivalence assessment tasks. Our results show that the unsupervised crosslingual STS metric using BERT without fine-tuning achieves performance on par with supervised or weakly supervised approaches. 1 Introduction Crosslingual semantic textual similarity (STS) (Agirre et al., 2016a; Cer et al., 2017) aims at measuring the degree of meaning overlap between two texts written in different languages. It is a key task in crosslingual natural language understanding (XLU), with applications in crosslingual information retrieval (Franco-Salvador et al., 2014; Vuli´c and Moens, 2015), crosslingual plagiarism detection (Franco-Salvador et al., 2016a,b), etc. It 206 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 206–215 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics w(f ) for lexical units e and f in each language directly on the texts under consideration, we rely on precomputed weights from monolingual corpora E and F of the two tested languages. The YiSi metrics are formulated as an F-score: by viewing the source text as a “query” and the target as an “answer”, precision"
K19-1020,W15-1521,0,0.195967,"Missing"
K19-1020,2012.amta-papers.7,0,0.197922,"This is done by means of log-linear model features that aim at maximizing n-gram precision between the MT output and the English sentence. More details on this method can be found in Lo et al. (2016). 3.2 4 Experiment on Parallel Corpus Filtering Next, we evaluate YiSi on the task of Parallel Corpus Filtering (PCF). Quality – or “cleanliness” – of parallel training data for MT has been shown to affect MT quality at different degrees, and various characteristics of the data – parallelism of the sentence pairs and the grammaticality of targetlanguage data – impact MT systems in different ways (Goutte et al., 2012; Simard, 2014; Khayrallah and Koehn, 2018). Results The results of these experiments are presented in Table 2, where performance is measured in terms of Pearson’s correlation with the test sets’ gold standard annotations. For reference, we also include results obtained by the UWB system (Brychc´ın and Svoboda, 2016), which was the best performing system in the SemEval 2016 crosslingual STS shared task. The UWB system is an MT-based system with a STS system trained on assorted lexical, syntactic and semantic features. Globally, using the YiSi metric to measure semantic similarity performs much"
K19-1020,D19-1632,0,0.059965,"Missing"
K19-1020,S13-2005,0,0.0608354,"Missing"
K19-1020,W18-2709,0,0.0298621,"r model features that aim at maximizing n-gram precision between the MT output and the English sentence. More details on this method can be found in Lo et al. (2016). 3.2 4 Experiment on Parallel Corpus Filtering Next, we evaluate YiSi on the task of Parallel Corpus Filtering (PCF). Quality – or “cleanliness” – of parallel training data for MT has been shown to affect MT quality at different degrees, and various characteristics of the data – parallelism of the sentence pairs and the grammaticality of targetlanguage data – impact MT systems in different ways (Goutte et al., 2012; Simard, 2014; Khayrallah and Koehn, 2018). Results The results of these experiments are presented in Table 2, where performance is measured in terms of Pearson’s correlation with the test sets’ gold standard annotations. For reference, we also include results obtained by the UWB system (Brychc´ın and Svoboda, 2016), which was the best performing system in the SemEval 2016 crosslingual STS shared task. The UWB system is an MT-based system with a STS system trained on assorted lexical, syntactic and semantic features. Globally, using the YiSi metric to measure semantic similarity performs much Here, we use data from the WMT19 shared ta"
K19-1020,P02-1040,0,0.109123,"used in training the bilingual word embeddings for evaluating crosslingual lexical semantic similarity in YiSi-2. hala, into English.7 Both corpora were crawled from the web, using ParaCrawl (Koehn et al., 2018a). Specifically, the task is to produce a score for each sentence pair in these noisy corpora, reflecting the quality of that pair. The scoring schemes are evaluated by extracting the topscoring sentence pairs from each corpus, then using them to train MT systems; these systems are run on test sets of Wikipedia articles (Guzm´an et al., 2019), and the results are evaluated using BLEU (Papineni et al., 2002). In addition to the noisy corpora, participants are allowed to use a few small sets of parallel data, covering different domains, for each of the two low-resource languages, as well as a third, related language, Hindi (which uses the same script as Nepali). The provided data also included much larger monolingual corpora for each of English, Hindi, Nepali and Sinhala. 4.1 WMT19 parallel corpus filtering system 1M-word 5M-word random 1.30 3.01 Zipporah 4.14 4.42 YiSi-2bivec 3.86 3.76 YiSi-2vecmap 4.00 3.76 YiSi-2bert 3.77 3.77 Table 4: Uncased BLEU scores on the official WMT19 PCF dev (“dev-tes"
K19-1020,W18-6453,0,0.115895,"imilarity can be broken. 2 Crosslingual STS metric v(u) = embedding of unit u s(e, f ) = cos(v(e), v(f )) Our crosslingual STS metric is based on YiSi (Lo, 2019). YiSi is a unified adequacy-oriented MT quality evaluation and estimation metric for languages with different levels of available resources. Lo et al. (2018) showed that YiSi-2, the crosslingual MT quality estimation metric, performed almost as well as the “MT + monolingual MT evaluation metric (YiSi-1)” pipeline for identifying parallel sentence pairs from a noisy web-crawled corpus in the Parallel Corpus Filtering task of WMT 2018 (Koehn et al., 2018b). To measure semantic similarity between pairs of segments, YiSi-2 proceeds by finding alignments between the words of these segments that maximize semantic similarity at the lexical level. For evaluating crosslingual lexical semantic similarity, it relies on a crosslingual embedding model, using cosine similarity of the embeddings from the crosslingual lexical representation model. Following the approach of Corley and Mihalcea (2005), these lexical semantic similarities are weighed by lexical specificity using inverse document frequency (IDF) collected from each side of the tested corpus. A"
K19-1020,P19-1493,0,0.0641239,"Missing"
K19-1020,J03-3002,0,0.258421,"Missing"
K19-1020,2014.amta-researchers.6,1,0.888004,"s of log-linear model features that aim at maximizing n-gram precision between the MT output and the English sentence. More details on this method can be found in Lo et al. (2016). 3.2 4 Experiment on Parallel Corpus Filtering Next, we evaluate YiSi on the task of Parallel Corpus Filtering (PCF). Quality – or “cleanliness” – of parallel training data for MT has been shown to affect MT quality at different degrees, and various characteristics of the data – parallelism of the sentence pairs and the grammaticality of targetlanguage data – impact MT systems in different ways (Goutte et al., 2012; Simard, 2014; Khayrallah and Koehn, 2018). Results The results of these experiments are presented in Table 2, where performance is measured in terms of Pearson’s correlation with the test sets’ gold standard annotations. For reference, we also include results obtained by the UWB system (Brychc´ın and Svoboda, 2016), which was the best performing system in the SemEval 2016 crosslingual STS shared task. The UWB system is an MT-based system with a STS system trained on assorted lexical, syntactic and semantic features. Globally, using the YiSi metric to measure semantic similarity performs much Here, we use"
K19-1020,W19-5358,1,0.824685,"from BERT without finetuning. In an intrinsic crosslingual STS evaluation and extrinsic parallel corpus filtering and human translation error detection tasks, we show that our BERT-based metric achieves performance on par with similar metrics based on supervised or weakly supervised approaches. With the availability of the multilingual context representation models, we show that the deadlock around parallel resources for crosslingual textual similarity can be broken. 2 Crosslingual STS metric v(u) = embedding of unit u s(e, f ) = cos(v(e), v(f )) Our crosslingual STS metric is based on YiSi (Lo, 2019). YiSi is a unified adequacy-oriented MT quality evaluation and estimation metric for languages with different levels of available resources. Lo et al. (2018) showed that YiSi-2, the crosslingual MT quality estimation metric, performed almost as well as the “MT + monolingual MT evaluation metric (YiSi-1)” pipeline for identifying parallel sentence pairs from a noisy web-crawled corpus in the Parallel Corpus Filtering task of WMT 2018 (Koehn et al., 2018b). To measure semantic similarity between pairs of segments, YiSi-2 proceeds by finding alignments between the words of these segments that ma"
K19-1020,W18-6451,0,0.0611751,"Missing"
K19-1020,S16-1102,1,0.901242,"Missing"
K19-1020,W16-2327,0,0.0190502,"filter out pairs which are not proper translations, possibly with some tolerance for pairs of segments that do share partial meaning. In TEED, the data is mostly expected to be high-quality translations; the task is then to identify those pairs that deviate from this norm, even on small details. extract the 1M-word and 5M-word samples from the original test corpora, using the scores of each of our systems in turn. We then trained MT systems using the extracted data: our MT systems are standard phrase-based SMT systems, with components and parameters similar to the GermanEnglish SMT system in Williams et al. (2016). 4.2 Results BLEU scores of the resulting MT systems are shown in Table 4. For comparison, we present the results of random scoring, as well as results obtained by the Zipporah PCF method (Xu and Koehn, 2017). Zipporah combines fluency and adequacy features to score sentence pairs; adequacy features are derived from existing parallel corpora, and the feature combination (logistic regression) is optimized on in-domain parallel data. Therefore, Zipporah can be seen as a fully supervised method. The Zipporah-based MT systems were trained similarly to other systems in the results reported here. A"
K19-1020,D17-1319,0,0.0366912,"k is then to identify those pairs that deviate from this norm, even on small details. extract the 1M-word and 5M-word samples from the original test corpora, using the scores of each of our systems in turn. We then trained MT systems using the extracted data: our MT systems are standard phrase-based SMT systems, with components and parameters similar to the GermanEnglish SMT system in Williams et al. (2016). 4.2 Results BLEU scores of the resulting MT systems are shown in Table 4. For comparison, we present the results of random scoring, as well as results obtained by the Zipporah PCF method (Xu and Koehn, 2017). Zipporah combines fluency and adequacy features to score sentence pairs; adequacy features are derived from existing parallel corpora, and the feature combination (logistic regression) is optimized on in-domain parallel data. Therefore, Zipporah can be seen as a fully supervised method. The Zipporah-based MT systems were trained similarly to other systems in the results reported here. All systems produced with YiSi-2 produce similar results. Interestingly, the MT systems produced with YiSi-2 in the 5M-word condition are not better than those of the 1M-word condition. This is possibly explain"
lo-wu-2010-evaluating,W07-0738,0,\N,Missing
lo-wu-2010-evaluating,W08-0332,0,\N,Missing
lo-wu-2010-evaluating,P02-1040,0,\N,Missing
lo-wu-2010-evaluating,D07-1007,1,\N,Missing
lo-wu-2010-evaluating,P07-1005,0,\N,Missing
lo-wu-2010-evaluating,J05-1004,0,\N,Missing
lo-wu-2010-evaluating,N09-2004,1,\N,Missing
lo-wu-2010-evaluating,2009.eamt-1.30,1,\N,Missing
lo-wu-2010-evaluating,N04-1030,0,\N,Missing
lo-wu-2014-reliability,lo-wu-2010-evaluating,1,\N,Missing
P11-1023,W05-0909,0,0.0836133,"Missing"
P11-1023,W07-0738,0,0.210377,"Missing"
P11-1023,W08-0332,0,0.180161,"Missing"
P11-1023,W06-3114,0,0.070746,"Missing"
P11-1023,E06-1031,0,0.0865674,"n judgment as HTER, at an even lower cost—and is still far better correlated than n-gram based evaluation metrics. 2 Related work Lexical similarity based metrics BLEU (Papineni et al., 2002) is the most widely used MT evaluation metric despite the fact that a number of large scale metaevaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where it strongly disagree with human judgment on translation accuracy. Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption—that a “good” translation is one that shares the same lexical choices as the reference translation—is not justified semantically. Lexical similarity does not adequately reflect similarity in meaning. State-of-the-art MT systems are often able to output translations containing roughly the correct words, yet expressing meaning that is not close to that of the input. We argue tha"
P11-1023,W05-0904,0,0.309261,"t checking whether the roles were appropriately attached to the correct predicate. Also, the actor, experiencer, and patient were all conflated into the undistinguished who role, while other crucial elements, like the action, purpose, manner, were ignored. Instead, we argue, evaluating meaning similarity should be done by evaluating the semantic structure as a whole: (a) all core semantic roles should be checked, and (b) not only should we evaluate the presence of semantic role fillers in isolation, but also their relations to the frames’ predicates. Syntax based metrics Unlike Voss and Tate, Liu and Gildea (2005) proposed a structural approach, but it was based on syntactic rather than semantic structure, and focused on checking the correctness of the role structure without checking the correctness of the role fillers. Their subtree metric (STM) and headword chain metric (HWC) address the failure of BLEU to evaluate translation grammaticality; however, the problem remains that a grammatical translation can achieve a high syntax-based score HTER (non-automatic) Despite the fact that Human- even if contains meaning errors arising from confusion of targeted Translation Edit Rate (HTER) as proposed by sem"
P11-1023,N07-1006,0,0.0205359,"updated translations from the same systems”. Instead, we aim for MT evaluation metrics that provide fine-grained scores in a way that also directly reflects interpretable insights on the strengths and weaknesses of MT systems rather than simply replicating human assessments. Semantic roles as features in aggregate metrics Gim´enez and M`arquez (2007, 2008) introduced ULC, an automatic MT evaluation metric that aggregates many types of features, including several shallow semantic similarity features: semantic role overlapping, semantic role matching, and semantic structure overlapping. Unlike Liu and Gildea (2007) who use discriminative training to tune the weight on each feature, ULC uses uniform weights. Although the metric shows an improved correlation with 3 MEANT: SRL for MT evaluation human judgment of translation quality (Callison-Burch et A good translation is one from which human readers al., 2007; Gim´enez and M`arquez, 2007; Callison-Burch may successfully understand at least the basic event strucet al., 2008; Gim´enez and M`arquez, 2008), it is not com- ture—“who did what to whom, when, where and why” monly used in large-scale MT evaluation campaigns, per- (Pradhan et al., 2004)—which repre"
P11-1023,lo-wu-2010-evaluating,1,0.906669,"Missing"
P11-1023,niessen-etal-2000-evaluation,0,0.187548,"lower cost—and is still far better correlated than n-gram based evaluation metrics. 2 Related work Lexical similarity based metrics BLEU (Papineni et al., 2002) is the most widely used MT evaluation metric despite the fact that a number of large scale metaevaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where it strongly disagree with human judgment on translation accuracy. Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption—that a “good” translation is one that shares the same lexical choices as the reference translation—is not justified semantically. Lexical similarity does not adequately reflect similarity in meaning. State-of-the-art MT systems are often able to output translations containing roughly the correct words, yet expressing meaning that is not close to that of the input. We argue that a translation metric that re"
P11-1023,P02-1040,0,0.121519,"Missing"
P11-1023,N04-1030,0,0.199132,"ing. Unlike Liu and Gildea (2007) who use discriminative training to tune the weight on each feature, ULC uses uniform weights. Although the metric shows an improved correlation with 3 MEANT: SRL for MT evaluation human judgment of translation quality (Callison-Burch et A good translation is one from which human readers al., 2007; Gim´enez and M`arquez, 2007; Callison-Burch may successfully understand at least the basic event strucet al., 2008; Gim´enez and M`arquez, 2008), it is not com- ture—“who did what to whom, when, where and why” monly used in large-scale MT evaluation campaigns, per- (Pradhan et al., 2004)—which represents the most essenhaps due to its high time cost and/or the difficulty of in- tial meaning of the source utterances. terpreting its score because of its highly complex combiMEANT measures this as follows. First, semantic role nation of many heterogenous types of features. labeling is performed (either manually or automatically) Specifically, note that the feature based representations on both the reference translation and the machine translaof semantic roles used in these aggregate metrics do not tion. The semantic frame structures thus obtained for the actually capture the struc"
P11-1023,2006.amta-papers.25,0,0.527447,"tructural approach, but it was based on syntactic rather than semantic structure, and focused on checking the correctness of the role structure without checking the correctness of the role fillers. Their subtree metric (STM) and headword chain metric (HWC) address the failure of BLEU to evaluate translation grammaticality; however, the problem remains that a grammatical translation can achieve a high syntax-based score HTER (non-automatic) Despite the fact that Human- even if contains meaning errors arising from confusion of targeted Translation Edit Rate (HTER) as proposed by semantic roles. Snover et al. (2006) shows a high correlation with human STM was the first proposed metric to incorporate synjudgment on translation adequacy, it is not widely used in tactic features in MT evaluation, and STM underlies most day-to-day machine translation evaluation because of its other recently proposed syntactic MT evaluation methigh labor cost. HTER not only requires human experts rics, for example the evaluation metric based on lexicalto understand the meaning expressed in both the refer- functional grammar of Owczarzak et al. (2008). STM is ence translation and the machine translation, but also re- a precisi"
P11-1023,2006.eamt-1.25,0,0.0177916,"eaning as the reference translation. Requiring such heavy manual decision making greatly increases the cost of evaluation, bottlenecking the evaluation cycle. To reduce the cost of evaluation, we aim to reduce any human decisions in the evaluation cycle to be as simple as possible, such that even untrained humans can quickly complete the evaluation. The human decisions should also be defined in a way that can be closely approximated by automatic methods, so that similar objective functions might potentially be used for tuning in MT system development cycles. Task based metrics (non-automatic) Voss and Tate (2006) proposed a task-based approach to MT evaluation that is in some ways similar in spirit to ours, but rather than evaluating how well people understand the meaning as a whole conveyed by a sentence translation, they measured the recall with which humans can extract one of the who, when, or where elements from MT output—and without attaching them to any predicate or frame. A large number of human subjects were instructed to extract only one particular type of wh-item from each sentence. They evaluated only whether the role fillers were correctly identified, without checking whether the roles wer"
P11-1023,E06-1032,0,\N,Missing
P11-1023,W07-0718,0,\N,Missing
P11-1023,W10-1703,0,\N,Missing
P11-1023,W08-0309,0,\N,Missing
P11-1023,W10-3807,1,\N,Missing
P13-2067,2006.iwslt-evaluation.11,0,0.0718925,"c metrics below to justify our choice. Utilizing semantics in SMT In the past few years, there has been a surge of work aimed at incorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorpo"
P13-2067,W05-0909,0,0.275982,"MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to semantic objectives. MT evaluation metrics As mentioned previously, tuning against n-gram based metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) does not sufficiently drive SMT into making decisions to produce adequate translations that correctly preserve ”who did what to whom, In contrast to TINE, MEANT (Lo et al., 2012), which is the weighted f-score over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER. This makes it more suitable for tuning SMT systems to produce much adequate translations. Although TINE (Rios et al., 2011) is an recalloriented automatic evaluation metric which aims to preserve the basic event structure, no work has be"
P13-2067,E06-1032,0,0.770131,"ein automatic semantic parsing might be expected to fare worse. These results strongly indicate that using a semantic frame based objective function for tuning would drive development of MT towards direction of higher utility. Glaring errors caused by semantic role confusion that plague the state-of-the-art MT systems are a consequence of using fast and cheap lexical n-gram based objective functions like BLEU to drive their development. Despite enforcing fluency it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). We argue that instead of BLEU, a metric that focuses on getting the meaning right should be used as an objective function for tuning SMT so as to drive continuing progress towards higher utility. MEANT (Lo et al., 2012), is an automatic semantic MT evaluation metric that measures similarity between the MT output and the reference translation via semantic frames. It correlates better with human adequacy judgment than other automatic MT evaluation metrics. Since a high MEANT score is contingent on correct lexical choices as well as syntactic and semantic structures, we b"
P13-2067,E06-1031,0,0.691061,"of our system with that of two baseline SMT systems tuned against BLEU and TER, the commonly used n-gram and edit distance 375 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375–381, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Related Work when, where and why”. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation accuracy. Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. Relatively little work has been done towards biasing the translation decisions of an SMT system to produce adequate translations that correctly preserve who did what to whom, when, where and why (Pradhan et al., 2004). This is because the development of SMT systems was predominantly driven by tuning against n-gram based evaluation metrics such as BLEU or edit distance based metrics such as TER which do not sufficiently bias SM"
P13-2067,W07-0718,0,0.470422,"Missing"
P13-2067,C10-1081,0,0.0750956,"ncorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Bu"
P13-2067,W08-0309,0,0.411594,"Missing"
P13-2067,niessen-etal-2000-evaluation,0,0.790958,"two baseline SMT systems tuned against BLEU and TER, the commonly used n-gram and edit distance 375 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375–381, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Related Work when, where and why”. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation accuracy. Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. Relatively little work has been done towards biasing the translation decisions of an SMT system to produce adequate translations that correctly preserve who did what to whom, when, where and why (Pradhan et al., 2004). This is because the development of SMT systems was predominantly driven by tuning against n-gram based evaluation metrics such as BLEU or edit distance based metrics such as TER which do not sufficiently bias SMT system’s decisions to pro"
P13-2067,P02-1040,0,0.107244,"àrquez, 2008) but no work has been done towards tuning an MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to semantic objectives. MT evaluation metrics As mentioned previously, tuning against n-gram based metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005) does not sufficiently drive SMT into making decisions to produce adequate translations that correctly preserve ”who did what to whom, In contrast to TINE, MEANT (Lo et al., 2012), which is the weighted f-score over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER. This makes it more suitable for tuning SMT systems to produce much adequate translations. Although TINE (Rios et al., 2011) is an recalloriented automatic evaluation metric whic"
P13-2067,W07-0738,0,0.382895,"he model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations"
P13-2067,W08-0332,0,0.421386,"ystem. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system against ULC perhaps due to its expensive running time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features and is based on pure lexical features. Although tuning on QUEEN produced slightly more preferable translations than solely tuning on BLEU, the metric does not make use of any semantic features and thus fails to exploit any potential gains from tuning to semantic objectives. MT evaluation metrics As mentioned previously, tuning against n-gram based metrics such as BLEU (Papineni et a"
P13-2067,2006.amta-papers.25,0,0.252688,"against BLEU and TER, the commonly used n-gram and edit distance 375 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 375–381, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics 2 Related Work when, where and why”. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation accuracy. Tuning against edit distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) also fails to sufficiently bias SMT systems towards producing translations that preserve semantic information. Relatively little work has been done towards biasing the translation decisions of an SMT system to produce adequate translations that correctly preserve who did what to whom, when, where and why (Pradhan et al., 2004). This is because the development of SMT systems was predominantly driven by tuning against n-gram based evaluation metrics such as BLEU or edit distance based metrics such as TER which do not sufficiently bias SMT system’s decisions to produce adequate translations. Alt"
P13-2067,N09-2004,1,0.891593,"ranslations. We argue that an SMT system tuned against an adequacy-oriented metric that correlates well with human adequacy judgement produces more adequate translations. For this purpose, we choose MEANT, an automatic semantic MT evaluation metric that focuses on getting the meaning right by comparing the semantic structures of the MT output and the reference. We briefly describe some of the alternative semantic metrics below to justify our choice. Utilizing semantics in SMT In the past few years, there has been a surge of work aimed at incorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train"
P13-2067,I11-1004,0,0.0218854,"our choice. Utilizing semantics in SMT In the past few years, there has been a surge of work aimed at incorporating semantics into various stages of the SMT. Wu and Fung (2009) propose a two-pass model that reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semanti"
P13-2067,P12-1095,0,0.0228147,"t reorders the MT output to match the SRL of the input, which is too late to affect the translation decisions made by the MT system during decoding. In contrast, training against a semantic objective function attempts to improve the decoding search strategy by incorporating a bias towards meaningful translations into the model instead of postprocessing its results. Komachi et al. (2006) and Wu et al. (2011) preprocess the input sentence to match the verb frame alternations in the output side. Liu and Gildea (2010) and Aziz et al. (2011) use input side SRL to train a tree-to-string SMT system. Xiong et al. (2012) trained a discriminative model to predict the position of the semantic roles in the output. All these approaches are orthogonal to the present question of whether to train toward a semantic objective function. Any of the above models could potentially benefit from tuning with semantic metrics. ULC (Giménez and Màrquez, 2007, 2008) is an aggregated metric that incorporates several semantic similarity features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008)"
P13-2067,W06-3114,0,\N,Missing
P13-2067,W11-2112,0,\N,Missing
P13-2067,W12-3129,1,\N,Missing
P13-2067,W11-2136,0,\N,Missing
P13-2067,N04-1030,0,\N,Missing
P14-2124,2011.mtsummit-papers.52,0,0.078029,"Missing"
P14-2124,W12-3108,0,0.0431379,"an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that"
P14-2124,W06-3114,0,0.0405831,"T assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it pe"
P14-2124,W05-0909,0,0.0845738,"s), pages 765–771, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency s"
P14-2124,E06-1031,0,0.689315,"ces and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. tems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human"
P14-2124,W05-0904,0,0.0689774,"R (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive"
P14-2124,C04-1046,0,0.0699212,"equate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et"
P14-2124,P11-1023,1,0.950242,"echnology Center Department of Computer Science and Engineering Hong Kong University of Science and Technology {jackielo|mbeloucif|masaers|dekai}@cs.ust.hk Abstract than that of the reference translation, and on the other hand, the BITG constraints the word alignment more accurately than the heuristic bag-ofword aggregation used in MEANT. Our results suggest that MT translation adequacy is more accurately evaluated via the cross-lingual semantic frame similarities of the input and the MT output which may obviate the need for expensive human reference translations. The MEANT family of metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012) adopt the principle that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metri"
P14-2124,E06-1032,0,0.0512033,"ing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic ev"
P14-2124,W07-0718,0,0.0464491,"fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: M"
P14-2124,W12-4206,1,0.863115,"the MT out766 Figure 2: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic shallow semantic parser. The reference and MT output are parsed by an English automatic shallow semantic parser. There are no semantic frames for MT3 since the system decided to drop the predicate. put. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. tems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two diff"
P14-2124,W08-0309,0,0.0729217,"Missing"
P14-2124,2013.mtsummit-papers.12,1,0.927794,"that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of in"
P14-2124,W13-2254,1,0.745244,"that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of in"
P14-2124,W12-3103,0,0.0176841,"event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: Monolingual MEANT algorithm. or paraphrase tables; the expensive training, tuning, and/or running time makes them hard to incorporate into the MT development cycle. 2.2 The MEANT family of metrics MEANT (Lo et al., 2012), which is the weighted fscore over the matched semantic role labels of the automatically aligned semantic"
P14-2124,W12-3129,1,0.91538,"judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurat"
P14-2124,2013.iwslt-evaluation.5,1,0.868047,"anslations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of input and output sentences. We therefore propose XMEANT, a cross-lingual MT evaluation metric, that modifies MEANT using (1) simple translation probabilities (in our experiments, We introduce XMEANT—a new cross-lingual version of the semantic frame based MT evaluation metric MEANT—which can correl"
P14-2124,W13-2202,0,0.0143653,"one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is improved by replacing BLEU or TER with MEANT during parameter tuning (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In order to continue driving MT towards better translation adequacy by deeply integrating semantic frame criteria into the MT training pipeline, it is necessary to have a cross-lingual semantic objective function that assesses the semantic frame similarities of input and output sentences. We th"
P14-2124,W12-3122,0,0.0699178,"judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurat"
P14-2124,niessen-etal-2000-evaluation,0,0.245663,"14. 2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005),"
P14-2124,W07-0411,0,0.159197,"Missing"
P14-2124,P02-1040,0,0.103731,"he Association for Computational Linguistics (Short Papers), pages 765–771, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG"
P14-2124,N04-1030,0,0.0990837,"nt more accurately than the heuristic bag-ofword aggregation used in MEANT. Our results suggest that MT translation adequacy is more accurately evaluated via the cross-lingual semantic frame similarities of the input and the MT output which may obviate the need for expensive human reference translations. The MEANT family of metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012) adopt the principle that a good translation is one where a human can successfully understand the central meaning of the foreign sentence as captured by the basic event structure: “who did what to whom, when, where and why” (Pradhan et al., 2004). MEANT measures similarity between the MT output and the reference translations by comparing the similarities between the semantic frame structures of output and reference translations. It is well established that the MEANT family of metrics correlates better with human adequacy judgments than commonly used MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Mach´acˇ ek and Bojar, 2013). In addition, the translation adequacy across different genres (ranging from formal news to informal web forum and public speech) and different languages (English and Chinese) is"
P14-2124,quirk-2004-training,0,0.0654216,"two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the pr"
P14-2124,W11-2112,0,0.0746507,"ce. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the referen"
P14-2124,W09-2304,1,0.953515,"ully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the 3.1 Applying MEANT’s f-score within semantic role fillers The first natural approach is to extend MEANT’s f-score based method of aggregating semantic parse accuracy, so as to also apply to aggregat768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G R p ([AA] |A) ≡ ≡ = p (e/f |A) = si,pred = si,j = Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric HMEANT XMEANT (BITG) MEANT (f-score) XMEANT (f-score) MEANT (2013) NIST BLEU/METEOR/TER/PER CDER WER ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p (⟨AA⟩|A) = 0.25 1√ t (e|f ) t (f |e) 2 1 ( ( )) ∗ ln P A⇒ei,pred /fi,pred |G 1− max(|ei,pred |,|fi,pred |) 5 Conclusion 1 1− ( ( )) ∗ ln P A⇒ei,j /fi,j |G We have presented XMEANT, a new cross-lingual variant of MEANT, that correlates even more closely with human translation adequacy judgm"
P14-2124,W09-3804,1,0.872735,"dy demonstrate XMEANT’s potential to drive research progress toward semantic SMT. max(|ei,j |,|fi,j |) where G is a bracketing ITG, whose only nonterminal is A, and where R is a set of transduction rules where e ∈ W 0 ∪ {ϵ} is an output token (or the null token), and f ∈ W 1 ∪ {ϵ} is an input token (or the null token). The rule probability function p is defined using fixed probabilities for the structural rules, and a translation table t trained using IBM model 1 in both directions. To calculate (the inside probability of a pair of seg) ∗ ments, P A ⇒ e/f|G , we use the algorithm described in Saers et al. (2009). si,pred and si,j are the length normalized BITG parsing probabilities of the predicates and role fillers of the arguments of type j between the input and the MT output. 4 Kendall 0.53 0.51 0.48 0.46 0.46 0.29 0.20 0.12 0.10 Results Table 1 shows that for human adequacy judgments at the sentence level, the f-score based XMEANT (1) correlates significantly more closely than other commonly used monolingual automatic MT evaluation metrics, and (2) even correlates nearly as well as monolingual MEANT. This suggests that the semantic structure of the MT output is indeed closer to that of the input"
P14-2124,2013.mtsummit-papers.21,0,0.0112627,"m of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more rec"
P14-2124,2006.amta-papers.25,0,0.117577,"ational Linguistics from quick IBM-1 training), to replace the monolingual context vector model in MEANT, and (2) constraints from BITGs (bracketing ITGs). We show that XMEANT assesses MT adequacy more accurately than MEANT (as measured by correlation with human adequacy judgement) without the need for expensive human reference translations in the output language. 2 Related Work 2.1 MT evaluation metrics Surface-form oriented metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. In fact, a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where BLEU strongly disagrees with human judgments of translation adequacy. This has caused a recent surge of work to develop better ways to automatically measure MT adequacy. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher corr"
P14-2124,2011.eamt-1.12,0,0.024364,"E system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et"
P14-2124,Y12-1062,1,0.912362,"judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we employ a newer version of MEANT that uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers, as our experiments indicate this is more accurat"
P14-2124,H05-1096,0,0.0325066,"s UMEANT is useful when human judgments on adequacy of the development set are unavailable. tems against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. 2.3 MT quality estimation Evaluating cross-lingual MT quality is similar to the work of MT quality estimation (QE). Broadly speaking, there are two different approaches to QE: surface-based and feature-based. Token-based QE models, such as those in Gandrabur et al. (2006) and Ueffing and Ney (2005) fail to assess the overall MT quality because translation goodness is not a compositional property. In contrast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small anno"
P14-2124,W12-3107,0,0.0199386,"correlation with human adequacy judgments than metrics like METEOR. TINE (Rios et al., 2011) is a recall-oriented metric which aims to preserve the basic event structure but it performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgments. ULC (Gim´enez and M`arquez, 2007, 2008) incorporates several semantic features and shows improved correlation with human judgement on translation quality (Callison-Burch et al., 2007, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Similarly, SPEDE (Wang and Manning, 2012) predicts the edit sequence for matching the MT output to the reference via an integrated probabilistic FSM and PDA model. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain several dozens of parameters to tune, and employ expensive linguistic resources like WordNet Figure 1: Monolingual MEANT algorithm. or paraphrase tables; the expensive training, tuning, and/or running time makes them hard to incorporate into the MT development cycle. 2.2"
P14-2124,J97-3002,1,0.352751,"that MEANT is based upon is fully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the 3.1 Applying MEANT’s f-score within semantic role fillers The first natural approach is to extend MEANT’s f-score based method of aggregating semantic parse accuracy, so as to also apply to aggregat768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G R p ([AA] |A) ≡ ≡ = p (e/f |A) = si,pred = si,j = Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric HMEANT XMEANT (BITG) MEANT (f-score) XMEANT (f-score) MEANT (2013) NIST BLEU/METEOR/TER/PER CDER WER ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p (⟨AA⟩|A) = 0.25 1√ t (e|f ) t (f |e) 2 1 ( ( )) ∗ ln P A⇒ei,pred /fi,pred |G 1− max(|ei,pred |,|fi,pred |) 5 Conclusion 1 1− ( ( )) ∗ ln P A⇒ei,j /fi,j |G We have presented XMEANT, a new cross-lingual variant of MEANT, that correlates even more clos"
P14-2124,P10-1062,0,0.0203401,"rast, Blatz et al. (2004) introduced a sentencelevel QE system where an arbitrary threshold is used to classify the MT output as good or bad. The fundamental problem of this approach is that it defines QE as a binary classification task rather than attempting to measure the degree of goodness of the MT output. To address this problem, Quirk (2004) related the sentence-level correctness of the QE model to human judgment and achieved a high correlation with human judgement for a small annotated corpus; however, the proposed model does not scale well to larger data sets. Feature-based QE models (Xiong et al., 2010; He et al., 2011; Ma et al., 2011; Specia, 2011; Avramidis, 2012; Mehdad et al., 2012; Almaghout and Specia, 2013; Avramidis and Popovi´c, 2013; Shah et al., 2013) throw a wide range of linguistic and non-linguistic features into machine learnsi,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of"
P14-2124,P03-1019,0,0.0342442,"T is based upon is fully covered within ITG constraints. In Wu et al. (2014), we extend ITG constraints into aligning the tokens within the semantic role fillers within monolingual MEANT, thus replacing its previous monolingual phrasal aggregation heuristic. Here we borrow the 3.1 Applying MEANT’s f-score within semantic role fillers The first natural approach is to extend MEANT’s f-score based method of aggregating semantic parse accuracy, so as to also apply to aggregat768 idea for the cross-lingual case, using the lengthnormalized inside probability at the root of a BITG biparse (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009) as follows: G R p ([AA] |A) ≡ ≡ = p (e/f |A) = si,pred = si,j = Table 1: Sentence-level correlation with HAJ (GALE phase 2.5 evaluation data) Metric HMEANT XMEANT (BITG) MEANT (f-score) XMEANT (f-score) MEANT (2013) NIST BLEU/METEOR/TER/PER CDER WER ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p (⟨AA⟩|A) = 0.25 1√ t (e|f ) t (f |e) 2 1 ( ( )) ∗ ln P A⇒ei,pred /fi,pred |G 1− max(|ei,pred |,|fi,pred |) 5 Conclusion 1 1− ( ( )) ∗ ln P A⇒ei,j /fi,j |G We have presented XMEANT, a new cross-lingual variant of MEANT, that correlates even more closely with human trans"
P14-2124,J93-2003,0,\N,Missing
P14-2124,H93-1040,0,\N,Missing
P14-2124,1993.mtsummit-1.24,0,\N,Missing
P14-2124,W07-0738,0,\N,Missing
P14-2124,W08-0332,0,\N,Missing
P14-2124,P11-1124,0,\N,Missing
P14-2124,P13-2067,1,\N,Missing
P14-2124,2012.eamt-1.64,1,\N,Missing
S16-1102,W09-1206,0,0.0837081,"Missing"
S16-1102,N12-1047,0,0.0449274,"feature functions are used in the log-linear model: three 5-gram language models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely with regard to its own training data; that translation may be arbitrarily distant from the English sentence to which it will be compared in the STS task. These arbitrary surface differences may complicate the task of measuring semantic similarity. To alleviate this problem, we bias the MT system to produce a translation that is as close as possible on the surface to the English sentence. This is done b"
S16-1102,W07-0717,0,0.0431433,"(NC) – totaling approximately 110M words in each language. Phrase extraction was done by aligning the corpora at the word level using HMM, IBM2 and IBM4 models, using the union of phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The following feature functions are used in the log-linear model: three 5-gram language models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely"
S16-1102,P07-1019,0,0.0734543,"age models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely with regard to its own training data; that translation may be arbitrarily distant from the English sentence to which it will be compared in the STS task. These arbitrary surface differences may complicate the task of measuring semantic similarity. To alleviate this problem, we bias the MT system to produce a translation that is as close as possible on the surface to the English sentence. This is done by means of loglinear model features that aim at maximizing n-gram prec"
S16-1102,P14-2124,1,0.849566,"arget English word using a word embeddings model. In our experiments, we used pretrained word2vec (Mikolov et al., 2013) embeddings.2 The resulting crosslingual lexical similarity of the targeted pair of Spanish and English words is the highest similarity between the 5 mapped words and the target English word. We then reconstruct the semantic phrasal similarity by averaging the Englishidf-weighted crosslingual embeddings mapped lexical similarity according to the 1-1 maximal matching alignment of the lexicons in the two phrases. In addition to the flat lexical semantic feature, we use XMEANT (Lo et al., 2014), the crosslingual semantic frame based machine translation evaluation metric, for generating shallow structural semantic features. We use MATE (Bj¨orkelund et al., 2009) for Spanish shallow semantic parsing and SENNA (Collobert et al., 2011) for English shallow semantic parsing. In evaluating machine translation quality, the confusion of semantic roles is a major source of errors due to reordering. However, in evaluating STS, confusion of semantic roles is less frequent while missing information in one of the test fragments is more frequent. This motivates a further simplification of the 12 s"
S16-1102,W15-3056,1,0.877844,"Missing"
S16-1102,N04-4026,0,0.0512051,"Missing"
S16-1102,N04-1033,0,0.0665645,"phrases extracted from these separate alignments for the phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. The following feature functions are used in the log-linear model: three 5-gram language models with KneserNey smoothing (Kneser and Ney, 1995), i.e. one for each of Europarl, CC and NC data, combined linearly (Foster and Kuhn, 2007) to best fit NC data; lexical estimates of the forward and backward translation probabilities obtained either by relative frequencies or using the method of (Zens and Ney, 2004); lexicalized distortion (Tillmann, 2004; Koehn et al., 2005); and word count. The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. For any given input in Spanish, the SMT system produces the translation that is most likely with regard to its own training data; that translation may be arbitrarily distant from the English sentence to which it will be compared in the STS task. These arbitrary surface"
S16-1102,2005.iwslt-1.8,0,\N,Missing
W10-3807,E06-1032,0,\N,Missing
W10-3807,brants-2000-inter,0,\N,Missing
W10-3807,W07-0738,0,\N,Missing
W10-3807,P02-1040,0,\N,Missing
W10-3807,D07-1007,1,\N,Missing
W10-3807,P07-1005,0,\N,Missing
W10-3807,W07-0718,0,\N,Missing
W10-3807,W06-3114,0,\N,Missing
W10-3807,P01-1017,0,\N,Missing
W10-3807,J05-1004,0,\N,Missing
W10-3807,W05-0904,0,\N,Missing
W10-3807,W08-0309,0,\N,Missing
W10-3807,N09-2004,1,\N,Missing
W10-3807,2009.eamt-1.30,1,\N,Missing
W10-3807,N04-1030,0,\N,Missing
W11-1002,W05-0909,0,0.315702,"eflects the assumption that a semantic frame that covers more tokens contributes more to the overall sentence translation. We demonstrate empirically that when the degree of each frame’s contribution to its sentence is taken into account, the properly structured role representation is more accurate and intuitive than the flattened role representation for SRL MT evaluation metrics. For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000). These metrics are excellent at ranking overall systems by averaging their scores over entire documents. However, as MT systems improve, the shortcomings of such metrics are becoming more apparent. Though containing roughly the correct words, MT output at the sentence remains often quite incomprehensible, and fails to preserve the meaning of the input. This results from the fact that n-gram based metrics are not as reliable at ranking the adequacy of translations of individual sentences, and are particularl"
W11-1002,W07-0738,0,0.41367,"Missing"
W11-1002,W08-0332,0,0.188443,"Missing"
W11-1002,W06-3114,0,0.139861,"the meaning of the input. This results from the fact that n-gram based metrics are not as reliable at ranking the adequacy of translations of individual sentences, and are particularly 10 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 10–20, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics poor at reflecting translation quality improvements involving more meaningful word sense or semantic frame decisions—which human judges have no trouble distinguishing. Callison-Burch et al. (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. Newer avenues of research seek substitutes for n-gram based MT evaluation metrics that are better at evaluating translation adequacy, particularly at the sentence level. One line of research emphasizes more the structural correctness of translation. Liu and Gildea (2005) propose STM, a metric based on syntactic structure, that addresses the failure of lexical similarity based metrics to evaluate translation grammaticality. However, the problem remains that a grammatical translation can ach"
W11-1002,E06-1031,0,0.290148,"o preserve the meaning of the input. This results from the fact that n-gram based metrics are not as reliable at ranking the adequacy of translations of individual sentences, and are particularly 10 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 10–20, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics poor at reflecting translation quality improvements involving more meaningful word sense or semantic frame decisions—which human judges have no trouble distinguishing. Callison-Burch et al. (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. Newer avenues of research seek substitutes for n-gram based MT evaluation metrics that are better at evaluating translation adequacy, particularly at the sentence level. One line of research emphasizes more the structural correctness of translation. Liu and Gildea (2005) propose STM, a metric based on syntactic structure, that addresses the failure of lexical similarity based metrics to evaluate translation grammaticality. However, the problem remains that a gramm"
W11-1002,W05-0904,0,0.0789477,"utational Linguistics poor at reflecting translation quality improvements involving more meaningful word sense or semantic frame decisions—which human judges have no trouble distinguishing. Callison-Burch et al. (2006) and Koehn and Monz (2006), for example, study situations where BLEU strongly disagrees with human judgment of translation quality. Newer avenues of research seek substitutes for n-gram based MT evaluation metrics that are better at evaluating translation adequacy, particularly at the sentence level. One line of research emphasizes more the structural correctness of translation. Liu and Gildea (2005) propose STM, a metric based on syntactic structure, that addresses the failure of lexical similarity based metrics to evaluate translation grammaticality. However, the problem remains that a grammatical translation can achieve a high syntaxbased score yet still make significant errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluated metrics, such as HTER (Snover et al., 2006), are more adequacy oriented exhibit much higher correlation with human adequacy judgment, their high labor cost prohibits widespread use. There has also"
W11-1002,lo-wu-2010-evaluating,1,0.895941,"erall sentence. The new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame’s contribution gives HMEANT higher correlations than the previously bestperforming flattened model, as well as HTER. 1 Introduction In this paper we provide a more concrete answer to the question: what would be a better representation, structured or flat, of the roles in semantic frames to be used in a semantic machine translation (MT) evaluation metric? We compare recent studies on the MEANT family of semantic role labeling (SRL) based MT evaluation metrics (Lo and Wu, 2010a,b, 2011a,b) by (1) contrasting their variations in semantic role representation and observing disturbing comparative results indicating that segregating the event frames in structured role representation actually damages correlation against human adequacy judgments and (2) showing how SRL based MT evaluation can be improved beyond the current state-of-the-art compared to previous MEANT variants as well as HTER, through the introduction of a simple weighting scheme that reflects the degree of contribution of each semantic frame to the overall sentence. The weighting scheme we propose uses a s"
W11-1002,W10-3807,1,0.840107,"erall sentence. The new results also show that, without flattening the structure of semantic frames, weighting the degree of each frame’s contribution gives HMEANT higher correlations than the previously bestperforming flattened model, as well as HTER. 1 Introduction In this paper we provide a more concrete answer to the question: what would be a better representation, structured or flat, of the roles in semantic frames to be used in a semantic machine translation (MT) evaluation metric? We compare recent studies on the MEANT family of semantic role labeling (SRL) based MT evaluation metrics (Lo and Wu, 2010a,b, 2011a,b) by (1) contrasting their variations in semantic role representation and observing disturbing comparative results indicating that segregating the event frames in structured role representation actually damages correlation against human adequacy judgments and (2) showing how SRL based MT evaluation can be improved beyond the current state-of-the-art compared to previous MEANT variants as well as HTER, through the introduction of a simple weighting scheme that reflects the degree of contribution of each semantic frame to the overall sentence. The weighting scheme we propose uses a s"
W11-1002,P11-1023,1,0.445504,"asic event structure—“who did what to whom, when, where and why” (Pradhan et al., 2004)–which represents the most essential meaning of the source utterances. Adopting this principle, the MEANT family of metrics compare the semantic frames in reference translations against those that can be reconstructed from machine translation output. Preliminary results reported in (Lo and Wu, 2010b) confirm that the blueprint model outperforms BLEU and similar n-gram oriented evalu11 ation metrics in correlation against human adequacy judgments, but does not fare as well as HTER. The more complete study of Lo and Wu (2011a) introduces MEANT and its human variants HMEANT, which implement an extended version of blueprint methodology. Experimental results show that HMEANT correlates against human adequacy judgments as well as the more expensive HTER, even though HMEANT can be evaluated using lowcost untrained monolingual semantic role annotators while still maintaining high inter-annotator agreement (both are far superior to BLEU or other surface oriented evaluation metrics). The study also shows that replacing the human semantic role labelers with an automatic shallow semantic parser yields an approximation that"
W11-1002,niessen-etal-2000-evaluation,0,0.269024,"overall sentence translation. We demonstrate empirically that when the degree of each frame’s contribution to its sentence is taken into account, the properly structured role representation is more accurate and intuitive than the flattened role representation for SRL MT evaluation metrics. For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000). These metrics are excellent at ranking overall systems by averaging their scores over entire documents. However, as MT systems improve, the shortcomings of such metrics are becoming more apparent. Though containing roughly the correct words, MT output at the sentence remains often quite incomprehensible, and fails to preserve the meaning of the input. This results from the fact that n-gram based metrics are not as reliable at ranking the adequacy of translations of individual sentences, and are particularly 10 Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statis"
W11-1002,P09-1034,0,0.0285111,"problem remains that a grammatical translation can achieve a high syntaxbased score yet still make significant errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluated metrics, such as HTER (Snover et al., 2006), are more adequacy oriented exhibit much higher correlation with human adequacy judgment, their high labor cost prohibits widespread use. There has also been work on explicitly evaluating MT adequacy by aggregating over a very large set of linguistic features (Gim´enez and M`arquez, 2007, 2008) and textual entailment (Pado et al., 2009). 2 SRL based MT evaluation metrics A blueprint for more direct assessment of meaning preservation across translation was outlined by Lo and Wu (2010a), in which translation utility is manually evaluated with respect to the accuracy of semantic role labels. A good translation is one from which human readers may successfully understand at least the basic event structure—“who did what to whom, when, where and why” (Pradhan et al., 2004)–which represents the most essential meaning of the source utterances. Adopting this principle, the MEANT family of metrics compare the semantic frames in referen"
W11-1002,J05-1004,0,0.0603586,"precision/recall. The sentence precision/recall is the weighted sum of the number of correctly translated roles in all frames normalized by the weighted sum of the total number of 13 3 Experimental setup The evaluation data for our experiments consists of 40 sentences randomly drawn from the DARPA GALE program Phase 2.5 newswire evaluation corpus containing Chinese input sentence, English reference translations, and the machine translation from three different state-of-the-art GALE systems. The Chinese and the English reference translation have both been annotated with gold standard PropBank (Palmer et al., 2005) semantic role labels. The weights wpred , wcore , wadj , wj and wpartial can be estimated by optimizing correlation against human adequacy judgments, using any of the many standard optimization search techniques. In the work of Lo and Figure 3: The flat role representation for the MEANT family of metrics as proposed in Lo and Wu (2011b) . Wu (2011b), the correlations of all individual roles with the human adequacy judgments were found to be non-negative, therefore we found grid search to be quite adequate for estimating the weights. We use linear weighting because we would like to keep the me"
W11-1002,P02-1040,0,0.0813186,"me we propose uses a simple length-based heuristic that reflects the assumption that a semantic frame that covers more tokens contributes more to the overall sentence translation. We demonstrate empirically that when the degree of each frame’s contribution to its sentence is taken into account, the properly structured role representation is more accurate and intuitive than the flattened role representation for SRL MT evaluation metrics. For years, the task of measuring the performance of MT systems has been dominated by lexical ngram based machine translation evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000). These metrics are excellent at ranking overall systems by averaging their scores over entire documents. However, as MT systems improve, the shortcomings of such metrics are becoming more apparent. Though containing roughly the correct words, MT output at the sentence remains often quite incomprehensible, and fails to preserve the meaning of the input. This results from the fact that n-gram based metrics are not as reliable at ranking the adequacy"
W11-1002,2006.amta-papers.25,0,0.132086,"at evaluating translation adequacy, particularly at the sentence level. One line of research emphasizes more the structural correctness of translation. Liu and Gildea (2005) propose STM, a metric based on syntactic structure, that addresses the failure of lexical similarity based metrics to evaluate translation grammaticality. However, the problem remains that a grammatical translation can achieve a high syntaxbased score yet still make significant errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluated metrics, such as HTER (Snover et al., 2006), are more adequacy oriented exhibit much higher correlation with human adequacy judgment, their high labor cost prohibits widespread use. There has also been work on explicitly evaluating MT adequacy by aggregating over a very large set of linguistic features (Gim´enez and M`arquez, 2007, 2008) and textual entailment (Pado et al., 2009). 2 SRL based MT evaluation metrics A blueprint for more direct assessment of meaning preservation across translation was outlined by Lo and Wu (2010a), in which translation utility is manually evaluated with respect to the accuracy of semantic role labels. A g"
W11-1002,E06-1032,0,\N,Missing
W11-1002,W10-1703,0,\N,Missing
W11-1002,N04-1030,0,\N,Missing
W12-3129,W05-0909,0,0.0711719,"achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR, PER, CDER, WER, or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANT’s virtues of simplicity, representational transparency, and inexpensiveness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly i"
W12-3129,E06-1032,0,0.184934,"and MT output. 2. Human judges align the semantic frames between the references and MT output by judging the correctness of the predicates. 3. For each pair of aligned semantic frames, (a) Human judges determine the translation correctness of the semantic role fillers. (b) Human judges align the semantic role fillers between the reference and MT output according to the correctness of the semantic role fillers. Automatic lexical similarity based metrics BLEU (Papineni et al., 2002) remains the most widely used MT evaluation metric despite the fact that a number of large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) report cases where it strongly disagrees with human judgments of translation accuracy. Other lexical similarity based automatic MT evaluation metrics, like NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006), also perform well in capturing translation fluency, but share the same problem that although evaluation with these metrics can be done very quickly at low cost, their underlying assumption that a good translation is one that shares the same lexi"
W12-3129,W07-0718,0,0.414855,"nths SK - 2 products resume in the mainland of China to stop selling nearly two months of SK - 2 products sales So far — — incorrect the ULC representation is based on flat semantic role label features that do not capture the structural relations in semantic frames, i.e., the predicate-argument relations. Also unlike HMEANT, which weights each semantic role type according to its empirically determined relative importance to the adequate preservation of meaning, ULC uses uniform weights. Although the automatic ULC metric shows an improved correlation with human judgment of translation quality (Callison-Burch et al., 2007; Gim´enez and M`arquez, 2007; Callison-Burch et al., 2008; Gim´enez and M`arquez, 2008), it is not commonly used in large-scale MT evaluation campaigns, perhaps due to its high time cost and/or the difficulty of interpreting its score because of its highly complex combination of many heterogeneous types of features. Like system combination approaches, ULC is a vastly more complex aggregate metric compared to widely used metrics like BLEU. We believe it is important for automatic semantic MT evaluation metrics to provide representational transparency via simple, clear, and transparent scoring"
W12-3129,W08-0309,0,0.0304526,"op selling nearly two months of SK - 2 products sales So far — — incorrect the ULC representation is based on flat semantic role label features that do not capture the structural relations in semantic frames, i.e., the predicate-argument relations. Also unlike HMEANT, which weights each semantic role type according to its empirically determined relative importance to the adequate preservation of meaning, ULC uses uniform weights. Although the automatic ULC metric shows an improved correlation with human judgment of translation quality (Callison-Burch et al., 2007; Gim´enez and M`arquez, 2007; Callison-Burch et al., 2008; Gim´enez and M`arquez, 2008), it is not commonly used in large-scale MT evaluation campaigns, perhaps due to its high time cost and/or the difficulty of interpreting its score because of its highly complex combination of many heterogeneous types of features. Like system combination approaches, ULC is a vastly more complex aggregate metric compared to widely used metrics like BLEU. We believe it is important for automatic semantic MT evaluation metrics to provide representational transparency via simple, clear, and transparent scoring schemes that are (a) easily human readable to support erro"
W12-3129,W07-0738,0,0.406358,"Missing"
W12-3129,W08-0332,0,0.268841,"Missing"
W12-3129,W06-3114,0,0.598552,"ems improve, the n-gram based evaluation metrics have begun to show their limits. State-of-the-art MT systems are often able to output translations containing roughly the correct words, while failing to convey important aspects of the meaning of the input sentence. Cases where BLEU strongly disagrees with human judgment of translation quality were 243 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 243–252, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics reported in large scale MT evaluation tasks by CallisonBurch et al. (2006) and Koehn and Monz (2006). Motivated by the goal of addressing the weaknesses of n-gram oriented automatic MT evaluation metrics at evaluating translation adequacy, the HMEANT metric assesses translation utility by matching the basic event structure—“who did what to whom, when, where and why” (Pradhan et al., 2004)—representing the central meaning conveyed by sentences. As mentioned above, however, HMEANT requires humans to manually annotate semantic frames in the reference and machine translations, and then to align the semantic frames—making it difficult to incorporate HMEANT as an objective function in the MT syste"
W12-3129,E06-1031,0,0.86743,"adequacy judgments than BLEU, NIST, METEOR, PER, CDER, WER, or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANT’s virtues of simplicity, representational transparency, and inexpensiveness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly into SMT training methods, due to the non-automatic proces"
W12-3129,P11-1023,1,0.323724,"veness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly into SMT training methods, due to the non-automatic processes of (1) semantic parsing and (2) aligning semantic frames. In this paper we introduce an automatic metric in which both the semantic parsing and the alignment of semantic frames are fully automated. Our aim is to show that even with full automation, this new metric still outperforms all the previous automatic metrics mentioned, thus providing a foundation for"
W12-3129,W11-1002,1,0.378166,"veness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly into SMT training methods, due to the non-automatic processes of (1) semantic parsing and (2) aligning semantic frames. In this paper we introduce an automatic metric in which both the semantic parsing and the alignment of semantic frames are fully automated. Our aim is to show that even with full automation, this new metric still outperforms all the previous automatic metrics mentioned, thus providing a foundation for"
W12-3129,1996.amta-1.13,0,0.209697,"what we did above, which was to sum the lexical similarities of all pairwise combinations of tokens. However, experimental results will show that, surprisingly, to judge the similarity of semantic role fillers, summing the lexical similarities over only word-aligned tokens—instead of all pairwise combinations of tokens— does not help to improve the correlation of the semantic MT evaluation with human adequacy judgment. 7.1 Experimental setup To avoid the danger of aligning a token in one segment to excessive numbers of tokens in the other segment, we adopt a variant of competitive linking by Melamed (1996). Competitive linking is a greedy best-first word alignment algorithm. Table 6: Sentence-level correlation with human adequacy judgments on GALE-A (training set) and GALE-B (testing set) for judging semantic role fillers similarity using pairwise tokens vs. only aligned tokens. Semantic role filler similarity All pairwise tokens Only aligned tokens GALE-A 0.37 0.36 GALE-B 0.19 0.17 The rest of the experimental setup is the same as that used in Section 4. 7.2 Results Table 6 shows that, surprisingly, judging semantic role filler similarity using only the aligned tokens (selected by competitive"
W12-3129,niessen-etal-2000-evaluation,0,0.929744,"U, NIST, METEOR, PER, CDER, WER, or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANT’s virtues of simplicity, representational transparency, and inexpensiveness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly into SMT training methods, due to the non-automatic processes of (1) semantic parsing"
W12-3129,P02-1040,0,0.123001,"otocol, shows that this fully automated version of HMEANT achieves significantly higher Kendall correlation with human adequacy judgments than BLEU, NIST, METEOR, PER, CDER, WER, or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANT’s virtues of simplicity, representational transparency, and inexpensiveness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortuna"
W12-3129,N04-1030,0,0.931733,"disagrees with human judgment of translation quality were 243 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 243–252, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics reported in large scale MT evaluation tasks by CallisonBurch et al. (2006) and Koehn and Monz (2006). Motivated by the goal of addressing the weaknesses of n-gram oriented automatic MT evaluation metrics at evaluating translation adequacy, the HMEANT metric assesses translation utility by matching the basic event structure—“who did what to whom, when, where and why” (Pradhan et al., 2004)—representing the central meaning conveyed by sentences. As mentioned above, however, HMEANT requires humans to manually annotate semantic frames in the reference and machine translations, and then to align the semantic frames—making it difficult to incorporate HMEANT as an objective function in the MT system training, evaluating, and optimizing cycle. We argue in this paper that both the human semantic parsing and the semantic frame alignment tasks performed within HMEANT can be successfully automated to produce a state-of-the-art automatic metric. Moreover, we show that the spirit of Occam’s"
W12-3129,2006.amta-papers.25,0,0.928821,", or TER. Furthermore, we demonstrate that performing the semantic frame alignment automatically actually tends to be just as good as performing it manually. Despite its high performance, fully automated MEANT is still able to preserve HMEANT’s virtues of simplicity, representational transparency, and inexpensiveness. 1 For the past decade, MT evaluation has relied heavily on inexpensive automatic metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). In large part, this is because automatic metrics significantly shorten the evaluation cycle by providing a fast, easy and cheap quantitative evaluation which can be effectively incorporated into modern SMT training methods. Despite the fact that HMEANT, a human metric recently proposed by Lo and Wu (2011b,c,d), was shown to reflect translation adequacy more accurately than all of these automatic metrics, it is unfortunately infeasible to incorporate the HMEANT metrics directly into SMT training methods, due to the non-automatic processes of (1) semantic parsing and (2) aligning semantic fram"
W12-3129,W10-1703,0,\N,Missing
W12-4206,W05-0909,0,0.506786,"ed from the supervised training is similar to the relative frequencies of the occurrences of each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) because of their support on fast and inexpensive evaluation. These metrics are good at ranking overall systems by averaging their scores over the entire document. As MT systems improve, the focus of MT evaluation changes from generally reflecting the quality of each system to assisting error analysis on each MT output in detail. The failure of such metrics in evaluating translation quality on sentence level are becoming more apparent. Though containing roughly the correct words, the MT output as a whole sen"
W12-4206,E06-1032,0,0.46584,"Missing"
W12-4206,W10-1703,0,0.0875079,"Missing"
W12-4206,W07-0738,0,0.711622,"Missing"
W12-4206,W08-0332,0,0.56885,"Missing"
W12-4206,W06-3114,0,0.15456,"metrics in evaluating translation quality on sentence level are becoming more apparent. Though containing roughly the correct words, the MT output as a whole sentence is still quite incomprehensible and fails to express meaning that is close to the input. Lexical n-gram based evaluation metrics are surface-oriented and do not do so well at ranking translations according to adequacy and are particularly poor at reflecting significant translation quality improvements on more meaningful word sense or semantic frame choices which human judges can indicate clearly. CallisonBurch et al. (2006) and Koehn and Monz (2006) even reported cases where BLEU strongly disagrees with human judgment on translation quality. 49 Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 49–56, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics Liu and Gildea (2005) proposed STM, a structural approach based on syntax to addresses the failure of lexical similarity based metrics in evaluating translation grammaticality. However, a grammatical translation can achieve a high syntax-based score but still contains meaning errors arising from con"
W12-4206,E06-1031,0,0.610611,"quencies of the occurrences of each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) because of their support on fast and inexpensive evaluation. These metrics are good at ranking overall systems by averaging their scores over the entire document. As MT systems improve, the focus of MT evaluation changes from generally reflecting the quality of each system to assisting error analysis on each MT output in detail. The failure of such metrics in evaluating translation quality on sentence level are becoming more apparent. Though containing roughly the correct words, the MT output as a whole sentence is still quite incomprehensible and fails to expres"
W12-4206,W05-0904,0,0.0918446,"do not do so well at ranking translations according to adequacy and are particularly poor at reflecting significant translation quality improvements on more meaningful word sense or semantic frame choices which human judges can indicate clearly. CallisonBurch et al. (2006) and Koehn and Monz (2006) even reported cases where BLEU strongly disagrees with human judgment on translation quality. 49 Proceedings of SSST-6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 49–56, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics Liu and Gildea (2005) proposed STM, a structural approach based on syntax to addresses the failure of lexical similarity based metrics in evaluating translation grammaticality. However, a grammatical translation can achieve a high syntax-based score but still contains meaning errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluations, such as HTER (Snover et al., 2006), are more adequacy oriented and show a high correlation with human adequacy judgment, the high labor cost prohibits their widespread use. There was also work on explicitly evaluating"
W12-4206,N07-1006,0,0.0143897,"ting more linguistic features into MT evaluation metrics raise the discussion on the appropriate approach in weighting and combining them. ULC (Gim´enez and M`arquez, 2007, 2008) uses uniform weights to aggregate linguistic features. This approach does not capture the importance of each feature to the overall translation quality to the MT output. One obvious example of different semantic roles contribute differently to the overall meaning is that readers usually accept translations with errors in adjunct arguments as a valid translation but not those with errors in core arguments. Unlike ULC, Liu and Gildea (2007); Lo and Wu (2011a) approach the weight estimation problem by maximum correlation training which directly optimize the correlation with human adequacy judg50 Figure 1: HMEANT structured role representation with a weighting scheme reflecting the degree of contribution of each semantic role type to the semantic frame. (Lo and Wu, 2011a,b,c). ments. However, the shortcomings of this approach is that it requires a human-ranked training corpus which is expensive, especially for languages with limited resource. We argue in this paper that for semantic MT evaluation, the importance of each semantic r"
W12-4206,P11-1023,1,0.388365,"tric that correlates comparable with human adequacy judgments to previous metrics that require far more expensive human rankings of adequacy over a training corpus. As a result, the cost of semantic MT evaluation is greatly reduced. 1 Introduction In this paper we investigate an unsupervised approach to estimate the degree of contribution of each semantic role type in semantic translation evaluation in low cost without using a human-ranked training corpus but still yields a evaluation metric that correlates comparably with human adequacy judgments to that of recent supervised approaches as in Lo and Wu (2011a, b, c). The new approach is motivated by an analysis showing that the distribution of the weights learned from the supervised training is similar to the relative frequencies of the occurrences of each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-"
W12-4206,W11-1002,1,0.173727,"tric that correlates comparable with human adequacy judgments to previous metrics that require far more expensive human rankings of adequacy over a training corpus. As a result, the cost of semantic MT evaluation is greatly reduced. 1 Introduction In this paper we investigate an unsupervised approach to estimate the degree of contribution of each semantic role type in semantic translation evaluation in low cost without using a human-ranked training corpus but still yields a evaluation metric that correlates comparably with human adequacy judgments to that of recent supervised approaches as in Lo and Wu (2011a, b, c). The new approach is motivated by an analysis showing that the distribution of the weights learned from the supervised training is similar to the relative frequencies of the occurrences of each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-"
W12-4206,niessen-etal-2000-evaluation,0,0.437044,"each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) because of their support on fast and inexpensive evaluation. These metrics are good at ranking overall systems by averaging their scores over the entire document. As MT systems improve, the focus of MT evaluation changes from generally reflecting the quality of each system to assisting error analysis on each MT output in detail. The failure of such metrics in evaluating translation quality on sentence level are becoming more apparent. Though containing roughly the correct words, the MT output as a whole sentence is still quite incomprehensible and fails to express meaning that is close to the"
W12-4206,P09-1034,0,0.0137753,"rics in evaluating translation grammaticality. However, a grammatical translation can achieve a high syntax-based score but still contains meaning errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluations, such as HTER (Snover et al., 2006), are more adequacy oriented and show a high correlation with human adequacy judgment, the high labor cost prohibits their widespread use. There was also work on explicitly evaluating MT adequacy with aggregated linguistic features (Gim´enez and M`arquez, 2007, 2008) and textual entailment (Pado et al., 2009). In the work of Lo and Wu (2011a), MEANT and its human variants HMEANT were introduced and empirical experimental results showed that HMEANT, which can be driven by low-cost monolingual semantic roles annotators with high interannotator agreement, correlates as well as HTER and far superior than BLEU and other surfaced oriented evaluation metrics. Along with additional improvements to the MEANT family of metrics, Lo and Wu (2011b) detailed the studies of the impact of each individual semantic role to the metric’s correlation with human adequacy judgments. Lo and Wu (2011c) further discussed t"
W12-4206,P02-1040,0,0.10653,"alysis showing that the distribution of the weights learned from the supervised training is similar to the relative frequencies of the occurrences of each semantic role in the reference translation. We then introduce a relative frequency weighting scheme to approximate the importance of each semantic role type. With such simple weighting scheme, the cost of evaluating translation of languages with fewer resources available is greatly reduced. For the past decade, the task of measuring the performance of MT systems has relied heavily on lexical n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER (Nießen et al., 2000) because of their support on fast and inexpensive evaluation. These metrics are good at ranking overall systems by averaging their scores over the entire document. As MT systems improve, the focus of MT evaluation changes from generally reflecting the quality of each system to assisting error analysis on each MT output in detail. The failure of such metrics in evaluating translation quality on sentence level are becoming more apparent. Though contai"
W12-4206,N04-1030,0,0.468461,"ic MT evaluation metric using the relative frequency weighting scheme to approximate the importance of each semantic role type correlates comparably with human adequacy judgments to previous metrics that use maximum correlation training, which requires expensive human rankings of adequacy over a training corpus. Therefore, the cost of semantic MT evaluation is greatly reduced. 2 Semantic MT evaluation metrics Adopting the principle that a good translation is one from which human readers may successfully understand at least the basic event structure-“who did what to whom, when, where and why” (Pradhan et al., 2004)-which represents the most essential meaning of the source utterances, Lo and Wu (2011a,b,c) proposed HMEANT to evaluate translation utility based on semantic frames reconstructed by human reader of machine translation output. Monolingual (or bilingual) annotators must label the semantic roles in both the reference and machine translations, and then to align the semantic predicates and role fillers in the MT output to the reference translations. These annotations allow HMEANT to then look at the aligned role fillers, and aggregate the translation accuracy for each role. In the spirit of Occam’"
W12-4206,2006.amta-papers.25,0,0.183236,"6, Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 49–56, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics Liu and Gildea (2005) proposed STM, a structural approach based on syntax to addresses the failure of lexical similarity based metrics in evaluating translation grammaticality. However, a grammatical translation can achieve a high syntax-based score but still contains meaning errors arising from confusion of semantic roles. On the other hand, despite the fact that non-automatic, manually evaluations, such as HTER (Snover et al., 2006), are more adequacy oriented and show a high correlation with human adequacy judgment, the high labor cost prohibits their widespread use. There was also work on explicitly evaluating MT adequacy with aggregated linguistic features (Gim´enez and M`arquez, 2007, 2008) and textual entailment (Pado et al., 2009). In the work of Lo and Wu (2011a), MEANT and its human variants HMEANT were introduced and empirical experimental results showed that HMEANT, which can be driven by low-cost monolingual semantic roles annotators with high interannotator agreement, correlates as well as HTER and far superi"
W13-2254,W07-0738,0,0.459432,"ystems are making. 2.2 3 MEANT and UMEANT MEANT (Lo et al., 2012), which is the weighted f-measure over the matched semantic role labels of the automatically aligned semantic frames and role fillers, outperforms BLEU, NIST, METEOR, WER, CDER and TER. Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations than the common practice of tuning against BLEU or TER across different data genres, such as formal newswire text, informal web forum text and informal public speech. PreLinguistic feature based metrics ULC (Giménez and Màrquez, 2007, 2008) is an automatic metric that incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez 423 Figure 1: Examples of automatic shallow semantic parses. The input is parsed by a Chinese automatic shallow semantic parser. The reference and MT output are parsed by an English automatic shallow semantic parser. There are no semantic frames for MT3 since there is no predicate. cisely, MEANT is computed as follows: Mi,j Ri,j Si,pred Si,j wpred wj 1. Apply an automatic shallow semantic parser on"
W13-2254,W05-0909,0,0.514438,"lison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, si"
W13-2254,W08-0332,0,0.379246,"ng nothing more than a monolingual corpus and an automatic shallow semantic parser. 1 Introduction We evaluate in the context of WMT 2013 the MEANT (Lo et al., 2012) and UMEANT (Lo and Wu, 2012) semantic machine translation (MT) evaluation metrics—tunable, simple yet highly effective, fully-automatic semantic frame based objective functions that score the degree of similarity 422 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 422–428, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features of ULC and is based on pure lexical similarity. Therefore, QUEEN suffers from the problem of failing to reflect translation adequacy similar to other n-gram based metrics. Similarly, SPEDE (Wang and Manning, 2012) is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan (Castillo a"
W13-2254,W06-3114,0,0.732089,"MT systems against MEANT more robustly improves translation adequacy, compared to tuning against BLEU or TER. In the past decade, the progress of machine translation (MT) research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al.,"
W13-2254,E06-1032,0,0.21532,"d Wu, 2013) show that tuning MT systems against MEANT more robustly improves translation adequacy, compared to tuning against BLEU or TER. In the past decade, the progress of machine translation (MT) research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005"
W13-2254,W07-0718,0,0.355592,"Missing"
W13-2254,W08-0309,0,0.257615,"Missing"
W13-2254,E06-1031,0,0.835172,"nd Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, f"
W13-2254,P11-1023,1,0.921751,"such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, fully automatic approximation to the human HMEANT MT evaluation metric which measures semantic frame similarity between MT output and reference translations. In this paper, we describe HKUST’s submission to the WMT 2013 metrics evaluation task, MEANT and UMEANT. MEANT is optimized by tuning a small number of weights—one for each semantic role label—so as to maximize correlation with human adequacy judgment on a development set. UMEANT is an unsupervised version where weights for each sem"
W13-2254,W12-3103,0,0.252366,"uez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Lambert et al. (2006) did tune on QUEEN, a simplified version of ULC that discards the semantic features of ULC and is based on pure lexical similarity. Therefore, QUEEN suffers from the problem of failing to reflect translation adequacy similar to other n-gram based metrics. Similarly, SPEDE (Wang and Manning, 2012) is an integrated probabilistic FSM and probabilistic PDA model that predicts the edit sequence needed for the MT output to match the reference. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase table. Like ULC, these matrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters to be estimated. Although ROSE (Song and Cohn, 2011) is a weighted linear model of shallow ling"
W13-2254,W12-4206,1,0.893952,"oices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a t"
W13-2254,2013.mtsummit-papers.12,1,0.836247,"at a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, fully automatic approximation to the human HMEANT MT evaluation metric which measures semantic frame similarit"
W13-2254,W11-2113,0,0.0503421,"put to match the reference. Sagan (Castillo and Estrella, 2012) is a semantic textual similarity metric based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps; contain several dozens of parameters to tune and employ expensive linguistic resources, like WordNet and paraphrase table. Like ULC, these matrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters to be estimated. Although ROSE (Song and Cohn, 2011) is a weighted linear model of shallow linguistic features which is cheaper in run time but it still contains several dozens of weights that need to be tuned which affects the portability of the metric for evaluating translations across domains. Rios et al. (2011) introduced TINE, an automatic recall-oriented evaluation metric which aims to preserve the basic event structure, but no work has been done toward tuning an SMT system against it. TINE performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. rameters in MEANT, i.e. the weight for each semantic ro"
W13-2254,Y12-1062,1,0.767235,"fillers. 424 semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, wpred and wj are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT, wpred and wj are estimated in an unsupervised manner using relative frequency of each semantic role label in the reference translations when the human judgments on adequacy of the development set were unavailable (Lo and Wu, 2012). In this experiment, we use a MEANT / UMEANT implementation along the lines described in Lo et al. (2012) and Tumuluru et al. (2012) but we incorporate a variant of the aggregation function proposed in Mihalcea et al. (2006) for phrasal similarity of role fillers as it normalizes the phrase length better than geometric mean as described in Tumuluru et al. (2012). In case there is no semantic frame in the sentence, we treat the whole sentence as a phrase and calculate the phrasal similarity, like the role fillers in step 3.1, as the MEANT score. 4 We evaluated MEANT and UMEANT on 3 groups of test sets. The first group is the original (without partition) test data for each language pair (translated in English) in WMT12. This"
W13-2254,P13-2067,1,0.797154,"the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, fully automatic approximation to the human HMEANT MT evaluation metric which measures semant"
W13-2254,W12-3107,0,0.193407,"oices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a t"
W13-2254,niessen-etal-2000-evaluation,0,0.923805,", or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—“who did what to whom, when, where and why”(Pradhan et al., 2004). Lo et al. (2012) show that MEANT correlates better with human adequacy judgment than other commonly used automatic MT evaluation metrics, such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). Recent studies (Lo et al., 2013; Lo and Wu, 2013) also show that tuning MT system against MEANT produces more robustly adequate translations on both formal news text genre and informal web forum or public speech genre compared to tuning against BLEU or TER. These studies show that MEANT is a tunable and highly-accurate MT evaluation metric that drives MT system development towards higher utility. As described in Lo and Wu (2011a), the paThe linguistically transparent MEANT and UMEANT metrics are tunable, simple yet highly effective, fully automatic approximatio"
W13-2254,P02-1040,0,0.113699,"WU HKUST Human Language Technology Center Department of Computer Science and Engineering Hong Kong University of Science and Technology {jackielo|dekai}@cs.ust.hk Abstract between the MT output and the reference translations via semantic role labels (SRL). Recent studies (Lo et al., 2013; Lo and Wu, 2013) show that tuning MT systems against MEANT more robustly improves translation adequacy, compared to tuning against BLEU or TER. In the past decade, the progress of machine translation (MT) research is predominantly driven by the fast and cheap n-gram based MT evaluation metrics, such as BLEU (Papineni et al., 2002), which assume that a good translation is one that shares the same lexical choices as the reference translation. Despite enforcing fluency, it has been established that these metrics do not enforce translation utility adequately and often fail to preserve meaning closely (Callison-Burch et al., 2006; Koehn and Monz, 2006). Unlike BLEU, or other n-gram based MT evaluation metrics, MEANT adopts at outset the principle that a good translation is one from which the human readers may successfully understand at least the central meaning of the input sentence as captured by the basic event structure—"
W13-2254,W11-2112,0,0.198213,"une and employ expensive linguistic resources, like WordNet and paraphrase table. Like ULC, these matrices are not useful in the MT system development cycle for tuning due to expensive running time. The metrics themselves are also expensive in training and tuning due to the large number of parameters to be estimated. Although ROSE (Song and Cohn, 2011) is a weighted linear model of shallow linguistic features which is cheaper in run time but it still contains several dozens of weights that need to be tuned which affects the portability of the metric for evaluating translations across domains. Rios et al. (2011) introduced TINE, an automatic recall-oriented evaluation metric which aims to preserve the basic event structure, but no work has been done toward tuning an SMT system against it. TINE performs comparably to BLEU and worse than METEOR on correlation with human adequacy judgment. rameters in MEANT, i.e. the weight for each semantic role label, could be estimated using simple grid search to optimize the correlation with human adequacy judgments. Later, Lo and Wu (2012) described an unsupervised approach for estimating the parameters of MEANT using relative frequency of each semantic role label"
W13-2254,W12-3102,0,\N,Missing
W13-2254,W12-3129,1,\N,Missing
W13-2254,W12-3104,0,\N,Missing
W13-2254,N04-1030,0,\N,Missing
W14-4003,W12-3103,0,0.116296,"porates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity metric Sagan (Castillo and Estrella, 2012) is based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain many parameters that need to be tuned, and employ expensive linguistic resources such as WordNet or paraphrase tables. The expensive training, tuning and/or running time renders these metrics difficult to use in the SMT training cycle. 3 replace humans with automatic SRL and alignment algorithms. MEANT typically outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlation with human adequacy judgment, and is relatively easy to port to other languages, requiri"
W14-4003,W14-3348,0,0.0416962,"ditdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Ri"
W14-4003,W07-0738,0,0.459718,"adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity"
W14-4003,2012.eamt-1.64,1,0.919302,"ITGs are guaranteed to have a two-normal form similar to context-free grammars, and( can) be biparsed(in polynomial time and space (O n6 ) 4 time and O n space). It (is also ) possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for evalWe introduce an inversion transduction grammar based restructuring of the MEANT automatic semantic frame based MT evaluation metric, which, by leveraging ITG language biases, is able to further improve upon MEANT’s already-high correlation with human adequacy judgments. The new metric, called IMEANT, uses bracketing ITGs to biparse the reference and machine translations, but subject to obeying the semantic frames in both. Resulting improvements support the presumption that ITGs, which constrain the allowable permutations between compositional"
W14-4003,W08-0332,0,0.114202,"high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to match the machine translation to the reference translation via an integrated probabilistic FSM and probabilistic PDA model. The semantic textual similarity metric Sagan (Castillo and Estrella, 2012) is based on a complex textual entailment pipeline. These aggregated metrics require sophisticated feature extraction steps, contain many parameters that need to be tuned, and employ expensive linguistic"
W14-4003,W06-3114,0,0.111485,"T systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG"
W14-4003,W05-0909,0,0.276417,"who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and"
W14-4003,E06-1032,0,0.101147,"e, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) reporting cases 23 where BLEU strongly disagrees with human judgments of translation adequacy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency"
W14-4003,2003.mtsummit-papers.32,0,0.0492008,"sible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related Work 2.1 ITGs and MT evaluation Relatively little investigation into the potential benefits of ITGs is found in previous MT evaluation work. One exception is invWER, proposed by Leusch et al. (2003) and Leusch and Ney (2008). The invWER metric interprets weighted BITGs as a generalization of the Levenshtein edit distance, in which entire segments (blocks) can be inverted, as long as this is done strictly compositionally so as not to violate legal ITG biparse tree structures. The input and output languages are considered to be those of the reference and machine translations, and thus are over the same vocabulary (say,English). At the sentence level, correlation of invWER with human adequacy judgments was found to be among the best. Our current approach differs in several key respects from"
W14-4003,W07-0718,0,0.496708,"Missing"
W14-4003,E06-1031,0,0.492343,")—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER"
W14-4003,W05-0904,0,0.146685,"acy. Such observations have generated a recent surge of work on developing MT evaluation metrics that would outperform BLEU in correlation with human adequacy judgment (HAJ). Like MEANT, the TINE automatic recall-oriented evaluation metric (Rios et al., 2011) aims to preserve basic event structure. However, its correlation with human adequacy judgment is comparable to that of BLEU and not as high as that of METEOR. Owczarzak et al. (2007a,b) improved correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity proposed by Liu and Gildea (2005), but did not achieve higher correlation with human adequacy judgments than metrics like METEOR. Another automatic metric, ULC (Giménez and Màrquez, 2007, 2008), incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an SMT system using a pure form of ULC perhaps due to its expensive run time. Likewise, SPEDE (Wang and Manning, 2012) predicts the edit sequence needed to mat"
W14-4003,W13-2202,0,0.0494942,"reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not correctly reflect the meaning similarities of the input sentence. There are in fact several large scale meta-e"
W14-4003,P11-1023,1,0.944104,"on grammars, is able to exploit bracketing ITGs (also known as BITGs or BTGs) which are ITGs containing only a single non-differentiated non terminal category (Wu, 1995a), so as to produce even higher correlation with human adequacy judgments than any automatic MEANT variants, or other common automatic metrics. We argue that the constraints provided by BITGs over the semantic frames and arguments of the reference and MT output sentences are essential for accurate evaluation of the phrasal similarity of the semantic role fillers. In common with the various MEANT semantic MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), our proposed IMEANT metric measures the degree to which the basic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEAN"
W14-4003,1996.amta-1.13,0,0.120255,"n be seen in Figure 3, which shows the result on the same example sentence as in Figure 1. Disregarding the semantic parsing errors arising from the current limitations of automatic SRL tools, the ITG tends to provide clean, sparse alignments for role fillers like the ARG1 of the resumed PRED, preferring to leave tokens like complete and range unaligned instead of aligning them anyway as MEANT’s maximal alignment algorithm tends to do. Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed that the competitive linking approach (Melamed, 1996) which also generally produces sparser alignments does not work as well in MEANT, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure. For contrast, Figure 4 shows a case where IMEANT appropriately accepts dense alignments. Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE P2.5 data. IMEANT always yields top correlations, and is more consistent than either MEANT or its recent cross-lingual XMEANT quality estimation variant. For reference, the human HMEANT upper bound is 0.53 for"
W14-4003,W12-4206,1,0.915736,"Missing"
W14-4003,niessen-etal-2000-evaluation,0,0.583405,"ranslation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and"
W14-4003,2013.mtsummit-papers.12,1,0.803851,"mprove the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related Work 2.1 ITGs and"
W14-4003,W13-2254,1,0.727687,"mprove the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related Work 2.1 ITGs and"
W14-4003,P02-1040,0,0.0984325,"ic semantic event structure is preserved by translation—the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papin"
W14-4003,P13-2067,1,0.947369,"rs (Wu, 1997) to improve the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related"
W14-4003,2013.iwslt-evaluation.5,1,0.888254,"rs (Wu, 1997) to improve the accuracy of MT evaluation metrics, despite long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints (and the observation that most current state-of-the-art SMT systems employ ITG decoders). We show that ITGs can be used to redesign the MEANT semantic frame based MT evaluation metric (Lo et al., 22 Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 22–33, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). This has motivated our choice of MEANT as the basis on which to experiment with deploying ITGs into semantic MT evaluation. uating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent alignments from wandering off in the wrong direction. 2 Related"
W14-4003,W11-2112,0,0.235694,"Missing"
W14-4003,W09-3804,1,0.903027,"rminal is A, and R is a set of transduction rules with e ∈ W 0 ∪{ϵ} denoting a token in the MT output (or the null token) and f ∈ W 1 ∪ {ϵ} denoting a token in the reference translation (or the null token). The rule probability (or more accurately, rule weight) function p is set to be 1 for structural transduction rules, and for lexical transduction rules it is defined using MEANT’s context vector model based lexical similarity measure. To calculate the inside probability (or more(accurately, inside score) of a ) ∗ pair of segments, P A ⇒ e/f|G , we use the algorithm described in Saers et al. (2009). Given this, si,pred and si,j now represent the length normalized BITG parse scores of the predicates and role fillers of the arguments of type j between the reference and machine translations. 4 Experiments In this section we discuss experiments indicating that IMEANT further improves upon MEANT’s 26 ARG0 PRED ARG1 [MT2] The level of reduction is conducive to raising the inspection and supervision work efficiency . [REF] The reduction in hierarchy helps raise the efficiency of inspection and supervisory work . ARG0 ARG0 pred The level of reduction is conducive to raising the inspection and s"
W14-4003,2006.amta-papers.25,0,0.129607,"essfully be understood by a human. In the other versions of MEANT, similarity between the MT output and the reference translations is computed as a modified weighted fscore over the semantic predicates and role fillers. Across a variety of language pairs and genres, it has been shown that MEANT correlates better with human adequacy judgment than both n-gram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as editdistance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation adequacy across different genres and different languages (English and Chinese) 2.2 MT evaluation metrics Like invWER, other common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do no"
W14-4003,Y12-1062,1,0.928956,"ly two months of SK - II line of products . Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using automatic English SRL. There are no semantic frames for MT3 since there is no predicate in the MT output. ∑ prece,f e∈e = ∑ rece,f = si,pred = si,j = max s(e, f ) |e| f ∈f max e∈e precei,pred ,fi,pred + recei,pred ,fi,pred si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fillers,since this has been sho"
W14-4003,W12-3107,0,0.556805,"na for nearly two months of SK - II line of products . Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using automatic English SRL. There are no semantic frames for MT3 since there is no predicate in the MT output. ∑ prece,f e∈e = ∑ rece,f = si,pred = si,j = max s(e, f ) |e| f ∈f max e∈e precei,pred ,fi,pred + recei,pred ,fi,pred si,pred and si,j are the lexical similarities based on a context vector model of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical and phrasal similarities of the semantic role fillers are computed. A subsequent variant of the aggregation function inspired by Mihalcea et al. (2006) that normalizes phrasal similarities according to the phrase length more accurately was used in more recent work (Lo et al., 2013a; Lo and Wu, 2013a; Lo et al., 2013b). In this paper, we will assess IMEANT against the latest version of MEANT (Lo et al., 2014) which, as shown, uses f-score to aggregate individual token similarities into the composite phrasal similarities of semantic role fill"
W14-4003,P95-1033,1,0.718211,"eorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and( can) be biparsed(in polynomial time and space (O n6 ) 4 time and O n space). It (is also ) possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for evalWe introduce an inversion transduction grammar based restructuring of the MEANT automatic semantic frame based MT evaluation metric, which, by leveraging ITG language biases, is able to further improve upon MEANT’s already-high correlation with human adequacy judgments. The new metric, called IMEANT, uses bracketing ITGs to biparse the reference"
W14-4003,W95-0106,1,0.710667,"eorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar to context-free grammars, and( can) be biparsed(in polynomial time and space (O n6 ) 4 time and O n space). It (is also ) possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This makes ITGs an appealing alternative for evalWe introduce an inversion transduction grammar based restructuring of the MEANT automatic semantic frame based MT evaluation metric, which, by leveraging ITG language biases, is able to further improve upon MEANT’s already-high correlation with human adequacy judgments. The new metric, called IMEANT, uses bracketing ITGs to biparse the reference"
W14-4003,J97-3002,1,0.672318,"Missing"
W14-4003,P03-1019,0,0.242835,"ANT still aims to evaluate MT output in terms of the degree to which the translation has preserved the essential “who did what to whom,for whom, when, where, how and why” of the foreign input sentence. Unlike MEANT, however, IMEANT aligns and scores under ITG assumptions. MEANT uses a maximum alignment algorithm to align the tokens in the role fillers between the reference and machine translations, and then scores by aggregating the lexical similarities into a phrasal similarity using an f-measure. In contrast, IMEANT aligns and scores by utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). To be precise in this regard, we can see IMEANT as differing from the foregoing description of MEANT in the definition of si,pred and si,j , as follows. The IMEANT metric Although MEANT was previously shown to produce higher correlation with human adequacy judgments compared to other automatic metrics, our error analyses suggest that it still suffers from a common weakness among metrics employing lexical similarity, namely that word/token alignments between the reference and machine translations are severely under constrained. No bijectivity or perm"
W14-4003,W07-0411,0,\N,Missing
W14-4003,W12-3129,1,\N,Missing
W14-4719,2012.eamt-1.64,1,0.930826,"Gs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic frames. We believe that ITGs are not only capable of generating the desired structural correspondences between the semantic structures of two languages, but also provide meaningful constraints to prevent"
W14-4719,W05-0909,0,0.0608269,"valuation metrics—a high-impact real-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring"
W14-4719,E06-1032,0,0.0515052,"oo many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a"
W14-4719,W08-0309,0,0.0215813,"e j between the reference and machine translations. 4 Experiments In this section we discuss experiments comparing the four alternative lexical access preference and constraint strategies. 4.1 Experimental setup We compared using the DARPA GALE P2.5 Chinese-English translation test set, as used in Lo and Wu (2011a). The corpus includes the Chinese input sentences, each accompanied by an English reference translation and three participating state-of-the-art MT systems’ output. We computed sentence-level correlations following the benchmark assessment procedure used by WMT and NIST MetricsMaTr (Callison-Burch et al., 2008, 2010, 2011, 2012; Macháček and Bojar, 2013), which use Kendall’s τ correlation coefficient, to evaluate the correlation of evaluation metrics against human judgment on ranking the translation adequacy of the three systems’ output. A higher value for Kendall’s τ indicates more similarity to the human adequacy rankings by the evaluation metrics. The range of possible values of Kendall’s τ correlation coefficient is [-1, 1], where 1 means the 148 Table 1: Sentence-level correlation with human adequacy judgements on different partitions of GALE P2.5 data. For reference, the human HMEANT upper bo"
W14-4719,W10-1703,0,0.0746439,"Missing"
W14-4719,W12-3102,0,0.0524454,"Missing"
W14-4719,W06-3114,0,0.0383454,"plicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety"
W14-4719,E06-1031,0,0.030895,"eal-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the bas"
W14-4719,P11-1023,1,0.865963,"e main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST"
W14-4719,W12-4206,1,0.780835,"MT and REF, respectively. These weights estimate the degree of contribution of each frame to the overall meaning of the sentence. wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). For MEANT, they are determined using supervised estimation via a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a). For UMEANT (Lo and Wu, 2012), they are estimated in an unsupervised manner using relative frequency of each semantic role label in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. 3 Comparison of multiword expression association approaches To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j , all of which employ a standard context vector model of the individual words/tokens in the multiword expressio"
W14-4719,2013.mtsummit-papers.12,1,0.794283,"surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee"
W14-4719,W13-2254,1,0.767435,"surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee"
W14-4719,W12-3129,1,0.780658,"Missing"
W14-4719,P13-2067,1,0.835635,"Missing"
W14-4719,2013.iwslt-evaluation.5,1,0.781022,"Missing"
W14-4719,W13-2202,0,0.0555413,"and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furthermore, tuning the parameters of MT systems with MEANT instead of BLEU or TER robustly improves translation This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: url{http://creativecommons.org/licenses/by/4.0/ 144 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 144–153, Dublin, Ireland, August 23, 2014. Figure 1: Examples of automatic shallow semantic parses. Both the reference and machine translations are parsed using auto"
W14-4719,1996.amta-1.13,0,0.0738935,", 2009). Secondly, the permutation and bijectivity constraints enforced by the ITG provide better leverage to reject token alignments when they are not appropriate, compared with the maximal alignment approach which tends to be rather promiscuous. The ITG tends whenever appropriate to accept clean, sparse alignments for role fillers, prefering to leave tokens unaligned instead of aligning them anyway as the other strategies tend to do. Note that it is not simply a matter of lowering thresholds for accepting token alignments: Tumuluru et al. (2012) showed that the competitive linking approach (Melamed, 1996) does not work as well as the strategies considered in this paper, whereas the ITG appears to be selective about the token alignments in a manner that better fits the semantic structure. 5 Conclusion We have compared four alternative lexical access strategies for aggregation, preferences, and constraints in scoring multiword expression associations that are far too numerous to be explicitly enumerated in lexicons, within the context of semantic frame based machine translation evaluation: bag-of-words, 149 Figure 2: An example of aligning automatic shallow semantic parses under ITGs, visualized"
W14-4719,niessen-etal-2000-evaluation,0,0.0616973,"sed performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is prese"
W14-4719,P02-1040,0,0.0917038,"rd expression associations within automatic semantic MT evaluation metrics—a high-impact real-world extrinsic task-based performance measure. The inadequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation"
W14-4719,N04-1030,0,0.320691,"the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who did what to whom, for whom, when, where, how and why” (Pradhan et al., 2004)—emphasizing that a good translation is one that can successfully be understood by a human. Across a variety of language pairs and genres, MEANT was shown to correlate better with human adequacy judgment than both ngram based MT evaluation metrics such as BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005), as well as edit-distance based metrics such as CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) when evaluating MT output (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b; Macháček and Bojar, 2013). Furth"
W14-4719,W09-2304,1,0.930201,"d expressions of semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inversion transduction grammar can potentially better constrain permissible token alignment patterns between aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed permutations between semantic role fillers across the reference and machine translations for a sample sentence from the evaluation data. In this approach, both alignment and scoring are performed utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define si,pred and si,j as follows. ( ( ))  ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |)  si,pred = si,j = where G R ≡ ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p ([AA] |A) p (e/f |A) = = p (⟨AA⟩|A) = 1 s(e, f ) Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with e ∈ W 0 ∪ {ϵ} denoting a token in the MT output (or the null token) and f ∈ W 1 ∪ {ϵ} denoting a token in the reference translation (or the null token)"
W14-4719,W09-3804,1,0.924813,"general syntax-directed transductions (Aho and Ullman, 1972) is exponential. By constraining a syntax-directed transduction grammar to allow only monotonic straight and inverted reorderings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for e"
W14-4719,2006.amta-papers.25,0,0.030085,"adequacy of lexical coverage of multiword expressions is one of the serious issues in machine translation and automatic MT evaluation; there are simply too many forms to enumerate explicitly within the lexicon. Automatic MT evaluation has driven machine translation research for a decade and a half, but until recently little has been done to use lexical semantics as the main foundation for MT metrics. Common surface-form oriented metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) do not explicitly reflect semantic similarity between the reference and machine translations. Several large scale meta-evaluations (Callison-Burch et al., 2006; Koehn and Monz, 2006) have in fact reported that BLEU significantly disagrees with human judgments of translation adequacy. Recently, the MEANT semantic frame based MT evaluation metrics (Lo and Wu, 2011a, 2012; Lo et al., 2012; Lo and Wu, 2013b), have instead directly couched MT evaluation in the more cognitive terms of semantic frames, by measuring the degree to which the basic event structure is preserved by translation— the “who d"
W14-4719,Y12-1062,1,0.864908,"bel in the references and thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. 3 Comparison of multiword expression association approaches To assess alternative lexical access preferences and constraints for computing multiword expression associations, we now consider four alternative approaches to defining the lexical similarities si,pred and si,j , all of which employ a standard context vector model of the individual words/tokens in the multiword expression arguments between the reference and machine translations, as descibed by Lo et al. (2012) and Tumuluru et al. (2012). 3.1 Bag of words (geometric mean) The original MEANT approaches employed standard a bag-of-words strategy for lexical association. This baseline approach applies no alignment constraints on multiword expressions: ∑ si,pred si,j = = e e ∑ ∑ e∈ei,pred f ∈fi,pred lg(s(e,f )) |ei,pred |·|fi,pred | e∈ei,j ∑ f ∈fi,j lg(s(e,f )) |ei,j |·|fi,j | 146 3.2 Maximum alignment (precision-recall average) In the first maximum alignment based approach we will consider, the definitions of si,pred and si,j are inspired by Mihalcea et al. (2006) who normalize phrasal similarities according to the phrase length."
W14-4719,P95-1033,1,0.486517,"derings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic f"
W14-4719,W95-0106,1,0.54073,"derings, or equivalently permitting only binary or ternary rank rules, it is possible to isolate the low end of that hierarchy into a single equivalence class of inversion transductions. ITGs are guaranteed to have a two-normal form similar ( ) ( ) to context-free grammars, and can be biparsed in polynomial( time ) and space (O n6 time and O n4 space). It is also possible to do approximate biparsing in O n3 time (Saers et al., 2009). These polynomial complexities makes it feasible to estimate the parameters of an ITG using standard machine learning techniques such as expectation maximization (Wu, 1995b) . At the same time, inversion transductions have also been directly shown to be more than sufficient to account for the reordering that occur within semantic frame alternations (Addanki et al., 2012). This language universal property has an evolutionary explanation in terms of computational efficiency and cognitive load for language learnability and interpretability (Wu, 2014). ITGs are thus an appealing alternative for evaluating the possible links between both semantic role fillers in different languages as well as the predicates, and how these parts fit together to form entire semantic f"
W14-4719,J97-3002,1,0.760126,"f precision and recall with a proper f-score. Although this is less consistent with the previous literature, such as Mihalcea et al. (2006), it seems more consistent with the overall f-score based approach of MEANT, and thus we include it in our comparison as a variant of the maximum alignment strategy. 3.4 si,pred = si,j = 2 · precei,pred ,fi,pred · recei,pred ,fi,pred precei,pred ,fi,pred + recei,pred ,fi,pred 2 · precei,j ,fi,j · recei,j ,fi,j precei,j ,fi,j + recei,j ,fi,j Inversion transduction grammar based There has been to date relatively little use of inversion transduction grammars (Wu, 1997) to improve the accuracy of MT evaluation metrics—despite (1) long empirical evidence the vast majority of translation patterns between human languages can be accommodated within ITG constraints, and (2) the observation that most current state-of-the-art SMT systems employ ITG decoders. Especially when considering semantic MT metrics, ITGs would seem to be a natural strategy for multiword expression association for several cognitively motivated reasons, having to do with language universal properties of cross-linguistic semantic frame structure. To begin with, it is quite natural to think of s"
W14-4719,P03-1019,0,0.0475249,"s can cause multiword expressions of semantic role fillers to be matched even when they should not be. In contrast, using a bracketing inversion transduction grammar can potentially better constrain permissible token alignment patterns between aligned role filler phrases. Figure 2 illustrates how the ITG constraints are consistent with the needed permutations between semantic role fillers across the reference and machine translations for a sample sentence from the evaluation data. In this approach, both alignment and scoring are performed utilizing a length-normalized weighted BITG (Wu, 1997; Zens and Ney, 2003; Saers and Wu, 2009; Addanki et al., 2012). We define si,pred and si,j as follows. ( ( ))  ∗ lg P A ⇒ ei,pred /fi,pred |G  lg−1  max( |ei,pred |, |fi,pred |) ))   ( ( ∗ lg P A ⇒ ei,j /fi,j |G  lg−1  max( |ei,j |, |fi,j |)  si,pred = si,j = where G R ≡ ≡ ⟨{A} , W 0 , W 1 , R, A⟩ {A → [AA] , A → ⟨AA⟩, A → e/f } p ([AA] |A) p (e/f |A) = = p (⟨AA⟩|A) = 1 s(e, f ) Here G is a bracketing ITG whose only nonterminal is A, and R is a set of transduction rules with e ∈ W 0 ∪ {ϵ} denoting a token in the MT output (or the null token) and f ∈ W 1 ∪ {ϵ} denoting a token in the reference translation"
W14-4719,W11-2103,0,\N,Missing
W15-3056,P12-1092,0,0.0486048,"els on a wide range of lexical semantic tasks. It is also common knowledge that raw co-occurrence counts do not work very well and performance can be improved when transformed by reweighing the counts for context informativeness and dimensionality reduction. In contrast to conventional word vector models, prediction based word vector models estimate the vectors directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus (Bengio et al., 2006; Collobert and Weston, 2008; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013; Turian et al., 2010). In this paper, we show that MEANT’s correlation with human adequacy judgments can be further improved by incorporating the word embeddings trained by the predict models. Subsequently, tuning MT system against the improved version of MEANT produce more adequate translations than tuning against BLEU. We show that, consistent with MEANTtuned systems that translate into Chinese, MEANT-tuned MT systems that translate into English also outperforms BLEUtuned systems across commonly used MT evaluation metrics, even in BLEU. The result is achieved by signif"
W15-3056,E06-1031,0,0.260585,"ce in both n-gram based metrics and edit distance based metrics, without overfitting to either type of metrics. We argue with the significant improvement in sentence-level correlation with human preferences in evaluating translations in English, the performance of MT system tuned against the newly improved MEANT would also improved. For WMT2015 tuning task, we tuned the basic Czech-English baseline system against the newly improved MEANT using the official development 5 Related Work Most of the common used MT evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) rely heavily on the exact match of the surface form of the tokens in the reference and the MT output. Thus, they do not only fail to capture the 438 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and backoff algorithm. system BLEU-tuned MEANT-tuned (official submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 19.38 18.20 18.96 19.74 NIST 6.48 6.27 6.44 6.62 WER 67.63 70.09 68.41 66.31"
W15-3056,W05-0904,0,0.0263281,"ed metrics, even in BLEU. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) evaluates lexical similarities beyond surface-form by incorporating a large collection of linguistic resources, like synonym table from hand-crafted WordNet and paraphrase table learned from large parallel corpus. Another trend of improving MT evaluation metrics is incorporating the evaluation of meaning structure of the translations. Owczarzak et al. (2007a,b) improved the correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation with human adequacy judgments when comparing to METEOR. Similarly, TINE, an automatic recall-oriented basic meaning event structured based evaluation metric (Rios et al., 2011) correlated with human adequacy judgment comparable to that of BLEU but not as high as that of METEOR. ULC (Giménez and Màrquez, 2007, 2008) incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no"
W15-3056,P11-1023,1,0.844558,"gn the arguments between the reference and MT output according to the lexical similarity of role fillers. = i wi0 ∑ wpred si,pred + j wj si,j ∑ 0 | wpred + j wj |qi,j ∑ i recall = wi0 ∑ 1 wpred si,pred +∑j wj si,j ∑ 1 | i wi wpred + j wj |qi,j ∑ wi1 2 · precision · recall precision + recall (1) (2) i MEANT 435 = (3) where wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). The value of these weights are determined in supervised manner using a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a) for MEANT and in unsupervised manner using relative frequency of each semantic role label in the references for UMEANT (Lo and Wu, 2012). Thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,j are the phrasal similarities of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et a"
W15-3056,W05-0909,0,0.01641,"BLEU-tuned MEANT-tuned (official submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 17.06 15.89 16.75 17.15 meaning similarities of lexicons that do not share the same surface form, but also ignore the meaning structures of the translations. NIST 5.99 5.80 5.95 6.08 WER 69.67 71.82 70.19 68.53 PER 52.86 53.93 53.05 52.03 CDER 59.85 61.43 60.29 59.07 TER 65.71 67.59 66.25 64.65 MEANT 40.10 39.34 40.12 40.23 structural and lexical semantics accurately and thus, MT system tuned against the improved MEANT beats BLEU-tuned system across commonly used metrics, even in BLEU. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) evaluates lexical similarities beyond surface-form by incorporating a large collection of linguistic resources, like synonym table from hand-crafted WordNet and paraphrase table learned from large parallel corpus. Another trend of improving MT evaluation metrics is incorporating the evaluation of meaning structure of the translations. Owczarzak et al. (2007a,b) improved the correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation"
W15-3056,P14-1023,0,0.0481982,"Missing"
W15-3056,W12-4206,1,0.882808,"T 435 = (3) where wpred and wj are the weights of the lexical similarities of the predicates and role fillers of the arguments of type j of all frame between the reference translations and the MT output. There is a total of 12 weights for the set of semantic role labels in MEANT as defined in Lo and Wu (2011b). The value of these weights are determined in supervised manner using a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a) for MEANT and in unsupervised manner using relative frequency of each semantic role label in the references for UMEANT (Lo and Wu, 2012). Thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,j are the phrasal similarities of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical similarities, s(e, f ), are computed using a discrete context vector model and how the phrasal similarities are computed by aggregating the lexical similarities via various heuristics. In the latest version of MEANT (Lo et al., 2014), as shown in above, it uses f-score t"
W15-3056,2013.mtsummit-papers.12,1,0.862524,"MT evaluation metrics, even in BLEU. The result is achieved by significantly improving MEANT’s sentence-level ranking correlation with human preferences through incorporating a more accurate distributional semantic model for lexical similarity and a novel backoff algorithm for evaluating MT output which automatic semantic parser fails to parse. The surprising result of MEANT-tuned systems having a higher BLEU score than BLEU-tuned systems suggests that MEANT is a more accurate objective function guiding the development of MT systems towards producing more adequate translation. 1 Introduction Lo and Wu (2013) showed that MEANT-tuned system for translating into Chinese outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU. However, such phenomena are not observed in MEANT-tuned system for translating into English. In this paper, for the first time, we present MT systems for translating into English, which is tuned to a improved version of MEANT, also outperforms BLEU-tuned system across commonly used MT evaluation metrics, even in BLEU. The improvements in MEANT include incorporating more accurate distributional semantic model for lexical similarity and a novel back"
W15-3056,W07-0718,0,0.0416452,"ts by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation with human adequacy judgments when comparing to METEOR. Similarly, TINE, an automatic recall-oriented basic meaning event structured based evaluation metric (Rios et al., 2011) correlated with human adequacy judgment comparable to that of BLEU but not as high as that of METEOR. ULC (Giménez and Màrquez, 2007, 2008) incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system using a pure form of ULC perhaps due to its expensive run time. 6 Conclusion In this paper we presented the first results of using word embeddings to improve the correlation with human adequacy judgments of MEANT, the stateof-the-art semantic MT evaluation metric. We also showed that using a smaller and easy-to-obtain monolingual corpus (e.g., Gigaword, Wikipedia) for training the word embeddings does not significantly affect the accuracy of MEANT. We showe"
W15-3056,W12-3129,1,0.835401,"Missing"
W15-3056,W08-0309,0,0.09684,"Missing"
W15-3056,P13-2067,1,0.831076,"Missing"
W15-3056,2013.iwslt-evaluation.5,1,0.809594,"Missing"
W15-3056,P14-2124,1,0.923906,"using automatic English SRL. There are no semantic frames for MT3 since there is no predicate in the MT output. tic frames and role fillers in the reference and machine translations. MEANT typically outperforms BLEU, NIST, METEOR, WER, CDER and TER in correlation with human adequacy judgment, and is relatively easy to port to other languages, requiring only an automatic semantic parser and a monolingual corpus of the output language, which is used to train the discrete context vector model for computing the lexical similarity between the semantic role fillers of the reference and translation. Lo et al. (2014) describe a cross-lingual quality estimation variant, XMEANT, capable of evaluating translation quality without the need for expensive human reference translations, by utilizing semantic parses of the original foreign input sentence instead of a reference translation. MEANT is generally computed as follows: 4. Compute the weighted f-score over the matching role labels of these aligned predicates and role fillers according to the following definitions: 1. Apply an automatic shallow semantic parser to both the reference and machine translations. (Figure 1 shows examples of automatic shallow sema"
W15-3056,W14-3348,0,0.0363668,"fficial submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 17.06 15.89 16.75 17.15 meaning similarities of lexicons that do not share the same surface form, but also ignore the meaning structures of the translations. NIST 5.99 5.80 5.95 6.08 WER 69.67 71.82 70.19 68.53 PER 52.86 53.93 53.05 52.03 CDER 59.85 61.43 60.29 59.07 TER 65.71 67.59 66.25 64.65 MEANT 40.10 39.34 40.12 40.23 structural and lexical semantics accurately and thus, MT system tuned against the improved MEANT beats BLEU-tuned system across commonly used metrics, even in BLEU. METEOR (Banerjee and Lavie, 2005; Denkowski and Lavie, 2014) evaluates lexical similarities beyond surface-form by incorporating a large collection of linguistic resources, like synonym table from hand-crafted WordNet and paraphrase table learned from large parallel corpus. Another trend of improving MT evaluation metrics is incorporating the evaluation of meaning structure of the translations. Owczarzak et al. (2007a,b) improved the correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation with human adequacy judgmen"
W15-3056,W14-3336,0,0.0808341,"different from that for MT optimization. = (4) 0 wi0 + wnf ∑ 1 wpred si,pred +∑j wj si,j ∑ 1 | i wi wpred + j wj |qi,j ∑ 0 + wnf ssent 1 + wnf ssent 1 wi1 + wnf precision · recall α · precision + (1 − α) · recall (5) i MEANT = (6) Note that we have also introduced the weight α for the precision and recall. Later, we show that 437 Table 1 shows the document-level Pearson’s score correlation and table 2 shows the sentence-level Kendall’s rank correlation with human preferences of the improved version of MEANT with the previous version of MEANT (Lo et al., 2014) on WMT2014 metrics task test set (Macháček and Bojar, 2014). For the sake of stable performance across all the tested language pairs, the weights of the semantic role labels are estimated in unsupervised manner. First and the most importantly, the documentlevel score correlation with human preferences of all versions of MEANT consistently outperforms all the submitted metrics in Macháček and Bojar (2014). While the variations on document-level correlation with human preferences of different versions of MEANT are not significant, we focus on discussing about the sentence-level results. On sentence-level ranking, MEANT with Gigaword word embeddings corr"
W15-3056,niessen-etal-2000-evaluation,0,0.0776404,"rics and edit distance based metrics, without overfitting to either type of metrics. We argue with the significant improvement in sentence-level correlation with human preferences in evaluating translations in English, the performance of MT system tuned against the newly improved MEANT would also improved. For WMT2015 tuning task, we tuned the basic Czech-English baseline system against the newly improved MEANT using the official development 5 Related Work Most of the common used MT evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) rely heavily on the exact match of the surface form of the tokens in the reference and the MT output. Thus, they do not only fail to capture the 438 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and backoff algorithm. system BLEU-tuned MEANT-tuned (official submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 19.38 18.20 18.96 19.74 NIST 6.48 6.27 6.44 6.62 WER 67.63 70.09 68.41 66.31 PER 50.48 51.84 50.77 49.22"
W15-3056,W07-0738,0,0.0261617,"etrics is incorporating the evaluation of meaning structure of the translations. Owczarzak et al. (2007a,b) improved the correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation with human adequacy judgments when comparing to METEOR. Similarly, TINE, an automatic recall-oriented basic meaning event structured based evaluation metric (Rios et al., 2011) correlated with human adequacy judgment comparable to that of BLEU but not as high as that of METEOR. ULC (Giménez and Màrquez, 2007, 2008) incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system using a pure form of ULC perhaps due to its expensive run time. 6 Conclusion In this paper we presented the first results of using word embeddings to improve the correlation with human adequacy judgments of MEANT, the stateof-the-art semantic MT evaluation metric. We also showed that using a smaller"
W15-3056,W07-0411,0,0.0565554,"Missing"
W15-3056,P02-1040,0,0.10148,"t tuning against MEANT only achieves balanced performance in both n-gram based metrics and edit distance based metrics, without overfitting to either type of metrics. We argue with the significant improvement in sentence-level correlation with human preferences in evaluating translations in English, the performance of MT system tuned against the newly improved MEANT would also improved. For WMT2015 tuning task, we tuned the basic Czech-English baseline system against the newly improved MEANT using the official development 5 Related Work Most of the common used MT evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) rely heavily on the exact match of the surface form of the tokens in the reference and the MT output. Thus, they do not only fail to capture the 438 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and backoff algorithm. system BLEU-tuned MEANT-tuned (official submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 19.38 18.20 18.96 19.74"
W15-3056,W11-2112,0,0.0129728,"from hand-crafted WordNet and paraphrase table learned from large parallel corpus. Another trend of improving MT evaluation metrics is incorporating the evaluation of meaning structure of the translations. Owczarzak et al. (2007a,b) improved the correlation with human fluency judgments by using LFG to extend the approach of evaluating syntactic dependency structure similarity in Liu and Gildea (2005), but did not improve the correlation with human adequacy judgments when comparing to METEOR. Similarly, TINE, an automatic recall-oriented basic meaning event structured based evaluation metric (Rios et al., 2011) correlated with human adequacy judgment comparable to that of BLEU but not as high as that of METEOR. ULC (Giménez and Màrquez, 2007, 2008) incorporates several semantic similarity features and shows improved correlation with human judgement of translation quality (Callison-Burch et al., 2007; Giménez and Màrquez, 2007; Callison-Burch et al., 2008; Giménez and Màrquez, 2008) but no work has been done towards tuning an MT system using a pure form of ULC perhaps due to its expensive run time. 6 Conclusion In this paper we presented the first results of using word embeddings to improve the corre"
W15-3056,2006.amta-papers.25,0,0.0809249,"trics, without overfitting to either type of metrics. We argue with the significant improvement in sentence-level correlation with human preferences in evaluating translations in English, the performance of MT system tuned against the newly improved MEANT would also improved. For WMT2015 tuning task, we tuned the basic Czech-English baseline system against the newly improved MEANT using the official development 5 Related Work Most of the common used MT evaluation metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), CDER (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006) rely heavily on the exact match of the surface form of the tokens in the reference and the MT output. Thus, they do not only fail to capture the 438 Table 3: Translation quality of MT system tuned against MEANT and BLEU on WMT15 tuning task dev set. MEANT reported here is the version using Google pretrained word embeddings with α=1 and backoff algorithm. system BLEU-tuned MEANT-tuned (official submitted buggy system) MEANT-tuned (α=1) MEANT-tuned (α=0.5) BLEU 19.38 18.20 18.96 19.74 NIST 6.48 6.27 6.44 6.62 WER 67.63 70.09 68.41 66.31 PER 50.48 51.84 50.77 49.22 CDER 58.17 59.93 58.74 57.20 T"
W15-3056,Y12-1062,1,0.830125,"and Wu (2011b). The value of these weights are determined in supervised manner using a simple grid search to optimize the correlation with human adequacy judgments (Lo and Wu, 2011a) for MEANT and in unsupervised manner using relative frequency of each semantic role label in the references for UMEANT (Lo and Wu, 2012). Thus UMEANT is useful when human judgments on adequacy of the development set are unavailable. si,pred and si,j are the phrasal similarities of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical similarities, s(e, f ), are computed using a discrete context vector model and how the phrasal similarities are computed by aggregating the lexical similarities via various heuristics. In the latest version of MEANT (Lo et al., 2014), as shown in above, it uses f-score to aggregate individual token similarities into the phrasal similarities of semantic role fillers. Another MEANT’s variant, IMEANT (Wu et al., 2014), which uses ITG to constrain the token alignments between the semantic role fillers of the reference and the machine translations and is shown outperformi"
W15-3056,P10-1040,0,0.010477,"sks. It is also common knowledge that raw co-occurrence counts do not work very well and performance can be improved when transformed by reweighing the counts for context informativeness and dimensionality reduction. In contrast to conventional word vector models, prediction based word vector models estimate the vectors directly as a supervised task, where the weights in a word vector are set to maximize the probability of the contexts in which the word is observed in the corpus (Bengio et al., 2006; Collobert and Weston, 2008; Collobert et al., 2011; Huang et al., 2012; Mikolov et al., 2013; Turian et al., 2010). In this paper, we show that MEANT’s correlation with human adequacy judgments can be further improved by incorporating the word embeddings trained by the predict models. Subsequently, tuning MT system against the improved version of MEANT produce more adequate translations than tuning against BLEU. We show that, consistent with MEANTtuned systems that translate into Chinese, MEANT-tuned MT systems that translate into English also outperforms BLEUtuned systems across commonly used MT evaluation metrics, even in BLEU. The result is achieved by significantly improving MEANT’s sentence-level ran"
W15-3056,W14-4003,1,0.719675,"imilarities of the predicates and role fillers of the arguments of type j between the reference translations and the MT output. Lo et al. (2012) and Tumuluru et al. (2012) described how the lexical similarities, s(e, f ), are computed using a discrete context vector model and how the phrasal similarities are computed by aggregating the lexical similarities via various heuristics. In the latest version of MEANT (Lo et al., 2014), as shown in above, it uses f-score to aggregate individual token similarities into the phrasal similarities of semantic role fillers. Another MEANT’s variant, IMEANT (Wu et al., 2014), which uses ITG to constrain the token alignments between the semantic role fillers of the reference and the machine translations and is shown outperforming MEANT (Lo et al., 2014). 3 vector model is the total number of token types in the training corpus. The vector sparsity issue makes the lexical similarity highly sensitive of exact token matching and thus hurts the accuracy of MEANT. We aim at tackling the sparse vector issue by replacing the discrete context vector model with the continuous word embeddings in order to further improve the accuracy of MEANT. We first train the word embeddin"
W16-2317,N12-1047,1,0.854573,"ierarchical distortion model and sparse feature model are based on Russian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a r"
W16-2317,N13-1003,1,0.836258,"). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs are extracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translation"
W16-2317,P14-1129,0,0.161205,"Missing"
W16-2317,E14-4029,0,0.0478782,"Missing"
W16-2317,D11-1125,0,0.0353707,"mentary v11 monolingual corpus and the NewsCrawl 2015 (206 million sentences in total). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs are extracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase l"
W16-2317,P07-1019,0,0.0247693,"ian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of MIRA (Cherry and Foster, 2012). Decoding uses the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 8"
W16-2317,W10-1717,1,0.823355,"ces) and the monolingual English corpus available for the constrained news translation task, which is a combination of the Europarl v7 corpus, the NewsCommentary v11 monolingual corpus and the NewsCrawl 2015 (206 million sentences in total). Due to resource limits, we have not used the newly released 3 billion sentence CommonCrawl monolingual English corpus. Our submitted system was tuned on the WMT 2014 test set. Both the Russian and English text in the parallel and monolingual corpora in the training/development/test corPortage - the NRC PBMT system The core of the NRC MT system is Portage (Larkin et al., 2010). Portage is a conventional log-linear phrase-based SMT system. We describe the basic features of Portage in this section and the 327 Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013). pora were lower cased and tokenized. 2.2 Translation model We obtain the word alignment by first lemmatizing the Russian side of the parallel training data using Yandex MyStem (Segalovich, 2003). Word alignments are built for the lemmatized Russian using IBM2, HMM and IBM4 models. The Russian is then restored to its fully inflected surface form, and phrase-pairs a"
W16-2317,N04-1021,0,0.137306,"the cube-pruning algorithm of (Huang and Chiang, 2007) with a 7-word distortion limit. 2.7 3 Our success in using Russian lemmas to improve word alignment and NNJMs to improve the overall system performance has inspired us to further develop new components to leverage these ideas. In this section, we describe the new features integrated with Portage in our submitted system: a neural network lexical translation model (NNLTM), a fallback Russian lemma phrase table, and a semi-supervised transliteration model. Rescoring We rescored 1000-best lists output from the decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 82 features: 27 decoder features and 55 additional rescoring features. The rescoring model was tuned using nbest MIRA. Of the rescoring features, 51 consisted of various IBM features for word- and lemmaaligned IBM1, IBM2, IBM4 and HMM models, as well as various other standard length, n-gram, and n-best features. The final four features used NNJMs for rescoring, two Russian-word NNJM rescoring features and two Russian-lemma ones. Following Devlin et al. (2014), one NNJM feature rescored the 1000best list using a English-to-Russian NNJM, where the roles of th"
W16-2317,E99-1010,0,0.130019,"(Stewart et al., 2014). Each is included as a distinct feature in the decoder’s log-linear model. • A 4-gram LM trained on the target side of all the WMT parallel training corpora. • A 6-gram LM trained on the Gigaword corpus. • A 6-gram LM trained on the WMT monolingual English training corpus. • A 6-gram, 200-word-class coarse LM trained on a concatenation of the target side of all the WMT parallel training corpora and the WMT monolingual English training corpus. • A 6-gram, 800-word-class coarse LM trained on the same corpus as the 200-word-class model. Word classes are built using mkcls (Och, 1999). 2.4 Neural network joint model Distortion and sparse feature models Similar to the translation model, our hierarchical distortion model and sparse feature model are based on Russian words but are built on the lemmatized alignment. The sparse feature model consists of the standard sparse features proposed in 328 transliteration (Section 3.3), so case for transliterated words was restored via a post-processing script. As a final step, the output was detokenized with rule-based methods. log partition function, weighted with α = 0.1 (Devlin et al., 2014). 2.6 Tuning and decoding The parameters o"
W16-2317,P12-1049,0,0.0701105,"Missing"
W16-2317,2014.amta-researchers.3,1,0.793782,"12 units for the single hidden layer. We train our models with mini-batch stochastic gradient descent, with a batch size of 128 words, and an initial learning rate of 0.3. We check our training objective on the development set every 20K batches, and if it fails to improve for two consecutive checks, the learning rate is halved. Training stops after 5 consecutive failed checks or after 90 checks. To enable efficient decoding, our models are self-normalized with a squared penalty on the Language models Our system consists of three n-gram language models (LMs) and two word class language models (Stewart et al., 2014). Each is included as a distinct feature in the decoder’s log-linear model. • A 4-gram LM trained on the target side of all the WMT parallel training corpora. • A 6-gram LM trained on the Gigaword corpus. • A 6-gram LM trained on the WMT monolingual English training corpus. • A 6-gram, 200-word-class coarse LM trained on a concatenation of the target side of all the WMT parallel training corpora and the WMT monolingual English training corpus. • A 6-gram, 800-word-class coarse LM trained on the same corpus as the 200-word-class model. Word classes are built using mkcls (Och, 1999). 2.4 Neural"
W16-2317,D13-1140,0,0.0283969,"xtracted for each of our three alignment methods. Counts from all three alignments are then combined into a single phrase table, with a maximum phrase length of 7 tokens. Phrase pairs were filtered so that the top 30 translations for each source phrase were retained. Our internal development experiments indicated that using lemma alignments improved the translation quality of a baseline phrase-based system by roughly 0.2 BLEU, and also benefited the perplexity of the bilingual neural language models described in Section 2.5 and 3.1. 2.3 2.5 We employ two neural network joint models, or NNJMs (Vaswani et al., 2013; Devlin et al., 2014). The NNJM is a feed-forward neural network language model that assumes access to a source sentence f and an aligned source index ai , which points to the most influential source word for the translation of the target word ei . The NNJM calculates the language modeling proba+m i−1 bility p(ei |ei−n+1 , faaii−m ), which accounts for the n−1 preceding target words, and for 2m+1 words of source context, centered around fai . Following Devlin et al. (2014), we use n = 4 and m = 5, resulting in 3 words of target context and 11 words of source context, effectively a 15-gram lan"
W17-4732,D11-1033,0,0.0310836,"omain. Finally, we experimented with a cost weighting domain adaptation technique (Chen et al., 2017). This technique trains a domain classifier concurrently with the NMT system, and uses the classifier probabilities to weight training instances according to their similarity to the development set. Since the majority of the 25 million sentence pairs in the training corpus are general domain, we experimented with different data selection and domain adaptation techniques to further train the NMT system with data that are similar to the development set so as to perform better in the news domain. Axelrod et al. (2011) introduced the bilingual language model cross-entropy difference as a similarity function for identifying sentence pairs from general-domain training corpora that are close to the target domain. We built four language models using the input and output sides of the training corpora and the development set respectively to select 3 million sentence pairs from the training corpora that are close to the news domain. However, the development set, which consists of only 1k sentence pairs, is too tiny to be a suitable corpus for building the in-domain language models that will enable the bilingual LM"
W17-4732,W16-2317,1,0.894593,"Missing"
W17-4732,W17-3205,1,0.837791,"cted by bilingual LM cross-entropy difference (xent), c) further trained with synthetic data, d) further trained with cost weighting, e) further trained with in-domain data selected by semi-supervised convolutional neural network classifier (sscnn), f) greedy model averaging and g) optimized against sentence-level BLEU on the intersection of the subsets of data selected by xent and sscnn using MRT. 3.2 Data selection and domain adaptation lion sentence pairs from the training corpora that are close to the news domain. Finally, we experimented with a cost weighting domain adaptation technique (Chen et al., 2017). This technique trains a domain classifier concurrently with the NMT system, and uses the classifier probabilities to weight training instances according to their similarity to the development set. Since the majority of the 25 million sentence pairs in the training corpus are general domain, we experimented with different data selection and domain adaptation techniques to further train the NMT system with data that are similar to the development set so as to perform better in the news domain. Axelrod et al. (2011) introduced the bilingual language model cross-entropy difference as a similarit"
W17-4732,K16-1031,1,0.92652,"-English (out of twenty participants) in WMT 2017 human evaluation. 1 George Foster∗ Work performed while at NRC. 330 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 330–337 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics the one described in Section 2.1,1 and then employed the machine-translated Russian and perfect English sentence pairs as additional data to train the Russian-English MT system. To select sentences for back-translation, we used a semi-supervised convolutional neural network classifier (Chen and Huang, 2016). We sampled two million sentences from the English monolingual News Crawl 2015 & 2016 corpora according to their classifier scores, which reflect their similarity to the the English half of our development set. formance (third place in both language pairs) in the preliminary automatic evaluation of WMT 2017. In this paper, we discuss the lessons learned in building large-scale state-of-the-art NMT systems. 2 Russian-English news translation We used all the Russian-English parallel corpora available for the constrained news translation task. They include the CommonCrawl corpus, the NewsComment"
W17-4732,N04-1021,0,0.0967458,"lt is rather disappointing by comparison with the exciting improvement reported in Sennrich et al. (2016a), i.e. 3-4 BLEU. Another disappointing result is that model averaging does not work well with the dropout models. We can see model averaging yields around 1 BLEU gain on non-dropout systems. However, the improvement achieved by model averaging drops to 0-0.1 BLEU on dropout systems. In other experiments not shown here, we also saw no improvement from ensembling the checkpoints of our dropout systems. We rescored 1000-best lists output from the phrase-based decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 13 features: 3 NMT models, 2 language models, 5 NNJMs and 3 n-best features. The rescoring model was tuned using n-best MIRA (Cherry and Foster, 2012). The three NMT systems used as rescoring features were: 1) baseline further trained with synthetic data, 2) dropout baseline further trained with synthetic data and with dropout turned off, and 3) the previous model optimized to the development set using minimum risk training. The five NNJM rescoring features include two Russian-word NNJMs and three Russian-lemma ones. Following Devlin et al. (2014), we take"
W17-4732,N12-1047,1,0.741262,"s not work well with the dropout models. We can see model averaging yields around 1 BLEU gain on non-dropout systems. However, the improvement achieved by model averaging drops to 0-0.1 BLEU on dropout systems. In other experiments not shown here, we also saw no improvement from ensembling the checkpoints of our dropout systems. We rescored 1000-best lists output from the phrase-based decoder using a rescoring model (Och et al., 2004; Foster et al., 2009) consisting of 13 features: 3 NMT models, 2 language models, 5 NNJMs and 3 n-best features. The rescoring model was tuned using n-best MIRA (Cherry and Foster, 2012). The three NMT systems used as rescoring features were: 1) baseline further trained with synthetic data, 2) dropout baseline further trained with synthetic data and with dropout turned off, and 3) the previous model optimized to the development set using minimum risk training. The five NNJM rescoring features include two Russian-word NNJMs and three Russian-lemma ones. Following Devlin et al. (2014), we take advantage of the rescoring framework to have our NNJMs view each candidate translation from 332 Figure 1: Russian-English learning curve on development set in cased BLEU of selected model"
W17-4732,P14-1129,0,0.0771307,"Missing"
W17-4732,E17-3017,0,0.0554434,"Missing"
W17-4732,P16-1009,0,0.450161,"inflections, since they play an important role in disambiguating the meaning of sentences. Chinese does not have clear word boundaries. The number of Chinese word types created by automatic word segmentation software is high, while naive character segmentation would result in a skewed Chinese to English sentence length ratio. These characteristics make it difficult for machine translation systems to learn the correct association between words in Chinese and English. Since this was the first time we deployed NMT models in an evaluation, we first tried to replicate the results of previous work (Sennrich et al., 2016a). Our NMT systems are based on Nematus (Sennrich et al., 2017). We used automatic back-translation (Sennrich et al., 2016b) of a subselected monolingual News corpus as additional training data, and all the training data is segmented into subword units using BPE (Sennrich et al., 2016c). We also experimented with pervasive dropout as implemented in Nematus. For Russian-English, our WMT16 PBMT system scored higher than all the NMT systems we built this year. We therefore experimented with using the NMT systems as features for rescoring the 1000-best output from our WMT16 PBMT system. This stra"
W17-4732,W16-2316,0,0.0118976,"and embedding layers to 0.15. For the hidden layers, we set the dropout probability to 0.3. NMT baseline system Our NMT baseline system is developed using Nematus (Sennrich et al., 2017). The dimension of word embeddings is set to 512 and that of the hidden layers is set to 1024. We train the models with rmsprop (Tieleman and Hinton, 2012), reshuffling the training corpus between epochs. We use minibatches of size 100 and validate the model every 8000 minibatches against BLEU on the WMT 15 news translation test set. We perform early stopping on the baseline system. We use AmuNMT C++ decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. 2.2 Pervasive dropout 2.4 Minimum risk training Minimum risk training (MRT) (Shen et al., 2016) allows model optimization to arbitrary loss functions, which do not need to be differentiable, thus enabling direct model tuning against automatic MT evaluation metrics. It uses the MT evaluation metric as the loss function and minimizes the expected loss on the training data at sentence-level. We experimented with further model optimization using MRT on the whole training corpus against sentence BLEU at the final stage. 2.5 Synthetic training data Greedy model averaging A"
W17-4732,P16-1162,0,0.103844,"Missing"
W17-4732,P16-1159,0,0.0343062,"ystem is developed using Nematus (Sennrich et al., 2017). The dimension of word embeddings is set to 512 and that of the hidden layers is set to 1024. We train the models with rmsprop (Tieleman and Hinton, 2012), reshuffling the training corpus between epochs. We use minibatches of size 100 and validate the model every 8000 minibatches against BLEU on the WMT 15 news translation test set. We perform early stopping on the baseline system. We use AmuNMT C++ decoder (Junczys-Dowmunt et al., 2016a) with a beam size of 4. 2.2 Pervasive dropout 2.4 Minimum risk training Minimum risk training (MRT) (Shen et al., 2016) allows model optimization to arbitrary loss functions, which do not need to be differentiable, thus enabling direct model tuning against automatic MT evaluation metrics. It uses the MT evaluation metric as the loss function and minimizes the expected loss on the training data at sentence-level. We experimented with further model optimization using MRT on the whole training corpus against sentence BLEU at the final stage. 2.5 Synthetic training data Greedy model averaging A common practice for avoiding overfitting to the training data is ensembling the last few models saved as checkpoints. Rec"
W17-4732,W03-1730,0,0.0742719,"We used all the Chinese-English parallel corpora available for the constrained news translation task. They include the UN corpus, the NewsCommentary v12 corpus and the CWMT corpus. In total, 25 million parallel Chinese-English sentences were used to train the baseline system. We used half of the WMT 17 news translation development set as our development set and the other half as internal test set. The English texts in the training/development/test corpora were tokenized and lowercased while the Chinese texts in the training/development/test corpora were segmented using the ICTCLAS segmenter (Zhang et al., 2003). Then the Chinese and English text were combined to train a BPE model with vocabulary size of 90k. Although in figure 1 we see that none of the NMT systems manage to beat our WMT16 PBMT submission, the more interesting result is that there is more than 1.8 BLEU gain on the development set and 1.1 BLEU gain on the test set by rescoring the PBMT 1000-best list using just one of our NMT systems and no other features, as in line (g). The final rescoring with weighted collections of NMT systems, language model features, NNJM features and n-best features shows 1.8 BLEU improvement over the WMT 16 s"
W17-4767,W13-0101,0,0.0297552,"original input and the reference and were asked to order up to 5 different MT outputs according to the translation quality. Two other kinds of human judgments of translation quality were collected in the WMT 2016 metrics task. The direct assessment evaluation protocol gave the annotators the reference and one MT output only and asked them to evaluate the translation adequacy of the MT output on an absolute scale. The HUME metric (Birch et al., 2016) is very similar to HMEANT, which evaluates translation adequacy via semantic units in the input sentence annotated by humans following the UCCA (Abend and Rappoport, 2013) guidelines. However, HUME also takes nominal and adjectival argument structures into account (instead of only predicate argument structure as in HMEANT). Due to space limitations, we only report the results of MEANT 2.0, MEANT 2.0 - nosrl, BLEU and the best correlation in each of the individual language pairs. Since we use exactly the same protocol for each of the test sets, our reported results are directly comparable with those reported in Machacek and Bojar (2014); Stanojevi´c et al. (2015); Bojar et al. (2016b). We summarize the observations in the following sections. 5.1 5.1.1 Correlatio"
W17-4767,D16-1134,0,0.0360575,"al., 2016b) for our development experiments. The official human judgments of translation quality were collected using relative ranking. The annotators were given the original input and the reference and were asked to order up to 5 different MT outputs according to the translation quality. Two other kinds of human judgments of translation quality were collected in the WMT 2016 metrics task. The direct assessment evaluation protocol gave the annotators the reference and one MT output only and asked them to evaluate the translation adequacy of the MT output on an absolute scale. The HUME metric (Birch et al., 2016) is very similar to HMEANT, which evaluates translation adequacy via semantic units in the input sentence annotated by humans following the UCCA (Abend and Rappoport, 2013) guidelines. However, HUME also takes nominal and adjectival argument structures into account (instead of only predicate argument structure as in HMEANT). Due to space limitations, we only report the results of MEANT 2.0, MEANT 2.0 - nosrl, BLEU and the best correlation in each of the individual language pairs. Since we use exactly the same protocol for each of the test sets, our reported results are directly comparable with"
W17-4767,W09-1206,0,0.149087,"Missing"
W17-4767,P14-2124,1,0.699528,"ight β to linearly combine the phrasal similarity of the whole sentence and the frame semantic similarity of the reference and the MT output into MEANT. Our development experiments show that the optimal value of β is 0.1. In summary, equations (1) to (8) are replaced by equations (9) to (16) as follow: max s(e, f ) prece set to 0.5 so that MEANT is the balance of precision and recall, when it is used for MT system optimization. HMEANT (Lo and Wu, 2011a) is the variant of MEANT for human evaluation, where the semantic roles in the reference and in the MT output are annotated by humans. XMEANT (Lo et al., 2014) is the cross-lingual variant of MEANT, which estimates translation quality of the MT output against the source sentence using automatic semantic parsers for the input and output languages and alignment probabilities to determine the crosslingual lexical semantic similarity. 0 + wnf ssent (6) 1 + wnf ssent (7) (8) where s(e, f ) is the lexical similarity computed using word embeddings (Mikolov et al., 2013). By aggregating the lexical similarities, we can obtain the phrasal similarities. ssent is the phrasal similarity of the whole sentence between the reference and the MT output. si,pred is t"
W17-4767,W15-3056,1,0.879211,"ty than earlier versions. The improved phrasal similarity enables a subversion of MEANT to accurately evaluate translation adequacy for any output language, even languages without an automatic semantic parser. Our results show that MEANT, which is a non-ensemble and untrained metric, consistently performs as well as the top participants in previous years including ensemble and trained ones across different output languages. We also present the timing statistics for MEANT for better estimation of the evaluation cost. MEANT 2.0 is open source and publicly available.1 1 2 MEANT and its variants (Lo et al., 2015, 2014; Lo and Wu, 2011a) evaluate translation adequacy by measuring the similarity of the semantic frames and their role fillers between the human reference and machine translations. Figure 1 illustrates the concept of MEANT - the semantic roles and their fillers of the reference translation match more with those of the MT2 than with those of the MT1, therefore MT2 is a more adequate translation than MT1. MEANT consistently outperforms the commonly used automatic MT evaluation metrics, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al"
W17-4767,P11-1023,1,0.921043,"s. The improved phrasal similarity enables a subversion of MEANT to accurately evaluate translation adequacy for any output language, even languages without an automatic semantic parser. Our results show that MEANT, which is a non-ensemble and untrained metric, consistently performs as well as the top participants in previous years including ensemble and trained ones across different output languages. We also present the timing statistics for MEANT for better estimation of the evaluation cost. MEANT 2.0 is open source and publicly available.1 1 2 MEANT and its variants (Lo et al., 2015, 2014; Lo and Wu, 2011a) evaluate translation adequacy by measuring the similarity of the semantic frames and their role fillers between the human reference and machine translations. Figure 1 illustrates the concept of MEANT - the semantic roles and their fillers of the reference translation match more with those of the MT2 than with those of the MT1, therefore MT2 is a more adequate translation than MT1. MEANT consistently outperforms the commonly used automatic MT evaluation metrics, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), CDER (Leusch"
W17-4767,W12-4206,1,0.853771,". si,pred is the phrasal similarities of the predicates between the reference translations and the MT output and si,j is that of the role fillers of the arguments of role type j. wpred is the weight of the lexical similarities of the aligned predicates in step 2. wj is the weight of the phrasal similarities of the role fillers of the arguments of role type j of the aligned frames between the reference translations and the MT output in step 3 if their role types are matching. There is a total of 12 weights for the set of semantic role labels in MEANT (Lo and Wu, 2011b) estimated by heuristics (Lo and Wu, 2012). Finally, the weight α for the precision and recall is introduced for different usages of MEANT. α should be set to 1 so that MEANT is pure recall when it is used for MT evaluation and α should be prece,f ≡ rece,f ≡ si,pred = idf-weighted max-aligned distributional ngram precision (9) idf-weighted max-aligned distributional ngram recall (10) prece ·rece i,pred ,fi,pred i,pred ,fi,pred α·prece +(1−α)·rece i,pred ,fi,pred i,pred ,fi,pred (11) prece ·rece i,j ,fi,j i,j ,fi,j α·prece +(1−α)·rece i,j ,fi,j i,j ,fi,j (12) ·rece prece sent ,fsent sent ,fsent α·prece +(1−α)·rece sent ,fsent sent ,fse"
W17-4767,W16-2302,0,0.143546,"Missing"
W17-4767,W14-3336,0,0.396067,"or more robust performance. For languages except Chinese, tokenization step simply involves separating punctuations at the end of the words in both the reference and the MT output. Chinese does not have clear word boundaries. Each individual Chinese character usually carries multiple meanings and relies on surrounding characters to disambiguate it. Naive Chinese character segmentation would affect the accuracy of the vector representation and the distributional lexical semantic similarity model. Thus, we use ICTCLAS Experiments and results We use the WMT 2014-2016 metrics task evaluation set (Machacek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016b) for our development experiments. The official human judgments of translation quality were collected using relative ranking. The annotators were given the original input and the reference and were asked to order up to 5 different MT outputs according to the translation quality. Two other kinds of human judgments of translation quality were collected in the WMT 2016 metrics task. The direct assessment evaluation protocol gave the annotators the reference and one MT output only and asked them to evaluate the translation adequacy of the MT output on"
W17-4767,W14-3348,0,0.0256655,"1 1 2 MEANT and its variants (Lo et al., 2015, 2014; Lo and Wu, 2011a) evaluate translation adequacy by measuring the similarity of the semantic frames and their role fillers between the human reference and machine translations. Figure 1 illustrates the concept of MEANT - the semantic roles and their fillers of the reference translation match more with those of the MT2 than with those of the MT1, therefore MT2 is a more adequate translation than MT1. MEANT consistently outperforms the commonly used automatic MT evaluation metrics, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), CDER (Leusch et al., 2006) and WER in correlation with human adequacy judgment. It is relatively easy to port to other languages. In the full version of MEANT, it required only a monolingual corpus (for evalIntroduction We introduce a new version of MEANT, which participated in evaluating MT systems for all language pairs in the metrics task of the Second Conference on Machine Translation (WMT 2017). MEANT 2.0 is a non-ensemble and untrained metric that only requires a monolingual corpus in the output language to build the word embeddings and an automatic shallow s"
W17-4767,E06-1031,0,0.106359,"u, 2011a) evaluate translation adequacy by measuring the similarity of the semantic frames and their role fillers between the human reference and machine translations. Figure 1 illustrates the concept of MEANT - the semantic roles and their fillers of the reference translation match more with those of the MT2 than with those of the MT1, therefore MT2 is a more adequate translation than MT1. MEANT consistently outperforms the commonly used automatic MT evaluation metrics, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), CDER (Leusch et al., 2006) and WER in correlation with human adequacy judgment. It is relatively easy to port to other languages. In the full version of MEANT, it required only a monolingual corpus (for evalIntroduction We introduce a new version of MEANT, which participated in evaluating MT systems for all language pairs in the metrics task of the Second Conference on Machine Translation (WMT 2017). MEANT 2.0 is a non-ensemble and untrained metric that only requires a monolingual corpus in the output language to build the word embeddings and an automatic shallow semantic parser to obtain the predicate-argument structu"
W17-4767,D14-1045,0,0.0416674,"uages without an automatic semantic parser or sentences without a valid predicate-argument structure recognized by an automatic semantic parser, the MEANT score is the phrasal similarity of the whole sentence. 4 Setup We use the monolingual corpora provided for the WMT translation task (Bojar et al., 2014, 2015, 2016a) to build the word embeddings for evaluating lexical similarities using word2vec (Mikolov et al., 2013). Table 1 summarizes the resources used to train the word embeddings and the resulting vocabulary size of the distributional lexical semantic similarity model. We use mateplus (Roth and Woodsend, 2014) for German and English semantic role labeling and mate-tools (Bj¨orkelund et al., 2009) for Chinese semantic role labeling. Instead of the 12 semantic role types used in (Lo and Wu, 2011b), we merge the semantic role labels of Chinese, English and German into 8 role types (who, did, what, whom, when, where, why, how) for more robust performance. For languages except Chinese, tokenization step simply involves separating punctuations at the end of the words in both the reference and the MT output. Chinese does not have clear word boundaries. Each individual Chinese character usually carries mul"
W17-4767,2006.amta-papers.25,0,0.0762091,"et al., 2015, 2014; Lo and Wu, 2011a) evaluate translation adequacy by measuring the similarity of the semantic frames and their role fillers between the human reference and machine translations. Figure 1 illustrates the concept of MEANT - the semantic roles and their fillers of the reference translation match more with those of the MT2 than with those of the MT1, therefore MT2 is a more adequate translation than MT1. MEANT consistently outperforms the commonly used automatic MT evaluation metrics, BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Denkowski and Lavie, 2014), TER (Snover et al., 2006), CDER (Leusch et al., 2006) and WER in correlation with human adequacy judgment. It is relatively easy to port to other languages. In the full version of MEANT, it required only a monolingual corpus (for evalIntroduction We introduce a new version of MEANT, which participated in evaluating MT systems for all language pairs in the metrics task of the Second Conference on Machine Translation (WMT 2017). MEANT 2.0 is a non-ensemble and untrained metric that only requires a monolingual corpus in the output language to build the word embeddings and an automatic shallow semantic parser to obtain th"
W17-4767,W15-3031,0,0.0801168,"Missing"
W17-4767,W03-1730,0,0.059886,",fi,j i,j ,fi,j α·prece +(1−α)·rece i,j ,fi,j i,j ,fi,j (12) ·rece prece sent ,fsent sent ,fsent α·prece +(1−α)·rece sent ,fsent sent ,fsent (13) si,j = ssent = P wpred si,pred + j wj si,j P wpred + j wj |q 0 | i,j P 0 i wi P P 1 wpred si,pred + j wj si,j P i wi wpred + j wj |q 1 | i,j P 1 i wi P i 591 precision = recall = wi0 (14) (15) lang. cs de en fi fr hi lv pl ro ru tr zh # sent. 67M 184M 331M 14M 38M 1M 11M 39M 2M 52M 2M 61M # tokens 1,088M 3,444M 6,585M 215M 1,047M 37M 194M 318M 57M 983M 34M 2,227M # resulted vocab. 1,963k 4,018k 2,368k 1,405k 950k 82k 586k 885k 233k 1,708k 279k 911k (Zhang et al., 2003) to segment the Chinese monolingual corpus into words before building the word embeddings. 5 Table 1: Statistics of resources used to train the word embeddings and the resulted vocabulary size of the model. MEANT = β precision · recall + (1 − β)ssent α · precision + (1 − α) · recall (16) As a result, for languages without an automatic semantic parser or sentences without a valid predicate-argument structure recognized by an automatic semantic parser, the MEANT score is the phrasal similarity of the whole sentence. 4 Setup We use the monolingual corpora provided for the WMT translation task (Bo"
W18-6480,W11-2123,0,0.0229985,"disproportionately Parallelism estimation With sentence vectors (§2.1) for the reduced corpus (§2.2) in hand, we set out to estimate the degree of parallelism of sentence pairs. A novel measure of parallelism, based on ratios of squared Mahalanobis distances, performed better on a synthetic dataset than some more obvious measurements, and the single-feature submission based on it was our best unsupervised submission. We also made several other unsupervised measurements: 2 901 https://github.com/aboSamoor/pycld2 1. Perplexity of the German sentence according to a 6-gram KenLM language model3 (Heafield, 2011) not just the one that happened to come first in the original corpus. 3 2. Perplexity of the English sentence according to a 6-gram KenLM language model Mahalanobis ratios for parallelism assessment As mentioned in §2.3, we performed several unsupervised measurements on each sentence pair; of these, the measurement that best predicted paralellism (on synthetic data and on our small 300sentence annotated set) was a novel measurement based on squared Mahalanobis distances. This measurement rests on two insights: 3. The ratio between (1) and (2), to find sentences pairs that contain different amo"
W18-6480,2005.mtsummit-papers.11,0,0.223295,"supervised entries, setting ourselves an additional constraint that we not utilize the additional clean parallel corpora. One such entry fairly consistently scored in the top ten systems in the 100M-word conditions, and for one task—translating the European Medicines Agency corpus (Tiedemann, 2009)—scored among the best systems even in the 10M-word conditions. 1 Introduction and motivation The WMT18 shared task on parallel corpus filtering assumes (but does not require) a supervised learning approach. Given 1. a set of “clean” German-English parallel corpora including past WMT data, Europarl (Koehn, 2005), etc., and 2. a large, potentially “dirty” corpus (i.e., one that may contain non-parallel data, nonlinguistic data, etc.) scraped from the internet (Koehn et al., 2018a), can one identify which sentences from (2) are clean? Supervised learning is an obvious approach in well-resourced languages like German and English, in which there exist well-cleaned parallel corpora across various domains. However, in much lower-resourced languages, we generally do not have multiple parallel corpora 2 Overall architecture The highest-ranked submission of our unsupervised submissions, NRC-seve-bicov, 1 We a"
W18-6480,W18-6481,1,0.639364,"onference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 900–907 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64107 dates, and performed comparatively poorly when tasked with training full sentences. To mitigate this, we ran an additional de-duplication step on the English side in which two sentences that differ only in numbers (e.g., “14 May 2017” and “19 May 1996”) were considered duplicates. shares the same general skeleton as NRC’s highest-ranked supervised submission, NRC-yisi-bicov (Lo et al., 2018); it differs primarily in the parallelism estimation component (§2.3). 2.1 Training sentence embeddings Without numerical de-duplication, we believe the parallelism estimation step in §2.3 would have had too much of a bias towards short numerical sentences. It is, after all, essentially just looking for sentence pairs that it considers likely given the distribution of sentence pairs in the target corpus; if the corpus has a large number of short numerical sentences (and it appears to), the measurement will come to prefer those, whether or not they are useful for the downstream task. We began b"
W18-6480,W03-0320,0,0.0898108,"craped from the internet (Koehn et al., 2018a), can one identify which sentences from (2) are clean? Supervised learning is an obvious approach in well-resourced languages like German and English, in which there exist well-cleaned parallel corpora across various domains. However, in much lower-resourced languages, we generally do not have multiple parallel corpora 2 Overall architecture The highest-ranked submission of our unsupervised submissions, NRC-seve-bicov, 1 We are thinking in particular of the English-Inuktitut translation pair, which is a long-standing research interest of NRC (e.g. Martin et al., 2003). 900 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 900–907 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64107 dates, and performed comparatively poorly when tasked with training full sentences. To mitigate this, we ran an additional de-duplication step on the English side in which two sentences that differ only in numbers (e.g., “14 May 2017” and “19 May 1996”) were considered duplicates. shares the same general skeleton as NRC’s highest-ranked supervi"
W18-6480,N18-1049,0,0.0495645,"ent (§2.3). 2.1 Training sentence embeddings Without numerical de-duplication, we believe the parallelism estimation step in §2.3 would have had too much of a bias towards short numerical sentences. It is, after all, essentially just looking for sentence pairs that it considers likely given the distribution of sentence pairs in the target corpus; if the corpus has a large number of short numerical sentences (and it appears to), the measurement will come to prefer those, whether or not they are useful for the downstream task. We began by training monolingual sentence embeddings using sent2vec (Pagliardini et al., 2018), on all available monolingual data. This included the monolingual data available in the “clean” parallel training data. That is to say, we did not completely throw out the clean parallel data for this task, we simply used it as two unaligned monolingual corpora. We trained sentence vectors of 10, 50, 100, 300, and 700 dimensions; our final submissions used the 300-dimensional vectors as a compromise between accuracy (lower-dimensional vectors had lower accuracy during sanity-checking) and efficiency (higher-dimensional vectors ended up exceeding our memory capacity in downstream components)."
W18-6481,2012.amta-papers.7,1,0.908512,"allel development corpus for tuning the weights to combine different parallelism and fluency features. 1 Introduction The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to find clean sentence pairs from ParaCrawl, a humongous high-recall, low-precision web crawled parallel corpus (Koehn et al., 2018a), for training machine translation (MT) systems. Data cleanliness of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality at different levels. Intuitively, the crosslingual semantic textual similarity of the sentence pairs in the corpora is one of the most important factors affecting the parallelism of the target sentence pairs. Lo et al. (2016) scored crosslingual 908 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 908–916 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64108"
W18-6481,W11-2123,0,0.10867,"aining data (tokenized and lowercased) to train an 1 909 https://github.com/aboSamoor/pycld2 SMT system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry"
W18-6481,J93-2003,0,0.0529985,"placeholder token, before deciding which sentences were duplicates. Sentence pairs were filtered out if the pair was seen before or if the input side was exactly the same as the output side. 2.2.1 Parallelism YiSi-1: monolingual semantic MT evaluation metric We first used the “clean” WMT18 news translation task monolingual and parallel training data (tokenized and lowercased) to train an 1 909 https://github.com/aboSamoor/pycld2 SMT system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network j"
W18-6481,E17-3017,0,0.0199669,"selection, it allowed for substantial BLEU score increases: +1.61 BLEU for SMT systems on average and +2.44 BLEU for NMT systems. MT quality check We used the official software to extract the 10Mword and 100M-word corpora from the original ParaCrawl according to the feature scores. We then trained SMT and NMT systems using the extracted data. The SMT systems were trained using Portage with components and parameters similar to the German-English SMT system in Williams et al. (2016). The NMT systems were transformer models with self-attention (Vaswani et al., 2017) trained using Sockeye1.18.20 (Hieber et al., 2017) with default parameter settings2 , except for the maximum sequence length, which was reduced to 60:60, and we also clip gradients to 1. We used newstest2017 and newstest2018 as the MT development and test set. Table 2 shows the BLEU scores for MT systems trained on the ParaCrawl data subselected by our scoring features. We have also included the random scoring feature (with initial filtering) as a baseline. The MT quality trained on data subselected by the feature scores showed the same trend as the results of the sanity check. That is to say, a feature that performed better in the sanity che"
W18-6481,W10-1703,0,0.0648474,"Missing"
W18-6481,D11-1125,0,0.0564437,"log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the fi"
W18-6481,P07-1019,0,0.0250594,"sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P a P k=0 w(ea+k )·s(ea+k ,fb+k ) −−−−− → w − e− a..a+n−1 · max n−1 P b k=0 w(ea+k ) P − −−−−−− → w e a a..a+n−1 n−1 P −−−−−−→ k=0 w(fb+k )·s(ea+k ,fb+k ) b w fb..b+n−1 · max n−1 P a k=0 w(fb+k ) −−−−−−→ P b w fb..b+n−1 −→ −−−→ precision = sp (− e− sent , fsent ) −→ −−−→ recall = sr (− e− sent , fsent ) precision · recall YiSi-1 = α · precision + (1 − α) · recall YiSi-1 srl measures the semantic similarity with additional frame semantic or"
W18-6481,W12-3102,0,0.0959196,"Missing"
W18-6481,W18-6453,0,0.0549896,". In fact, our best performing system—NRC-yisi-bicov is one of the only four submissions ranked top 10 in both evaluations. Our submitted systems also include some initial filtering steps for scaling down the size of the test corpus and a final redundancy removal step for better semantic and token coverage of the filtered corpus. In this paper, we also describe our unsuccessful attempt in automatically synthesizing a noisy parallel development corpus for tuning the weights to combine different parallelism and fluency features. 1 Introduction The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to find clean sentence pairs from ParaCrawl, a humongous high-recall, low-precision web crawled parallel corpus (Koehn et al., 2018a), for training machine translation (MT) systems. Data cleanliness of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality at different levels. Intuitively, the crosslingual semantic textua"
W18-6481,W11-2103,0,0.0555991,"Missing"
W18-6481,N03-1017,0,0.0132764,"ce pairs were filtered out if the pair was seen before or if the input side was exactly the same as the output side. 2.2.1 Parallelism YiSi-1: monolingual semantic MT evaluation metric We first used the “clean” WMT18 news translation task monolingual and parallel training data (tokenized and lowercased) to train an 1 909 https://github.com/aboSamoor/pycld2 SMT system using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system. The translation model of the SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source"
W18-6481,N13-1003,0,0.0199856,"word alignments (Brown et al., 1993) with grow-diag-final-and phrase extraction heuristics (Koehn et al., 2003). The system has two n-gram language models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P"
W18-6481,N12-1047,0,0.0247327,"2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P a P k=0 w(ea+k )·s(ea+k ,fb+k ) −−−−− → w − e− a..a+n−1 · max n−1 P b k=0 w(ea+k ) P − −−−−−− → w e a a..a+n−1 n−1 P −−−−−−→ k=0 w(fb+k )·s(ea+k ,fb+k ) b w fb..b+n−1 · max n−1 P a k=0 w(fb+k ) −−−−−−→ P b w fb..b+n−1 −→ −−−→ precision = sp (− e− sent , fsent ) −→ −−−→ recall = sr (− e− sent , fsent ) precision · recall YiSi-1 = α · precision + (1 − α) · recall YiSi-1 s"
W18-6481,P14-1129,0,0.126783,"Missing"
W18-6481,W18-6480,1,0.552227,"he larger one. When we evaluate MT output in practice, YiSi score is a weighted harmonic mean of the precision and recall. However, in this work, we segregated the precision and recall of YiSi into separate features as we planned to let the regression decide suitable weights to combine them. Further details of YiSi are provided in Lo (2018). Distance of sentence vectors Sentence vectors were trained using sent2vec (Pagliardini et al., 2018) on each side of the “clean” parallel WMT18 news translation task parallel training data. Further details on how to compute these features are described in Littell et al. (2018). YiSi-2: crosslingual semantic MT evaluation metric For the crosslingual version of YiSi, YiSi-2, instead of training a German-English MT system, we used the “clean” WMT18 news translation task parallel training data to train bilingual word embeddings using bivec (Luong et al., 2015) for evaluating crosslingual lexical semantic similarity. Similar to YiSi-1, YiSi-2 precision and recall are the weighted sum of the crosslingual lexical semantic similarity of the sentence pairs over the weighted count of tokens in the German and English sentences respectively. In this work, we set the n-gram siz"
W18-6481,W17-4767,1,0.789134,"egmental semantic precision of the semantic role fillers according to the shallow semantic structure parsed by the mateplus (Roth and Woodsend, 2014) English semantic parser over the weighted counts of roles and frames according to the shallow semantic structure of the MT output and similarly, for the frame semantic recall. Precisely, YiSi-1 srl is computed as follows: We also used the monolingual English data to train word embeddings using word2vec (Mikolov et al., 2013) for evaluating monolingual lexical semantic similarity. YiSi is new a semantic MT evaluation metric inspired by MEANT 2.0 (Lo, 2017). YiSi1 is equivalent to MEANT 2.0-nosrl. It measures the segmental semantic similarity. The segmental semantic precision and recall divide the inverse-document-frequency weighted sum of the n-gram lexical semantic similarity of the MT output and the English sentence of the target pair by the weighted count of n-grams in the MT output and the English sentences, respectively. In this work, we set the n-gram size to two. Precisely, YiSi-1 is computed as follows: 0 qi,j = ARG j of aligned frame i in MT 1 qi,j = ARG j of aligned frame i in REF #tokens filled in aligned frame i of MT = total #token"
W18-6481,D13-1140,0,0.0158726,"e models: a 5-gram mixture language model (LM) trained on the four corpora components using SRILM (Stolcke, 2002), and a pruned 6-gram LM trained on the WMT monolingual English training corpus built using KenLM (Heafield, 2011). The SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set (newstest2017) using the batch variant of margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. We then translated the German side of the filtered ParaCrawl into English. n−1 → − − sp (→ e, f)= P → − − sr (→ e, f)= P a P k=0 w(ea+k )·s(ea+k ,fb+k ) −−−−− → w − e− a..a+n−1 · max n−1 P b k=0 w(ea+k ) P − −−−−−− → w e a a..a+n−1 n−1 P −−−−−−→ k=0 w(fb+k )·s(ea+k ,fb+"
W18-6481,S16-1102,1,0.839606,"Missing"
W18-6481,C96-2141,0,0.302528,"nguage. The target scores of these pairs are proportional to the percentage of tokens offset, deleted or introduced. Lastly, misaligned sentence pairs were added as fluent but non-parallel negative examples. The resulting development set had 11k sentence pairs of positive and synthetic negative examples. Alignment scores The SMT model trained on the “clean” WMT18 news translation task parallel training data for YiSi score computation include several alignment models as components, from which probabilities p(d|e) and p(e|d) were computed. We find the hidden markov model (HMM) alignment models (Vogel et al., 1996) are reliably useful for scoring parallelism of the sentence pairs in the target corpus. Perplexity ratio of input sentences and output sentences The perplexity ratio reflects the different amounts of information contained in each side of the sentence pairs. This is computed by dividing the smaller perplexity score of the two sentences in the target pair by the larger one. Thus, the ratio ranged from 0 to 1, where a larger value represents better parallelism. 911 features baselines random hunalign parallelism YiSi-1 precision YiSi-1 recall YiSi-1 srl (β=1) precision YiSi-1 srl (β=1) recall YiS"
W18-6481,W15-1521,0,0.159484,"Missing"
W18-6481,W16-2327,0,0.0946713,"M words) selections, the redundancy removal had virtually no effect when applied after YiSi scoring. However, on the smaller (10M words) selection, it allowed for substantial BLEU score increases: +1.61 BLEU for SMT systems on average and +2.44 BLEU for NMT systems. MT quality check We used the official software to extract the 10Mword and 100M-word corpora from the original ParaCrawl according to the feature scores. We then trained SMT and NMT systems using the extracted data. The SMT systems were trained using Portage with components and parameters similar to the German-English SMT system in Williams et al. (2016). The NMT systems were transformer models with self-attention (Vaswani et al., 2017) trained using Sockeye1.18.20 (Hieber et al., 2017) with default parameter settings2 , except for the maximum sequence length, which was reduced to 60:60, and we also clip gradients to 1. We used newstest2017 and newstest2018 as the MT development and test set. Table 2 shows the BLEU scores for MT systems trained on the ParaCrawl data subselected by our scoring features. We have also included the random scoring feature (with initial filtering) as a baseline. The MT quality trained on data subselected by the fea"
W18-6481,N18-1049,0,0.039983,"evious feature, the perplexity ratio of the input and output sentences POS tags is computed by dividing the smaller POS perplexity score of the two sentences in the target pair by the larger one. When we evaluate MT output in practice, YiSi score is a weighted harmonic mean of the precision and recall. However, in this work, we segregated the precision and recall of YiSi into separate features as we planned to let the regression decide suitable weights to combine them. Further details of YiSi are provided in Lo (2018). Distance of sentence vectors Sentence vectors were trained using sent2vec (Pagliardini et al., 2018) on each side of the “clean” parallel WMT18 news translation task parallel training data. Further details on how to compute these features are described in Littell et al. (2018). YiSi-2: crosslingual semantic MT evaluation metric For the crosslingual version of YiSi, YiSi-2, instead of training a German-English MT system, we used the “clean” WMT18 news translation task parallel training data to train bilingual word embeddings using bivec (Luong et al., 2015) for evaluating crosslingual lexical semantic similarity. Similar to YiSi-1, YiSi-2 precision and recall are the weighted sum of the cross"
W18-6481,P02-1040,0,0.115557,"on processing only, and includes non-parallel, or even non-linguistic data. It contains 104 million German-English sentence pairs, with 1 billion English tokens and 964 million German tokens before punctuation tokenization. A 10-million-word (10M-word) and a 100-millionword (100M-word) corpus sub-selected by the participating cleanliness scoring system were used to train statistical machine translation (SMT) and neural machine translation (NMT) systems. The success of the participating scoring systems was determined by the quality of the MT output from the four MT systems as measured by BLEU (Papineni et al., 2002) on some in-domain and out-ofdomain evaluation sets. In this paper, we describe the efforts in developing our supervised submissions: the initial filWe present our semantic textual similarity approach in filtering a noisy web crawled parallel corpus using YiSi—a novel semantic machine translation evaluation metric. The systems mainly based on this supervised approach perform well in the WMT18 Parallel Corpus Filtering shared task (4th place in 100-millionword evaluation, 8th place in 10-million-word evaluation, and 6th place overall, out of 48 submissions). In fact, our best performing system—"
W18-6481,D14-1045,0,0.0312771,"nformation. It uses a more principle way to compute the precision and recall of semantic similarity between the translation output and the reference when comparing to MEANT 2.0. Instead of aggregating the precision and recall at the segmental semantic similarity level, YiSi-1 srl precision is the weighted sum of the segmental semantic precision and the frame semantic precision and similarly, for YiSi-1 srl recall. The frame semantic precision is the weighted sum of the segmental semantic precision of the semantic role fillers according to the shallow semantic structure parsed by the mateplus (Roth and Woodsend, 2014) English semantic parser over the weighted counts of roles and frames according to the shallow semantic structure of the MT output and similarly, for the frame semantic recall. Precisely, YiSi-1 srl is computed as follows: We also used the monolingual English data to train word embeddings using word2vec (Mikolov et al., 2013) for evaluating monolingual lexical semantic similarity. YiSi is new a semantic MT evaluation metric inspired by MEANT 2.0 (Lo, 2017). YiSi1 is equivalent to MEANT 2.0-nosrl. It measures the segmental semantic similarity. The segmental semantic precision and recall divide"
W18-6481,2014.amta-researchers.6,1,0.700032,"pus for tuning the weights to combine different parallelism and fluency features. 1 Introduction The WMT18 shared task on parallel corpus filtering (Koehn et al., 2018b) challenged teams to find clean sentence pairs from ParaCrawl, a humongous high-recall, low-precision web crawled parallel corpus (Koehn et al., 2018a), for training machine translation (MT) systems. Data cleanliness of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) showed that different types of errors in the parallel training data degrade MT quality at different levels. Intuitively, the crosslingual semantic textual similarity of the sentence pairs in the corpora is one of the most important factors affecting the parallelism of the target sentence pairs. Lo et al. (2016) scored crosslingual 908 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 908–916 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64108 We also observ"
W19-5326,J93-2003,0,0.0825579,"to 4096 words. 5.2 We then used these SMT systems to backtranslate a ˜2M sentence subselection of monolingual English news into Kazakh and Russian, and a ˜5M sentence subselection of monolingual Russian news into Kazakh, as well as cross-translating the Russian of the ru-en parallel corpora into Kazakh. SMT Setup We trained en2kk, ru2kk and en2ru SMT systems using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system, using the corresponding BPEed parallel corpora prepared as described in Section 3. The translation model of each SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-finaland phrase extraction heuristics (Koehn et al., 2003). The systems each have two n-gram language models: a 5-gram language model (LM) (a mixture LM in the kk2en case) trained on the target-side of the corresponding parallel corpora 5.3 Building the NRC Submission System Our final submission involved several SMT components and several NMT components to produce back-translations and cross-translations needed for our multi-source submission system, as shown in Figure 3. 270 Available Resources kk-en kk-en, ru-en kk-en, kk-ru, ru-en kk-ru, ru-en kk-en, kk-ru, ru-en Source 1 kk"
W19-5326,P17-2031,0,0.068971,"Missing"
W19-5326,P14-1129,0,0.0281917,"m LM trained on the monolingual training corpora (for en2ru, trained just on news using KenLM (Heafield, 2011); for ru2kk and en2kk, a static mixture LM trained on all monolingual Kazakh data using SRILM). Each SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. NMT Setup Our code extends sockeye-1.18.72 from Hieber et al. (2017). Each source encoder has 6 layers and our decoder also has 6 layers, with a model dimension of dmodel = 512 and 2048 hidden units sub-layer feed-forward networks. We use weight tying, where the source embeddings, the target embeddings and"
W19-5326,W18-6326,0,0.0343711,"Missing"
W19-5326,W11-2123,0,0.0493711,"5) in internal testing, our submitted systems utilized the latter. (3)  V (4) where D = (d1 , d2 , · · · , dn ), di ∈ Rdmodel represents the decoder states, H = (h1 , h2 , · · · , hm ), hi ∈ Rdmodel represents the outputs of the encoder’s final self-attention layer, WiQ Rdmodel ×dk , (s) WiK ∈ Rdmodel ×dk , (s) (s) WiV ∈ ∈ 269 Figure 3: The relations of all the MT systems involved in building the NRC final submitted system. 5 5.1 Experiments and results using SRILM (Stolcke, 2002), and a pruned 6gram LM trained on the monolingual training corpora (for en2ru, trained just on news using KenLM (Heafield, 2011); for ru2kk and en2kk, a static mixture LM trained on all monolingual Kazakh data using SRILM). Each SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development"
W19-5326,W19-5358,1,0.864808,"Missing"
W19-5326,E17-3017,0,0.0887506,"t set. 268 • all monolingual Kazakh news and wiki data; • all available English from bilingual kk-en; • a sample of ˜8M English sentences from bilingual ru-en and monolingual en; • all available Russian from bilinugual kk-ru; • a sample of ˜3.2M Russian sentences from bilingual ru-en and monolingual ru. A separate vocabulary was extracted for each language using the corpora used to create the BPE model. The BPE model was then applied to all training, dev and devtest data. 4 Multi-encoder transformer We implemented a multi-source Transformer (Vaswani et al., 2017) architecture, in the Sockeye (Hieber et al., 2017) framework, that combines the output of two encoders (one for Kazakh, one for Russian); this architecture will be described in greater detail in a companion paper. Our encoder combination takes place during attention (that is, the attention step in which information from the decoder and encoders are combined, rather than the self-attention steps inside each encoder and decoder); Figure 2 illustrates the position in which the multiple sources are combined into a single representation. First, we perform multi-head scaled dot-product attention between the the decoder and each encoder separately."
W19-5326,W18-6481,1,0.81702,"iterated Kazakh, transliterated Russian, and English. Using fastBPE8 , we created a 90k-operation BPE model, balancing the three languages with ˜8.2M sentences of each, using: The raw bilingual Russian-English data we used in our systems consists of web-crawled data, news commentary data and Wikipedia article titles. In total they account for ˜14M sentence pairs. All these data were used to train the foundation systems for back-translation. Since the Paracrawl portion of the bilingual data is very noisy, before training our final submitted system we ran our parallel corpus filtering pipeline (Lo et al., 2018) with YiSi-2 as the scoring function (instead of MT + YiSi-1) and trimmed the size of the Paracrawl portion from 12M sentence pairs to 4M sentence pairs. • all available Kazakh from bilinugual kk-en; • all available Kazakh from bilinugual kk-ru; 3 github.com/moses-smt/ mosesdecoder/scripts/tokenizer/ remove-non-printing-char.perl 4 github.com/moses-smt/mosesdecoder/ scripts/tokenizer/normalize-punctuation. perl 5 github.com/moses-smt/mosesdecoder/ scripts/tokenizer/tokenizer.perl 6 en.wiktionary.org/wiki/Module: kk-translit 7 en.wiktionary.org/wiki/Module: ru-translit 8 github.com/glample/fast"
W19-5326,D11-1125,0,0.0361028,"attention layer, WiQ Rdmodel ×dk , (s) WiK ∈ Rdmodel ×dk , (s) (s) WiV ∈ ∈ 269 Figure 3: The relations of all the MT systems involved in building the NRC final submitted system. 5 5.1 Experiments and results using SRILM (Stolcke, 2002), and a pruned 6gram LM trained on the monolingual training corpora (for en2ru, trained just on news using KenLM (Heafield, 2011); for ru2kk and en2kk, a static mixture LM trained on all monolingual Kazakh data using SRILM). Each SMT system also includes a hierachical distortion model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. NMT Setup Our code extends sockeye-1.18.72 from Hieber"
W19-5326,P07-1019,0,0.0365354,"on model, a sparse feature model consisting of the standard sparse features proposed in Hopkins and May (2011) and sparse hierarchical distortion model features proposed in Cherry (2013), and a neural network joint model, or NNJM, with 3 words of target context and 11 words of source context, effectively a 15-gram LM (Vaswani et al., 2013; Devlin et al., 2014). The parameters of the log-linear model were tuned by optimizing BLEU on the development set using the batch variant of the margin infused relaxed algorithm (MIRA) by Cherry and Foster (2012). Decoding uses the cube-pruning algorithm of Huang and Chiang (2007) with a 7word distortion limit. NMT Setup Our code extends sockeye-1.18.72 from Hieber et al. (2017). Each source encoder has 6 layers and our decoder also has 6 layers, with a model dimension of dmodel = 512 and 2048 hidden units sub-layer feed-forward networks. We use weight tying, where the source embeddings, the target embeddings and the target softmax weights are tied, which implies a shared vocab. We trained employing a cross-entropy loss with Adam (Kingma and Ba, 2014), β1 = 0.9, β2 = 0.999,  = 1e − 8 and an initial learning rate of 0.0001, decreasing the learning by 0.7 each time the"
W19-5326,W18-2711,0,0.0786992,"ada (NRC) for the Kazakh-English news translation task of the Fourth Conference on Machine Translation (WMT19). Our submission is a multi-source NMT system taking both the original Kazakh sentence and its Russian translation as input for translating into English. 1 Techniques (1) and (2) both involve the translation of genuine data into a synthetic translation (into Russian in the first case, and into Kazakh in the second case). It is, however, possible to attend to both the original sentence and its translation using multi-source techniques (Zoph and Knight, 2016; Libovick´y and Helcl, 2017; Nishimura et al., 2018); we hypothesized that giving the system both the originals and “cross-translations”, in both directions (Kazakh-to-Russian and Russianto-Kazakh), would allow the system to make use of the additional information available by seeing the sources before translation. Our multi-encoder Transformer approach performed best among our submitted systems by a considerable margin, outperforming pivoting by 4.2 BLEU and augmentation by one-way crosstranslation by 10.2 BLEU.2 Introduction The WMT19 (Bojar et al., 2019) Kazakh-English News Translation task presented a machine translation scenario in which pa"
W19-5326,P16-1009,0,0.0266839,"ortion from the training data before training our final submitted system. For tuning and evaluating, we used the newsdev2019-kken data set; for SMT, we 2. Creating a synthetic Kazakh-English parallel corpus by training a Russian-Kazakh MT system and using it to “cross-translate”1 the Russian-English corpus (Fig. 1b). 3. Training a multi-encoder (Libovick´y and Helcl, 2017; Libovick´y et al., 2018) Transformer system (Vaswani et al., 2017) from 1 We term synthetic data creation by translation between source languages “cross-translation” to distinguish it from “back-translation” in the sense of Sennrich et al. (2016). Nishimura et al. (2018), which also uses source1 -to-source2 translation, calls both kinds of synthetic data creation “backtranslation”, but because our pipeline uses both kinds we distinguish them with separate terms. 2 However, these systems, as submitted, are not directly comparable due to some additional data filtering in our final submitted system; we will be releasing more direct comparisons and a more thorough description of the architecture in a companion article. 267 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 267–274"
W19-5326,N03-1017,0,0.070801,"entence subselection of monolingual English news into Kazakh and Russian, and a ˜5M sentence subselection of monolingual Russian news into Kazakh, as well as cross-translating the Russian of the ru-en parallel corpora into Kazakh. SMT Setup We trained en2kk, ru2kk and en2ru SMT systems using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system, using the corresponding BPEed parallel corpora prepared as described in Section 3. The translation model of each SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-finaland phrase extraction heuristics (Koehn et al., 2003). The systems each have two n-gram language models: a 5-gram language model (LM) (a mixture LM in the kk2en case) trained on the target-side of the corresponding parallel corpora 5.3 Building the NRC Submission System Our final submission involved several SMT components and several NMT components to produce back-translations and cross-translations needed for our multi-source submission system, as shown in Figure 3. 270 Available Resources kk-en kk-en, ru-en kk-en, kk-ru, ru-en kk-ru, ru-en kk-en, kk-ru, ru-en Source 1 kk+en2kk kk+ru+en2kk kk+ru2kk+en2kk pivoting kk+ru2kk+en2kk Training Source"
W19-5326,P18-1007,0,0.029579,"in the foundation systems for cross-translation. 3.2 For tuning and evaluating, we randomly selected 1000 sentence pairs each for the dev and devtest sets from the provided bilingual data. The remaining bilingual data is de-duplicated against the bag of 6-grams collected from the dev and devtest sets. The de-duplicated bilingual data has ˜4.2M sentence pairs. To mitigate some of the overall complexity, and allow greater sharing in joint BPE models and weight tying, we first converted the Kazakh and Russian text from Cyrillic to Roman, using official Romanization standards using spm normalize (Kudo, 2018) and transliteration tables from Wiktionary for Kazakh6 and Russian7 . 3.3 2.3 Russian-English Transliteration Byte-pair encoding Our BPE model is a joint one across transliterated Kazakh, transliterated Russian, and English. Using fastBPE8 , we created a 90k-operation BPE model, balancing the three languages with ˜8.2M sentences of each, using: The raw bilingual Russian-English data we used in our systems consists of web-crawled data, news commentary data and Wikipedia article titles. In total they account for ˜14M sentence pairs. All these data were used to train the foundation systems for b"
W19-5326,W10-1717,1,0.717548,"prove for 32 checkpoints of 1000 updates each. The inputs and output lengths were restricted to a maximum of 60 tokens, and mini-batches were of variable size depending on sentence length, with each mini-batch containing up to 4096 words. 5.2 We then used these SMT systems to backtranslate a ˜2M sentence subselection of monolingual English news into Kazakh and Russian, and a ˜5M sentence subselection of monolingual Russian news into Kazakh, as well as cross-translating the Russian of the ru-en parallel corpora into Kazakh. SMT Setup We trained en2kk, ru2kk and en2ru SMT systems using Portage (Larkin et al., 2010), a conventional log-linear phrase-based SMT system, using the corresponding BPEed parallel corpora prepared as described in Section 3. The translation model of each SMT system uses IBM4 word alignments (Brown et al., 1993) with grow-diag-finaland phrase extraction heuristics (Koehn et al., 2003). The systems each have two n-gram language models: a 5-gram language model (LM) (a mixture LM in the kk2en case) trained on the target-side of the corresponding parallel corpora 5.3 Building the NRC Submission System Our final submission involved several SMT components and several NMT components to pr"
W19-5326,N16-1004,0,0.0199026,"m developed at the National Research Council of Canada (NRC) for the Kazakh-English news translation task of the Fourth Conference on Machine Translation (WMT19). Our submission is a multi-source NMT system taking both the original Kazakh sentence and its Russian translation as input for translating into English. 1 Techniques (1) and (2) both involve the translation of genuine data into a synthetic translation (into Russian in the first case, and into Kazakh in the second case). It is, however, possible to attend to both the original sentence and its translation using multi-source techniques (Zoph and Knight, 2016; Libovick´y and Helcl, 2017; Nishimura et al., 2018); we hypothesized that giving the system both the originals and “cross-translations”, in both directions (Kazakh-to-Russian and Russianto-Kazakh), would allow the system to make use of the additional information available by seeing the sources before translation. Our multi-encoder Transformer approach performed best among our submitted systems by a considerable margin, outperforming pivoting by 4.2 BLEU and augmentation by one-way crosstranslation by 10.2 BLEU.2 Introduction The WMT19 (Bojar et al., 2019) Kazakh-English News Translation task"
W19-5358,W09-1206,0,0.1014,"Missing"
W19-5358,W17-4755,0,0.2131,"Missing"
W19-5358,W16-2302,0,0.0612144,"Missing"
W19-5358,W15-3049,0,0.151273,"Missing"
W19-5358,W14-3348,0,0.0667797,"at closely reflect the usefulness of the translation, in terms of assisting human readers to understand the meaning of the input sentence. BLEU (Papineni et al., 2002) has long been shown not to correlate well with human judgment on translation quality (Machacek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018). However, it is still the most commonly used metric for reporting quality of machine translation systems. One of the major reasons is that BLEU is ready-to-deploy to all languages due to its simplicity. Semantic MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2014) and MEANT (Lo, 2017), require additional linguistic resources to more accurately evaluate the meaning similarity between the MT output and the reference translation. The lower portability hinders the wide adoption of these metrics. We, therefore, propose a unified framework, YiSi, for MT quality evaluation and estimation that take advantage of both metric paradigms by providing options to fallback to surface-level lexiYiSi YiSi2 is a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources. Inspired by MEANT (Lo, 2017), YiSi-1 is"
W19-5358,D14-1045,0,0.018296,"an • the 9th layer of the pretrained multilingual cased BERT-Base model to represent the subword units in the reference and MT output in languages other than Chinese and English for computing YiSi-1 and to represent the subword units in the original input and MT output in all language pairs for computing YiSi2. 2.3 Experiments and results Using MATE/MATEPLUS for structural semantic similarity There are a handful of shallow semantic parsers available publicly. mate-tools (Bj¨orkelund et al., 2009) is an SVM classifier based on features extracted from a dependency parse. Its successor mateplus (Roth and Woodsend, 2014) also uses features extracted from distributional word embeddings. mate-tools and mateplus are 511 input lang. cs de output lang. en en individual best .981 .997 chrF .966 .994 BLEU .970 .971 YiSi-0 .962 .995 YiSi-1 .990 .998 YiSi-1 srl .989 .999 Quality estimation as a metric YiSi-2 .919 .946 YiSi-2 srl – .948 et en .991 .981 .986 .982 .986 .987 fi en .996 .987 .973 .986 .994 .993 ru en .995 ,990 .979 .985 .993 .993 tr en .958 .452 .657 .857 .830 .793 zh en .982 .960 .978 .972 .988 .989 en cs .999 .990 .995 .984 .993 – en de .991 .990 .981 .989 .995 .995 en et .984 .981 .975 .984 .988 – en fi"
W19-5358,W15-3031,0,0.0500861,"Missing"
W19-5358,W17-4767,1,0.937519,"he translation, in terms of assisting human readers to understand the meaning of the input sentence. BLEU (Papineni et al., 2002) has long been shown not to correlate well with human judgment on translation quality (Machacek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018). However, it is still the most commonly used metric for reporting quality of machine translation systems. One of the major reasons is that BLEU is ready-to-deploy to all languages due to its simplicity. Semantic MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2014) and MEANT (Lo, 2017), require additional linguistic resources to more accurately evaluate the meaning similarity between the MT output and the reference translation. The lower portability hinders the wide adoption of these metrics. We, therefore, propose a unified framework, YiSi, for MT quality evaluation and estimation that take advantage of both metric paradigms by providing options to fallback to surface-level lexiYiSi YiSi2 is a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources. Inspired by MEANT (Lo, 2017), YiSi-1 is a MT quality evaluati"
W19-5358,W18-6481,1,0.919465,"mation metric for languages with different levels of available resources Chi-kiu Lo NRC-CNRC Multilingual Text Processing National Research Council Canada 1200 Montreal Road, Ottawa, ON K1A 0R6, Canada chikiu.lo@nrc-cnrc.gc.ca Abstract cal similarity when semantic models are not available for the languages in assessment. YiSi were first used in WMT 2018 metrics shared task (Ma et al., 2018) and performed well and consistently at segment-level across the tested language pairs in correlating with human judgment. An YiSi based system successfully served in WMT2018 parallel corpus filtering task (Lo et al., 2018). This year, instead of using word2vec (Mikolov et al., 2013) to evaluate lexical semantic similarity in YiSi, we use BERT –Bidirectional Encoder Representation from Transformers (Devlin et al., 2018). YiSi is open source and publicly available.1 We present YiSi, a unified automatic semantic machine translation quality evaluation and estimation metric for languages with different levels of available resources. Underneath the interface with different language resources settings, YiSi uses the same representation for the two sentences in assessment. Besides, we show significant improvement in th"
W19-5358,W18-6450,0,0.558414,"gment is made by using contextual embeddings in multilingual BERT–Bidirectional Encoder Representations from Transformers to evaluate lexical semantic similarity. YiSi is open source and publicly available. 1 Introduction 2 A good automatic MT quality metric is one that closely reflect the usefulness of the translation, in terms of assisting human readers to understand the meaning of the input sentence. BLEU (Papineni et al., 2002) has long been shown not to correlate well with human judgment on translation quality (Machacek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018). However, it is still the most commonly used metric for reporting quality of machine translation systems. One of the major reasons is that BLEU is ready-to-deploy to all languages due to its simplicity. Semantic MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2014) and MEANT (Lo, 2017), require additional linguistic resources to more accurately evaluate the meaning similarity between the MT output and the reference translation. The lower portability hinders the wide adoption of these metrics. We, therefore, propose a unified framework, YiSi, for MT quality evaluation and estimatio"
W19-5358,W14-3336,0,0.0416018,"w significant improvement in the correlation of YiSi-1’s scores with human judgment is made by using contextual embeddings in multilingual BERT–Bidirectional Encoder Representations from Transformers to evaluate lexical semantic similarity. YiSi is open source and publicly available. 1 Introduction 2 A good automatic MT quality metric is one that closely reflect the usefulness of the translation, in terms of assisting human readers to understand the meaning of the input sentence. BLEU (Papineni et al., 2002) has long been shown not to correlate well with human judgment on translation quality (Machacek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018). However, it is still the most commonly used metric for reporting quality of machine translation systems. One of the major reasons is that BLEU is ready-to-deploy to all languages due to its simplicity. Semantic MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2014) and MEANT (Lo, 2017), require additional linguistic resources to more accurately evaluate the meaning similarity between the MT output and the reference translation. The lower portability hinders the wide adoption of these metrics. We, therefore, prop"
W19-5358,P02-1040,0,0.104715,"ge resources settings, YiSi uses the same representation for the two sentences in assessment. Besides, we show significant improvement in the correlation of YiSi-1’s scores with human judgment is made by using contextual embeddings in multilingual BERT–Bidirectional Encoder Representations from Transformers to evaluate lexical semantic similarity. YiSi is open source and publicly available. 1 Introduction 2 A good automatic MT quality metric is one that closely reflect the usefulness of the translation, in terms of assisting human readers to understand the meaning of the input sentence. BLEU (Papineni et al., 2002) has long been shown not to correlate well with human judgment on translation quality (Machacek and Bojar, 2014; Stanojevi´c et al., 2015; Bojar et al., 2016, 2017; Ma et al., 2018). However, it is still the most commonly used metric for reporting quality of machine translation systems. One of the major reasons is that BLEU is ready-to-deploy to all languages due to its simplicity. Semantic MT evaluation metrics, such as METEOR (Denkowski and Lavie, 2014) and MEANT (Lo, 2017), require additional linguistic resources to more accurately evaluate the meaning similarity between the MT output and t"
W19-5358,D14-1162,0,0.0819855,", β=0.1, E=MT, F =REF) YiSi-2 srl = YiSi(s=s2 , β=0.1, E=MT, F =IN) 510 2.2 Using BERT for lexical unit semantic similarity integrated into YiSi because of their support for languages other than English. We use mateplus for German’s and English’s semantic role labeling and mate-tools for Chinese’s semantic role labeling. In WMT 2018 metrics shared task, YiSi-1 uses word2vec (Mikolov et al., 2013) to evaluate lexical semantic similarity between the MT output and the human reference at word level. The shortcomings of this kind of static embedding models (also including but not limited to GloVe (Pennington et al., 2014)) is that they provide the same embedding representation for the same word without reflecting context of different sentences. In contrast, BERT (Devlin et al., 2018) uses a bidirectional transformer encoder (Vaswani et al., 2017) to capture the sentence context in the output embeddings (at subword unit level), such that the embedding for the same word/subword unit in different sentences would be different and better represented in the embedding space. Zhang et al. (2019) provided an extensive study on the performance of the output embeddings of difference layers of BERT model in correlation wi"
W19-5434,W18-6453,0,0.0203192,"n components: pre-filtering rules, sentence pair scoring, and reranking to improve vocabulary coverage. The systems vary in the way they score sentence pairs. Official results indicate our best systems were ranked 3rd or 4th out of over 20 submissions in most test settings, the ensemble system providing the most robust results. We describe the National Research Council Canada team’s submissions to the parallel corpus filtering task at the Fourth Conference on Machine Translation. 1 Introduction The WMT19 shared task on parallel corpus filtering was essentially the same as last year’s edition (Koehn et al., 2018b), except under lowresource conditions: the language pairs were Nepali-English and Sinhala-English instead of German-English, and the data participants were allowed to use was constrained. The aim of the challenge was to identify high-quality sentence pairs in a noisy corpus crawled from the web using ParaCrawl (Koehn et al., 2018a), in order to train machine translation (MT) systems on the clean data. Specifically, participating systems must produce a score for each sentence pair in the test corpora, this score indicating the quality of that pair. Then samples containing 1M or 5M words would"
W19-5434,W19-5358,1,0.797177,"l. However, there is no semantic role labeler for Nepali/Sinhalese readily available off-the-shelf, thus the version of YiSi-2 used in this work is purely based on cross-lingual lexical semantic similarity. In addition, instead of evaluating through the bag of trigrams to reward the same word order between the two sentences as in YiSi-1, YiSi-2 evaluates through the bag of unigrams to allow reordering between the two sentences in the two languages. Here is a simplified version of YiSi without using shallow semantic structures and bag of n-grams (it is the same as the original version of YiSi (Lo, 2019) with the hyperparameter β set to 0 and n to 1): • High fluency and grammaticality, especially for sentences in the output language, that constitutes translation fluency. • High vocabulary coverage, especially in the input language, which should help make the translation system more robust. • High variety of sentence lengths, which should also improve robustness. The systems developed by the NRC exploit different strategies to identify a set of sentence pairs that has these properties. The four systems shared the same pipeline architecture: 1. Initial filtering to remove specific types of nois"
W19-5434,S16-1102,1,0.867151,"Missing"
W19-5434,W18-6481,1,0.935198,"Chikiu.Lo@nrc-cnrc.gc.ca Abstract showed that different types of errors in the parallel training data degrade MT quality in different ways. Intuitively, cross-lingual semantic textual similarity is one of the most important properties of high-quality sentence pairs. Lo et al. (2016) scored cross-lingual semantic textual similarity in two ways, either using a semantic MT quality estimation metric, or by first translating one of the sentences using MT, and then comparing the result to the other sentence, using a semantic MT evaluation metric. At last year’s edition of the corpus filtering task, Lo et al. (2018)’s supervised submissions were developed in the same philosophy using a new semantic MT evaluation metric, YiSi-1. This year, the National Research Council (NRC) Canada team submitted 4 systems to the corpus filtering task, which use different strategies to evaluate the parallelism of sentence pairs. Two of these systems exploit the quality estimation metric YiSi-2, the third uses a deep Transformer network (Vaswani et al., 2017), and the fourth is an ensemble combining these approaches. In this paper, we describe the 4 systems we submitted, which have three main components: pre-filtering rule"
W19-5434,D16-1250,0,0.0425514,"n and recall in the final YiSi score. In this experiment, we set α to 0.5 for a balanced ratio of precision and recall. This year, we experimented with two methods to build the bilingual word embeddings for evaluating cross-lingual lexical semantic similarity in YiSi-2. The supervised bilingual word embeddings are trained on the parallel data provided using bivec (Luong et al., 2015). The unsupervised (weakly supervised, to be precise) bilingual word embeddings are built by transforming monolingual w2v (Mikolov et al., 2013) embeddings of each language into the same vector space using vecmap (Artetxe et al., 2016). Table 1 shows the statistics of the data used to train the two bilingual word embedding models. Common Crawl data was not used to train the bilingual word embeddings. 2.2.2 Deep Transformer Network (XLM) The other approach we tested to score sentence pairs exploits self-supervised cross-lingual language model pre-training (Lample and Conneau, 2019) of a deep Transformer network, followed by a fine-tuning stage where we teach the network to distinguish real (good) parallel sentences from bad ones. We thereby transfer over knowledge acquired from a token-level (cross-lingual) language modellin"
W19-5434,W15-1521,0,0.130389,"Missing"
W19-5434,2012.amta-papers.7,0,0.372298,"as measured by BLEU (Papineni et al., 2002). Participants were provided with a few small sources of parallel data, covering different domains, for each of the two low-resource languages, as well as a third, related language, Hindi (which uses the same script as Nepali). The provided data also included much larger monolingual corpora for each of the four languages (en, hi, ne, si). Cleanliness or quality of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) 2 System architecture There are a wide range of factors that determine whether a sentence pair is good for training MT systems. Some of the more important properties of a good training corpus include: 252 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 252–260 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics • High parallelism in the sentence pairs, that constitutes translation adequacy. 2.2.1 YiSi2 is a unified semantic MT quality evaluation and estimation metric for languages with"
W19-5434,D19-1632,0,0.0617337,"Missing"
W19-5434,P02-1040,0,0.118208,"ned. The aim of the challenge was to identify high-quality sentence pairs in a noisy corpus crawled from the web using ParaCrawl (Koehn et al., 2018a), in order to train machine translation (MT) systems on the clean data. Specifically, participating systems must produce a score for each sentence pair in the test corpora, this score indicating the quality of that pair. Then samples containing 1M or 5M words would be used to train MT systems. Participants were ranked based on the performance of these MT systems on a test set of Wikipedia translations (Guzm´an et al., 2019), as measured by BLEU (Papineni et al., 2002). Participants were provided with a few small sources of parallel data, covering different domains, for each of the two low-resource languages, as well as a third, related language, Hindi (which uses the same script as Nepali). The provided data also included much larger monolingual corpora for each of the four languages (en, hi, ne, si). Cleanliness or quality of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) 2 System"
W19-5434,W16-2323,0,0.0604627,"Missing"
W19-5434,P16-1162,0,0.0624645,"Missing"
W19-5434,2014.amta-researchers.6,0,0.560525,"Papineni et al., 2002). Participants were provided with a few small sources of parallel data, covering different domains, for each of the two low-resource languages, as well as a third, related language, Hindi (which uses the same script as Nepali). The provided data also included much larger monolingual corpora for each of the four languages (en, hi, ne, si). Cleanliness or quality of parallel corpora for MT systems is affected by a wide range of factors, e.g., the parallelism of the sentence pairs, the fluency of the sentences in the output language, etc. Previous work (Goutte et al., 2012; Simard, 2014) 2 System architecture There are a wide range of factors that determine whether a sentence pair is good for training MT systems. Some of the more important properties of a good training corpus include: 252 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 252–260 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics • High parallelism in the sentence pairs, that constitutes translation adequacy. 2.2.1 YiSi2 is a unified semantic MT quality evaluation and estimation metric for languages with different leve"
W19-5434,W16-2327,0,0.0280768,"the top-scoring sentences in the test set,10 and in the case of ne–en, these seem to contain a lot of biblical texts, which suggests a domain bias, as the ne–en fine-tuning data included biblical texts. 3.2 MT quality check 4 We used the software provided by the task organizers to extract the 1M-word and 5M-word samples from the original test corpora, using the scores of each of our 4 systems in turn. We then trained SMT systems using the extracted data. The SMT systems were trained using Portage (Larkin et al., 2010) with components and parameters similar to the German-English SMT system in Williams et al. (2016). The MT systems were then evaluated on the official dev set (“dev-test”). Table 4 shows their BLEU scores. We have also included the results of a random scoring baseline (with initial filtering and token coverage re-ranking), as well as those of Zipporah. These results show that all our BLEU scores are above the random baseline, and some of our systems outperform Zipporah when using a 1M-word sample (for both ne–en and si–en), but not when using a larger, 5M-word sample. We also see that our ensembling method produced good results on si–en, but not on ne–en, where individual systems fared bet"
W19-5434,D17-1319,0,0.118023,"subsections. 1 https://github.com/aboSamoor/pycld2 YiSi is the romanization of the Cantonese word 意思 (‘meaning’). 2 253 model supervised unsupervised lang. ne en si en ne en si en training data domain #sent IT and religious 563k IT and subtitles 647k wiki news wiki news 92k 779M 156k 779M #word 8M 5M 6M 5M 5M 13B 8M 13B dict. #pair — 9k 8k size #vocab 34k 46k 43k 33k 55k 3M 72k 3M Table 1: Statistics of data used to train the bilingual word embeddings for evaluating cross-lingual lexical semantic similarity in YiSi-2. Our use of XLM for sentence pair scoring is similar to the Zipporah system (Xu and Koehn, 2017), in that we train a model to discriminate between positive examples of actual translations and procedurally generated negative examples, then use the predicted probability of the positive class to score sentence pairs. The way we generate negative examples, which we will explain below, is also similar, but the model itself is very different. Lample and Conneau (2019) introduced selfsupervised cross-lingual language model pretraining of deep Transformer networks, and released a system called XLM (for cross-lingual language model).3 The cross-lingual LM pre-training task is similar to the maske"
Y12-1062,W05-0909,0,0.865368,"t of information for training the word vectors. The combined results suggest a new formulation of MEANT with significantly improved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word vectors are trained on a window size of 5 and use arithmetic mean to aggregate token similarity scores into segment similarity scores. We"
Y12-1062,P93-1022,0,0.214809,"rd in the lexicon is represented by a word vector, where each entry corresponds to the frequency of cooccurence with every other word in the lexicon. The definition of the cooccurence relation decides the nature of the context we capture and have been used in a wide variety of tasks, such as in word sense disambiguation by Gale et al. (1992) by defining the relation as the cooccurence within a distance of 50 words. Grammatical and syntactic relations were also identified, by defining the relation as the cooccurence in a relatively shorter window of 5 words, as in the work of Smadja (1993) and Dagan et al. (1993) . The word vector models can be readily trained on any large mono-lingual corpora 575 and hence their utility is not constrained to resource rich languages. In this work, we make a choice of defining the cooccurence relation as the joint cooccurence of the word within a short window of text, by the principle of occam’s razor. A window size of n symmetrically encomon both passes word tokens at a distance of upto (n−1) 2 directions and hence captures not only semantic context, but but also a mixture of grammatical and topical cooccurences. We make a choice of not using any techniques such as st"
Y12-1062,E06-1031,0,0.876312,"ts suggest a new formulation of MEANT with significantly improved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word vectors are trained on a window size of 5 and use arithmetic mean to aggregate token similarity scores into segment similarity scores. We explore the potential of alternate similarity metrics on wor"
Y12-1062,W11-1002,1,0.913882,"cs - cosine similarity, Min/Max with mutual information, Jensen Shannon divergence, Jaccard coefficient and the Dice’s coefficient on the word vector models as described above as criterion for aligning semantic frames in MEANT. We train the word vector models on the uncased Gigaword corpus. We do not use techniqes such as stemming, lemmatisation or stop-word pruning. We train the word vectors on the Gigaword corpus with window sizes ranging from 3 to 13. For our benchmark comparison, the evaluation data for our experiments is the same two sets of sentences, GALE-A and GALE-B that were used in Lo and Wu (2011), where in GALE-A is used for estimating the weight parameters of the metric by optimizing the correlation with human adequacy judgment, and then the learned weights are applied to testing on GALE-B. For the automatic semantic role labeling, we used the publicly available off-the-shelf shallow semantic parser, ASSERT (Pradhan et al., 2004). Semantic frame alignment is done by applying maximum bipartite matching algorithm with the lexical similrity of predicates as edge weights. The correlation with human adequacy judgments on sentence-level system ranking is assessed by the standard NIST Metri"
Y12-1062,W12-3129,1,0.844765,"ing style word alignments. Furthermore, we show empirically that a context window size of 5 captures the optimal amount of information for training the word vectors. The combined results suggest a new formulation of MEANT with significantly improved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word"
Y12-1062,1996.amta-1.13,0,0.291369,"is → − the number of word tokens in v . S(ui , vj )is the token − similarity score of the ith token in → u and the j th token → − in v obtained using any of the above mentioned token similarity metrics. 4.3 Σ max (c( x, wxi ), c(y, wy i )) 1 ΣΣS(ui , vj ) t×s i j Modified Competitive Linking In this method we attempt to align the tokens in the phrases using the similarity score of the token pair as a heuristic. As in the previous methods, we avoid the danger of aligning a token in one segment to excessive numbers of tokens in the other segment, by adopting a variant of competitive linking by Melamed (1996). The competitive linking algorithm adopts a greedy best first strategy in making strictly one to one word alignments. Since we frequently encounter phrases for alignment with unequal lengths, this one to one constraint severely restricts alignments and so we modify the competitive linking strategy by allowing one to many alignments. The number of such one to many alignments must be equal to the difference in the segment lengths. Once these alignments have been made, we compute the similarity of the two phrases as the arithmetic mean of the similarity scores of the aligned tokens. 5 Jaccard co"
Y12-1062,niessen-etal-2000-evaluation,0,0.870845,"n of MEANT with significantly improved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word vectors are trained on a window size of 5 and use arithmetic mean to aggregate token similarity scores into segment similarity scores. We explore the potential of alternate similarity metrics on word vectors such as the Jense"
Y12-1062,P02-1040,0,0.0940357,"hat a context window size of 5 captures the optimal amount of information for training the word vectors. The combined results suggest a new formulation of MEANT with significantly improved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word vectors are trained on a window size of 5 and use arithmetic mean to aggregat"
Y12-1062,N04-1030,0,0.379075,"mmatisation or stop-word pruning. We train the word vectors on the Gigaword corpus with window sizes ranging from 3 to 13. For our benchmark comparison, the evaluation data for our experiments is the same two sets of sentences, GALE-A and GALE-B that were used in Lo and Wu (2011), where in GALE-A is used for estimating the weight parameters of the metric by optimizing the correlation with human adequacy judgment, and then the learned weights are applied to testing on GALE-B. For the automatic semantic role labeling, we used the publicly available off-the-shelf shallow semantic parser, ASSERT (Pradhan et al., 2004). Semantic frame alignment is done by applying maximum bipartite matching algorithm with the lexical similrity of predicates as edge weights. The correlation with human adequacy judgments on sentence-level system ranking is assessed by the standard NIST MetricsMaTr procedure (Callison-Burch et al., 2010) using Kendall correlation coefficients. We first run a grid search on the GALE-A data set for each of these metrics on all window sizes to obtain weights for the role labels. We then use these weights to evaluate the GALE-C data set. The Kendall correlation score is obtained using MEANT as des"
Y12-1062,2006.amta-papers.25,0,0.485642,"mproved robustness across data sets. 1 Introduction We present larger-scale evidence overturning previous results, showing that the Jaccard coefficient among the alternative lexical similarity measure based on word vectors most increases the robustness of MEANT, even more than that of the Min/Max metric with mutual information metric, as used by Lo et al. (2012) in their formulation of MEANT that outperformed BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), PER (Tillmann et al., 1997), CDER 574 (Leusch et al., 2006), WER (Nießen et al., 2000), and TER (Snover et al., 2006). MEANT, the fully-automatic, state-of-the-art semantic MT evaluation metric as introduced by Lo et al. (2012) uses the Min/Max metric with mutual information on word vectors as the similarity measure to score phrasal similarity of the semantic role fillers which is the matching criterion to align semantic frames. In achieving the same, word vectors are trained on a window size of 5 and use arithmetic mean to aggregate token similarity scores into segment similarity scores. We explore the potential of alternate similarity metrics on word vectors such as the Jensen Shannon divergence, the Dice’"
Y12-1062,J93-1007,0,\N,Missing
Y12-1062,W10-1703,0,\N,Missing
