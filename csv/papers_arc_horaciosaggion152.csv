2021.inlg-1.38,Controllable Sentence Simplification with a Unified Text-to-Text Transfer Transformer,2021,-1,-1,2,0,5985,kim sheang,Proceedings of the 14th International Conference on Natural Language Generation,0,"Recently, a large pre-trained language model called T5 (A Unified Text-to-Text Transfer Transformer) has achieved state-of-the-art performance in many NLP tasks. However, no study has been found using this pre-trained model on Text Simplification. Therefore in this paper, we explore the use of T5 fine-tuning on Text Simplification combining with a controllable mechanism to regulate the system outputs that can help generate adapted text for different target audiences. Our experiments show that our model achieves remarkable results with gains of between +0.69 and +1.41 over the current state-of-the-art (BART+ACCESS). We argue that using a pre-trained model such as T5, trained on several tasks with large amounts of data, can help improve Text Simplification."
2020.trac-1.13,"{L}a{STUS}/{TALN} at {TRAC} - 2020 Trolling, Aggression and Cyberbullying",2020,-1,-1,3,0,14314,lutfiye altin,"Proceedings of the Second Workshop on Trolling, Aggression and Cyberbullying",0,"This paper presents the participation of the LaSTUS/TALN team at TRAC-2020 Trolling, Aggression and Cyberbullying shared task. The aim of the task is to determine whether a given text is aggressive and contains misogynistic content. Our approach is based on a bidirectional Long Short Term Memory network (bi-LSTM). Our system performed well at sub-task A, aggression detection; however underachieved at sub-task B, misogyny detection."
2020.lrec-1.824,A Multi-level Annotated Corpus of Scientific Papers for Scientific Document Summarization and Cross-document Relation Discovery,2020,-1,-1,2,1,2826,ahmed aburaed,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Related work sections or literature reviews are an essential part of every scientific article being crucial for paper reviewing and assessment. The automatic generation of related work sections can be considered an instance of the multi-document summarization problem. In order to allow the study of this specific problem, we have developed a manually annotated, machine readable data-set of related work sections, cited papers (e.g. references) and sentences, together with an additional layer of papers citing the references. We additionally present experiments on the identification of cited sentences, using as input citation contexts. The corpus alongside the gold standard are made available for use by the scientific community."
W19-4505,Transferring Knowledge from Discourse to Arguments: A Case Study with Scientific Abstracts,2019,0,1,2,0,24130,pablo accuosto,Proceedings of the 6th Workshop on Argument Mining,0,"In this work we propose to leverage resources available with discourse-level annotations to facilitate the identification of argumentative components and relations in scientific texts, which has been recognized as a particularly challenging task. In particular, we implement and evaluate a transfer learning approach in which contextualized representations learned from discourse parsing tasks are used as input of argument mining models. As a pilot application, we explore the feasibility of using automatically identified argumentative components and relations to predict the acceptance of papers in computer science venues. In order to conduct our experiments, we propose an annotation scheme for argumentative units and relations and use it to enrich an existing corpus with an argumentation layer."
S19-2120,{L}a{STUS}/{TALN} at {S}em{E}val-2019 Task 6: Identification and Categorization of Offensive Language in Social Media with Attention-based {B}i-{LSTM} model,2019,0,2,3,0,14314,lutfiye altin,Proceedings of the 13th International Workshop on Semantic Evaluation,0,"We present a bidirectional Long-Short Term Memory network for identifying offensive language in Twitter. Our system has been developed in the context of the SemEval 2019 Task 6 which comprises three different sub-tasks, namely A: Offensive Language Detection, B: Categorization of Offensive Language, C: Offensive Language Target Identification. We used a pre-trained Word Embeddings in tweet data, including information about emojis and hashtags. Our approach achieves good performance in the three sub-tasks."
W18-0517,{L}a{STUS}/{TALN} at Complex Word Identification ({CWI}) 2018 Shared Task,2018,0,0,2,1,2826,ahmed aburaed,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper presents the participation of the LaSTUS/TALN team in the Complex Word Identification (CWI) Shared Task 2018 in the English monolingual track . The purpose of the task was to determine if a word in a given sentence can be judged as complex or not by a certain target audience. For the English track, task organizers provided a training and a development datasets of 27,299 and 3,328 words respectively together with the sentence in which each word occurs. The words were judged as complex or not by 20 human evaluators; ten of whom are natives. We submitted two systems: one system modeled each word to evaluate as a numeric vector populated with a set of lexical, semantic and contextual features while the other system relies on a word embedding representation and a distance metric. We trained two separate classifiers to automatically decide if each word is complex or not. We submitted six runs, two for each of the three subsets of the English monolingual CWI track."
S18-1003,{S}em{E}val 2018 Task 2: Multilingual Emoji Prediction,2018,0,12,8,1,4127,francesco barbieri,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes the results of the first Shared Task on Multilingual Emoji Prediction, organized as part of SemEval 2018. Given the text of a tweet, the task consists of predicting the most likely emoji to be used along such tweet. Two subtasks were proposed, one for English and one for Spanish, and participants were allowed to submit a system run to one or both subtasks. In total, 49 teams participated to the English subtask and 22 teams submitted a system run to the Spanish subtask. Evaluation was carried out emoji-wise, and the final ranking was based on macro F-Score. Data and further information about this task can be found at \url{https://competitions.codalab.org/competitions/17344}."
S18-1115,{S}em{E}val-2018 Task 9: Hypernym Discovery,2018,0,12,9,0.276194,5213,jose camachocollados,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes the SemEval 2018 Shared Task on Hypernym Discovery. We put forward this task as a complementary benchmark for modeling hypernymy, a problem which has traditionally been cast as a binary classification task, taking a pair of candidate words as input. Instead, our reformulated task is defined as follows: given an input term, retrieve (or discover) its suitable hypernyms from a target corpus. We proposed five different subtasks covering three languages (English, Spanish, and Italian), and two specific domains of knowledge in English (Medical and Music). Participants were allowed to compete in any or all of the subtasks. Overall, a total of 11 teams participated, with a total of 39 different systems submitted through all subtasks. Data, results and further information about the task can be found at \url{https://competitions.codalab.org/competitions/17119}."
N18-2107,Multimodal Emoji Prediction,2018,39,6,4,1,4127,francesco barbieri,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"Emojis are small images that are commonly included in social media text messages. The combination of visual and textual content in the same message builds up a modern way of communication, that automatic systems are not used to deal with. In this paper we extend recent advances in emoji prediction by putting forward a multimodal approach that is able to predict emojis in Instagram posts. Instagram posts are composed of pictures together with texts which sometimes include emojis. We show that these emojis can be predicted by using the text, but also using the picture. Our main finding is that incorporating the two synergistic modalities, in a combined model, improves accuracy in an emoji prediction task. This result demonstrates that these two modalities (text and images) encode different information on the use of emojis and therefore can complement each other."
L18-1298,{PDF}digest: an Adaptable Layout-Aware {PDF}-to-{XML} Textual Content Extractor for Scientific Articles,2018,0,0,2,1,29838,daniel ferres,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Comunicacio presentada a la Language Resources and Evaluation Conference (LREC) 2018, celebrada els dies 7 a 12 de maig de 2018 a Miyazaki, Japo."
D18-1508,Interpretable Emoji Prediction via Label-Wise Attention {LSTM}s,2018,0,6,5,1,4127,francesco barbieri,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Human language has evolved towards newer forms of communication such as social media, where emojis (i.e., ideograms bearing a visual meaning) play a key role. While there is an increasing body of work aimed at the computational modeling of emoji semantics, there is currently little understanding about what makes a computational model represent or predict a given emoji in a certain way. In this paper we propose a label-wise attention mechanism with which we attempt to better understand the nuances underlying emoji prediction. In addition to advantages in terms of interpretability, we show that our proposed architecture improves over standard baselines in emoji prediction, and does particularly well when predicting infrequent emojis."
C18-3005,Data-Driven Text Simplification,2018,0,2,2,0.455545,24998,sanja vstajner,Proceedings of the 27th International Conference on Computational Linguistics: Tutorial Abstracts,0,None
W17-5406,An Adaptable Lexical Simplification Architecture for Major {I}bero-{R}omance Languages,2017,0,1,2,1,29838,daniel ferres,Proceedings of the First Workshop on Building Linguistically Generalizable {NLP} Systems,0,"Lexical Simplification is the task of reducing the lexical complexity of textual documents by replacing difficult words with easier to read (or understand) expressions while preserving the original meaning. The development of robust pipelined multilingual architectures able to adapt to new languages is of paramount importance in lexical simplification. This paper describes and evaluates a modular hybrid linguistic-statistical Lexical Simplifier that deals with the four major Ibero-Romance Languages: Spanish, Portuguese, Catalan, and Galician. The architecture of the system is the same for the four languages addressed, only the language resources used during simplification are language specific."
W17-4402,Towards the Understanding of Gaming Audiences by Modeling Twitch Emotes,2017,29,6,5,1,4127,francesco barbieri,Proceedings of the 3rd Workshop on Noisy User-generated Text,0,"Videogame streaming platforms have become a paramount example of noisy user-generated text. These are websites where gaming is broadcasted, and allows interaction with viewers via integrated chatrooms. Probably the best known platform of this kind is Twitch, which has more than 100 million monthly viewers. Despite these numbers, and unlike other platforms featuring short messages (e.g. Twitter), Twitch has not received much attention from the Natural Language Processing community. In this paper we aim at bridging this gap by proposing two important tasks specific to the Twitch platform, namely (1) Emote prediction; and (2) Trolling detection. In our experiments, we evaluate three models: a BOW baseline, a logistic supervised classifiers based on word embeddings, and a bidirectional long short-term memory recurrent neural network (LSTM). Our results show that the LSTM model outperforms the other two models, where explicit features with proven effectiveness for similar tasks were encoded."
aburaed-etal-2017-sentence,What Sentence are you Referring to and Why? Identifying Cited Sentences in Scientific Literature,2017,0,1,3,1,2826,ahmed aburaed,"Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",0,"In the current context of scientific information overload, text mining tools are of paramount importance for researchers who have to read scientific papers and assess their value. Current citation networks, which link papers by citation relationships (reference and citing paper), are useful to quantitatively understand the value of a piece of scientific work, however they are limited in that they do not provide information about what specific part of the reference paper the citing paper is referring to. This qualitative information is very important, for example, in the context of current community-based scientific summarization activities. In this paper, and relying on an annotated dataset of co-citation sentences, we carry out a number of experiments aimed at, given a citation sentence, automatically identify a part of a reference paper being cited. Additionally our algorithm predicts the specific reason why such reference sentence has been cited out of five possible reasons."
E17-2017,Are Emojis Predictable?,2017,13,16,3,1,4127,francesco barbieri,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Emojis are ideograms which are naturally combined with plain text to visually complement or condense the meaning of a message. Despite being widely used in social media, their underlying semantics have received little attention from a Natural Language Processing standpoint. In this paper, we investigate the relation between words and emojis, studying the novel task of predicting which emojis are evoked by text-based tweet messages. We train several models based on Long Short-Term Memory networks (LSTMs) in this task. Our experimental results show that our neural model outperforms a baseline as well as humans solving the same task, suggesting that computational models are able to better capture the underlying semantics of emojis."
W16-1505,Making Sense of Massive Amounts of Scientific Publications: the Scientific Knowledge Miner Project,2016,-1,-1,4,1,28736,francesco ronzano,Proceedings of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries ({BIRNDL}),0,None
W16-1520,Trainable Citation-enhanced Summarization of Scientific Articles,2016,-1,-1,1,1,5986,horacio saggion,Proceedings of the Joint Workshop on Bibliometric-enhanced Information Retrieval and Natural Language Processing for Digital Libraries ({BIRNDL}),0,None
S16-1157,"{TALN} at {S}em{E}val-2016 Task 11: Modelling Complex Words by Contextual, Lexical and Semantic Features",2016,5,3,4,1,28736,francesco ronzano,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"Comunicacio presentada al 10th International Workshop on Semantic Evaluation (SemEval 2016), celebrat els dies 16 i 17 de juny de 2016 a San Diego, EUA."
S16-1208,{TALN} at {S}em{E}val-2016 Task 14: Semantic Taxonomy Enrichment Via Sense-Based Embeddings,2016,7,0,3,1,15046,luis espinosaanke,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
L16-1492,A Multi-Layered Annotated Corpus of Scientific Papers,2016,0,6,3,1,16487,beatriz fisas,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Scientific literature records the research process with a standardized structure and provides the clues to track the progress in a scientific field. Understanding its internal structure and content is of paramount importance for natural language processing (NLP) technologies. To meet this requirement, we have developed a multi-layered annotated corpus of scientific papers in the domain of Computer Graphics. Sentences are annotated with respect to their role in the argumentative structure of the discourse. The purpose of each citation is specified. Special features of the scientific discourse such as advantages and disadvantages are identified. In addition, a grade is allocated to each sentence according to its relevance for being included in a summary.To the best of our knowledge, this complex, multi-layered collection of annotations and metadata characterizing a set of research papers had never been grouped together before in one corpus and therefore constitutes a newer, richer resource with respect to those currently available in the field."
L16-1528,{ELMD}: An Automatically Generated Entity Linking Gold Standard Dataset in the Music Domain,2016,14,11,4,0,4689,sergio oramas,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper we present a gold standard dataset for Entity Linking (EL) in the Music Domain. It contains thousands of musical named entities such as Artist, Song or Record Label, which have been automatically annotated on a set of artist biographies coming from the Music website and social network Last.fm. The annotation process relies on the analysis of the hyperlinks present in the source texts and in a voting-based algorithm for EL, which considers, for each entity mention in text, the degree of agreement across three state-of-the-art EL systems. Manual evaluation shows that EL Precision is at least 94{\%}, and due to its tunable nature, it is possible to derive annotations favouring higher Precision or Recall, at will. We make available the annotated dataset along with evaluation data and the code."
L16-1626,What does this Emoji Mean? A Vector Space Skip-Gram Model for {T}witter Emojis,2016,17,48,3,1,4127,francesco barbieri,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Emojis allow us to describe objects, situations and even feelings with small images, providing a visual and quick way to communicate. In this paper, we analyse emojis used in Twitter with distributional semantic models. We retrieve 10 millions tweets posted by USA users, and we build several skip gram word embedding models by mapping in the same vectorial space both words and emojis. We test our models with semantic similarity experiments, comparing the output of our models with human assessment. We also carry out an exhaustive qualitative evaluation, showing interesting results."
D16-1041,Supervised Distributional Hypernym Discovery via Domain Adaptation,2016,41,18,4,1,15046,luis espinosaanke,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Comunicacio presentada a la Conference on Empirical Methods in Natural Language Processing celebrada els dies 1 a 5 de novembre de 2016 a Austin, Texas."
C16-3003,Natural Language Processing for Intelligent Access to Scientific Information,2016,0,1,1,1,5986,horacio saggion,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Tutorial Abstracts",0,"During the last decade the amount of scientific information available on-line increased at an unprecedented rate. As a consequence, nowadays researchers are overwhelmed by an enormous and continuously growing number of articles to consider when they perform research activities like the exploration of advances in specific topics, peer reviewing, writing and evaluation of proposals. Natural Language Processing Technology represents a key enabling factor in providing scientists with intelligent patterns to access to scientific information. Extracting information from scientific papers, for example, can contribute to the development of rich scientific knowledge bases which can be leveraged to support intelligent knowledge access and question answering. Summarization techniques can reduce the size of long papers to their essential content or automatically generate state-of-the-art-reviews. Paraphrase or textual entailment techniques can contribute to the identification of relations across different scientific textual sources. This tutorial provides an overview of the most relevant tasks related to the processing of scientific documents, including but not limited to the in-depth analysis of the structure of the scientific articles, their semantic interpretation, content extraction and summarization."
C16-1323,Extending {W}ord{N}et with Fine-Grained Collocational Information via Supervised Distributional Learning,2016,29,2,4,1,15046,luis espinosaanke,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"WordNet is probably the best known lexical resource in Natural Language Processing. While it is widely regarded as a high quality repository of concepts and semantic relations, updating and extending it manually is costly. One important type of relation which could potentially add enormous value to WordNet is the inclusion of collocational information, which is paramount in tasks such as Machine Translation, Natural Language Generation and Second Language Learning. In this paper, we present ColWordNet (CWN), an extended WordNet version with fine-grained collocational information, automatically introduced thanks to a method exploiting linear relations between analogous sense-level embeddings spaces. We perform both intrinsic and extrinsic evaluations, and release CWN for the use and scrutiny of the community."
W15-1605,On the Discoursive Structure of Computer Graphics Research Papers,2015,26,15,2,1,16487,beatriz fisas,Proceedings of The 9th Linguistic Annotation Workshop,0,"Understanding the structure of scientific discourse is of paramount importance for the development of appropriate Natural Language Processing tools able to extract and summarize information from research articles. In this paper we present an annotated corpus of scientific discourse in the domain of Computer Graphics. We describe the way we built our corpus by designing an annotation schema and relying on three annotators for manually classifying all sentences into the defined categories. Our corpus constitutes a semantically rich resource for scientific text mining. In this respect, we also present the results of our initial experiments of automatic classification of sentences into the 5 main categories in our corpus."
S15-2119,{UPF}-taln: {S}em{E}val 2015 Tasks 10 and 11. Sentiment Analysis of Literal and Figurative Language in {T}witter,2015,15,6,3,1,4127,francesco barbieri,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"In this paper, we describe the approach used by the UPF-taln team for tasks 10 and 11 of SemEval 2015 that respectively focused on xe2x80x9cSentiment Analysis in Twitterxe2x80x9d and xe2x80x9cSentiment Analysis of Figurative Language in Twitterxe2x80x9d. Our approach achieved satisfactory results in the figurative language analysis task, obtaining the second best result. In task 10, our approach obtained acceptable performances. We experimented with both wordbased features and domain-independent intrinsic word features. We exploited two machine learning methods: the supervised algorithm Support Vector Machines for task 10, and Random-Sub-Space with M5P as base algorithm for task 11."
S15-2158,{TALN}-{UPF}: Taxonomy Learning Exploiting {CRF}-Based Hypernym Extraction on Encyclopedic Definitions,2015,17,2,2,1,15046,luis espinosaanke,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the system submitted by the TALN-UPF team to SEMEVAL Task 17 (Taxonomy Extraction Evaluation). We present a method for automatically learning a taxonomy from a flat terminology, which benefits from a definition corpus obtained by querying the BabelNet semantic network. Then, we combine a machine-learning algorithm for term-hypernym extraction with linguistically-motivated heuristics for hypernym decomposition. Our approach performs well in terms of vertex coverage and newly added vertices, while it shows room for improvement in terms of graph topology, edge coverage and precision of novel edges."
R15-1006,How Topic Biases Your Results? A Case Study of Sentiment Analysis and Irony Detection in {I}talian,2015,20,1,3,1,4127,francesco barbieri,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"In this paper we present our approach to automatically identify the subjectivity, polarity and irony of Italian Tweets. Our system which reaches and outperforms the state of the art in Italian is well adapted for different domains since it uses abstract word features instead of bag of words. We also present experiments carried out to study how Italian Sentiment Analysis systems react to domain changes. We show that bag of words approaches commonly used in Sentiment Analysis do not adapt well to domain changes."
R15-1025,Weakly Supervised Definition Extraction,2015,40,4,2,1,15046,luis espinosaanke,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,Paper presented at International Conference on Recent Advances in Natural Language Processing 2015 (RANLP 2015)
R15-1079,Translating from Original to Simplified Sentences using {M}oses: When does it Actually Work?,2015,16,0,2,0.980392,24998,sanja vstajner,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"In recent years, several studies have approached the Text Simplification (TS) task as a machine translation (MT) problem. They report promising results in learning how to translate from xe2x80x98originalxe2x80x99 to xe2x80x98simplifiedxe2x80x99 language using the standard phrasebased translation model. However, our results indicate that this approach works well only when the training dataset consists mostly of those sentence pairs in which the simplified sentence is already very similar to its original. Our findings suggest that the standard phrase-based approach might not be appropriate to learn strong simplifications which are needed for certain target populations."
R15-1080,Automatic Text Simplification for {S}panish: Comparative Evaluation of Various Simplification Strategies,2015,21,4,3,0.980392,24998,sanja vstajner,Proceedings of the International Conference Recent Advances in Natural Language Processing,0,"In this paper, we explore statistical machine translation (SMT) approaches to automatic text simplification (ATS) for Spanish. First, we compare the performances of the standard phrase-based (PB) and hierarchical (HIERO) SMT models in this specific task. In both cases, we build two models, one using the TS corpus with xe2x80x9clightxe2x80x9d simplifications and the other using the TS corpus with xe2x80x9cheavyxe2x80x9d simplifications. Next, we compare the two best systems with the state-of-the-art text simplification system for Spanish (Simplext). Our results, based on an extensive human evaluation, show that the SMT-based systems perform equally as well as, or better than, Simplext, despite the very small datasets used for training and tuning."
P15-2135,A Deeper Exploration of the Standard {PB}-{SMT} Approach to Text Simplification and its Evaluation,2015,14,9,3,0.980392,24998,sanja vstajner,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Comunicacio presentada a: the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing del 26 al 31 de juliol de 2015 a Beijing, Xina."
W14-2609,"Modelling Sarcasm in {T}witter, a Novel Approach",2014,20,65,2,1,4127,francesco barbieri,"Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Automatic detection of figurative language is a challenging task in computational linguistics. Recognising both literal and figurative meaning is not trivial for a machine and in some cases it is hard even for humans. For this reason novel and accurate systems able to recognise figurative languages are necessary. We present in this paper a novel computational model capable to detect sarcasm in the social network Twitter (a popular microblogging service which allows users to post short messages). Our model is easy to implement and, unlike previous systems, it does not include patterns of words as features. Our seven sets of lexical features aim to detect sarcasm by its inner structure (for example unexpectedness, intensity of the terms or imbalance between registers), abstracting from the use of specific terms."
W14-1201,One Step Closer to Automatic Evaluation of Text Simplification Systems,2014,36,10,3,0.980392,24998,sanja vstajner,Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations ({PITR}),0,"This study explores the possibility of replacing the costly and time-consuming human evaluation of the grammaticality and meaning preservation of the output of text simplification (TS) systems with some automatic measures. The focus is on six widely used machine translation (MT) evaluation metrics and their correlation with human judgements of grammaticality and meaning preservation in text snippets. As the results show a significant correlation between them, we go further and try to classify simplified sentences into: (1) those which are acceptable; (2) those which need minimal post-editing; and (3) those which should be discarded. The preliminary results, reported in this paper, are promising."
W14-1204,Keyword Highlighting Improves Comprehension for People with Dyslexia,2014,35,10,2,0.757576,34744,luz rello,Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations ({PITR}),0,"The use of certain font types and sizes improve the reading performance of people with dyslexia. However, the impact of combining such features with the semantics of the text has not yet been studied. In this eye-tracking study with 62 people (31 with dyslexia), we explore whether highlighting the main ideas of the text in boldface has an impact on readability and comprehensibility. We found that highlighting"
saggion-2014-creating,Creating Summarization Systems with {SUMMA},2014,22,8,1,1,5986,horacio saggion,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Automatic text summarization, the reduction of a text to its essential content is fundamental for an on-line information society. Although many summarization algorithms exist, there are few tools or infrastructures providing capabilities for developing summarization applications. This paper presents a new version of SUMMA, a text summarization toolkit for the development of adaptive summarization applications. SUMMA includes algorithms for computation of various sentence relevance features and functionality for single and multidocument summarization in various languages. It also offers methods for content-based evaluation of summaries."
barbieri-saggion-2014-modelling-irony,Modelling Irony in {T}witter: Feature Analysis and Evaluation,2014,24,18,2,1,4127,francesco barbieri,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Irony, a creative use of language, has received scarce attention from the computational linguistics research point of view. We propose an automatic system capable of detecting irony with good accuracy in the social network Twitter. Twitter allows users to post short messages (140 characters) which usually do not follow the expected rules of the grammar, users tend to truncate words and use particular punctuation. For these reason automatic detection of Irony in Twitter is not trivial and requires specific linguistic tools. We propose in this paper a new set of experiments to assess the relevance of the features included in our model. Our model does not include words or sequences of words as features, aiming to detect inner characteristic of Irony."
bautista-saggion-2014-numerical,Can Numerical Expressions Be Simpler? Implementation and Demostration of a Numerical Simplification System for {S}panish,2014,24,3,2,0,31769,susana bautista,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Information in newspapers is often showed in the form of numerical expressions which present comprehension problems for many people, including people with disabilities, illiteracy or lack of access to advanced technology. The purpose of this paper is to motivate, describe, and demonstrate a rule-based lexical component that simplifies numerical expressions in Spanish texts. We propose an approach that makes news articles more accessible to certain readers by rewriting difficult numerical expressions in a simpler way. We will showcase the numerical simplification system with a live demo based on the execution of our components over different texts, and which will consider both successful and unsuccessful simplification cases."
E14-3007,Modelling Irony in {T}witter,2014,26,48,2,1,4127,francesco barbieri,Proceedings of the Student Research Workshop at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Computational creativity is one of the central research topics of Artificial Intelligence and Natural Language Processing today. Irony, a creative use of language, has received very little attention from the computational linguistics research point of view. In this study we investigate the automatic detection of irony casting it as a classification problem. We propose a model capable of detecting irony in the social network Twitter. In cross-domain classification experiments our model based on lexical features outperforms a word-based baseline previously used in opinion mining and achieves state-of-the-art performance. Our features are simple to implement making the approach easily replicable."
N13-1027,Unsupervised Learning Summarization Templates from Concise Summaries,2013,28,5,1,1,5986,horacio saggion,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We here present and compare two unsupervised approaches for inducing the main conceptual information in rather stereotypical summaries in two different languages. We evaluate the two approaches in two different information extraction settings: monolingual and cross-lingual information extraction. The extraction systems are trained on auto-annotated summaries (containing the induced concepts) and evaluated on humanannotated documents. Extraction results are promising, being close in performance to those achieved when the system is trained on human-annotated summaries."
I13-1043,Readability Indices for Automatic Evaluation of Text Simplification Systems: A Feasibility Study for {S}panish,2013,25,8,2,0.980392,24998,sanja vstajner,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"This paper addresses the problem of automatic evaluation of text simplification systems for Spanish. We test whether already-existing readability formulae would be suitable for this task. We adapt three existing readability indices (two measuring lexical complexity and one measuring syntactic complexity) to be computed automatically, which are then applied to a corpus of original news texts and their manual simplifications aimed at people with cognitive disabilities. We show that there is a significant correlation between each of the three readability indices and several linguistically motivated features which might be seen as reading obstacles for various target populations. Furthermore, we show that there is a significant correlation between the two readability indices which measure lexical complexity."
W12-3003,Unsupervised Content Discovery from Concise Summaries,2012,20,1,1,1,5986,horacio saggion,Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction ({AKBC}-{WEKEX}),0,Domain adaptation is a time consuming and costly procedure calling for the development of algorithms and tools to facilitate its automation. This paper presents an unsupervised algorithm able to learn the main concepts in event summaries. The method takes as input a set of domain summaries annotated with shallow linguistic information and produces a domain template. We demonstrate the viability of the method by applying it to three different domains and two languages. We have evaluated the generated templates against human templates obtaining encouraging results.
W12-2910,A Hybrid System for {S}panish Text Simplification,2012,28,7,2,1,32029,stefan bott,Proceedings of the Third Workshop on Speech and Language Processing for Assistive Technologies,0,"This paper addresses the problem of automatic text simplification. Automatic text simplifications aims at reducing the reading difficulty for people with cognitive disability, among other target groups. We describe an automatic text simplification system for Spanish which combines a rule based core module with a statistical support module that controls the application of rules in the wrong contexts. Our system is integrated in a service architecture which includes a web service and mobile applications."
W12-2202,Towards Automatic Lexical Simplification in {S}panish: An Empirical Study,2012,17,17,2,0,42357,biljana drndarevic,Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations,0,"In this paper we present the results of the analysis of a parallel corpus of original and simplified texts in Spanish, gathered for the purpose of developing an automatic simplification system for this language. The system is intended for individuals with cognitive disabilities who experience difficulties reading and interpreting informative texts. We here concentrate on lexical simplification operations applied by human editors on the basis of which we derive a set of rules to be implemented automatically. We have so far addressed the issue of lexical units substitution, with special attention to reporting verbs and adjectives of nationality; insertion of definitions; simplification of numerical expressions; and simplification of named entities."
W12-2204,Graphical Schemes May Improve Readability but Not Understandability for People with Dyslexia,2012,25,13,2,0.757576,34744,luz rello,Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations,0,"Generally, people with dyslexia are poor readers but strong visual thinkers. The use of graphical schemes for helping text comprehension is recommended in education manuals. This study explores the relation between text readability and the visual conceptual schemes which aim to make the text more clear for these specific target readers. Our results are based on a user study for Spanish native speakers through a group of twenty three dyslexic users and a control group of similar size. The data collected from our study combines qualitative data from questionnaires and quantitative data from tests carried out using eye tracking. The findings suggest that graphical schemes may help to improve readability for dyslexics but are, unexpectedly, counter-productive for understandability."
saggion-szasz-2012-concisus,The {CONCISUS} Corpus of Event Summaries,2012,20,12,1,1,5986,horacio saggion,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Text summarization and information extraction systems require adaptation to new domains and languages. This adaptation usually depends on the availability of language resources such as corpora. In this paper we present a comparable corpus in Spanish and English for the study of cross-lingual information extraction and summarization: the CONCISUS Corpus. It is a rich human-annotated dataset composed of comparable event summaries in Spanish and English covering four different domains: aviation accidents, rail accidents, earthquakes, and terrorist attacks. In addition to the monolingual summaries in English and Spanish, we provide automatic translations and ``comparable'' full event reports of the events. The human annotations are concepts marked in the textual sources representing the key event information associated to the event type. The dataset has also been annotated using text processing pipelines. It is being made freely available to the research community for research purposes."
bott-etal-2012-text,Text Simplification Tools for {S}panish,2012,19,20,2,1,32029,stefan bott,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper we describe the development of a text simplification system for Spanish. Text simplification is the adaptation of a text to the special needs of certain groups of readers, such as language learners, people with cognitive difficulties and elderly people, among others. There is a clear need for simplified texts, but manual production and adaptation of existing texts is labour intensive and costly. Automatic simplification is a field which attracts growing attention in Natural Language Processing, but, to the best of our knowledge, there are no simplification tools for Spanish. We present a prototype for automatic simplification, which shows that the most important structural simplification operations can be successfully treated with an approach based on rules which can potentially be improved by statistical methods. For the development of this prototype we carried out a corpus study which aims at identifying the operations a text simplification system needs to carry out in order to produce an output similar to what human editors produce when they simplify texts."
C12-1023,Can {S}panish Be Simpler? {L}ex{S}i{S}: Lexical Simplification for {S}panish,2012,33,39,4,1,32029,stefan bott,Proceedings of {COLING} 2012,0,"Lexical simplification is the task of replacing a word in a given context by an easier-to-understand synonym. Although a number of lexical simplification approaches have been developed in recent years, most of them have been applied to English, with recent work taking advantage of parallel monolingual datasets for training. Here we present LexSiS, a lexical simplification system for Spanish that does not require a parallel corpus, but instead relies on freely available resources, such as an on-line dictionary and the Web as a corpus. LexSiS uses three techniques for finding a suitable word substitute: a word vector model, word frequency, and word length. In experiments with human informants, we have verified that LexSiS performs better than a hard-to-beat baseline based on synonym frequency."
W11-4502,Multi-domain Cross-lingual Information Extraction from Clean and Noisy Texts,2011,11,1,1,1,5986,horacio saggion,Proceedings of the 8th {B}razilian Symposium in Information and Human Language Technology,0,"We have created a human-annotated, multi-event, cross-lingual cor- pus of equivalent summaries in Spanish and English to investigate cross-lingual information extraction. The corpus contains, in addition to pairs of equivalent non-translated summaries, automatic translations of each summary produced using an available translation tool. We have developed trainable information extraction systems per language and have applied them to both original sum- maries and their automatic translations obtaining encouraging results. Resumo. Apresentamos um estudo de extracxc2xb8xcbx9c"
W11-1603,An Unsupervised Alignment Algorithm for Text Simplification Corpus Construction,2011,14,21,2,1,32029,stefan bott,Proceedings of the Workshop on Monolingual Text-To-Text Generation,0,"We present a method for the sentence-level alignment of short simplified text to the original text from which they were adapted. Our goal is to align a medium-sized corpus of parallel text, consisting of short news texts in Spanish with their simplified counterpart. No training data is available for this task, so we have to rely on unsupervised learning. In contrast to bilingual sentence alignment, in this task we can exploit the fact that the probability of sentence correspondence can be estimated from lexical similarity between sentences. We show that the algoithm employed performs better than a baseline which approaches the problem with a TF*IDF sentence similarity metric. The alignment algorithm is being used for the creation of a corpus for the study of text simplification in the Spanish language."
W10-1605,Human Language Technology for Text-based Analysis of Psychotherapy Sessions in the {S}panish Language,2010,15,1,1,1,5986,horacio saggion,Proceedings of the {NAACL} {HLT} 2010 Young Investigators Workshop on Computational Approaches to Languages of the {A}mericas,0,"We present work in progress in the application of Natural Language Processing (NLP) technology to the analysis of textual transcriptions of psychotherapy sessions in the Spanish Language. We are developing a set of NLP tools as well as adapting an existing dictionary for the analysis of interviews framed on a psychoanalytic theory. We investigate the application of NLP techniques, including dictionary-based interpretation, and speech act identification and classification for the (semi) automatic identification in text of a set of psychoanalytical variables. The objective of the work is to provide a set of tools and resources to assist therapist during discourse analysis."
W10-0213,Experiments on Summary-based Opinion Classification,2010,32,15,2,0,25381,elena lloret,Proceedings of the {NAACL} {HLT} 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,0,We investigate the effect of text summarisation in the problem of rating-inference -- the task of associating a fine-grained numerical rating to an opinionated document. We set-up a comparison framework to study the effect of different summarisation algorithms of various compression rates in this task and compare the classification accuracy of summaries and documents for associating documents to classes. We make use of SVM algorithms to associate numerical ratings to opinionated documents. The algorithms are informed by linguistic and sentiment-based features computed from full documents and summaries. Preliminary results show that some types of summaries could be as effective or better as full documents in this problem.
saggion-etal-2010-nlp,{NLP} Resources for the Analysis of Patient/Therapist Interviews,2010,10,1,1,1,5986,horacio saggion,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,We present a set of tools and resources for the analysis of interviews during psychotherapy sessions. One of the main components of the work is a dictionary-based text interpretation tool for the Spanish language. The tool is designed to identify a subset of Freudian drives in patient and therapist discourse.
saggion-funk-2010-interpreting,Interpreting {S}enti{W}ord{N}et for Opinion Classification,2010,16,22,1,1,5986,horacio saggion,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We describe a set of tools, resources, and experiments for opinion classification in business-related datasources in two languages. In particular we concentrate on SentiWordNet text interpretation to produce word, sentence, and text-based sentiment features for opinion classification. We achieve good results in experiments using supervised learning machine over syntactic and sentiment-based features. We also show preliminary experiments where the use of summaries before opinion classification provides competitive advantage over the use of full documents."
C10-2122,Multilingual Summarization Evaluation without Human Models,2010,21,58,1,1,5986,horacio saggion,Coling 2010: Posters,0,"We study correlation of rankings of text summarization systems using evaluation methods with and without human models. We apply our comparison framework to various well-established content-based evaluation measures in text summarization such as coverage, Responsiveness, Pyramids and Rouge studying their associations in various text summarization tasks including generic and focus-based multi-document summarization in English and generic single-document summarization in French and Spanish. The research is carried out using a new content-based evaluation framework called Fresa to compute a variety of divergences among probability distributions."
2010.jeptalnrecital-long.25,{\\'E}valuation automatique de r{\\'e}sum{\\'e}s avec et sans r{\\'e}f{\\'e}rence,2010,-1,-1,2,0,28171,juanmanuel torresmoreno,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous {\'e}tudions diff{\'e}rentes m{\'e}thodes d{'}{\'e}valuation de r{\'e}sum{\'e} de documents bas{\'e}es sur le contenu. Nous nous int{\'e}ressons en particulier {\`a} la corr{\'e}lation entre les mesures d{'}{\'e}valuation avec et sans r{\'e}f{\'e}rence humaine. Nous avons d{\'e}velopp{\'e} FRESA, un nouveau syst{\`e}me d{'}{\'e}valuation fond{\'e} sur le contenu qui calcule les divergences entre les distributions de probabilit{\'e}. Nous appliquons notre syst{\`e}me de comparaison aux diverses mesures d{'}{\'e}valuation bien connues en r{\'e}sum{\'e} de texte telles que la Couverture, Responsiveness, Pyramids et Rouge en {\'e}tudiant leurs associations dans les t{\^a}ches du r{\'e}sum{\'e} multi-document g{\'e}n{\'e}rique (francais/anglais), focalis{\'e} (anglais) et r{\'e}sum{\'e} mono-document g{\'e}n{\'e}rique (fran{\c{c}}ais/espagnol)."
W09-2807,A Classification Algorithm for Predicting the Structure of Summaries,2009,26,14,1,1,5986,horacio saggion,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"We investigate the problem of generating the structure of short domain independent abstracts. We apply a supervised machine learning approach trained over a set of abstracts collected from abstracting services and automatically annotated with a text analysis tool. We design a set of features for learning inspired from past research in content selection, information ordering, and rhetorical analysis for training an algorithm which then predicts the discourse structure of unseen abstracts. The proposed approach to the problem which combines local and contextual features is able to predict the local structure of the abstracts in just over 60% of the cases."
yankova-etal-2008-framework,A Framework for Identity Resolution and Merging for Multi-source Information Extraction,2008,13,2,2,0,48205,milena yankova,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In the context of ontology-based information extraction, identity resolution is the process of deciding whether an instance extracted from text refers to a known entity in the target domain (e.g. the ontology). We present an ontology-based framework for identity resolution which can be customized to different application domains and extraction tasks. Rules for identify resolution, which compute similarities between target and source entities based on class information and instance properties and values, can be defined for each class in the ontology. We present a case study of the application of the framework to the problem of multi-source job vacancy extraction"
I08-2137,Introduction to Text Summarization and Other Information Access Technologies,2008,0,0,1,1,5986,horacio saggion,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,None
I08-1020,Experiments on Semantic-based Clustering for Cross-document Coreference,2008,16,13,1,1,5986,horacio saggion,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,We describe clustering experiments for cross-document coreference for the first Web People Search Evaluation. In our experiments we apply agglomerative clustering to group together documents potentially referring to the same individual. The algorithm is informed by the results of two different summarization strategies and an offthe-shelf named entity recognition component. We present different configurations of the system and show the potential of the applied techniques. We also present an analysis of the impact that semantic information and text summarization have in the clustering process.
S07-1063,{SHEF}: Semantic Tagging and Summarization Techniques Applied to Cross-document Coreference,2007,7,15,1,1,5986,horacio saggion,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,We describe experiments for the cross-document coreference task in SemEval 2007. Our cross-document coreference system uses an in-house agglomerative clustering implementation to group documents referring to the same entity. Clustering uses vector representations created by summarization and semantic tagging analysis components. We present evaluation results for four system configurations demonstrating the potential of the applied techniques.
saggion-2006-multilingual,Multilingual Multidocument Summarization Tools and Evaluation,2006,16,10,1,1,5986,horacio saggion,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,We describe a number of experiments carried out to address the problem of creating summaries from multiple sources in multiple languages. A centroid-based sentence extraction system has been developed which decides the content of the summary using texts in different languages and uses sentences from English sources alone to create the final output. We describe the evaluation of the system in the recent Multilingual Summarization Evaluation MSE 2005 using the pyramids and ROUGE methods.
saggion-gaizauskas-2006-language,Language Resources for Background Gathering,2006,5,1,1,1,5986,horacio saggion,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We describe the Cubreporter information access system which allows access to news archives through the use of natural language technology. The system includes advanced text search, question answering, summarization, and entity profiling capabilities. It has been designed taking into account the characteristics of the background gathering task."
W05-1527,{SUPPLE}: A Practical Parser for Natural Language Engineering Applications,2005,8,25,3,0,33330,robert gaizauskas,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We describe SUPPLE, a freely-available, open source natural language parsing system, implemented in Prolog, and designed for practical use in language engineering (LE) applications. SUPPLE can be run as a stand-alone application, or as a component within the GATE General Architecture for Text Engineering. SUPPLE is distributed with an example grammar that has been developed over a number of years across several LE projects. This paper describes the key characteristics of the parser and the distributed grammar."
saggion-2004-identifying,Identifying Definitions in Text Collections for Question Answering,2004,9,38,1,1,5986,horacio saggion,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"One particular type of question which was made the focus of its own subtask within the TREC2003 QA track was the definition question (xe2x80x9cWhat is X?xe2x80x9d or xe2x80x9cWho is X?xe2x80x9d). One of the main problems with this type of question is how to discriminate in vast text collections between definitional and non-definitional text passages about a particular definiendum (i.e., the term to be defined). A method will be presented that uses definition patterns and terms that co-occurr with the definiendum in on-line sources for both passage selection and definition extraction."
radev-etal-2004-mead,{MEAD} - A Platform for Multidocument Multilingual Text Summarization,2004,11,273,13,0.209269,2447,dragomir radev,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Abstract This paper describes the functionality of MEAD, a comprehensive, public domain, open source, multidocument multilingual summarization environment that has been thus far downloaded by more than 500 organizations. MEAD has been used in a variety of summarization applications ranging from summarization for mobile devices to Web page summarization within a search engine and to novelty detection."
W03-2805,Colouring Summaries {BLEU},2003,11,15,2,0,46284,katerina pastra,"Proceedings of the {EACL} 2003 Workshop on Evaluation Initiatives in Natural Language Processing: are evaluation methods, metrics and resources reusable?",0,"In this paper we attempt to apply the IBM algorithm, BLEU, to the output of four different summarizers in order to perform an intrinsic evaluation of their output. The objective of this experiment is to explore whether a metric, originally developed for the evaluation of machine translation output, could be used for assessing another type of output reliably. Changing the type of text to be evaluated by BLEU into automatically generated extracts and setting the conditions and parameters of the evaluation experiment according to the idiosyncrasies of the task, we put the feasibility of porting BLEU in different Natural Language Processing research areas under test. Furthermore, some important conclusions relevant to the resources needed for evaluating summaries have come up as a side-effect of running the whole experiment."
P03-1048,Evaluation Challenges in Large-Scale Document Summarization,2003,18,92,3,0.209269,2447,dragomir radev,Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,1,"We present a large-scale meta evaluation of eight evaluation measures for both single-document and multi-document summarizers. To this end we built a corpus consisting of (a) 100 Million automatic summaries using six summarizers and baselines at ten summary lengths in both English and Chinese, (b) more than 10,000 manual abstracts and extracts, and (c) 200 Million automatic document and summary retrievals using 20 queries. We present both qualitative and quantitative results showing the strengths and draw-backs of all evaluation methods and how they rank the different summarizers."
E03-2013,Robust Generic and Query-based Summarization,2003,7,50,1,1,5986,horacio saggion,Demonstrations,0,We present a robust summarisation system developed within the GATE architecture that makes use of robust components for semantic tagging and coreference resolution provided by GATE. Our system combines GATE components with well established statistical techniques developed for the purpose of text summarisation research. The system supports generic and query-based summarisation addressing the need for user adaptation.
E03-2014,"Event-Coreference across Multiple, Multi-lingual Sources in the Mumis Project",2003,5,4,1,1,5986,horacio saggion,Demonstrations,0,"We present our work on information extraction from multiple, multi-lingual sources for the Multimedia Indexing and Searching Environment (MUMIS), a project aiming at developing technology to produce formal annotations about essential events in multimedia programme material. The novelty of our approach consists on the use of a merging or cross-document coreference algorithm that aims at combining the output delivered by the information extraction systems."
E03-1065,{NLP} for Indexing and Retrieval of Captioned Photographs,2003,8,5,1,1,5986,horacio saggion,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present a text-based approach for the automatic indexing and retrieval of digital photographs taken at crime scenes. Our research prototype, SOCIS, goes beyond keyword-based approaches and methods that extract syntactic relations from captions; it relies on advanced Natural Language Processing techniques in order to extract relational facts. These relational facts consist of a pragmatic relation and the entities this relation connects (triples of the form: ARG1-REL- ARG2). In SOCIS, the triples are used as complex image indexing terms; however, the extraction mechanism is used not only for indexing purposes but also for image retrieval using free text queries. The retrieval mechanism computes similarity scores between query-triples and indexing-triples making use of a domain-specific ontology."
W02-0403,Using a text engineering framework to build an extendable and portable {IE}-based summarisation system,2002,11,18,3,0,25114,diana maynard,Proceedings of the {ACL}-02 Workshop on Automatic Summarization,0,"In this paper we describe how information extraction technology has been used to build a summarisation system in the domain of occupational health and safety. The core of the application is based on named entity recognition using pattern-action semantic grammar rules. Co-occurrence of the named entities is used as a criteria to identify the sentences to be included in the summary. The system is developed and automatically evaluated within the GATE framework, and can easily be extended or ported to new domains."
saggion-etal-2002-extracting,Extracting Information for Automatic Indexing of Multimedia Material,2002,12,5,1,1,5986,horacio saggion,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper discusses our work on information extraction (IE) from multi-lingual, multi-media, multi-genre Language Resources, in a domain where there are many different event types. This work is being carried out in the context of MUMIS, an EU-funded project that aims at the development of basic technology for the creation of a composite index from multiple and multi-lingual sources. Our approach to IE relies on a finite state machinery provided by GATE, a General Architecture for Text Engineering, pipelined with full syntactic analysis and discourse interpretation implemented in Prolog."
saggion-etal-2002-developing,Developing Infrastructure for the Evaluation of Single and Multi-document Summarization Systems in a Cross-lingual Environment,2002,29,29,1,1,5986,horacio saggion,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"We describe our work on the development of Language and Evaluation Resources for the evaluation of summaries in English and Chinese. The language resources include a parallel corpus of English and Chinese texts which are translations of each other, a set of queries in both languages, clusters of documents relevants to each query, sentence relevance measures for each sentence in the document clusters, and manual multi-document summaries at different compression rates. The evaluation resources consist of metrics for measuring the content of automatic summaries against reference summaries. The framework can be used in the evaluation of extractive, non-extractive, single and multi-document summarization. We focus on the resources developed that are made available for the research community."
J02-4005,Generating Indicative-Informative Summaries with {S}um{UM},2002,61,104,1,1,5986,horacio saggion,Computational Linguistics,0,"We present and evaluate SumUM, a text summarization system that takes a raw technical text as input and produces an indicative informative summary. The indicative part of the summary identifies the topics of the document, and the informative part elaborates on some of these topics according to the reader's interest. SumUM motivates the topics, describes entities, and defines concepts. It is a first step for exploring the issue of dynamic summarization. This is accomplished through a process of shallow syntactic and semantic analysis, concept identification, and text regeneration. Our method was developed through the study of a corpus of abstracts written by professional abstractors. Relying on human judgment, we have evaluated indicativeness, informativeness, and text acceptability of the automatic summaries. The results thus far indicate good performance when compared with other summarization technologies."
C02-1073,Meta-evaluation of Summaries in a Cross-lingual Environment using Content-based Metrics,2002,17,41,1,1,5986,horacio saggion,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We describe a framework for the evaluation of summaries in English and Chinese using similarity measures. The framework can be used to evaluate extractive, non-extractive, single and multi-document summarization. We focus on the resources developed that are made available for the research community."
W00-0401,Concept Identification and Presentation in the Context of Technical Text Summarization,2000,9,51,1,1,5986,horacio saggion,NAACL-ANLP 2000 Workshop: Automatic Summarization,0,"We describe a method of text summarization that produces indicative-informative abstracts for technical papers. The abstracts are generated by a process of conceptual identification, topic extraction and re-generation. We have carried out an evaluation to assess indicativeness and text acceptability relying on human judgment. The results so far indicate good performance in both tasks when compared with other summarization technologies."
P99-1078,Using Linguistic Knowledge in Automatic Abstracting,1999,10,7,1,1,5986,horacio saggion,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"We present work on the automatic generation of short indicative-informative abstracts of scientific and technical articles. The indicative part of the abstract identifies the topics of the document while the informative part of the abstract elaborate some topics according to the reader's interest by motivating the topics, describing entities and defining concepts. We have defined our method of automatic abstracting by studying a corpus professional abstracts. The method also considers the reader's interest as essential in the process of abstracting."
1994.bcs-1.7,Anaphora resolution in a machine translation system,1994,-1,-1,1,1,5986,horacio saggion,Proceedings of the Second International Conference on Machine Translation: Ten years on,0,None
