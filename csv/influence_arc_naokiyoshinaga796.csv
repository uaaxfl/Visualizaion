2020.acl-srw.27,W14-4012,0,0.0375257,"Missing"
2020.acl-srw.27,N19-1423,0,0.00598337,"tation to train the evaluation model. Moreover, the evaluation model has been reported to overfit the dialogue systems used for generating the training data. RUBER (Tao et al., 2018) is an automatic evaluation method that combines two approaches: its referenced scorer evaluates the similarity between a reference and a generated response by using the cosine similarity of their vector representations, while its unreferenced scorer, trained by negative sampling, evaluates the relevance between an input utterance and a generated response. Ghazarian et al. (2019) showed that use of BERT embedding (Devlin et al., 2019) in pretrained vectors improves the unreferenced scorer but not the referenced scorer in RUBER . the referenced scorer is similar to ∆ BLEU in that they both are referenced-based evaluation metrics. We later confirm that the referenced scorer in RUBER underperforms our method, and we thus propose replacing it with our method (§ 5.5). 3 Preliminaries This section reviews ∆BLEU (Galley et al., 2015), a human-aided evaluation method for text generation tasks with uncertain outputs, after explaining the underlying metric, BLEU (Papineni et al., 2002). 1 Perplexity is sometimes used to evaluate dia"
2020.acl-srw.27,P15-2073,0,0.126082,"or text generation tasks, such as BLEU (Papineni et al., 2002), correlate poorly with human judgment on evaluating responses generated by dialogue systems (Liu et al., 2016). In open-domain dialogues, even though responses with various contents and styles are acceptable (Sato et al., 2017), only a few responses, or often only one, are available as reference responses in evaluation datasets made from actual conversations. It is, therefore, hard for these reference-based metrics to consider uncertain responses without writing additional reference responses by hand (§ 2). To remedy this problem, Galley et al. (2015) proposed ∆BLEU (§ 3), a human-aided evaluation method for text generation tasks with uncertain outputs. The key idea behind ∆BLEU is to consider human judgments on reference responses with diverse quality in BLEU computation. Although ∆BLEU correlates more strongly with human judgment than BLEU does, it still requires human intervention. Therefore it cannot effectively evaluate open-domain dialogue systems in a wide range of domains. To remove the human intervention in ∆BLEU, we propose an automatic, uncertainty-aware evaluation metric, υ BLEU. This metric exploits reference responses that ar"
2020.acl-srw.27,W19-2310,0,0.309007,"ue systems. The drawback of that method is the cost of annotation to train the evaluation model. Moreover, the evaluation model has been reported to overfit the dialogue systems used for generating the training data. RUBER (Tao et al., 2018) is an automatic evaluation method that combines two approaches: its referenced scorer evaluates the similarity between a reference and a generated response by using the cosine similarity of their vector representations, while its unreferenced scorer, trained by negative sampling, evaluates the relevance between an input utterance and a generated response. Ghazarian et al. (2019) showed that use of BERT embedding (Devlin et al., 2019) in pretrained vectors improves the unreferenced scorer but not the referenced scorer in RUBER . the referenced scorer is similar to ∆ BLEU in that they both are referenced-based evaluation metrics. We later confirm that the referenced scorer in RUBER underperforms our method, and we thus propose replacing it with our method (§ 5.5). 3 Preliminaries This section reviews ∆BLEU (Galley et al., 2015), a human-aided evaluation method for text generation tasks with uncertain outputs, after explaining the underlying metric, BLEU (Papineni et al"
2020.acl-srw.27,W19-5944,0,0.213824,". This is because only a few responses, or often only one, can be used as reference responses when actual conversations are used as datasets, even though responses in open-domain dialogues can be diverse (Sato et al., 2017). To consider uncertain responses in open-domain dialogues, Sordoni et al. (2015) attempted to collect multiple reference responses from dialogue logs for each test utterance-response pair. Galley et al. (2015) improved that method by manually rating the augmented reference responses and used the ratings to perform discriminative BLEU evaluation, as detailed later in § 3.2. Gupta et al. (2019) created multiple reference responses by hand for the Daily Dialogue dataset (Li et al., 2017). Although the last two studies empirically showed that the use of human-rated or -created reference responses in evaluation improves the correlation with human judgment, it is costly to create such evaluation datasets for various domains. As for evaluation methods, ADEM (Lowe et al., 2017) learns an evaluation model that predicts human scores for given responses by using large-scale human-rated responses that are originally generated by humans or dialogue systems. The drawback of that method is the c"
2020.acl-srw.27,N19-1169,0,0.0523323,"vectors improves the unreferenced scorer but not the referenced scorer in RUBER . the referenced scorer is similar to ∆ BLEU in that they both are referenced-based evaluation metrics. We later confirm that the referenced scorer in RUBER underperforms our method, and we thus propose replacing it with our method (§ 5.5). 3 Preliminaries This section reviews ∆BLEU (Galley et al., 2015), a human-aided evaluation method for text generation tasks with uncertain outputs, after explaining the underlying metric, BLEU (Papineni et al., 2002). 1 Perplexity is sometimes used to evaluate dialogue systems (Hashimoto et al., 2019). It is only applicable, however, to generation-based dialogue systems, so we do not discuss it here, like (Liu et al., 2016). 200 3.1 BLEU BLEU (Papineni et al., 2002) calculates an evaluation score based on the number of occurrences of n-gram tokens that appear in both reference and generated response. Specifically, the score is calculated from a modified n-gram precision pn and a brevity penalty (BP): ! X 1 BLEU = BP · exp log pn , (1) N n  1 if η &gt; ρ BP = , (2) e(1−ρ/η) otherwise P P i g∈n-grams(hi ) maxj {#g (hi , ri,j )} P P pn = . i g∈n-grams(hi ) #g (hi ) (3) Here, ρ and η are the ave"
2020.acl-srw.27,D18-2012,0,0.0118811,"he reference response. Following Ritter et al. (2010) and Higashinaka et al. (2011), to use a series of dialogues as training data for the above methods, we recursively follow replies from each non-reply post to obtain a dialogue between two users that consists of at least three posts. We then randomly selected pairs of the first utterances and its replies in the obtained dialogues as our dialogue data: 2.4M pairs for training VHRED and for retrieving responses in C - BM 25, 10K pairs as validation data for VHRED, and 100 pairs as test data.2 These dialogues were tokenized with SentencePiece (Kudo and Richardson, 2018) for VHRED and with MeCab 0.996 (ipadic 2.7.0)3 202 2 To obtain HUMAN responses for evaluation, we only used dialogues whose first utterances had more than one responses. 3 https://taku910.github.io/mecab/ Metric Reference retrieval method Target to compute similarity Function to compute similarity Spearman’s ρ max min Pearson’s r max min BLEU BLEU BLEU BLEU BLEU (Only one reference response) Utterance & Response BM 25 Utterance only BM 25 Utterance & Response Cosine similarity for GloVe vector Utterance only Cosine similarity for GloVe vector .186 .257 .265 .280 .333 .276 .298 .296 .322 .366"
2020.acl-srw.27,I17-1099,0,0.0554827,"n actual conversations are used as datasets, even though responses in open-domain dialogues can be diverse (Sato et al., 2017). To consider uncertain responses in open-domain dialogues, Sordoni et al. (2015) attempted to collect multiple reference responses from dialogue logs for each test utterance-response pair. Galley et al. (2015) improved that method by manually rating the augmented reference responses and used the ratings to perform discriminative BLEU evaluation, as detailed later in § 3.2. Gupta et al. (2019) created multiple reference responses by hand for the Daily Dialogue dataset (Li et al., 2017). Although the last two studies empirically showed that the use of human-rated or -created reference responses in evaluation improves the correlation with human judgment, it is costly to create such evaluation datasets for various domains. As for evaluation methods, ADEM (Lowe et al., 2017) learns an evaluation model that predicts human scores for given responses by using large-scale human-rated responses that are originally generated by humans or dialogue systems. The drawback of that method is the cost of annotation to train the evaluation model. Moreover, the evaluation model has been repor"
2020.acl-srw.27,W04-1013,0,0.0213777,"at integrating υ BLEU into RU BER greatly improves RUBER ’s performance by providing the robustness to evaluate responses with uncertainty. 2 Related work This section introduces recent studies on evaluating open-domain dialogue systems. We focus here on model-agnostic methods than can evaluate the quality of a response for a given utterance.1 For evaluation of dialogue systems, researchers have adopted existing evaluation metrics for other text generation tasks such as machine translation and summarization. Unfortunately, referencebased metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) correlate poorly with human judgment on evaluating dialogue systems (Liu et al., 2016). This is because only a few responses, or often only one, can be used as reference responses when actual conversations are used as datasets, even though responses in open-domain dialogues can be diverse (Sato et al., 2017). To consider uncertain responses in open-domain dialogues, Sordoni et al. (2015) attempted to collect multiple reference responses from dialogue logs for each test utterance-response pair. Galley et al. (2015) improved that method by manually rating the augmented reference responses and u"
2020.acl-srw.27,D16-1230,0,0.0962143,"Missing"
2020.acl-srw.27,P17-1103,0,0.204023,"t utterance-response pair. Galley et al. (2015) improved that method by manually rating the augmented reference responses and used the ratings to perform discriminative BLEU evaluation, as detailed later in § 3.2. Gupta et al. (2019) created multiple reference responses by hand for the Daily Dialogue dataset (Li et al., 2017). Although the last two studies empirically showed that the use of human-rated or -created reference responses in evaluation improves the correlation with human judgment, it is costly to create such evaluation datasets for various domains. As for evaluation methods, ADEM (Lowe et al., 2017) learns an evaluation model that predicts human scores for given responses by using large-scale human-rated responses that are originally generated by humans or dialogue systems. The drawback of that method is the cost of annotation to train the evaluation model. Moreover, the evaluation model has been reported to overfit the dialogue systems used for generating the training data. RUBER (Tao et al., 2018) is an automatic evaluation method that combines two approaches: its referenced scorer evaluates the similarity between a reference and a generated response by using the cosine similarity of t"
2020.acl-srw.27,P02-1040,0,0.118386,"terms of its correlation with human judgment and that the state of the art automatic evaluation method, RUBER, is improved by integrating υ BLEU. 1 Introduction There has been increasing interest in intelligent dialogue agents such as Apple Siri, Amazon Alexa, and Google Assistant. The key to achieving higher user engagement with those dialogue agents is to support open-domain non-task-oriented dialogues to return a meaningful response for any user input. The major challenge in developing open-domain dialogue systems is that existing evaluation metrics for text generation tasks, such as BLEU (Papineni et al., 2002), correlate poorly with human judgment on evaluating responses generated by dialogue systems (Liu et al., 2016). In open-domain dialogues, even though responses with various contents and styles are acceptable (Sato et al., 2017), only a few responses, or often only one, are available as reference responses in evaluation datasets made from actual conversations. It is, therefore, hard for these reference-based metrics to consider uncertain responses without writing additional reference responses by hand (§ 2). To remedy this problem, Galley et al. (2015) proposed ∆BLEU (§ 3), a human-aided evalu"
2020.acl-srw.27,D14-1162,0,0.0903917,"The model with parameters that achieved the minimum loss on the validation data was used for evaluating the test data. 5.4 Response retrieval and scoring Following Galley et al. (2015), for each test example, the 15 most similar utterance-response pairs were retrieved to augment the reference response in addition to the utterance (as a parrot return) to apply ∆BLEU and υ BLEU. We retrieved utteranceresponse pairs from approximately 16M utteranceresponse pairs of our dialogue data (Table 1). These dialogue data were tokenized with MeCab for response retrieval; we then trained GloVe embeddings (Pennington et al., 2014) to compute utterance or response vectors (§ 4.1) from this data. We then judged the quality of each retrieved reference response by humans for ∆BLEU and by NN-rater for υ BLEU in terms of appropriateness as a response to a given utterance. We asked four of the six Japanese native speakers to judge the quality of each retrieved reference response. 5.5 Compared response evaluation methods We have so far proposed two modifications to improve and automate ∆BLEU: more diverse reference retrieval (§ 4.1) and automatic reference quality judgment (§ 4.2). To see the impact of each modification, we fi"
2020.acl-srw.27,P17-3020,1,0.887706,"as Apple Siri, Amazon Alexa, and Google Assistant. The key to achieving higher user engagement with those dialogue agents is to support open-domain non-task-oriented dialogues to return a meaningful response for any user input. The major challenge in developing open-domain dialogue systems is that existing evaluation metrics for text generation tasks, such as BLEU (Papineni et al., 2002), correlate poorly with human judgment on evaluating responses generated by dialogue systems (Liu et al., 2016). In open-domain dialogues, even though responses with various contents and styles are acceptable (Sato et al., 2017), only a few responses, or often only one, are available as reference responses in evaluation datasets made from actual conversations. It is, therefore, hard for these reference-based metrics to consider uncertain responses without writing additional reference responses by hand (§ 2). To remedy this problem, Galley et al. (2015) proposed ∆BLEU (§ 3), a human-aided evaluation method for text generation tasks with uncertain outputs. The key idea behind ∆BLEU is to consider human judgments on reference responses with diverse quality in BLEU computation. Although ∆BLEU correlates more strongly wit"
2020.acl-srw.27,N15-1020,0,0.0671581,"Missing"
2020.findings-emnlp.381,2020.acl-main.688,0,0.0869393,"since the vocabulary size of an NMT model is limited due to practical requirements (e.g., GPU memory) (Jean et al., 2015; Luong et al., 2015). The current standard approach to the unknown word problem is to use token units shorter than words such as characters (Ling et al., 2015; Luong and Manning, 2016) and subwords (Sennrich et al., 2016b; Kudo, 2018) to handle rare words as a sequence of known tokens. However, more drastic semantic shifts will occur for characters or subwords than for words because they are shorter than words and naturally ambiguous. Besides these studies mentioned above, Aji et al. (2020) reported that transferring embeddings and vocabulary mismatches between parent and child models significantly affected the performance of models also in cross-lingual transfer learning. In this study, we aim to provide pre-trained NMT models with functionality that directly handles both target-domain-specific unknown words and semantic shifts by exploiting cross-domain embeddings learned from target-domain data. 4270 3 Vocabulary Adaptation for Domain Adaptation in NMT As we have discussed (§ 1), vocabulary mismatches between source and target domains are the important challenge in domain ada"
2020.findings-emnlp.381,D19-1165,0,0.168213,"main adaptation in NMT using cross-domain embedding projection. Introduction The performance of neural machine translation (NMT) models remarkably drops in domains different from the training data (Koehn and Knowles, 2017). Since a massive amount of parallel data is available only in a limited number of domains, domain adaptation is often required to employ NMT in practical applications. Researchers have therefore developed fine-tuning, a dominant approach for this problem (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Thompson et al., 2018; Khayrallah et al., 2018; Bapna and Firat, 2019) (§ 2). Assuming a massive amount of source-domain and small amount of target-domain parallel data, fine-tuning adjusts the parameters of a model pre-trained in the source-domain to the target domain. However, in fine-tuning, inheriting the embedding layers of the model pre-trained in the source domain causes vocabulary mismatches; namely, a model can handle neither domain-specific words that are not covered by a small amount of targetdomain parallel data (unknown words) nor words that have different meanings across domains (semantic shift). Moreover, adopting the standard subword tokenization"
2020.findings-emnlp.381,W17-4712,0,0.108371,"adapts the vocabulary of a pre-trained model to a target domain. • We showed that vocabulary adaptation exhibited additive improvements over backtranslation that uses monolingual corpora. 2 Related Work In this section, we first review two approaches to supervised domain adaptation in NMT: multi-domain learning and fine-tuning. We then introduce unsupervised domain adaptation using target-domain monolingual data and approaches to unknown word problems in NMT. Multi-domain learning induces an NMT model from parallel data in both source and target domains (Kobus et al., 2017; Wang et al., 2017; Britz et al., 2017). Since this approach requires training with a massive amount of source-domain parallel data, the training cost becomes problematic when we perform adaptation to many target domains. Fine-tuning (or continued learning) is a standard domain adaptation method in NMT. Given an NMT model pre-trained with a massive amount of sourcedomain parallel data, it continues the training of this pre-trained model with a small amount of target-domain parallel data (Luong and Manning, 2015; Chu et al., 2017; Thompson et al., 2018; Bapna and Firat, 2019; Gu et al., 2019). Due to the small cost of training, rese"
2020.findings-emnlp.381,P17-2061,0,0.037987,"Corpus (ASPEC) (Nakazawa et al., 2016) as the target domain. JESC was constructed from subtitles of movies and TV shows, while ASPEC was constructed from abstracts of scientific papers. These domains are substantially distant, and ASPEC contains many technical terms that are unknown in the JESC domain. We followed the official splitting of training, development, and test sets, except that the last 1,000,000 sentence pairs were omitted in the training set of the ASPEC corpus as they contain low-quality translations. For De→En translation, we adopted the dataset constructed by Koehn and Knowles (2017) from the OPUS corpus (Tiedemann, 2012). This dataset includes multiple domains that are distant from each other and is suitable for experiments on realistic domain adaptation. We chose the IT domain and the Law domain from the dataset as the source and target domain, respectively. We followed the same splitting of training, development, and test sets as Koehn and Knowles (2017). Preprocessing As preprocessing for the En→Ja datasets, we first tokenized the parallel data using the Moses toolkit (v4.0)1 for English sentences and KyTea (v0.4.2)2 for Japanese sentences. We 1 https://github.com/mos"
2020.findings-emnlp.381,D17-1158,0,0.0201118,"ng, 2015; Chu et al., 2017; Thompson et al., 2018; Bapna and Firat, 2019; Gu et al., 2019). Due to the small cost of training, research trends have shifted to fine-tuning from multi-domain learning. Recent studies focus on model architectures, training objectives, and strategies in training. Meanwhile, no attempts have been made to resolve the vocabulary mismatch problem in domain adaptation. Unsupervised domain adaptation exploits targetdomain monolingual data to train a language model to support the model’s decoder in generating natural sentences in a target domain (G¨ulc¸ehre et al., 2015; Domhan and Hieber, 2017). Data augmentation using back-translation (Sennrich et al., 2016a; Hu et al., 2019) is another approach to using targetdomain monolingual data. These approaches can partly address the problem of semantic shift. However, it is possible that the source-domain encoder will fail to handle targetdomain-specific words. In such cases, a decoder with the target-domain language model becomes less helpful in the former approach, and the generated pseudo-parallel corpus has low-quality sentences on the encoder side in the latter approach. Handling unknown words has been extensively studied for NMT since"
2020.findings-emnlp.381,N19-1312,0,0.0743748,"obus et al., 2017; Wang et al., 2017; Britz et al., 2017). Since this approach requires training with a massive amount of source-domain parallel data, the training cost becomes problematic when we perform adaptation to many target domains. Fine-tuning (or continued learning) is a standard domain adaptation method in NMT. Given an NMT model pre-trained with a massive amount of sourcedomain parallel data, it continues the training of this pre-trained model with a small amount of target-domain parallel data (Luong and Manning, 2015; Chu et al., 2017; Thompson et al., 2018; Bapna and Firat, 2019; Gu et al., 2019). Due to the small cost of training, research trends have shifted to fine-tuning from multi-domain learning. Recent studies focus on model architectures, training objectives, and strategies in training. Meanwhile, no attempts have been made to resolve the vocabulary mismatch problem in domain adaptation. Unsupervised domain adaptation exploits targetdomain monolingual data to train a language model to support the model’s decoder in generating natural sentences in a target domain (G¨ulc¸ehre et al., 2015; Domhan and Hieber, 2017). Data augmentation using back-translation (Sennrich et al., 2016a"
2020.findings-emnlp.381,P19-1286,0,0.0617305,"Due to the small cost of training, research trends have shifted to fine-tuning from multi-domain learning. Recent studies focus on model architectures, training objectives, and strategies in training. Meanwhile, no attempts have been made to resolve the vocabulary mismatch problem in domain adaptation. Unsupervised domain adaptation exploits targetdomain monolingual data to train a language model to support the model’s decoder in generating natural sentences in a target domain (G¨ulc¸ehre et al., 2015; Domhan and Hieber, 2017). Data augmentation using back-translation (Sennrich et al., 2016a; Hu et al., 2019) is another approach to using targetdomain monolingual data. These approaches can partly address the problem of semantic shift. However, it is possible that the source-domain encoder will fail to handle targetdomain-specific words. In such cases, a decoder with the target-domain language model becomes less helpful in the former approach, and the generated pseudo-parallel corpus has low-quality sentences on the encoder side in the latter approach. Handling unknown words has been extensively studied for NMT since the vocabulary size of an NMT model is limited due to practical requirements (e.g.,"
2020.findings-emnlp.381,P15-1001,0,0.0374845,"proach to using targetdomain monolingual data. These approaches can partly address the problem of semantic shift. However, it is possible that the source-domain encoder will fail to handle targetdomain-specific words. In such cases, a decoder with the target-domain language model becomes less helpful in the former approach, and the generated pseudo-parallel corpus has low-quality sentences on the encoder side in the latter approach. Handling unknown words has been extensively studied for NMT since the vocabulary size of an NMT model is limited due to practical requirements (e.g., GPU memory) (Jean et al., 2015; Luong et al., 2015). The current standard approach to the unknown word problem is to use token units shorter than words such as characters (Ling et al., 2015; Luong and Manning, 2016) and subwords (Sennrich et al., 2016b; Kudo, 2018) to handle rare words as a sequence of known tokens. However, more drastic semantic shifts will occur for characters or subwords than for words because they are shorter than words and naturally ambiguous. Besides these studies mentioned above, Aji et al. (2020) reported that transferring embeddings and vocabulary mismatches between parent and child models signifi"
2020.findings-emnlp.381,W18-2705,0,0.14939,"abulary adaptation for domain adaptation in NMT using cross-domain embedding projection. Introduction The performance of neural machine translation (NMT) models remarkably drops in domains different from the training data (Koehn and Knowles, 2017). Since a massive amount of parallel data is available only in a limited number of domains, domain adaptation is often required to employ NMT in practical applications. Researchers have therefore developed fine-tuning, a dominant approach for this problem (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Thompson et al., 2018; Khayrallah et al., 2018; Bapna and Firat, 2019) (§ 2). Assuming a massive amount of source-domain and small amount of target-domain parallel data, fine-tuning adjusts the parameters of a model pre-trained in the source-domain to the target domain. However, in fine-tuning, inheriting the embedding layers of the model pre-trained in the source domain causes vocabulary mismatches; namely, a model can handle neither domain-specific words that are not covered by a small amount of targetdomain parallel data (unknown words) nor words that have different meanings across domains (semantic shift). Moreover, adopting the stand"
2020.findings-emnlp.381,kobus-etal-2017-domain,0,0.0522665,"ve, model-free finetuning for NMT that adapts the vocabulary of a pre-trained model to a target domain. • We showed that vocabulary adaptation exhibited additive improvements over backtranslation that uses monolingual corpora. 2 Related Work In this section, we first review two approaches to supervised domain adaptation in NMT: multi-domain learning and fine-tuning. We then introduce unsupervised domain adaptation using target-domain monolingual data and approaches to unknown word problems in NMT. Multi-domain learning induces an NMT model from parallel data in both source and target domains (Kobus et al., 2017; Wang et al., 2017; Britz et al., 2017). Since this approach requires training with a massive amount of source-domain parallel data, the training cost becomes problematic when we perform adaptation to many target domains. Fine-tuning (or continued learning) is a standard domain adaptation method in NMT. Given an NMT model pre-trained with a massive amount of sourcedomain parallel data, it continues the training of this pre-trained model with a small amount of target-domain parallel data (Luong and Manning, 2015; Chu et al., 2017; Thompson et al., 2018; Bapna and Firat, 2019; Gu et al., 2019)."
2020.findings-emnlp.381,W17-3204,0,0.272246,"t-domain NMT model Source-domain NMT model Massive source-domain parallel data Encoder-Decoder 0) Pre-training composer conductor coil wire conductor conductor composer wire Source-domain embeddings Small target-domain parallel data 3) Fine-tuning 2) Cross-domain emb. projection Target-domain embeddings coil 1) CBoW training Target-domain monolingual data music Figure 1: Vocabulary adaptation for domain adaptation in NMT using cross-domain embedding projection. Introduction The performance of neural machine translation (NMT) models remarkably drops in domains different from the training data (Koehn and Knowles, 2017). Since a massive amount of parallel data is available only in a limited number of domains, domain adaptation is often required to employ NMT in practical applications. Researchers have therefore developed fine-tuning, a dominant approach for this problem (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Thompson et al., 2018; Khayrallah et al., 2018; Bapna and Firat, 2019) (§ 2). Assuming a massive amount of source-domain and small amount of target-domain parallel data, fine-tuning adjusts the parameters of a model pre-trained in the source-domain to the target domain."
2020.findings-emnlp.381,P18-1007,0,0.189586,"amount of source-domain and small amount of target-domain parallel data, fine-tuning adjusts the parameters of a model pre-trained in the source-domain to the target domain. However, in fine-tuning, inheriting the embedding layers of the model pre-trained in the source domain causes vocabulary mismatches; namely, a model can handle neither domain-specific words that are not covered by a small amount of targetdomain parallel data (unknown words) nor words that have different meanings across domains (semantic shift). Moreover, adopting the standard subword tokenization (Sennrich et al., 2016b; Kudo, 2018) accelerates the semantic shift. Targetdomain-specific words are often finely decomposed into source-domain subwords (e.g., “alloy” → “ all” + “o” + “y”), which introduces improper subword meanings and hinders adaptation (Table 7 in § 5). To resolve these vocabulary-mismatch problems in domain adaptation, we propose vocabulary adaptation (Figure 1), a method of directly adapting the vocabulary (and embedding layers) of a pre-trained NMT model to a target domain, to perform effective fine-tuning (§ 3). Given an NMT model pretrained in a source domain, we first induce a wide coverage of target-d"
2020.findings-emnlp.381,D18-2012,0,0.0594297,"Missing"
2020.findings-emnlp.381,2015.iwslt-evaluation.11,0,0.204804,"Missing"
2020.findings-emnlp.381,P16-1100,0,0.0474518,"Missing"
2020.findings-emnlp.381,P15-1002,0,0.0291176,"getdomain monolingual data. These approaches can partly address the problem of semantic shift. However, it is possible that the source-domain encoder will fail to handle targetdomain-specific words. In such cases, a decoder with the target-domain language model becomes less helpful in the former approach, and the generated pseudo-parallel corpus has low-quality sentences on the encoder side in the latter approach. Handling unknown words has been extensively studied for NMT since the vocabulary size of an NMT model is limited due to practical requirements (e.g., GPU memory) (Jean et al., 2015; Luong et al., 2015). The current standard approach to the unknown word problem is to use token units shorter than words such as characters (Ling et al., 2015; Luong and Manning, 2016) and subwords (Sennrich et al., 2016b; Kudo, 2018) to handle rare words as a sequence of known tokens. However, more drastic semantic shifts will occur for characters or subwords than for words because they are shorter than words and naturally ambiguous. Besides these studies mentioned above, Aji et al. (2020) reported that transferring embeddings and vocabulary mismatches between parent and child models significantly affected the p"
2020.findings-emnlp.381,W17-5708,1,0.776414,"he pretrained model in order to make the embeddings compatible with the pre-trained model (Figure 1 in § 1). This approach is inspired by cross-lingual and cross-task word embeddings that bridge word embeddings across languages and tasks. An overview of our proposed method is given as follows. Step 1 (Inducing target-domain embeddings) We induce word embeddings from monolingual data in the target domain for each language. Although we can use any method for induction, we adopt Continuous Bag-of-Words (CBOW) (Mikolov et al., 2013) here since CBOW is effective for initializing embeddings in NMT (Neishi et al., 2017), which suggests embedding spaces of CBOW and NMT are topologically similar. Step 2 (Projecting embeddings across domains) We project the target-domain embeddings of the source and target languages into the embedding spaces of the pre-trained encoder and decoder, respectively, to obtain cross-domain embeddings (§ 3.2.1, § 3.2.2). Step 3 (Fine-tuning) We replace the vocabularies and the embedding layers with the cross-domain embeddings and apply fine-tuning using the targetdomain parallel data. To induce cross-domain embedding projection, we regard the two domains as different languages/tasks a"
2020.findings-emnlp.381,N19-4009,0,0.0167898,"ortions. We then used the first half and the second half as simulated monolingual data for the source language and the target language, respectively. The monolingual data was used for training SentencePiece and CBOW vectors in the target domain and data augmentation by back-translation. When models did not use the monolingual data, the data used for training SentencePiece and CBOW vectors was exactly identical to the training set in each domain. 3 https://github.com/google/ sentencepiece Models and Embeddings We adopted Transformer-base (Vaswani et al., 2017) implemented in fairseq (v0.8.0)4 (Ott et al., 2019), as the core architecture for the NMT models.5 Major hyperparameters are shown in Table 2.6 We evaluated the performance of the models on the basis of BLEU (Papineni et al., 2002). Before pretraining the models, we induced subword embeddings from the monolingual corpus by Continuous Bag-of-Words (CBOW) (Mikolov et al., 2013) to initialize the embedding layers of the NMT models. To evaluate the effect of vocabulary adaptation, we compared the following settings (and their combinations) that used either or both the source- and target-domain parallel data. Out-/In-domain trains a model only from"
2020.findings-emnlp.381,P02-1040,0,0.107721,"ce-domain embedding space. To perform this cross-domain embedding projection, we explore two methods: cross-lingual (Xing et al., 2015) and cross-task embedding projection (Sakuma and Yoshinaga, 2019). We evaluate fine-tuning with the proposed vocabulary adaptation for two domain pairs: 1) from JESC (Pryzant et al., 2018) to ASPEC (Nakazawa et al., 2016) for English to Japanese translation (En→Ja) and 2) from the IT domain to Law domain (Koehn and Knowles, 2017) for German to English translation (De→En). Experimental results demonstrate that our vocabulary adaptation improves the BLEU scores (Papineni et al., 2002) of fine-tuning (Luong and Manning, 2015) by 3.86 points (21.45 to 25.31) for En→Ja and 3.28 points (24.59 to 27.87) for De→En (§ 5). Moreover, it shows further improvements when combined with back-translation (Sennrich et al., 2016a). The contributions of this paper are as follows. • We empirically confirmed that vocabulary mismatches hindered domain adaptation. • We established an effective, model-free finetuning for NMT that adapts the vocabulary of a pre-trained model to a target domain. • We showed that vocabulary adaptation exhibited additive improvements over backtranslation that uses m"
2020.findings-emnlp.381,P19-1021,0,0.0196164,"distinct words in the references, respectively. BLEU score 6 1.0 30 0.8 28 0.6 26 En-Ja 26.24 25.31 0.4 24 23.57 23.65 0.2 22 De-En VA-LLM (w/o monolingual) VA-LLM (w/ monolingual) 27.77 27.59 24.91 23.81 26.79 27.55 27.87 26.87 26.40 27.04 25.70 FT-srcV (baseline) 23.42 23.43 23.12 FT-srcV (baseline) 21.79 20.95 0.0 200.02k 4k 0.2 8k 16k 0.4 32k 2k 0.6 4k Target-domain vocabulary size 8k 0.8 16k 32k1.0 Figure 3: BLEU scores of VA - LLM while varying target-domain vocabulary size. The source-domain vocabulary size was fixed to 16k. 6.2 Effect of Vocabulary Size in Fine Tuning As reported in (Sennrich and Zhang, 2019), the vocabulary size of an NMT model can affect its translation quality in a low-resource setting. How about in fine-tuning? To explore this, we varied only the target-domain vocabulary size of VA-LLM before fine-tuning by vocabulary adaptation. Figure 3 shows that VA-LLM preferred large vocabulary sizes when additional target-domain monolingual data was used for training CBOW, whereas it preferred small vocabulary sizes when the data was not used. We consider the reason to be as follows. In the former case, a large vocabulary contains low-frequency subwords of which representation is unlikel"
2020.findings-emnlp.381,W18-6313,0,0.0896461,"ata music Figure 1: Vocabulary adaptation for domain adaptation in NMT using cross-domain embedding projection. Introduction The performance of neural machine translation (NMT) models remarkably drops in domains different from the training data (Koehn and Knowles, 2017). Since a massive amount of parallel data is available only in a limited number of domains, domain adaptation is often required to employ NMT in practical applications. Researchers have therefore developed fine-tuning, a dominant approach for this problem (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Thompson et al., 2018; Khayrallah et al., 2018; Bapna and Firat, 2019) (§ 2). Assuming a massive amount of source-domain and small amount of target-domain parallel data, fine-tuning adjusts the parameters of a model pre-trained in the source-domain to the target domain. However, in fine-tuning, inheriting the embedding layers of the model pre-trained in the source domain causes vocabulary mismatches; namely, a model can handle neither domain-specific words that are not covered by a small amount of targetdomain parallel data (unknown words) nor words that have different meanings across domains (semantic shift). Mor"
2020.findings-emnlp.381,tiedemann-2012-parallel,0,0.0204262,"l., 2016) as the target domain. JESC was constructed from subtitles of movies and TV shows, while ASPEC was constructed from abstracts of scientific papers. These domains are substantially distant, and ASPEC contains many technical terms that are unknown in the JESC domain. We followed the official splitting of training, development, and test sets, except that the last 1,000,000 sentence pairs were omitted in the training set of the ASPEC corpus as they contain low-quality translations. For De→En translation, we adopted the dataset constructed by Koehn and Knowles (2017) from the OPUS corpus (Tiedemann, 2012). This dataset includes multiple domains that are distant from each other and is suitable for experiments on realistic domain adaptation. We chose the IT domain and the Law domain from the dataset as the source and target domain, respectively. We followed the same splitting of training, development, and test sets as Koehn and Knowles (2017). Preprocessing As preprocessing for the En→Ja datasets, we first tokenized the parallel data using the Moses toolkit (v4.0)1 for English sentences and KyTea (v0.4.2)2 for Japanese sentences. We 1 https://github.com/moses-smt/ mosesdecoder 2 http://www.phont"
2020.findings-emnlp.381,D17-1155,0,0.0189257,"uning for NMT that adapts the vocabulary of a pre-trained model to a target domain. • We showed that vocabulary adaptation exhibited additive improvements over backtranslation that uses monolingual corpora. 2 Related Work In this section, we first review two approaches to supervised domain adaptation in NMT: multi-domain learning and fine-tuning. We then introduce unsupervised domain adaptation using target-domain monolingual data and approaches to unknown word problems in NMT. Multi-domain learning induces an NMT model from parallel data in both source and target domains (Kobus et al., 2017; Wang et al., 2017; Britz et al., 2017). Since this approach requires training with a massive amount of source-domain parallel data, the training cost becomes problematic when we perform adaptation to many target domains. Fine-tuning (or continued learning) is a standard domain adaptation method in NMT. Given an NMT model pre-trained with a massive amount of sourcedomain parallel data, it continues the training of this pre-trained model with a small amount of target-domain parallel data (Luong and Manning, 2015; Chu et al., 2017; Thompson et al., 2018; Bapna and Firat, 2019; Gu et al., 2019). Due to the small c"
2020.findings-emnlp.381,N15-1104,0,0.0759842,"Missing"
2020.findings-emnlp.381,L18-1182,0,0.141189,"November 16 - 20, 2020. 2020 Association for Computational Linguistics target-domain monolingual data. We then fit the obtained target-domain word embeddings to the embedding space of the pre-trained NMT model by inducing a cross-domain projection from the targetdomain embedding space to the source-domain embedding space. To perform this cross-domain embedding projection, we explore two methods: cross-lingual (Xing et al., 2015) and cross-task embedding projection (Sakuma and Yoshinaga, 2019). We evaluate fine-tuning with the proposed vocabulary adaptation for two domain pairs: 1) from JESC (Pryzant et al., 2018) to ASPEC (Nakazawa et al., 2016) for English to Japanese translation (En→Ja) and 2) from the IT domain to Law domain (Koehn and Knowles, 2017) for German to English translation (De→En). Experimental results demonstrate that our vocabulary adaptation improves the BLEU scores (Papineni et al., 2002) of fine-tuning (Luong and Manning, 2015) by 3.86 points (21.45 to 25.31) for En→Ja and 3.28 points (24.59 to 27.87) for De→En (§ 5). Moreover, it shows further improvements when combined with back-translation (Sennrich et al., 2016a). The contributions of this paper are as follows. • We empirically"
2020.findings-emnlp.381,K19-1003,1,0.940215,"target-domain word embeddings from 4269 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4269–4279 c November 16 - 20, 2020. 2020 Association for Computational Linguistics target-domain monolingual data. We then fit the obtained target-domain word embeddings to the embedding space of the pre-trained NMT model by inducing a cross-domain projection from the targetdomain embedding space to the source-domain embedding space. To perform this cross-domain embedding projection, we explore two methods: cross-lingual (Xing et al., 2015) and cross-task embedding projection (Sakuma and Yoshinaga, 2019). We evaluate fine-tuning with the proposed vocabulary adaptation for two domain pairs: 1) from JESC (Pryzant et al., 2018) to ASPEC (Nakazawa et al., 2016) for English to Japanese translation (En→Ja) and 2) from the IT domain to Law domain (Koehn and Knowles, 2017) for German to English translation (De→En). Experimental results demonstrate that our vocabulary adaptation improves the BLEU scores (Papineni et al., 2002) of fine-tuning (Luong and Manning, 2015) by 3.86 points (21.45 to 25.31) for En→Ja and 3.28 points (24.59 to 27.87) for De→En (§ 5). Moreover, it shows further improvements when"
2020.findings-emnlp.381,P16-1009,0,0.537825,"§ 2). Assuming a massive amount of source-domain and small amount of target-domain parallel data, fine-tuning adjusts the parameters of a model pre-trained in the source-domain to the target domain. However, in fine-tuning, inheriting the embedding layers of the model pre-trained in the source domain causes vocabulary mismatches; namely, a model can handle neither domain-specific words that are not covered by a small amount of targetdomain parallel data (unknown words) nor words that have different meanings across domains (semantic shift). Moreover, adopting the standard subword tokenization (Sennrich et al., 2016b; Kudo, 2018) accelerates the semantic shift. Targetdomain-specific words are often finely decomposed into source-domain subwords (e.g., “alloy” → “ all” + “o” + “y”), which introduces improper subword meanings and hinders adaptation (Table 7 in § 5). To resolve these vocabulary-mismatch problems in domain adaptation, we propose vocabulary adaptation (Figure 1), a method of directly adapting the vocabulary (and embedding layers) of a pre-trained NMT model to a target domain, to perform effective fine-tuning (§ 3). Given an NMT model pretrained in a source domain, we first induce a wide covera"
2020.findings-emnlp.381,L16-1350,0,\N,Missing
2020.findings-emnlp.381,P16-1162,0,\N,Missing
2020.findings-emnlp.434,Q17-1010,0,0.0437423,"BERT except for one POS dataset and robustness to adversarial perturbations on a sentiment dataset. 2 Related work Existing approaches for leveraging surface information in computing OOV word embeddings basically learn the embeddings of characters or subwords to reconstruct pre-trained word embeddings from them and then use the obtained embeddings to compute embeddings for OOV words (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019). Zhao et al. (2018) proposed Bag-of-Subwords (BoS) to reconstruct pre-trained word embeddings from bag-ofcharacter n-grams in the same way as fastText (Bojanowski et al., 2017). Sasaki et al. (2019) extended BoS to reduce the number of embedding vectors and introduce a self-attention mechanism into the aggregation of subword embeddings. However, these methods compute embeddings via ambiguous character or subword embeddings. This will degrade the quality of embeddings for target OOV words as we will confirm later in § 4. Other approaches utilize the embeddings of the known words around a target OOV word as its contextual information (Lazaridou et al., 2017; Khodak et al., 2018; Schick and Sch¨utze, 2019; Hu et al., 2019). Schick and Sch¨utze (2020) reported that they"
2020.findings-emnlp.434,W17-4418,0,0.0546274,"Missing"
2020.findings-emnlp.434,R13-1026,0,0.0722584,"Missing"
2020.findings-emnlp.434,N19-1423,0,0.644855,"ratories. 34.2 Table 1: Cosine similarity between Glove.840B embedding of “higher” and related embeddings: subword and reconstructed embeddings of “higher” by BoS (Zhao et al., 2018) and Glove.840B embedding of “high.” Introduction ∗ 48.4 proaches are subject to the noisiness and ambiguity of intermediate subwords. For example, the Glove.840B1 embedding of “higher” is closer to “high” compared with the embedding of “higher” reconstructed from its subwords using the method of BoS (Zhao et al., 2018), due to the ambiguous subword er> as shown in Table 1. Contextual word embeddings such as BERT (Devlin et al., 2019) can mitigate subword ambiguity by considering context. However, it has been reported that adversarial typos can degrade a BERT model that uses subword tokenization (Pruthi et al., 2019; Sun et al., 2020). Subword meanings change across domains, making domain adaptation difficult (Sato et al., 2020). These problems are more critical in the processing of noisy text (Wang et al., 2020; Niu et al., 2020). To solve the above problems, we propose di1 http://nlp.stanford.edu/data/glove. 840B.300d.zip 4827 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4827–4838 c Novemb"
2020.findings-emnlp.434,W19-4407,0,0.0783769,"-based fine-tuning model. In essence, the proposed method directly computes OOV embeddings from pre-trained word embeddings, whereas the existing methods compute indirectly via their subwords. To investigate the performance of the proposed method against baseline approaches, we conduct both intrinsic and extrinsic evaluations of OOV word embeddings (§ 4). In the experiments for the intrinsic evaluation, we examine the performance of the proposed method in inducing embeddings for rare words by using the CARD benchmark (Pilehvar et al., 2018) and for misspelled words by using the TOEFL dataset (Flor et al., 2019). Then, in those for the extrinsic evaluation, we demonstrate the effectiveness of the calculated OOV word embeddings in two downstream tasks, named entity recognition (NER) and part-of-speech (POS) tagging for Twitter and biomedical domains, where OOV words frequently appear. We finally evaluate the BERT-based fine-tuning model with our method on these tasks and adversarial perturbations (Sun et al., 2020). The contributions of this work are as follows. • We propose a robust backed-off approach for estimating OOV word embeddings, inspired by two processes for creating words: compounding and d"
2020.findings-emnlp.434,P11-2008,0,0.0216899,"Missing"
2020.findings-emnlp.434,P19-1402,0,0.0150094,"r n-grams in the same way as fastText (Bojanowski et al., 2017). Sasaki et al. (2019) extended BoS to reduce the number of embedding vectors and introduce a self-attention mechanism into the aggregation of subword embeddings. However, these methods compute embeddings via ambiguous character or subword embeddings. This will degrade the quality of embeddings for target OOV words as we will confirm later in § 4. Other approaches utilize the embeddings of the known words around a target OOV word as its contextual information (Lazaridou et al., 2017; Khodak et al., 2018; Schick and Sch¨utze, 2019; Hu et al., 2019). Schick and Sch¨utze (2020) reported that they can improve BERT (Devlin et al., 2019) for understanding rare words. Notably, in these approaches for utilizing both surface and context information, the surface-based embeddings are the same as (Zhao et al., 2018). These approaches can have difficulties in representing misspelled words or spelling variations when a small number of contexts are available in a text corpus. Several approaches utilize external data such as a knowledge base (Bahdanau et al., 2018; Yang et al., 2019; Yao et al., 2019). Existing approaches successfully impute OOV word"
2020.findings-emnlp.434,P18-1002,0,0.0222066,"pre-trained word embeddings from bag-ofcharacter n-grams in the same way as fastText (Bojanowski et al., 2017). Sasaki et al. (2019) extended BoS to reduce the number of embedding vectors and introduce a self-attention mechanism into the aggregation of subword embeddings. However, these methods compute embeddings via ambiguous character or subword embeddings. This will degrade the quality of embeddings for target OOV words as we will confirm later in § 4. Other approaches utilize the embeddings of the known words around a target OOV word as its contextual information (Lazaridou et al., 2017; Khodak et al., 2018; Schick and Sch¨utze, 2019; Hu et al., 2019). Schick and Sch¨utze (2020) reported that they can improve BERT (Devlin et al., 2019) for understanding rare words. Notably, in these approaches for utilizing both surface and context information, the surface-based embeddings are the same as (Zhao et al., 2018). These approaches can have difficulties in representing misspelled words or spelling variations when a small number of contexts are available in a text corpus. Several approaches utilize external data such as a knowledge base (Bahdanau et al., 2018; Yang et al., 2019; Yao et al., 2019). Exis"
2020.findings-emnlp.434,N16-1030,0,0.07291,"Missing"
2020.findings-emnlp.434,2020.acl-main.755,0,0.0275299,"mbedding of “higher” reconstructed from its subwords using the method of BoS (Zhao et al., 2018), due to the ambiguous subword er> as shown in Table 1. Contextual word embeddings such as BERT (Devlin et al., 2019) can mitigate subword ambiguity by considering context. However, it has been reported that adversarial typos can degrade a BERT model that uses subword tokenization (Pruthi et al., 2019; Sun et al., 2020). Subword meanings change across domains, making domain adaptation difficult (Sato et al., 2020). These problems are more critical in the processing of noisy text (Wang et al., 2020; Niu et al., 2020). To solve the above problems, we propose di1 http://nlp.stanford.edu/data/glove. 840B.300d.zip 4827 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4827–4838 c November 16 - 20, 2020. 2020 Association for Computational Linguistics rectly leveraging word-based pre-trained embeddings to compute OOV embeddings (Figure 1) (§ 3). Inspired by the two major processes for creating words, compounding and derivation, our method dynamically extracts words with pre-trained embeddings whose surfaces are similar to the target OOV word. Our method then aggregates pretrained word"
2020.findings-emnlp.434,S19-2220,0,0.0186886,"ed from an affinity matrix of entities (Yao et al., 2019). These approaches can have difficulties in representing OOV words that do not exist in the external data and have little versatile applicability to misspelled words. Recently, contextualized word embeddings such as BERT (Devlin et al., 2019) mitigate the problem of subword ambiguities by dynamically inferring meanings of OOV words from their contexts. However, several researchers reported that BERT remains brittle to misspellings (Pruthi et al., 2019; Sun et al., 2020), rare words (Schick and Sch¨utze, 2020), and out-of-domain samples (Park et al., 2019). Pre-trained word embeddings are reported to be more effective for these cases and morphological tasks such as entity typing and NER (Zhu et al., 2019). 4828 We therefore improve not only models based on pre-trained word embeddings but also the brittle subword-based BERT. Our approach will broaden the application range of neural-network models. 3 Robust backed-off estimation of OOV word embeddings In this section, we describe our method of computing embeddings for target OOV words by using a weighted sum of pre-trained word embeddings. Specifically, we calculate the weights over known words f"
2020.findings-emnlp.434,D14-1162,0,0.0978209,"extracting known words with a similar surface to a target OOV word: (i) segmentation of the target OOV word referring to known words and (ii) approximate string matching used for extracting known words with a similar surface from the OOV word. These components are inspired by the two major processes for creating words, namely, compounding and derivation, from existing words; we back-off unknown words to known words to rewind and replay the processes for creating words. In this paper, we assume that word embeddings are already trained on a large corpus in an unsupervised method such as GloVe (Pennington et al., 2014). Backing off to these known words can alleviate the ambiguity of subwords because wordlevel pre-trained embeddings can be expected to be less polysemous than subword embeddings. Moreover, we do not update word-level pre-trained embeddings in training the reconstruction task described below. Then, we dynamically calculate the embeddings for OOV words in the same continuous space with known words. Segmentation by known words Inspired by the compounding of words such as German nouns (e.g., “Kinder|garten”) and chemical compounds (e.g., “dichloro|difluoro|methane”), the first approach extracts kn"
2020.findings-emnlp.434,D18-1169,0,0.0822615,"a character-level CNN encoder. We further integrate this method into a BERT-based fine-tuning model. In essence, the proposed method directly computes OOV embeddings from pre-trained word embeddings, whereas the existing methods compute indirectly via their subwords. To investigate the performance of the proposed method against baseline approaches, we conduct both intrinsic and extrinsic evaluations of OOV word embeddings (§ 4). In the experiments for the intrinsic evaluation, we examine the performance of the proposed method in inducing embeddings for rare words by using the CARD benchmark (Pilehvar et al., 2018) and for misspelled words by using the TOEFL dataset (Flor et al., 2019). Then, in those for the extrinsic evaluation, we demonstrate the effectiveness of the calculated OOV word embeddings in two downstream tasks, named entity recognition (NER) and part-of-speech (POS) tagging for Twitter and biomedical domains, where OOV words frequently appear. We finally evaluate the BERT-based fine-tuning model with our method on these tasks and adversarial perturbations (Sun et al., 2020). The contributions of this work are as follows. • We propose a robust backed-off approach for estimating OOV word emb"
2020.findings-emnlp.434,D17-1010,0,0.184086,"Back-off to known words Surface encoder Calculate similarity scores Pre-trained word embeddings Output embedding Figure 1: Backed-off estimation of OOV embedding. The dynamic nature of language and the limited size of training data requires neural network models to handle out-of-vocabulary (OOV) words that are absent from the training data. We thus use an UNK embedding shared among diverse OOV words or break those OOV words into semanticallyambiguous subwords (even characters), leading to poor task performance (Peng et al., 2019; Sato et al., 2020). To solve this problem, several approaches (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019) learn subword embeddings from pre-trained embeddings and then use these subword embeddings for computing OOV word embeddings (§ 2). However, the embeddings computed by these apCurrently, he works for NTT Laboratories. 34.2 Table 1: Cosine similarity between Glove.840B embedding of “higher” and related embeddings: subword and reconstructed embeddings of “higher” by BoS (Zhao et al., 2018) and Glove.840B embedding of “high.” Introduction ∗ 48.4 proaches are subject to the noisiness and ambiguity of intermediate subwords. For example, the Glove.840B1 embe"
2020.findings-emnlp.434,P19-1561,0,0.263347,"Glove.840B embedding of “high.” Introduction ∗ 48.4 proaches are subject to the noisiness and ambiguity of intermediate subwords. For example, the Glove.840B1 embedding of “higher” is closer to “high” compared with the embedding of “higher” reconstructed from its subwords using the method of BoS (Zhao et al., 2018), due to the ambiguous subword er> as shown in Table 1. Contextual word embeddings such as BERT (Devlin et al., 2019) can mitigate subword ambiguity by considering context. However, it has been reported that adversarial typos can degrade a BERT model that uses subword tokenization (Pruthi et al., 2019; Sun et al., 2020). Subword meanings change across domains, making domain adaptation difficult (Sato et al., 2020). These problems are more critical in the processing of noisy text (Wang et al., 2020; Niu et al., 2020). To solve the above problems, we propose di1 http://nlp.stanford.edu/data/glove. 840B.300d.zip 4827 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4827–4838 c November 16 - 20, 2020. 2020 Association for Computational Linguistics rectly leveraging word-based pre-trained embeddings to compute OOV embeddings (Figure 1) (§ 3). Inspired by the two majo"
2020.findings-emnlp.434,D11-1141,0,0.200153,"Missing"
2020.findings-emnlp.434,N19-1353,0,0.164853,"Missing"
2020.findings-emnlp.434,2020.findings-emnlp.381,1,0.390756,"trong baseline. 1 cosine 20.1 −7.8 36.5 69.8 brexit exit grexit Back-off to known words Surface encoder Calculate similarity scores Pre-trained word embeddings Output embedding Figure 1: Backed-off estimation of OOV embedding. The dynamic nature of language and the limited size of training data requires neural network models to handle out-of-vocabulary (OOV) words that are absent from the training data. We thus use an UNK embedding shared among diverse OOV words or break those OOV words into semanticallyambiguous subwords (even characters), leading to poor task performance (Peng et al., 2019; Sato et al., 2020). To solve this problem, several approaches (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019) learn subword embeddings from pre-trained embeddings and then use these subword embeddings for computing OOV word embeddings (§ 2). However, the embeddings computed by these apCurrently, he works for NTT Laboratories. 34.2 Table 1: Cosine similarity between Glove.840B embedding of “higher” and related embeddings: subword and reconstructed embeddings of “higher” by BoS (Zhao et al., 2018) and Glove.840B embedding of “high.” Introduction ∗ 48.4 proaches are subject to the noisiness and ambig"
2020.findings-emnlp.434,N19-1048,0,0.0358412,"Missing"
2020.findings-emnlp.434,D13-1170,0,0.0037199,"ts for each word: (i) swapping two adjacent characters in a word, (ii) removing a character in a word, (iii) replacing a character with an adjacent character on the keyboard, and (iv) inserting an adjacent character on the keyboard before a character in a word. We did not edit stop words or words with less than three characters, following Pruthi et al. (2019). To reduce the computational cost, we limited the number of candidates of typos to N typos randomly. In this paper, we set the hyperparameters as K ∈ {0, 1, 3, 5}, N = 10. We used movie reviews from the Stanford Sentiment Treebank (SST) (Socher et al., 2013), which consists of 8544 movie reviews. With only positive and negative reviews, we trained a BERT with a sequence classification head on top (Wolf et al., 2019) to generate the adversarial examples described above. We evaluated the accuracy for the BERT -based fine-tuning model and a word-based LSTM model with and without embeddings computed with the proposed method. For comparison, we also evaluated a variant of our extension to BERT that assigns a zero vector instead of embeddings computed by the proposed method to OOV words (+GloVe). As with the LSTM tagger in § 4.2, we used two bidirectio"
2020.findings-emnlp.434,P19-1326,0,0.0727296,"azaridou et al., 2017; Khodak et al., 2018; Schick and Sch¨utze, 2019; Hu et al., 2019). Schick and Sch¨utze (2020) reported that they can improve BERT (Devlin et al., 2019) for understanding rare words. Notably, in these approaches for utilizing both surface and context information, the surface-based embeddings are the same as (Zhao et al., 2018). These approaches can have difficulties in representing misspelled words or spelling variations when a small number of contexts are available in a text corpus. Several approaches utilize external data such as a knowledge base (Bahdanau et al., 2018; Yang et al., 2019; Yao et al., 2019). Existing approaches successfully impute OOV word embeddings by convolutional graph neural network (Yang et al., 2019) or by spectral embeddings derived from an affinity matrix of entities (Yao et al., 2019). These approaches can have difficulties in representing OOV words that do not exist in the external data and have little versatile applicability to misspelled words. Recently, contextualized word embeddings such as BERT (Devlin et al., 2019) mitigate the problem of subword ambiguities by dynamically inferring meanings of OOV words from their contexts. However, several r"
2020.findings-emnlp.434,D18-1059,0,0.485868,"rds Surface encoder Calculate similarity scores Pre-trained word embeddings Output embedding Figure 1: Backed-off estimation of OOV embedding. The dynamic nature of language and the limited size of training data requires neural network models to handle out-of-vocabulary (OOV) words that are absent from the training data. We thus use an UNK embedding shared among diverse OOV words or break those OOV words into semanticallyambiguous subwords (even characters), leading to poor task performance (Peng et al., 2019; Sato et al., 2020). To solve this problem, several approaches (Pinter et al., 2017; Zhao et al., 2018; Sasaki et al., 2019) learn subword embeddings from pre-trained embeddings and then use these subword embeddings for computing OOV word embeddings (§ 2). However, the embeddings computed by these apCurrently, he works for NTT Laboratories. 34.2 Table 1: Cosine similarity between Glove.840B embedding of “higher” and related embeddings: subword and reconstructed embeddings of “higher” by BoS (Zhao et al., 2018) and Glove.840B embedding of “high.” Introduction ∗ 48.4 proaches are subject to the noisiness and ambiguity of intermediate subwords. For example, the Glove.840B1 embedding of “higher” i"
2020.findings-emnlp.434,K19-1021,0,0.0336563,"Missing"
2021.blackboxnlp-1.41,Q17-1010,0,0.0180509,"guistic signatures extracted from the top-10K sentences to which they strongly responded. The neurons were manually named to highlight the linguistic phenomena they capture (§ 4.2). The “word” and “word/POS pattern” rows show the most frequent patterns in the 10K sentences. The “typical sent.” is that is closest to the average vector of the collected 10K sentences when they are vectorized (§ 4.1). sentence as vectors, and take average over the 10K sentence vectors to obtain neuron representations. Specifically, we represent each sentence with an average of 300-dimensional fastText embeddings (Bojanowski et al., 2017)12 of the tokens. nomena they captured in Table 2. With the help of the automatic linguistic annotations, we found neurons that responded only to nouns or verb-rich sentences (Noun or Verb neuron), and neurons that responded only to short sentences (Short-length neuron). Digging deeper into 4.2 Single-Neuron Analysis what kind of text Noun neuron is actually responding to, we can see another aspect; it responds to We collected 10K sentences corresponding to each neuron, and identified the common linguistic phe- names of people such as “william” and “john.” It nomena in those sentences by using"
2021.blackboxnlp-1.41,K19-1063,0,0.0164822,"tic phenomena are captured by neural NLP models, researchers have analyzed the internal representations of the models at various levels of granularity such as neurons (Karpathy et al., 2016; Shi et al., 2016; Qian et al., 2016; Bau et al., 2018; Lakretz et al., 2019; Vig et al., 2020; Cao et al., 2021), layers (Hewitt and Manning, 2019; Liu et al., 2019; Tenney et al., 2019a; Goldberg, 2019; Jawahar et al., 2019; Miaschi and Dell’Orletta, 2020), attentions (Kovaleva et al., 2019; Clark et al., 2019; Brunner et al., 2020; Kobayashi et al., 2020), and the model as a whole (Petroni et al., 2019; Broscheit, 2019; Roberts et al., 2020). In what follows, we start by reviewing probing methods that investigate the classifier performance of external tasks based on the target model components. We next describe some microscopic methods that focus on individual neurons. We then discuss a method of inspecting each neuron using text generated for that neuron. Most of the recent methods adopt a top-down approach called probing, which takes target component (e.g., layer) as inputs, trains a classifier that predicts the linguistic phenomena of interest such as syntactic information (Jawahar et al., 2019; Miaschi"
2021.blackboxnlp-1.41,2021.naacl-main.74,0,0.507102,"omena from the tarto the model as a whole, we present a bottomget components (Bau et al., 2018; Giulianelli et al., up approach to inspect the target neural model 2018; Dalvi et al., 2019; Lakretz et al., 2019; Koby using neuron representations obtained from valeva et al., 2019; Goldberg, 2019; Petroni et al., a massive corpus of text. We first feed massive amount of text to the target model and collect 2019; Hewitt and Manning, 2019; Jawahar et al., sentences that strongly activate each neuron. 2019; Durrani et al., 2020; Zhou and Srikumar, We then abstract the collected sentences to ob2021; Cao et al., 2021; Jumelet et al., 2021). tain neuron representations that help us interAlthough this top-down approach based on probpret the corresponding neurons; we augment ing classifiers has provided thought-provoking inthe sentences with linguistic annotations (e.g., sights into the target model components, it becomes part-of-speech tags) and various metadata (e.g., topic and sentiment), and apply pattern mincostly when we want to inspect many combinations ing and clustering techniques to the augmented of model components and linguistic phenomena. sentences. We demonstrate the utility of our This is beca"
2021.blackboxnlp-1.41,W19-4828,0,0.0178701,"BERT (§ 4.3). The contribution of this study is threefold: 2 Related Work In order to clarify what kind of linguistic phenomena are captured by neural NLP models, researchers have analyzed the internal representations of the models at various levels of granularity such as neurons (Karpathy et al., 2016; Shi et al., 2016; Qian et al., 2016; Bau et al., 2018; Lakretz et al., 2019; Vig et al., 2020; Cao et al., 2021), layers (Hewitt and Manning, 2019; Liu et al., 2019; Tenney et al., 2019a; Goldberg, 2019; Jawahar et al., 2019; Miaschi and Dell’Orletta, 2020), attentions (Kovaleva et al., 2019; Clark et al., 2019; Brunner et al., 2020; Kobayashi et al., 2020), and the model as a whole (Petroni et al., 2019; Broscheit, 2019; Roberts et al., 2020). In what follows, we start by reviewing probing methods that investigate the classifier performance of external tasks based on the target model components. We next describe some microscopic methods that focus on individual neurons. We then discuss a method of inspecting each neuron using text generated for that neuron. Most of the recent methods adopt a top-down approach called probing, which takes target component (e.g., layer) as inputs, trains a classifier"
2021.blackboxnlp-1.41,N19-1423,0,0.193508,"entences with the linguistic annotations (e.g., part of speech tags) and various metadata (e.g., topic and sentiment). With the help of these annotations, we then apply text mining techniques such as frequent pattern mining to extract common patterns as the linguistic signatures3 that exist repeatedly and intricately in the sentences collected for the target neurons (§ 3.2). We finally investigate relationships between multiple neurons by comparing and clustering continuous neuron representations induced from the collected sentences. We apply our method to the pre-trained BERT (base-uncased) (Devlin et al., 2019) to demonstrate how much insight into BERT the method can actually provide. Our exploratory model analysis have confirmed that it is possible to identify, without any prior assumptions, a wide variety of specific linguistic phenomena to which each neuron responds (§ 4.2). Furthermore, by comparing the linguistic phenomena and sentences corresponding to individual neurons, we revealed the existence of neurons that work cooperatively for the same purpose. We finally revealed the impact of optimizing BERT to the target task (here, the pre-training task and sentiment classification) by comparing n"
2021.blackboxnlp-1.41,2020.emnlp-main.395,0,0.0306871,"ifier that prevarious levels ranging from individual neurons dicts the chosen linguistic phenomena from the tarto the model as a whole, we present a bottomget components (Bau et al., 2018; Giulianelli et al., up approach to inspect the target neural model 2018; Dalvi et al., 2019; Lakretz et al., 2019; Koby using neuron representations obtained from valeva et al., 2019; Goldberg, 2019; Petroni et al., a massive corpus of text. We first feed massive amount of text to the target model and collect 2019; Hewitt and Manning, 2019; Jawahar et al., sentences that strongly activate each neuron. 2019; Durrani et al., 2020; Zhou and Srikumar, We then abstract the collected sentences to ob2021; Cao et al., 2021; Jumelet et al., 2021). tain neuron representations that help us interAlthough this top-down approach based on probpret the corresponding neurons; we augment ing classifiers has provided thought-provoking inthe sentences with linguistic annotations (e.g., sights into the target model components, it becomes part-of-speech tags) and various metadata (e.g., topic and sentiment), and apply pattern mincostly when we want to inspect many combinations ing and clustering techniques to the augmented of model compo"
2021.blackboxnlp-1.41,2020.tacl-1.3,0,0.0179878,"target model components. We next describe some microscopic methods that focus on individual neurons. We then discuss a method of inspecting each neuron using text generated for that neuron. Most of the recent methods adopt a top-down approach called probing, which takes target component (e.g., layer) as inputs, trains a classifier that predicts the linguistic phenomena of interest such as syntactic information (Jawahar et al., 2019; Miaschi and Dell’Orletta, 2020; Wu et al., 2020) , agreement information (Giulianelli et al., 2018; Goldberg, 2019), and semantic knowledge (Tenney et al., 2019b; Ettinger, 2020), and evaluates the • We present a method of exploratory model analysis to understand neural NLP models. properties of the internal representation with reference to the accuracy of the classifier. To reduce We investigate concrete linguistic phenomena the cost of training a classifier, Zhou and Srikumar (e.g., skip n-grams) captured by the neurons (2021) indirectly predict the performance of probof the target model, without any prior assumping classifiers by analyzing how the labeled data tions about the phenomena to be examined. is represented in the vector space. Some studies • We revealed c"
2021.blackboxnlp-1.41,W18-5426,0,0.0282425,"obing methods that investigate the classifier performance of external tasks based on the target model components. We next describe some microscopic methods that focus on individual neurons. We then discuss a method of inspecting each neuron using text generated for that neuron. Most of the recent methods adopt a top-down approach called probing, which takes target component (e.g., layer) as inputs, trains a classifier that predicts the linguistic phenomena of interest such as syntactic information (Jawahar et al., 2019; Miaschi and Dell’Orletta, 2020; Wu et al., 2020) , agreement information (Giulianelli et al., 2018; Goldberg, 2019), and semantic knowledge (Tenney et al., 2019b; Ettinger, 2020), and evaluates the • We present a method of exploratory model analysis to understand neural NLP models. properties of the internal representation with reference to the accuracy of the classifier. To reduce We investigate concrete linguistic phenomena the cost of training a classifier, Zhou and Srikumar (e.g., skip n-grams) captured by the neurons (2021) indirectly predict the performance of probof the target model, without any prior assumping classifiers by analyzing how the labeled data tions about the phenomena"
2021.blackboxnlp-1.41,N19-1419,0,0.0911938,"et compoible, exploratory analysis of a neural model at nents, and then trains a probing classifier that prevarious levels ranging from individual neurons dicts the chosen linguistic phenomena from the tarto the model as a whole, we present a bottomget components (Bau et al., 2018; Giulianelli et al., up approach to inspect the target neural model 2018; Dalvi et al., 2019; Lakretz et al., 2019; Koby using neuron representations obtained from valeva et al., 2019; Goldberg, 2019; Petroni et al., a massive corpus of text. We first feed massive amount of text to the target model and collect 2019; Hewitt and Manning, 2019; Jawahar et al., sentences that strongly activate each neuron. 2019; Durrani et al., 2020; Zhou and Srikumar, We then abstract the collected sentences to ob2021; Cao et al., 2021; Jumelet et al., 2021). tain neuron representations that help us interAlthough this top-down approach based on probpret the corresponding neurons; we augment ing classifiers has provided thought-provoking inthe sentences with linguistic annotations (e.g., sights into the target model components, it becomes part-of-speech tags) and various metadata (e.g., topic and sentiment), and apply pattern mincostly when we want"
2021.blackboxnlp-1.41,P19-1356,0,0.0116712,"ng neurons of randomly initialized BERT with neurons of the pre-trained and the fine-tuned BERT (§ 4.3). The contribution of this study is threefold: 2 Related Work In order to clarify what kind of linguistic phenomena are captured by neural NLP models, researchers have analyzed the internal representations of the models at various levels of granularity such as neurons (Karpathy et al., 2016; Shi et al., 2016; Qian et al., 2016; Bau et al., 2018; Lakretz et al., 2019; Vig et al., 2020; Cao et al., 2021), layers (Hewitt and Manning, 2019; Liu et al., 2019; Tenney et al., 2019a; Goldberg, 2019; Jawahar et al., 2019; Miaschi and Dell’Orletta, 2020), attentions (Kovaleva et al., 2019; Clark et al., 2019; Brunner et al., 2020; Kobayashi et al., 2020), and the model as a whole (Petroni et al., 2019; Broscheit, 2019; Roberts et al., 2020). In what follows, we start by reviewing probing methods that investigate the classifier performance of external tasks based on the target model components. We next describe some microscopic methods that focus on individual neurons. We then discuss a method of inspecting each neuron using text generated for that neuron. Most of the recent methods adopt a top-down approach ca"
2021.blackboxnlp-1.41,2021.findings-acl.439,0,0.022422,"to the model as a whole, we present a bottomget components (Bau et al., 2018; Giulianelli et al., up approach to inspect the target neural model 2018; Dalvi et al., 2019; Lakretz et al., 2019; Koby using neuron representations obtained from valeva et al., 2019; Goldberg, 2019; Petroni et al., a massive corpus of text. We first feed massive amount of text to the target model and collect 2019; Hewitt and Manning, 2019; Jawahar et al., sentences that strongly activate each neuron. 2019; Durrani et al., 2020; Zhou and Srikumar, We then abstract the collected sentences to ob2021; Cao et al., 2021; Jumelet et al., 2021). tain neuron representations that help us interAlthough this top-down approach based on probpret the corresponding neurons; we augment ing classifiers has provided thought-provoking inthe sentences with linguistic annotations (e.g., sights into the target model components, it becomes part-of-speech tags) and various metadata (e.g., topic and sentiment), and apply pattern mincostly when we want to inspect many combinations ing and clustering techniques to the augmented of model components and linguistic phenomena. sentences. We demonstrate the utility of our This is because the probing require"
2021.blackboxnlp-1.41,2020.emnlp-main.574,0,0.0166311,"udy is threefold: 2 Related Work In order to clarify what kind of linguistic phenomena are captured by neural NLP models, researchers have analyzed the internal representations of the models at various levels of granularity such as neurons (Karpathy et al., 2016; Shi et al., 2016; Qian et al., 2016; Bau et al., 2018; Lakretz et al., 2019; Vig et al., 2020; Cao et al., 2021), layers (Hewitt and Manning, 2019; Liu et al., 2019; Tenney et al., 2019a; Goldberg, 2019; Jawahar et al., 2019; Miaschi and Dell’Orletta, 2020), attentions (Kovaleva et al., 2019; Clark et al., 2019; Brunner et al., 2020; Kobayashi et al., 2020), and the model as a whole (Petroni et al., 2019; Broscheit, 2019; Roberts et al., 2020). In what follows, we start by reviewing probing methods that investigate the classifier performance of external tasks based on the target model components. We next describe some microscopic methods that focus on individual neurons. We then discuss a method of inspecting each neuron using text generated for that neuron. Most of the recent methods adopt a top-down approach called probing, which takes target component (e.g., layer) as inputs, trains a classifier that predicts the linguistic phenomena of inter"
2021.blackboxnlp-1.41,D19-1445,0,0.0214972,"ined and the fine-tuned BERT (§ 4.3). The contribution of this study is threefold: 2 Related Work In order to clarify what kind of linguistic phenomena are captured by neural NLP models, researchers have analyzed the internal representations of the models at various levels of granularity such as neurons (Karpathy et al., 2016; Shi et al., 2016; Qian et al., 2016; Bau et al., 2018; Lakretz et al., 2019; Vig et al., 2020; Cao et al., 2021), layers (Hewitt and Manning, 2019; Liu et al., 2019; Tenney et al., 2019a; Goldberg, 2019; Jawahar et al., 2019; Miaschi and Dell’Orletta, 2020), attentions (Kovaleva et al., 2019; Clark et al., 2019; Brunner et al., 2020; Kobayashi et al., 2020), and the model as a whole (Petroni et al., 2019; Broscheit, 2019; Roberts et al., 2020). In what follows, we start by reviewing probing methods that investigate the classifier performance of external tasks based on the target model components. We next describe some microscopic methods that focus on individual neurons. We then discuss a method of inspecting each neuron using text generated for that neuron. Most of the recent methods adopt a top-down approach called probing, which takes target component (e.g., layer) as inputs,"
2021.blackboxnlp-1.41,N19-1002,0,0.0958173,"ve no probable hypothesis on the association between the target model component and pheproach first takes aim at specific linguistic phenomnomena. In this study, aiming to provide a flexena that would be captured by the target compoible, exploratory analysis of a neural model at nents, and then trains a probing classifier that prevarious levels ranging from individual neurons dicts the chosen linguistic phenomena from the tarto the model as a whole, we present a bottomget components (Bau et al., 2018; Giulianelli et al., up approach to inspect the target neural model 2018; Dalvi et al., 2019; Lakretz et al., 2019; Koby using neuron representations obtained from valeva et al., 2019; Goldberg, 2019; Petroni et al., a massive corpus of text. We first feed massive amount of text to the target model and collect 2019; Hewitt and Manning, 2019; Jawahar et al., sentences that strongly activate each neuron. 2019; Durrani et al., 2020; Zhou and Srikumar, We then abstract the collected sentences to ob2021; Cao et al., 2021; Jumelet et al., 2021). tain neuron representations that help us interAlthough this top-down approach based on probpret the corresponding neurons; we augment ing classifiers has provided thoug"
2021.blackboxnlp-1.41,N19-1112,0,0.0236341,"e-training task and sentiment classification) by comparing neurons of randomly initialized BERT with neurons of the pre-trained and the fine-tuned BERT (§ 4.3). The contribution of this study is threefold: 2 Related Work In order to clarify what kind of linguistic phenomena are captured by neural NLP models, researchers have analyzed the internal representations of the models at various levels of granularity such as neurons (Karpathy et al., 2016; Shi et al., 2016; Qian et al., 2016; Bau et al., 2018; Lakretz et al., 2019; Vig et al., 2020; Cao et al., 2021), layers (Hewitt and Manning, 2019; Liu et al., 2019; Tenney et al., 2019a; Goldberg, 2019; Jawahar et al., 2019; Miaschi and Dell’Orletta, 2020), attentions (Kovaleva et al., 2019; Clark et al., 2019; Brunner et al., 2020; Kobayashi et al., 2020), and the model as a whole (Petroni et al., 2019; Broscheit, 2019; Roberts et al., 2020). In what follows, we start by reviewing probing methods that investigate the classifier performance of external tasks based on the target model components. We next describe some microscopic methods that focus on individual neurons. We then discuss a method of inspecting each neuron using text generated for that neu"
2021.blackboxnlp-1.41,P11-1015,0,0.227454,"Missing"
2021.blackboxnlp-1.41,2020.repl4nlp-1.15,0,0.0520513,"Missing"
2021.blackboxnlp-1.41,D19-1250,0,0.0935149,"y what kind of linguistic phenomena are captured by neural NLP models, researchers have analyzed the internal representations of the models at various levels of granularity such as neurons (Karpathy et al., 2016; Shi et al., 2016; Qian et al., 2016; Bau et al., 2018; Lakretz et al., 2019; Vig et al., 2020; Cao et al., 2021), layers (Hewitt and Manning, 2019; Liu et al., 2019; Tenney et al., 2019a; Goldberg, 2019; Jawahar et al., 2019; Miaschi and Dell’Orletta, 2020), attentions (Kovaleva et al., 2019; Clark et al., 2019; Brunner et al., 2020; Kobayashi et al., 2020), and the model as a whole (Petroni et al., 2019; Broscheit, 2019; Roberts et al., 2020). In what follows, we start by reviewing probing methods that investigate the classifier performance of external tasks based on the target model components. We next describe some microscopic methods that focus on individual neurons. We then discuss a method of inspecting each neuron using text generated for that neuron. Most of the recent methods adopt a top-down approach called probing, which takes target component (e.g., layer) as inputs, trains a classifier that predicts the linguistic phenomena of interest such as syntactic information (Jawahar et al"
2021.blackboxnlp-1.41,W18-5437,0,0.0282671,"ces (Karpathy et al., 2016; Shi et al., 2016; Qian et al., 2016; Vig et al., 2020; Lakretz et al., 2021). However, these methods are tailored to analyze a few pre-defined particular phenomena such as sequence length, or need to perform the analysis per phenomenon. Our method, on the other hand, allows for a wide range of analytical perspectives in that it uses a massive amount of raw text to collect sentences in which individual neurons show strong interest and leverages pattern mining to highlight linguistic phenomena contained in the collected sentences. As the most similar to our approach, Poerner et al. (2018) proposed a gradient-based method that generates input text which strongly activates neurons, inspired by work in computer vision (Simonyan et al., 2014). Their method differs from the probing methods in that it embodies the information captured by the neurons as text, which is similar to our study. However, their method can generate only text of pre-fixed length, and requires hyper-parameter tuning (e.g., annealing temperature) to generate natural text, making it costly to apply to massive neurons in the models. Moreover, it is difficult for humans to interpret properties of the generated tex"
2021.blackboxnlp-1.41,D16-1079,0,0.0666808,"Missing"
2021.blackboxnlp-1.41,2020.emnlp-main.437,0,0.0411077,"Missing"
2021.blackboxnlp-1.41,D16-1248,0,0.149705,"pecting the pre-trained BERT. machine-learning classifiers for each combination Our exploratory analysis reveals that i) specific of model component and linguistic phenomenon. phrases and domains of text are captured by Although there are a few bottom-up approaches individual neurons in BERT, ii) a group of neuto inspect DNNs by examining the response of the rons simultaneously capture the same linguisneurons towards (perturbed) inputs that represent tic phenomena, and iii) deeper-level layers capture more specific linguistic phenomena. the target linguistic phenomenon (Karpathy et al., 2016; Shi et al., 2016; Qian et al., 2016), these 1 Introduction approaches are labor intensive since they require Deep neural networks (DNNs) learn to induce inter- manual intervention. nal feature representations or neurons1 for a given This study aims at efficiently inspecting neural input, which are optimized for the target task. The NLP models at various levels of granularity ranging success of DNNs in the field of natural language from individual neurons to the model as a whole, processing (NLP) is underpinned by this flexibility and presents a bottom-up approach to inspecting to induce internal vector repres"
2021.blackboxnlp-1.41,P19-1452,0,0.0819705,"d sentiment classification) by comparing neurons of randomly initialized BERT with neurons of the pre-trained and the fine-tuned BERT (§ 4.3). The contribution of this study is threefold: 2 Related Work In order to clarify what kind of linguistic phenomena are captured by neural NLP models, researchers have analyzed the internal representations of the models at various levels of granularity such as neurons (Karpathy et al., 2016; Shi et al., 2016; Qian et al., 2016; Bau et al., 2018; Lakretz et al., 2019; Vig et al., 2020; Cao et al., 2021), layers (Hewitt and Manning, 2019; Liu et al., 2019; Tenney et al., 2019a; Goldberg, 2019; Jawahar et al., 2019; Miaschi and Dell’Orletta, 2020), attentions (Kovaleva et al., 2019; Clark et al., 2019; Brunner et al., 2020; Kobayashi et al., 2020), and the model as a whole (Petroni et al., 2019; Broscheit, 2019; Roberts et al., 2020). In what follows, we start by reviewing probing methods that investigate the classifier performance of external tasks based on the target model components. We next describe some microscopic methods that focus on individual neurons. We then discuss a method of inspecting each neuron using text generated for that neuron. Most of the rece"
2021.blackboxnlp-1.41,2021.naacl-main.94,0,0.040447,", in comparing the corresponding linguistic phenomena with those of neurons in randomly initialized and fine-tuned models, we found that neurons in deeper BERT layers capture more linguistic phenomena specific to language modeling and sentiment classification. In the future, we plan to investigate models with different architectures. For example, our method can be used to compare the differences of encoder and decoder neurons in end2end models; there is a debate in the field of neural machine translation as to whether modifying the encoder or the decoder contributes more to domain adaptation (Wang et al., 2021). Our method enables us to find non-apriori linguistic phenomena the neurons may capture, so that it is possible to assist in the construction of a novel training/evaluation dataset for the probingbased evaluation methods. In addition, our method provides information of a different nature, i.e., the concrete linguistic phenomena to which the internal representations respond, as opposed to numerical information such as accuracy of probing tasks. As mentioned in § 4.2, our method has the potential to perform with less time complexity by reducing the corpus size fed to the target model in Step 1."
2021.blackboxnlp-1.41,2020.acl-main.383,0,0.0116997,"In what follows, we start by reviewing probing methods that investigate the classifier performance of external tasks based on the target model components. We next describe some microscopic methods that focus on individual neurons. We then discuss a method of inspecting each neuron using text generated for that neuron. Most of the recent methods adopt a top-down approach called probing, which takes target component (e.g., layer) as inputs, trains a classifier that predicts the linguistic phenomena of interest such as syntactic information (Jawahar et al., 2019; Miaschi and Dell’Orletta, 2020; Wu et al., 2020) , agreement information (Giulianelli et al., 2018; Goldberg, 2019), and semantic knowledge (Tenney et al., 2019b; Ettinger, 2020), and evaluates the • We present a method of exploratory model analysis to understand neural NLP models. properties of the internal representation with reference to the accuracy of the classifier. To reduce We investigate concrete linguistic phenomena the cost of training a classifier, Zhou and Srikumar (e.g., skip n-grams) captured by the neurons (2021) indirectly predict the performance of probof the target model, without any prior assumping classifiers by analyzi"
2021.blackboxnlp-1.41,2021.naacl-main.401,0,0.0287928,"Missing"
2021.findings-emnlp.399,P16-1101,0,0.0159606,"ities) appear. Using the datasets collected for Japanese, they trained an emerging entity recognizer, which successfully discovered various emerging entities more than one year before their registrations into Wikipedia. In this study, we adopt the definition of emerging entities proposed by Akasaki et al. (2019) and conduct time-sensitive distant supervision to automatically construct large-scale English and Japanese Twitter datasets for typing emerging entities. 2.2 Entity Typing Traditionally, named entity recognition (Sang and De Meulder, 2003; Ritter et al., 2011; Weischedel et al., 2013; Ma and Hovy, 2016; Akbik et al., 2019) jointly performs recognition and typing of entity mentions in the text. However, most of the NER • We set up a task of fine-grained typing of models require costly training data that fully annoemerging entities in microblogs (§ 3.2). tate all entities in the text. Indeed, many studies • We built two large-scale Twitter datasets for adopt less than ten coarse types (e.g., person, locaEnglish and Japanese (§ 3.3). We will release tion, and organization) (Mai et al., 2018). them to facilitate future studies. Focusing on fine-grained entity typing, recent • We proposed an ent"
2021.findings-emnlp.407,K16-1002,0,0.313895,"t variables hinder models from constructing a modulated latent space. As a result, the models stop handling uncertainty in conversations. To resolve that, we propose speculative sampling of latent variables. Our method chooses the most probable one from redundantly sampled latent variables for tying up the variable with a given response. We confirm the efficacy of our method in response generation with massive dialogue data constructed from Twitter posts. 1 Introduction Figure 1: The posterior can produce a variable leading to another probable response: illustrative example. encoder-decoders (Bowman et al., 2016). It is also possible for latent variables to work too aggressively and lead the models to generate responses that are less relevant to the contexts. Although many existing studies have tried to improve variational models (Kingma et al., 2016; Zhao et al., 2017; Shen et al., 2018; Gu et al., 2018; Fu et al., 2019) (§ 2), we postulate that there still remains a problem that degrades the models; during training, a sampled latent variable can be inappropriate to represent a given response, due to 1) immature parameters in early stages of training and 2) a trade-off in training objectives (Figure"
2021.findings-emnlp.407,N19-1021,0,0.10729,"th a given response. We confirm the efficacy of our method in response generation with massive dialogue data constructed from Twitter posts. 1 Introduction Figure 1: The posterior can produce a variable leading to another probable response: illustrative example. encoder-decoders (Bowman et al., 2016). It is also possible for latent variables to work too aggressively and lead the models to generate responses that are less relevant to the contexts. Although many existing studies have tried to improve variational models (Kingma et al., 2016; Zhao et al., 2017; Shen et al., 2018; Gu et al., 2018; Fu et al., 2019) (§ 2), we postulate that there still remains a problem that degrades the models; during training, a sampled latent variable can be inappropriate to represent a given response, due to 1) immature parameters in early stages of training and 2) a trade-off in training objectives (Figure 1). We hypothesize that the discrepancy between an unreliable latent variable and a given response can hinder models from structuring a modulated latent space. To address the problem, we propose speculative sampling of latent variables, a simple model-agnostic method to help variational models implicitly handle th"
2021.findings-emnlp.407,N19-1125,0,0.0554017,"died as one of the promising solutions to the safe response problem in chat response generation (Sohn et al., 2015; Serban et al., 2016). However, the difficulty in optimization has been studied mainly from a machine learning perspective; models with a sufficient number of parameters can ignore latent variables and work similarly to conventional encoder-decoder models (a.k.a. KL vanishing). Thus, many studies have proposed methods to control the optimization of variational models (Bowman et al., 2016; Zhao et al., 2017; Kingma et al., 2016; Shen et al., 2018; Li et al., 2018; Gu et al., 2018; Gao et al., 2019; He et al., 2019). They mainly focused on regularization, the architecture, and the training schedule. To design a latent space where the relevance and diversity of probable outputs are reflected geometrically, Gao et al. (2019) proposed SPACEFUSION. Among the aforementioned studies, their approach shares with us a similar goal of organizing the latent space. Kruengkrai (2019) proposed to sample multiple latent variables in text modeling, similarly to our method. However, the intention is different as their method was for better approximation of the expected reconstruction term in training. 3"
2021.findings-emnlp.407,C12-1059,0,0.0847834,"Missing"
2021.findings-emnlp.407,P19-1553,0,0.0950234,"KL vanishing). Thus, many studies have proposed methods to control the optimization of variational models (Bowman et al., 2016; Zhao et al., 2017; Kingma et al., 2016; Shen et al., 2018; Li et al., 2018; Gu et al., 2018; Gao et al., 2019; He et al., 2019). They mainly focused on regularization, the architecture, and the training schedule. To design a latent space where the relevance and diversity of probable outputs are reflected geometrically, Gao et al. (2019) proposed SPACEFUSION. Among the aforementioned studies, their approach shares with us a similar goal of organizing the latent space. Kruengkrai (2019) proposed to sample multiple latent variables in text modeling, similarly to our method. However, the intention is different as their method was for better approximation of the expected reconstruction term in training. 3 Speculative Latent Variables Sampling reconstruction loss and Kullback-Leibler (KL) divergence DKL , is maximized: Z log p(y|x) = log pφ (y|x, z)pθ (z|x)dz z ≥ Eqθ (z|x,y) [log pφ (y|x, zq )] − DKL (qθ (z|x, y)|pθ (z|x)) . (1) Here, in training, what if a latent variable zq sampled from the posterior distribution qθ (z|x, y) is inappropriate to represent the response y? Althou"
2021.findings-emnlp.407,D18-2012,0,0.0117157,"hanging the sampling method in training (Monte Carlo sampling and Speculative sampling). 4.2 for training, 196,253 for development, and 97,433 for testing. The numbers of Japanese conversations were 18,116,756 for training, 191,890 for development, and 96,276 for testing. We employed multi-bleu.perl in Moses toolkit (v4.0)7 for tokenizing English text. This tokenization was applied only for generated outputs to compute automatic evaluation metrics. We employed MeCab8 for tokenizing Japanese text. From the training data, we trained subword tokenization models through unigram language modeling (Kudo and Richardson, 2018) and CBOW vectors (Mikolov et al., 2013) for initialization of the model’s embedding layers. For human evaluation, we manually chose 100 conversations from the Japanese test data. This was because randomly sampled conversations 1) can be difficult to understand for evaluators due to the lack of contexts or knowledge, and 2) can contain utterances where possible responses are not diverse (e.g., greetings or yes/no questions). Using such conversations for human evaluation not only increases annotation costs, but also makes it difficult to analyze differences between models. We will also release"
2021.findings-emnlp.407,N16-1014,0,0.252327,"ing of latent variables, a simple model-agnostic method to help variational models implicitly handle the uncertainty in conversations. In experiments, we evaluated our method on massive open-domain dialogue data taken from Twitter. Automatic and human evaluations on the generated responses confirmed that our method improved both sensibleness and specificity of responses. The contributions of this paper are as follows. In early neural-based approaches to chat dialogue modeling, conventional encoder-decoder frameworks (Cho et al., 2014; Sutskever et al., 2014) tended to generate safe responses (Li et al., 2016). The main reason was that these frameworks model response generation as one-to-one projections from an utterance to a response, while many probable responses often exist in open-domain conversations. The use of conditioned variational autoencoders (CVAE) is a promising approach for resolving the problem (Sohn et al., 2015; Serban et al., 2016). In these models, latent variables sampled from approximated distributions are expected to serve as a clue to handle the uncertainty in probable responses. The uncertainty can correspond to topics, domains, or styles that are not explicitly controlled."
2021.findings-emnlp.407,N19-4009,0,0.0246503,"models with a sufficient number of parameters can ignore latent variables and work similarly to conventional encoder-decoder models (a.k.a. KL vanishing). Thus, many studies have proposed methods to control the optimization of variational models (Bowman et al., 2016; Zhao et al., 2017; Kingma et al., 2016; Shen et al., 2018; Li et al., 2018; Gu et al., 2018; Gao et al., 2019; He et al., 2019). They mainly focused on regularization, the architecture, and the training schedule. To design a latent space where the relevance and diversity of probable outputs are reflected geometrically, Gao et al. (2019) proposed SPACEFUSION. Among the aforementioned studies, their approach shares with us a similar goal of organizing the latent space. Kruengkrai (2019) proposed to sample multiple latent variables in text modeling, similarly to our method. However, the intention is different as their method was for better approximation of the expected reconstruction term in training. 3 Speculative Latent Variables Sampling reconstruction loss and Kullback-Leibler (KL) divergence DKL , is maximized: Z log p(y|x) = log pφ (y|x, z)pθ (z|x)dz z ≥ Eqθ (z|x,y) [log pφ (y|x, zq )] − DKL (qθ (z|x, y)|pθ (z|x)) . (1) H"
2021.findings-emnlp.407,P02-1040,0,0.110352,"h conversations were 19,627,263 3 Note that the covariance of Gaussian distribution of this model is not parametrized, and thus, only this model is slightly different from other models based on T-CVAE. 4 In T-CVAE, latent variables are combined with the last decoder state before softmax. Thus, we simply averaged the latent variables instead of averaging the decoder states. 5 From the validation loss, we chose five as the number of sampled latent variables. 6 https://developer.twitter.com/ 4.3 Evaluation Metrics For automatic evaluation, we employed several common metrics: case-sensitive BLEU (Papineni et al., 2002) in Moses9 and dist-n (Li et al., 2016). Additionally, we compared the KL-divergence of trained models to investigate how the resolution of KL vanishing affected generated responses. We also conducted human evaluation with similar metrics to Adiwardana et al. (2020). Annotators provided scores of 1) sensibleness and 2) specificity from 1 to 5 for each anonymized response.10 5 Results This section reports results of automatic (§ 5.1) and human evaluations (§ 5.2) of generated responses on the Twitter datasets, and then analyzes the models’ outputs (§ 5.3). 7 https://github.com/moses-smt/ mosesd"
2021.findings-emnlp.407,2020.acl-main.634,0,0.0167234,"ot diverse (e.g., greetings or yes/no questions). Using such conversations for human evaluation not only increases annotation costs, but also makes it difficult to analyze differences between models. We will also release these conversations as a challenging set that enables developers to evaluate the ability of models for diversification with a low cost. Datasets and Preprocessing To evaluate the ability of models to generate diverse responses, the dataset needs to contain various topics and styles. Following existing studies (Ritter et al., 2011; Serban et al., 2017; Adiwardana et al., 2020; Su et al., 2020), we constructed massive English and Japanese dialogue datasets from social media conversations. Concretely, we exploited Twitter posts while treating a post and the subsequent replies as a conversation. We used posts in 2017 and 2018 for both training and development, posts in 2019 for testing. They were randomly sampled from our Twitter archive (Nishi et al., 2016) collected via the Twitter API.6 We filtered out noisy posts with a rule-based filtering following Adiwardana et al. (2020). The numbers of English conversations were 19,627,263 3 Note that the covariance of Gaussian distribution o"
2021.findings-emnlp.407,P17-1061,0,0.0513881,"Missing"
2021.naacl-main.461,N18-1118,0,0.270439,"ble without considering in smoothly reading the translated documents. contexts. We thus adjust the probability distribuTo address this issue, context-aware NMT models tions in a context-aware manner using a target-side which incorporate document-level information in document-level LM which models inter-sentential translation have recently been explored (Jean et al., dependencies in the target-side document. 2017; Wang et al., 2017; Tiedemann and Scherrer, We evaluate our methods on English to Rus2017; Maruf and Haffari, 2018; Voita et al., 2018; sian translations with the OpenSubtitles2018 corBawden et al., 2018; Miculicich et al., 2018; Maruf pus (Lison et al., 2018) in terms of the BLEU et al., 2019; Voita et al., 2019b; Yu et al., 2020; scores and contrastive discourse test sets (Voita ∗ Currently at Mitsubishi UFJ Morgan Stanley Securities et al., 2019b). Experimental results confirm that 5781 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5781–5791 June 6–11, 2021. ©2021 Association for Computational Linguistics our method achieved comparable performance with existing context-aware NMT models t"
2021.naacl-main.461,P19-1285,0,0.0669331,"Missing"
2021.naacl-main.461,W17-4123,0,0.0181325,"e current sentence. We first provide the formulation of the objective, C - SCORE, and the computation process of the C - SCORE using a sentence-level translation model and a document-level language model. We investigate two search methods, reranking and beam search, and evaluate the methods for EnglishRussian translation. We also provide some analysis and visualization to better understand the nature of PMI between the context and the current sentence. We plan to design context-aware BLEU using PMI for evaluating context-aware NMT models. We will evaluate our method on non-autoregressive NMT (Gu et al., 2017). We will release all code and data to promote the reproducibility of results.9 8 This preprint is submitted to and rejected from EMNLP 2020; the interested reader may refer to this paper for experiments on other language pairs such as English to French and English to Japanese translation. 9 http://www.tkl.iis.u-tokyo.ac.jp/ ~sugi/NAACL2021/ 5789 Acknowledgements We thank anonymous reviewers for their valuable comments. We also thank Joshua Tanner for proofreading this paper. We also thank Masato Neishi for technical advice on implementations of neural machine translation. The research was sup"
2021.naacl-main.461,2020.spnlp-1.11,0,0.184101,"oshinaga, 2019). PMI correlation gives us a good explanation of how Recent studies have therefore started to focus C - AWARE beam without T -scaling fails. We plot on modeling contexts using document-level monothe PMI correlation between the source sentences lingual data. The current approaches are grouped and their translations obtained with NMT models into three categories: data augmentation via back5788 translation (Sugiyama and Yoshinaga, 2019), a post-editing model (Voita et al., 2019a), and modeling document-level fluency via document-level LM s (Stahlberg et al., 2019; Yu et al., 2020; Jean and Cho, 2020). In what follows, we review these approaches in detail. Sugiyama and Yoshinaga (2019) reported that the data augmentation by back-translation (Sennrich et al., 2016) enhances a document-level NMT model with a single encoder architecture in lowresource settings. However, we have obtained limited improvements in our settings (Table 2 and Table 3). Moreover, this approach is expensive since it learns a document-level NMT model from a massive amount of pseudo parallel data. Voita et al. (2019a) proposed DocRepair, a context-aware post-editing model that corrects outputs of a sentence-level NMT mo"
2021.naacl-main.461,2020.emnlp-main.175,0,0.0487363,"ce sentences. C - AWARE beam with T scaling (T = 4) seems to solve this issue and achieves the highest PMI correlation R = 0.740. 5 Related Work The effectiveness of incorporating context into translation was shown in earlier literature on document-level NMT (Tiedemann and Scherrer, 2017; Bawden et al., 2018) using the single encoder architecture. Multi-encoder architectures were explored to better capture contextual information (Wang et al., 2017; Tu et al., 2018; Jean et al., 2017; Miculicich et al., 2018; Voita et al., 2018; Bawden et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Kang et al., 2020; Zhang et al., 2020). However, since parallel data is often constructed by picking up reliable sentential alignments from comparable documents, documentlevel sentence-aligned parallel data for training these document-level NMT models are expensive to obtain and available in only a few domains and Analysis of the model outputs language pairs (Sugiyama and Yoshinaga, 2019). PMI correlation gives us a good explanation of how Recent studies have therefore started to focus C - AWARE beam without T -scaling fails. We plot on modeling contexts using document-level monothe PMI correlation between the"
2021.naacl-main.461,D18-1512,0,0.0658925,"Missing"
2021.naacl-main.461,L18-1275,0,0.16468,"d documents. contexts. We thus adjust the probability distribuTo address this issue, context-aware NMT models tions in a context-aware manner using a target-side which incorporate document-level information in document-level LM which models inter-sentential translation have recently been explored (Jean et al., dependencies in the target-side document. 2017; Wang et al., 2017; Tiedemann and Scherrer, We evaluate our methods on English to Rus2017; Maruf and Haffari, 2018; Voita et al., 2018; sian translations with the OpenSubtitles2018 corBawden et al., 2018; Miculicich et al., 2018; Maruf pus (Lison et al., 2018) in terms of the BLEU et al., 2019; Voita et al., 2019b; Yu et al., 2020; scores and contrastive discourse test sets (Voita ∗ Currently at Mitsubishi UFJ Morgan Stanley Securities et al., 2019b). Experimental results confirm that 5781 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5781–5791 June 6–11, 2021. ©2021 Association for Computational Linguistics our method achieved comparable performance with existing context-aware NMT models that require either document-level parallel data (Zhang et"
2021.naacl-main.461,P18-1118,0,0.0602454,"ability to the BLEU scores (Papineni et al., 2002), they are vital translation that is plausible without considering in smoothly reading the translated documents. contexts. We thus adjust the probability distribuTo address this issue, context-aware NMT models tions in a context-aware manner using a target-side which incorporate document-level information in document-level LM which models inter-sentential translation have recently been explored (Jean et al., dependencies in the target-side document. 2017; Wang et al., 2017; Tiedemann and Scherrer, We evaluate our methods on English to Rus2017; Maruf and Haffari, 2018; Voita et al., 2018; sian translations with the OpenSubtitles2018 corBawden et al., 2018; Miculicich et al., 2018; Maruf pus (Lison et al., 2018) in terms of the BLEU et al., 2019; Voita et al., 2019b; Yu et al., 2020; scores and contrastive discourse test sets (Voita ∗ Currently at Mitsubishi UFJ Morgan Stanley Securities et al., 2019b). Experimental results confirm that 5781 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5781–5791 June 6–11, 2021. ©2021 Association for Computational Lingui"
2021.naacl-main.461,N19-1313,0,0.0416663,"aligned to the source sentences. C - AWARE beam with T scaling (T = 4) seems to solve this issue and achieves the highest PMI correlation R = 0.740. 5 Related Work The effectiveness of incorporating context into translation was shown in earlier literature on document-level NMT (Tiedemann and Scherrer, 2017; Bawden et al., 2018) using the single encoder architecture. Multi-encoder architectures were explored to better capture contextual information (Wang et al., 2017; Tu et al., 2018; Jean et al., 2017; Miculicich et al., 2018; Voita et al., 2018; Bawden et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Kang et al., 2020; Zhang et al., 2020). However, since parallel data is often constructed by picking up reliable sentential alignments from comparable documents, documentlevel sentence-aligned parallel data for training these document-level NMT models are expensive to obtain and available in only a few domains and Analysis of the model outputs language pairs (Sugiyama and Yoshinaga, 2019). PMI correlation gives us a good explanation of how Recent studies have therefore started to focus C - AWARE beam without T -scaling fails. We plot on modeling contexts using document-level monothe PMI corr"
2021.naacl-main.461,D18-1325,0,0.0737875,"ng in smoothly reading the translated documents. contexts. We thus adjust the probability distribuTo address this issue, context-aware NMT models tions in a context-aware manner using a target-side which incorporate document-level information in document-level LM which models inter-sentential translation have recently been explored (Jean et al., dependencies in the target-side document. 2017; Wang et al., 2017; Tiedemann and Scherrer, We evaluate our methods on English to Rus2017; Maruf and Haffari, 2018; Voita et al., 2018; sian translations with the OpenSubtitles2018 corBawden et al., 2018; Miculicich et al., 2018; Maruf pus (Lison et al., 2018) in terms of the BLEU et al., 2019; Voita et al., 2019b; Yu et al., 2020; scores and contrastive discourse test sets (Voita ∗ Currently at Mitsubishi UFJ Morgan Stanley Securities et al., 2019b). Experimental results confirm that 5781 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5781–5791 June 6–11, 2021. ©2021 Association for Computational Linguistics our method achieved comparable performance with existing context-aware NMT models that require either docume"
2021.naacl-main.461,W18-6307,0,0.0173271,"t comparable to the baseline. Althogh Bayes DocReranker performed the best among all the models, the comparison to Bayes DocReranker without context information (using pS - LM (yi ) instead of pD - LM (yi |y<i )) reveals that most of the improvement is not obtained by the use of contexts. Back-translation did not contribute to BLEU possibly because the original parallel data is already large and there was little room for improvement with additional pseudo data. The existing automatic metrics are not adequate to evaluate gains from additional contexts (Bawden et al., 2018; Läubli et al., 2018; Müller et al., 2018; Voita et al., 2019b; Sugiyama and Yoshinaga, 2019). We thus adopt a contrastive test set (Voita et al., 2019b) to evaluate the model’s ability to capture contextual information in translation, in addition to the evaluation by BLEU scores (Papineni 4.2 Results on contrastive test sets et al., 2002) to confirm that the methods do not sacrifice general translation performance. BLEU Tables 3 lists evaluation results (accuracy) of the is computed using multi-bleu.perl from the contrastive tests with models using 30M monoMoses Toolkit after decoding the subword repre- lingual data. The highest sco"
2021.naacl-main.461,P02-1040,0,0.11016,"iguities when a source sentence has categorical probability distribution over the vocabumultiple plausible interpretations. Examples of lary at every time step. The decoder assigns higher such ambiguities include anaphora, ellipsis, and probabilities to the tokens that would be more suitlexical coherence (Voita et al., 2019b); although able at that step. Therefore, when multiple valid resolving these ambiguities has only a minor im- translations are possible for the source sentence, pact on the translation performance measured by the decoder just gives a higher probability to the BLEU scores (Papineni et al., 2002), they are vital translation that is plausible without considering in smoothly reading the translated documents. contexts. We thus adjust the probability distribuTo address this issue, context-aware NMT models tions in a context-aware manner using a target-side which incorporate document-level information in document-level LM which models inter-sentential translation have recently been explored (Jean et al., dependencies in the target-side document. 2017; Wang et al., 2017; Tiedemann and Scherrer, We evaluate our methods on English to Rus2017; Maruf and Haffari, 2018; Voita et al., 2018; sian"
2021.naacl-main.461,P16-1009,0,0.0988394,"based on the same configuration of Transformer base (see (Vaswani et al., 2017) for hyperparameter settings). The SentTransformer is trained using the 5.8M sentence pairs and is also used as the sentence-level NMT model in DocRepair, Bayes DocReranker, and our methods. For the training of DocTransformer, we use the 5.8M sentence pairs with document-level source context, which share the target-side sentences with the training data of SentTransformer. Consequently, scores obtained from the model are for reference.5 We also evaluate DocTransformer and SentTransformer using back-translation (BT) (Sennrich et al., 2016) with the same monolingual data as the other models. We use no pre-existing document-level parallel data to train the neural networks of DocRepair, Bayes DocReranker, and our methods, although we use a small amount of document-level parallel data as the development set to tune hyperparameters in the methods that combine multiple models. Instead, document-level information is fed to the models via the round-trip augmented data (DocRepair) or language models (Bayes DocReranker and our methods). Hyper-parameters We tune the models’ hyperparameters based on BLEU score on the development set in the"
2021.naacl-main.461,D19-6504,1,0.776389,"the BLEU et al., 2019; Voita et al., 2019b; Yu et al., 2020; scores and contrastive discourse test sets (Voita ∗ Currently at Mitsubishi UFJ Morgan Stanley Securities et al., 2019b). Experimental results confirm that 5781 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5781–5791 June 6–11, 2021. ©2021 Association for Computational Linguistics our method achieved comparable performance with existing context-aware NMT models that require either document-level parallel data (Zhang et al., 2018; Sugiyama and Yoshinaga, 2019) or more than one additional model (Voita et al., 2019a; Yu et al., 2020) for capturing contexts in translation. The contributions of this paper are as follows: • We theoretically derived C - SCORE, a score to qualify context-aware translation without the need for document-level parallel data. • Two formulations with C - SCORE turn any pre-trained sentence-level NMT model into a context-aware model, if it generates n-best outputs or performs left-to-right decoding. • A comparison between our approach and shallow fusion (Gulcehre et al., 2015) reveals that our approach reformulates shallow fusi"
2021.naacl-main.461,W17-4811,0,0.0158439,"BLEU. Translation of the SentTransformer shows a higher correlation with the source texts than the reference translation (Figure 1a). One possible explanation for this is alignment errors in the corpus: although worse than the reference translations in quality, outputs of SentTransformer are considered to be perfectly aligned to the source sentences. C - AWARE beam with T scaling (T = 4) seems to solve this issue and achieves the highest PMI correlation R = 0.740. 5 Related Work The effectiveness of incorporating context into translation was shown in earlier literature on document-level NMT (Tiedemann and Scherrer, 2017; Bawden et al., 2018) using the single encoder architecture. Multi-encoder architectures were explored to better capture contextual information (Wang et al., 2017; Tu et al., 2018; Jean et al., 2017; Miculicich et al., 2018; Voita et al., 2018; Bawden et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Kang et al., 2020; Zhang et al., 2020). However, since parallel data is often constructed by picking up reliable sentential alignments from comparable documents, documentlevel sentence-aligned parallel data for training these document-level NMT models are expensive to obtain and availabl"
2021.naacl-main.461,D18-1049,0,0.0386295,"Missing"
2021.naacl-main.461,2020.emnlp-main.81,0,0.0685724,"Missing"
2021.naacl-main.461,Q18-1029,0,0.0217077,"he corpus: although worse than the reference translations in quality, outputs of SentTransformer are considered to be perfectly aligned to the source sentences. C - AWARE beam with T scaling (T = 4) seems to solve this issue and achieves the highest PMI correlation R = 0.740. 5 Related Work The effectiveness of incorporating context into translation was shown in earlier literature on document-level NMT (Tiedemann and Scherrer, 2017; Bawden et al., 2018) using the single encoder architecture. Multi-encoder architectures were explored to better capture contextual information (Wang et al., 2017; Tu et al., 2018; Jean et al., 2017; Miculicich et al., 2018; Voita et al., 2018; Bawden et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019; Kang et al., 2020; Zhang et al., 2020). However, since parallel data is often constructed by picking up reliable sentential alignments from comparable documents, documentlevel sentence-aligned parallel data for training these document-level NMT models are expensive to obtain and available in only a few domains and Analysis of the model outputs language pairs (Sugiyama and Yoshinaga, 2019). PMI correlation gives us a good explanation of how Recent studies have ther"
2021.naacl-main.461,D19-1081,0,0.0581221,"Missing"
2021.naacl-main.461,P19-1116,0,0.0327696,"Missing"
2021.naacl-main.461,P18-1117,0,0.0783731,"s (Papineni et al., 2002), they are vital translation that is plausible without considering in smoothly reading the translated documents. contexts. We thus adjust the probability distribuTo address this issue, context-aware NMT models tions in a context-aware manner using a target-side which incorporate document-level information in document-level LM which models inter-sentential translation have recently been explored (Jean et al., dependencies in the target-side document. 2017; Wang et al., 2017; Tiedemann and Scherrer, We evaluate our methods on English to Rus2017; Maruf and Haffari, 2018; Voita et al., 2018; sian translations with the OpenSubtitles2018 corBawden et al., 2018; Miculicich et al., 2018; Maruf pus (Lison et al., 2018) in terms of the BLEU et al., 2019; Voita et al., 2019b; Yu et al., 2020; scores and contrastive discourse test sets (Voita ∗ Currently at Mitsubishi UFJ Morgan Stanley Securities et al., 2019b). Experimental results confirm that 5781 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5781–5791 June 6–11, 2021. ©2021 Association for Computational Linguistics our method ach"
2021.naacl-main.461,D17-1301,0,0.0683768,"ence, pact on the translation performance measured by the decoder just gives a higher probability to the BLEU scores (Papineni et al., 2002), they are vital translation that is plausible without considering in smoothly reading the translated documents. contexts. We thus adjust the probability distribuTo address this issue, context-aware NMT models tions in a context-aware manner using a target-side which incorporate document-level information in document-level LM which models inter-sentential translation have recently been explored (Jean et al., dependencies in the target-side document. 2017; Wang et al., 2017; Tiedemann and Scherrer, We evaluate our methods on English to Rus2017; Maruf and Haffari, 2018; Voita et al., 2018; sian translations with the OpenSubtitles2018 corBawden et al., 2018; Miculicich et al., 2018; Maruf pus (Lison et al., 2018) in terms of the BLEU et al., 2019; Voita et al., 2019b; Yu et al., 2020; scores and contrastive discourse test sets (Voita ∗ Currently at Mitsubishi UFJ Morgan Stanley Securities et al., 2019b). Experimental results confirm that 5781 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human La"
C10-1140,P03-1004,0,0.588223,"computes a (signed) margin mt (x) of x by using the kernel function with support set St−1 and coefficients αt−1 (Line 4). PA - I then suffers a hinge-loss, `t = max {0, 1 − ymt (x)} (Line 5). If `t > 0, PA - I adds x to St−1 (Line 7). Hyperparameter C controls the aggressiveness of parameter updates. The kernel function computes a dot product in 2.2 Kernel Computation for Classification This section explains fast, exact methods of computing the polynomial kernel, which are meant to test the trained model, (S, α), and involve substantial computational cost in preparation. 2.2.1 Kernel Inverted Kudo and Matsumoto (2003) proposed polynomial kernel inverted (PKI), which builds inverted indices h(fj ) ≡ {s |s ∈ S, fj ∈ s} from each feature fj to support vector s ∈ S to only consider support vector s relevant to given x such that sT x 6= 0. The time complexity of PKI is 1 P O(B · |x |+ |S|) where B ≡ |x| fj ∈x |h(fj )|, which is smaller than O(|S||x|) if x has many rare features fj such that |h(fj ) | |S|. To the best of our knowledge, this is the only exact method that has been used to speed up margin computation in the context of kernel-based online learning (Okanohara and Tsujii, 2007). 2.2.2 Kernel Expansio"
C10-1140,P07-1010,0,0.124493,"n. 2.2.1 Kernel Inverted Kudo and Matsumoto (2003) proposed polynomial kernel inverted (PKI), which builds inverted indices h(fj ) ≡ {s |s ∈ S, fj ∈ s} from each feature fj to support vector s ∈ S to only consider support vector s relevant to given x such that sT x 6= 0. The time complexity of PKI is 1 P O(B · |x |+ |S|) where B ≡ |x| fj ∈x |h(fj )|, which is smaller than O(|S||x|) if x has many rare features fj such that |h(fj ) | |S|. To the best of our knowledge, this is the only exact method that has been used to speed up margin computation in the context of kernel-based online learning (Okanohara and Tsujii, 2007). 2.2.2 Kernel Expansion Isozaki and Kazawa (2002) and Kudo and Matsumoto (2003) proposed kernel expansion, which explicitly maps both support set S and given example x ∈ Rn into RH by mapping φd imposed by kd : 1246 m(x) = X si ∈S !T αi φd (si ) φd (x) = X fi ∈xd wi , where xd ∈ {0, 1}H is a binary feature vector in which xdi = 1 for (φd (x))i 6= 0, and w is a weight vector in the expanded feature space, F d . The weight vector w is computed from S and α: w= X αi si ∈S d X ckd Ik (sdi ), (1) k=0 where ckd is a squared coefficient of k-th order conjunctive features for d-th order polynomial ke"
C10-1140,N09-2025,0,0.0433352,"Missing"
C10-1140,C04-1002,0,0.395889,"T x = s + 1 for all k k−1 i i si ∈ Sk+ ∪ Sk− ). We accumulate Eqs. 8 and 9 from rare to frequent features, and use the intermediate results to estimate the possible range of mt (x) before Line 3 in Algorithm 2. If the lower bound of ymt (x) turns out to be larger than one, we terminate the computation of mt (x). As training continues, the model becomes discriminative and given x is likely to have a larger margin. The impact of this termination will increase as the amount of training data expands. 1249 4 Evaluation DATA SET We evaluated the proposed method in two NLP tasks: dependency parsing (Sassano, 2004) and hyponymy-relation extraction (Sumida et al., 2008). We used labeled data included in opensource softwares to promote the reproducibility of our results.5 All the experiments were conducted TM R Xeon on a server with an Intel 3.2 GHz CPU. We used a double-array trie (Aoe, 1989; Yata et al., 2009) as an implementation of the weight trie and the partial margin trie. 4.1 Task Descriptions Japanese Dependency Parsing A parser inputs a sentence segmented by a bunsetsu (base phrase in Japanese), and selects a particular pair of bunsetsus (dependent and head candidates); the classifier then outpu"
C10-1140,sumida-etal-2008-boosting,1,0.925928,"e accumulate Eqs. 8 and 9 from rare to frequent features, and use the intermediate results to estimate the possible range of mt (x) before Line 3 in Algorithm 2. If the lower bound of ymt (x) turns out to be larger than one, we terminate the computation of mt (x). As training continues, the model becomes discriminative and given x is likely to have a larger margin. The impact of this termination will increase as the amount of training data expands. 1249 4 Evaluation DATA SET We evaluated the proposed method in two NLP tasks: dependency parsing (Sassano, 2004) and hyponymy-relation extraction (Sumida et al., 2008). We used labeled data included in opensource softwares to promote the reproducibility of our results.5 All the experiments were conducted TM R Xeon on a server with an Intel 3.2 GHz CPU. We used a double-array trie (Aoe, 1989; Yata et al., 2009) as an implementation of the weight trie and the partial margin trie. 4.1 Task Descriptions Japanese Dependency Parsing A parser inputs a sentence segmented by a bunsetsu (base phrase in Japanese), and selects a particular pair of bunsetsus (dependent and head candidates); the classifier then outputs label y = +1 (dependent) or −1 (independent) for the"
C10-1140,D08-1071,0,0.0266398,"Missing"
C10-1140,P09-1054,0,0.0320449,"0 examples from the labeled data for development and testing, and used the remaining examples for training. Note that the number of active features, |F d |, dramatically grows when we consider higher-order conjunctive features. We compared the proposed method, PA - I SL (Algorithm 1 with Algorithm 2), to PA - I KER NEL (Algorithm 1 with PKI ; Okanohara and Tsujii (2007)), PA - I KE (Algorithm 1 with kernel expansion; viz., kernel splitting with N = |F|), SVM (batch training of support vector machines),7 and `1 -LLM (stochastic gradient descent training of the `1 -regularized log-linear model: Tsuruoka et al. (2009)). We refer to PA - I SL that does not reuse temporal partial margins as PA - I SL∗ . To demonstrate the impact of conjunctive features on model accuracy, we also trained PA - I without conjunctive features. The number of iterations in PA - I was set to 20, and the parameters of PA - I were averaged in an efficient manner (Daum´e III, 2006). We explicitly considered conjunctions among topN (N = 125 × 2n ; n ≥ 0) features in PA - I SL and PA - I SL∗ . The hyperparameters were tuned to maximize accuracy on the development set. 4.3 Results Tables 2 and 3 list the experimental results for the two"
C10-1140,P07-2017,0,0.036712,"Missing"
C10-1140,P08-2060,0,0.16404,"rticular clues of each constituent, to achieve a high degree of accuracy in those tasks. Training with conjunctive features involves a space-time trade-off in the way conjunctive features are handled. Linear models, such as loglinear models, explicitly estimate the weights of conjunctive features, and training thus requires a great deal of memory when we take higher-order We aim at resolving this dilemma in training with conjunctive features, and propose online learning that combines the time efficiency of linear training and the space efficiency of kernelbased training. Following the work by Goldberg and Elhadad (2008), we explicitly take conjunctive features into account that frequently appear in the training data, and implicitly consider the other conjunctive features by using the polynomial kernel. We then improve the scalability of this training by a method called kernel slicing, which allows us to reuse the temporal margins of partial feature vectors and to terminate computations that do not contribute to parameter updates. We evaluate our method in two NLP tasks: dependency parsing and hyponymy-relation extraction. We demonstrate that our method is orders of magnitude faster than kernel-based online l"
C10-1140,C02-1054,0,0.163828,"roposed polynomial kernel inverted (PKI), which builds inverted indices h(fj ) ≡ {s |s ∈ S, fj ∈ s} from each feature fj to support vector s ∈ S to only consider support vector s relevant to given x such that sT x 6= 0. The time complexity of PKI is 1 P O(B · |x |+ |S|) where B ≡ |x| fj ∈x |h(fj )|, which is smaller than O(|S||x|) if x has many rare features fj such that |h(fj ) | |S|. To the best of our knowledge, this is the only exact method that has been used to speed up margin computation in the context of kernel-based online learning (Okanohara and Tsujii, 2007). 2.2.2 Kernel Expansion Isozaki and Kazawa (2002) and Kudo and Matsumoto (2003) proposed kernel expansion, which explicitly maps both support set S and given example x ∈ Rn into RH by mapping φd imposed by kd : 1246 m(x) = X si ∈S !T αi φd (si ) φd (x) = X fi ∈xd wi , where xd ∈ {0, 1}H is a binary feature vector in which xdi = 1 for (φd (x))i 6= 0, and w is a weight vector in the expanded feature space, F d . The weight vector w is computed from S and α: w= X αi si ∈S d X ckd Ik (sdi ), (1) k=0 where ckd is a squared coefficient of k-th order conjunctive features for d-th order polynomial kernel (e.g., c02 = 1, c12 = 3, and c22 = 2)1 and Ik"
C10-1140,W08-2103,0,0.0526474,"Missing"
C10-1140,D09-1160,1,0.90513,"the trie. We carry out an analogous traversal in updating the parameters of conjunctive features, while registering a new conjunctive feature by adding an edge to the trie. The base learner with kernel splitting combines the virtues of linear training and kernel-based training. It reduces to linear training when we increase N to |F|, while it reduces to kernel-based training when we decrease N to 0. The output is support set S|T |and coefficients α|T |(option˜ to which the efficient classification techally, w), niques discussed in Section 2.2 and the one proposed by Yoshinaga and Kitsuregawa (2009) can be applied. Note on weight trie construction The time and space efficiency of this learner strongly depends on the way the weight trie is constructed. We need to address two practical issues that greatly affect efficiency. First, we traverse the trie from the rarest feature that constitutes a conjunctive feature. This rare-to-frequent mining helps us to avoid enumerating higher-order conjunctive features that have not been registered in the trie, when computing margin. Second, we use RANK(f ) encoded into a dlog128 RANK(f )e-byte string by using variable-byte coding (Williams and Zobel, 1"
C14-1103,D11-1145,0,0.021401,"on The rapid growth in popularity of microblogs (e.g., Twitter) is enabling more and more people to instantly publish their experiences or thoughts any time they want from mobile devices. Since information in text posted by hundreds of millions of those people covers every space and time in the real world, analyzing such a text stream tells us what is going on in the real world and is therefore beneficial for reducing damage caused by natural disasters (Sakaki et al., 2010; Neubig et al., 2011a; Varga et al., 2013), monitoring political sentiment (Tumasjan et al., 2010) and disease epidemics (Aramaki et al., 2011), and predicting stock market (Gilbert and Karahalios, 2010) and criminal incident (Wang et al., 2012). Text-stream processing, however, faces a new challenge; namely, the quality (content) and quantity (volume of flow) changes dramatically, reflecting a change in the real world. Current studies on processing microblogs have focused mainly on the difference between the quality of microblogs (or spoken languages) and news articles (or written languages) (Gimpel et al., 2011; Foster et al., 2011; Ritter et al., 2011; Han and Baldwin, 2011), and they have not addressed the issue of so-called “bur"
C14-1103,P11-2008,0,0.0546692,"Missing"
C14-1103,P13-2111,0,0.0206487,"ency parsing, respectively. An analogous algorithm for integer linear program (ILP) used to solve structured classification was proposed by Srikumar, Kundu, and Roth (2012; 2013). The algorithm was reported to achieve speed-up factors of 2.6 and 1.6 for semantic role labeling and entityrelation extraction, respectively. Although these two algorithms can be applied to various NLP tasks that can be solved by using a linear classifier or an ILP solver, how effective they are for processing a text stream is not clear. A method of feature sharing for beam-search incremental parsers was proposed by Goldberg et al. (2013). Motivated by the observation that beam parsers solve similar classification problems in different parts of the beam, this method reuses partial results computed in the previous beam items. It reportedly achieved a speed-up factor of 1.2 for arc-standard and arc-eager dependency parsers. The key differences between the method proposed in this study and their feature-sharing method are twofold. First, the feature sharing in Goldberg et al. (2013) is performed in a token-wise manner in the sense that a key to retrieve a cached result is represented by a bag of tokens that invoke features, which"
C14-1103,P11-1038,0,0.018977,"sentiment (Tumasjan et al., 2010) and disease epidemics (Aramaki et al., 2011), and predicting stock market (Gilbert and Karahalios, 2010) and criminal incident (Wang et al., 2012). Text-stream processing, however, faces a new challenge; namely, the quality (content) and quantity (volume of flow) changes dramatically, reflecting a change in the real world. Current studies on processing microblogs have focused mainly on the difference between the quality of microblogs (or spoken languages) and news articles (or written languages) (Gimpel et al., 2011; Foster et al., 2011; Ritter et al., 2011; Han and Baldwin, 2011), and they have not addressed the issue of so-called “bursts” that increase the volume of text. Although it is desirable to use NLP analyzers with the highest possible accuracy for processing a text stream, high accuracy is generally attained by costly structured classification or classification with rich features, typically conjunctive features (Liang et al., 2008). It is therefore inevitable to trade accuracy for speed by using only a small fraction of features to assure real-time processing. In this study, the aforementioned text-quantity issue concerning processing a text stream is address"
C14-1103,C02-1054,0,0.0636047,"ifier that enumerates common classification problems from a given text stream and reuses their results is proposed. As a result, the proposed classifier adaptively speeds up the classification of forthcoming classification problems. 3 Preliminaries As the basis of the proposed classifier, the previously-presented classifier that uses results of common classification problems (Yoshinaga and Kitsuregawa, 2009) is described as follows. This base classifier targets a linear classifier trained with many conjunctive features (including one converted from a classifier trained with polynomial kernel (Isozaki and Kazawa, 2002)) that are widely used for many NLP tasks. Although this classifier (and also the one proposed in this paper) can handle a multi-class classification problem, a binary classification problem is assumed here for brevity. 1092 A binary classifier such as a perceptron and a support vector machine determines label y ∈ {+1, −1} of input classification problem x by using the following equation (from which the bias term is omitted for brevity): X m(x; φ, w) = w T φ(x) = wi φi (x) (1) i  +1 (m(x; φ, w) ≥ 0) −1 (m(x; φ, w) &lt; 0). y= (2) Here, φi is a feature function, wi is a weight for φi obtained as"
C14-1103,C08-1046,0,0.0203699,"Missing"
C14-1103,P10-1050,1,0.83504,"cense. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1091 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1091–1102, Dublin, Ireland, August 23-29 2014. 2 Related work A sentence is the processing unit used for fundamental NLP tasks such as word segmentation, part-ofspeech tagging, phrase chunking, syntactic parsing, and semantic role labeling. Most efficient algorithms solving these tasks thus aim at speeding up the processing based on this unit (Kaji et al., 2010; Koo et al., 2010; Rush and Petrov, 2012), and few studies have attempted to speed up the processing of a given text (a set of sentences) as a whole. In the following, reported algorithms that adaptively speed up NLP analyzers for a given text are introduced. A method of speeding up a classifier trained with many conjunctive features by using precomputed results for common classification problems was proposed by Yoshinaga and Kitsuregawa (2009; 2012). It solves classification problems that commonly appear in the processing of a large amount of text in advance and stores the results in a trie,"
C14-1103,kawahara-etal-2002-construction,0,0.0834299,"-phrase chunker processes each token in a sentence identified by a morphological analyzer, MeCab,7 and judges whether the token is the beginning of a base-phrase chunk in Japanese (called a bunsetsu8 ) or not. The shift-reduce dependency parser processes each chunk in the chunked sentences and determines whether the head candidate chosen by the parser is correct head or not. The classifiers for base-phrase chunking and dependency parsing were trained by using a variant of a passive-aggressive algorithm (PA - I) (Crammer et al., 2006) with a standard split9 of the Kyoto-University Text Corpus (Kawahara et al., 2002) Version 4.0.10 A third-order polynomial kernel was used to consider combinations of up-to three primitive features. The features used for training the classifiers were identical to those implemented in J.DepP. The polynomial kernel expanded (Kudo and Matsumoto, 2003) was used to make the number of resulting conjunctive features tractable without harming the accuracy. Table 1 lists the statistics of the models trained for chunking and parsing. In Table 1, “accuracy (partial)” is the ratio of chunks (or dependency arcs) correctly identified by the chunker (or the parser), while “accuracy (compl"
C14-1103,D10-1125,0,0.0144887,"and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1091 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1091–1102, Dublin, Ireland, August 23-29 2014. 2 Related work A sentence is the processing unit used for fundamental NLP tasks such as word segmentation, part-ofspeech tagging, phrase chunking, syntactic parsing, and semantic role labeling. Most efficient algorithms solving these tasks thus aim at speeding up the processing based on this unit (Kaji et al., 2010; Koo et al., 2010; Rush and Petrov, 2012), and few studies have attempted to speed up the processing of a given text (a set of sentences) as a whole. In the following, reported algorithms that adaptively speed up NLP analyzers for a given text are introduced. A method of speeding up a classifier trained with many conjunctive features by using precomputed results for common classification problems was proposed by Yoshinaga and Kitsuregawa (2009; 2012). It solves classification problems that commonly appear in the processing of a large amount of text in advance and stores the results in a trie, so that they can"
C14-1103,P03-1004,0,0.0313757,"in the chunked sentences and determines whether the head candidate chosen by the parser is correct head or not. The classifiers for base-phrase chunking and dependency parsing were trained by using a variant of a passive-aggressive algorithm (PA - I) (Crammer et al., 2006) with a standard split9 of the Kyoto-University Text Corpus (Kawahara et al., 2002) Version 4.0.10 A third-order polynomial kernel was used to consider combinations of up-to three primitive features. The features used for training the classifiers were identical to those implemented in J.DepP. The polynomial kernel expanded (Kudo and Matsumoto, 2003) was used to make the number of resulting conjunctive features tractable without harming the accuracy. Table 1 lists the statistics of the models trained for chunking and parsing. In Table 1, “accuracy (partial)” is the ratio of chunks (or dependency arcs) correctly identified by the chunker (or the parser), while “accuracy (complete)” is the exact-match accuracy of complete chunks (or dependency arcs) in a sentence. The accuracy of the resulting parser on the standard split was better than any published results 6 https://dev.twitter.com/docs/api http:://mecab.sourceforge.net/ 8 A bunsetsu is"
C14-1103,P13-1089,0,0.0441456,"Missing"
C14-1103,P05-1012,0,0.0222811,"Missing"
C14-1103,I11-1108,0,0.110877,"e-phrase chunker and dependency parser speeds up its classification by factors of 3.2 and 5.7, respectively. 1 Introduction The rapid growth in popularity of microblogs (e.g., Twitter) is enabling more and more people to instantly publish their experiences or thoughts any time they want from mobile devices. Since information in text posted by hundreds of millions of those people covers every space and time in the real world, analyzing such a text stream tells us what is going on in the real world and is therefore beneficial for reducing damage caused by natural disasters (Sakaki et al., 2010; Neubig et al., 2011a; Varga et al., 2013), monitoring political sentiment (Tumasjan et al., 2010) and disease epidemics (Aramaki et al., 2011), and predicting stock market (Gilbert and Karahalios, 2010) and criminal incident (Wang et al., 2012). Text-stream processing, however, faces a new challenge; namely, the quality (content) and quantity (volume of flow) changes dramatically, reflecting a change in the real world. Current studies on processing microblogs have focused mainly on the difference between the quality of microblogs (or spoken languages) and news articles (or written languages) (Gimpel et al., 2011"
C14-1103,P11-2093,0,0.0905709,"e-phrase chunker and dependency parser speeds up its classification by factors of 3.2 and 5.7, respectively. 1 Introduction The rapid growth in popularity of microblogs (e.g., Twitter) is enabling more and more people to instantly publish their experiences or thoughts any time they want from mobile devices. Since information in text posted by hundreds of millions of those people covers every space and time in the real world, analyzing such a text stream tells us what is going on in the real world and is therefore beneficial for reducing damage caused by natural disasters (Sakaki et al., 2010; Neubig et al., 2011a; Varga et al., 2013), monitoring political sentiment (Tumasjan et al., 2010) and disease epidemics (Aramaki et al., 2011), and predicting stock market (Gilbert and Karahalios, 2010) and criminal incident (Wang et al., 2012). Text-stream processing, however, faces a new challenge; namely, the quality (content) and quantity (volume of flow) changes dramatically, reflecting a change in the real world. Current studies on processing microblogs have focused mainly on the difference between the quality of microblogs (or spoken languages) and news articles (or written languages) (Gimpel et al., 2011"
C14-1103,P08-1108,0,0.0148722,"ing two sets of Twitter streams on the day of the 2011 Great East Japan Earthquake and the second weekend in March 2012 using a state-of-the-art base-phrase chunker and dependency parser. The proposed classifier speeds up the classification by factors of 3.2 (chunking) and 5.7 (parsing), which are significant factors in regard to processing a massive text stream. It is planned to evaluate the classifier on other NLP tasks. A linear classifier with conjunctive features is widely used for NLP tasks such as word segmentation, part-of-speech tagging (Neubig et al., 2011b), and dependency parsing (Nivre and McDonald, 2008). Even for NLP tasks in which structured classification is effective (e.g., named entity recognition), structure compilation (Liang et al., 2008) (or “uptraining” (Petrov et al., 2010)) gives state-of-the-art accuracy when a linear classifier with many conjunctive features is used. The proposed classifier is expected to be applied to a range of NLP tasks. All the codes have been available for the research community as open-source software, including pecco (a self-adaptive classifier)12 and J.DepP (a base-phrase chunker and dependency parser).13 Acknowledgments This work was supported by the Re"
C14-1103,D10-1069,0,0.0259796,"Missing"
C14-1103,D11-1141,0,0.0204531,"monitoring political sentiment (Tumasjan et al., 2010) and disease epidemics (Aramaki et al., 2011), and predicting stock market (Gilbert and Karahalios, 2010) and criminal incident (Wang et al., 2012). Text-stream processing, however, faces a new challenge; namely, the quality (content) and quantity (volume of flow) changes dramatically, reflecting a change in the real world. Current studies on processing microblogs have focused mainly on the difference between the quality of microblogs (or spoken languages) and news articles (or written languages) (Gimpel et al., 2011; Foster et al., 2011; Ritter et al., 2011; Han and Baldwin, 2011), and they have not addressed the issue of so-called “bursts” that increase the volume of text. Although it is desirable to use NLP analyzers with the highest possible accuracy for processing a text stream, high accuracy is generally attained by costly structured classification or classification with rich features, typically conjunctive features (Liang et al., 2008). It is therefore inevitable to trade accuracy for speed by using only a small fraction of features to assure real-time processing. In this study, the aforementioned text-quantity issue concerning processing"
C14-1103,N12-1054,0,0.0126454,"ooter are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1091 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1091–1102, Dublin, Ireland, August 23-29 2014. 2 Related work A sentence is the processing unit used for fundamental NLP tasks such as word segmentation, part-ofspeech tagging, phrase chunking, syntactic parsing, and semantic role labeling. Most efficient algorithms solving these tasks thus aim at speeding up the processing based on this unit (Kaji et al., 2010; Koo et al., 2010; Rush and Petrov, 2012), and few studies have attempted to speed up the processing of a given text (a set of sentences) as a whole. In the following, reported algorithms that adaptively speed up NLP analyzers for a given text are introduced. A method of speeding up a classifier trained with many conjunctive features by using precomputed results for common classification problems was proposed by Yoshinaga and Kitsuregawa (2009; 2012). It solves classification problems that commonly appear in the processing of a large amount of text in advance and stores the results in a trie, so that they can be reused as partial res"
C14-1103,C04-1002,0,0.228977,"a text stream increases. To adaptively speed up the NLP classifier, the proposed algorithm thus enumerates common classification problems from seen classification problems and keeps their classification results as partial results for use in solving forthcoming classification problems. The proposed classifier was evaluated by applying it to streams of classification problems generated during the processing of the Twitter streams on the day of the 2011 Great East Japan Earthquake and on another day in March 2012 using a state-of-the-art base-phrase chunker (Sassano, 2008) and dependency parser (Sassano, 2004), and the obtained results confirm the effectiveness of the proposed algorithm. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1091 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1091–1102, Dublin, Ireland, August 23-29 2014. 2 Related work A sentence is the processing unit used for fundamental NLP tasks such as word segmentation, part-ofspeech tagging, phrase chu"
C14-1103,I08-2117,0,0.0448277,"ese events when the volume of flow in a text stream increases. To adaptively speed up the NLP classifier, the proposed algorithm thus enumerates common classification problems from seen classification problems and keeps their classification results as partial results for use in solving forthcoming classification problems. The proposed classifier was evaluated by applying it to streams of classification problems generated during the processing of the Twitter streams on the day of the 2011 Great East Japan Earthquake and on another day in March 2012 using a state-of-the-art base-phrase chunker (Sassano, 2008) and dependency parser (Sassano, 2004), and the obtained results confirm the effectiveness of the proposed algorithm. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1091 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1091–1102, Dublin, Ireland, August 23-29 2014. 2 Related work A sentence is the processing unit used for fundamental NLP tasks such as word segmentat"
C14-1103,D12-1102,0,0.0659887,"Missing"
C14-1103,E09-1093,0,0.0710303,"Missing"
C14-1103,I13-1061,0,0.0169572,"arser by filtering out those unpromising derivation steps. Although this method was reported to attain a speed-up factor of four while keeping parsing accuracy, it needs to be tuned to trade parsing accuracy and speed for each domain. It is difficult to derive the true potential of their method in regard to processing a text stream whose domain shifts from time to time. It has been demonstrated by Wachsmuth et al. (2011) that tuning a pipeline schedule of an information extraction (IE) system improves the efficiency of the system. Furthermore, the self-supervised learning algorithm devised by Wachsmuth et al. (2013) predicts the processing time for each possible pipeline schedule of an IE system, and the prediction is used to adaptively change the pipeline schedule for a given text stream. This method and the proposed method for speeding up an NLP classifier are complementary, and a combination of both methods is expected to synergistically speed up various NLP-systems. In this study, based on the classifier proposed by Yoshinaga and Kitsuregawa (2009), a self-adaptive classifier that enumerates common classification problems from a given text stream and reuses their results is proposed. As a result, the"
C14-1103,D13-1023,0,0.0167559,"s used to encode feature sequences. Each feature index is replaced with a gap from the preceding feature index (the first feature index is used as is). Each gap is then encoded by variable-byte coding (Williams and Zobel, 1999) to obtain shorter representations of feature sequences. A reduced double-array trie The standard implementation of a double-array trie stores an (integer) index with a key at a child node (value node) traversed by a terminal symbol ’ ’ (or an alphabet not included in a key, e.g., ’#’) from the node reached after reading the entire key (Yoshinaga and Kitsuregawa, 2009; Yasuhara et al., 2013). However, when a key is not a prefix to the other keys, the value node has no sibling node, so a value can be directly embedded on the BASE of the node reached after reading the entire key instead of the offset address of the child (value) node. All the value nodes for the longest prefixes are thereby eliminated from the trie. The resulting double-array trie is referred to as a reduced double-array trie. These two tricks reduce the number of trie nodes (memory usage), and make the trie operations faster. A reduced double-array trie is also used to compactly store the weights of conjunctive fe"
C14-1103,D09-1160,1,0.322818,"ase chunking, syntactic parsing, and semantic role labeling. Most efficient algorithms solving these tasks thus aim at speeding up the processing based on this unit (Kaji et al., 2010; Koo et al., 2010; Rush and Petrov, 2012), and few studies have attempted to speed up the processing of a given text (a set of sentences) as a whole. In the following, reported algorithms that adaptively speed up NLP analyzers for a given text are introduced. A method of speeding up a classifier trained with many conjunctive features by using precomputed results for common classification problems was proposed by Yoshinaga and Kitsuregawa (2009; 2012). It solves classification problems that commonly appear in the processing of a large amount of text in advance and stores the results in a trie, so that they can be reused as partial results for solving new classification problems. This method was reported to achieve speed-up factors of 3.3 and 10.6 for base-phrase chunking and dependency parsing, respectively. An analogous algorithm for integer linear program (ILP) used to solve structured classification was proposed by Srikumar, Kundu, and Roth (2012; 2013). The algorithm was reported to achieve speed-up factors of 2.6 and 1.6 for se"
C14-1103,C10-1140,1,0.946247,"ce trie size To effectively maintain common classification problems in a trie, it is critical to reduce the number of trie nodes accessed in look-up, update, and deletion operations. The number of trie nodes was therefore reduced as much as possible by adopting a more compact representation of keys (common classification problems) and by elaborating the way to store values for the keys in the double-array trie. Gap-based key representation To compress representations of common classification problems (feature sequences) in the trie, frequency-based indices are allocated to primitive features (Yoshinaga and Kitsuregawa, 2010). A gap representation (used to compress posting lists in information retrieval (Manning et al., 2008, Chapter 5)) is used to encode feature sequences. Each feature index is replaced with a gap from the preceding feature index (the first feature index is used as is). Each gap is then encoded by variable-byte coding (Williams and Zobel, 1999) to obtain shorter representations of feature sequences. A reduced double-array trie The standard implementation of a double-array trie stores an (integer) index with a key at a child node (value node) traversed by a terminal symbol ’ ’ (or an alphabet not"
C14-1103,P13-1159,0,\N,Missing
C16-2061,sumida-etal-2008-boosting,1,0.903751,"ts in order to come up with a convincing ordering. In this study, we present Kotonush, a system that induces people’s values on given concepts from social media text as concept orderings on the basis of common attribute intensity expressed by an adjective. Our system enables users to interactively ask queries (concepts and an adjective) and compare the induced orderings for deeper understanding of the concepts. Assuming that a user has at least one target concept (or entity) in mind, our querying interface helps the user to interactively list similar entities using massive hyponymy relations (Sumida et al., 2008). Receiving a query, a text-to-ordering module (Iwanari et al., 2016) collects posts from social media text written by specific (gender, region) users and at a certain time of interest (say, domain) to induce concept orderings specific to the chosen domain. Our ordering visualizer then provides intelligent interfaces to compare orderings from various perspectives to gain a deeper insight into the domain-specific values of concepts. Our system is beneficial not only in practical terms for understanding entities from others’ values (orderings with related entities) to make correct decisions (e.g"
C16-2061,C14-1103,1,0.817562,"not be able to remember appropriate movies for comparison. The same applies here. To help such users, Kotonush suggests concepts related to given concepts. We exploit hyponymy relations extracted from Wikipedia (Sumida et al., 2008) to suggest concepts that share the same hypernym with the given concepts. Concept ordering After receiving a query, the text-to-ordering module retrieves posts including one or more of the given concepts and the adjective from social media text in the specified domain. The posts are then online parsed with J.DepP, a state-of-the-art dependency parser for Japanese (Yoshinaga and Kitsuregawa, 2014), to process massive text online (&gt; 10, 000 sentences/s). The parsed text is given to our implementation of Iwanari et al. (2016) to induce a concept ordering. The method uses four types of evidence to capture the common view on concepts from social media text: (1) co-occurrences of a concept and an adjective (e.g., How large that whale is!), (2) dependencies from a concept to an adjective (e.g., A whale is so big.), (3) similes (e.g., He is brave as a lion.), and (4) comparative expressions (e.g., Whales are larger than cats.). The first three implicitly suggest attribute intensity and can be"
D09-1160,J96-1002,0,0.00658884,"ssociated with fi,y , and Z(x) = y exp i wi,y fi,y (x, y) is the partition function. We can consider feature combinations in LLM s by explicitly introducing a new conjunctive feature fF 0 ,y (x, y) that is activated when a particular set of features F 0 ⊆ F to V be combined is activated (namely, fF 0 ,y (x, y) = fi,y ∈F 0 fi,y (x, y)). We then introduce an `1 -regularized LLM (`1 LLM ), in which the weight vector w is tuned so as to maximize the logarithm of the a posteriori probability of the training data: Log-Linear Models The log-linear model (LLM), or also known as maximum-entropy model (Berger et al., 1996), is a linear classifier widely used in the NLP literature. Let the training data of LLMs be {hxi , yi i}L i=1 , n where xi ∈ {0, 1} is a feature vector and yi is a class label associated with xi . We assume a binary label yi ∈ {±1} here to simplify the argument. The classifier provides conditional probability p(y|x) for a given feature vector x and a label y: p(y|x) = X 1 exp wi,y fi,y (x, y), (1) Z(x) i L X log p(yi |xi ) − Ckwk1 . (2) i=1 Hyper-parameter C thereby controls the degree of over-fitting (solution sparseness). Interested readers may refer to the cited literature (Andrew and Gao,"
D09-1160,W08-0804,0,0.031948,"n of the fstrie required to achieve the same speed-up. The implementations of the proposed algorithm for LLMs and SVMs (with a polynomial kernel) and the Japanese dependency parser will be available at http://www.tkl.iis.u-tokyo.ac.jp/˜ynaga/. We plan to apply our method to wider range of classifiers used in various NLP tasks. To speed up classifiers used in a real-time application, we can build fstries incrementally by using feature vectors generated from user inputs. When we run our classifiers on resource-tight environments such as cell-phones, we can use a random feature mixing technique (Ganchev and Dredze, 2008) or a memory-efficient trie implementation based on a succinct data structure (Jacobson, 1989; Delpratt et al., 2006) to reduce required memory usage. We will combine our method with other techniques that provide sparse solutions, for example, kernel methods on a budget (Dekel and Singer, 2007; Dekel et al., 2008; Orabona et al., 2008) or kernel approximation (surveyed in Kashima et al. (2009)). It is also easy to combine our method with SVMs with partial kernel expansion (Goldberg and Elhadad, 2008), which will yield slower but more space-efficient classifiers. We will in the future consider"
D09-1160,P07-1104,0,0.0199724,"combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). `1 -regularized log-linear models (`1 - LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training `1 - LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy. In this study, we provide a simple, but effective solution to the inefficiency"
D09-1160,N06-1020,0,0.0815334,"Missing"
D09-1160,P79-1022,0,0.183584,"Missing"
D09-1160,N04-1039,0,0.046835,"nsider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). `1 -regularized log-linear models (`1 - LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training `1 - LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy. In this study, we provide a simple, but effective solution"
D09-1160,C08-1079,0,0.012756,"Spoustová et al., 2009). One of the main reasons for this inefficiency is attributed to the inefficiency of core classifiers trained with many feature combinations (e.g., word n-grams). Hereafter, we refer to features that explicitly represent combinations of features as conjunctive features and the other atomic features as primitive features. The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et al., 2008), parse re-ranking (McClosky et al., 2006), pronoun resolution (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007). However, ‘explicit’ feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier. Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003;"
D09-1160,C02-1054,0,0.694635,"., 2006), pronoun resolution (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007). However, ‘explicit’ feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier. Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). `1 -regularized log-linear models (`1 - LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training `1 - LLMs appear rarely in the task, we cannot greatly reduce the number of acti"
D09-1160,W03-3017,0,0.042231,"ent words followed by zero or more function words. A parser generates a feature vec1546 Modifier, head word (surface-form, POS, POS-subcategory, modifiee inflection form), functional word (surface-form, bunsetsu POS, POS-subcategory, inflection form), brackets, quotation marks, punctuation marks, position in sentence (beginning, end) Between distance (1, 2–5, 6–), case-particles, brackets, bunsetsus quotation marks, punctuation marks Table 1: Feature set used for experiments. tor for a particular pair of bunsetsus (modifier and modifiee candidates) by exploiting the head-final and projective (Nivre, 2003) nature of dependency relations in Japanese. The classifier then outputs label y = ‘+1’ (dependent) or ‘−1’ (independent). Since our classifier is independent of individual parsing algorithms, we targeted speeding up (a classifier in) the shift-reduce parser proposed by Sassano (2004), which has been reported to be the most efficient for this task, with almost stateof-the-art accuracy (Iwatate et al., 2008). This parser decreases the number of classification steps by using the fact that a bunsetsu is likely to modify a bunsetsu close to itself. Due to space limitations, we omit the details of"
D09-1160,C08-1046,0,0.0601255,"s quotation marks, punctuation marks Table 1: Feature set used for experiments. tor for a particular pair of bunsetsus (modifier and modifiee candidates) by exploiting the head-final and projective (Nivre, 2003) nature of dependency relations in Japanese. The classifier then outputs label y = ‘+1’ (dependent) or ‘−1’ (independent). Since our classifier is independent of individual parsing algorithms, we targeted speeding up (a classifier in) the shift-reduce parser proposed by Sassano (2004), which has been reported to be the most efficient for this task, with almost stateof-the-art accuracy (Iwatate et al., 2008). This parser decreases the number of classification steps by using the fact that a bunsetsu is likely to modify a bunsetsu close to itself. Due to space limitations, we omit the details of the parsing algorithm. We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1). Note that features listed in the ‘Between bunsetsus’ row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector. This task is therefore a bett"
D09-1160,N09-2025,0,0.0533814,"ions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training `1 - LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy. In this study, we provide a simple, but effective solution to the inefficiency of classifiers trained with higher-order conjunctive features (or polynomial kernel), by exploiting the Zipfian nature of language data. The key idea is to precompute the weights of primitive feature vectors and use them as partial results to compute the weight of a given feature vector. We use a trie called the feature sequence trie to efficiently find for a given feature vector its longest prefix feature vector whose weight has been co"
D09-1160,W03-1018,0,0.0177232,"vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). `1 -regularized log-linear models (`1 - LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training `1 - LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classification. In the end, when efficiency is a major concern, we must use exhaustive feature selection (Wu et al., 2007; Okanohara and Tsujii, 2009) or even restrict the order of conjunctive features at the expense of accuracy. In this study, we provide a simple, but effe"
D09-1160,C04-1002,0,0.245036,"tuation marks, position in sentence (beginning, end) Between distance (1, 2–5, 6–), case-particles, brackets, bunsetsus quotation marks, punctuation marks Table 1: Feature set used for experiments. tor for a particular pair of bunsetsus (modifier and modifiee candidates) by exploiting the head-final and projective (Nivre, 2003) nature of dependency relations in Japanese. The classifier then outputs label y = ‘+1’ (dependent) or ‘−1’ (independent). Since our classifier is independent of individual parsing algorithms, we targeted speeding up (a classifier in) the shift-reduce parser proposed by Sassano (2004), which has been reported to be the most efficient for this task, with almost stateof-the-art accuracy (Iwatate et al., 2008). This parser decreases the number of classification steps by using the fact that a bunsetsu is likely to modify a bunsetsu close to itself. Due to space limitations, we omit the details of the parsing algorithm. We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1). Note that features listed in the ‘Between bunsetsus’ row represent contexts between the target pair of bunsetsus and appear indepen"
D09-1160,E09-1087,0,0.0331956,"Missing"
D09-1160,W02-2016,0,0.0213352,"= ‘+1’ (dependent) or ‘−1’ (independent). Since our classifier is independent of individual parsing algorithms, we targeted speeding up (a classifier in) the shift-reduce parser proposed by Sassano (2004), which has been reported to be the most efficient for this task, with almost stateof-the-art accuracy (Iwatate et al., 2008). This parser decreases the number of classification steps by using the fact that a bunsetsu is likely to modify a bunsetsu close to itself. Due to space limitations, we omit the details of the parsing algorithm. We used the standard feature set tailored for this task (Kudo and Matsumoto, 2002; Sassano, 2004; Iwatate et al., 2008) (Table 1). Note that features listed in the ‘Between bunsetsus’ row represent contexts between the target pair of bunsetsus and appear independently from other features, which will become an obstacle to finding the longest prefix vector. This task is therefore a better measure of our method than simple sequential labeling such as POS tagging or named-entity recognition. For evaluation, we used Kyoto Text Corpus Version 4.0 (Kurohashi and Nagao, 2003), Mainichi news articles in 1995 that have been manually annotated with dependency relations.6 The training"
D09-1160,P03-1004,0,0.631317,"on (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007). However, ‘explicit’ feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier. Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). `1 -regularized log-linear models (`1 - LLMs), on the other hand, provide sparse solutions, in which weights of irrelevant features are exactly zero, by assuming a Laplacian prior on the weights (Tibshirani, 1996; Kazama and Tsujii, 2003; Goodman, 2004; Gao et al., 2007). However, as Kazama and Tsujii (2005) have reported in a text categorization task and we later confirm in a dependency parsing task, when most features regarded as irrelevant during training `1 - LLMs appear rarely in the task, we cannot greatly reduce the number of active features in each classi"
D09-1160,P07-2017,0,0.161818,"Missing"
D09-1160,D07-1062,0,0.0217089,"or this inefficiency is attributed to the inefficiency of core classifiers trained with many feature combinations (e.g., word n-grams). Hereafter, we refer to features that explicitly represent combinations of features as conjunctive features and the other atomic features as primitive features. The feature combinations play an essential role in obtaining a classifier with state-of-the-art accuracy for several NLP tasks; recent examples include dependency parsing (Koo et al., 2008), parse re-ranking (McClosky et al., 2006), pronoun resolution (Nguyen and Kim, 2008), and semantic role labeling (Liu and Sarkar, 2007). However, ‘explicit’ feature combinations significantly increase the feature space, which slows down not only training but also testing of the classifier. Kernel-based methods such as support vector machines (SVMs) consider feature combinations space-efficiently by using a polynomial kernel function (Cortes and Vapnik, 1995). The kernelbased classification is, however, known to be very slow in NLP tasks, so efficient classifiers should sum up the weights of the explicit conjunctive features (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Goldberg and Elhadad, 2008). `1 -regularized log-l"
D09-1160,P08-2060,0,\N,Missing
D09-1160,P08-1068,0,\N,Missing
D12-1081,E12-1019,0,0.0190958,"hey sometimes leaded to misclassification. 6 Related Work In recent years, much attention has been given to extracting relations from a massive amount of textual data, especially the web (cf. section 1). Most of those studies, however, explored just extracting relations from text. Only a few studies, as described below, have discussed classifying those relations. There has been no previous work on identifying the constancy of relations. The most relevant research topic is the temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; Ling and Weld, 2010; Wang et al., 2010; Hovy et al., 2012). This is the task of extracting from textual data an event and the time it happened, e.g., Othello was written by Shakespeare in 1602. Such temporal information alone is not sufficient for identifying the constancy of relations, while we think it would be helpful. On the other hand, the uniqueness of relations has so far been discussed in some studies. Ritter et al. (2008) have pointed out the importance of identifying unique relations for various NLP tasks such as contradiction detection, quantifier scope disambiguation, and synonym resolution. They proposed an EM-style algorithm for scoring"
D12-1081,P09-1113,0,0.0134588,"Experimental results confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification. (1) a. 1Q84 is written by Haruki Murakami. b. Moselle river flows through Germany. c. U.S.’s president is George Bush. d. Pentax sells K-5, a digital SLR. 1 Introduction We have witnessed a number of success stories in acquiring semantic relations between entities from ever-increasing text on the web (Pantel and Pennacchiotti, 2006; Banko et al., 2007; Suchanek et al., 2007; Wu et al., 2008; Zhu et al., 2009; Mintz et al., 2009; Wu and Weld, 2010). These studies have successfully revealed to us millions of relations between real-world entities, which have been proven to be beneficial in solving knowledge-rich problems such as question answering and textual entailment (Ferrucci et al., 2010). ∗ This work was conducted while the first author was a graduate student at University of Tokyo. Here, italicized predicates represent the relations, while underlined entities are their arguments. The relations in statements 1a and 1b are true across time, so we can simply accumulate all the relation instances. The relations in 1"
D12-1081,P06-1015,0,0.0406745,"t massive time-series web texts to induce features on the basis of time-series frequency and linguistic cues. Experimental results confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification. (1) a. 1Q84 is written by Haruki Murakami. b. Moselle river flows through Germany. c. U.S.’s president is George Bush. d. Pentax sells K-5, a digital SLR. 1 Introduction We have witnessed a number of success stories in acquiring semantic relations between entities from ever-increasing text on the web (Pantel and Pennacchiotti, 2006; Banko et al., 2007; Suchanek et al., 2007; Wu et al., 2008; Zhu et al., 2009; Mintz et al., 2009; Wu and Weld, 2010). These studies have successfully revealed to us millions of relations between real-world entities, which have been proven to be beneficial in solving knowledge-rich problems such as question answering and textual entailment (Ferrucci et al., 2010). ∗ This work was conducted while the first author was a graduate student at University of Tokyo. Here, italicized predicates represent the relations, while underlined entities are their arguments. The relations in statements 1a and 1"
D12-1081,D08-1002,0,0.159876,"ions prevalent in their outcome incur a serious problem in maintaining the acquired relations. The notion of constancy is meant to resolve this stalemate. • We have for the first time demonstrated the usefulness of a time-series text in relation acquisition and confirmed its impact in the two relation classification tasks. The features induced from the time-series text have greatly contributed to the accuracy of the classification based on uniqueness as well as the recall of the classification based on constancy. 2 This kind of relation is referred to as functional relation in the literature (Ritter et al., 2008; Lin et al., 2010). 884 Constant arg1 was born in arg2 arg1 is a father of arg2 arg1 is written by arg2 Non-constant arg1’s president is arg2 arg1 belongs to arg2 arg1 lives in arg2 Table 1: Examples of constant, non-constant relations. The reminder of this paper is structured as follows. Section 2 introduces the two properties of relations (constancy and uniqueness) and then defines the task setting of this study. Sections 3 and 4 describe the features induced from time-series text for constancy and uniqueness classification, respectively. Section 5 reports experimental results. Section 6 ad"
D12-1081,S07-1014,0,0.0380865,"eature induction could in most cases avoid an adverse effect caused by these noises, they sometimes leaded to misclassification. 6 Related Work In recent years, much attention has been given to extracting relations from a massive amount of textual data, especially the web (cf. section 1). Most of those studies, however, explored just extracting relations from text. Only a few studies, as described below, have discussed classifying those relations. There has been no previous work on identifying the constancy of relations. The most relevant research topic is the temporal information extraction (Verhagen et al., 2007; Verhagen et al., 2010; Ling and Weld, 2010; Wang et al., 2010; Hovy et al., 2012). This is the task of extracting from textual data an event and the time it happened, e.g., Othello was written by Shakespeare in 1602. Such temporal information alone is not sufficient for identifying the constancy of relations, while we think it would be helpful. On the other hand, the uniqueness of relations has so far been discussed in some studies. Ritter et al. (2008) have pointed out the importance of identifying unique relations for various NLP tasks such as contradiction detection, quantifier scope disa"
D12-1081,P10-1013,0,0.0293401,"confirmed that the time-series frequency distributions contributed much to the recall of constancy identification and the precision of the uniqueness identification. (1) a. 1Q84 is written by Haruki Murakami. b. Moselle river flows through Germany. c. U.S.’s president is George Bush. d. Pentax sells K-5, a digital SLR. 1 Introduction We have witnessed a number of success stories in acquiring semantic relations between entities from ever-increasing text on the web (Pantel and Pennacchiotti, 2006; Banko et al., 2007; Suchanek et al., 2007; Wu et al., 2008; Zhu et al., 2009; Mintz et al., 2009; Wu and Weld, 2010). These studies have successfully revealed to us millions of relations between real-world entities, which have been proven to be beneficial in solving knowledge-rich problems such as question answering and textual entailment (Ferrucci et al., 2010). ∗ This work was conducted while the first author was a graduate student at University of Tokyo. Here, italicized predicates represent the relations, while underlined entities are their arguments. The relations in statements 1a and 1b are true across time, so we can simply accumulate all the relation instances. The relations in 1c and 1d in contrast"
D12-1081,D09-1160,1,0.709548,"and discussions We built labeled data and examine the classification performance of the proposed method. We also analyzed the influence of window size T on the performance, as well as major errors caused by our method. 5.1 Data We built a dataset for evaluation by extracting relations from the time-series text (section 3.1) and then manually annotating 1000 relations. The detailed procedure is as follows. First, we parsed the time-series text and extracted as relation dependency paths connecting two named entities. We used J.DepP,7 an efficient shift-reduce parser with feature sequence trie (Yoshinaga and Kitsuregawa, 2009; Yoshinaga and Kitsuregawa, 2010), for parsing. All Japanese words that conjugate were normalized into standard forms. 5 θ3 = 10 in our experiment. The keywords we used are 等, ら, たち, and 達. 7 http://www.tkl.iis.u-tokyo.ac.jp/ ∼ynaga/jdepp/ 6 888 Precision と, とか, や, やら, だの, なり, か Proposed Baseline 0.6 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1.0 Recall Figure 2: Recall-precision curve (constancy classification). Then, annotators were asked to label 1000 relations as not only constant or non-constant but also unique or non-unique. Three annotators were assigned to each relation, and the goldstandard label i"
D12-1081,C10-1140,1,0.848887,"data and examine the classification performance of the proposed method. We also analyzed the influence of window size T on the performance, as well as major errors caused by our method. 5.1 Data We built a dataset for evaluation by extracting relations from the time-series text (section 3.1) and then manually annotating 1000 relations. The detailed procedure is as follows. First, we parsed the time-series text and extracted as relation dependency paths connecting two named entities. We used J.DepP,7 an efficient shift-reduce parser with feature sequence trie (Yoshinaga and Kitsuregawa, 2009; Yoshinaga and Kitsuregawa, 2010), for parsing. All Japanese words that conjugate were normalized into standard forms. 5 θ3 = 10 in our experiment. The keywords we used are 等, ら, たち, and 達. 7 http://www.tkl.iis.u-tokyo.ac.jp/ ∼ynaga/jdepp/ 6 888 Precision と, とか, や, やら, だの, なり, か Proposed Baseline 0.6 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1.0 Recall Figure 2: Recall-precision curve (constancy classification). Then, annotators were asked to label 1000 relations as not only constant or non-constant but also unique or non-unique. Three annotators were assigned to each relation, and the goldstandard label is determined by majority vote. The"
D12-1081,D10-1123,0,\N,Missing
D12-1081,S10-1010,0,\N,Missing
D19-6504,N18-1118,0,0.17901,"strate the large impact of the data augmentation for context-aware NMT models in terms of BLEU score and specialized test sets on ja→en1 and fr→en. 1 Backtranslate Train Original Parallel Corpus Context-aware Forward-translation Model ja→en Target-side Original Monolingual Corpus (en) Source-side Pseudo Monolingual Corpus (ja) Pseudo Parallel Corpus Figure 1: Overview of the data augmentation for context-aware NMT (Japanese to English in this case). language (§ 2). Typically, contexts considered by context-aware NMT are surrounding sentences in the same document (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Voita et al., 2019), which provide beneficial information in translating zero pronouns, anaphoric pronouns, lexically ambiguous words, and so on. Although the context-aware NMT models outperform the baseline sentence-level NMT models in terms of BLEU score and some specialized test sets (Bawden et al., 2018; Voita et al., 2019; M¨uller et al., 2018), the reported gains, especially in BLEU score, are often marginal. We can think of several reasons for this; 1) the ratio of sentences (or linguistic phenomena) that require contexts for translation is"
D19-6504,2012.eamt-1.60,0,0.084387,"ext-aware NMT models using pseudo parallel data which is automatically generated by back-translating a large monolingual data (§ 3, Figure 1). The back-translation model here is trained on an existing parallel corpus. Since context-aware models are designed to recover information that is absent from the source sentence but should be present in the target sentence, back-translation can produce effective training data if it could naturally drop the information to be recovered in translating sentences in the target language into the source language. We evaluate our method on IWSLT2017 data sets (Cettolo et al., 2012), which are collections of subtitles of TED Talks, on two language pairs: English-Japanese (en-ja) and English-French (enfr) (§ 4). We exploit BookCorpus (Zhu et al., 2015), Europarl v7 (Koehn, 2005), and the record of the National Diet of Japan as monolingual corpora for back-translation (§ 5). Experimental results revealed that the data augmentation improved the translation in terms of BLEU score (Papineni et al., 2002) and the accuracy on specialized test sets for context-aware NMT. The contribution of this paper is as follows: 2 Context-aware NMT Models To incorporate contexts to translate"
D19-6504,D18-1045,0,0.100826,"el or context-aware NMT model for back-translation. In the following experiments, we first adopt 2-to-1 NMT model as a back-translator for data augmentation, and evaluate the impact of the data augmentation on the translation performance of context-aware NMT models. We then compare those results with results obtained by the data augmentation using 1-to-1 and 2-to-2 models instead of 2-to-1 model for back-translation. Back-translation for data augmentation The data augmentation in this study follows the existing back-translation strategies for NMT (Sennrich et al., 2016a; Imamura et al., 2018; Edunov et al., 2018) except that we assume a context-aware model for the forward-translation; the monolingual data for back-translation must be a set of doc37 en→ja ja→en en→fr fr→en # sentence pairs avg. source length avg. target length 223k / 0.87k / 1.54k 212k / 0.87k / 1.54k 222k / 0.89k / 1.56k 222k / 0.89k / 1.56k 24.7 / 28.0 / 24.6 22.3 / 28.0 / 24.6 22.1 / 27.2 / 24.3 23.5 / 28.0 / 25.8 25.4 / 27.9 / 24.5 22.8 / 27.9 / 24.5 23.5 / 28.0 / 25.8 22.1 / 27.2 / 24.3 Table 1: Statistics of IWSLT2017 corpora: the number of sentence pairs and the average length (number of tokens per sentence) for the train / dev"
D19-6504,W18-2707,0,0.241782,"either a sentence-level or context-aware NMT model for back-translation. In the following experiments, we first adopt 2-to-1 NMT model as a back-translator for data augmentation, and evaluate the impact of the data augmentation on the translation performance of context-aware NMT models. We then compare those results with results obtained by the data augmentation using 1-to-1 and 2-to-2 models instead of 2-to-1 model for back-translation. Back-translation for data augmentation The data augmentation in this study follows the existing back-translation strategies for NMT (Sennrich et al., 2016a; Imamura et al., 2018; Edunov et al., 2018) except that we assume a context-aware model for the forward-translation; the monolingual data for back-translation must be a set of doc37 en→ja ja→en en→fr fr→en # sentence pairs avg. source length avg. target length 223k / 0.87k / 1.54k 212k / 0.87k / 1.54k 222k / 0.89k / 1.56k 222k / 0.89k / 1.56k 24.7 / 28.0 / 24.6 22.3 / 28.0 / 24.6 22.1 / 27.2 / 24.3 23.5 / 28.0 / 25.8 25.4 / 27.9 / 24.5 22.8 / 27.9 / 24.5 23.5 / 28.0 / 25.8 22.1 / 27.2 / 24.3 Table 1: Statistics of IWSLT2017 corpora: the number of sentence pairs and the average length (number of tokens per sentence"
D19-6504,Q17-1024,0,0.0610534,"Missing"
D19-6504,P16-1009,0,0.522331,"are translation; existing large-scale and high-quality parallel corpora are usually obtained by extracting reliable sentence alignments from translations by humans (Nakazawa et al., 2016; Pryzant et al., 2018). Considering that context-aware NMT models have larger input spaces than sentence-level models, they will demand larger training data to fully exert the models’ performance. In this study, we hypothesize that context-aware NMT models can benefit from an increase of the training data more than sentence-level models, and confirm this by performing data augmentation using back-translation (Sennrich et al., 2016b) (§ 6) for context-aware NMT models. We propose to assist the training of context-aware NMT models using pseudo parallel data which is automatically generated by back-translating a large monolingual data (§ 3, Figure 1). The back-translation model here is trained on an existing parallel corpus. Since context-aware models are designed to recover information that is absent from the source sentence but should be present in the target sentence, back-translation can produce effective training data if it could naturally drop the information to be recovered in translating sentences in the target la"
D19-6504,2005.mtsummit-papers.11,0,0.566213,"l corpus. Since context-aware models are designed to recover information that is absent from the source sentence but should be present in the target sentence, back-translation can produce effective training data if it could naturally drop the information to be recovered in translating sentences in the target language into the source language. We evaluate our method on IWSLT2017 data sets (Cettolo et al., 2012), which are collections of subtitles of TED Talks, on two language pairs: English-Japanese (en-ja) and English-French (enfr) (§ 4). We exploit BookCorpus (Zhu et al., 2015), Europarl v7 (Koehn, 2005), and the record of the National Diet of Japan as monolingual corpora for back-translation (§ 5). Experimental results revealed that the data augmentation improved the translation in terms of BLEU score (Papineni et al., 2002) and the accuracy on specialized test sets for context-aware NMT. The contribution of this paper is as follows: 2 Context-aware NMT Models To incorporate contexts to translate sentences, recent studies on NMT have explored context-aware models which take sentences around the source sentence as auxiliary inputs. Typical contexts considered in those models are a few sentenc"
D19-6504,L16-1147,0,0.0599658,"Missing"
D19-6504,P16-1162,0,0.658364,"are translation; existing large-scale and high-quality parallel corpora are usually obtained by extracting reliable sentence alignments from translations by humans (Nakazawa et al., 2016; Pryzant et al., 2018). Considering that context-aware NMT models have larger input spaces than sentence-level models, they will demand larger training data to fully exert the models’ performance. In this study, we hypothesize that context-aware NMT models can benefit from an increase of the training data more than sentence-level models, and confirm this by performing data augmentation using back-translation (Sennrich et al., 2016b) (§ 6) for context-aware NMT models. We propose to assist the training of context-aware NMT models using pseudo parallel data which is automatically generated by back-translating a large monolingual data (§ 3, Figure 1). The back-translation model here is trained on an existing parallel corpus. Since context-aware models are designed to recover information that is absent from the source sentence but should be present in the target sentence, back-translation can produce effective training data if it could naturally drop the information to be recovered in translating sentences in the target la"
D19-6504,P18-1118,0,0.401726,"ntation for context-aware NMT models in terms of BLEU score and specialized test sets on ja→en1 and fr→en. 1 Backtranslate Train Original Parallel Corpus Context-aware Forward-translation Model ja→en Target-side Original Monolingual Corpus (en) Source-side Pseudo Monolingual Corpus (ja) Pseudo Parallel Corpus Figure 1: Overview of the data augmentation for context-aware NMT (Japanese to English in this case). language (§ 2). Typically, contexts considered by context-aware NMT are surrounding sentences in the same document (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Voita et al., 2019), which provide beneficial information in translating zero pronouns, anaphoric pronouns, lexically ambiguous words, and so on. Although the context-aware NMT models outperform the baseline sentence-level NMT models in terms of BLEU score and some specialized test sets (Bawden et al., 2018; Voita et al., 2019; M¨uller et al., 2018), the reported gains, especially in BLEU score, are often marginal. We can think of several reasons for this; 1) the ratio of sentences (or linguistic phenomena) that require contexts for translation is small in the evaluation datasets, 2) the cur"
D19-6504,W17-4811,0,0.420134,"ish-French datasets, and demonstrate the large impact of the data augmentation for context-aware NMT models in terms of BLEU score and specialized test sets on ja→en1 and fr→en. 1 Backtranslate Train Original Parallel Corpus Context-aware Forward-translation Model ja→en Target-side Original Monolingual Corpus (en) Source-side Pseudo Monolingual Corpus (ja) Pseudo Parallel Corpus Figure 1: Overview of the data augmentation for context-aware NMT (Japanese to English in this case). language (§ 2). Typically, contexts considered by context-aware NMT are surrounding sentences in the same document (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Voita et al., 2019), which provide beneficial information in translating zero pronouns, anaphoric pronouns, lexically ambiguous words, and so on. Although the context-aware NMT models outperform the baseline sentence-level NMT models in terms of BLEU score and some specialized test sets (Bawden et al., 2018; Voita et al., 2019; M¨uller et al., 2018), the reported gains, especially in BLEU score, are often marginal. We can think of several reasons for this; 1) the ratio of sentences (or linguistic phenomena) that require contex"
D19-6504,N19-1313,0,0.209114,"ered in context-aware NMT models do not include information required for translation, 4) the size of training data is not enough to effectively train context-aware NMT models. Although there are some studies that investigate the first to third Introduction Following the success of neural machine translation (NMT) models in sentence-level translation, context-aware NMT models have been studied to further boost the quality of translation (Jean et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Voita et al., 2019). These context-aware models take auxiliary inputs (contexts) to translate the source sentence which lacks information needed for translating into the target 1 http://www.tkl.iis.u-tokyo.ac.jp/ sugi/DiscoMT2019/ ˜ 35 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 35–44 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics proves context-aware translation through the existing en→fr (Bawden et al., 2018) and our specialized test set for ja→en translation. aspects (Bawden et al., 2018; Voita et al.,"
D19-6504,Q18-1029,0,0.190402,"sentence as auxiliary inputs. Typical contexts considered in those models are a few sentences that precede the source sentence. The context-aware NMT models are grouped into two types: single-encoder models that apply a sentence-level NMT model to the source sentence concatenated after their contexts (preceding sentence(s)) (Tiedemann and Scherrer, 2017; Bawden et al., 2018) and multi-encoder models that design an additional context encoder to process the contexts (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Tu et al., 2018; Maruf et al., 2019). In what follows, we briefly review these models. Single-encoder models take the preceding sentence(s) as the contexts in addition to the source sentence and concatenate them with a special symbol &lt;CONC> (Tiedemann and Scherrer, 2017). The concatenated sentences are then translated using an existing sentence-level NMT model. There are two subtypes of the single-encoder models that differ in handling contexts in the target language. The first model, which we refer to as 2-to-1, only considers contexts in the source language, and is trained on pairs of the source sentence w"
D19-6504,D18-1325,0,0.322555,"(narrow) contexts considered in context-aware NMT models do not include information required for translation, 4) the size of training data is not enough to effectively train context-aware NMT models. Although there are some studies that investigate the first to third Introduction Following the success of neural machine translation (NMT) models in sentence-level translation, context-aware NMT models have been studied to further boost the quality of translation (Jean et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Voita et al., 2019). These context-aware models take auxiliary inputs (contexts) to translate the source sentence which lacks information needed for translating into the target 1 http://www.tkl.iis.u-tokyo.ac.jp/ sugi/DiscoMT2019/ ˜ 35 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 35–44 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics proves context-aware translation through the existing en→fr (Bawden et al., 2018) and our specialized test set for ja→en translation. aspects (Bawden et al.,"
D19-6504,P19-1116,0,0.116258,"Missing"
D19-6504,W18-6307,0,0.0758798,"Missing"
D19-6504,P18-1117,0,0.423515,"ct of the data augmentation for context-aware NMT models in terms of BLEU score and specialized test sets on ja→en1 and fr→en. 1 Backtranslate Train Original Parallel Corpus Context-aware Forward-translation Model ja→en Target-side Original Monolingual Corpus (en) Source-side Pseudo Monolingual Corpus (ja) Pseudo Parallel Corpus Figure 1: Overview of the data augmentation for context-aware NMT (Japanese to English in this case). language (§ 2). Typically, contexts considered by context-aware NMT are surrounding sentences in the same document (Tiedemann and Scherrer, 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Voita et al., 2019), which provide beneficial information in translating zero pronouns, anaphoric pronouns, lexically ambiguous words, and so on. Although the context-aware NMT models outperform the baseline sentence-level NMT models in terms of BLEU score and some specialized test sets (Bawden et al., 2018; Voita et al., 2019; M¨uller et al., 2018), the reported gains, especially in BLEU score, are often marginal. We can think of several reasons for this; 1) the ratio of sentences (or linguistic phenomena) that require contexts for translation is small in the evalua"
D19-6504,D17-1301,0,0.265856,"sets, 2) the current context-aware models do not fully utilize the given contexts, 3) (narrow) contexts considered in context-aware NMT models do not include information required for translation, 4) the size of training data is not enough to effectively train context-aware NMT models. Although there are some studies that investigate the first to third Introduction Following the success of neural machine translation (NMT) models in sentence-level translation, context-aware NMT models have been studied to further boost the quality of translation (Jean et al., 2017; Tiedemann and Scherrer, 2017; Wang et al., 2017; Bawden et al., 2018; Voita et al., 2018; Maruf and Haffari, 2018; Miculicich et al., 2018; Maruf et al., 2019; Voita et al., 2019). These context-aware models take auxiliary inputs (contexts) to translate the source sentence which lacks information needed for translating into the target 1 http://www.tkl.iis.u-tokyo.ac.jp/ sugi/DiscoMT2019/ ˜ 35 Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 35–44 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics proves context-aware translation through the existing en→fr (Bawden"
D19-6504,P02-1040,0,0.104991,"t could naturally drop the information to be recovered in translating sentences in the target language into the source language. We evaluate our method on IWSLT2017 data sets (Cettolo et al., 2012), which are collections of subtitles of TED Talks, on two language pairs: English-Japanese (en-ja) and English-French (enfr) (§ 4). We exploit BookCorpus (Zhu et al., 2015), Europarl v7 (Koehn, 2005), and the record of the National Diet of Japan as monolingual corpora for back-translation (§ 5). Experimental results revealed that the data augmentation improved the translation in terms of BLEU score (Papineni et al., 2002) and the accuracy on specialized test sets for context-aware NMT. The contribution of this paper is as follows: 2 Context-aware NMT Models To incorporate contexts to translate sentences, recent studies on NMT have explored context-aware models which take sentences around the source sentence as auxiliary inputs. Typical contexts considered in those models are a few sentences that precede the source sentence. The context-aware NMT models are grouped into two types: single-encoder models that apply a sentence-level NMT model to the source sentence concatenated after their contexts (preceding sent"
D19-6504,L18-1182,0,0.0265949,"35–44 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics proves context-aware translation through the existing en→fr (Bawden et al., 2018) and our specialized test set for ja→en translation. aspects (Bawden et al., 2018; Voita et al., 2018; Imamura and Sumita, 2019), few studies have investigated the last possibility (§ 6), since there are few parallel corpora for context-aware translation; existing large-scale and high-quality parallel corpora are usually obtained by extracting reliable sentence alignments from translations by humans (Nakazawa et al., 2016; Pryzant et al., 2018). Considering that context-aware NMT models have larger input spaces than sentence-level models, they will demand larger training data to fully exert the models’ performance. In this study, we hypothesize that context-aware NMT models can benefit from an increase of the training data more than sentence-level models, and confirm this by performing data augmentation using back-translation (Sennrich et al., 2016b) (§ 6) for context-aware NMT models. We propose to assist the training of context-aware NMT models using pseudo parallel data which is automatically generated by back-translating a large"
I13-1156,P07-1056,0,0.836918,"he sentiment polarity. In this study, we build a model that automatically computes and uses user leniency and product popularity for sentiment classification. We represent these biases with two types of real-valued global features. Because these features and the labels of the test reviews mutually depend on each other, it is challenging to globally optimize a configuration of polarity labels for a given set of reviews. We here adopt a two-stage decoding strategy (Krishnan and Manning, 2006) for resolving the mutual dependencies in our model. We evaluated our method on two real-world datasets (Blitzer et al., 2007; Maas et al., 2011). Experimental results demonstrated that the proposed method significantly improved the classification accuracy against the state-of-the-art methods (Dredze et al., 2008; Seroussi et al., 2010). The remainder of this paper is organized as follows. We first discuss some related work in Section 2. We describe our method in Section 3. We then report experimental results in Section 4. Finally, we conclude our study in Section 5. 1107 International Joint Conference on Natural Language Processing, pages 1107–1111, Nagoya, Japan, 14-18 October 2013. 2 Related Work Recently, social"
I13-1156,Y13-1036,1,0.753512,"Missing"
I13-1156,P11-1016,0,0.0177874,"ws. We first discuss some related work in Section 2. We describe our method in Section 3. We then report experimental results in Section 4. Finally, we conclude our study in Section 5. 1107 International Joint Conference on Natural Language Processing, pages 1107–1111, Nagoya, Japan, 14-18 October 2013. 2 Related Work Recently, social media such as Twitter has attracted much attention from researchers because it is now apparently the major source of subjective text on the Web. The traditional text-based methods, such as Pang et al. (2002), could not easily handle such short and informal text (Jiang et al., 2011). Tan et al. (2010) and Speriosu et al. (2011) exploited the user network behind a social media website (Twitter in their case) and assumed that friends give similar ratings towards similar products. Seroussi et al. (2010) proposed a framework that computes users’ similarity on the basis of their usage of text and their rating histories. They then classify a given review by referring to ratings given for the same product by other users who are similar to the user in question. However, such user networks are not always available in the real world. Li et al. (2011) incorporate user- or productde"
I13-1156,P06-1141,0,0.0211755,"Missing"
I13-1156,P11-1015,0,0.168158,"In this study, we build a model that automatically computes and uses user leniency and product popularity for sentiment classification. We represent these biases with two types of real-valued global features. Because these features and the labels of the test reviews mutually depend on each other, it is challenging to globally optimize a configuration of polarity labels for a given set of reviews. We here adopt a two-stage decoding strategy (Krishnan and Manning, 2006) for resolving the mutual dependencies in our model. We evaluated our method on two real-world datasets (Blitzer et al., 2007; Maas et al., 2011). Experimental results demonstrated that the proposed method significantly improved the classification accuracy against the state-of-the-art methods (Dredze et al., 2008; Seroussi et al., 2010). The remainder of this paper is organized as follows. We first discuss some related work in Section 2. We describe our method in Section 3. We then report experimental results in Section 4. Finally, we conclude our study in Section 5. 1107 International Joint Conference on Natural Language Processing, pages 1107–1111, Nagoya, Japan, 14-18 October 2013. 2 Related Work Recently, social media such as Twitt"
I13-1156,P04-1035,0,0.0842901,"ity by referring to the user leniency and product popularity computed during testing. For decoding with this model, we adopt an approximate strategy called “two-stage decoding.” Preliminary experimental results on two realworld datasets show that our method significantly improves classification accuracy over existing state-of-the-art methods. 1 Introduction Document-level sentiment classification estimates the sentiment polarity for a given subjective text (hereafter, review). Traditionally, researchers have tried to estimate the sentiment polarity from only the textual content of the review (Pang and Lee, 2004; Li et al., 2011). However, since reviews are written by a user to express his/her emotion toward a particular product, taking the users and products into consideration would play an important role in solving this task. Recently, the increase of opinionated text within social media, e.g., Twitter, has motivated researchers to exploit the user or product information in the sentiment classification task. Some researchers take advantages of the friend relation in a social network because friends are likely to hold common tastes (Tan et al., 2011; Seroussi et al., 2010; Speriosu et al., 2011). Ot"
I13-1156,J11-1002,0,0.0787851,"he sentiment toward a product is described by product-specific language. This approach, however, requires the training data to contain reviews written by test users and written for test products. This is infeasible since labeling reviews requires too much manual work. Note that we assume to know which reviews are written by the same user and which are written on the same product. This assumption is realistic nowadays since user information is available in many real-world datasets (Blitzer et al., 2007; Pang and Lee, 2004), while product information can be extracted from text if not available (Qiu et al., 2011). We should emphasize here that our method does not require user profiles, product descriptions, or any sort of extrinsic knowledge on the users and products. 3.2 Features The review r’s feature vector, xr , is composed of local features (xlr ) and global features (xgr ), such that xr = (xlr , xgr ). In this study, we use word n-grams (n = 1, 2) in the textual content of the review as local features, while we encode the user leniency and product popularity into global features. We introduce four global features to capture the user leniency and product polarity: xgr = {f u+ , f u− , f p+ , f p−"
I13-1156,W11-2207,0,0.0955049,"review (Pang and Lee, 2004; Li et al., 2011). However, since reviews are written by a user to express his/her emotion toward a particular product, taking the users and products into consideration would play an important role in solving this task. Recently, the increase of opinionated text within social media, e.g., Twitter, has motivated researchers to exploit the user or product information in the sentiment classification task. Some researchers take advantages of the friend relation in a social network because friends are likely to hold common tastes (Tan et al., 2011; Seroussi et al., 2010; Speriosu et al., 2011). Others incorporate user- or product-specific n-gram features (Li et al., 2011; Seroussi et al., 2010). Although these studies have showed that user or product information is useful for sentiment classification, they implicitly assume that the same users or products appear in both training and testing data. Thus, to train such a model, a large amount of the reviews should be labeled for each user and each product. In a realworld scenario, however, this is unrealistic since new users and products are ceaselessly emerging and labeling reviews written by such users (or on such products) is impra"
K15-1030,P14-1006,0,0.0205252,"uding neural networks. This suggests that count-based word vectors have a great advantage when learning a crosslingual projection. As a future work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; Mikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 “small” and “importance,” and so on. Since, for example, “friend” is a English translation of “amigo,” the Spanish dimension associated with “amigo” is likely to be mapped to the English dimension associated with “friend.” Such knowledge about the cross-lingual correspondence between dimensions is cons"
K15-1030,W13-3214,0,0.0157002,"n of word representations from one language into another. Our method utilizes translatable context pairs as bonus terms of the objective function. In the experiments, our method outperformed existing methods in three language pairs, (English, Spanish), (Japanese, Chinese) and (English, Japanese), without using any additional supervisions. 1 Introduction Vector-based representations of word meanings, hereafter word vectors, have been widely used in a variety of NLP applications including synonym detection (Baroni et al., 2014), paraphrase detection (Erk and Pad´o, 2008), and dialogue analysis (Kalchbrenner and Blunsom, 2013). The basic idea behind those representation methods is the distributional hypothesis (Harris, 1954; Firth, 1957) that similar words are likely to co-occur with similar context words. A problem with the word vectors is that they are not meant for capturing the similarity between words in different languages, i.e., translation pairs such as “gato” and “cat.” The meaning representations of such word pairs are usually dissimilar, because the vast majority of the context words are from the same language as the target words (e.g., Spanish for “gato” and English for “cat”). This prevents using word"
K15-1030,C12-1089,0,0.0376743,"d method performs significantly better than strong baselines including neural networks. This suggests that count-based word vectors have a great advantage when learning a crosslingual projection. As a future work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; Mikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 “small” and “importance,” and so on. Since, for example, “friend” is a English translation of “amigo,” the Spanish dimension associated with “amigo” is likely to be mapped to the English dimension associated with “friend.” Such knowledg"
K15-1030,P14-1023,0,0.516524,"asaru Kitsuregawa♦♠ ♦ Institute of Industrial Science, the University of Tokyo ♠ National Institute of Informatics, Japan {toyoda, kitsure}@tkl.iis.u-tokyo.ac.jp Abstract al., 2012; Mikolov et al., 2013b). In particular, Mikolov et al. (2013b) recently explored learning a linear transformation between word vectors of different languages from a small amount of training data, i.e., a set of bilingual word pairs. This study explores incorporating prior knowledge about the correspondence between dimensions of word vectors to learn more accurate transformation, when using count-based word vectors (Baroni et al., 2014). Since the dimensions of count-based word vectors are explicitly associated with context words, we can partially be aware of the cross-lingual correspondence between the dimensions of word vectors by diverting the training data. Also, word surface forms present noisy yet useful clues on the correspondence when targeting the language pairs that have exchanged their vocabulary (e.g., “cocktail” in English and “c´octel” in Spanish). Although apparently useful, how to exploit such knowledge within the learning framework has not been addressed so far. We evaluated the proposed method in three lang"
K15-1030,2009.mtsummit-posters.14,0,0.0250058,"(m = 100, 200, 300) vectors for the target language and n-dimensional (n = 2m, 3m, 4m) vectors for the source language, and optimized their combinations on the development data. Direct Mapping exploits the training data to map each dimension in a word vector in the source language to the corresponding dimension in a word vector in the target language, referring to the bilingual pairs in the training data (Fung, 1998). To deal with words that have more than one translation, we weighted each translation by a reciprocal rank of its frequency among the translations in the target language, as in (Prochasson et al., 2009). Figure 1: The impact of the size of training data (Es → En). gain is smaller than that obtained by our new objective. Proposedw/o surface uses only the training data to find translatable context pairs by setting βsim = 0. Thus, its advantage over Direct Mapping confirms the importance of learning a translation matrix. In addition, the greater advantage of Proposed over Proposedw/o surface in the translation between (En, Es) or (Jp, Cn) conforms to our expectation that surface-level similarity is more useful for translation between the language pairs which have often exchanged their vocabular"
K15-1030,J90-1003,0,0.344294,"two methods. βtrain and βsim are parameters representing the strength of the new terms, and are tuned on held-out development data. 3.4 Next, we induced count-based word vectors from the obtained text. We considered context windows of five words to both sides of the target word. The function words are then excluded from the extracted context words. Since the count vectors are very high-dimensional and sparse, we selected top-10k frequent words as contexts words (in other words, the number of dimensions of the word vectors). We converted the counts into positive point-wise mutual information (Church and Hanks, 1990) and normalized the resulting vectors to remove the bias that is introduced by the difference of the word frequency. Then, we compiled a seed bilingual dictionary (a set of bilingual word pairs) for each language pair that is used to learn and evaluate the translation matrix. We utilized cross-lingual synsets in the Open Multilingual Wordnet8 to obtain bilingual pairs. Since our method aims to be used in expanding bilingual dictionaries, we designed datasets assuming such a situation. Considering that more frequent words are likely to be registered in a dictionary, we sorted words in the sourc"
K15-1030,P14-1059,0,0.0182242,"of the 19th Conference on Computational Language Learning, pages 300–304, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; Mikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 “small” and “importance,” and so on. Since, for example, “friend” is a English translation of “amigo,” the Spanish dimension associated with “amigo” is likely to be mapped to the English dimension associated with “friend.” Such knowledge about the cross-lingual correspondence between dimensions is considered beneficial for learning accurate translation matrix. We take two approaches to obtaining such correspondence. Firstly, since we have already assumed that a small amoun"
K15-1030,W14-1613,0,0.0231896,"trong baselines including neural networks. This suggests that count-based word vectors have a great advantage when learning a crosslingual projection. As a future work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; Mikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 “small” and “importance,” and so on. Since, for example, “friend” is a English translation of “amigo,” the Spanish dimension associated with “amigo” is likely to be mapped to the English dimension associated with “friend.” Such knowledge about the cross-lingual correspondence"
K15-1030,D08-1094,0,0.106227,"Missing"
K15-1030,E14-1049,0,0.026144,"suggests that count-based word vectors have a great advantage when learning a crosslingual projection. As a future work, we are also 300 Proceedings of the 19th Conference on Computational Language Learning, pages 300–304, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics interested in extending the method presented here to apply word vectors learned by neural networks. There are also methods that directly inducing meaning representations shared by different languages (Klementiev et al., 2012; Lauly et al., 2014; Xiao and Guo, 2014; Hermann and Blunsom, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015), rather than learning transformation between different languages (Fung, 1998; Mikolov et al., 2013b; Dinu and Baroni, 2014). However, the former approach is unable to handle words not appearing in the training data, unlike the latter approach. 3 “small” and “importance,” and so on. Since, for example, “friend” is a English translation of “amigo,” the Spanish dimension associated with “amigo” is likely to be mapped to the English dimension associated with “friend.” Such knowledge about the cross-lingual correspondence between dimensions is considered beneficial for le"
K15-1030,N15-1157,0,\N,Missing
K19-1003,P18-1073,0,0.304972,"s, as we confirm in § 5. • We established a method of obtaining fully task-specific multilingual models by learning a cross-task embedding projection (§ 3). Multilingual models with character embeddings Several studies utilize character level embeddings shared across languages to obtain multilingual models (Kim et al., 2017; Yang et al., 2017). An obvious weak point of these methods is that they do not apply to distant language pairs with different alphabets. In contrast, our method only relies on cross-lingual word embeddings which are obtainable regardless of the alphabets of the languages (Artetxe et al., 2018). • Our cross-task projection is simple and has an analytical solution with one hyperparameter; the solution is a global optima (§ 3.2). • We confirmed the limitation of the traditional multilingual model with embedding layers fixed to pre-trained cross-lingual word embeddings (§ 5.1). • We showed the effectiveness of our method over the existing models (§ 5.2). 2 Task-specific word embeddings Few efforts have been previously made to obtain cross-lingual task-specific word embeddings. Gouws and Søgaard (2015) obtain task-specific cross-lingual word embeddings by constructing a task-specific bi"
K19-1003,Q17-1010,0,0.0430381,"00 10,000 10,000 10,000 100,000 10,000 10,000 10,000 Table 2: Number of examples for sentiment analysis. General cross-lingual word embeddings were obtained using a state-of-the-art unsupervised method with self-learning framework (Artetxe et al., 2018).5 This method takes monolingual word embeddings of two languages and learns a mapping between them to obtain cross-lingual word embeddings. For monolingual word embeddings, we used pre-trained word embeddings available online (Grave et al., 2018).6 They are word embeddings with 300 dimensions obtained by applying subword-information skip-gram (Bojanowski et al., 2017) to the Wikipedia corpus. Preprocessing We use the tokenizer of Europarl tools7 to tokenize all datasets except for Japanese. For Japanese, we use MeCab v0.9968 with IPA dictionary v2.7.0. After tokenization, the tokens are lowercased to match vocabularies of the pretrained word embeddings. Models To evaluate the impact of our taskspecific word embeddings on multilingual models and to compare the two methods for the cross-task embeddings projections we proposed in § 3, we compare the following five models. CLWE fixed trains a bag-of-embeddings model in the target language with its embedding la"
K19-1003,P12-1060,0,0.0259533,"dings for sentiment analysis task. This method is tailored for the sentiment analysis task and thus, not applicable to other tasks. Related work Lack of resources in resource-poor languages has been a deeply rooted problem in NLP, and there have been many pieces of researches contributed to mitigating this problem by transferring models across languages. Multilingual models using parallel corpora An intuitive approach to realize the cross-lingual transfer of a model is to utilize machine translation by either translating the training set or the model input (Wan, 2009). Instead of translating, Meng et al. (2012) leverage a parallel corpus of the source and target languages to obtain cross-lingual mixture model to bridge the language gap. Xu and Wan (2017) also utilize parallel corpus with word alignment to train a multilingual model for sen23 3 Fully task-specific multilingual model 3.2 Here, we explain the detailed construction of our cross-task projection φ for cross-lingual word embeddings used in Step 3 in § 3.1. Given general cross-lingual word embeddings, X gen and Y gen , of the source and target languages and task-specific word embeddings X spec of the source language, we compute task-specifi"
K19-1003,Q18-1039,0,0.0226125,"ross-lingual word embeddings Another method to obtain multilingual models is to fix the embedding layer of a neural network to pre-trained cross-lingual word embeddings. Many existing pieces of researches implemented this for various tasks in unsupervised senario (Duong et al., 2017; Can et al., 2018) where no annotated corpus is available in the target language as ours and supervised scenario (Pappas and Popescu-Belis, 2017; Upadhyay et al., 2018) where a small annotated corpus is available in the target language. Another study enhanced this method by employing language-adversarial networks (Chen et al., 2018). These methods do not induce task-specific word embeddings, thereby failing to exert true potential of neural networks, as we confirm in § 5. • We established a method of obtaining fully task-specific multilingual models by learning a cross-task embedding projection (§ 3). Multilingual models with character embeddings Several studies utilize character level embeddings shared across languages to obtain multilingual models (Kim et al., 2017; Yang et al., 2017). An obvious weak point of these methods is that they do not apply to distant language pairs with different alphabets. In contrast, our m"
K19-1003,I17-1102,0,0.0317878,"Missing"
K19-1003,E17-1084,0,0.28939,"ecific word embedding of the target word as a linear combination of task-specific word embeddings of the k neighboring source words (§ 3.2). We evaluate our method on topic classification and sentiment analysis tasks (§ 4). We first obtain a task-specific neural network using annotated corpora in the source language (English) and then induce task-specific cross-lingual word embeddings for the target languages to apply the accurate taskspecific neural network to those languages. Experimental results demonstrate that our method has improved the classification accuracy of the multilingual model (Duong et al., 2017) in most of the task-language pairs (§ 5). Our contributions are as follows: Multilingual models with cross-lingual word embeddings Another method to obtain multilingual models is to fix the embedding layer of a neural network to pre-trained cross-lingual word embeddings. Many existing pieces of researches implemented this for various tasks in unsupervised senario (Duong et al., 2017; Can et al., 2018) where no annotated corpus is available in the target language as ours and supervised scenario (Pappas and Popescu-Belis, 2017; Upadhyay et al., 2018) where a small annotated corpus is available"
K19-1003,N19-1040,0,0.0135185,"ic word embeddings Few efforts have been previously made to obtain cross-lingual task-specific word embeddings. Gouws and Søgaard (2015) obtain task-specific cross-lingual word embeddings by constructing a task-specific bilingual dictionary, which defines “equivalent classes” designed for the given task instead of equivalent semantics. Although they successfully obtained task-specific cross-lingual word embeddings for POS tagging and supersense tagging tasks, the open problems are how to define a taskspecific bilingual dictionary for many of other tasks, and cost of developing such resources. Feng and Wan (2019) exploit multi-task learning to induce cross-lingual task-specific word embeddings for sentiment analysis task. This method is tailored for the sentiment analysis task and thus, not applicable to other tasks. Related work Lack of resources in resource-poor languages has been a deeply rooted problem in NLP, and there have been many pieces of researches contributed to mitigating this problem by transferring models across languages. Multilingual models using parallel corpora An intuitive approach to realize the cross-lingual transfer of a model is to utilize machine translation by either translat"
K19-1003,N15-1157,0,0.0193521,"ngual word embeddings which are obtainable regardless of the alphabets of the languages (Artetxe et al., 2018). • Our cross-task projection is simple and has an analytical solution with one hyperparameter; the solution is a global optima (§ 3.2). • We confirmed the limitation of the traditional multilingual model with embedding layers fixed to pre-trained cross-lingual word embeddings (§ 5.1). • We showed the effectiveness of our method over the existing models (§ 5.2). 2 Task-specific word embeddings Few efforts have been previously made to obtain cross-lingual task-specific word embeddings. Gouws and Søgaard (2015) obtain task-specific cross-lingual word embeddings by constructing a task-specific bilingual dictionary, which defines “equivalent classes” designed for the given task instead of equivalent semantics. Although they successfully obtained task-specific cross-lingual word embeddings for POS tagging and supersense tagging tasks, the open problems are how to define a taskspecific bilingual dictionary for many of other tasks, and cost of developing such resources. Feng and Wan (2019) exploit multi-task learning to induce cross-lingual task-specific word embeddings for sentiment analysis task. This"
K19-1003,L18-1550,0,0.0257039,"00 1462 1237 1125 855 English (en) German (de) Amazon French (fr) Japanese (ja) 6,731,166 659,121 234,080 242,431 100,000 10,000 10,000 10,000 100,000 10,000 10,000 10,000 Table 2: Number of examples for sentiment analysis. General cross-lingual word embeddings were obtained using a state-of-the-art unsupervised method with self-learning framework (Artetxe et al., 2018).5 This method takes monolingual word embeddings of two languages and learns a mapping between them to obtain cross-lingual word embeddings. For monolingual word embeddings, we used pre-trained word embeddings available online (Grave et al., 2018).6 They are word embeddings with 300 dimensions obtained by applying subword-information skip-gram (Bojanowski et al., 2017) to the Wikipedia corpus. Preprocessing We use the tokenizer of Europarl tools7 to tokenize all datasets except for Japanese. For Japanese, we use MeCab v0.9968 with IPA dictionary v2.7.0. After tokenization, the tokens are lowercased to match vocabularies of the pretrained word embeddings. Models To evaluate the impact of our taskspecific word embeddings on multilingual models and to compare the two methods for the cross-task embeddings projections we proposed in § 3, we"
K19-1003,D17-1302,0,0.020715,", 2018) where a small annotated corpus is available in the target language. Another study enhanced this method by employing language-adversarial networks (Chen et al., 2018). These methods do not induce task-specific word embeddings, thereby failing to exert true potential of neural networks, as we confirm in § 5. • We established a method of obtaining fully task-specific multilingual models by learning a cross-task embedding projection (§ 3). Multilingual models with character embeddings Several studies utilize character level embeddings shared across languages to obtain multilingual models (Kim et al., 2017; Yang et al., 2017). An obvious weak point of these methods is that they do not apply to distant language pairs with different alphabets. In contrast, our method only relies on cross-lingual word embeddings which are obtainable regardless of the alphabets of the languages (Artetxe et al., 2018). • Our cross-task projection is simple and has an analytical solution with one hyperparameter; the solution is a global optima (§ 3.2). • We confirmed the limitation of the traditional multilingual model with embedding layers fixed to pre-trained cross-lingual word embeddings (§ 5.1). • We showed the e"
K19-1003,P09-1027,0,0.0586039,"oss-lingual task-specific word embeddings for sentiment analysis task. This method is tailored for the sentiment analysis task and thus, not applicable to other tasks. Related work Lack of resources in resource-poor languages has been a deeply rooted problem in NLP, and there have been many pieces of researches contributed to mitigating this problem by transferring models across languages. Multilingual models using parallel corpora An intuitive approach to realize the cross-lingual transfer of a model is to utilize machine translation by either translating the training set or the model input (Wan, 2009). Instead of translating, Meng et al. (2012) leverage a parallel corpus of the source and target languages to obtain cross-lingual mixture model to bridge the language gap. Xu and Wan (2017) also utilize parallel corpus with word alignment to train a multilingual model for sen23 3 Fully task-specific multilingual model 3.2 Here, we explain the detailed construction of our cross-task projection φ for cross-lingual word embeddings used in Step 3 in § 3.1. Given general cross-lingual word embeddings, X gen and Y gen , of the source and target languages and task-specific word embeddings X spec of"
K19-1003,D14-1181,0,0.00427068,"del) Figure 1: Locally linear mapping for sentiment analysis task. The relationship between “merveilleux (wonderful)” and its neighboring English words, “wonderful” and “good,” are preserved after projection. absorb the differences among languages in the vocabularies of neural network models; specifically, these multilingual models are trained with embedding layers fixed to pre-trained cross-lingual word embeddings. However, because those embedding layers are not optimized for the target task, the resulting model cannot exploit the true potential of representation learning, as demonstrated by Kim (2014) and our experimental results (§ 5.1). We propose methods of projecting pre-trained cross-lingual word embeddings to word embeddings of a fully task-specific neural network all of whose parameters are optimized to the training data in a source language, to realize fully task-specific multilingual model (§ 3). In addition to naive linear projection, we present an element-wise projection method inspired by locally linear embeddings used for dimension reduction (Roweis and Saul, 2000). This method is built on the assumption that local topology is preserved between the semantic spaces of word embe"
K19-1003,N15-1104,0,0.193585,"Missing"
K19-1031,W17-3204,0,0.0276157,"t to the existing model by Shaw et al. (2018) which also incorporates relative position. • We revealed the overfitting property of Transformer to both short and long sentences. 2 Related Work 3 Early studies on NMT, at that time RNN-based model, analyze the translation quality in terms of sentence length (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and a few studies shed light on the details. Shi et al. (2016) examine why RNN-based model generates translations of the right length without special mechanism for the length, and report how LSTM regulates the output length. Koehn and Knowles (2017) reveal that 3.1 Preliminaries Transformer Transformer (Vaswani et al., 2017) is a sequence to sequence model that has an encoder to process and represent input sequence and a decoder to generate output sequence from the encoder outputs. Both the encoder and decoder have a word embedding layer, a positional encoding layer, and 329 Outputs Outputs Outputs Outputs Feed-forward Feed-forward Feed-forward Feed-forward Self-attention Self-attention w/ relative position Self-attention Self-attention w/ relative position xN xN xN Positional encoding xN RNN RNN Word embedding Word embedding Word embedd"
K19-1031,P18-1008,0,0.0326028,"t al. (2017) adopt them in their model, Transformer, which has neither RNN nor CNN. Recently, Shaw et al. (2018) propose to incorporate relative position into Transformer by modifying the self-attention layer while removing positional encodings. Lei et al. (2018) propose a fast RNN named Simple Recurrent Units (SRU) and replace the feed-forward layers of Transformer by SRU considering that recurrent process would better capture sequential information. Although both approaches succeeded in improving BLEU score, the researchers did not report in what respect the models improved the translation. Chen et al. (2018) propose a RNN-based model, RNMT+, which is based on stacked LSTMs and incorporates some components from Transformer such as layer normalization and multi-head attention. On the other hand, our model is based on Transformer and incorporates RNN into Transformer. ploys positional encodings, which give fixed vectors to positions using sine and cosine functions. In this study, we suspect that these differences in position information types of the models have an impact on the accuracy of translating long sentences, and investigate the impact of position information on translating long sentences to"
K19-1031,D18-2012,0,0.0185763,"translation tasks. For Englishto-Japanese translation task, we exploit ASPEC (Nakazawa et al., 2016), a parallel corpus compiled from abstract sections of scientific papers. For English-to-German translation task, we exploit a dataset in WMT2014, which is one of the most common dataset for translation task. For ASPEC English-to-Japanese data, we used scripts of Moses toolkit2 (ver. 2.2.1) (Koehn et al., 2007) for English tokenization and truecasing, and KyTea3 (ver. 0.4.2) (Neubig et al., 2011) for Japanese segmentations. Following those wordlevel preprocess, we further applied SentencePiece (Kudo and Richardson, 2018) to segment texts down to subword level with shared vocabulary size of 16,000. Finally we selected the first 1,500,000 sentence pairs for the poor quality of the latter part, and filtered out sentence pairs with more than 49 subwords in either of the languages. For WMT2014 English-to-German translation task, we used preprocessed data provided from the Stanford NLP Group,4 and used newstest2013 and newstest2014 as development and test data, respectively. We also applied SentencePiece to this data to segment into subwords with shared vocabulary size of 40,000. We filtered out the sentence pairs"
K19-1031,D18-1477,0,0.13374,"length. Relative information has been implicitly used in the models using RNN or CNN. Gehring et al. (2017) introduce position embeddings which represent absolute position information to their CNN-based model. Sukhbaatar et al. (2015) introduce another absolute position information, positional encodings, which need no parameter training, and Vaswani et al. (2017) adopt them in their model, Transformer, which has neither RNN nor CNN. Recently, Shaw et al. (2018) propose to incorporate relative position into Transformer by modifying the self-attention layer while removing positional encodings. Lei et al. (2018) propose a fast RNN named Simple Recurrent Units (SRU) and replace the feed-forward layers of Transformer by SRU considering that recurrent process would better capture sequential information. Although both approaches succeeded in improving BLEU score, the researchers did not report in what respect the models improved the translation. Chen et al. (2018) propose a RNN-based model, RNMT+, which is based on stacked LSTMs and incorporates some components from Transformer such as layer normalization and multi-head attention. On the other hand, our model is based on Transformer and incorporates RNN"
K19-1031,D15-1166,0,0.655552,"15; Luong et al., 2015). However, Koehn and Knowles (2017) report that even RNN-based model with the attention mechanism performs worse than phrase-based statistical machine translation (Koehn et al., 2007) in translating very long sentences, which challenges us to develop an NMT model that is robust to long sentences or more generally, variations in input length. Have the recent advances in NMT achieved the robustness to the variations in input length? NMT has been advancing by upgrading the model architecture: RNN-based model (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) followed by convolutional neural network (CNN)-based model (Kalchbrenner et al., 2016; Gehring et al., 2017) and attention-based model (Vaswani et al., 2017) called Transformer (§ 2). Transformer is the de facto standard NMT model today for its better performance compared to the former standard RNN-based model. We thus came up with a question whether Transformer have acquired the robustness to the variations in input length. On the length of input sentence(s), the key difference between existing NMT models is how they incorporate information on word positions in the input. RNN or CNN-based NM"
K19-1031,N16-1012,0,0.0244614,"sformer. Experiments on ASPEC English-to-Japanese and WMT2014 Englishto-German translation tasks demonstrate that relative position helps translating sentences longer than those in the training data. Further experiments on length-controlled training data reveal that absolute position actually causes overfitting to the sentence length. 1 Naoki Yoshinaga Institute of Industrial Science, the University of Tokyo ynaga@iis.u-tokyo.ac.jp Introduction Sequence to sequence models for neural machine translation (NMT) are now utilized for various text generation tasks including automatic summarization (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015) and dialogue systems (Vinyals and Le, 2015; Shang et al., 2015); the models are required to take inputs of various length. Early studies on recurrent neural network (RNN)-based model analyze the translation quality with respect to the sentence length, and show that their models improve translations for long sentences, using the long short-term memory (LSTM) (Sutskever 328 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 328–338 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics"
K19-1031,K18-1010,0,0.014942,"the 23rd Conference on Computational Natural Language Learning, pages 328–338 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics RNN-based model has lower translation quality on very long sentences. Although researchers have proposed various new NMT architecture, they usually evaluate their models only in terms of the overall translation quality and rarely mention how the translation has changed (Gehring et al., 2017; Kalchbrenner et al., 2016; Vaswani et al., 2017). Only a few studies do the analysis on the translation quality in terms of sentence length (Elbayad et al., 2018; Zhang et al., 2019). The robustness of the recent NMT models on very long sentences remains to be assessed. What we focus on in this study is the word position information which will closely relate to the decodable sentence length. Relative information has been implicitly used in the models using RNN or CNN. Gehring et al. (2017) introduce position embeddings which represent absolute position information to their CNN-based model. Sukhbaatar et al. (2015) introduce another absolute position information, positional encodings, which need no parameter training, and Vaswani et al. (2017) adopt th"
K19-1031,K16-1028,0,0.057629,"Missing"
K19-1031,P11-2093,0,0.013714,"30 20 0-9 -19 0 Dataset and Preprocess: We perform a series of experiments on English-to-Japanese and English-to-German translation tasks. For Englishto-Japanese translation task, we exploit ASPEC (Nakazawa et al., 2016), a parallel corpus compiled from abstract sections of scientific papers. For English-to-German translation task, we exploit a dataset in WMT2014, which is one of the most common dataset for translation task. For ASPEC English-to-Japanese data, we used scripts of Moses toolkit2 (ver. 2.2.1) (Koehn et al., 2007) for English tokenization and truecasing, and KyTea3 (ver. 0.4.2) (Neubig et al., 2011) for Japanese segmentations. Following those wordlevel preprocess, we further applied SentencePiece (Kudo and Richardson, 2018) to segment texts down to subword level with shared vocabulary size of 16,000. Finally we selected the first 1,500,000 sentence pairs for the poor quality of the latter part, and filtered out sentence pairs with more than 49 subwords in either of the languages. For WMT2014 English-to-German translation task, we used preprocessed data provided from the Stanford NLP Group,4 and used newstest2013 and newstest2014 as development and test data, respectively. We also applied"
K19-1031,P02-1040,0,0.103654,", the only model that uses absolute position, more sharply drops than the BLEU scores of the other models at the input length of 50-59, which is outside of the length range of the training data. As for the input length of 60-, Transformer performs the worst among all the models. These results indicate that relative position works better than absolute position in translating sentences longer than those of the training data. Meanwhile, for the lengths with enough Evaluation: We performed greedy search for translation with the models, and evaluated the translation quality in terms of BLEU score (Papineni et al., 2002) using multi-bleu.perl in the Moses toolkit. We checked model’s BLEU score on the development data at every 10k steps during the training, and took the best performing model for evaluation on the test data. Long Sentence Translation Table 3 shows the BLEU scores of the NMT models on the test data of ASPEC English-to-Japanese and WMT2014 English-to-German when using all the preprocessed training data for training. Table 4 lists the results of statistical significance 6 Rel Table 4: Results of statistical significance test on ASPEC English-to-Japanese (lower-left) and WMT2014 English-to-German ("
K19-1031,D15-1044,0,0.0476033,"nese and WMT2014 Englishto-German translation tasks demonstrate that relative position helps translating sentences longer than those in the training data. Further experiments on length-controlled training data reveal that absolute position actually causes overfitting to the sentence length. 1 Naoki Yoshinaga Institute of Industrial Science, the University of Tokyo ynaga@iis.u-tokyo.ac.jp Introduction Sequence to sequence models for neural machine translation (NMT) are now utilized for various text generation tasks including automatic summarization (Chopra et al., 2016; Nallapati et al., 2016; Rush et al., 2015) and dialogue systems (Vinyals and Le, 2015; Shang et al., 2015); the models are required to take inputs of various length. Early studies on recurrent neural network (RNN)-based model analyze the translation quality with respect to the sentence length, and show that their models improve translations for long sentences, using the long short-term memory (LSTM) (Sutskever 328 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 328–338 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics RNN-based model has lower translation qualit"
K19-1031,P15-1152,0,0.0819546,"Missing"
K19-1031,N18-2074,0,0.19273,"ng sentences remains to be assessed. What we focus on in this study is the word position information which will closely relate to the decodable sentence length. Relative information has been implicitly used in the models using RNN or CNN. Gehring et al. (2017) introduce position embeddings which represent absolute position information to their CNN-based model. Sukhbaatar et al. (2015) introduce another absolute position information, positional encodings, which need no parameter training, and Vaswani et al. (2017) adopt them in their model, Transformer, which has neither RNN nor CNN. Recently, Shaw et al. (2018) propose to incorporate relative position into Transformer by modifying the self-attention layer while removing positional encodings. Lei et al. (2018) propose a fast RNN named Simple Recurrent Units (SRU) and replace the feed-forward layers of Transformer by SRU considering that recurrent process would better capture sequential information. Although both approaches succeeded in improving BLEU score, the researchers did not report in what respect the models improved the translation. Chen et al. (2018) propose a RNN-based model, RNMT+, which is based on stacked LSTMs and incorporates some compo"
K19-1031,D16-1248,0,0.0189824,"olute position makes it difficult to translate very long sentences. • We proposed a simple method to incorporate relative position into Transformer; it gives an additive improvement to the existing model by Shaw et al. (2018) which also incorporates relative position. • We revealed the overfitting property of Transformer to both short and long sentences. 2 Related Work 3 Early studies on NMT, at that time RNN-based model, analyze the translation quality in terms of sentence length (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and a few studies shed light on the details. Shi et al. (2016) examine why RNN-based model generates translations of the right length without special mechanism for the length, and report how LSTM regulates the output length. Koehn and Knowles (2017) reveal that 3.1 Preliminaries Transformer Transformer (Vaswani et al., 2017) is a sequence to sequence model that has an encoder to process and represent input sequence and a decoder to generate output sequence from the encoder outputs. Both the encoder and decoder have a word embedding layer, a positional encoding layer, and 329 Outputs Outputs Outputs Outputs Feed-forward Feed-forward Feed-forward Feed-forw"
K19-1031,P19-1426,0,0.0468862,"Missing"
N19-1215,P14-2134,0,0.0311652,"and dialogue systems (Li et al., 2016; Zhang et al., 2018). However, it is difficult to untangle the different facets of personal biases, there is no study aiming to analyze solely personal semantic variations. Meanwhile, word embeddings induced for a simple NLP task such as sentiment classification conveys less information, which are not suitable for analyzing semantic variations. Computational linguists have utilized word embeddings to capture semantic variations of words caused by diachronic (Hamilton et al., 2016; Szymanski, 2017; Rosenfeld and Erk, 2018; Jaidka et al., 2018), geographic (Bamman et al., 2014; Garimella et al., 2016) or domain (Tredici and Fern´andez, 2017) differences. In these studies, they have mainly discussed relationships between semantic variations of words and their frequency, dissemination (the number of users), or polysemy of the words. Hamilton et al. (2016) report that the meanings of more frequent words are more stable over time, and the meanings of polysemous words are likely to change over time since polysemous words appear in diverse contexts (Winter et al., 2014; Br´eal, 1897). Tredici and Fern´andez (2017) report that the meanings of words used by more people are"
N19-1215,D16-1171,0,0.124046,"LSTM-encoder Related Work Bi-LSTM As discussed in § 1, biases considered by personalization in NLP tasks have three facets: (1) semantic variation in task inputs (biases in how people use words; our target) (2) annotation bias of output labels (biases in how annotators label) and (3) selection bias of output labels (biases in how people choose perspectives (e.g., review-targets) that directly affects outputs (e.g., polarity labels)). Existing studies have modeled (2) and (3) with or without (1) for NLP tasks such as sentiment analysis (Li et al., 2011; Gao et al., 2013; Tang et al., 2015a,b; Chen et al., 2016), machine translation (Mirkin and Meunier, 2015; Michel and Neubig, 2018; Wuebker et al., 2018), and dialogue systems (Li et al., 2016; Zhang et al., 2018). However, it is difficult to untangle the different facets of personal biases, there is no study aiming to analyze solely personal semantic variations. Meanwhile, word embeddings induced for a simple NLP task such as sentiment classification conveys less information, which are not suitable for analyzing semantic variations. Computational linguists have utilized word embeddings to capture semantic variations of words caused by diachronic (Ha"
N19-1215,P15-1166,0,0.0230591,"probability distribution from the encoded representation of the review. Multi-task Learning (MTL): The extremely large number of labels (review-targets) makes it difficult to stably train the target identification model. To mitigate this, we jointly train auxiliary tasks that estimate the metadata of the reviewtarget along with the target task. This approach assumes that understanding metadata contributes the performance of the target identification. Concretely, our MTL model contains a task-shared embedding layer, a task-shared LSTM-encoder, and task-private feed-forward layers similarly to (Dong et al., 2015; Luong et al., 2016). In our experiments, these task-private layers consist of three layers for classification and one layer for regression (Figure 1). In the classification tasks, the model computes log probability over target labels as the output and cross-entropy is used as the loss 2103 function. In the regression task, the output is the metadata itself represented as a scalar value and squared error is used as the loss function. Here, multi-task learning raises a new problem. In auxiliary tasks, since the same reviewer can select the same label multiple times, the personalized word embed"
N19-1215,I13-1156,1,0.86864,"verbalize what we have sensed, there exist inevitable personal biases in word meanings (hereafter, (personal) semantic variations). For example, when we say “this pizza is greasy,” how greasy can vary widely among individuals. When we see the same beer, we may use different words (e.g., red, amber) to refer its color. The semantic variations will thereby cause problems not only in communicating with each other, but also in building natural language processing (NLP) systems. Several studies have attempted to personalize models to improve the performance on NLP tasks such as sentiment analysis (Gao et al., 2013) and dialogue systems (Li et al., 2016; Zhang et al., 2018). All of these studies, however, tried to estimate subjective output from subjective input (e.g., estimating sentiment scores given by reviewers). These personalized models are thereby affected by not only semantic variations in subjective input but also annotation bias (deviation of outputs given by the annotators) and selection bias (deviation of outputs caused by the deviation of input) (§ 2). This makes it difficult to understand the pure impact of the personal semantic variations. In this study, aiming at understanding semantic va"
N19-1215,C16-1065,0,0.0308533,"(Li et al., 2016; Zhang et al., 2018). However, it is difficult to untangle the different facets of personal biases, there is no study aiming to analyze solely personal semantic variations. Meanwhile, word embeddings induced for a simple NLP task such as sentiment classification conveys less information, which are not suitable for analyzing semantic variations. Computational linguists have utilized word embeddings to capture semantic variations of words caused by diachronic (Hamilton et al., 2016; Szymanski, 2017; Rosenfeld and Erk, 2018; Jaidka et al., 2018), geographic (Bamman et al., 2014; Garimella et al., 2016) or domain (Tredici and Fern´andez, 2017) differences. In these studies, they have mainly discussed relationships between semantic variations of words and their frequency, dissemination (the number of users), or polysemy of the words. Hamilton et al. (2016) report that the meanings of more frequent words are more stable over time, and the meanings of polysemous words are likely to change over time since polysemous words appear in diverse contexts (Winter et al., 2014; Br´eal, 1897). Tredici and Fern´andez (2017) report that the meanings of words used by more people are more stable. In this stu"
N19-1215,P16-1141,0,0.193659,"6), machine translation (Mirkin and Meunier, 2015; Michel and Neubig, 2018; Wuebker et al., 2018), and dialogue systems (Li et al., 2016; Zhang et al., 2018). However, it is difficult to untangle the different facets of personal biases, there is no study aiming to analyze solely personal semantic variations. Meanwhile, word embeddings induced for a simple NLP task such as sentiment classification conveys less information, which are not suitable for analyzing semantic variations. Computational linguists have utilized word embeddings to capture semantic variations of words caused by diachronic (Hamilton et al., 2016; Szymanski, 2017; Rosenfeld and Erk, 2018; Jaidka et al., 2018), geographic (Bamman et al., 2014; Garimella et al., 2016) or domain (Tredici and Fern´andez, 2017) differences. In these studies, they have mainly discussed relationships between semantic variations of words and their frequency, dissemination (the number of users), or polysemy of the words. Hamilton et al. (2016) report that the meanings of more frequent words are more stable over time, and the meanings of polysemous words are likely to change over time since polysemous words appear in diverse contexts (Winter et al., 2014; Br´ea"
N19-1215,P18-2032,0,0.0322788,"Missing"
N19-1215,P16-1094,0,0.197852,"st inevitable personal biases in word meanings (hereafter, (personal) semantic variations). For example, when we say “this pizza is greasy,” how greasy can vary widely among individuals. When we see the same beer, we may use different words (e.g., red, amber) to refer its color. The semantic variations will thereby cause problems not only in communicating with each other, but also in building natural language processing (NLP) systems. Several studies have attempted to personalize models to improve the performance on NLP tasks such as sentiment analysis (Gao et al., 2013) and dialogue systems (Li et al., 2016; Zhang et al., 2018). All of these studies, however, tried to estimate subjective output from subjective input (e.g., estimating sentiment scores given by reviewers). These personalized models are thereby affected by not only semantic variations in subjective input but also annotation bias (deviation of outputs given by the annotators) and selection bias (deviation of outputs caused by the deviation of input) (§ 2). This makes it difficult to understand the pure impact of the personal semantic variations. In this study, aiming at understanding semantic variations and their impact on NLP tasks"
N19-1215,P18-2050,0,0.0300071,"idered by personalization in NLP tasks have three facets: (1) semantic variation in task inputs (biases in how people use words; our target) (2) annotation bias of output labels (biases in how annotators label) and (3) selection bias of output labels (biases in how people choose perspectives (e.g., review-targets) that directly affects outputs (e.g., polarity labels)). Existing studies have modeled (2) and (3) with or without (1) for NLP tasks such as sentiment analysis (Li et al., 2011; Gao et al., 2013; Tang et al., 2015a,b; Chen et al., 2016), machine translation (Mirkin and Meunier, 2015; Michel and Neubig, 2018; Wuebker et al., 2018), and dialogue systems (Li et al., 2016; Zhang et al., 2018). However, it is difficult to untangle the different facets of personal biases, there is no study aiming to analyze solely personal semantic variations. Meanwhile, word embeddings induced for a simple NLP task such as sentiment classification conveys less information, which are not suitable for analyzing semantic variations. Computational linguists have utilized word embeddings to capture semantic variations of words caused by diachronic (Hamilton et al., 2016; Szymanski, 2017; Rosenfeld and Erk, 2018; Jaidka et"
N19-1215,D15-1238,0,0.0283224,"cussed in § 1, biases considered by personalization in NLP tasks have three facets: (1) semantic variation in task inputs (biases in how people use words; our target) (2) annotation bias of output labels (biases in how annotators label) and (3) selection bias of output labels (biases in how people choose perspectives (e.g., review-targets) that directly affects outputs (e.g., polarity labels)). Existing studies have modeled (2) and (3) with or without (1) for NLP tasks such as sentiment analysis (Li et al., 2011; Gao et al., 2013; Tang et al., 2015a,b; Chen et al., 2016), machine translation (Mirkin and Meunier, 2015; Michel and Neubig, 2018; Wuebker et al., 2018), and dialogue systems (Li et al., 2016; Zhang et al., 2018). However, it is difficult to untangle the different facets of personal biases, there is no study aiming to analyze solely personal semantic variations. Meanwhile, word embeddings induced for a simple NLP task such as sentiment classification conveys less information, which are not suitable for analyzing semantic variations. Computational linguists have utilized word embeddings to capture semantic variations of words caused by diachronic (Hamilton et al., 2016; Szymanski, 2017; Rosenfeld"
N19-1215,N18-1044,0,0.0227142,"ier, 2015; Michel and Neubig, 2018; Wuebker et al., 2018), and dialogue systems (Li et al., 2016; Zhang et al., 2018). However, it is difficult to untangle the different facets of personal biases, there is no study aiming to analyze solely personal semantic variations. Meanwhile, word embeddings induced for a simple NLP task such as sentiment classification conveys less information, which are not suitable for analyzing semantic variations. Computational linguists have utilized word embeddings to capture semantic variations of words caused by diachronic (Hamilton et al., 2016; Szymanski, 2017; Rosenfeld and Erk, 2018; Jaidka et al., 2018), geographic (Bamman et al., 2014; Garimella et al., 2016) or domain (Tredici and Fern´andez, 2017) differences. In these studies, they have mainly discussed relationships between semantic variations of words and their frequency, dissemination (the number of users), or polysemy of the words. Hamilton et al. (2016) report that the meanings of more frequent words are more stable over time, and the meanings of polysemous words are likely to change over time since polysemous words appear in diverse contexts (Winter et al., 2014; Br´eal, 1897). Tredici and Fern´andez (2017) re"
N19-1215,P17-2071,0,0.0174157,"(Mirkin and Meunier, 2015; Michel and Neubig, 2018; Wuebker et al., 2018), and dialogue systems (Li et al., 2016; Zhang et al., 2018). However, it is difficult to untangle the different facets of personal biases, there is no study aiming to analyze solely personal semantic variations. Meanwhile, word embeddings induced for a simple NLP task such as sentiment classification conveys less information, which are not suitable for analyzing semantic variations. Computational linguists have utilized word embeddings to capture semantic variations of words caused by diachronic (Hamilton et al., 2016; Szymanski, 2017; Rosenfeld and Erk, 2018; Jaidka et al., 2018), geographic (Bamman et al., 2014; Garimella et al., 2016) or domain (Tredici and Fern´andez, 2017) differences. In these studies, they have mainly discussed relationships between semantic variations of words and their frequency, dissemination (the number of users), or polysemy of the words. Hamilton et al. (2016) report that the meanings of more frequent words are more stable over time, and the meanings of polysemous words are likely to change over time since polysemous words appear in diverse contexts (Winter et al., 2014; Br´eal, 1897). Tredici"
N19-1215,P15-1098,0,0.202807,"layer FFNN Task-shared LSTM-encoder Related Work Bi-LSTM As discussed in § 1, biases considered by personalization in NLP tasks have three facets: (1) semantic variation in task inputs (biases in how people use words; our target) (2) annotation bias of output labels (biases in how annotators label) and (3) selection bias of output labels (biases in how people choose perspectives (e.g., review-targets) that directly affects outputs (e.g., polarity labels)). Existing studies have modeled (2) and (3) with or without (1) for NLP tasks such as sentiment analysis (Li et al., 2011; Gao et al., 2013; Tang et al., 2015a,b; Chen et al., 2016), machine translation (Mirkin and Meunier, 2015; Michel and Neubig, 2018; Wuebker et al., 2018), and dialogue systems (Li et al., 2016; Zhang et al., 2018). However, it is difficult to untangle the different facets of personal biases, there is no study aiming to analyze solely personal semantic variations. Meanwhile, word embeddings induced for a simple NLP task such as sentiment classification conveys less information, which are not suitable for analyzing semantic variations. Computational linguists have utilized word embeddings to capture semantic variations of words c"
N19-1215,W17-6804,0,0.02791,"Missing"
N19-1215,D18-1104,0,0.0203343,"in NLP tasks have three facets: (1) semantic variation in task inputs (biases in how people use words; our target) (2) annotation bias of output labels (biases in how annotators label) and (3) selection bias of output labels (biases in how people choose perspectives (e.g., review-targets) that directly affects outputs (e.g., polarity labels)). Existing studies have modeled (2) and (3) with or without (1) for NLP tasks such as sentiment analysis (Li et al., 2011; Gao et al., 2013; Tang et al., 2015a,b; Chen et al., 2016), machine translation (Mirkin and Meunier, 2015; Michel and Neubig, 2018; Wuebker et al., 2018), and dialogue systems (Li et al., 2016; Zhang et al., 2018). However, it is difficult to untangle the different facets of personal biases, there is no study aiming to analyze solely personal semantic variations. Meanwhile, word embeddings induced for a simple NLP task such as sentiment classification conveys less information, which are not suitable for analyzing semantic variations. Computational linguists have utilized word embeddings to capture semantic variations of words caused by diachronic (Hamilton et al., 2016; Szymanski, 2017; Rosenfeld and Erk, 2018; Jaidka et al., 2018), geographic"
N19-1215,P18-1205,0,0.135712,"sonal biases in word meanings (hereafter, (personal) semantic variations). For example, when we say “this pizza is greasy,” how greasy can vary widely among individuals. When we see the same beer, we may use different words (e.g., red, amber) to refer its color. The semantic variations will thereby cause problems not only in communicating with each other, but also in building natural language processing (NLP) systems. Several studies have attempted to personalize models to improve the performance on NLP tasks such as sentiment analysis (Gao et al., 2013) and dialogue systems (Li et al., 2016; Zhang et al., 2018). All of these studies, however, tried to estimate subjective output from subjective input (e.g., estimating sentiment scores given by reviewers). These personalized models are thereby affected by not only semantic variations in subjective input but also annotation bias (deviation of outputs given by the annotators) and selection bias (deviation of outputs caused by the deviation of input) (§ 2). This makes it difficult to understand the pure impact of the personal semantic variations. In this study, aiming at understanding semantic variations and their impact on NLP tasks, we propose a method"
N19-1350,N06-1017,0,0.0413271,"lows, we explain existing tasks that are related to our work. Our task is closely related to word sense disambiguation (WSD) (Navigli, 2009), which identifies a pre-defined sense for the target word with its context. Although we can use it to solve our task by retrieving the definition sentence for the sense identified by WSD, it requires a substantial amount of training data to handle a different set of meanings of each word, and cannot handle words (or senses) which are not registered in the dictionary. Although some studies have attempted to detect novel senses of words for given contexts (Erk, 2006; Lau et al., 2014), they do not provide definition sentences. Our task avoids these difficulties in WSD by directly generating descriptions for 3474 phrases or words. It also allows us to flexibly tailor a fine-grained definition for the specific context. Paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) (or text simplification (Siddharthan, 2014)) can be used to rephrase words with unknown senses. However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) co"
N19-1350,P18-2043,0,0.141504,"s or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation (Ni and Wang, 2017) and definition generation (Noraset et al., 2017; Gadetsky et al., 2018), our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work. 1 Figure 1: Local & Global Context-aware Description generator (LOG-CaD). Introduction When we read news text with emerging entities, text in unfamiliar domains, or text in foreign languages, we often encounter expressions (words or phrases) whose senses we do not understand. In such cases, we may first try"
N19-1350,H92-1045,0,0.356747,"get phrase’s embedding induced from massive text. We performed experiments on three existing datasets and one newly built from Wikipedia and Wikidata. The experimental results confirmed that the local and global contexts complement one another and are both essential; global contexts are crucial when local contexts are short and vague, while the local context is important when the target phrase is polysemous, rare, or unseen. As future work, we plan to modify our model to use multiple contexts in text to improve the quality of descriptions, considering the “one sense per discourse” hypothesis (Gale et al., 1992). We will release the newly built Wikipedia dataset and the experimental codes for the academic and industrial communities at https://github.com/shonosuke/ ishiwatari-naacl2019 to facilitate the reproducibility of our results and their use in various application contexts. Acknowledgements The authors are grateful to Thanapon Noraset for sharing the details of his implementation of the previous work. We also thank the anonymous reviewers for their careful reading of our paper and insightful comments, and the members of Kitsuregawa-Toyoda-Nemoto-Yoshinaga-Goda laboratory in the University of Tok"
N19-1350,W09-2503,0,0.041638,"s for given contexts (Erk, 2006; Lau et al., 2014), they do not provide definition sentences. Our task avoids these difficulties in WSD by directly generating descriptions for 3474 phrases or words. It also allows us to flexibly tailor a fine-grained definition for the specific context. Paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) (or text simplification (Siddharthan, 2014)) can be used to rephrase words with unknown senses. However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, Noraset et al. (2017) introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) proposed a definition generation method that works with polysemous words in dictionaries. The"
N19-1350,D12-1066,0,0.0129473,"contexts (Erk, 2006; Lau et al., 2014), they do not provide definition sentences. Our task avoids these difficulties in WSD by directly generating descriptions for 3474 phrases or words. It also allows us to flexibly tailor a fine-grained definition for the specific context. Paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) (or text simplification (Siddharthan, 2014)) can be used to rephrase words with unknown senses. However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, Noraset et al. (2017) introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs, their method cannot generate an appropriate definition for a polysemous word for a specific context. To cope with this problem, Gadetsky et al. (2018) proposed a definition generation method that works with polysemous words in dictionaries. They presented a model"
N19-1350,I17-2070,0,0.273408,"the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation (Ni and Wang, 2017) and definition generation (Noraset et al., 2017; Gadetsky et al., 2018), our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work. 1 Figure 1: Local & Global Context-aware Description generator (LOG-CaD). Introduction When we read news text with emerging entities, text in unfamiliar domains, or text in foreign languages, we often encounter expressions (words or phra"
N19-1350,P02-1040,0,0.103786,"o predict descriptions. Also, it cannot directly use the local context to predict the words in descriptions. This is because the I-Attention model indirectly uses the local context only to disambiguate the phrase embedding xtrg as x0trg = xtrg m, PI FFNN(hi ) m = σ(Wm i=1 + bm ). I Context: #1 #2 after being enlarged by publisher daniel o’neill it was reportedly one of the largest and most prosperous newspapers in the united states. in 1967 he returned to belfast where he met fellow belfast artist daniel o’neill. Reference: american journalist (18) Automatic Evaluation Table 4 shows the BLEU (Papineni et al., 2002) scores of the output descriptions. We can see that the LOG-CaD model consistently outperforms the three baselines in all four datasets. This result indicates that using both local and global contexts helps describe the unknown words/phrases correctly. While the http://pytorch.org/ Input: daniel o’neill (17) All four models (Table 3) are implemented with the PyTorch framework (Ver. 1.0.0).11 11 Table 6: Descriptions for a word in WordNet. (16) Here, the FFNN(·) function is a feed-forward neural network that maps the encoded local contexts hi to another space. The mapped local contexts are then"
N19-1350,W04-3250,0,0.0275215,"8: Descriptions for a word in Wikipedia. Manual Evaluation To compare the proposed model and the strongest baseline in Table 4 (i.e., the Local model), we performed a human evaluation on our dataset. We randomly selected 100 samples from the test set of the Wikipedia dataset and asked three native English speakers to rate the output descriptions from 1 to 5 points as: 1) completely wrong or self-definition, 2) correct topic with wrong information, 3) correct but incomplete, 4) small details missing, 5) correct. The averaged scores are reported in Table 5. Pair-wise bootstrap resampling test (Koehn, 2004) for the annotated scores has shown that the superiority of LOG-CaD over the Local model is statistically significant (p &lt; 0.01). dataset, both the Local and LOG-CaD models can describe the word/phrase considering its local context. For example, both the Local and LOG-CaD models could generate “american” in the description for “daniel o’neill” given “united states” in context #1, while they could generate “british” given “belfast” in context #2. A similar trend can also be observed in Table 8, where LOG-CaD could generate the locational expressions such as “philippines” and “british” given the"
N19-1350,P14-1025,0,0.0151069,"plain existing tasks that are related to our work. Our task is closely related to word sense disambiguation (WSD) (Navigli, 2009), which identifies a pre-defined sense for the target word with its context. Although we can use it to solve our task by retrieving the definition sentence for the sense identified by WSD, it requires a substantial amount of training data to handle a different set of meanings of each word, and cannot handle words (or senses) which are not registered in the dictionary. Although some studies have attempted to detect novel senses of words for given contexts (Erk, 2006; Lau et al., 2014), they do not provide definition sentences. Our task avoids these difficulties in WSD by directly generating descriptions for 3474 phrases or words. It also allows us to flexibly tailor a fine-grained definition for the specific context. Paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) (or text simplification (Siddharthan, 2014)) can be used to rephrase words with unknown senses. However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsententia"
N19-1350,P16-1100,0,0.120675,"ncluded in a given sentence with the target phrase (i.e., the X in Eq. (1)) as “local context,” and the implicit contextual information in massive text as “global context.” While both local and global contexts are crucial for humans to understand unfamiliar phrases, are they also useful for machines to generate descriptions? To verify this idea, we propose to incorporate both local and global contexts to describe an unknown phrase. 3468 3.2 Proposed model Figure 1 shows an illustration of our LOG-CaD model. Similarly to the standard encoder-decoder model with attention (Bahdanau et al., 2015; Luong and Manning, 2016), it has a context encoder and a description decoder. The challenge here is that the decoder needs to be conditioned not only on the local context, but also on its global context. To incorporate the different types of contexts, we propose to use a gate function similar to Noraset et al. (2017) to dynamically control how the global and local contexts influence the description. Local & global context encoders We first describe how to model local and global contexts. Given a sentence X and a phrase Xtrg , a bidirectional LSTM (Gers et al., 1999) encoder generates a sequence of continuous vectors"
N19-1350,J10-3003,0,0.0217848,"by WSD, it requires a substantial amount of training data to handle a different set of meanings of each word, and cannot handle words (or senses) which are not registered in the dictionary. Although some studies have attempted to detect novel senses of words for given contexts (Erk, 2006; Lau et al., 2014), they do not provide definition sentences. Our task avoids these difficulties in WSD by directly generating descriptions for 3474 phrases or words. It also allows us to flexibly tailor a fine-grained definition for the specific context. Paraphrasing (Androutsopoulos and Malakasiotis, 2010; Madnani and Dorr, 2010) (or text simplification (Siddharthan, 2014)) can be used to rephrase words with unknown senses. However, the target of paraphrase acquisition are words/phrases with no specified context. Although a few studies (Connor and Roth, 2007; Max, 2009; Max et al., 2012) consider subsentential (context-sensitive) paraphrases, they do not intend to obtain a definition-like description as a paraphrase of a word. Recently, Noraset et al. (2017) introduced a task of generating a definition sentence of a word from its pre-trained embedding. Since their task does not take local contexts of words as inputs,"
P03-2033,W97-1506,0,0.0736449,"Missing"
P03-2033,W02-1508,0,0.0189577,"l-purpose grammar through using language intuition encoded in syntactically tagged corpora in XML format. Second, it records data of grammar defects to allow developers to have a whole picture of parsing errors found in the target corpora to save debugging time and effort by prioritizing them. 2 What Is the Ideal Grammar Debugging? There are already other grammar developing tools, such as a grammar writer of XTAG (Paroubek et al., 1992), ALEP (Schmidt et al., 1996), ConTroll (G¨otz and Meurers, 1997), a tool by Nara Institute of Science and Technology (Miyata et al., 1999), and [incr tsdb()] (Oepen et al., 2002). But these tools have following problems; they largely depend on human debuggers’ language intuition, they do not help users to handle large amount of parsing results effectively, and they let human debuggers correct the bugs one after another manually and locally. To cope with these shortcomings, willex proposes an alternative method for more efficient debugging process. The workflow of the conventional grammar developing tools and willex are different in the following ways. With the conventional tools, human debuggers must check each sentence to find out grammar defects and modify them one"
P03-2033,A92-1030,0,0.0317506,"n effort. Hence, we have developed willex that helps to improve the general-purpose grammars. Willex has two major functions. First, it reduces a human workload to improve the general-purpose grammar through using language intuition encoded in syntactically tagged corpora in XML format. Second, it records data of grammar defects to allow developers to have a whole picture of parsing errors found in the target corpora to save debugging time and effort by prioritizing them. 2 What Is the Ideal Grammar Debugging? There are already other grammar developing tools, such as a grammar writer of XTAG (Paroubek et al., 1992), ALEP (Schmidt et al., 1996), ConTroll (G¨otz and Meurers, 1997), a tool by Nara Institute of Science and Technology (Miyata et al., 1999), and [incr tsdb()] (Oepen et al., 2002). But these tools have following problems; they largely depend on human debuggers’ language intuition, they do not help users to handle large amount of parsing results effectively, and they let human debuggers correct the bugs one after another manually and locally. To cope with these shortcomings, willex proposes an alternative method for more efficient debugging process. The workflow of the conventional grammar deve"
P03-2033,C96-1049,0,0.0133803,"oped willex that helps to improve the general-purpose grammars. Willex has two major functions. First, it reduces a human workload to improve the general-purpose grammar through using language intuition encoded in syntactically tagged corpora in XML format. Second, it records data of grammar defects to allow developers to have a whole picture of parsing errors found in the target corpora to save debugging time and effort by prioritizing them. 2 What Is the Ideal Grammar Debugging? There are already other grammar developing tools, such as a grammar writer of XTAG (Paroubek et al., 1992), ALEP (Schmidt et al., 1996), ConTroll (G¨otz and Meurers, 1997), a tool by Nara Institute of Science and Technology (Miyata et al., 1999), and [incr tsdb()] (Oepen et al., 2002). But these tools have following problems; they largely depend on human debuggers’ language intuition, they do not help users to handle large amount of parsing results effectively, and they let human debuggers correct the bugs one after another manually and locally. To cope with these shortcomings, willex proposes an alternative method for more efficient debugging process. The workflow of the conventional grammar developing tools and willex are d"
P03-2033,W98-0141,1,0.819893,"ons. First, it decreases ambiguity of the parsing results by comparing them to an annotated corpus and removing wrong partial results both automatically and manually. Second, willex accumulates parsing errors as data for the developers to clarify the defects of the grammar statistically. We applied willex to a large-scale HPSG-style grammar as an example. 1 Introduction There is an increasing need for syntactical parsers for practical usages, such as information extraction. For example, Yakushiji et al. (2001) extracted argument structures from biomedical papers using a parser based on XHPSG (Tateisi et al., 1998), which is a large-scale HPSG. Although large-scale and general-purpose grammars have been developed, they have a problem of limited coverage. The limits are derived from deficiencies of grammars themselves. For example, XHPSG cannot treat coordinations of verbs (ex. “Molybdate slowed but did not prevent the conversion.”) nor reduced relatives (ex. “Rb mutants derived from patients with retinoblastoma.”). Finding these grammar defects and modifying them require tremendous human effort. Hence, we have developed willex that helps to improve the general-purpose grammars. Willex has two major func"
P03-2033,W98-1118,0,\N,Missing
P03-2033,C00-2102,0,\N,Missing
P03-2033,P02-1060,0,\N,Missing
P03-2036,P81-1022,0,0.0851703,"Missing"
P03-2036,2000.iwpt-1.15,0,0.76089,"ations sometimes exhibit quite different performance in each grammar formalism (Yoshida et al., 1999; Yoshinaga et al., 2001). If we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations. This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the further advancement of the whole parsing community. In this paper, we compare CFG filtering techniques for LTAG (Harbusch, 1990; Poller and Becker, 1998) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000), following an approach to parsing comparison among different grammar formalisms (Yoshinaga et al., 2001). The key idea of the approach is to use strongly equivalent grammars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated by Yoshinaga and Miyao (2001). The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing. Investigating"
P03-2036,J93-2004,0,0.0307471,"from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity of the parsers close to that of CFG parsing. Investigating the difference between the ways of context-free (CF) approximation of LTAG and HPSG will thereby enlighten a way of further optimization for both techniques. We performed a comparison between the existing CFG filtering techniques for LTAG (Poller and Becker, 1998) and HPSG (Torisawa et al., 2000), using strongly equivalent grammars obtained by converting LTAGs extracted from the Penn Treebank (Marcus et al., 1993) into HPSG-style. We compared the parsers with respect to the size of the approximated CFG and its effectiveness as a filter. 2 Background In this section, we introduce a grammar conversion (Yoshinaga and Miyao, 2001) and CFG filtering (Harbusch, 1990; Poller and Becker, 1998; Torisawa et al., 2000; Kiefer and Krieger, 2000). 2.1 Grammar conversion The grammar conversion consists of a conversion of LTAG elementary trees to HPSG lexical entries and an emulation of substitution and adjunction by Tree 5: Tree 9: S 5.ε NP5.1 VP5.2 V 5.2.1 NP5.2.2 S 9.ε NP9.1 VP 9.2 V 9.2.1 S 9.2.2 CFG rules NP VP"
P03-2036,J93-4001,0,0.584153,"Missing"
P03-2036,E03-1047,1,0.493609,",115 58,356 68,239 118,464 Table 2: Parsing performance (sec.) with the strongly equivalent grammars for Section 2 of WSJ Parser PB TNT 3 G2 1.4 0.044 G2-4 9.1 0.097 G2-6 17.4 0.144 G2-8 24.0 0.182 G2-10 34.2 0.224 G2-21 124.3 0.542 Comparison with CFG filtering In this section, we compare a pair of CFG filtering techniques for LTAG (Poller and Becker, 1998) and HPSG (Torisawa et al., 2000) described in Section 2.2.1 and 2.2.2. We hereafter refer to PB and TNT for the C++ implementations of the former and a valiant1 of the latter, respectively.2 We first acquired LTAGs by a method proposed in Miyao et al. (2003) from Sections 2-21 of the Wall Street Journal (WSJ) in the Penn Treebank (Marcus et al., 1993) and its subsets.3 We then converted them into strongly equivalent HPSG-style grammars using the grammar conversion described in Section 2.1. Table 1 shows the size of CFG approximated from the strongly equivalent grammars. Gx , CFGPB , and CFGTNT henceforth refer to the LTAG extracted from Section x of WSJ and CFGs approximated from Gx by PB and TNT, respectively. The size of CFGTNT is much larger than that of CFGPB . By investigating parsing performance using these CFGs, we show that the larger siz"
P03-2036,W98-0134,0,0.552982,"t al., 1999; Torisawa et al., 2000). However, these realizations sometimes exhibit quite different performance in each grammar formalism (Yoshida et al., 1999; Yoshinaga et al., 2001). If we could identify an algorithmic difference that causes performance difference, it would reveal advantages and disadvantages of the different realizations. This should also allow us to integrate the advantages of the realizations into one generic parsing technique, which yields the further advancement of the whole parsing community. In this paper, we compare CFG filtering techniques for LTAG (Harbusch, 1990; Poller and Becker, 1998) and HPSG (Torisawa et al., 2000; Kiefer and Krieger, 2000), following an approach to parsing comparison among different grammar formalisms (Yoshinaga et al., 2001). The key idea of the approach is to use strongly equivalent grammars, which generate equivalent parse results for the same input, obtained by a grammar conversion as demonstrated by Yoshinaga and Miyao (2001). The parsers with CFG filtering predict possible parse trees by a CFG approximated from a given grammar. Comparison of those parsers are interesting because effective CFG filters allow us to bring the empirical time complexity"
P03-2036,C88-2121,0,0.322459,"Missing"
P03-2036,P90-1036,0,\N,Missing
P04-2008,J87-3002,0,0.17457,"Missing"
P04-2008,A97-1052,0,0.0749536,"Missing"
P04-2008,C94-1042,0,0.124697,"Missing"
P04-2008,P03-1009,0,0.194359,"Missing"
P04-2008,W00-1605,0,0.0273459,"as lexicons of lexicalized grammars. However, there has been little work on evaluating the impact of acquired SCFs with the exception of (Carroll and Fang, 2004). The problem when we integrate acquired SCFs into existing lexicalized grammars is lower quality of the acquired SCFs, since they are acquired in an unsupervised manner, rather than being manually coded. If we attempt to compensate for the poor precision by being less strict in filtering out less likely SCFs, then we will end up with a larger number of noisy lexical entries, which is problematic for parsing with lexicalized grammars (Sarkar et al., 2000). We thus need some method of selecting the most reliable set of SCFs from the system output as demonstrated in (Korhonen, 2002). In this paper, I present a method of improving the accuracy of SCFs acquired from corpora in order to augment existing lexicon resources. I first estimate a confidence value that a word can have each SCF, using corpus-based statistics. To capture latent co-occurrence tendency among SCFs in the target lexicon, I next perform clustering of SCF confidence-value vectors of words in the acquired lexicon and the target lexicon. Since each centroid value of the obtained cl"
P04-2008,P02-1029,0,0.218768,"Missing"
P10-1050,A00-1031,0,0.0742223,"exity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1 The first-order Markov assumption is made throughout this paper, although our algorithm is applicable to higherorder Markov models as well. it becomes prohibitively slow when dealing with a large number of labels, since its computational cost is quadratic in L (Dietterich et al., 2008). Unfortunately, several sequence-labeling problems in NLP involve a large number of labels. For example, there are more than 40 and 2000 labels in POS tagging and supertagging, respectively (Brants, 2000; Matsuzaki et al., 2007). These tasks incur much higher computational costs than simpler tasks like NP chunking. What is worse, the number of labels grows drastically if we jointly perform multiple tasks. As we shall see later, we need over 300 labels to reduce joint POS tagging and chunking into the single sequence labeling problem. Although joint learning has attracted much attention in recent years, how to perform decoding efficiently still remains an open problem. In this paper, we present a new decoding algorithm that overcomes this problem. The proposed algorithm has three distinguishin"
P10-1050,N06-1022,0,0.221958,"Missing"
P10-1050,W02-1001,0,0.895096,"rty of our algorithm is that it efficiently reduces the labels to be decoded, while still allowing us to check the optimality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, C ARPE D IEM (Esposito and Radicioni, 2009). 1 Introduction In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins’ perceptrons have been extensively studied in the field of NLP (Rabiner, 1989; Lafferty et al., 2001; Collins, 2002). Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and so on (Sha and Pereira, 2003; Tsuruoka and Tsujii, 2005; Lin and Wu, 2009). One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viterbi, 1967). The Viterbi algorithm has O(N L2 ) time complexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1 The first-order Markov assumption is"
P10-1050,P09-2071,0,0.0834978,"rongly correlated. This assumption is appropriate for some NLP tasks. For example, as suggested in (Liang et al., 2008), adjacent labels do not provide strong information in POS tagging. However, the applicability of this idea to other NLP tasks is still unclear. Approximate algorithms, such as beam search or island-driven search, have been proposed for speeding up decoding. Tsuruoka and Tsujii (2005) proposed easiest-first deterministic decoding. Siddiqi and Moore (2005) presented the parameter tying approach for fast inference in HMMs. A similar idea was applied to CRFs as well (Cohn, 2006; Jeong et al., 2009). In general, approximate algorithms have the advantage of speed over exact algorithms. However, both types of algorithms are still widely adopted by practitioners, since exact algorithms have merits other than speed. First, the optimality of the solution is always guaranteed. It is hard for most of the approximate algorithms to even bound the error rate. Second, approximate algorithms usually require hyperparameters, which control the tradeoff between accuracy and efficiency (e.g., beam width), and these have to be manually adjusted. On the other hand, most of the exact algorithms, including"
P10-1050,W06-1619,0,0.0545116,"Missing"
P10-1050,N03-1028,0,0.365368,"ality of the solution. Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, C ARPE D IEM (Esposito and Radicioni, 2009). 1 Introduction In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins’ perceptrons have been extensively studied in the field of NLP (Rabiner, 1989; Lafferty et al., 2001; Collins, 2002). Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and so on (Sha and Pereira, 2003; Tsuruoka and Tsujii, 2005; Lin and Wu, 2009). One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viterbi, 1967). The Viterbi algorithm has O(N L2 ) time complexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1 The first-order Markov assumption is made throughout this paper, although our algorithm is applicable to higherorder Markov models as well. it becomes prohibitivel"
P10-1050,H05-1059,0,0.594671,"Experiments on three tasks (POS tagging, joint POS tagging and chunking, and supertagging) show that the new algorithm is several orders of magnitude faster than the basic Viterbi and a state-of-the-art algorithm, C ARPE D IEM (Esposito and Radicioni, 2009). 1 Introduction In the past decade, sequence labeling algorithms such as HMMs, CRFs, and Collins’ perceptrons have been extensively studied in the field of NLP (Rabiner, 1989; Lafferty et al., 2001; Collins, 2002). Now they are indispensable in a wide range of NLP tasks including chunking, POS tagging, NER and so on (Sha and Pereira, 2003; Tsuruoka and Tsujii, 2005; Lin and Wu, 2009). One important task in sequence labeling is how to find the most probable label sequence from among all possible ones. This task, referred to as decoding, is usually carried out using the Viterbi algorithm (Viterbi, 1967). The Viterbi algorithm has O(N L2 ) time complexity,1 where N is the input size and L is the number of labels. Although the Viterbi algorithm is generally efficient, 1 The first-order Markov assumption is made throughout this paper, although our algorithm is applicable to higherorder Markov models as well. it becomes prohibitively slow when dealing with a"
P10-1050,P09-1116,0,\N,Missing
P13-1095,W03-2506,0,0.0141977,"n is generated as a response to I have had a high fever for 3 days when the goal emotion is specified as JOY, while Sorry, but you can’t join us today is generated for SADNESS (Figure 1). Systems that can perform the two tasks not only serve as crucial components of dialogue systems but also have interesting applications of their own. Predicting the emotion of an addressee is useful for filtering flames or infelicitous expressions from online messages (Spertus, 1997). The response generator that is aware of the emotion of an addressee is also useful for text completion in online conversation (Hasselgren et al., 2003; Pang and Ravi, 2012). This paper explores a data-driven approach to performing the two tasks. With the recent emergence of social media, especially microblogs, the amount of dialogue data available is rapidly increasing. Therefore, we are taking this opportunity to building large-scale training data from microblog posts automatically. This approach allows us to perform the two tasks in a large-scale with little human effort. We employ standard classifiers for predicting the emotion of an addressee. Our contribution here is to investigate the effectiveness of new features that cannot be used"
P13-1095,P12-2030,0,0.0228433,"blossoms) and an emotion elicited by it (e.g., SADNESS) from the Web text. The extracted data are used for emotion classification. A similar technique would be useful for prediction the emotion of an addressee as well. Response generation has a long research history (Weizenbaum, 1966), although it is only very recently that a fully statistical approach was introduced in this field (Ritter et al., 2011). At this moment, we are unaware of any statistical response generators that model the emotion of the user. Some researchers have explored generating jokes or humorous text (Dybala et al., 2010; Labtov and Lipson, 2012). Those attempts are similar to our work in that they also aim at eliciting a certain emotion in the addressee. They are, however, restricted to elicit a specific emotion. The linear interpolation of translation and/or language models is a widely-used technique for adapting machine translation systems to new domains (Sennrich, 2012). However, it has not been touched in the context of response generation. 6 Related Work There have been a tremendous amount of studies on predicting the emotion from text or speech data (Ayadi et al., 2011; Bandyopadhyay and Okumura, 2011; Balahur et al., 2011; Bal"
P13-1095,P03-1021,0,0.0233925,"t frequently finds paraphrase pairs, learning to parrot back the input (Ritter et al., 2011). To avoid using such pairs for response generation, a phrase pair is removed if one phrase is the substring of the other. We use Moses decoder10 to search for the best response to a given utterance. Unlike machine translation, we do not use reordering models, because the positions of phrases are not considered to correlate strongly with the appropriateness of responses (Ritter et al., 2011). In addition, we do not use any discriminative training methods such as MERT for optimizing the feature weights (Och, 2003). They are set as default values provided by Moses (Ritter et al., 2011). 5 Experiments 5.1 Test data To evaluate the proposed method, we built, as test data, sets of an utterance paired with responses that elicit a certain goal emotion (Table 5). Note that they were used for evaluation in both of the two tasks. Each utterance in the test data has more than one responses that elicit the same goal emotion, because they are used to compute BLEU score (see section 5.3). The data set was built in the following manner. We first asked five human worker to produce responses to 80 utterances (10 utter"
P13-1095,D12-1136,0,0.0155653,"nse to I have had a high fever for 3 days when the goal emotion is specified as JOY, while Sorry, but you can’t join us today is generated for SADNESS (Figure 1). Systems that can perform the two tasks not only serve as crucial components of dialogue systems but also have interesting applications of their own. Predicting the emotion of an addressee is useful for filtering flames or infelicitous expressions from online messages (Spertus, 1997). The response generator that is aware of the emotion of an addressee is also useful for text completion in online conversation (Hasselgren et al., 2003; Pang and Ravi, 2012). This paper explores a data-driven approach to performing the two tasks. With the recent emergence of social media, especially microblogs, the amount of dialogue data available is rapidly increasing. Therefore, we are taking this opportunity to building large-scale training data from microblog posts automatically. This approach allows us to perform the two tasks in a large-scale with little human effort. We employ standard classifiers for predicting the emotion of an addressee. Our contribution here is to investigate the effectiveness of new features that cannot be used in ordinary emotion re"
P13-1095,P02-1040,0,0.0913039,"ers; 74 and 92 responses were regarded as appropirate by both of the workers. These results suggest the effectiveness of the proposed method. Especially, we can confirm that the proposed method can generate responses that elicit addresee’s emotion more clearly. We investigated the agreement between the two workers in this evaluation. We found that the κ coefficient is 0.59, which indicates moderate agreement. This supports the reliability of our evaluation. Table 9: Examples of utterance-response pairs to which the system predicted wrong emotions. Automatic evaluation We first use BLEU score (Papineni et al., 2002) to perform automatic evaluation (Ritter et al., 2011). In this evaluation, the system is provided with the utterance and the goal emotion in the test data and the generated responses are evaluated through BLEU score. Specifically, we conducted two-fold cross-validation to optimize the weights of our method. We tried α and β in {0.0, 0.2, 0.4, 0.6, 0.8, 1.0} and selected the weights that achieved the best BLEU score. Note that we adopted different values of the weights for different emotional categories. Table 10 compares BLEU scores of three methods including the proposed one. The first row r"
P13-1095,D11-1054,0,0.656486,"ctiveness of new features that cannot be used in ordinary emotion recognition, the task of estimating the emotion of a speaker (or writer) from her/his utterance (or writing) (Ayadi et al., 2011; Bandyopadhyay and Okumura, 2011; Balahur et al., 2011; Balahur et al., 2012). We specifically extract features from the addressee’s last utterance (e.g., I have had a high fever for 3 days in Figure 1) and explore the effectiveness of using such features. Such information is characteristic of a dialogue situation. To perform the generation task, we build a statistical response generator by following (Ritter et al., 2011). To improve on the previous study, we investigate a method for controlling the contents of the response for, in our case, eliciting the goal emotion. We achieve this by using a technique inspired by domain adaptation. We learn multiple models, each of which is adapted for eliciting one 2 Emotion-tagged Dialogue Corpus The key in making a supervised approach to predicting and eliciting addressee’s emotion successful is to obtain large-scale, reliable training data effectually. We thus automatically build a largescale emotion-tagged dialogue corpus from microblog posts, and use it as the traini"
P13-1095,E12-1055,0,0.037559,"re short, we expect emotions elicited by a response post not to be very diverse and a multiclass classification to be able to capture the essential crux of the prediction task. 6 We should note that a one-versus-the-rest classifier can be used in the multi-label classification scenario, just by allowing the classifier to output more than one emotional category (Ghamrawi and McCallum, 2005). 7 We have excluded n-grams that matched the emotional expressions used in Section 2 to avoid overfitting. 8 http://code.google.com/p/giza-pp/ 9 http://www.speech.sri.com/projects/ srilm/ 967 lation models (Sennrich, 2012). For all the four features (i.e., two phrase translation probabilities and two lexical weights) derived from translation model, the weights of the adapted model are equally set as α (0 ≤ α ≤ 1.0). On the other hand, we use SRILM for the interpolation of language models. The weight of the adapted model is set as β (0 ≤ β ≤ 1.0). The parameters α and β control the strength of the adapted models. Only adapted models are used when α (or β) = 1.0, while the adapted models are not at all used when α (or β) = 0. When both α and β are specified as 0, the model becomes equivalent to the original one d"
P13-1095,D11-1014,0,0.0620487,"tional expressions, we independently train for each emotional category a binary classifier that estimates the addresser’s emotion from her/his utterance and apply it to the unlabeled utterances. The training data for these classifiers are the emotion-tagged utterances obtained in Section 2, while the features are n-grams (n ≤ 3)7 in the utterance. We should emphasize that the features induced from the addressee’s utterance are unique to this task and are hardly available in the related tasks that predicted the emotion of a reader of news articles (Lin and Hsin-Yihn, 2008) or personal stories (Socher et al., 2011). We will later confirm the impact of these features on the prediction accuracy in the experiments. This section describes a method for predicting emotion elicited in an addressee when s/he receives a response to her/his utterance. The input to this task is a pair of an utterance and a response to it, e.g., the two utterances in Figure 1, while the output is the addressee’s emotion among the emotional categories of Plutchik (1980) (JOY and SADNESS for the top and bottom dialogues in Figure 1, respectively). Although a response could elicit multiple emotions in the addressee, in this paper we f"
P13-1095,N04-1026,0,0.00766201,"). Given the history, the system predicts the addressee’s emotion that will be caused by the response. For example, the system outputs JOY when the response is I hope you feel better soon, while it outputs SADNESS when the response is Sorry, but you can’t join us today When we have a conversation, we usually care about the emotion of the person to whom we speak. For example, we try to cheer her/him up if we find out s/he feels down, or we avoid saying things that would trouble her/him. To date, the modeling of emotion in a dialogue has extensively been studied in NLP as well as related areas (Forbes-Riley and Litman, 2004; Ayadi et al., 2011). However, the past attempts are virtually restricted to estimating the emotion of an addresser1 from her/his utterance. In contrast, few studies have explored how the emotion of the addressee is affected by the utterance. We consider the insufficiency of such research to be fatal for ∗ This work was conducted while the first author was a graduate student at the University of Tokyo. 1 We use the terms addresser/addressee rather than a speaker/listener, because we target not spoken but online dialogue. 2 We adopt Plutchik (1980)’s eight emotional categories in both tasks. 9"
P13-1095,C08-1111,0,0.106123,"data (Ayadi et al., 2011; Bandyopadhyay and Okumura, 2011; Balahur et al., 2011; Balahur et al., 2012). Unlike our prediction task, most of them have exclusively focused on estimating the emotion of a speaker (or writer) from her/his utterance (or writing). Analogous to our prediction task, Lin and HsinYihn (2008) and Socher et al. (2011) investigated predicting the emotion of a reader from the text that s/he reads. Our work differs from them in that we focus on dialogue data, and we exploit features that are not available within their task settings, e.g., the addressee’s previous utterance. Tokuhisa et al. (2008) proposed a method for 7 Conclusion and Future Work In this paper, we have explored predicting and eliciting the emotion of an addressee by using a large amount of dialogue data obtained from microblog posts. In the first attempt to model the emotion of an addressee in the field of NLP, we demonstrated that the response of the dialogue partner and the previous utterance of the addressee are useful for predicting the emotion. In the generation task, on the other hand, we showed that the 971 model adaptation approach successfully generates the responses that elicit the goal emotion. For future w"
P13-1095,P11-2102,0,0.069634,"Missing"
P13-1095,D08-1015,0,\N,Missing
P17-1174,W14-4012,0,0.135364,"Missing"
P17-1174,D14-1179,0,0.139136,"Missing"
P17-1174,W16-4616,0,0.0627963,"Missing"
P17-1174,W16-4617,0,0.281753,"re the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1). Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2). Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3). We evaluate the three models on the WAT ’16 English-to-Japanese translation task (§ 4). The experimental results show that our best model outperforms the best single NMT model reported in WAT ’16 (Eriguchi et al., 2016b). Our contributions are twofold: (1) chunk information is introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps"
P17-1174,P16-1078,0,0.444205,"re the hierarchical structure, we start from a hierarchical RNN that consists of a chunk-level decoder and a word-level decoder (Model 1). Then, we improve the word-level decoder by introducing inter-chunk connections to capture the interaction between chunks (Model 2). Finally, we introduce a feedback mechanism to the chunk-level decoder to enhance the memory capacity of previous outputs (Model 3). We evaluate the three models on the WAT ’16 English-to-Japanese translation task (§ 4). The experimental results show that our best model outperforms the best single NMT model reported in WAT ’16 (Eriguchi et al., 2016b). Our contributions are twofold: (1) chunk information is introduced into NMT to improve translation performance, and (2) a novel hierarchical decoder is devised to model the properties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps"
P17-1174,W08-0509,0,0.0958144,"Missing"
P17-1174,D10-1092,0,0.0822748,"Missing"
P17-1174,D13-1176,0,0.543806,"roperties of chunk structure in the encoder-decoder framework. 2 Preliminaries: Attention-based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence. The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al., 2016b). While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as the decoder. Following (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper. A GRU unit computes its hidden state vector hi given an input vector xi and the previous hidden state h"
P17-1174,2010.eamt-1.27,0,0.242941,"re serious in free word-order languages such as Czech, German, Japanese, and Turkish. In the case of the example in Figure 1, the order of the phrase “早く (early)” and the phrase “家へ (to home)” is flexible. This means that simply memorizing the word order in training data is not enough to train a model that can assign a high probability to a correct sentence regardless of its word order. In the past, chunks (or phrases) were utilized to handle the above problems in statistical machine translation (SMT) (Watanabe et al., 2003; Koehn et al., 2003) and in example-based machine translation (EBMT) (Kim et al., 2010). By using a chunk rather than a word as the basic translation unit, one can treat a sentence as a shorter sequence. This makes it easy to capture the longer dependencies in a target sentence. The order of words in a chunk is relatively fixed while that in a sentence is much more flexible. Thus, modeling intra-chunk (local) word orders and inter-chunk (global) dependencies independently can help capture the difference of the flexibility between the word order and the chunk order in free word-order languages. In this paper, we refine the original RNN decoder to consider chunk information in NMT"
P17-1174,N03-1017,0,0.138863,"Missing"
P17-1174,P15-1107,0,0.0436534,"Missing"
P17-1174,D15-1106,1,0.817883,"tructures of the source language. In contrast, our work focuses on the decoding process to capture the structure of the target language. The encoders described above and our proposed decoders are complementary so they can be combined into a single network. Considering that our Model 1 described in § 3.1 can be seen as a hierarchical RNN, our work is also related to previous studies that utilize multi-layer RNN s to capture hierarchical structures in data. Hierarchical RNNs are used for various NLP tasks such as machine translation (Luong and Manning, 2016), document modeling (Li et al., 2015; Lin et al., 2015), dialog generation (Serban et al., 2017), image captioning (Krause et al., 2016), and video captioning (Yu et al., 2016). In particular, Li et al. (2015) and Luong and Manning (2016) use hierarchical encoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly ca"
P17-1174,P02-1040,0,0.098266,"Missing"
P17-1174,P16-1100,0,0.223268,"based Neural Machine Translation In this section, we briefly introduce the architecture of the attention-based NMT model (Bahdanau et al., 2015), which is the basis of our proposed models. 2.1 Neural Machine Translation An NMT model usually consists of two connected neural networks: an encoder and a decoder. After the encoder maps a source sentence into a fixed-length vector, the decoder maps the vector into a target sentence. The implementation of the encoder can be a convolutional neural network (CNN) (Kalchbrenner and Blunsom, 2013), a long short-term memory (LSTM) (Sutskever et al., 2014; Luong and Manning, 2016), a gated recurrent unit (GRU) (Cho et al., 2014b; Bahdanau et al., 2015), or a Tree-LSTM (Eriguchi et al., 2016b). While various architectures are leveraged as an encoder to capture the structural information in the source language, most of the NMT models rely on a standard sequential network such as LSTM or GRU as the decoder. Following (Bahdanau et al., 2015), we use GRU as the recurrent unit in this paper. A GRU unit computes its hidden state vector hi given an input vector xi and the previous hidden state hi−1 : hi = GRU(hi−1 , xi ). (1) The function GRU(·) is calculated as ri = σ(Wr xi +"
P17-1174,P15-1002,0,0.0264998,"ncoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly capture the syntactic structure based on chunk segmentation. In addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective in improving the translation quality (Luong et al., 2015a; Sutskever et al., 2014). Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers. A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every time step. In contrast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the infor"
P17-1174,D15-1166,0,0.0747669,"ncoder-decoder models, but not for the purpose of learning syntactic structures of target sentences. Li et al. (2015) build hierarchical models at the sentence-word level to obtain better document representations. Luong and Manning (2016) build the word-character level to cope with the out-of-vocabulary problem. In contrast, we build a hierarchical models at the chunk-word level to explicitly capture the syntactic structure based on chunk segmentation. In addition, the architecture of Model 3 is also related to stacked RNN, which has shown to be effective in improving the translation quality (Luong et al., 2015a; Sutskever et al., 2014). Although these architectures look similar to each other, there is a fundamental difference between the directions of the connection between two layers. A stacked RNN consists of multiple RNN layers that are connected from the input side to the output side at every time step. In contrast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the infor"
P17-1174,C00-1082,0,0.0179728,"Missing"
P17-1174,W16-4610,0,0.0340474,"Missing"
P17-1174,W15-5003,0,0.0366542,"Missing"
P17-1174,P11-2093,0,0.0894041,"Missing"
P17-1174,J03-1002,0,0.017893,"Missing"
P17-1174,W16-2323,0,0.0517383,"Missing"
P17-1174,P15-1150,0,0.0592716,"Missing"
P17-1174,P03-1039,0,0.041371,"Missing"
P17-1174,D09-1160,1,0.802151,"Missing"
P17-1174,C10-1140,1,0.849059,"Missing"
P17-1174,C14-1103,1,0.828074,"Missing"
P17-1174,P17-2092,0,0.0515711,"trast, our Model 3 has a different connection at each time step. Before it generates a chunk, there is a feed-forward connection from the chunk-level decoder to the word-level decoder. However, after generating a chunk representation, the connection is to be reversed to feed back the information from the word-level decoder to the chunk-level decoder. By switching the connections between two layers, our model can capture the chunk structure explicitly. This is the first work that proposes decoders for NMT that can capture plausible linguistic structures such as chunk. Finally, we noticed that (Zhou et al., 2017) (which is accepted at the same time as this paper) have also proposed a chunk-based decoder for NMT . Their good experimental result on Chinese to English translation task also indicates the effectiveness of “chunk-by-chunk” decoders. Although their architecture is similar to our Model 2, there are several differences: (1) they adopt chunk-level attention instead of word-level attention; (2) their model predicts chunk tags (such as noun phrase), while ours only predicts chunk boundaries; and (3) they employ a boundary gate to decide the chunk boundaries, while we do that by simply having the"
P17-1174,P07-2045,0,\N,Missing
P17-3020,D16-1012,0,0.266263,"nces and decoding its responses, respectively. To capture the conversational situations, we design two mechanisms that differ in how strong of an effect a given situation has on generating responses. In experiments, we examined the proposed conversational models by incorporating three types of concrete conversational situations (§ 2): utterance, speaker/addressee (profiles), and time (season), respectively. Although the models are capable of generating responses, we evaluate the models with a response selection test to avoid known issues in automatic evaluation metrics of generated responses (Liu et al., 2016a). Experimental results obtained using massive dialogue data from Twitter showed that modeling conversational situations improved the relevance of responses (§ 4). 2 User (profiles) User characteristics should affect his/her responses as Li et al. (2016b) have already discussed. We classify profiles provided by each user in our dialogue dataset (§ 4.1) to acquire conversational situations specific to the speakers and addressees. The same as with the input utterance, we first construct a distributed representation of each user’s profile by averaging the pre-trained word2vec vectors for verbs,"
P17-3020,C16-1239,0,0.0609998,"Missing"
P17-3020,P13-1095,1,0.890852,"ch Workshop, pages 120–127 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-3020 are represented in the utterance but are not captured by the S EQ 2S EQ sequential encoder (Sato et al., 2016). We first represent each utterance of utterance-response pairs in our dialogue dataset by a distributed representation obtained by averaging word2vec1 vectors (pre-trained from our dialogue datasets (§ 4.1)) for words in the utterances. The utterances are then classified by k-means clustering to identify utterance types.2 tions (Hasegawa et al., 2013) or personal characteristics (Li et al., 2016b) and topics (Xing et al., 2017), the methods are specially designed for and evaluated using specific types of situations. In this study, we explore neural conversational models that have general mechanisms to incorporate various types of situations behind chat conversations (§ 3.2). These models take into account situations on the speaker’s side and the addressee’s side (or those who respond) when encoding utterances and decoding its responses, respectively. To capture the conversational situations, we design two mechanisms that differ in how stro"
P17-3020,P15-1001,0,0.0363085,"r decoder.7 Table 1 shows statistics on our dialogue datasets. Models In our experiments, we compare our situation-aware neural conversational models (we refer to the model in § 3.2.1 as L/G S EQ 2S EQ and the model in § 3.2.2 as S EQ 2S EQ emb) with situation-unaware baseline (§ 3.1) for taking each type of conversational situations (§ 2) into consideration. We also evaluate the model in § 3.2.1 without global-RNNs (referred to as L S EQ 2S EQ) to observe the impact of global-RNNs. We used a long-short term memory (LSTM) (Zaremba et al., 2014) as the RNN encoder and decoder, sampled softmax (Jean et al., 2015) to accelerate the training, and TensorFlow8 to implement the models. Our LSTMs have three layers and are optimized by Adam (Kingma and Ba, 2015). The hyperparameters are fixed as in Table 2. Evaluation In this section, we evaluate our situation-aware neural conversational models on massive dialogue data obtained from Twitter. We compare our models (§ 3.2) with S EQ 2S EQ baseline (§ 3.1) using a response selection test instead of evaluating generated responses, since Liu et al. (2016a) recently pointed out several problems of existing metrics such as BLEU (Papineni et al., 2002) for evaluatin"
P17-3020,P02-1040,0,0.0995602,"sampled softmax (Jean et al., 2015) to accelerate the training, and TensorFlow8 to implement the models. Our LSTMs have three layers and are optimized by Adam (Kingma and Ba, 2015). The hyperparameters are fixed as in Table 2. Evaluation In this section, we evaluate our situation-aware neural conversational models on massive dialogue data obtained from Twitter. We compare our models (§ 3.2) with S EQ 2S EQ baseline (§ 3.1) using a response selection test instead of evaluating generated responses, since Liu et al. (2016a) recently pointed out several problems of existing metrics such as BLEU (Papineni et al., 2002) for evaluating generated responses. 4.1 Settings Data We built massive dialogue datasets from our Twitter archive that have been compiled since March, 2011. In this archive, timelines of about 1.5 million users4 have been continuously collected with the official API. It is therefore suitable for extracting users’ conversations in timelines. On Twitter, a post (tweet) and a mention to it can be considered as an utterance-response pair. We randomly extracted 23,563,865 and 1,200,000 pairs from dialogues in 2014 as training and validation datasets, and extracted 6000 pairs in 2015 as a test data"
P17-3020,D11-1054,0,0.0226315,"Missing"
P17-3020,W16-3646,0,0.01685,"Table 4: Responses selected by the systems. Examples Table 4 lists the response candidate selected by the baseline and our models. As we had expected, the situation-aware conversational models are better at selecting ground-truth responses for situation-specific conversations. 5 Related Work Conversational situations have been implicitly addressed by preparing datasets specific to the target situations and by solving the problem as a taskoriented conversation task (Williams and Young, 2007); examples include troubleshooting (Vinyals and Le, 2015), navigation (Wen et al., 2015), interviewing (Kobori et al., 2016), and restaurant search (Wen et al., 2017). In what follows, we introduce non-task-oriented conversational models that explicitly consider conversational situations. Hasegawa et al. (2013) presented a conversational model that generates a response so that it elicits a certain emotion (e.g., joy) in the addressee mind. Their model is based on statistical machine translation and linearly interpolates two conversational models that are trained from a small emotion-labeled dialogue corpus and a large nonlabeled dialogue corpus, respectively. This model is similar to our local-global S EQ 2S EQ but"
P17-3020,P15-1152,0,0.0715997,"Missing"
P17-3020,N16-1014,0,0.611141,"oriented dialogue systems (Williams and Young, 2007), the response of which has a low degree of freedom thanks to domain and goal specificity. Although a few studies have tried to exploit conversational situations such as speakers’ emoIntroduction The increasing amount of dialogue data in social media has opened the door to data-driven modeling of non-task-oriented, or chat, dialogues (Ritter et al., 2011). The data-driven models assume a response generation as a sequence to sequence mapping task, and recent ones are based on neural S EQ 2S EQ models (Vinyals and Le, 2015; Shang et al., 2015; Li et al., 2016a,b; Xing et al., 2017). However, the adequacy of responses generated by these neural models is somewhat insufficient, in contrast to the acknowledged success of the neural S EQ 2S EQ models in machine translation (Johnson et al., 2016). The contrasting outcomes in machine translation and chat dialogue modeling can be explained 120 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics- Student Research Workshop, pages 120–127 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-3020 are r"
P17-3020,N15-1020,0,0.159635,"Missing"
P17-3020,P16-1094,0,0.142501,"Missing"
P17-3020,D16-1230,0,0.0385558,"Missing"
P17-3020,D15-1199,0,0.0607988,"Missing"
P17-3020,E17-1042,0,0.0241183,"Missing"
P17-3020,C16-1063,0,0.011898,"without past dialogue exchanges are chosen for evaluation). For each utterance-response pair in the test dataset, we randomly chose four (in total, 24,000) responses in 2015 as false response Evaluation procedure We use the above models to rank response candidates for a given utterance in the test set. We compute the averaged cross-entropy loss for words in each response candidate (namely, its perplexity) by giving the candidate following the input utterance to each conversational model, and used the resulting values for ranking candidates to choose top-k plausible ones. We adopt 1 in t P@k (Wu et al., 2016) as the evaluation metric, which indicates the ratio of utterances that are provided the single ground truth in top k responses chosen from t candidates. Here we use 1 in 2 P@1,9 1 in 5 P@1, and 1 in 5 P@2. 5 http://taku910.github.io/mecab/ https://github.com/neologd/ mecab-ipadic-neologd 7 The number of words in the utterances and the response candidates in the test set is limited to equal or less than 20, since very long posts do not constitute usual conversation. 8 https://www.tensorflow.org/ 9 We randomly selected one false response candidate from the four pre-selected ones when t = 2. 6 4"
P17-3020,P17-1061,0,0.0227514,"is too warm to wear a hoodie.) Baseline そうなんです! (Yes!) S EQ 2S EQ embまだ着てたの!? (Do you still wear one?) speaker ID to their model and represent individual (known) speakers with embeddings, Their model cannot handle unknown speakers. In contrast, our model can consider any speakers with profiles because we represent each cluster of profiles with an embedding and find an appropriate profile type for the given profile by nearest-neighbor search. Sordoni et al. (2015) encoded a given utterance and the past dialogue exchanges, and combined the resulting representations for RNN to decode a response. Zhao et al. (2017) used a conditional variational autoencoder and automaticallyinduced dialogue acts to handle discourse-level diversity in the encoder. While these sophisticated architectures are designed to take dialogue histories into consideration, our simple models can easily exploit various situations. Recently, Xing et al. (2017) proposed to explicitly consider topics of utterances to generate topiccoherent responses. Although they used latent Dirichlet allocation while we use k-means clustering, both methods confirmed the importance of utterance situations. The way to obtain specific situations is still"
sumida-etal-2008-boosting,D07-1073,1,\N,Missing
sumida-etal-2008-boosting,C92-2082,0,\N,Missing
sumida-etal-2008-boosting,P99-1016,0,\N,Missing
sumida-etal-2008-boosting,W02-1111,0,\N,Missing
sumida-etal-2008-boosting,P06-1015,0,\N,Missing
sumida-etal-2008-boosting,I08-2126,1,\N,Missing
sumida-etal-2008-boosting,W04-3221,0,\N,Missing
sumida-etal-2008-boosting,N04-1010,1,\N,Missing
sumida-etal-2008-boosting,P03-1001,0,\N,Missing
W01-1510,W00-2006,0,0.022452,"to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages. Section 2 reviews the source and the target grammar formalisms of the conversion algorithm. Section 3 describes the conversion algorithm which the core module in the RenTAL system uses. Section"
W01-1510,2000.iwpt-1.9,0,0.035923,"Missing"
W01-1510,P00-1058,0,0.0138769,"r (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2: Elementary trees Derivation trees Derivation translator Parse trees F"
W01-1510,C00-1060,1,0.921494,"nd Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2: Elementary trees Derivation trees Derivation translator Parse trees Figure 1: The RenTAL Syst"
W01-1510,W98-0134,0,0.0195913,"matically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly equivalent grammars enable the sharing of ideas developed in each formalism. There have been many studies on parsing techniques (Poller and Becker, 1998; Flickinger et al., 2000), ones on disambiguation models (Chiang, 2000; Kanayama et al., 2000), and ones on programming/grammar-development environ1 In this paper, we use the term LTAG to refer to FBLTAG, if not confusing. LTAG-based application RenTAL System HPSG-based application LTAG Resources Tree converter HPSG Resources Grammar: Elementary tree templates Lexicon Type hierarchy extractor Lexicon converter LTAG parsers anchor * Grammar: Lexical entry templates Initial tree foot node Auxiliary tree α2 S substitution node α1 NP Lexicon VP VP V N V can We run NP β1 VP * HPSG parsers Figure 2"
W01-1510,W00-1605,0,0.0466259,"Missing"
W01-1510,C88-2121,0,0.469209,"Missing"
W01-1510,W98-0141,1,0.83789,"rarchy extractor module extracts the symbols of the node, features, and feature values from the LTAG elementary tree templates and lexicon, and construct the type hierarchy from them. The lexicon converter module converts LTAG elementary tree templates into HPSG lexical entries. The derivation translator module takes HPSG parse trees, and map them to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions"
W01-1510,P95-1013,0,0.83404,"trees, and map them to LTAG derivation trees. All modules other than the last one are related to the conversion process from LTAG into HPSG, and the last one enables to obtain LTAG analysis from the obtained HPSG analysis. Tateisi et al. also translated LTAG into HPSG (Tateisi et al., 1998). However, their method depended on translator’s intuitive analysis of the original grammar. Thus the translation was manual and grammar dependent. The manual translation demanded considerable efforts from the translator, and obscures the equivalence between the original and obtained grammars. Other works (Kasper et al., 1995; Becker and Lopez, 2000) convert HPSG grammars into LTAG grammars. However, given the greater expressive power of HPSG, it is impossible to convert an arbitrary HPSG grammar into an LTAG grammar. Therefore, a conversion from HPSG into LTAG often requires some restrictions on the HPSG grammar to suppress its generative capacity. Thus, the conversion loses the equivalence of the grammars, and we cannot gain the above advantages. Section 2 reviews the source and the target grammar formalisms of the conversion algorithm. Section 3 describes the conversion algorithm which the core module in the Re"
W01-1510,P98-2144,1,0.85278,"ure 6 shows a rule application to “can run” and “we”. There are a variety of works on efficient parsing with HPSG, which allow the use of HPSGbased processing in practical application contexts (Flickinger et al., 2000). Stanford University is developing the English Resource Grammar, an HPSG grammar for English, as a part of the Linguistic Grammars Online (LinGO) project (Flickinger, 2000). In practical context, German, English, and Japanese HPSG-based grammars are developed and used in the Verbmobil project (Kay et al., 1994). Our group has developed a wide-coverage HPSG grammar for Japanese (Mitsuishi et al., 1998), which is used in a high-accuracy Japanese dependency analyzer (Kanayama et al., 2000). S anchor * foot node substitution node trunk NP VP V S* think think: Sym: V Sym : VP Arg: Leaf : S Dir : right Foot?: + , Sym : S Leaf : NP Dir : left Foot?: _ Figure 8: A conversion from a canonical elementary tree into an HPSG lexical entry  h mother Sym : Arg :  1 i 2 X*X2XXX 3 4 Arg : 4 5j Sym : 3 Arg : h i substitution node 2 Sym : 1 Leaf : 3 Dir : lef t Foot? : 2 + 3 5 trunk node Figure 9: Left substitution rule 3 Grammar conversion The grammar conversion from LTAG to HPSG (Yoshinaga and Miyao"
W01-1510,W98-0131,0,0.0541757,"Missing"
W01-1510,C88-2147,0,0.0507767,"rammar conversion from an FB-LTAG grammar to a strongly equivalent HPSG-style grammar. The system is applied to the latest version of the XTAG English grammar. Experimental results show that the obtained HPSG-style grammar successfully worked with an HPSG parser, and achieved a drastic speed-up against an LTAG parser. This system enables to share not only grammars and lexicons but also parsing techniques. 1 Introduction This paper describes an approach for sharing resources in various grammar formalisms such as Feature-Based Lexicalized Tree Adjoining Grammar (FB-LTAG1 ) (Vijay-Shanker, 1987; Vijay-Shanker and Joshi, 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) by a method of grammar conversion. The RenTAL system automatically converts an FB-LTAG grammar into a strongly equivalent HPSG-style grammar (Yoshinaga and Miyao, 2001). Strong equivalence means that both grammars generate exactly equivalent parse results, and that we can share the LTAG grammars and lexicons in HPSG applications. Our system can reduce considerable workload to develop a huge resource (grammars and lexicons) from scratch. Our concern is, however, not limited to the sharing of grammars and lexicons. Strongly"
W01-1510,J93-2004,0,\N,Missing
W01-1510,C98-2139,1,\N,Missing
W01-1510,C98-2128,1,\N,Missing
W01-1510,P98-2132,1,\N,Missing
W02-2227,P95-1013,0,0.0217347,"nature of strong equivalence (Yoshinaga et al., 2001b; Yoshinaga et al., 2001a), applications which contribute much to the developments of the two formalisms. In the past decades, LTAG and HPSG have received considerable attention as approaches to the formalization of natural languages in the field of computational linguistics. Discussion of the correspondences between the two formalisms has accompanied their development; that is, their linguistic relationships and differences have been investigated (Abeill´e, 1993; Kasper, 1998), as has conversion between two grammars in the two formalisms (Kasper et al., 1995; Tateisi et al., 1998; Becker and Lopez, 2000). These ongoing efforts have contributed greatly to the development of the two formalisms. Following this direction, in our earlier work (Yoshinaga and Miyao, 2001), we provided a method for converting grammars from LTAG to HPSG-style, which is the notion that we defined according to the computational device that underlies HPSG. We used the grammar conversion to obtain an HPSG-style grammar from LTAG (The XTAG Research Group, 2001), and then empirically showed strong equivalence between the LTAG and the obtained HPSG-style grammar for the sentence"
W02-2227,W00-2027,0,0.0223548,"rcinkiewicz, 1994). We exploited the nature of strong equivalence between the LTAG and the HPSG-style grammars to provide some applications such as sharing of existing resources between the two grammar formalisms (Yoshinaga et al., 2001b), a comparison of performance between parsers based on the two different formalisms (Yoshinaga et al., 2001a), and linguistic correspondence between the HPSG-style grammar and HPSG. As the most important result for the LTAG community, through the experiments of parsing within the above sentences, we showed that the empirical time complexity of an LTAG parser (Sarkar, 2000) is higher than that of an HPSG parser (Torisawa et al., 2000). This result is contrary to the general expectations from the viewpoint of the theoretical bound of worst time complexity, which is worth exploring further. However, the lack of the formal proof of strong equivalence restricts scope of the applications of our grammar conversion to grammars which are empirically attested the strong equivalence, and this prevents the applications from maximizing their true potential. In this paper we give a formal proof of strong equivalence between any LTAG and an HPSG-style grammar converted from"
W02-2227,C88-2121,0,0.21412,"Missing"
W02-2227,J95-4002,0,0.0747168,"Missing"
W02-2227,W01-1510,1,0.881133,"Missing"
W02-2227,J93-2004,0,\N,Missing
W02-2227,W98-0141,1,\N,Missing
W04-3314,J87-3002,0,0.256158,"Missing"
W04-3314,P02-1029,0,0.0307799,"e attempt to compensate for the lack of recall by being less strict in filtering out less likely SCFs, then we will end up with a larger number of lexical entries. This is fatal for parsing Jun’ichi Tsujii†‡ ‡ CREST, JST 4-1-8, Honcho, Kawaguchi-shi, Saitama, 332-0012 Japan tsujii@is.s.u-tokyo.ac.jp with lexicalized grammars, because empirical parsing efficiency and syntactic ambiguity of lexicalized grammars are known to be proportional to the number of lexical entries used in parsing (Sarkar et al., 2000). We therefore need some method to improve the quality of the acquired SCFs. Schulte im Walde and Brew (2002) and Korhonen (2003) employed clustering of verb SCF (probability) distributions to induce verb semantic classes. Their studies are based on the assumption that verb SCF distributions are closely related to verb semantic classes. Conversely, if we could induce word classes whose element words have the same set of SCFs, we can eliminate SCFs acquired in error from the corpora and predict plausible SCFs unseen in the corpora. This kind of generalization would be useful to improve the quality of the acquired SCFs. In this paper, we present a method of generalizing SCFs acquired from corpora in or"
W04-3314,A97-1052,0,0.163114,"f the acquired SCFs by clustering vectors obtained from the acquired SCF lexicon and the lexicon of the target grammar. We apply our method to SCFs acquired from corpora by using a subset of the SCF lexicon of the XTAG English grammar. A comparison between the resulting SCF lexicon and the rest of the lexicon of the XTAG English grammar reveals that we can achieve higher precision and recall compared to naive frequency cut-off. 1 Introduction Recently, a variety of methods have been proposed for automatic acquisition of subcategorization frames (SCFs) from corpora (Brent, 1993; Manning, 1993; Briscoe and Carroll, 1997; Sarkar and Zeman, 2000; Korhonen, 2002). Although these research efforts aimed at enhancing lexicon resources, there has been little work on evaluating the impact of acquired SCFs on grammar coverage using large-scale lexicalized grammars with the exception of (Carroll and Fang, 2004). The problem when we combine acquired SCFs with existing lexicalized grammars is lower quality of the acquired SCFs, since they are acquired in an unsupervised manner, rather than being manually coded. If we attempt to compensate for the lack of recall by being less strict in filtering out less likely SCFs, the"
W04-3314,C94-1042,0,0.101034,"Missing"
W04-3314,P03-1009,0,0.0307254,"Missing"
W04-3314,C00-2100,0,0.0194804,"tering vectors obtained from the acquired SCF lexicon and the lexicon of the target grammar. We apply our method to SCFs acquired from corpora by using a subset of the SCF lexicon of the XTAG English grammar. A comparison between the resulting SCF lexicon and the rest of the lexicon of the XTAG English grammar reveals that we can achieve higher precision and recall compared to naive frequency cut-off. 1 Introduction Recently, a variety of methods have been proposed for automatic acquisition of subcategorization frames (SCFs) from corpora (Brent, 1993; Manning, 1993; Briscoe and Carroll, 1997; Sarkar and Zeman, 2000; Korhonen, 2002). Although these research efforts aimed at enhancing lexicon resources, there has been little work on evaluating the impact of acquired SCFs on grammar coverage using large-scale lexicalized grammars with the exception of (Carroll and Fang, 2004). The problem when we combine acquired SCFs with existing lexicalized grammars is lower quality of the acquired SCFs, since they are acquired in an unsupervised manner, rather than being manually coded. If we attempt to compensate for the lack of recall by being less strict in filtering out less likely SCFs, then we will end up with a"
W04-3314,W00-1605,0,0.0176536,"the acquired SCFs, since they are acquired in an unsupervised manner, rather than being manually coded. If we attempt to compensate for the lack of recall by being less strict in filtering out less likely SCFs, then we will end up with a larger number of lexical entries. This is fatal for parsing Jun’ichi Tsujii†‡ ‡ CREST, JST 4-1-8, Honcho, Kawaguchi-shi, Saitama, 332-0012 Japan tsujii@is.s.u-tokyo.ac.jp with lexicalized grammars, because empirical parsing efficiency and syntactic ambiguity of lexicalized grammars are known to be proportional to the number of lexical entries used in parsing (Sarkar et al., 2000). We therefore need some method to improve the quality of the acquired SCFs. Schulte im Walde and Brew (2002) and Korhonen (2003) employed clustering of verb SCF (probability) distributions to induce verb semantic classes. Their studies are based on the assumption that verb SCF distributions are closely related to verb semantic classes. Conversely, if we could induce word classes whose element words have the same set of SCFs, we can eliminate SCFs acquired in error from the corpora and predict plausible SCFs unseen in the corpora. This kind of generalization would be useful to improve the qual"
W04-3314,C88-2121,0,0.0788923,"Missing"
W04-3314,P93-1032,0,\N,Missing
W04-3323,W98-0108,0,0.0297342,"mmar and LTAG grammars which are extracted from the Penn Treebank, and investigated characteristics of the obtained CFGs. We perform CFG filtering for LTAG by the obtained CFG. In the experiments, we describe that the obtained CFG is useful for CFG filtering for LTAG parser. 1 Introduction Recently, lexicalized grammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) have attracted much attention in practical application context (Deep Thought Project, 2003; Kototoi Project, 2001; Kay et al., 1994; Carroll et al., 1998). However, inefficiency of parsing with those grammars have prevented us from adopting them for practical usage. Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG g"
W04-3323,P90-1036,0,0.209058,"s foot node Input “I can run” run correct parse trees parsing by the lexicalized grammar parsing by CFG N I Figure 2: CFG filtering Figure 1: LTAG: elementary trees, substitution and adjunction S “NP” of α1 is replaced by α2 , which has a root node labeled “NP.” Adjunction replaces an internal node of an elementary tree by another elementary tree whose root node and one leaf node called a foot node have the same label as the internal node. In Figure 1, the internal node labeled “VP” of α1 is replaced by β 2 , which has a root node and a foot node labeled “VP.” 2.2 CFG filtering CFG filtering (Harbusch, 1990; Maxwell III and Kaplan, 1993; Torisawa and Tsujii, 1996) is a parsing scheme that filters out impossible parse trees using a CFG extracted from a given grammar prior to parsing. In CFG filtering, we first perform an off-line extraction of a CFG from a given grammar, (Context-free (CF) approximation). By using the obtained CFG we can compute efficiently the necessary condition for parse trees the original grammar could generate. Parsing using the obtained CFG as a filter comprises two phases (Figure 2). In the first phase, we parse a sentence by the obtained CFG. In this phase, the necessary"
W04-3323,2000.iwpt-1.15,0,0.281558,"Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they found that an HPSG parser with CFG filtering (Torisawa et al., 2000) outperformed a theoretically efficient LTAG parser (Sarkar, 2000) in terms of empirical time complexity. Although their approach does not guarantee the theoretical bound of parsing complexity, O(n 6 ) for a sentence of length n, the empirical results of their CFG filtering are still satisfactory. In this paper, we propose a novel context-free approximation method for LTAG by reinterpreting the method by Yoshinaga et al"
W04-3323,J93-4001,0,0.0275722,"Missing"
W04-3323,E03-1047,1,0.804804,"by Yoshinaga et al. in the context of LTAG parsing. A fundamental idea is to enumerate partial parse results that can be generated during parsing. We assign CFG nonterminal labels to the partial parse results, and then regard their possible combinations as CFG rules. In order to investigate the characteristics of CFGs produced by our method, we applied our method to two kinds of LTAG grammars. One is the XTAG English grammar, which is a large-scale hand-crafted LTAG, and the other is LTAG grammars extracted from Penn Treebank Wall Street Journal by the grammar extraction method described in (Miyao et al., 2003). Then, we compare parsing speed of a CKY parser using the obtained CFG with parsing speed of an existing LTAG parser. The remainder of the paper is organized as follows. Section 2 introduces background of our research. Section 3 describes our approximation method. Section 4 reports experimental results with the two kinds of LTAG grammars. 2 Background 2.1 Lexicalized Tree-Adjoining Grammar (LTAG) An LTAG consists of a set of tree structures, which are assigned to words, called elementary trees. A parse tree is derived by combining elementary trees using two grammar rules called substitution a"
W04-3323,P98-2156,0,0.0242972,"rammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) have attracted much attention in practical application context (Deep Thought Project, 2003; Kototoi Project, 2001; Kay et al., 1994; Carroll et al., 1998). However, inefficiency of parsing with those grammars have prevented us from adopting them for practical usage. Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they found that an HPSG parser with CFG filtering (Torisawa et al.,"
W04-3323,W98-0134,0,0.0229444,"enerated parse trees, and eliminate overgenerated parse trees. The performance of parsers with CFG filtering depends on the degree of the CF approximation (Yoshinaga et al., 2003). If CF approximation is good, the number of overgenerated parse trees is small. Thus, the key to achieve efficiency in LTAG parsing is to maintain grammatical restrictions in CFG as efficiently as possible. The more of the grammatical constraints in the given grammar the obtained CFG captures, the more effectively we can restrict the search space. There are existing CFG filtering techniques for LTAG (Harbusch, 1990; Poller and Becker, 1998). These techniques extract CFG rules by simply dividing elementary trees into branching structures as shown in Figure 3. Since the obtained CFG can capture only local constraints CFG rules VP NP V S NP VP NP VP V NP Figure 3: The existing CF approximation for LTAG given in the elementary trees, we must examine many global constraints in the second phase. CFG filtering techniques have also been developed for HPSG (Torisawa and Tsujii, 1996; Torisawa et al., 2000; Kiefer and Krieger, 2000). CFG rules are extracted by applying grammar rules to lexical entries and by enumerating partial parse resu"
W04-3323,W00-2027,0,0.0176303,"ciently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they found that an HPSG parser with CFG filtering (Torisawa et al., 2000) outperformed a theoretically efficient LTAG parser (Sarkar, 2000) in terms of empirical time complexity. Although their approach does not guarantee the theoretical bound of parsing complexity, O(n 6 ) for a sentence of length n, the empirical results of their CFG filtering are still satisfactory. In this paper, we propose a novel context-free approximation method for LTAG by reinterpreting the method by Yoshinaga et al. in the context of LTAG parsing. A fundamental idea is to enumerate partial parse results that can be generated during parsing. We assign CFG nonterminal labels to the partial parse results, and then regard their possible combinations as CFG"
W04-3323,P88-1032,0,0.0836776,"er. 1 Introduction Recently, lexicalized grammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) have attracted much attention in practical application context (Deep Thought Project, 2003; Kototoi Project, 2001; Kay et al., 1994; Carroll et al., 1998). However, inefficiency of parsing with those grammars have prevented us from adopting them for practical usage. Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they found that an HPSG parser wi"
W04-3323,C88-2121,0,0.079462,"Missing"
W04-3323,P03-2036,1,0.829607,"cation context (Deep Thought Project, 2003; Kototoi Project, 2001; Kay et al., 1994; Carroll et al., 1998). However, inefficiency of parsing with those grammars have prevented us from adopting them for practical usage. Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they found that an HPSG parser with CFG filtering (Torisawa et al., 2000) outperformed a theoretically efficient LTAG parser (Sarkar, 2000) in terms of empirical time complexity. Although their approach does not guarantee the theoretical bound of parsing complexity, O(n 6 ) f"
W04-3323,C96-2160,1,0.905031,"e trees parsing by the lexicalized grammar parsing by CFG N I Figure 2: CFG filtering Figure 1: LTAG: elementary trees, substitution and adjunction S “NP” of α1 is replaced by α2 , which has a root node labeled “NP.” Adjunction replaces an internal node of an elementary tree by another elementary tree whose root node and one leaf node called a foot node have the same label as the internal node. In Figure 1, the internal node labeled “VP” of α1 is replaced by β 2 , which has a root node and a foot node labeled “VP.” 2.2 CFG filtering CFG filtering (Harbusch, 1990; Maxwell III and Kaplan, 1993; Torisawa and Tsujii, 1996) is a parsing scheme that filters out impossible parse trees using a CFG extracted from a given grammar prior to parsing. In CFG filtering, we first perform an off-line extraction of a CFG from a given grammar, (Context-free (CF) approximation). By using the obtained CFG we can compute efficiently the necessary condition for parse trees the original grammar could generate. Parsing using the obtained CFG as a filter comprises two phases (Figure 2). In the first phase, we parse a sentence by the obtained CFG. In this phase, the necessary condition represented by the CFG acts as a filter of parse"
W04-3323,P85-1011,0,0.130692,"for CFG filtering for LTAG parser. 1 Introduction Recently, lexicalized grammars such as Lexicalized Tree Adjoining Grammar (LTAG) (Schabes et al., 1988) and Head-Driven Phrase Structure Grammar (HPSG) (Pollard and Sag, 1994) have attracted much attention in practical application context (Deep Thought Project, 2003; Kototoi Project, 2001; Kay et al., 1994; Carroll et al., 1998). However, inefficiency of parsing with those grammars have prevented us from adopting them for practical usage. Especially in the LTAG framework, although many studies proposed parsers that are theoretically efficient (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; van Noord, 1994; Nederhof, 1998), we do not attain any practical LTAG parser that runs efficiently with large-scale hand-crafted grammars such as the XTAG English grammar (XTAG Research Group, 2001). Yoshinaga et al. (Yoshinaga et al., 2003) demonstrated that a drastic speed-up of LTAG parsing can be achieved when a LTAG grammar is compiled into a HPSG (Yoshinaga and Miyao, 2002) and a CFG filtering technique for HPSG-Style grammar (Kiefer and Krieger, 2000; Torisawa et al., 2000) is applied to the obtained HPSG. In experiments with the XTAG English grammar, they fou"
W04-3323,H86-1020,0,\N,Missing
W04-3323,C98-2151,0,\N,Missing
W08-2322,2000.iwpt-1.9,0,0.557306,"nn Treebank. Experimental results show that probabilistic SCF lexicons obtained by our model achieved a lower test-set perplexity against ones obtained by a naive smoothing model using twice as large training data. 1 Introduction This paper proposes a smoothing model for probabilistic subcategorization (SCF) lexicons of lexicalized grammars acquired from corpora. Here, an SCF lexicon consists of pairs of words and lexical (SCF) types (e.g, tree family), from which individual lexical entry templates are derived by lexical rules (Jackendoff, 1975; Pollard and Sag, 1994) (e.g., metarules: Becker (2000) and Prolo (2002)).1 Recently, the corpus-oriented approaches have enabled us to acquire widecoverage lexicalized grammars from large treebanks (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000; Hockenmaier and Steedman, 2002; 1 In the linguistic literature, the term ‘lexical rules’ is used to define either syntactic transformations (e.g., whmovement), diathesis alternations (e.g., dative shift) or both. In this paper, we use the term lexical rules to define syntactic transformations among lexical entry templates that belong to the same lexical type. Cahill et al., 2002; Frank et al., 200"
W08-2322,P00-1058,0,0.483878,"s large training data. 1 Introduction This paper proposes a smoothing model for probabilistic subcategorization (SCF) lexicons of lexicalized grammars acquired from corpora. Here, an SCF lexicon consists of pairs of words and lexical (SCF) types (e.g, tree family), from which individual lexical entry templates are derived by lexical rules (Jackendoff, 1975; Pollard and Sag, 1994) (e.g., metarules: Becker (2000) and Prolo (2002)).1 Recently, the corpus-oriented approaches have enabled us to acquire widecoverage lexicalized grammars from large treebanks (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000; Hockenmaier and Steedman, 2002; 1 In the linguistic literature, the term ‘lexical rules’ is used to define either syntactic transformations (e.g., whmovement), diathesis alternations (e.g., dative shift) or both. In this paper, we use the term lexical rules to define syntactic transformations among lexical entry templates that belong to the same lexical type. Cahill et al., 2002; Frank et al., 2003; Miyao et al., 2005). However, a great workload is required to develop such large treebanks for languages or domains where a base bracketed corpus (e.g., the Penn Treebank: Marcus et al. (1993)) i"
W08-2322,W02-2232,0,0.0179959,"ical entry templates using manually defined lexical rules. et al. (2006) converted the lexical entry templates into linguistically-motivated feature vectors, and Nakanishi et al. (2004) manually defined lexical rules. These methods, however, just translate the problem of unseen word-template pairs into the problem of unseen word-type pairs, and does not predict any unseen word-type pairs. We will hereafter refer to four types of unseen word-type pairs by (sw, sT), (sw, uT), (uw, sT), and (uw, uT) where sT/uT stand for seen/unseen lexical types. Another type of the approaches has been taken by Hara et al. (2002) and Chen et al. (2006) to predict unseen (sw, st) pairs. Hara et al. (2002) conducted a hard clustering (Forgy, 1965) of words according to their lexical entry templates in order to find classes of words that take the same lexical entry templates. It will be difficult for the hard clustering method to appropriately classify polysemic verbs, which take several lexical types. Chen et al. (2006) performed a clustering of lexical entry templates according to words that take those templates in order to find lexical entry templates that belong to the same tree family. They reported that it was diff"
W08-2322,hockenmaier-steedman-2002-acquiring,0,0.345961,"rimental results show that probabilistic SCF lexicons obtained by our model achieved a lower test-set perplexity against ones obtained by a naive smoothing model using twice as large training data. 1 Introduction This paper proposes a smoothing model for probabilistic subcategorization (SCF) lexicons of lexicalized grammars acquired from corpora. Here, an SCF lexicon consists of pairs of words and lexical (SCF) types (e.g, tree family), from which individual lexical entry templates are derived by lexical rules (Jackendoff, 1975; Pollard and Sag, 1994) (e.g., metarules: Becker (2000) and Prolo (2002)).1 Recently, the corpus-oriented approaches have enabled us to acquire widecoverage lexicalized grammars from large treebanks (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000; Hockenmaier and Steedman, 2002; 1 In the linguistic literature, the term ‘lexical rules’ is used to define either syntactic transformations (e.g., whmovement), diathesis alternations (e.g., dative shift) or both. In this paper, we use the term lexical rules to define syntactic transformations among lexical entry templates that belong to the same lexical type. Cahill et al., 2002; Frank et al., 2003; Miyao et al.,"
W08-2322,W04-2606,0,0.0629522,"Missing"
W08-2322,P03-1009,0,0.0281532,"3 PLSA -based Probabilistic SCF Lexicon This section first applies the probabilistic latent semantic analysis (PLSA: Hofmann (2001)) to cooccurrences between verbs and SCFs, and then describes a PLSA-based smoothing model to estimate the co-occurrence probabilities. 3.1 to model co-occurrences between verbs and SCF types PLSA We employ the probabilistic latent semantic analysis to model co-occurrences between words and SCF types, where the latent variables are classes whose members have the same SCF distribution. Our modeling is inspired by the studies by Schulte im Walde and Brew (2002) and Korhonen et al. (2003), which demonstrated that a semantic classification of verbs can be obtained by clustering verbs according to their SCF distributions.3 The PLSA is suitable for this task since it performs a kind of soft clustering, which can naturally handle highly polysemic nature of verbs. We assume that a lexicon of a lexicalized grammar is acquired from a source treebank. Let the conditional probability that a word w ∈ W appears as a member of a latent class c ∈ C be p(c|w), and each latent class c ∈ C takes an SCF s ∈ S with a conditional probability p(s|c). Here, W and S are a set of words and lexical t"
W08-2322,J93-2004,0,0.0291496,"ker, 2000; Chiang, 2000; Hockenmaier and Steedman, 2002; 1 In the linguistic literature, the term ‘lexical rules’ is used to define either syntactic transformations (e.g., whmovement), diathesis alternations (e.g., dative shift) or both. In this paper, we use the term lexical rules to define syntactic transformations among lexical entry templates that belong to the same lexical type. Cahill et al., 2002; Frank et al., 2003; Miyao et al., 2005). However, a great workload is required to develop such large treebanks for languages or domains where a base bracketed corpus (e.g., the Penn Treebank: Marcus et al. (1993)) is not available. When the size of the source treebank is small, we encounter the serious problem of a lack of lexical entries (unseen word-template pairs). Previous studies investigated unseen wordtemplate pairs in lexicalized grammars acquired from the Penn Treebank (Xia, 1999; Chen and Vijay-Shanker, 2000; Hockenmaier and Steedman, 2002; Miyao et al., 2005); the words can be seen (sw) or unseen (uw), and similarly, the templates can be seen (st) or unseen (ut), so that there are four types of unseen pairs. All the studies reported that unseen (sw, st) pairs caused the major problem in lex"
W08-2322,C02-1153,0,0.0318565,". Experimental results show that probabilistic SCF lexicons obtained by our model achieved a lower test-set perplexity against ones obtained by a naive smoothing model using twice as large training data. 1 Introduction This paper proposes a smoothing model for probabilistic subcategorization (SCF) lexicons of lexicalized grammars acquired from corpora. Here, an SCF lexicon consists of pairs of words and lexical (SCF) types (e.g, tree family), from which individual lexical entry templates are derived by lexical rules (Jackendoff, 1975; Pollard and Sag, 1994) (e.g., metarules: Becker (2000) and Prolo (2002)).1 Recently, the corpus-oriented approaches have enabled us to acquire widecoverage lexicalized grammars from large treebanks (Xia, 1999; Chen and Vijay-Shanker, 2000; Chiang, 2000; Hockenmaier and Steedman, 2002; 1 In the linguistic literature, the term ‘lexical rules’ is used to define either syntactic transformations (e.g., whmovement), diathesis alternations (e.g., dative shift) or both. In this paper, we use the term lexical rules to define syntactic transformations among lexical entry templates that belong to the same lexical type. Cahill et al., 2002; Frank et al., 2003; Miyao et al.,"
W08-2322,P02-1029,0,0.07269,"Missing"
W08-2322,P04-2008,1,0.906686,"05); the words can be seen (sw) or unseen (uw), and similarly, the templates can be seen (st) or unseen (ut), so that there are four types of unseen pairs. All the studies reported that unseen (sw, st) pairs caused the major problem in lexical coverage. This paper focuses on a verb SCF lexicon, and employs a latent variable model (Hofmann, 2001) to smooth co-occurrence probabilities between verbs and SCF types acquired from smallsized corpora. If we can obtain such an accurate probabilistic SCF lexicon, we can construct a widecoverage SCF lexicon by setting the threshold of the probabilities (Yoshinaga, 2004). Alternatively we can directly use the acquired probabilistic lexicon in supertagging (Chen et al., 2006) and probabilistic parsing (Miyao et al., 2005; Ninomiya et al., 2005). We applied our method to a verb SCF lexicon of an HPSG grammar acquired from the Penn Treebank (Miyao et al., 2005; Nakanishi et al., 2004). The acquired probabilistic SCF lexicons were more accurate than ones acquired by a naive smoothing model. Proceedings of The Ninth International Workshop on Tree Adjoining Grammars and Related Formalisms Tübingen, Germany. June 6-8, 2008. 166 2 Yoshinaga Related Work In this secti"
W08-2322,W05-1511,0,0.0244542,"s reported that unseen (sw, st) pairs caused the major problem in lexical coverage. This paper focuses on a verb SCF lexicon, and employs a latent variable model (Hofmann, 2001) to smooth co-occurrence probabilities between verbs and SCF types acquired from smallsized corpora. If we can obtain such an accurate probabilistic SCF lexicon, we can construct a widecoverage SCF lexicon by setting the threshold of the probabilities (Yoshinaga, 2004). Alternatively we can directly use the acquired probabilistic lexicon in supertagging (Chen et al., 2006) and probabilistic parsing (Miyao et al., 2005; Ninomiya et al., 2005). We applied our method to a verb SCF lexicon of an HPSG grammar acquired from the Penn Treebank (Miyao et al., 2005; Nakanishi et al., 2004). The acquired probabilistic SCF lexicons were more accurate than ones acquired by a naive smoothing model. Proceedings of The Ninth International Workshop on Tree Adjoining Grammars and Related Formalisms Tübingen, Germany. June 6-8, 2008. 166 2 Yoshinaga Related Work In this section, we first describe previous approaches to the problem of unseen word-template pairs in the lexicalized grammars acquired from treebanks. We then address smoothing methods fo"
W17-5708,D17-1151,0,0.023269,"which we have confirmed the effect. 2.1 2.2 Batch Size Batch size is the number of data points in a minibatch, which is a representative portion of the training data from which the gradient is calculated at each step in the stochastic gradient descent (SGD) optimizer (or its variants). In general, the batch size chosen for deep neural networks ranges from 32 to 512. It is known that a batch size that is too large leads to performance degradation in deep neural networks (Keskar et al., 2017). Recent studies in NMT have used values such as 64 (Rush et al., 2015) or 128 (Wu et al., 2016). While Britz et al. (2017) conducted a thorough investigation of hyperparameters in NMT, they fixed batch size to 128. The specific effect of batch size on NMT was studied by Morishita et al. (2017), who found that, for batch sizes of 8 to 64, a larger batch size has a positive impact on model performance. In this study, we seek to empirically clarify the point where increasing the batch size no longer improves NMT performance. Our work expands upon Morishita et al. (2017) and further investigates how NMT performance varies with larger batch sizes, up to 512. Pretraining Training deep neural networks with a relatively"
W17-5708,P17-1177,0,0.0267216,"e-art system of WAT 2016. Our code is available on https: //github.com/nem6ishi/wat17. 1 Introduction The advent of neural networks in machine translation has contributed greatly to the translation quality. Since proposed in (Cho et al., 2014; Sutskever et al., 2014), the sequence-to-sequence (S EQ 2S EQ) model has been achieving the stateof-the-art performance when combined with the attention mechanism (Bahdanau et al., 2015). Many studies have focused on modifying the S EQ 2S EQ network structure, including modifying the encoder (Eriguchi et al., 2016; Gehring et al., 2017; Li et al., 2017; Chen et al., 2017), or the decoder (Ishiwatari et al., 2017; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Wu et al., 2017). While these network structure modifications have been found to improve the translation quality, many systems, including the best system from WAT 2016 (Cromieres et al., 2016), still depend on the vanilla SEQ 2 SEQ model, the model with the attention mechanism. Denkowski and Neubig (2017) confirmed the large impact of common techniques such as training algorithms, subwords (Sennrich et al., 2016) and model ensem∗ Authors contributed equally. 99 Proceedings of the 4th Workshop on Asian"
W17-5708,P17-2021,0,0.0252427,"7. 1 Introduction The advent of neural networks in machine translation has contributed greatly to the translation quality. Since proposed in (Cho et al., 2014; Sutskever et al., 2014), the sequence-to-sequence (S EQ 2S EQ) model has been achieving the stateof-the-art performance when combined with the attention mechanism (Bahdanau et al., 2015). Many studies have focused on modifying the S EQ 2S EQ network structure, including modifying the encoder (Eriguchi et al., 2016; Gehring et al., 2017; Li et al., 2017; Chen et al., 2017), or the decoder (Ishiwatari et al., 2017; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Wu et al., 2017). While these network structure modifications have been found to improve the translation quality, many systems, including the best system from WAT 2016 (Cromieres et al., 2016), still depend on the vanilla SEQ 2 SEQ model, the model with the attention mechanism. Denkowski and Neubig (2017) confirmed the large impact of common techniques such as training algorithms, subwords (Sennrich et al., 2016) and model ensem∗ Authors contributed equally. 99 Proceedings of the 4th Workshop on Asian Translation, pages 99–109, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP 2 Related Work O"
W17-5708,W16-4616,0,0.0275238,"-to-sequence (S EQ 2S EQ) model has been achieving the stateof-the-art performance when combined with the attention mechanism (Bahdanau et al., 2015). Many studies have focused on modifying the S EQ 2S EQ network structure, including modifying the encoder (Eriguchi et al., 2016; Gehring et al., 2017; Li et al., 2017; Chen et al., 2017), or the decoder (Ishiwatari et al., 2017; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Wu et al., 2017). While these network structure modifications have been found to improve the translation quality, many systems, including the best system from WAT 2016 (Cromieres et al., 2016), still depend on the vanilla SEQ 2 SEQ model, the model with the attention mechanism. Denkowski and Neubig (2017) confirmed the large impact of common techniques such as training algorithms, subwords (Sennrich et al., 2016) and model ensem∗ Authors contributed equally. 99 Proceedings of the 4th Workshop on Asian Translation, pages 99–109, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP 2 Related Work Our work investigates the effect of initializing only the embedding layer using embeddings pretrained at low cost from the parallel corpus. We will later confirm that this initialization leads to"
W17-5708,Q17-1010,0,0.0446393,"h sizes of 32, 64, 128, and 512, and this effect was observed across all batch sizes. The results indicate that embedding layer initialization works in our NMT model, even though the embeddings are generated by CBOW, which is a totally task-irrelevant method. Since we confirmed the effectiveness of our embedding layer initialization, we then investigate the effect of different embedding methods on translation performance. There are various methods other than CBOW to create word embeddings. Mikolov et al. (2013) proposed Skip-gram. Pennington et al. (2014) proposed another method called GloVe. Bojanowski et al. (2017) proposed Subword Information Skip-gram (SI-Skip-gram) that utilizes morphological information by including character n-grams of words in the model. These methods train word embeddings using windows that obtain co-occurrences of neighboring words. It is known that a smaller window size leads to more syntactic embeddings and a larger one leads to more semantic embeddings (Lin and Wu, 2009; Levy and Goldberg, 2014). The question is: which embedding method and window size yield the best results for the translation task when used to initialize the embedding layer? To answer this question, we train"
W17-5708,W17-3203,0,0.281942,"ntion mechanism (Bahdanau et al., 2015). Many studies have focused on modifying the S EQ 2S EQ network structure, including modifying the encoder (Eriguchi et al., 2016; Gehring et al., 2017; Li et al., 2017; Chen et al., 2017), or the decoder (Ishiwatari et al., 2017; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Wu et al., 2017). While these network structure modifications have been found to improve the translation quality, many systems, including the best system from WAT 2016 (Cromieres et al., 2016), still depend on the vanilla SEQ 2 SEQ model, the model with the attention mechanism. Denkowski and Neubig (2017) confirmed the large impact of common techniques such as training algorithms, subwords (Sennrich et al., 2016) and model ensem∗ Authors contributed equally. 99 Proceedings of the 4th Workshop on Asian Translation, pages 99–109, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP 2 Related Work Our work investigates the effect of initializing only the embedding layer using embeddings pretrained at low cost from the parallel corpus. We will later confirm that this initialization leads to a BLEU score increase of 1.28 (§ 5.3.1). In this section, we will survey existing techniques used in NMT systems."
W17-5708,P16-1078,0,0.0225498,"tely, our system obtained a better result than the state-of-the-art system of WAT 2016. Our code is available on https: //github.com/nem6ishi/wat17. 1 Introduction The advent of neural networks in machine translation has contributed greatly to the translation quality. Since proposed in (Cho et al., 2014; Sutskever et al., 2014), the sequence-to-sequence (S EQ 2S EQ) model has been achieving the stateof-the-art performance when combined with the attention mechanism (Bahdanau et al., 2015). Many studies have focused on modifying the S EQ 2S EQ network structure, including modifying the encoder (Eriguchi et al., 2016; Gehring et al., 2017; Li et al., 2017; Chen et al., 2017), or the decoder (Ishiwatari et al., 2017; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Wu et al., 2017). While these network structure modifications have been found to improve the translation quality, many systems, including the best system from WAT 2016 (Cromieres et al., 2016), still depend on the vanilla SEQ 2 SEQ model, the model with the attention mechanism. Denkowski and Neubig (2017) confirmed the large impact of common techniques such as training algorithms, subwords (Sennrich et al., 2016) and model ensem∗ Authors contr"
W17-5708,P14-2050,0,0.0429985,"formance. There are various methods other than CBOW to create word embeddings. Mikolov et al. (2013) proposed Skip-gram. Pennington et al. (2014) proposed another method called GloVe. Bojanowski et al. (2017) proposed Subword Information Skip-gram (SI-Skip-gram) that utilizes morphological information by including character n-grams of words in the model. These methods train word embeddings using windows that obtain co-occurrences of neighboring words. It is known that a smaller window size leads to more syntactic embeddings and a larger one leads to more semantic embeddings (Lin and Wu, 2009; Levy and Goldberg, 2014). The question is: which embedding method and window size yield the best results for the translation task when used to initialize the embedding layer? To answer this question, we trained 13 models using CBOW, Skip-gram, Subword Information Skip-gram (SI-Skip-gram), and GloVe, with window sizes of 2, 5, and 10, as well as a window size of 15 with GloVe, as this was its default value. For implementations of CBOW and Skipeffect of batch size (§ 5.3.2). We then conduct experiments to discover the optimal learning rate when our initialization trick is employed (§ 5.3.3). Lastly, we examine the rela"
W17-5708,P17-2012,0,0.0242634,"ithub.com/nem6ishi/wat17. 1 Introduction The advent of neural networks in machine translation has contributed greatly to the translation quality. Since proposed in (Cho et al., 2014; Sutskever et al., 2014), the sequence-to-sequence (S EQ 2S EQ) model has been achieving the stateof-the-art performance when combined with the attention mechanism (Bahdanau et al., 2015). Many studies have focused on modifying the S EQ 2S EQ network structure, including modifying the encoder (Eriguchi et al., 2016; Gehring et al., 2017; Li et al., 2017; Chen et al., 2017), or the decoder (Ishiwatari et al., 2017; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Wu et al., 2017). While these network structure modifications have been found to improve the translation quality, many systems, including the best system from WAT 2016 (Cromieres et al., 2016), still depend on the vanilla SEQ 2 SEQ model, the model with the attention mechanism. Denkowski and Neubig (2017) confirmed the large impact of common techniques such as training algorithms, subwords (Sennrich et al., 2016) and model ensem∗ Authors contributed equally. 99 Proceedings of the 4th Workshop on Asian Translation, pages 99–109, c Taipei, Taiwan, November 27, 2017."
W17-5708,P17-1064,0,0.0201298,"n the state-of-the-art system of WAT 2016. Our code is available on https: //github.com/nem6ishi/wat17. 1 Introduction The advent of neural networks in machine translation has contributed greatly to the translation quality. Since proposed in (Cho et al., 2014; Sutskever et al., 2014), the sequence-to-sequence (S EQ 2S EQ) model has been achieving the stateof-the-art performance when combined with the attention mechanism (Bahdanau et al., 2015). Many studies have focused on modifying the S EQ 2S EQ network structure, including modifying the encoder (Eriguchi et al., 2016; Gehring et al., 2017; Li et al., 2017; Chen et al., 2017), or the decoder (Ishiwatari et al., 2017; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Wu et al., 2017). While these network structure modifications have been found to improve the translation quality, many systems, including the best system from WAT 2016 (Cromieres et al., 2016), still depend on the vanilla SEQ 2 SEQ model, the model with the attention mechanism. Denkowski and Neubig (2017) confirmed the large impact of common techniques such as training algorithms, subwords (Sennrich et al., 2016) and model ensem∗ Authors contributed equally. 99 Proceedings of the 4"
W17-5708,P17-1012,0,0.023119,"ed a better result than the state-of-the-art system of WAT 2016. Our code is available on https: //github.com/nem6ishi/wat17. 1 Introduction The advent of neural networks in machine translation has contributed greatly to the translation quality. Since proposed in (Cho et al., 2014; Sutskever et al., 2014), the sequence-to-sequence (S EQ 2S EQ) model has been achieving the stateof-the-art performance when combined with the attention mechanism (Bahdanau et al., 2015). Many studies have focused on modifying the S EQ 2S EQ network structure, including modifying the encoder (Eriguchi et al., 2016; Gehring et al., 2017; Li et al., 2017; Chen et al., 2017), or the decoder (Ishiwatari et al., 2017; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Wu et al., 2017). While these network structure modifications have been found to improve the translation quality, many systems, including the best system from WAT 2016 (Cromieres et al., 2016), still depend on the vanilla SEQ 2 SEQ model, the model with the attention mechanism. Denkowski and Neubig (2017) confirmed the large impact of common techniques such as training algorithms, subwords (Sennrich et al., 2016) and model ensem∗ Authors contributed equally. 99 Pro"
W17-5708,P09-1116,0,0.0384055,"on translation performance. There are various methods other than CBOW to create word embeddings. Mikolov et al. (2013) proposed Skip-gram. Pennington et al. (2014) proposed another method called GloVe. Bojanowski et al. (2017) proposed Subword Information Skip-gram (SI-Skip-gram) that utilizes morphological information by including character n-grams of words in the model. These methods train word embeddings using windows that obtain co-occurrences of neighboring words. It is known that a smaller window size leads to more syntactic embeddings and a larger one leads to more semantic embeddings (Lin and Wu, 2009; Levy and Goldberg, 2014). The question is: which embedding method and window size yield the best results for the translation task when used to initialize the embedding layer? To answer this question, we trained 13 models using CBOW, Skip-gram, Subword Information Skip-gram (SI-Skip-gram), and GloVe, with window sizes of 2, 5, and 10, as well as a window size of 15 with GloVe, as this was its default value. For implementations of CBOW and Skipeffect of batch size (§ 5.3.2). We then conduct experiments to discover the optimal learning rate when our initialization trick is employed (§ 5.3.3). L"
W17-5708,W17-3208,0,0.0921303,"rom which the gradient is calculated at each step in the stochastic gradient descent (SGD) optimizer (or its variants). In general, the batch size chosen for deep neural networks ranges from 32 to 512. It is known that a batch size that is too large leads to performance degradation in deep neural networks (Keskar et al., 2017). Recent studies in NMT have used values such as 64 (Rush et al., 2015) or 128 (Wu et al., 2016). While Britz et al. (2017) conducted a thorough investigation of hyperparameters in NMT, they fixed batch size to 128. The specific effect of batch size on NMT was studied by Morishita et al. (2017), who found that, for batch sizes of 8 to 64, a larger batch size has a positive impact on model performance. In this study, we seek to empirically clarify the point where increasing the batch size no longer improves NMT performance. Our work expands upon Morishita et al. (2017) and further investigates how NMT performance varies with larger batch sizes, up to 512. Pretraining Training deep neural networks with a relatively small amount of training data risks creating a model that performs poorly. One technique used to minimize this drawback is pretraining of the model (Hinton et al., 2006; Be"
W17-5708,P82-1020,0,0.782066,"Missing"
W17-5708,P17-1174,1,0.817192,"s available on https: //github.com/nem6ishi/wat17. 1 Introduction The advent of neural networks in machine translation has contributed greatly to the translation quality. Since proposed in (Cho et al., 2014; Sutskever et al., 2014), the sequence-to-sequence (S EQ 2S EQ) model has been achieving the stateof-the-art performance when combined with the attention mechanism (Bahdanau et al., 2015). Many studies have focused on modifying the S EQ 2S EQ network structure, including modifying the encoder (Eriguchi et al., 2016; Gehring et al., 2017; Li et al., 2017; Chen et al., 2017), or the decoder (Ishiwatari et al., 2017; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Wu et al., 2017). While these network structure modifications have been found to improve the translation quality, many systems, including the best system from WAT 2016 (Cromieres et al., 2016), still depend on the vanilla SEQ 2 SEQ model, the model with the attention mechanism. Denkowski and Neubig (2017) confirmed the large impact of common techniques such as training algorithms, subwords (Sennrich et al., 2016) and model ensem∗ Authors contributed equally. 99 Proceedings of the 4th Workshop on Asian Translation, pages 99–109, c Taipei, Tai"
W17-5708,D13-1176,0,0.0262574,"the baseline. The aforementioned studies, however, demand a large computational cost for pretraining a complex language model on large external data. Although Ramachandran et al. (2017) has provided a comparison of a system initialized using a language model trained only on the parallel corpus (in addition to their proposed method) to a baseline system without initialization, the translation performance did not improve but rather degraded with this setting. 3 The Vanilla SEQ 2 SEQ Model The S EQ 2S EQ (or encoder-decoder) model have been achieving the state-of-the-art in machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). Bahdanau et al. (2015) further improved this model by proposing the attention mechanism. This neural machine translation (NMT) approach involves an RNN-based encoder that converts the source sentence into vector representations which are then converted into the output sentence by an RNN-based decoder. While there are several variations in encoder implementation, including long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997), gated recurrent unit (GRU) (Cho et al., 2014), and convolutional neural network (CNN) (Gehring et al., 2017), our"
W17-5708,P11-2093,0,0.023069,"Missing"
W17-5708,D14-1162,0,0.0898783,"osed a method using a combination of the output probabilities of a language model trained on large monolingual corpora and a SEQ 2 SEQ NMT model, which are both trained separately. Venugopalan et al. (2016) studied different types of systems combined with a language model under the video description generation task and also introduced a method to initialize the embedding layer and the RNN layer of the decoder of the SEQ 2 SEQ based model with pretrained parameters of the language model. They additionally proposed a method to initialize the embedding layer of the decoder with pretrained GloVe (Pennington et al., 2014) embeddings. Ramachandran et al. (2017) initializes both the encoder and decoder of the SEQ 2 SEQ model with attention using language models trained on monolingual, unlabeled corpus of the source and target domains, respectively. This led to a significant improvement over the baseline. The aforementioned studies, however, demand a large computational cost for pretraining a complex language model on large external data. Although Ramachandran et al. (2017) has provided a comparison of a system initialized using a language model trained only on the parallel corpus (in addition to their proposed m"
W17-5708,D17-1039,0,0.0537573,"Missing"
W17-5708,D15-1044,0,0.0301805,"have proposed a new method, and then batch size, of which we have confirmed the effect. 2.1 2.2 Batch Size Batch size is the number of data points in a minibatch, which is a representative portion of the training data from which the gradient is calculated at each step in the stochastic gradient descent (SGD) optimizer (or its variants). In general, the batch size chosen for deep neural networks ranges from 32 to 512. It is known that a batch size that is too large leads to performance degradation in deep neural networks (Keskar et al., 2017). Recent studies in NMT have used values such as 64 (Rush et al., 2015) or 128 (Wu et al., 2016). While Britz et al. (2017) conducted a thorough investigation of hyperparameters in NMT, they fixed batch size to 128. The specific effect of batch size on NMT was studied by Morishita et al. (2017), who found that, for batch sizes of 8 to 64, a larger batch size has a positive impact on model performance. In this study, we seek to empirically clarify the point where increasing the batch size no longer improves NMT performance. Our work expands upon Morishita et al. (2017) and further investigates how NMT performance varies with larger batch sizes, up to 512. Pretrain"
W17-5708,P16-1162,0,0.0957835,"ncluding modifying the encoder (Eriguchi et al., 2016; Gehring et al., 2017; Li et al., 2017; Chen et al., 2017), or the decoder (Ishiwatari et al., 2017; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Wu et al., 2017). While these network structure modifications have been found to improve the translation quality, many systems, including the best system from WAT 2016 (Cromieres et al., 2016), still depend on the vanilla SEQ 2 SEQ model, the model with the attention mechanism. Denkowski and Neubig (2017) confirmed the large impact of common techniques such as training algorithms, subwords (Sennrich et al., 2016) and model ensem∗ Authors contributed equally. 99 Proceedings of the 4th Workshop on Asian Translation, pages 99–109, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP 2 Related Work Our work investigates the effect of initializing only the embedding layer using embeddings pretrained at low cost from the parallel corpus. We will later confirm that this initialization leads to a BLEU score increase of 1.28 (§ 5.3.1). In this section, we will survey existing techniques used in NMT systems. We first focus on pretraining, for which we have proposed a new method, and then batch size, of which we have"
W17-5708,D16-1204,0,0.0257855,"al., 2007), which initializes (part of) the parameters of the model using parameters of another model. Pretraining has led to promising results in NLP tasks using SEQ 2 SEQ models. In languages with a small amount of supervised data, it has been found that NMT results can be improved by transferring parameters from a high-resource language pair to a low-resource one (Zoph et al., 2016). G¨ulc¸ehre et al. (2015) proposed a method using a combination of the output probabilities of a language model trained on large monolingual corpora and a SEQ 2 SEQ NMT model, which are both trained separately. Venugopalan et al. (2016) studied different types of systems combined with a language model under the video description generation task and also introduced a method to initialize the embedding layer and the RNN layer of the decoder of the SEQ 2 SEQ based model with pretrained parameters of the language model. They additionally proposed a method to initialize the embedding layer of the decoder with pretrained GloVe (Pennington et al., 2014) embeddings. Ramachandran et al. (2017) initializes both the encoder and decoder of the SEQ 2 SEQ model with attention using language models trained on monolingual, unlabeled corpus"
W17-5708,P17-1065,0,0.171753,"of neural networks in machine translation has contributed greatly to the translation quality. Since proposed in (Cho et al., 2014; Sutskever et al., 2014), the sequence-to-sequence (S EQ 2S EQ) model has been achieving the stateof-the-art performance when combined with the attention mechanism (Bahdanau et al., 2015). Many studies have focused on modifying the S EQ 2S EQ network structure, including modifying the encoder (Eriguchi et al., 2016; Gehring et al., 2017; Li et al., 2017; Chen et al., 2017), or the decoder (Ishiwatari et al., 2017; Eriguchi et al., 2017; Aharoni and Goldberg, 2017; Wu et al., 2017). While these network structure modifications have been found to improve the translation quality, many systems, including the best system from WAT 2016 (Cromieres et al., 2016), still depend on the vanilla SEQ 2 SEQ model, the model with the attention mechanism. Denkowski and Neubig (2017) confirmed the large impact of common techniques such as training algorithms, subwords (Sennrich et al., 2016) and model ensem∗ Authors contributed equally. 99 Proceedings of the 4th Workshop on Asian Translation, pages 99–109, c Taipei, Taiwan, November 27, 2017. 2017 AFNLP 2 Related Work Our work investigat"
W17-5708,D16-1163,0,0.0363333,"g Training deep neural networks with a relatively small amount of training data risks creating a model that performs poorly. One technique used to minimize this drawback is pretraining of the model (Hinton et al., 2006; Bengio et al., 2007), which initializes (part of) the parameters of the model using parameters of another model. Pretraining has led to promising results in NLP tasks using SEQ 2 SEQ models. In languages with a small amount of supervised data, it has been found that NMT results can be improved by transferring parameters from a high-resource language pair to a low-resource one (Zoph et al., 2016). G¨ulc¸ehre et al. (2015) proposed a method using a combination of the output probabilities of a language model trained on large monolingual corpora and a SEQ 2 SEQ NMT model, which are both trained separately. Venugopalan et al. (2016) studied different types of systems combined with a language model under the video description generation task and also introduced a method to initialize the embedding layer and the RNN layer of the decoder of the SEQ 2 SEQ based model with pretrained parameters of the language model. They additionally proposed a method to initialize the embedding layer of the"
Y11-1044,P09-1079,0,0.594408,"lassification problem and train a reliable classifier from a large amount of labeled data (Pang et al., 2002; Mullen and Collier, 2004; Matsumoto et al., 2005; Gamon, 2005). The main disadvantage of such supervised approaches is that it is quite expensive in both time and labor to annotate a large amount of training data. Unfortunately, in some languages such as Chinese and Hindi, a sufficient amount of training data is not always available. Sentiment classification becomes a quite challenging problem for such resource-scarce languages. While some studies have tackled this problem (Wan, 2009; Dasgupta and Ng, 2009), they still require substantial human efforts or specific linguistic resources that are only available in particular languages as we will see later in Section 2. We therefore want to develop a low-cost, general method that can be readily applicable to sentiment classification in any languages. In this paper, we explore the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a document-level sentiment classifier under a minimally-supervised setting, where we have only a small number of labeled reviews other than the target reviews that we want to classify. Having a similarity"
Y11-1044,W04-3253,0,0.277682,"s) in a document-level sentiment classification task for resource-scarce languages (Chinese in our case). Keywords: sentiment classification, label propagation, semi-supervised learning 1 Introduction Over the last decade, document-level sentiment classification has attracted much attention from NLP researchers; its potential applications include opinion summarization and opinion mining (Pang and Lee, 2008). Most of the existing methods locate sentiment classification as a supervised classification problem and train a reliable classifier from a large amount of labeled data (Pang et al., 2002; Mullen and Collier, 2004; Matsumoto et al., 2005; Gamon, 2005). The main disadvantage of such supervised approaches is that it is quite expensive in both time and labor to annotate a large amount of training data. Unfortunately, in some languages such as Chinese and Hindi, a sufficient amount of training data is not always available. Sentiment classification becomes a quite challenging problem for such resource-scarce languages. While some studies have tackled this problem (Wan, 2009; Dasgupta and Ng, 2009), they still require substantial human efforts or specific linguistic resources that are only available in parti"
Y11-1044,P05-1049,0,0.0294562,"vironment excellent) ddd(facilities old) dddd (attitude OK) dddd (language concise ) Table 2: Reviews and their sentiment phrases Reviews dddddddddddd The room is very small and cold. Unsatisfying! dddddddddddd Service attitude is OK. The food is very good. Sentimental features extracted dd dd ddd Very small, very cold, Unsatisfying dddd dd Attitude is OK, very good lot of advantages including convergence and a well defined objective function. It has been successfully employed in several NLP tasks, such as sentiment lexicon induction (Rao and Ravichandran, 2009) and word sense disambiguation (Niu et al., 2005). To the best of our knowledge, there is no previous work that uses this algorithm in document-level sentiment classification where there are only a few amount of training data available. Our method is divided into three steps: Step 1: We extract from each review sentiment features, which are words/phrases with sentiment polarity, and then represent the review with a vector of extracted sentiment features (sentiment feature vectors). Step 2: We construct a similarity graph by regarding the reviews (sentiment feature vectors) as vertices. The edge (weight) between two vertices (reviews) represe"
Y11-1044,W02-1011,0,0.0266463,"ctor machines (TSVMs) in a document-level sentiment classification task for resource-scarce languages (Chinese in our case). Keywords: sentiment classification, label propagation, semi-supervised learning 1 Introduction Over the last decade, document-level sentiment classification has attracted much attention from NLP researchers; its potential applications include opinion summarization and opinion mining (Pang and Lee, 2008). Most of the existing methods locate sentiment classification as a supervised classification problem and train a reliable classifier from a large amount of labeled data (Pang et al., 2002; Mullen and Collier, 2004; Matsumoto et al., 2005; Gamon, 2005). The main disadvantage of such supervised approaches is that it is quite expensive in both time and labor to annotate a large amount of training data. Unfortunately, in some languages such as Chinese and Hindi, a sufficient amount of training data is not always available. Sentiment classification becomes a quite challenging problem for such resource-scarce languages. While some studies have tackled this problem (Wan, 2009; Dasgupta and Ng, 2009), they still require substantial human efforts or specific linguistic resources that a"
Y11-1044,E09-1077,0,0.132547,"d (do not hesitate) dd (too slow) dddd (so simple) dddd (environment excellent) ddd(facilities old) dddd (attitude OK) dddd (language concise ) Table 2: Reviews and their sentiment phrases Reviews dddddddddddd The room is very small and cold. Unsatisfying! dddddddddddd Service attitude is OK. The food is very good. Sentimental features extracted dd dd ddd Very small, very cold, Unsatisfying dddd dd Attitude is OK, very good lot of advantages including convergence and a well defined objective function. It has been successfully employed in several NLP tasks, such as sentiment lexicon induction (Rao and Ravichandran, 2009) and word sense disambiguation (Niu et al., 2005). To the best of our knowledge, there is no previous work that uses this algorithm in document-level sentiment classification where there are only a few amount of training data available. Our method is divided into three steps: Step 1: We extract from each review sentiment features, which are words/phrases with sentiment polarity, and then represent the review with a vector of extracted sentiment features (sentiment feature vectors). Step 2: We construct a similarity graph by regarding the reviews (sentiment feature vectors) as vertices. The edg"
Y11-1044,P02-1053,0,0.015293,"anually-tailored POS patterns. Table 1 lists five POS patterns that we used to extract phrases, along with corresponding phrases extracted by them. These POS patters motivates from an intuition that 2 3 http://nlp.stanford.edu/software/segmenter.shtml http://nlp.stanford.edu/software/tagger.shtml 422 Table 3: Similarity measure methods Name Computing formula 2|A∩B| |A|+|B| |A∩B| |A∪B| |A∩B| |min(A,B)| A·B |A||B| |A∩B| √ (|A|×|B|) Dice Jaccard Index Overlap Cosine (tf-idf) Cosine (binary) they are common indicators to identify sentiment expressed in reviews. Note that negation is tagged as AD. Turney (2002) used similar feature extracting strategy. adjective: We extract only adjective words with POS tag VA and JJ. Table 2 lists two examples of sentiment features extracted from reviews in the dataset we used in our experiment. They are positive and negative reviews in hotel domains, respectively. 3.3 Step 2: Build Similarity Graph We try several similarity measures to define the similarity between the feature vectors extracted from reviews in Step 1. We assume that the more similar the sentiment polarity of two reviews is, the higher the similarity score between them is. Table 3 lists similarity"
Y11-1044,N10-1119,0,0.0400313,"Missing"
Y11-1044,P09-1027,0,0.315881,"upervised classification problem and train a reliable classifier from a large amount of labeled data (Pang et al., 2002; Mullen and Collier, 2004; Matsumoto et al., 2005; Gamon, 2005). The main disadvantage of such supervised approaches is that it is quite expensive in both time and labor to annotate a large amount of training data. Unfortunately, in some languages such as Chinese and Hindi, a sufficient amount of training data is not always available. Sentiment classification becomes a quite challenging problem for such resource-scarce languages. While some studies have tackled this problem (Wan, 2009; Dasgupta and Ng, 2009), they still require substantial human efforts or specific linguistic resources that are only available in particular languages as we will see later in Section 2. We therefore want to develop a low-cost, general method that can be readily applicable to sentiment classification in any languages. In this paper, we explore the use of label propagation (LP) (Zhu and Ghahramani, 2002) in building a document-level sentiment classifier under a minimally-supervised setting, where we have only a small number of labeled reviews other than the target reviews that we want to classi"
Y11-1044,C04-1121,0,\N,Missing
Y13-1036,I13-1156,1,0.763652,"Missing"
Y13-1036,P06-1141,0,0.531489,"Missing"
Y13-1036,C10-1072,0,0.021957,"scoring function, the label is predicted as follows:  +1 if sr > 0, yr = sgn(sr ) = −1 otherwise. imental results. Finally, Section 5 concludes this study and addresses future work. 2 Related Work Early studies on sentiment analysis considers only textual content for classifying the sentiment of a given review (Pang and Lee, 2008). Pang et al. (2002) developed a supervised sentiment classifier which only takes n-gram features. Nakagawa et al. (2010) and Socher et al. (2011) considered structural interaction among words to capture complex intra-sentential phenomena such as polarity shifting (Li et al., 2010). On the other hand, recent studies started exploring the effectiveness of user and/or product information. Tan et al., (2011) and Speriosu et al., (2011) exploited user network behind a social media (Twitter in their case) and assumed that friends give similar ratings towards similar products. Seroussi et al. (2010) proposed a framework that computes users’ similarity on the basis of text and their rating histories. Then, they classify a given review by referring to ratings given for the same product by other users who are similar to the user in question. However, such user networks are not a"
Y13-1036,P11-1015,0,0.500791,"Our method is therefore applicable to reviews written by users and on products that are not observed in the training data. Because global features depend on labels of test reviews while the labels reversely depend on the global features, we need to globally optimize a label configuration for a given set of reviews. In this study, we resort to approximate algorithms, easiest-first (Tsuruoka and Tsujii, 2005) and twostage strategies (Krishnan and Manning, 2006), in decoding labels, and empirically compare their speed and accuracy. We evaluated our method on two real-world datasets with product (Maas et al., 2011) and user/product information (Blitzer et al., 2007). Experimental results demonstrated that the collective sentiment classification significantly improved the classification accuracy against the state-of-the-art methods, regardless of the choice of decoding strategy. The remainder of this paper is organized as follows. Section 2 discusses related work that exploits user and product information in a sentiment classification task. Then, Section 3 proposes a method that collectively classifies polarity of given set of reviews. Section 4 reports exper357 Copyright 2013 by Wenliang Gao, Naoki Yosh"
Y13-1036,N10-1120,0,0.0285265,"rence on Language, Information, and Computation pages 357－365 PACLIC-27 where xr is feature vector representation of the review r and w is the weight vector. With this scoring function, the label is predicted as follows:  +1 if sr > 0, yr = sgn(sr ) = −1 otherwise. imental results. Finally, Section 5 concludes this study and addresses future work. 2 Related Work Early studies on sentiment analysis considers only textual content for classifying the sentiment of a given review (Pang and Lee, 2008). Pang et al. (2002) developed a supervised sentiment classifier which only takes n-gram features. Nakagawa et al. (2010) and Socher et al. (2011) considered structural interaction among words to capture complex intra-sentential phenomena such as polarity shifting (Li et al., 2010). On the other hand, recent studies started exploring the effectiveness of user and/or product information. Tan et al., (2011) and Speriosu et al., (2011) exploited user network behind a social media (Twitter in their case) and assumed that friends give similar ratings towards similar products. Seroussi et al. (2010) proposed a framework that computes users’ similarity on the basis of text and their rating histories. Then, they classif"
Y13-1036,P04-1035,0,0.210259,"rest is to exploit user leniency and product popularity for improving sentiment classification. We realize this by encoding such biases as two global features, as detailed in Section 3.2. Since global features make it impossible to independently predict the labels of reviews, we explored two approximate decoding strategies in Section 3.3. Note that we assume the review is associated with the user who wrote that review, the product on which that review is written, or both. This assumption is not unrealistic nowadays. User information is available in many standard dataset (Blitzer et al., 2007; Pang and Lee, 2004). Moreover, as for product information, even if such information is not available, it is possible to extract it (Qiu et al., 2011). We should emphasize here that our method does not require user profiles, product descriptions, or any sort of extrinsic knowledge on the users and the products. 3.2 Our features can be divided into local and global ones such that xr = {xlr , xgr }. While local features (xlr ) are conventional word n-grams (n = 1 and n = 2), global features (xgr ) represent the user leniency and product popularity. Our global features are computed as: xgr = {f u+ (r), f u− (r), f p"
Y13-1036,P07-1056,0,0.948014,"tten by users and on products that are not observed in the training data. Because global features depend on labels of test reviews while the labels reversely depend on the global features, we need to globally optimize a label configuration for a given set of reviews. In this study, we resort to approximate algorithms, easiest-first (Tsuruoka and Tsujii, 2005) and twostage strategies (Krishnan and Manning, 2006), in decoding labels, and empirically compare their speed and accuracy. We evaluated our method on two real-world datasets with product (Maas et al., 2011) and user/product information (Blitzer et al., 2007). Experimental results demonstrated that the collective sentiment classification significantly improved the classification accuracy against the state-of-the-art methods, regardless of the choice of decoding strategy. The remainder of this paper is organized as follows. Section 2 discusses related work that exploits user and product information in a sentiment classification task. Then, Section 3 proposes a method that collectively classifies polarity of given set of reviews. Section 4 reports exper357 Copyright 2013 by Wenliang Gao, Naoki Yoshinaga, Nobuhiro Kaji, and Masaru Kitsuregawa 27th Pa"
Y13-1036,W02-1011,0,0.0358196,"user leniency and product popularity) by introducing global features in supervised learning. To resolve dependencies among labels of a given set of reviews, we explore two approximated decoding algorithms, “easiest-first decoding” and “twostage decoding”. Experimental results on two real-world datasets with product and user/product information confirmed that our method contributed greatly to the classification accuracy. 1 Introduction In document-level sentiment classification, early studies have exploited language-based clues (e.g., n-grams) extracted from the textual content (Turney, 2002; Pang et al., 2002), followed by recent studies which adapt the classifier to the reviews written by a specific user or written on a specific product (Tan et al., 2011; Seroussi et al., 2010; Speriosu et al., 2011; Li et al., 2011). Although the user- and product-aware methods exhibited better performance over the methods based on purely textual clues, most of them use only the user information (Tan et al., 2011; Seroussi et al., 2010; Speriosu et al., 2011), or they assume that the user and the product of a test review is known in advance (Li et al., 2011). These assumptions heavily limit their applicability in"
Y13-1036,J11-1002,0,0.135007,"ses as two global features, as detailed in Section 3.2. Since global features make it impossible to independently predict the labels of reviews, we explored two approximate decoding strategies in Section 3.3. Note that we assume the review is associated with the user who wrote that review, the product on which that review is written, or both. This assumption is not unrealistic nowadays. User information is available in many standard dataset (Blitzer et al., 2007; Pang and Lee, 2004). Moreover, as for product information, even if such information is not available, it is possible to extract it (Qiu et al., 2011). We should emphasize here that our method does not require user profiles, product descriptions, or any sort of extrinsic knowledge on the users and the products. 3.2 Our features can be divided into local and global ones such that xr = {xlr , xgr }. While local features (xlr ) are conventional word n-grams (n = 1 and n = 2), global features (xgr ) represent the user leniency and product popularity. Our global features are computed as: xgr = {f u+ (r), f u− (r), f p+ (r), f p− (r)}, where Method |{rj |yj = +1, rj ∈ Nu (r)}| , |Nu (r)| |{rj |yj = −1, rj ∈ Nu (r)}| f u− (r) = , |Nu (r)| |{rj |yj"
Y13-1036,D11-1014,0,0.0273669,"ion, and Computation pages 357－365 PACLIC-27 where xr is feature vector representation of the review r and w is the weight vector. With this scoring function, the label is predicted as follows:  +1 if sr > 0, yr = sgn(sr ) = −1 otherwise. imental results. Finally, Section 5 concludes this study and addresses future work. 2 Related Work Early studies on sentiment analysis considers only textual content for classifying the sentiment of a given review (Pang and Lee, 2008). Pang et al. (2002) developed a supervised sentiment classifier which only takes n-gram features. Nakagawa et al. (2010) and Socher et al. (2011) considered structural interaction among words to capture complex intra-sentential phenomena such as polarity shifting (Li et al., 2010). On the other hand, recent studies started exploring the effectiveness of user and/or product information. Tan et al., (2011) and Speriosu et al., (2011) exploited user network behind a social media (Twitter in their case) and assumed that friends give similar ratings towards similar products. Seroussi et al. (2010) proposed a framework that computes users’ similarity on the basis of text and their rating histories. Then, they classify a given review by refer"
Y13-1036,W11-2207,0,0.462647,"ing algorithms, “easiest-first decoding” and “twostage decoding”. Experimental results on two real-world datasets with product and user/product information confirmed that our method contributed greatly to the classification accuracy. 1 Introduction In document-level sentiment classification, early studies have exploited language-based clues (e.g., n-grams) extracted from the textual content (Turney, 2002; Pang et al., 2002), followed by recent studies which adapt the classifier to the reviews written by a specific user or written on a specific product (Tan et al., 2011; Seroussi et al., 2010; Speriosu et al., 2011; Li et al., 2011). Although the user- and product-aware methods exhibited better performance over the methods based on purely textual clues, most of them use only the user information (Tan et al., 2011; Seroussi et al., 2010; Speriosu et al., 2011), or they assume that the user and the product of a test review is known in advance (Li et al., 2011). These assumptions heavily limit their applicability in a real-world scenario where new users and new products are ceaselessly emerging. This paper proposes a method of collective sentiment classification that is aware of the user and the product of"
Y13-1036,H05-1059,0,0.258446,"praise. We introduce global features to encode these characteristics of a user and a product (referred to as user leniency and product popularity), and then compute the values of global features along with testing. Our method is therefore applicable to reviews written by users and on products that are not observed in the training data. Because global features depend on labels of test reviews while the labels reversely depend on the global features, we need to globally optimize a label configuration for a given set of reviews. In this study, we resort to approximate algorithms, easiest-first (Tsuruoka and Tsujii, 2005) and twostage strategies (Krishnan and Manning, 2006), in decoding labels, and empirically compare their speed and accuracy. We evaluated our method on two real-world datasets with product (Maas et al., 2011) and user/product information (Blitzer et al., 2007). Experimental results demonstrated that the collective sentiment classification significantly improved the classification accuracy against the state-of-the-art methods, regardless of the choice of decoding strategy. The remainder of this paper is organized as follows. Section 2 discusses related work that exploits user and product inform"
Y13-1036,P02-1053,0,0.00481208,"referred to as user leniency and product popularity) by introducing global features in supervised learning. To resolve dependencies among labels of a given set of reviews, we explore two approximated decoding algorithms, “easiest-first decoding” and “twostage decoding”. Experimental results on two real-world datasets with product and user/product information confirmed that our method contributed greatly to the classification accuracy. 1 Introduction In document-level sentiment classification, early studies have exploited language-based clues (e.g., n-grams) extracted from the textual content (Turney, 2002; Pang et al., 2002), followed by recent studies which adapt the classifier to the reviews written by a specific user or written on a specific product (Tan et al., 2011; Seroussi et al., 2010; Speriosu et al., 2011; Li et al., 2011). Although the user- and product-aware methods exhibited better performance over the methods based on purely textual clues, most of them use only the user information (Tan et al., 2011; Seroussi et al., 2010; Speriosu et al., 2011), or they assume that the user and the product of a test review is known in advance (Li et al., 2011). These assumptions heavily limit th"
