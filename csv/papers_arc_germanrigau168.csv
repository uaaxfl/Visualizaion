2021.gwc-1.6,{A}sk2{T}ransformers: Zero-Shot Domain labelling with Pretrained Language Models,2021,-1,-1,2,0,6128,oscar sainz,Proceedings of the 11th Global Wordnet Conference,0,"In this paper we present a system that exploits different pre-trained Language Models for assigning domain labels to WordNet synsets without any kind of supervision. Furthermore, the system is not restricted to use a particular set of domain labels. We exploit the knowledge encoded within different off-the-shelf pre-trained Language Models and task formulations to infer the domain label of a particular WordNet definition. The proposed zero-shot system achieves a new state-of-the-art on the English dataset used in the evaluation."
2021.findings-emnlp.333,Benchmarking Meta-embeddings: What Works and What Does Not,2021,-1,-1,3,0,7238,iker garciaferrero,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"In the last few years, several methods have been proposed to build meta-embeddings. The general aim was to obtain new representations integrating complementary knowledge from different source pre-trained embeddings thereby improving their overall quality. However, previous meta-embeddings have been evaluated using a variety of methods and datasets, which makes it difficult to draw meaningful conclusions regarding the merits of each approach. In this paper we propose a unified common framework, including both intrinsic and extrinsic tasks, for a fair and objective meta-embeddings evaluation. Furthermore, we present a new method to generate meta-embeddings, outperforming previous work on a large number of intrinsic evaluation benchmarks. Our evaluation framework also allows us to conclude that previous extrinsic evaluations of meta-embeddings have been overestimated."
2020.mmw-1.1,Towards modelling {SUMO} attributes through {W}ord{N}et adjectives: a Case Study on Qualities,2020,-1,-1,3,0.740741,6186,itziar gonzalezdios,Proceedings of the LREC 2020 Workshop on Multimodal Wordnets (MMW2020),0,"Previous studies have shown that the knowledge about attributes and properties in the SUMO ontology and its mapping to WordNet adjectives lacks of an accurate and complete characterization. A proper characterization of this type of knowledge is required to perform formal commonsense reasoning based on the SUMO properties, for instance to distinguish one concept from another based on their properties. In this context, we propose a new semi-automatic approach to model the knowledge about properties and attributes in SUMO by exploiting the information encoded in WordNet adjectives and its mapping to SUMO. To that end, we considered clusters of semantically related groups of WordNet adjectival and nominal synsets. Based on these clusters, we propose a new semi-automatic model for SUMO attributes and their mapping to WordNet, which also includes polarity information. In this paper, as an exploratory approach, we focus on qualities."
2020.lrec-1.171,Multilingual Stance Detection in Tweets: The {C}atalonia Independence Corpus,2020,25,0,4,0,16954,elena zotova,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Stance detection aims to determine the attitude of a given text with respect to a specific topic or claim. While stance detection has been fairly well researched in the last years, most the work has been focused on English. This is mainly due to the relative lack of annotated data in other languages. The TW-10 referendum Dataset released at IberEval 2018 is a previous effort to provide multilingual stance-annotated data in Catalan and Spanish. Unfortunately, the TW-10 Catalan subset is extremely imbalanced. This paper addresses these issues by presenting a new multilingual dataset for stance detection in Twitter for the Catalan and Spanish languages, with the aim of facilitating research on stance detection in multilingual and cross-lingual settings. The dataset is annotated with stance towards one topic, namely, the ndependence of Catalonia. We also provide a semi-automatic method to annotate the dataset based on a categorization of Twitter users. We experiment on the new corpus with a number of supervised approaches, including linear classifiers and deep learning methods. Comparison of our new corpus with the with the TW-1O dataset shows both the benefits and potential of a well balanced corpus for multilingual and cross-lingual research on stance detection. Finally, we establish new state-of-the-art results on the TW-10 dataset, both for Catalan and Spanish."
2020.lrec-1.708,{NUB}es: A Corpus of Negation and Uncertainty in {S}panish Clinical Texts,2020,22,1,4,0,1152,salvador lopez,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper introduces the first version of the NUBes corpus (Negation and Uncertainty annotations in Biomedical texts in Spanish). The corpus is part of an on-going research and currently consists of 29,682 sentences obtained from anonymised health records annotated with negation and uncertainty. The article includes an exhaustive comparison with similar corpora in Spanish, and presents the main annotation and design decisions. Additionally, we perform preliminary experiments using deep learning algorithms to validate the annotated dataset. As far as we know, NUBes is the largest available corpora for negation in Spanish and the first that also incorporates the annotation of speculation cues, scopes, and events."
2019.gwc-1.25,Commonsense Reasoning Using {W}ord{N}et and {SUMO}: a Detailed Analysis,2019,19,1,3,1,16536,javier alvez,Proceedings of the 10th Global Wordnet Conference,0,"We describe a detailed analysis of a sample of large benchmark of commonsense reasoning problems that has been automatically obtained from WordNet, SUMO and their mapping. The objective is to provide a better assessment of the quality of both the benchmark and the involved knowledge resources for advanced commonsense reasoning tasks. By means of this analysis, we are able to detect some knowledge misalignments, mapping errors and lack of knowledge and resources. Our final objective is the extraction of some guidelines towards a better exploitation of this commonsense knowledge framework by the improvement of the included resources."
L18-1322,Biomedical term normalization of {EHR}s with {UMLS},2018,14,0,3,0,29864,naiara perezmiguel,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"This paper presents a novel prototype for biomedical term normalization of electronic health record excerpts with the Unified Medical Language System (UMLS) Metathesaurus. Despite being multilingual and cross-lingual by design, we first focus on processing clinical text in Spanish because there is no existing tool for this language and for this specific purpose. The tool is based on Apache Lucene to index the Metathesaurus and generate mapping candidates from input text. It uses the IXA pipeline for basic language processing and resolves ambiguities with the UKB toolkit. It has been evaluated by measuring its agreement with MetaMap in two English-Spanish parallel corpora. In addition, we present a web-based interface for the tool."
L18-1367,Developing New Linguistic Resources and Tools for the {G}alician Language,2018,0,0,3,1,7239,rodrigo agerri,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1557,Building Named Entity Recognition Taggers via Parallel Corpora,2018,0,2,6,1,7239,rodrigo agerri,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1723,Cross-checking {W}ord{N}et and {SUMO} Using Meronymy,2018,0,3,3,1,16536,javier alvez,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
2018.gwc-1.4,Towards Cross-checking {W}ord{N}et and {SUMO} Using Meronymy,2018,0,3,2,1,16536,javier alvez,Proceedings of the 9th Global Wordnet Conference,0,"We describe the practical application of a black-box testing methodology for the validation of the knowledge encoded in WordNet, SUMO and their mapping by using automated theorem provers. In this paper,weconcentrateonthepart-whole information provided by WordNet and create a large set of tests on the basis of few question patterns. From our preliminary evaluation results, we report on some of the detected inconsistencies."
S16-1081,"{S}em{E}val-2016 Task 1: Semantic Textual Similarity, Monolingual and Cross-Lingual Evaluation",2016,29,110,7,0,8824,eneko agirre,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"Comunicacio presentada al 10th International Workshop on Semantic Evaluation (SemEval-2016), celebrat els dies 16 i 17 de juny de 2016 a San Diego, California."
S16-1082,{S}em{E}val-2016 Task 2: Interpretable Semantic Textual Similarity,2016,11,18,5,0,8824,eneko agirre,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"Comunicacio presentada al 10th International Workshop on Semantic Evaluation (SemEval-2016), celebrat els dies 16 i 17 de juny de 2016 a San Diego, California."
L16-1009,A Comparison of Domain-based Word Polarity Estimation using different Word Embeddings,2016,0,3,3,0,17768,aitor pablos,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"A key point in Sentiment Analysis is to determine the polarity of the sentiment implied by a certain word or expression. In basic Sentiment Analysis systems this sentiment polarity of the words is accounted and weighted in different ways to provide a degree of positivity/negativity. Currently words are also modelled as continuous dense vectors, known as word embeddings, which seem to encode interesting semantic knowledge. With regard to Sentiment Analysis, word embeddings are used as features to more complex supervised classification systems to obtain sentiment classifiers. In this paper we compare a set of existing sentiment lexicons and sentiment lexicon generation techniques. We also show a simple but effective technique to calculate a word polarity value for each word in a domain using existing continuous word embeddings generation methods. Further, we also show that word embeddings calculated on in-domain corpus capture the polarity better than the ones calculated on general-domain corpus."
L16-1233,The Event and Implied Situation Ontology ({ESO}): Application and Evaluation,2016,16,2,5,0,30052,roxane segers,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents the Event and Implied Situation Ontology (ESO), a manually constructed resource which formalizes the pre and post situations of events and the roles of the entities affected by an event. The ontology is built on top of existing resources such as WordNet, SUMO and FrameNet. The ontology is injected to the Predicate Matrix, a resource that integrates predicate and role information from amongst others FrameNet, VerbNet, PropBank, NomBank and WordNet. We illustrate how these resources are used on large document collections to detect information that otherwise would have remained implicit. The ontology is evaluated on two aspects: recall and precision based on a manually annotated corpus and secondly, on the quality of the knowledge inferred by the situation assertions in the ontology. Evaluation results on the quality of the system show that 50{\%} of the events typed and enriched with ESO assertions are correct."
L16-1268,Addressing the {MFS} Bias in {WSD} systems,2016,13,4,4,0,17438,marten postma,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Word Sense Disambiguation (WSD) systems tend to have a strong bias towards assigning the Most Frequent Sense (MFS), which results in high performance on the MFS but in a very low performance on the less frequent senses. We addressed the MFS bias in WSD systems by combining the output from a WSD system with a set of mostly static features to create a MFS classifier to decide when to and not to choose the MFS. The output from this MFS classifier, which is based on the Random Forest algorithm, is then used to modify the output from the original WSD system. We applied our classifier to one of the state-of-the-art supervised WSD systems, i.e. IMS, and to of the best state-of-the-art unsupervised WSD systems, i.e. UKB. Our main finding is that we are able to improve the system output in terms of choosing between the MFS and the less frequent senses. When we apply the MFS classifier to fine-grained WSD, we observe an improvement on the less frequent sense cases, whereas we maintain the overall recall."
L16-1423,A Multilingual Predicate Matrix,2016,17,6,4,1,17341,maddalen lacalle,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents the Predicate Matrix 1.3, a lexical resource resulting from the integration of multiple sources of predicate information including FrameNet, VerbNet, PropBank and WordNet. This new version of the Predicate Matrix has been extended to cover nominal predicates by adding mappings to NomBank. Similarly, we have integrated resources in Spanish, Catalan and Basque. As a result, the Predicate Matrix 1.3 provides a multilingual lexicon to allow interoperable semantic analysis in multiple languages."
2016.gwc-1.51,The Predicate Matrix and the Event and Implied Situation Ontology: Making More of Events,2016,15,4,5,0,30052,roxane segers,Proceedings of the 8th Global WordNet Conference (GWC),0,"This paper presents the Event and Implied Situation Ontology (ESO), a resource which formalizes the pre and post situations of events and the roles of the entities affected by an event. The ontology reuses and maps across existing resources such as WordNet, SUMO, VerbNet, PropBank and FrameNet. We describe how ESO is injected into a new version of the Predicate Matrix and illustrate how these resources are used to detect information in large document collections that otherwise would have remained implicit. The model targets interpretations of situations rather than the semantics of verbs per se. The event is interpreted as a situation using RDF taking all event components into account. Hence, the ontology and the linked resources need to be considered from the perspective of this interpretation model."
W15-5501,Cross-lingual Event Detection in Discourse,2015,0,0,1,1,6129,german rigau,Proceedings of the Second Workshop on Natural Language Processing and Linked Open Data,0,"Conferencian  We describe a system for event extraction across documents and languages. We developed a framework for the interoperable semantic interpretation of mentions of events, participants, locations and time, as well as the relations between them. Furthermore, we use a common RDF model to represent instances of events and normalised entities and dates. We convert multiple mentions of the same event in English and Spanish to a single representation. We thus resolve cross-document event and entity coreference within a language but also across languages. We tested our system on a Wikinews corpus of 120 English articles that have been manually translated to Spanish. We report on the cross-lingual cross-document event and entity extraction comparing the Spanish output with respect to English."
W15-4508,From {T}ime{L}ines to {S}tory{L}ines: A preliminary proposal for evaluating narratives,2015,15,5,3,1,1744,egoitz laparra,Proceedings of the First Workshop on Computing News Storylines,0,"We formulate a proposal that covers a new definition of StoryLines based on the shared data provided by the NewsStory workshop. We re-use the SemEval 2015 Task 4: Timelines dataset to provide a gold-standard dataset and an evaluation measure for evaluating StoryLines extraction systems. We also present a system to explore the feasibility of capturing StoryLines automatically. Finally, based on our initial findings, we also discuss some simple changes that will improve the existing annotations to complete our initial Story-"
W15-0814,Semantic Interoperability for Cross-lingual and cross-document Event Detection,2015,0,1,3,0,5469,piek vossen,"Proceedings of the The 3rd Workshop on {EVENTS}: Definition, Detection, Coreference, and Representation",0,None
S15-2032,{UBC}: Cubes for {E}nglish Semantic Textual Similarity and Supervised Approaches for Interpretable {STS},2015,13,8,5,0,8824,eneko agirre,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"In Semantic Textual Similarity, systems rate the degree of semantic equivalence on a graded scale from 0 to 5, with 5 being the most similar. For the English subtask, we present a system which relies on several resources for token-to-token and phrase-to-phrase similarity to build a data-structure which holds all the information, and then combine the information to get a similarity score. We also participated in the pilot on Interpretable STS, where we apply a pipeline which first aligns tokens, then chunks, and finally uses supervised systems to label and score each chunk alignment."
S15-2045,"{S}em{E}val-2015 Task 2: Semantic Textual Similarity, {E}nglish, {S}panish and Pilot on Interpretability",2015,15,106,11,0,8824,eneko agirre,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new datasets in English and Spanish. The annotations for both subtasks leveraged crowdsourcing. The English subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs. In addition, this year we ran a pilot task on interpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair. The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years. 7 teams participated with 29 runs."
S15-2121,{V}3: Unsupervised Aspect Based Sentiment Analysis for {S}em{E}val2015 Task 12,2015,6,12,3,1,20895,aitor garciapablos,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper presents our participation in SemEval-2015 task 12 (Aspect Based Sentiment Analysis). We participated employing only unsupervised or weakly-supervised approaches. Our attempt is based on requiring the minimum annotated or hand-crafted content, and avoids training a model using the provided training set. We use a continuous word representations (Word2Vec) to leverage in-domain semantic similarities of words for many of the involved subtasks."
S15-2132,{S}em{E}val-2015 Task 4: {T}ime{L}ine: Cross-Document Event Ordering,2015,14,25,7,0,5690,annelyse minard,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes the outcomes of the TimeLine task (Cross-Document Event Ordering), that was organised within the Time and Space track of SemEval-2015. Given a set of documents and a set of target entities, the task consisted of building a timeline for each entity, by detecting, anchoring in time and ordering the events involving that entity. The TimeLine task goes a step further than previous evaluation challenges by requiring participant systems to perform both event coreference and temporal relation extraction across documents. Four teams submitted the output of their systems to the four proposed subtracks for a total of 13 runs, the best of which obtained an F1-score of 7.85 in the main track (timeline creation from raw text)."
P15-2059,Document Level Time-anchoring for {T}ime{L}ine Extraction,2015,25,10,3,1,1744,egoitz laparra,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"This paper investigates the contribution of document level processing of timeanchors for TimeLine event extraction. We developed and tested two different systems. The first one is a baseline system that captures explicit time-anchors. The second one extends the baseline system by also capturing implicit time relations. We have evaluated both approaches in the SemEval 2015 task 4 TimeLine: CrossDocument Event Ordering. We empirically demonstrate that the document-based approach obtains a much more complete time anchoring. Moreover, this approach almost doubles the performance of the systems that participated in the task."
W14-0150,First steps towards a Predicate Matrix,2014,28,7,3,1,17341,maddalen lacalle,Proceedings of the Seventh Global {W}ordnet Conference,0,"This paper presents the first steps towards building the Predicate Matrix, a new lexical resource resulting from the integration of multiple sources of predicate information including FrameNet (Baker et al., 1997), VerbNet (Kipper, 2005), PropBank (Palmer et al., 2005) and WordNet (Fellbaum, 1998). By using the Predicate Matrix, we expect to provide a more robust interoperable lexicon by discovering and solving inherent inconsistencies among the resources. Moreover, we plan to extend the coverage of current predicate resources (by including from WordNet morphologically related nominal and verbal concepts), to enrich WordNet with predicate information, and possibly to extend predicate information to languages other than English (by exploiting the local wordnets aligned to the English WordNet)."
S14-2010,{S}em{E}val-2014 Task 10: Multilingual Semantic Textual Similarity,2014,25,158,9,0,8824,eneko agirre,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity. For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotesWordNet sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline mappings. For Spanish, since, to our knowledge, this is the first time that official evaluations are conducted, we used well-formed text, by featuring sentences extracted from encyclopedic content and newswire. The annotations for both tasks leveraged crowdsourcing. The Spanish subtask engaged 9 teams participating with 22 system runs, and the English subtask attracted 15 teams with 38 system runs."
S14-2148,{V}3: Unsupervised Generation of Domain Aspect Terms for Aspect Based Sentiment Analysis,2014,19,8,3,1,20895,aitor garciapablos,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper presents V3, an unsupervised system for aspect-based Sentiment Analysis when evaluated on the SemEval 2014 Task 4. V3 focuses on generating a list of aspect terms for a new domain using a collection of raw texts from the domain. We also implement a very basic approach to classify the aspect terms into categories and assign polarities to them."
vossen-etal-2014-newsreader,{N}ews{R}eader: recording history from daily news streams,2014,8,17,2,0,5469,piek vossen,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The European project NewsReader develops technology to process daily news streams in 4 languages, extracting what happened, when, where and who was involved. NewsReader does not just read a single newspaper but massive amounts of news coming from thousands of sources. It compares the results across sources to complement information and determine where they disagree. Furthermore, it merges news of today with previous news, creating a long-term history rather than separate events. The result is stored in a KnowledgeStore, that cumulates information over time, producing an extremely large knowledge graph that is visualized using new techniques to provide more comprehensive access. We present the first version of the system and the results of processing first batches of data."
lopez-de-lacalle-etal-2014-predicate,Predicate Matrix: extending {S}em{L}ink through {W}ord{N}et mappings,2014,30,21,3,1,17341,maddalen lacalle,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents the Predicate Matrix v1.1, a new lexical resource resulting from the integration of multiple sources of predicate information including FrameNet, VerbNet, PropBank and WordNet. We start from the basis of SemLink. Then, we use advanced graph-based algorithms to further extend the mapping coverage of SemLink. Second, we also exploit the current content of SemLink to infer new role mappings among the different predicate schemas. As a result, we have obtained a new version of the Predicate Matrix which largely extends the current coverage of SemLink and the previous version of the Predicate Matrix."
agerri-etal-2014-ixa,{IXA} pipeline: Efficient and Ready to Use Multilingual {NLP} tools,2014,26,37,3,1,7239,rodrigo agerri,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"IXA pipeline is a modular set of Natural Language Processing tools (or pipes) which provide easy access to NLP technology. It offers robust and efficient linguistic annotation to both researchers and non-NLP experts with the aim of lowering the barriers of using NLP technology either for research purposes or for small industrial developers and SMEs. IXA pipeline can be used {``}as is{''} or exploit its modularity to pick and change different components. Given its open-source nature, it can also be modified and extended for it to work with other languages. This paper describes the general data-centric architecture of IXA pipeline and presents competitive results in several NLP annotations for English and Spanish."
E14-2002,"Multilingual, Efficient and Easy {NLP} Processing with {IXA} Pipeline",2014,14,4,3,1,7239,rodrigo agerri,Proceedings of the Demonstrations at the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,IXA pipeline is a modular set of Natural Language Processing tools (or pipes) which provide easy access to NLP technology. It aims at lowering the barriers of using NLP technology both for research purposes and for small industrial developers and SMEs by offering robust and efficient linguistic annotation to both researchers and non-NLP experts. IXA pipeline can be used xe2x80x9cas isxe2x80x9d or exploit its modularity to pick and change different components. This paper describes the general data-centric architecture of IXA pipeline and presents competitive results in several NLP annotations for English and Spanish.
E14-1010,"Simple, Robust and (almost) Unsupervised Generation of Polarity Lexicons for Multiple Languages",2014,28,16,3,0,5339,inaki vicente,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper presents a simple, robust and (almost) unsupervised dictionary-based method, qwn-ppv (Q-WordNet as Personalized PageRanking Vector) to automatically generate polarity lexicons. We show that qwn-ppv outperforms other automatically generated lexicons for the four extrinsic evaluations presented here. It also shows very competitive and robust results with respect to manually annotated ones. Results suggest that no single lexicon is best for every task and dataset and that the intrinsic evaluation of polarity lexicons is not a good performance indicator on a Sentiment Analysis task. The qwn-ppv method allows to easily create quality polarity lexicons whenever no domain-based annotated corpora are available for a given language."
W13-0114,Sources of Evidence for Implicit Argument Resolution,2013,25,9,2,1,1744,egoitz laparra,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,"Traditionally, semantic role labelling systems have focused on searching the fillers of those explicit roles appearing within sentence boundaries. However, when the participants of a predicate are implicit and can not be found inside sentence boundaries, this approach obtains incomplete predicative structures with null arguments. Previous research facing this task have coincided in identifying the implicit argument filling as a special case of anaphora or coreference resolution. In this work, we review a number of theories that model the behaviour of discourse coreference and propose some adaptations to capture evidence for the implicit argument resolution task. We empirically demonstrate that exploiting such evidence our system outperforms previous approaches evaluated on the SemEval-2010 task 10 dataset. We complete our study identifying those cases that traditional coreference theories can not cover."
S13-1018,{UBC}{\\_}{UOS}-{TYPED}: Regression for typed-similarity,2013,12,4,4,0,8824,eneko agirre,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity",0,We approach the typed-similarity task using a range of heuristics that rely on information from the appropriate metadata fields for each type of similarity. In addition we train a linear regressor for each type of similarity. The results indicate that the linear regression is key for good performance. Our best system was ranked third in the task.
P13-1116,{I}mp{A}r: A Deterministic Algorithm for Implicit Semantic Role Labelling,2013,27,26,2,1,1744,egoitz laparra,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"This paper presents a novel deterministic algorithm for implicit Semantic Role Labeling. The system exploits a very simple but relevant discursive property, the argument coherence over different instances of a predicate. The algorithm solves the implicit arguments sequentially, exploiting not only explicit but also the implicit arguments previously solved. In addition, we empirically demonstrate that the algorithm obtains very competitive and robust performances with respect to supervised approaches that require large amounts of costly training data."
gonzalez-agirre-etal-2012-multilingual,Multilingual Central Repository version 3.0,2012,17,55,3,1,8511,aitor gonzalezagirre,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the upgrading process of the Multilingual Central Repository (MCR). The new MCR uses WordNet 3.0 as Interlingual-Index (ILI). Now, the current version of the MCR integrates in the same EuroWordNet framework wordnets from five different languages: English, Spanish, Catalan, Basque and Galician. In order to provide ontological coherence to all the integrated wordnets, the MCR has also been enriched with a disparate set of ontologies: Base Concepts, Top Ontology, WordNet Domains and Suggested Upper Merged Ontology. The whole content of the MCR is freely available."
gonzalez-agirre-etal-2012-proposal,A proposal for improving {W}ord{N}et Domains,2012,8,2,3,1,8511,aitor gonzalezagirre,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"WordNet Domains (WND) is a lexical resource where synsets have been semi-automatically annotated with one or more domain labels from a set of 165 hierarchically organized domains. The uses of WND include the power to reduce the polysemy degree of the words, grouping those senses that belong to the same domain. But the semi-automatic method used to develop this resource was far from being perfect. By cross-checking the content of the Multilingual Central Repository (MCR) it is possible to find some errors and inconsistencies. Many are very subtle. Others, however, leave no doubt. Moreover, it is very difficult to quantify the number of errors in the original version of WND. This paper presents a novel semi-automatic method to propagate domain information through the MCR. We also compare both labellings (the original and the new one) allowing us to detect anomalies in the original WND labels."
cuadros-etal-2012-highlighting,Highlighting relevant concepts from Topic Signatures,2012,23,2,3,1,17770,montse cuadros,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents deepKnowNet, a new fully automatic method for building highly dense and accurate knowledge bases from existing semantic resources. Basically, the method applies a knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate WordNet sense to large sets of topically related words acquired from the web, named TSWEB. This Word Sense Disambiguation algorithm is the personalized PageRank algorithm implemented in UKB. This new method improves by automatic means the current content of WordNet by creating large volumes of new and accurate semantic relations between synsets. KnowNet was our first attempt towards the acquisition of large volumes of semantic relations. However, KnowNet had some limitations that have been overcomed with deepKnowNet. deepKnowNet disambiguates the first hundred words of all Topic Signatures from the web (TSWEB). In this case, the method highlights the most relevant word senses of each Topic Signature and filter out the ones that are not so related to the topic. In fact, the knowledge it contains outperforms any other resource when is empirically evaluated in a common framework based on a similarity task annotated with human judgements."
laparra-etal-2012-mapping,Mapping {W}ord{N}et to the {K}yoto ontology,2012,14,4,2,1,1744,egoitz laparra,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the connection of WordNet to a generic ontology based on DOLCE. We developed a complete set of heuristics for mapping all WordNet nouns, verbs and adjectives to the ontology. Moreover, the mapping also allows to represent predicates in a uniform and interoperable way, regardless of the way they are expressed in the text and in which language. Together with the ontology, the WordNet mappings provide a extremely rich and powerful basis for semantic processing of text in any domain. In particular, the mapping has been used in a knowledge-rich event-mining system developed for the Asian-European project KYOTO."
W11-1819,Using Kybots for Extracting Events in Biomedical Texts,2011,3,6,5,0,33528,arantza casillas,Proceedings of {B}io{NLP} Shared Task 2011 Workshop,0,In this paper we describe a rule-based system developed for the BioNLP 2011 GENIA event detection task. The system applies Kybots (Knowledge Yielding Robots) on annotated texts to extract bio-events involving proteins or genes. The main goal of this work is to verify the usefulness and portability of the Kybot technology to the domain of biomedicine.
W10-3301,{KYOTO}: an open platform for mining facts,2010,13,14,2,0.359361,5469,piek vossen,Proceedings of the 6th Workshop on {O}ntologies and {L}exical {R}esources,0,"This document describes an open text-mining system that was developed for the Asian-European project KYOTO. The KYOTO system uses an open text representation format and a central ontology to enable extraction of knowledge and facts from large volumes of text in many different languages. We implemented a semantic tagging approach that performs off-line reasoning. Mining of facts and knowledge is achieved through a flexible pattern matching module that can work in much the same way for different languages, can handle efficiently large volumes of documents and is not restricted to a specific domain. We applied the system to an English database on estuaries."
S10-1090,{GPLSI}-{IXA}: Using Semantic Classes to Acquire Monosemous Training Examples from Domain Texts,2010,17,2,3,1,34998,ruben izquierdo,Proceedings of the 5th International Workshop on Semantic Evaluation,0,"This paper summarizes our participation in task #17 of SemEval--2 (All--words WSD on a specific domain) using a supervised class-based Word Sense Disambiguation system. Basically, we use Support Vector Machines (SVM) as learning algorithm and a set of simple features to build three different models. Each model considers a different training corpus: SemCor (SC), examples from monosemous words extracted automatically from background data (BG), and both SC and BG (SCBG). Our system explodes the monosemous words appearing as members of a particular WordNet semantic class to automatically acquire class-based annotated examples from the domain text. We use the class-based examples gathered from the domain corpus to adapt our traditional system trained on SemCor. The evaluation reveal that the best results are achieved training with SemCor and the background examples from monosemous words, obtaining results above the first sense baseline and the fifth best position in the competition rank."
reese-etal-2010-wikicorpus,{W}ikicorpus: A Word-Sense Disambiguated Multilingual {W}ikipedia Corpus,2010,16,21,5,0,45828,samuel reese,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This article presents a new freely available trilingual corpus (Catalan, Spanish, English) that contains large portions of the Wikipedia and has been automatically enriched with linguistic information. To our knowledge, this is the largest such corpus that is freely available to the community: In its present version, it contains over 750 million words. The corpora have been annotated with lemma and part of speech information using the open source library FreeLing. Also, they have been sense annotated with the state of the art Word Sense Disambiguation algorithm UKB. As UKB assigns WordNet senses, and WordNet has been aligned across languages via the InterLingual Index, this sort of annotation opens the way to massive explorations in lexical semantics that were not possible before. We present a first attempt at creating a trilingual lexical resource from the sense-tagged Wikipedia corpora, namely, WikiNet. Moreover, we present two by-products of the project that are of use for the NLP community: An open source Java-based parser for Wikipedia pages developed for the construction of the corpus, and the integration of the WSD algorithm UKB in FreeLing."
agirre-etal-2010-exploring,Exploring Knowledge Bases for Similarity,2010,28,35,3,0,8824,eneko agirre,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Graph-based similarity over WordNet has been previously shown to perform very well on word similarity. This paper presents a study of the performance of such a graph-based algorithm when using different relations and versions of Wordnet. The graph algorithm is based on Personalized PageRank, a random-walk based algorithm which computes the probability of a random-walk initiated in the target word to reach any synset following the relations in WordNet (Haveliwala, 2002). Similarity is computed as the cosine of the probability distributions for each word over WordNet. The best combination of relations includes all relations in WordNet 3.0, included disambiguated glosses, and automatically disambiguated topic signatures called KnowNets. All relations are part of the official release of WordNet, except KnowNets, which have been derived automatically. The results over the WordSim 353 dataset show that using the adequate relations the performance improves over previously published WordNet-based results on the WordSim353 dataset (Finkelstein et al., 2002). The similarity software and some graphs used in this paper are publicly available at http://ixa2.si.ehu.es/ukb."
cuadros-etal-2010-integrating,Integrating a Large Domain Ontology of Species into {W}ord{N}et,2010,26,4,3,1,17770,montse cuadros,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"With the proliferation of applications sharing information represented in multiple ontologies, the development of automatic methods for robust and accurate ontology matching will be crucial to their success. Connecting and merging already existing semantic networks is perhaps one of the most challenging task related to knowledge engineering. This paper presents a new approach for aligning automatically a very large domain ontology of Species to WordNet in the framework of the KYOTO project. The approach relies on the use of knowledge-based Word Sense Disambiguation algorithm which accurately assigns WordNet synsets to the concepts represented in Species 2000."
laparra-rigau-2010-extended,e{X}tended {W}ord{F}rame{N}et,2010,-1,-1,2,1,1744,egoitz laparra,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper presents a novel automatic approach to partially integrate FrameNet and WordNet. In that way we expect to extend FrameNet coverage, to enrich WordNet with frame semantic information and possibly to extend FrameNet to languages other than English. The method uses a knowledge-based Word Sense Disambiguation algorithm for matching the FrameNet lexical units to WordNet synsets. Specifically, we exploit a graph-based Word Sense Disambiguation algorithm that uses a large-scale knowledge-base derived from existing semantic resources. We have developed and tested additional versions of this algorithm showing substantial improvements over state-of-the-art results. Finally, we show some examples and figures of the resulting semantic resource."
R09-1039,Integrating {W}ord{N}et and {F}rame{N}et using a Knowledge-based Word Sense Disambiguation Algorithm,2009,28,16,2,1,1744,egoitz laparra,Proceedings of the International Conference {RANLP}-2009,0,"This paper presents a novel automatic approach to partially integrate FrameNet and WordNet. In that way we expect to extend FrameNet coverage, to enrich WordNet with frame semantic information and possibly to extend FrameNet to languages other than English. The method uses a knowledge-based Word Sense Disambiguation algorithm for linking FrameNet lexical units to WordNet synsets. Specifically, we exploit a graph-based Word Sense Disambiguation algorithm that uses a large-scale knowledge-base derived from WordNet. We have developed and tested four additional versions of this algorithm showing a substantial improvement over previous results."
E09-1045,An Empirical Study on Class-Based Word Sense Disambiguation,2009,24,18,3,1,34998,ruben izquierdo,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"As empirically demonstrated by the last SensEval exercises, assigning the appropriate meaning to words in context has resisted all attempts to be successfully addressed. One possible reason could be the use of inappropriate set of meanings. In fact, WordNet has been used as a de-facto standard repository of meanings. However, to our knowledge, the meanings represented by WordNet have been only used for WSD at a very fine-grained sense level or at a very coarse-grained class level. We suspect that selecting the appropriate level of abstraction could be on between both levels. We use a very simple method for deriving a small set of appropriate meanings using basic structural properties of WordNet. We also empirically demonstrate that this automatically derived set of meanings groups senses into an adequate level of abstraction in order to perform class-based Word Sense Disambiguation, allowing accuracy figures over 80%."
W08-2207,{K}now{N}et: A Proposal for Building Highly Connected and Dense Knowledge Bases from the {W}eb,2008,23,5,2,1,17770,montse cuadros,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"This paper presents a new fully automatic method for building highly dense and accurate knowledge bases from existing semantic resources. Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguation algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web. KnowNet, the resulting knowledge-base which connects large sets of semantically-related concepts is a major step towards the autonomous acquisition of knowledge from raw corpora. In fact, KnowNet is several times larger than any available knowledge resource encoding relations between synsets, and the knowledge that KnowNet contains outperform any other resource when empirically evaluated in a common multilingual framework."
alvez-etal-2008-complete,Complete and Consistent Annotation of {W}ord{N}et using the Top Concept Ontology,2008,16,29,7,1,16536,javier alvez,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper presents the complete and consistent ontological annotation of the nominal part of WordNet. The annotation has been carried out using the semantic features defined in the EuroWordNet Top Concept Ontology and made available to the NLP community. Up to now only an initial core set of 1,024 synsets, the so-called Base Concepts, was ontologized in such a way. The work has been achieved by following a methodology based on an iterative and incremental expansion of the initial labeling through the hierarchy while setting inheritance blockage points. Since this labeling has been set on the EuroWordNetÂs Interlingual Index (ILI), it can be also used to populate any other wordnet linked to it through a simple porting process. This feature-annotated WordNet is intended to be useful for a large number of semantic NLP tasks and for testing for the first time componential analysis on real environments. Moreover, the quantitative analysis of the work shows that more than 40{\%} of the nominal part of WordNet is involved in structure errors or inadequacies."
vossen-etal-2008-kyoto,"{KYOTO}: a System for Mining, Structuring and Distributing Knowledge across Languages and Cultures",2008,32,44,13,0.359361,5469,piek vossen,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We outline work performed within the framework of a current EC project. The goal is to construct a language-independent information system for a specific domain (environment/ecology/biodiversity) anchored in a language-independent ontology that is linked to wordnets in seven languages. For each language, information extraction and identification of lexicalized concepts with ontological entries is carried out by text miners (ÂKybotsÂ). The mapping of language-specific lexemes to the ontology allows for crosslinguistic identification and translation of equivalent terms. The infrastructure developed within this project enables long-range knowledge sharing and transfer across many languages and cultures, addressing the need for global and uniform transition of knowledge beyond the specific domains addressed here."
pociello-etal-2008-wnterm,{WNTERM}: Enriching the {MCR} with a Terminological Dictionary,2008,15,3,5,0,48081,eli pociello,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper we describe the methodology and the first steps for the creation of WNTERM (from WordNet and Terminology), a specialized lexicon produced from the merger of the EuroWordNet-based Multilingual Central Repository (MCR) and the Basic Encyclopaedic Dictionary of Science and Technology (BDST). As an example, the ecology domain has been used. The final result is a multilingual (Basque and English) light-weight domain ontology, including taxonomic and other semantic relations among its concepts, which is tightly connected to other wordnets."
C08-1021,{K}now{N}et: Building a Large Net of Knowledge from the Web,2008,0,2,2,1,17770,montse cuadros,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper presents a new fully automaticn method for building highly dense and accurate knowledge bases from existingn semantic resources. Basically, the method uses a wide-coverage and accurate knowledge-based Word Sense Disambiguationn algorithm to assign the most appropriate senses to large sets of topically related words acquired from the web.n KnowNet, the resulting knowledge-basen which connects large sets of semantically related concepts is a major step towardsn the autonomous acquisition of knowledgen from raw corpora. In fact, KnowNet is severaln times larger than any available knowledgen resource encoding relations betweenn synsets, and the knowledge KnowNet containsn outperform any other resource when is empirically evaluated in a common framework."
S07-1001,{S}em{E}val-2007 Task 01: Evaluating {WSD} on Cross-Language Information Retrieval,2007,17,19,5,0.0811758,8824,eneko agirre,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This paper presents a first attempt of an application-driven evaluation exercise of WSD. We used a CLIR testbed from the Cross Lingual Evaluation Forum. The expansion, indexing and retrieval strategies where fixed by the organizers. The participants had to return both the topics and documents tagged with WordNet 1.6 word senses. The organization provided training data in the form of a pre-processed Semcor which could be readily used by participants. The task had two participants, and the organizer also provide an in-house WSD system for comparison."
S07-1015,{S}em{E}val-2007 Task 16: Evaluation of Wide Coverage Knowledge Resources,2007,13,9,2,1,17770,montse cuadros,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"This task tries to establish the relative quality of available semantic resources (derived by manual or automatic means). The quality of each large-scale knowledge resource is indirectly evaluated on a Word Sense Disambiguation task. In particular, we use Senseval-3 and SemEval-2007 English Lexical Sample tasks as evaluation bechmarks to evaluate the relative quality of each resource. Furthermore, trying to be as neutral as possible with respect the knowledge bases studied, we apply systematically the same disambiguation method to all the resources. A completely different behaviour is observed on both lexical data sets (Senseval-3 and SemEval-2007)."
S07-1032,{GPLSI}: Word Coarse-grained Disambiguation aided by Basic Level Concepts,2007,15,6,3,1,34998,ruben izquierdo,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We present a corpus-based supervised learning system for coarse-grained sense disambiguation. In addition to usual features for training in word sense disambiguation, our system also uses Base Level Concepts automatically obtained from WordNet. Base Level Concepts are some synsets that generalize a hyponymy sub-hierarchy, and provides an extra level of abstraction as well as relevant information about the context of a word to be disambiguated. Our experiments proved that using this type of features results on a significant improvement of precision. Our system has achieved almost 0.8 F1 (fifth place) in the coarse--grained English all-words task using a very simple set of features plus Base Level Concepts annotation."
W06-1663,Quality Assessment of Large Scale Knowledge Resources,2006,12,40,2,1,17770,montse cuadros,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents an empirical evaluation of the quality of publicly available large-scale knowledge resources. The study includes a wide range of manually and automatically derived large-scale knowledge resources. In order to establish a fair and neutral comparison, the quality of each knowledge resource is indirectly evaluated using the same method on a Word Sense Disambiguation task. The evaluation framework selected has been the Senseval-3 English Lexical Sample Task. The study empirically demonstrates that automatically acquired knowledge resources surpass both in terms of precision and recall the knowledge resources derived manually, and that the combination of the knowledge contained in these resources is very close to the most frequent sense classifier. As far as we know, this is the first time that such a quality assessment has been performed showing a clear picture of the current state-of-the-art of publicly available wide coverage semantic resources."
W04-0823,The {TALP} systems for disambiguating {W}ord{N}et glosses,2004,12,7,4,0,42944,mauro castillo,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,"This paper describes the TALP systems presented at Senseval-3 task 12 xe2x80x9cWord-Sense Disambiguation of WordNet Glossesxe2x80x9d. Our method combines a set of knowledge-based heuristics integrating several information sources and techniques. Using large scale lexicoxe2x80x93semantic knowledge bases, such as WN, has become a usual, often necessary, practice for most current Natural Language Processing systems. Building appropriate resources of this nature for broadxe2x80x93coverage semantic processing is a hard and expensive task, involving large research groups during long periods of development. For example, dozens of personxe2x80x93 years are been invested worldxe2x80x93wide into the development of wordnets for various languages (Fellbaum, 1998), (Atserias et al., 1997), (Agirre et al., 2002), (Pianta et al., 2002). Dictionaries are special texts describing the meaning of a language. They provide a wide range of information of words by giving definitions of the word senses and as, a side effect, they supply knowledge about the world itself. WordNet (WN) (Fellbaum, 1998) can be also seen as an structured dictionary with thouthands of semantic relations, defining the most common concepts of the English language. Although the importance of (WN) has widely exceeded the purpose of its creation (Miller et al., 1990), and it has become an essential semantic resource for many applications, at the moment is not rich enough to directly support advanced semantic processing (Harabagiu et al., 1999). Sense disambiguation of definitions in any lexical resource is an important objective in the language engineering community because this process can increase the semantic conectivity among concepts. The first significant disambiguation of dictionary definitions took place 20 years ago (see (Rigau, 1998) for an extended survey on acquiring lexical knowledge from Machine Readable Dictionaries). Recently, several research groups have presented different approaches to perform this process on WN. In the eXtended WordNet1 (Mihalcea and Moldovan, 2001) the WN glosses have been syntactically parsed, transformed into logic forms and the content words are also semantically disambiguated. Being derived from an automatic process, disambiguated words included into the glosses have assigned a confidence label indicating the quality of the annotation (gold, silver or normal)."
W04-0828,{TALP} system for the {E}nglish lexical sample task,2004,4,11,3,1,51637,gerard escudero,"Proceedings of {SENSEVAL}-3, the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text",0,None
fernandez-etal-2004-automatic,Automatic Acquisition of Sense Examples Using {E}x{R}etriever,2004,5,15,3,0,52215,juan fernandez,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"A current research line for word sense disambiguation (WSD) focuses on the use of supervised machine learning techniques. One of the drawbacks of using such techniques is that previously sense annotated data is required. This paper presents ExRetriever, a new software tool for automatically acquiring large sets of sense tagged examples from large collections of text and the Web. ExRetriever exploits the knowledge contained in large-scale knowledge bases (e.g., WordNet) to build complex queries, each of them characterising particularn senses of a word. These examples can be used as training instances for supervised WSD algorithms."
atserias-etal-2004-towards,Towards the Meaning Top Ontology: Sources of Ontological Meaning,2004,10,6,3,1,36866,jordi atserias,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper describes the initial research steps towards the Top Ontology for the Multilingual Central Repository (MCR) built in the MEANING project. The current version of the MCR integrates five local wordnets plus four versions of Princetonxe2x80x99s English WordNet, three ontologies and hundreds of thousands of new semantic relations and properties automatically acquired from corpora. In order to maintain compatibility among all these heterogeneous knowledge resources, it is fundamental to have a robust and advanced ontological support. This paper studies the mapping of main Sources of Ontological Meaning onto the wordnets and, in particular, the current work in mapping the EuroWordNet Top Concept Ontology."
atserias-etal-2004-cross,Cross-Language Acquisition of Semantic Models for Verbal Predicates,2004,11,2,6,1,36866,jordi atserias,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper presents a semantic-driven methodology for the automatic acquisition of verbal models. Our approach relies strongly on the semantic generalizations allowed by already existing resources (e.g. Domain labels, Named Entity categories, concepts in the SUMO ontology, etc). Several experiments have been carried out using comparable corpora in four languages (Italian, Spanish, Basque and English) and two domains (FINANCE and SPORT) showing that the semantic patterns acquired can be general enough to be ported from one language to the other language."
atserias-etal-2004-spanish,{S}panish {W}ord{N}et 1.6: Porting the {S}panish {W}ordnet Across {P}rinceton Versions,2004,5,13,3,1,36866,jordi atserias,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper describes the new Spanish Wordnet aligned to Princeton WordNet1.6 and the analysis of the transformation from the previous version aligned to Princeton WordNet1.5. Although a mapping technology exists, to our knowledge it is the first time a whole local wordnet has been ported to a newer release of the Princeton WordNet."
W02-1304,{MEANING}: a Roadmap to Knowledge Technologies,2002,30,30,1,1,6129,german rigau,{COLING}-02: A Roadmap for Computational Linguistics,0,"Knowledge Technologies need to extract knowledge from existing texts, which calls for advanced Human Language Technologies (HLT). Progress is being made in Natural Language Processing but there is still a long way towards Natural Language Understanding. An important step towards this goal is the development of technologies and resources that deal with concepts rather than words. The MEANING project argues that we need to solve two complementary and intermediate tasks to enable the next generation of intelligent open domain HLT application systems: Word Sense Disambiguation and large-scale enrichment of Lexical Knowledge Bases. Innovations in this area will lead to HLT with deeper understanding of texts, and immediate progress in real applications of Knowledge Technologies."
W01-1013,Multilingual Authoring: the {NAMIC} Approach,2001,11,7,9,0,12620,roberto basili,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,"With increasing amounts of electronic information available, and the increase in the variety of languages used to produce documents of the same type, the problem of how to manage similar documents in different languages arises. This paper proposes an approach to processing/structuring text so that Multilingual Authoring (creating hypertext links) can be effectively carried out. This work, funded by the European Union, is applied to the Multilingual Authoring of news agency text. We have applied methods from Natural Language Processing, especially Information Extraction technology, to both monolingual and Multilingual Authoring."
S01-1010,Framework and Results for the {S}panish {SENSEVAL},2001,2,4,1,1,6129,german rigau,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"In this paper we describe the structure, organisation and results of the SENSEVAL exercise for Spanish. We present several design decisions we taked for the exercise, we describe the creation of the gold-standard data and finally, we present the results of the evaluation. Twelve systems from five different universities were evaluated. Final scores ranged from 0.56 to 0.65."
S01-1017,Using {L}azy{B}oosting for Word Sense Disambiguation,2001,9,10,3,1,51637,gerard escudero,Proceedings of {SENSEVAL}-2 Second International Workshop on Evaluating Word Sense Disambiguation Systems,0,"This paper describes the architecture and results of the TALP system presented at the SENSEVAL-2 exercise for the English lexical--sample task. This system is based on the LazyBoosting algorithm for Word Sense Disambiguation (Escudero et al., 2000), and incorporates some improvements and adaptations to this task. The evaluation reported here includes an analysis of the contribution of each component to the overall system performance."
W00-1322,An Empirical Study of the Domain Dependence of Supervised Word Disambiguation Systems,2000,28,52,3,1,51637,gerard escudero,2000 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,"This paper describes a set of experiments carried out to explore the domain dependence of alternative supervised Word Sense Disambiguation algorithms. The aim of the work is threefold: studying the performance of these algorithms when tested on a different corpus from that they were trained on; exploring their ability to tune to new domains, and demonstrating empirically that the Lazy-Boosting algorithm outperforms state-of-the-art supervised WSD algorithms in both previous situations."
W00-0706,A Comparison between Supervised Learning Algorithms for Word Sense Disambiguation,2000,30,26,3,1,51637,gerard escudero,Fourth Conference on Computational Natural Language Learning and the Second Learning Language in Logic Workshop,0,"This paper describes a set of comparative experiments, including cross-corpus evaluation, between five alternative algorithms for supervised Word Sense Disambiguation (WSD), namely Naive Bayes, Exemplar-based learning, SNoW, Decision Lists, and Boosting. Two main conclusions can be drawn: 1) The LazyBoosting algorithm outperforms the other four state-of-the-art algorithms in terms of accuracy and ability to tune to new domains; 2) The domain dependence of WSD systems seems very strong and suggests that some kind of adaptation or tuning is required for cross-corpus application."
W98-0709,Using {W}ord{N}et for Building {W}ord{N}ets,1998,10,48,2,0,55199,xavier farreres,Usage of {W}ord{N}et in Natural Language Processing Systems,0,"This paper summar ises a set of methodologies and techniques for the fast construction of multilingual WordNets. The English WordNet is used in this approach as a backbone for Catalan and Spanish WordNets and as a lexical knowledge resource for several subtasks. 1 Motivation and Introduction One of the main issues in last years as regards NLP activit ies is the i nc r ea s ing ly fast development of generic language resources. A lot of such resources, including both software and l ingware items (lexicons, lexical databases, grammars, corpora marked in several ways) have been made available for research a n d industrial applications. Special interest presents, for knowledge-based NLP tasks, the availability of wide coverage ontologies. Most known ontologies (as GUM, CYC, ONTOS, MICROKOSMOS, EDR or WORDNET, see [Gomez 98] for an extensive survey) defe r in great extent on several characteristics (e.g. broad coverage vs. domain specific, lexicaUy oriented vs. conceptually oriented, granularity, kind of information placed in nodes, kind of relations, way of building, etc.). It is clear, however, that for a wide range of applications, WordNet (WN) [Miller 90] as become a de-facto standard. The success of WordNet has determined the emergence of several projects that aim the construction of WordNets for other languages than English (e.g., [Hamp & Feldweg 97], [Artale et al. 97]) or to develop multilingual WordNets (the mos t important project in this line is EuroWordNet (EWN)I). lhttp://www.let.uva.rd/~ewn/The aim of EWN vroject is to braid a multi.lingual database with WordN'ets for several european languages (in the first phase, Dutch, Italian and Spanish in addltion to English). The construction of a WN for a language Lg (LgWN) can be tackled in d i f fe ren t ways according to the lexical sources available. Of course the manual construction can be undertaken quite straightforwardly and leads to the best results in terms of accuracy, but has the important drawback of its cost. So, other approaches have been carried out taking profi t of available resources in fully automatic or semi-automatic ways. Which are these lexical resources? Basically four kinds of resources have been used: 1) English WN (EnWN0, as an initial skeleton for trying to attach the words of Lg to it, 2) a l ready existing taxonomies of Lg (both at word and at sense level), 3) bilingual (English and Lg) and 4) monolingual (Lg) dictionaries. All the approaches using EnWN as skeleton are based on the assumption of a close conceptual similarity between English and Lg, in such a way that most of the structure (relations) in EnWN could be maintained for LgWN. In the case of bilingual dictionaries the usual approach is to try to link the English counterpart of entries to synsets in EnWN and to assume that the entry can be ]inked to the same synset. Monolingual dictionaries have been used basically as a source for extracting taxonomic (hypemym) links between words (or senses [Bruce & Guthrie 92], [Rigau et al. 97]) and in lower extent for extracting other kinds of semantic relations [Richardson 97] (e.g. meronymic links). Once a taxonomy of Lg (already existing or built from a monolingual MILD) is available, the task can consist of 1) enriching the taxonomic structure with other semantic links (manually or automatically), as is the case of bu i ld ing individual WNs, or 2) merging this structure with other already existing ontologies (as EnWN or EWN). This paper presents our approach to the construction of WNs for two languages, Spanish and Catalan, and linking the first one to EWN. We have developed a methodology that uses as core source EnWN 2. The methodology implies 1) 2We have used WordNet 1.5. 65"
P98-2181,Building Accurate Semantic Taxonomies from Monolingual {MRD}s,1998,13,30,1,1,6129,german rigau,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"This paper presents a method that conbines a set of unsupervised algorithms in order to accurately build large taxonomies from any machine-readable dictionary (MRD). Our aim is to profit from conventional MRDs, with no explicit semantic coding. We propose a system that 1) performs fully automatic extraction of taxonomic links from MRD entries and 2) ranks the extracted relations in a way that selective manual refinement is allowed. Tested accuracy can reach around 100% depending on the degree of coverage selected, showing that taxonomy building is not limited to structured dictionaries such as LDOCE."
C98-2176,Building Accurate Semantic Taxonomies Monolingual {MRD}s,1998,13,30,1,1,6129,german rigau,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"This paper presents a method that conbines a set of unsupervised algorithms in order to accurately build large taxonomies from any machine-readable dictionary (MRD). Our aim is to profit from conventional MRDs, with no explicit semantic coding. We propose a system that 1) performs fully automatic extraction of taxonomic links from MRD entries and 2) ranks the extracted relations in a way that selective manual refinement is allowed. Tested accuracy can reach around 100% depending on the degree of coverage selected, showing that taxonomy building is not limited to structured dictionaries such as LDOCE."
P97-1007,Combining Unsupervised Lexical Knowledge Methods for Word Sense Disambiguation,1997,23,62,1,1,6129,german rigau,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"This paper presents a method to combine a set of unsupervised algorithms that can accurately disambiguate word senses in a large, completely untagged corpus. Although most of the techniques for word sense resolution have been presented as stand-alone, it is our belief that full-fledged lexical ambiguity resolution should combine several information sources and techniques. The set of techniques have been applied in a combined way to disambiguate the genus terms of two machine-readable dictionaries (MRD), enabling us to construct complete taxonomies for Spanish and French. Texted accuracy is above 80% overall and 95% for two-way ambiguous genus terms, showing that texonomy building is not limited to structured dictionaries such as LDOCE."
C96-1005,Word Sense Disambiguation using Conceptual Density,1996,17,372,2,0.0811758,8824,eneko agirre,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"This paper present a method for the resolution of lexical ambiguity of nouns and its automatic evaluation over the Brown Corpus. The method relies on the use of the wide-coverage noun taxonomy of WordNet and the notion of conceptual distance among concepts, captured by a Conceptual Density formula developed for this purpose. This fully automatic method requires no hand coding of lexical entries, hand tagging of text nor any kind of training process. The results of the experiments have been automatically evaluted against SemCor, the sense-tagged version of the Brown Corpus."
C94-1052,{TGE}: Tlinks Generation Environment,1994,9,19,3,0,49713,alicia ageno,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper describes the enhancements made, within a unification framework, based on typed feature structures, in order to support linking of lexical entries to their translation equivalents. To help this task we have developed an interactive environment: TGE. Several experiments, corresponding to rather closed semantic domains, have been developed in order to generate lexical cross-relations between English and Spanish."
A92-1044,{SEISD}: An environment for extraction of Semantic Information from on-line dictionaries,1992,0,9,4,0,49713,alicia ageno,Third Conference on Applied Natural Language Processing,0,"Knowledge Acquisition constitutes a main problem as regards the development of real Knowledge-based systems. This problem has been dealt with in a variety of ways. One of the most promising paradigms is based on the use of already existing sources in order to extract knowledge from them semiautomatically which will then be used in Knowledge-based applications. The Acquilex Project, within which we are working, follows this paradigm. The basic aim of Acquilex is the development of techniques and methods in order to use Machine Readable Dictionaries (MRD) * for building lexical components for Natural Language Processing Systems. SEISD (Sistema de Extracci6n de Informaci6n Semfintica de Diccionarios) is an environment for extracting semantic information from MRDs [Agent et al. 91b]. The system takes as its input a Lexical Database (LDB) where all the information contained in the MRD has been stored in an structured format. The extraction process is not fully automatic. To some extent, the choices made by the system must be both validated and confirmed by a human expert. Thus, an interactive environment must be used for performing such a task. One of the main contribution of our system lies in the way it guides the interactive process, focusing on the choice points and providing access to the information relevant to decision taking. System performance is controlled by a set of weighted heuristics that supplies the lack of algorithmic criteria or their vagueness in several crucial decision points. We will now summarize the most important characteristics of our system: xe2x80xa2 An underlying methodology for semantic extraction from lexical sources has been developped taking into account the characteristics of LDB and the intented semantic features to be extracted. xe2x80xa2 The Environment has been conceived as a support for the Methodology. xe2x80xa2 The Environment allows both interactive and batch modes of performance. xe2x80xa2 Great attention has been paid to reusability. The design and implementation of the system has involved an intensive"
