2020.acl-main.623,D08-1112,0,\N,Missing
2020.acl-main.623,S17-2006,1,\N,Missing
2020.acl-main.623,P17-1066,0,\N,Missing
2020.acl-main.623,S17-2082,0,\N,Missing
2020.acl-main.623,P18-1184,0,\N,Missing
2020.acl-main.623,C18-1288,1,\N,Missing
2020.acl-main.623,P19-1113,0,\N,Missing
2020.acl-main.630,I05-5002,0,0.0573444,"putational Linguistics 2 Datasets and Tasks We select popular benchmark datasets featuring different sizes (small vs. large), tasks (QA vs. paraphrase detection) and sentence lengths (short vs. long) as summarised in Table 1. Examples for each dataset are provided in Appendix A. similarity between two sentences in a binary classification task. We use a binary classification setup as this is more generic and applies to all above datasets. 3 tBERT 3.1 MSRP The Microsoft Research Paraphrase dataset (MSRP) contains pairs of sentences from news websites with binary labels for paraphrase detection (Dolan and Brockett, 2005). SemEval The SemEval CQA dataset (Nakov et al., 2015, 2016, 2017) comprises three subtasks based on threads and posts from the online expat forum Qatar Living.2 Each subtask contains an initial post as well as 10 possibly relevant posts with binary labels and requires to rank relevant posts above non-relevant ones. In subtask A, the posts are questions and comments from the same thread, in an answer ranking scenario. Subtask B is question paraphrase ranking. Subtask C is similar to A but comments were retrieved from an external thread, which increases the difficulty of the task. Quora The Quo"
2020.acl-main.630,S17-2053,0,0.0456546,"propose a novel topic-informed BERT-based architecture for pairwise semantic similarity detection and show that our model improves performance over strong neural baselines across a variety of English language datasets. We find that the addition of topics to BERT helps particularly with resolving domain-specific cases. 1 Introduction Modelling the semantic similarity between a pair of texts is a crucial NLP task with applications ranging from question answering to plagiarism detection. A variety of models have been proposed for this problem, including traditional featureengineered techniques (Filice et al., 2017), hybrid approaches (Wu et al., 2017; Feng et al., 2017; Koreeda et al., 2017) and purely neural architectures (Wang et al., 2017; Tan et al., 2018; Deriu and Cieliebak, 2017). Recent pretrained contextualised representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have led to impressive performance gains across a variety of NLP tasks, including semantic similarity detection. These models leverage large amounts of data to pretrain text encoders (in contrast to just individual word embeddings as in previous work) and have established a new pretrain-finetune paradigm. W"
2020.acl-main.630,N09-1032,0,0.102879,"between question-answer pairs which can be challenging due to the highly domain-specific language of certain online forums and low levels of direct text overlap between questions and answers. Topic models may provide additional signals for semantic similarity, as earlier feature-engineered models for semantic similarity detection successfully incorporated topics (Qin et al., 2009; Tran et al., 2015; Mihaylov and Nakov, 2016; Wu et al., 2017). They could be especially useful for dealing with domain-specific language since topic models have been exploited for domain adaptation (Hu et al., 2014; Guo et al., 2009). Moreover, recent work on neural architectures has shown that the integration of topics can yield improvements in other tasks such as language modelling (Ghosh et al., 2016), machine translation (Chen et al., 2016), and summarisation (Narayan et al., 2018; Wang et al., 2018). We therefore introduce a novel architecture for semantic similarity detection which incorporates topic models and BERT. More specifically, we make the following contributions: 1. We propose tBERT — a simple architecture combining topics with BERT for semantic similarity prediction (section 3).1 2. We demonstrate that tBE"
2020.acl-main.630,S15-2047,0,0.0226615,"r benchmark datasets featuring different sizes (small vs. large), tasks (QA vs. paraphrase detection) and sentence lengths (short vs. long) as summarised in Table 1. Examples for each dataset are provided in Appendix A. similarity between two sentences in a binary classification task. We use a binary classification setup as this is more generic and applies to all above datasets. 3 tBERT 3.1 MSRP The Microsoft Research Paraphrase dataset (MSRP) contains pairs of sentences from news websites with binary labels for paraphrase detection (Dolan and Brockett, 2005). SemEval The SemEval CQA dataset (Nakov et al., 2015, 2016, 2017) comprises three subtasks based on threads and posts from the online expat forum Qatar Living.2 Each subtask contains an initial post as well as 10 possibly relevant posts with binary labels and requires to rank relevant posts above non-relevant ones. In subtask A, the posts are questions and comments from the same thread, in an answer ranking scenario. Subtask B is question paraphrase ranking. Subtask C is similar to A but comments were retrieved from an external thread, which increases the difficulty of the task. Quora The Quora duplicate questions dataset contains more than 400"
2020.acl-main.630,D18-1206,0,0.139238,"larity, as earlier feature-engineered models for semantic similarity detection successfully incorporated topics (Qin et al., 2009; Tran et al., 2015; Mihaylov and Nakov, 2016; Wu et al., 2017). They could be especially useful for dealing with domain-specific language since topic models have been exploited for domain adaptation (Hu et al., 2014; Guo et al., 2009). Moreover, recent work on neural architectures has shown that the integration of topics can yield improvements in other tasks such as language modelling (Ghosh et al., 2016), machine translation (Chen et al., 2016), and summarisation (Narayan et al., 2018; Wang et al., 2018). We therefore introduce a novel architecture for semantic similarity detection which incorporates topic models and BERT. More specifically, we make the following contributions: 1. We propose tBERT — a simple architecture combining topics with BERT for semantic similarity prediction (section 3).1 2. We demonstrate that tBERT achieves improvements across multiple semantic similarity prediction datasets against a finetuned vanilla BERT and other neural models in both F1 and stricter evaluation metrics (section 5). 3. We show in our error analysis that tBERT’s gains are promin"
2020.acl-main.630,P19-1268,1,0.870735,"and encode them with two weight-sharing BiLSTMs, followed by max pooling and hidden layers. BERT We encode the sentence pair with BERT’s C vector (as in tBERT) followed by a softmax layer and finetune all layers for 3 epochs with early stopping. Following Devlin et al. (2019), we tune learning rates on the development set of each dataset.4 7049 5 Results MSRP Quora A Evaluation We evaluate systems based on F1 scores ( Table 3) as this is more reliable for datasets with imbalanced labels (e.g. SemEval C) than accuracy. We further report performance on difficult cases with non-obvious F1 score (Peinelt et al., 2019) which identifies challenging instances in the dataset based on lexical overlap and gold labels. Dodge et al. (2020) recently showed that early stopping and random seeds can have considerable impact on the performance of finetuned BERT models. We therefore use early stopping during finetuning and report average model performance across two seeds for BERT and tBERT models. F1 on cases with domain-specific words (total: 159/500) BERT .18 .00 .36 .36 .26 tBERT .67 .50 .62 .40 .58 (# of cases) (14) (7) (36) (41) (61) F1 on cases with non-standard spelling (total: 53/500) BERT .00 N/A .20 .71 .43 t"
2020.acl-main.630,N18-1202,0,0.0641112,"ERT helps particularly with resolving domain-specific cases. 1 Introduction Modelling the semantic similarity between a pair of texts is a crucial NLP task with applications ranging from question answering to plagiarism detection. A variety of models have been proposed for this problem, including traditional featureengineered techniques (Filice et al., 2017), hybrid approaches (Wu et al., 2017; Feng et al., 2017; Koreeda et al., 2017) and purely neural architectures (Wang et al., 2017; Tan et al., 2018; Deriu and Cieliebak, 2017). Recent pretrained contextualised representations such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have led to impressive performance gains across a variety of NLP tasks, including semantic similarity detection. These models leverage large amounts of data to pretrain text encoders (in contrast to just individual word embeddings as in previous work) and have established a new pretrain-finetune paradigm. While large improvements have been achieved on paraphrase detection (Tomar et al., 2017; Gong et al., 2018), semantic similarity detection in Community Question Answering (CQA) remains a challenging problem. CQA leverages user-generated content from question an"
2020.acl-main.630,W17-4121,0,0.0589503,"Missing"
2020.acl-main.630,W11-0329,0,0.015136,"For SemEval, we compare against the highest performing system of earlier work based on F1 score. As these models rely on hand-crafted dataset-specific features (providing an advantage on the small datasets), we also include the only neural system without manual features (Deriu and Cieliebak, 2017). For MSRP, we show a neural matching architecture (Pang et al., 2016). For Quora, we compare against the Interactive Inference Network (Gong et al., 2018) using accuracy, as no F1 has been reported. Siamese BiLSTM Siamese networks are a common neural baseline for sentence pair classification tasks (Yih et al., 2011; Wang et al., 2017). We embed both sentences with pretrained GloVe embeddings (concatenated with ELMo for BiLSTM + ELMo) and encode them with two weight-sharing BiLSTMs, followed by max pooling and hidden layers. BERT We encode the sentence pair with BERT’s C vector (as in tBERT) followed by a softmax layer and finetune all layers for 3 epochs with early stopping. Following Devlin et al. (2019), we tune learning rates on the development set of each dataset.4 7049 5 Results MSRP Quora A Evaluation We evaluate systems based on F1 scores ( Table 3) as this is more reliable for datasets with imba"
2021.acl-long.530,I17-3006,1,0.900877,"Missing"
2021.acl-long.530,W00-0403,0,0.578912,",tweetj ∈T i&lt;j Representative Tweet Approach We assume there exists a representative tweet able to summarise the content in the cluster, denoted as the representative tweet (i.e. tweetrep ). This is formally defined as: tweetrep (C) = arg min DKL (θ, tweeti ), tweeti ∈C where we compute the Kullback–Leibler divergence (DKL ) between the word distributions of the topic θ representing the cluster C and each tweet in C (Wan and Wang, 2016); we describe the computation of DKL in Appendix A. We also considered other text summarisation methods (Basave et al., 2014; Wan and Wang, 2016) such as MEAD (Radev et al., 2000) and Lexrank (Erkan and Radev, 8 Tweets are embedded into a vector space of TF-IDF representations within their corresponding cluster. 2004) to extract the best representative tweet, but our initial empirical study indicated DKL consistently finds the most appropriate representative tweet. In this case cluster coherence is defined as below and has linear time complexity O(|T |): 1 X f (C) = M(tweeti , tweetrep ). |T | tweeti ∈T As S = {(tweet, tweetrep ) |tweet ∈ T }T × T , the coherence of a cluster is heavily influenced by the correct identification of the representative tweet. 4.3 Graph App"
2021.acl-long.530,W15-1212,0,0.0176938,"topic coherence metrics, they are not as consistent as TGMs. TGMs are more reliable than all other metrics considered for capturing thematic coherence in microblog clusters due to being less sensitive to the effect of time windows. 1 Introduction As social media gains popularity for news tracking, unfolding stories are accompanied by a vast spectrum of reactions from users of social media platforms. Topic modelling and clustering methods have emerged as potential solutions to challenges of filtering and making sense of large volumes of microblog posts (Rosa et al., 2011; Aiello et al., 2013; Resnik et al., 2015; Surian et al., 2016). Providing a way to access easily a wide range of reactions around a topic or event has the potential to help those, such as journalists (Tolmie et al., 2017), police (Procter et al., 2013), health (Furini and Menegoni, 2018) and public safety professionals (Procter et al., 2020), who increasingly rely on social media to detect and monitor progress of events, public opinion and spread of misinformation. Recent work on grouping together views about tweets expressing opinions about the same entities has obtained clusters of tweets by leveraging two topic models in a hierar"
2021.acl-long.530,2020.acl-main.704,0,0.334183,"ng, pages 6800–6814 August 1–6, 2021. ©2021 Association for Computational Linguistics mal balance between coherence and diversity, such that resulting topics describe a logical exposition of views and beliefs with a low level of duplication. Here we evaluate thematic coherence in microblogs on the basis of topic coherence metrics, while also using research in text generation evaluation to assess semantic similarity and thematic relatedness. We consider a range of state-of-the-art text generation metrics (TGMs), such as BERTScore (Zhang et al., 2019), MoverScore (Zhao et al., 2019) and BLEURT (Sellam et al., 2020), which we repurpose for evaluating thematic coherence in microblogs and correlate them with assessments of coherence by journalist experts. The main contributions of this paper are: • We define the task of assessing thematic coherence in microblogs and use it as the basis for creating microblog clusters (Sec. 3). • We provide guidelines for the annotation of thematic coherence in microblog clusters and construct a dataset of clusters annotated for thematic coherence spanning two different domains (political tweets and COVID-19 related tweets). The dataset is annotated by journalist experts an"
2021.acl-long.530,P16-1217,0,0.020004,"r, it is computationally expensive as it requires O(|T |2 ) operations. Formally, given a TGM M, we define this approach as: X 1 f (C) = |T | · M(tweeti , tweetj ). 2 4.2 tweeti ,tweetj ∈T i&lt;j Representative Tweet Approach We assume there exists a representative tweet able to summarise the content in the cluster, denoted as the representative tweet (i.e. tweetrep ). This is formally defined as: tweetrep (C) = arg min DKL (θ, tweeti ), tweeti ∈C where we compute the Kullback–Leibler divergence (DKL ) between the word distributions of the topic θ representing the cluster C and each tweet in C (Wan and Wang, 2016); we describe the computation of DKL in Appendix A. We also considered other text summarisation methods (Basave et al., 2014; Wan and Wang, 2016) such as MEAD (Radev et al., 2000) and Lexrank (Erkan and Radev, 8 Tweets are embedded into a vector space of TF-IDF representations within their corresponding cluster. 2004) to extract the best representative tweet, but our initial empirical study indicated DKL consistently finds the most appropriate representative tweet. In this case cluster coherence is defined as below and has linear time complexity O(|T |): 1 X f (C) = M(tweeti , tweetrep ). |T |"
2021.acl-long.530,E17-1046,1,0.863891,"2016). Providing a way to access easily a wide range of reactions around a topic or event has the potential to help those, such as journalists (Tolmie et al., 2017), police (Procter et al., 2013), health (Furini and Menegoni, 2018) and public safety professionals (Procter et al., 2020), who increasingly rely on social media to detect and monitor progress of events, public opinion and spread of misinformation. Recent work on grouping together views about tweets expressing opinions about the same entities has obtained clusters of tweets by leveraging two topic models in a hierarchical approach (Wang et al., 2017b). The theme of such clusters can either be represented by their top-N highest-probability words or measured by the semantic similarity among the tweets. One of the questions regarding thematic clusters is how well the posts grouped together relate to each other (thematic coherence) and how useful such clusters can be. For example, the clusters can be used to discover topics that have low coverage in traditional news media (Zhao et al., 2011). Wang et al. (2017a) employ the centroids of Twitter clusters as the basis for topic specific temporal summaries. The aim of our work is to identify rel"
2021.acl-long.530,2020.emnlp-main.138,0,0.0218449,"of Lau and Baldwin (2016), who consider the mean topic coherence over several topic cardinalities |W |∈ {5, 10, 15, 20}. Another approach to topic coherence involves detecting intruder words given a set of topic words, an intruder and a document. If the intruder is identified correctly then the topic is considered coherent. Researchers have explored varying the number of ‘intruders’ (Morstatter and Liu, 2018) and automating the task of intruder detection (Lau et al., 2014). There is also work on topic diversity (Nan et al., 2019). However, there is a tradeoff between diversity and coherence (Wu et al., 2020), meaning high diversity for topic modelling is likely to be in conflict with thematic coherence, the main focus of the paper. Moreover, we are ensuring semantic diversity of microblog clusters through our sampling strategy (See Sec. 3.4). Text Generation Metrics: TGMs have been of great use in applications such as machine translation (Zhao et al., 2019; Zhang et al., 2019; Guo and Hu, 2019; Sellam et al., 2020), text summarisation (Zhao et al., 2019) and image captioning (Vedantam et al., 2015; Zhang et al., 2019; Zhao et al., 2019), where a machine generated response is evaluated against gro"
2021.acl-long.530,D19-1053,0,0.342126,"ce on Natural Language Processing, pages 6800–6814 August 1–6, 2021. ©2021 Association for Computational Linguistics mal balance between coherence and diversity, such that resulting topics describe a logical exposition of views and beliefs with a low level of duplication. Here we evaluate thematic coherence in microblogs on the basis of topic coherence metrics, while also using research in text generation evaluation to assess semantic similarity and thematic relatedness. We consider a range of state-of-the-art text generation metrics (TGMs), such as BERTScore (Zhang et al., 2019), MoverScore (Zhao et al., 2019) and BLEURT (Sellam et al., 2020), which we repurpose for evaluating thematic coherence in microblogs and correlate them with assessments of coherence by journalist experts. The main contributions of this paper are: • We define the task of assessing thematic coherence in microblogs and use it as the basis for creating microblog clusters (Sec. 3). • We provide guidelines for the annotation of thematic coherence in microblog clusters and construct a dataset of clusters annotated for thematic coherence spanning two different domains (political tweets and COVID-19 related tweets). The dataset is a"
2021.clpsych-1.15,N19-1423,0,0.00865772,"hin a session to predict the occurrence of alliance rupture based on the perception of both the therapist and the client. We yield good performance and showcase the potential for using NLP for aiding therapists in Finally, we inspect the language used within rup- identifying rupture during psychotherapy sessions. ture vs non-rupture sessions. We are particularly in- In the future we plan to build on our initial findterested the sessions that were labelled as ‘rupture’ ings by incorporating contextual language models by the client only (see Table 2) and also correctly (Chriqui and Yahav, 2021; Devlin et al., 2019) and 125 I had to pick up my kind from his music lessons and I was busy and I asked my husband if he could take the child and he said he was busy and that I was the one who should give up. Why do you think this is happening? I always have to run from one thing to another. He’s busy with his own affairs. But what did you ask? I’m not in focus. No, no, it’s okay, please continue. I feel like I was unlucky in life. Yesterday I needed his help, but he is never there to help or hug me. I just don’t have anyone who can do that for me. It’s hard. I need someone who can support me. I never had such su"
2021.clpsych-1.15,C16-1033,0,0.0296445,"Missing"
2021.clpsych-1.15,2020.findings-emnlp.337,0,0.0179368,"e existing monitoring tools to inform therapists about meaningful instances of alliance rupture that went unrecognized. • Subtler and more implicit content associated with a rupture would be captured, increasing our understanding of the specific moments and reasons for it. • Alliance rupture would be captured in a costeffective manner. Contributions: To the best of our knowledge there is no work on capturing alliance rupture automatiChallenges in capturing alliance rupture: Most cally from transcribed therapist or client utterances. studies have explored alliance ruptures using self- Recently Goldberg et al. (2020) used 1,235 tranreports at relatively low time resolution (once each scribed recorded sessions with client reported alsession, typically weekly). However, ruptures may liance to automatically predict per session alliance occur at higher time resolutions within a session using the text from both therapist and client. They (Coutinho et al., 2014). In addition, standardized used four variants of a linear regression model with subjective measures have critical shortcomings, in- linguistic features from either the therapist or client. cluding the extent of participants’ self-insights, Their best pe"
2021.eacl-main.169,D19-1005,0,0.0494712,"Missing"
2021.eacl-main.21,P98-1012,0,0.862855,"annotation for CD2 CR including a novel sampling mechanism for calculating inter-annotator agreement (Section 3.4). 2 Co-reference Resolution Intra-document co-reference resolution is a well understood task with mature training data sets (Weischedel et al., 2013) and academic tasks (Recasens et al., 2010). The current state of the art model by Joshi et al. (2020) is based on Lee et al. (2017, 2018) and uses a modern BERT-based (Devlin et al., 2019) architecture. Comparatively, CDCR, which involves co-reference resolution across multiple documents, has received less attention in recent years (Bagga and Baldwin, 1998; Rao et al., 2010; Dutta and Weikum, 2015; Barhom et al., 2019). Cattan et al. (2020) jointly learns both entity and event co-reference tasks, achieving current state of the art performance for CDCR, and as such provides a strong baseline for experiments in CD2 CR. Both Cattan et al. (2020) and Barhom et al. (2019) models are trained and evaluated using the ECB+ corpus (Cybulska and Vossen, 2014) which contains news articles annotated with both entity and event mentions. • A novel task setting for CDCR that is more challenging than those that already exist due to linguistic variation between"
2021.eacl-main.21,P19-1409,1,0.946256,"on (CDCR) is the task of recognising when multiple documents mention and refer to the same real-world entity or concept. CDCR is a useful NLP process that has many downstream applications. For example, CDCR carried out on separate news articles that refer to the same politician can facilitate inter-document sentence alignment required for stance detection and natural language inference models. Furthermore, CDCR can improve information retrieval and multi-document summarisation by grouping documents based on the entities that are mentioned within them. Recent CDCR work (Dutta and Weikum, 2015; Barhom et al., 2019; Cattan et al., 2020) has primarily focused on resolution of entity mentions across news articles. Despite differences in tone and political alignment, most news articles are relatively similar in terms of grammatical and lexical structure. Work based on modern transformer networks such as BERT (Devlin et al., 2019) and ElMo (Peters et al., 2018) have been pre-trained on large news corpora and are therefore well suited to news-based CDCR (Barhom et al., 2019). However, there are cases where CDCR across documents from different domains (i.e. that differ much more significantly in style, vocabu"
2021.eacl-main.21,D19-1371,0,0.0199462,"nilla (CA-V) Baseline Here we aim to evaluate whether training the CA model on the CD2 CR dataset from the RoBERTa baseline without first training on the ECB+ corpus allows it to fit well to the new task setting. We re-initialise the CA encoder (Section 4.2) using weights from RoBERTa (Liu et al., 2019) and randomly initialise the remaining model parameters. We then train the model on the CD2 CR corpus for up to 20 epochs with early stopping with pseudorandom sub-sampling as above. 4.5 CA - SciBERT (CA-S) Baseline This model is the same as CA-V but we replace the RoBERTa encoder with SciBERT (Beltagy et al., 2019), a version of BERT pre-trained on scientific literature in order to test whether the scientific terms and context captured by SciBERT improve performance at the CD2 CR task compared to RoBERTa. Similarly to CA-V in section 4.4, we initialise the BERT model with weights from SciBERTscivocab-uncased (Beltagy et al., 2019) and randomly initialise the remaining model parameters, training on the CD2 CR corpus for up to 20 epochs with early stopping. 5 the “gold standard” in all experiments rather than using the end-to-end Named Entity Recognition capabilities provided by some of the models. We eva"
2021.eacl-main.21,cybulska-vossen-2014-using,0,0.13871,"2018) and uses a modern BERT-based (Devlin et al., 2019) architecture. Comparatively, CDCR, which involves co-reference resolution across multiple documents, has received less attention in recent years (Bagga and Baldwin, 1998; Rao et al., 2010; Dutta and Weikum, 2015; Barhom et al., 2019). Cattan et al. (2020) jointly learns both entity and event co-reference tasks, achieving current state of the art performance for CDCR, and as such provides a strong baseline for experiments in CD2 CR. Both Cattan et al. (2020) and Barhom et al. (2019) models are trained and evaluated using the ECB+ corpus (Cybulska and Vossen, 2014) which contains news articles annotated with both entity and event mentions. • A novel task setting for CDCR that is more challenging than those that already exist due to linguistic variation between different domains and document types (we call this CD2 CR). 1 Related Work 2.2 Entity Linking (EL) focuses on alignment of mentions in documents to resources in an external knowledge resource (Ji et al., 2010) such as SNOMED CT3 or DBPedia4 . EL is challenging due to the large number of pairwise comparisons between document mentions and knowledge resource entities that may need to be carried out."
2021.eacl-main.21,N19-1423,0,0.46707,"alignment required for stance detection and natural language inference models. Furthermore, CDCR can improve information retrieval and multi-document summarisation by grouping documents based on the entities that are mentioned within them. Recent CDCR work (Dutta and Weikum, 2015; Barhom et al., 2019; Cattan et al., 2020) has primarily focused on resolution of entity mentions across news articles. Despite differences in tone and political alignment, most news articles are relatively similar in terms of grammatical and lexical structure. Work based on modern transformer networks such as BERT (Devlin et al., 2019) and ElMo (Peters et al., 2018) have been pre-trained on large news corpora and are therefore well suited to news-based CDCR (Barhom et al., 2019). However, there are cases where CDCR across documents from different domains (i.e. that differ much more significantly in style, vocabulary and structure) is useful. One such example is the task of resolving references to concepts across scientific papers and related news articles. This can help scientists understand how their work is being presented to the public by mainstream media or facilitate fact checking of journalists’ work (Wadden et al., 2"
2021.eacl-main.21,Q15-1002,0,0.149971,"nt co-reference resolution (CDCR) is the task of recognising when multiple documents mention and refer to the same real-world entity or concept. CDCR is a useful NLP process that has many downstream applications. For example, CDCR carried out on separate news articles that refer to the same politician can facilitate inter-document sentence alignment required for stance detection and natural language inference models. Furthermore, CDCR can improve information retrieval and multi-document summarisation by grouping documents based on the entities that are mentioned within them. Recent CDCR work (Dutta and Weikum, 2015; Barhom et al., 2019; Cattan et al., 2020) has primarily focused on resolution of entity mentions across news articles. Despite differences in tone and political alignment, most news articles are relatively similar in terms of grammatical and lexical structure. Work based on modern transformer networks such as BERT (Devlin et al., 2019) and ElMo (Peters et al., 2018) have been pre-trained on large news corpora and are therefore well suited to news-based CDCR (Barhom et al., 2019). However, there are cases where CDCR across documents from different domains (i.e. that differ much more significa"
2021.eacl-main.21,D19-1620,0,0.0196852,"he scientific paper (abstract). For each document pair, we ask the annotators to identify co-referent mentions between the scientific paper abstract and a summary of the news article that is of similar length (e.g. 5-10 sentences). Scientific paper abstracts act as a natural summary of a scientific work and have been used as a strong baseline or even a gold-standard in scientific summarisation tasks (Liakata et al., 2013). Furthermore, abstracts are almost always available rather than behind paywalls like full text articles. For news summarisation, we used a state-of-the-art extractive model (Grenander et al., 2019) to extract sentences forming a summary of the original text. This model provides a summary de-biasing mechanism preventing it from focusing on specific parts of the full article, preserving the summary’s informational authenticity as much as possible. The difference in style between the two documents is preserved by both types of summary since abstracts are written in the same scientific style as full papers and the extractive summaries use verbatim excerpts of the original news articles. 3.3 Generation of pairs for annotation To populate our annotation tool, we generate pairs of candidate cr"
2021.eacl-main.21,2020.coling-main.118,0,0.0405676,"Missing"
2021.eacl-main.21,D17-1018,0,0.304597,"source English language CD2 CR dataset with 7602 co-reference pair annotations over 528 documents and detailed 11 page annotation guidelines (section 3.1). • A novel annotation tool to support ongoing data collection and annotation for CD2 CR including a novel sampling mechanism for calculating inter-annotator agreement (Section 3.4). 2 Co-reference Resolution Intra-document co-reference resolution is a well understood task with mature training data sets (Weischedel et al., 2013) and academic tasks (Recasens et al., 2010). The current state of the art model by Joshi et al. (2020) is based on Lee et al. (2017, 2018) and uses a modern BERT-based (Devlin et al., 2019) architecture. Comparatively, CDCR, which involves co-reference resolution across multiple documents, has received less attention in recent years (Bagga and Baldwin, 1998; Rao et al., 2010; Dutta and Weikum, 2015; Barhom et al., 2019). Cattan et al. (2020) jointly learns both entity and event co-reference tasks, achieving current state of the art performance for CDCR, and as such provides a strong baseline for experiments in CD2 CR. Both Cattan et al. (2020) and Barhom et al. (2019) models are trained and evaluated using the ECB+ corpus"
2021.eacl-main.21,N18-2108,0,0.132247,"Missing"
2021.eacl-main.21,2020.acl-main.738,0,0.0172838,"ere shown in bold font whereas mentions already flagged as co-referent were shown in green. This enabled annotators to understand the implications for existing co-reference chains before responding (see Figure 3). Questions were generated and ranked via our task generation pipeline (see Section 3.3 above). We added two additional features to our annotation interface to improve annotators’ experience and to speed up the annotation process. Firstly, if the candidate pair is marked as co-referent, the user is allowed to add more mentions to the coreference cluster at once. Secondly, inspired by (Li et al., 2020), if the automatically shown mention pair is not co-referent, the user can select a different mention that is co-referent. The upstream automated mention detection mechanism can sometimes introduce incomplete or erroneous mentions, leading to comparisons that don’t make sense or that are particularly difficult. Therefore, annotators can also move or resize the mention spans they are annotating. We use string offsets of mention span pairs to tokens to check that they do not overlap with each other in order to prevent the creation of duplicates. Figure 1 shows an illustrated example of the gener"
2021.eacl-main.21,D13-1070,1,0.735366,"a very low chance for good inter-annotator agreement (IAA). We therefore decided to simplify the task by asking annotators to compare summaries of the newspaper article (5-10 sentences long) and the scientific paper (abstract). For each document pair, we ask the annotators to identify co-referent mentions between the scientific paper abstract and a summary of the news article that is of similar length (e.g. 5-10 sentences). Scientific paper abstracts act as a natural summary of a scientific work and have been used as a strong baseline or even a gold-standard in scientific summarisation tasks (Liakata et al., 2013). Furthermore, abstracts are almost always available rather than behind paywalls like full text articles. For news summarisation, we used a state-of-the-art extractive model (Grenander et al., 2019) to extract sentences forming a summary of the original text. This model provides a summary de-biasing mechanism preventing it from focusing on specific parts of the full article, preserving the summary’s informational authenticity as much as possible. The difference in style between the two documents is preserved by both types of summary since abstracts are written in the same scientific style as f"
2021.eacl-main.21,Q15-1023,0,0.0317577,"• A novel task setting for CDCR that is more challenging than those that already exist due to linguistic variation between different domains and document types (we call this CD2 CR). 1 Related Work 2.2 Entity Linking (EL) focuses on alignment of mentions in documents to resources in an external knowledge resource (Ji et al., 2010) such as SNOMED CT3 or DBPedia4 . EL is challenging due to the large number of pairwise comparisons between document mentions and knowledge resource entities that may need to be carried out. Raiman and Raiman (2018) provide state of the art performance by building on Ling et al. (2015)’s work in which an entity type system is used to limit the number of required pairwise comparisons to related types. Yin et al. (2019) achieved comparable results using a graph-traversal method to similarly constrain the problem space to candidates within a similar graph neighbourhood. EL can be considered a narrow sub-task of CDCR since it cannot resolve novel and rare entities or pronouns (Shen et al., 2015). Moreover EL’s dependency on expensiveto-maintain external knowledge graphs is also problematic when limited human expertise is available. 3 DOI: 10.1101/2020.03.16.20036145 https://tin"
2021.eacl-main.21,N18-1202,0,0.296749,"detection and natural language inference models. Furthermore, CDCR can improve information retrieval and multi-document summarisation by grouping documents based on the entities that are mentioned within them. Recent CDCR work (Dutta and Weikum, 2015; Barhom et al., 2019; Cattan et al., 2020) has primarily focused on resolution of entity mentions across news articles. Despite differences in tone and political alignment, most news articles are relatively similar in terms of grammatical and lexical structure. Work based on modern transformer networks such as BERT (Devlin et al., 2019) and ElMo (Peters et al., 2018) have been pre-trained on large news corpora and are therefore well suited to news-based CDCR (Barhom et al., 2019). However, there are cases where CDCR across documents from different domains (i.e. that differ much more significantly in style, vocabulary and structure) is useful. One such example is the task of resolving references to concepts across scientific papers and related news articles. This can help scientists understand how their work is being presented to the public by mainstream media or facilitate fact checking of journalists’ work (Wadden et al., 2020). A chatbot or recommender"
2021.eacl-main.21,D18-1026,0,0.0221577,"Missing"
2021.eacl-main.21,Q15-1025,0,0.0157882,"rpora to learn context-aware word embeddings that can be used for downstream NLP tasks. However, these models do not learn about formal lexical constraints, often conflating different types of semantic relatedness (Ponti et al., 2018; Lauscher et al., 2020). This is a weakness of all distributional language models that is particularly problematic in the context of CD2 CR for entity mentions that are related but not co-referent (e.g. “Mars” and “Jupiter”) as shown in section 5. A number of solutions have been proposed for adding lexical knowledge to static word embeddings (Yu and Dredze, 2014; Wieting et al., 2015; Ponti et al., 2018) but contextual language models have received comparatively less attention. Lauscher et al (2020) propose adding a lexical relation classification step to BERT’s language model pre-training phase to allow the model to integrate both lexical and distributional knowledge. Their model, LIBERT, has been shown to facilitate statistically-significant performance boosts on a variety of downstream NLP tasks. 3 Dataset creation Our dataset is composed of pairs of news articles and scientific papers gathered automatically (Section 3.1). Our annotation process begins by obtaining sum"
2021.eacl-main.21,P18-4004,1,0.830395,"or scoring (Section 3.4). Annotation quality is measured on an ongoing basis as new candidates are added to the system (Section 3.5). 3.1 Documents 300 142 86 Mentions 4,604 1,821 1,177 Clusters 426 199 101 Table 1: Total individual documents, mentions, coreference clusters of each subset excluding singletons. and is split into training, development and test sets (statistics for each subset are provided in Table 1). Each pair of documents consists of a scientific paper and a newspaper article that discusses the scientific work. In order to detect pairs of documents, we follow the approach of (Ravenscroft et al., 2018), using approximate matching of author name and affiliation metadata, date of publishing and exact DOI matching where available to connect news articles to scientific publications. We built a web scraper that scans for new articles from the ‘Science’ and ‘Technology’ sections of 3 well-known online news outlets (BBC, The Guardian, New York Times) and press releases from Eurekalert, a widely popular scientific press release aggregator. Once a newspaper article and related scientific paper are detected, the full text from the news article and the scientific paper abstract and metadata are stored"
2021.eacl-main.21,W09-2411,0,0.112286,"Missing"
2021.eacl-main.21,2020.acl-main.442,0,0.0243761,"erent’ respectively B3 F1 P R F1 0.84 0.63 0.68 0.65 0.81 0.56 0.53 0.55 suggests that disentangling these pairs is likely to be a challenging task for the downstream classification layer in the CA-V model. These challenges are less likely to occur in homogeneous corpora like ECB+ where descriptions and relationships remain consistent in detail and complexity. Table 5: MUC and B 3 results from running the CD2 CR baseline model (CA-V) on ECB+ dataset compared with original Cattan et al. (2020) (CA). Finally, the best model (CA-V) is analysed using a series of challenging test cases inspired by Ribeiro et al (2020). These test cases were created using 210 manually annotated mention-pairs found in the test subset of the CD2 CR corpus according to the type of relationship illustrated (Anaphora & Exophora, Subset relationships, paraphrases). We collected a balanced set of 30-40 examples of both co-referent and non-coreferentbut-challenging pairs for each type of relationship (exact numbers in Table 4). We then recorded whether the model correctly predicted co-reference for these pairs. The results along with illustrative examples of each relationship type are shown in Table 4. The results suggest that the"
2021.eacl-main.21,M95-1005,0,0.536732,"terature in order to test whether the scientific terms and context captured by SciBERT improve performance at the CD2 CR task compared to RoBERTa. Similarly to CA-V in section 4.4, we initialise the BERT model with weights from SciBERTscivocab-uncased (Beltagy et al., 2019) and randomly initialise the remaining model parameters, training on the CD2 CR corpus for up to 20 epochs with early stopping. 5 the “gold standard” in all experiments rather than using the end-to-end Named Entity Recognition capabilities provided by some of the models. We evaluate the models using the metrics described by Vilain et al. (1995) (henceforth MUC) and Bagga and Baldwin (1998) (henceforth B 3 ). MUC F1, precision and recall are defined in terms of pairwise co-reference relationships between each mention. B 3 , F1, precision and recall are defined in terms of presence or absence of specific entities in the cluster. When measuring B 3 , we remove entities with no co-references (singletons) from the evaluation to avoid inflation of results (Cattan et al., 2020). The threshold baseline (BCOS) gives the highest MUC recall but also poor MUC precision and poorest B 3 precision. The B 3 metric is highly specific with respect to"
2021.eacl-main.21,P14-2089,0,0.0378128,"rent in large text corpora to learn context-aware word embeddings that can be used for downstream NLP tasks. However, these models do not learn about formal lexical constraints, often conflating different types of semantic relatedness (Ponti et al., 2018; Lauscher et al., 2020). This is a weakness of all distributional language models that is particularly problematic in the context of CD2 CR for entity mentions that are related but not co-referent (e.g. “Mars” and “Jupiter”) as shown in section 5. A number of solutions have been proposed for adding lexical knowledge to static word embeddings (Yu and Dredze, 2014; Wieting et al., 2015; Ponti et al., 2018) but contextual language models have received comparatively less attention. Lauscher et al (2020) propose adding a lexical relation classification step to BERT’s language model pre-training phase to allow the model to integrate both lexical and distributional knowledge. Their model, LIBERT, has been shown to facilitate statistically-significant performance boosts on a variety of downstream NLP tasks. 3 Dataset creation Our dataset is composed of pairs of news articles and scientific papers gathered automatically (Section 3.1). Our annotation process b"
2021.findings-acl.341,N19-1423,0,0.0108314,"., 2018), which consists of Twitter rumours around 5 high profile real-world events. Statistics regarding the dataset can be found in the Appendix. The PHEME dataset was chosen as it is a particularly challenging dataset due to class imbalance and a leave-one-event out cross validation setting, reflecting a real-world evaluation scenario. Baseline Models We perform comparison of the proposed model SAVED with existing state-of-theart models (Kochkina et al., 2018; Li et al., 2019b; Cheng et al., 2020) and several strong baselines, described in this section. BERT We use the pretrained BERTBASE (Devlin et al., 2019), uncased, which consists of 12 selfattention layers, and returns a 768-dimension vector representation of a sentence. We generate BERT representations for each tweet in the conversation before feeding them into the Veracity Module. VAED is a version of SAVED, where the Topic Learning Module is reduced to only its VAE component, without the stance classifiers from Section 2.1. The loss is Lc + Lx + γLM I . VAED Without Disentanglement (VAE) is a simplified version of VAED, where the Topic Learning Module is reduced to only using loss from the context-enriched latent factor without the target m"
2021.findings-acl.341,C18-1284,0,0.0217335,"D model alone (with disentanglement, without any other modifications or stance/veracity classifiers) scores 0.363, showcasing the efficacy of disentanglement per se. The BERT-based model only outperformed VAE. The proposed SAVED model outperforms those of prior work on overall accuracy and the True and Unverified classes. However, results for the False class are rather low - which is in fact the case for most of the models in Table 1, with only Cheng et al. (2020) being an exception. Results of VAED+V are lower than that of SAVED, in line with the knowledge that stance is related to veracity (Dungs et al., 2018). This suggests that stance is also a worthwhile intermediate classification target. Event False True Unverified MacroF1 Charlie Hebdo Ferguson Germanwings Crash Ottawa Shooting Sydney Siege 0.223 0.129 0.033 0.058 0.157 0.505 0.080 0.520 0.735 0.700 0.324 0.906 0.289 0.119 0.140 0.351 0.372 0.281 0.304 0.332 Overall 0.164 0.642 0.531 0.434 Table 2: Per-fold evaluation results of SAVED. Per-fold Results Table 2 shows the per-fold results in our leave-one-event-out setting. Interestingly, the model tends to perform best on rumors of True veracity and worst on those which are False. Performance"
2021.findings-acl.341,Q19-1017,1,0.822952,"ssment. Given a rumour of unknown veracity introduced by a tweet in a conversation thread and the responses to it, our goal is to automatically determine the veracity of the rumour by assigning it one of the classes true, false, or unverified. Prior approaches to rumour veracity classification have primarily relied on careful feature engineering. For example, Li et al. (2019a) used meta-features such as user credibility together with more traditional features to top the leaderboard in SemEval 2019 Task 7 (Gorrell et al., 2019). This task encouraged teams to use the stances of responses to the Zeng et al. (2019) presented an unsupervised approach built on Variational Autoencoder (VAE) to jointly model topic content and discourse behaviour in microblog conversations. We propose a novel architecture which incorporates a VAE with adversarial learning to disentangle topics which are informative for stance classification from those which are not. We then derive tweet representations based on the word representations learned in the latent stance-dependent topic space. Our results show that using such tweet representations for rumour veracity classification achieves superior performance on the PHEME dataset"
2021.findings-acl.341,S19-2147,1,0.828716,"re is increasing need for machine learning algorithms to assist with rumour veracity assessment. Given a rumour of unknown veracity introduced by a tweet in a conversation thread and the responses to it, our goal is to automatically determine the veracity of the rumour by assigning it one of the classes true, false, or unverified. Prior approaches to rumour veracity classification have primarily relied on careful feature engineering. For example, Li et al. (2019a) used meta-features such as user credibility together with more traditional features to top the leaderboard in SemEval 2019 Task 7 (Gorrell et al., 2019). This task encouraged teams to use the stances of responses to the Zeng et al. (2019) presented an unsupervised approach built on Variational Autoencoder (VAE) to jointly model topic content and discourse behaviour in microblog conversations. We propose a novel architecture which incorporates a VAE with adversarial learning to disentangle topics which are informative for stance classification from those which are not. We then derive tweet representations based on the word representations learned in the latent stance-dependent topic space. Our results show that using such tweet representations"
2021.findings-acl.341,C18-1288,1,0.760705,"1 Department of Computer Science, University of Warwick, UK 2 Queen-Mary University of London, UK 3 Alan Turing Institute, UK {j.Dougrez-Lewis,yulan.he}@warwick.ac.uk {m.liakata,e.kochkina}@qmul.ac.uk Abstract rumour to assist in veracity classification, which has previously been shown to be predictive of rumour veracity (Dungs et al., 2018). A number of approaches (Kochkina et al., 2018; Li et al., 2019b) also showed benefits of using stance classification as an auxiliary task in a multitask learning setup. Some recent approaches exploit the structure of the conversation discussing a rumour. Kochkina et al. (2018) used LSTM to model linear branches extracted from the conversation tree, while Ma et al. (2018) and Bian et al. (2020) modelled a tree structure to capture information from responses. With the rapid growth of social media in the past decade, the news are no longer controlled by just a few mainstream sources. Users themselves create large numbers of potentially fictitious rumours, necessitating automated veracity classification systems. Here we present a novel approach towards automatically classifying rumours circulating on Twitter with respect to their veracity. We use a model built on Varia"
2021.findings-acl.341,S19-2148,0,0.0813606,"rumours online with the potential to influence and pose as news. Since it is impossible to manually check the vast volume of circulating tweets, there is increasing need for machine learning algorithms to assist with rumour veracity assessment. Given a rumour of unknown veracity introduced by a tweet in a conversation thread and the responses to it, our goal is to automatically determine the veracity of the rumour by assigning it one of the classes true, false, or unverified. Prior approaches to rumour veracity classification have primarily relied on careful feature engineering. For example, Li et al. (2019a) used meta-features such as user credibility together with more traditional features to top the leaderboard in SemEval 2019 Task 7 (Gorrell et al., 2019). This task encouraged teams to use the stances of responses to the Zeng et al. (2019) presented an unsupervised approach built on Variational Autoencoder (VAE) to jointly model topic content and discourse behaviour in microblog conversations. We propose a novel architecture which incorporates a VAE with adversarial learning to disentangle topics which are informative for stance classification from those which are not. We then derive tweet r"
2021.findings-acl.341,P19-1113,0,0.143205,"rumours online with the potential to influence and pose as news. Since it is impossible to manually check the vast volume of circulating tweets, there is increasing need for machine learning algorithms to assist with rumour veracity assessment. Given a rumour of unknown veracity introduced by a tweet in a conversation thread and the responses to it, our goal is to automatically determine the veracity of the rumour by assigning it one of the classes true, false, or unverified. Prior approaches to rumour veracity classification have primarily relied on careful feature engineering. For example, Li et al. (2019a) used meta-features such as user credibility together with more traditional features to top the leaderboard in SemEval 2019 Task 7 (Gorrell et al., 2019). This task encouraged teams to use the stances of responses to the Zeng et al. (2019) presented an unsupervised approach built on Variational Autoencoder (VAE) to jointly model topic content and discourse behaviour in microblog conversations. We propose a novel architecture which incorporates a VAE with adversarial learning to disentangle topics which are informative for stance classification from those which are not. We then derive tweet r"
2021.findings-acl.341,P18-1184,0,0.0290631,"an Turing Institute, UK {j.Dougrez-Lewis,yulan.he}@warwick.ac.uk {m.liakata,e.kochkina}@qmul.ac.uk Abstract rumour to assist in veracity classification, which has previously been shown to be predictive of rumour veracity (Dungs et al., 2018). A number of approaches (Kochkina et al., 2018; Li et al., 2019b) also showed benefits of using stance classification as an auxiliary task in a multitask learning setup. Some recent approaches exploit the structure of the conversation discussing a rumour. Kochkina et al. (2018) used LSTM to model linear branches extracted from the conversation tree, while Ma et al. (2018) and Bian et al. (2020) modelled a tree structure to capture information from responses. With the rapid growth of social media in the past decade, the news are no longer controlled by just a few mainstream sources. Users themselves create large numbers of potentially fictitious rumours, necessitating automated veracity classification systems. Here we present a novel approach towards automatically classifying rumours circulating on Twitter with respect to their veracity. We use a model built on Variational Autoencoder which disentangles the informational content of a tweet from the manner in wh"
2021.findings-emnlp.200,I05-5002,0,0.0113363,"NLP and involves modelling the semantic relationship between two sentences in a binary classification setup. We work with the following five widely used English language datasets which cover a range of sizes and tasks (including paraphrase detection, duplicate question identification and answer sentence selection, see Appendix A for details). MSRP The Microsoft Research Paraphrase dataset (MSRP) contains 5K pairs of sentences from news websites which were collected based on heuristics and an SVM classifier. Gold labels are based on human binary annotations for sentential paraphrase detection (Dolan and Brockett, 2005). SemEval The SemEval 2017 CQA dataset (Nakov et al., 2017) consists of three subtasks involving posts from the online forum Qatar Living3 . Each subtask provides an initial post as well as 10 posts which were retrieved by a search engine and annotated with binary labels by humans. The task requires the distinction between relevant and non-relevant posts. The original problem is a ranking setting, but since the gold labels are binary, we focus on a classification setup. In subtask A, the posts are questions and comments from the same thread, in an answer sentence selection setup (26K instances"
2021.findings-emnlp.200,N15-1184,0,0.056371,"Missing"
2021.findings-emnlp.200,S17-2053,0,0.0838483,"variety of application areas and technical duplicate question identification and answer senapproaches.We broadly categorise such approaches tence selection which require detecting the semaninto input-related, external and internal. Input modifications (Zhao et al., 2020; Singh et al., tic similarity between text pairs (Peinelt et al., 2020). Early semantic similarity methods used 2020; Lai et al., 2020; Ruan et al., 2020) adapt the feature-engineering techniques, exploring various information that is fed to BERT – e.g. feeding text triples separated by [SEP] tokens instead of sen- syntactic (Filice et al., 2017), semantic (Balchev et al., 2016) and lexical features (Tran et al., 2015; tence pairs as in Lai et al. (2020) – while leaving Almarwani and Diab, 2017). Subsequent work the architecture unchanged. Output modifications tried to model text pair relationships either based on (Xuan et al., 2020; Zhang et al., 2020) build on increasingly complex neural architectures (Deriu BERT’s pre-trained representation by adding external information after the encoding step – e.g. and Cieliebak, 2017; Wang et al., 2017; Tan et al., 2018) or by combining both approaches through combining it with additional seman"
2021.findings-emnlp.200,J15-4004,0,0.0725811,"Missing"
2021.findings-emnlp.200,P14-2050,0,0.310452,"tative analysis shows that counter-fitted embedding injection is particularly beneficial, with notable improvements on examples that require synonym resolution. 1 Introduction Before the rise of contextualised models, transfer of pre-trained information between datasets and tasks in NLP was based on word embeddings. Over many years, substantial effort was placed into the creation of such embeddings. While originally capturing mainly collocation patterns (Mikolov et al., 2013; Pennington et al., 2014), subsequent work enriched these embeddings with additional information, such as dependencies (Levy and Goldberg, 2014), subword information (Luong et al., 2013; Bojanowski et al., 2017) and semantic lexicons (Faruqui et al., 2015). As a result, there exists a wealth of pre-trained embedding resources for many languages in a unified format which could provide complementary information for contemporary pre-trained contextual models. Moreover, aligning contextual embeddings with static embeddings has shown to increase the performance of the former (Liu et al., 2020). We propose a new method for injecting pretrained linguistically-enriched embeddings into any layer of BERT. The model maps any word embeddings into"
2021.findings-emnlp.200,2020.emnlp-main.333,0,0.0165923,"(Mikolov et al., 2013; Pennington et al., 2014), subsequent work enriched these embeddings with additional information, such as dependencies (Levy and Goldberg, 2014), subword information (Luong et al., 2013; Bojanowski et al., 2017) and semantic lexicons (Faruqui et al., 2015). As a result, there exists a wealth of pre-trained embedding resources for many languages in a unified format which could provide complementary information for contemporary pre-trained contextual models. Moreover, aligning contextual embeddings with static embeddings has shown to increase the performance of the former (Liu et al., 2020). We propose a new method for injecting pretrained linguistically-enriched embeddings into any layer of BERT. The model maps any word embeddings into the same space as BERT’s hidden representations, then combines them using learned gating parameters. Evaluation of this method on five semantic similarity tasks shows that injecting pre-trained dependency-based and counter-fitted embeddings can further enhance BERT’s performance. More specifically, we make the following contributions: Detecting the semantic similarity between a given text pair is at the core of many NLP tasks. It is a challenging"
2021.findings-emnlp.200,W13-3512,0,0.0592034,"ding injection is particularly beneficial, with notable improvements on examples that require synonym resolution. 1 Introduction Before the rise of contextualised models, transfer of pre-trained information between datasets and tasks in NLP was based on word embeddings. Over many years, substantial effort was placed into the creation of such embeddings. While originally capturing mainly collocation patterns (Mikolov et al., 2013; Pennington et al., 2014), subsequent work enriched these embeddings with additional information, such as dependencies (Levy and Goldberg, 2014), subword information (Luong et al., 2013; Bojanowski et al., 2017) and semantic lexicons (Faruqui et al., 2015). As a result, there exists a wealth of pre-trained embedding resources for many languages in a unified format which could provide complementary information for contemporary pre-trained contextual models. Moreover, aligning contextual embeddings with static embeddings has shown to increase the performance of the former (Liu et al., 2020). We propose a new method for injecting pretrained linguistically-enriched embeddings into any layer of BERT. The model maps any word embeddings into the same space as BERT’s hidden represen"
2021.findings-emnlp.200,S17-2003,0,0.0292533,"sentences in a binary classification setup. We work with the following five widely used English language datasets which cover a range of sizes and tasks (including paraphrase detection, duplicate question identification and answer sentence selection, see Appendix A for details). MSRP The Microsoft Research Paraphrase dataset (MSRP) contains 5K pairs of sentences from news websites which were collected based on heuristics and an SVM classifier. Gold labels are based on human binary annotations for sentential paraphrase detection (Dolan and Brockett, 2005). SemEval The SemEval 2017 CQA dataset (Nakov et al., 2017) consists of three subtasks involving posts from the online forum Qatar Living3 . Each subtask provides an initial post as well as 10 posts which were retrieved by a search engine and annotated with binary labels by humans. The task requires the distinction between relevant and non-relevant posts. The original problem is a ranking setting, but since the gold labels are binary, we focus on a classification setup. In subtask A, the posts are questions and comments from the same thread, in an answer sentence selection setup (26K instances). Subtask B is question paraphrase detection (4K instances"
2021.findings-emnlp.200,P19-1268,1,0.853642,"TBASE . Metrics SemBERT Additionally we compare with the semantics-aware BERT model (SemBERT, Zhang et al. 2020) which uses a semantic role labeler. As the original paper reports results on different dataset versions, we ran the official code on our datasets. The longer sentences in SemEval could not fit on a single GPU due to the larger model size. Our main evaluation metric is the F1 score as this is more meaningful than accuracy for datasets with imbalanced label distributions (such as SemEval C, see Appendix A). We also report performance on difficult cases using the non-obvious F1 score (Peinelt et al., 2019). This metric distinguishes obvious from non-obvious instances in a dataset based on lexical overlap and gold labels, and calculates a separate F1 score for challenging cases. This value therefore tends to be lower than the regular F1 score. Dodge et al. (2020) recently showed that early stopping and random seeds can have considerable impact on the performance of finetuned BERT models, therefore we finetune all models for 3 epochs with early stopping (based on dev F1) and report average model performance across two different seeds. Hyperparameter settings of all BERT-based models are identical"
2021.findings-emnlp.200,2020.acl-main.630,1,0.903015,"pread success in NLP, recent studies Semantic similarity detection Semantic simihave focused on further improving BERT by larity detection is a framework for binary text pair introducing external information. Such work classification tasks such as paraphrase detection, covers a variety of application areas and technical duplicate question identification and answer senapproaches.We broadly categorise such approaches tence selection which require detecting the semaninto input-related, external and internal. Input modifications (Zhao et al., 2020; Singh et al., tic similarity between text pairs (Peinelt et al., 2020). Early semantic similarity methods used 2020; Lai et al., 2020; Ruan et al., 2020) adapt the feature-engineering techniques, exploring various information that is fed to BERT – e.g. feeding text triples separated by [SEP] tokens instead of sen- syntactic (Filice et al., 2017), semantic (Balchev et al., 2016) and lexical features (Tran et al., 2015; tence pairs as in Lai et al. (2020) – while leaving Almarwani and Diab, 2017). Subsequent work the architecture unchanged. Output modifications tried to model text pair relationships either based on (Xuan et al., 2020; Zhang et al., 2020) build on"
2021.findings-emnlp.200,D14-1162,0,0.0990297,"milarity datasets indicate that such information is beneficial and currently missing from the original model. Our qualitative analysis shows that counter-fitted embedding injection is particularly beneficial, with notable improvements on examples that require synonym resolution. 1 Introduction Before the rise of contextualised models, transfer of pre-trained information between datasets and tasks in NLP was based on word embeddings. Over many years, substantial effort was placed into the creation of such embeddings. While originally capturing mainly collocation patterns (Mikolov et al., 2013; Pennington et al., 2014), subsequent work enriched these embeddings with additional information, such as dependencies (Levy and Goldberg, 2014), subword information (Luong et al., 2013; Bojanowski et al., 2017) and semantic lexicons (Faruqui et al., 2015). As a result, there exists a wealth of pre-trained embedding resources for many languages in a unified format which could provide complementary information for contemporary pre-trained contextual models. Moreover, aligning contextual embeddings with static embeddings has shown to increase the performance of the former (Liu et al., 2020). We propose a new method for"
2021.findings-emnlp.200,N18-1202,0,0.337499,"the same space as BERT’s hidden representations, then combines them using learned gating parameters. Evaluation of this method on five semantic similarity tasks shows that injecting pre-trained dependency-based and counter-fitted embeddings can further enhance BERT’s performance. More specifically, we make the following contributions: Detecting the semantic similarity between a given text pair is at the core of many NLP tasks. It is a challenging problem due to the inherent variability of language and the limitations of surface form similarity. Recent pre-trained language models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019) have led to noticeable improvements in semantic similarity detection and subsequent work has explored how these architectures can be fur1. We propose GiBERT - a lightweight gated ther improved. One line of work aims at model method for injecting externally pre-trained compression, making BERT smaller and accessible embeddings into BERT (section 3.1).1 while mostly preserving its performance (Xu et al., 2. We provide an ablation study and a detailed 2020; Goyal et al., 2020; Sanh et al., 2019; Aguilar analysis of the components in the injection et al., 2020; Lan"
2021.findings-emnlp.200,D19-1005,0,0.287391,"One line of work aims at model method for injecting externally pre-trained compression, making BERT smaller and accessible embeddings into BERT (section 3.1).1 while mostly preserving its performance (Xu et al., 2. We provide an ablation study and a detailed 2020; Goyal et al., 2020; Sanh et al., 2019; Aguilar analysis of the components in the injection et al., 2020; Lan et al., 2020; Chen et al., 2020). architecture (section 5). Other studies seek to further improve model performance by enhancing BERT with external infor3. We demonstrate that the proposed model immation from knowledge bases (Peters et al., 2019; proves BERT’s performance on multiple seWang et al., 2020) or additional modalities (Lu 1 et al., 2019; Lin et al., 2020). Code available at https://github.com/wuningxi/GiBERT. 2322 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2322–2336 November 7–11, 2021. ©2021 Association for Computational Linguistics mantic similarity detection datasets. In comparison to multi-head attention injection, our gated injection method uses fewer parameters while achieving comparable performance for dependency embeddings and improved results for counter-fitted embeddings (section"
2021.findings-emnlp.200,2020.tacl-1.54,0,0.115823,"resentations that are beneficial for the semantic similarity detection tasks and also contain information complementary to BERT. Embeddings such as word2vec and Glove leverage cooccurrence patterns which have been shown to be also captured by BERT (Gan et al., 2020). Recent contextualised embeddings risk redundancy with BERT due to the similarity of used approaches. We reason that linguistically-enriched embeddings are most likely to be complementary to BERT, as the model has not been explicitly trained on semantic or syntactic resources and has only partial knowledge of syntax and semantics (Rogers et al., 2020). We hence experiment with injecting dependencybased (Levy and Goldberg, 2014) and counter-fitted embeddings (Mrkši´c et al., 2016) into BERT, which have been found useful for semantic similarity modelling and other related tasks (Filice et al., 2017; Feng et al., 2017; Alzantot et al., 2018; Jin et al., 2020). The 300-dim dependency-based embeddings by Levy and Goldberg (2014) extend the SkipGram embedding algorithm proposed by Mikolov et al. (2013) by replacing linear bag-of-word contexts with dependency-based contexts which are extracted from parsed English Wikipedia sentences. As BERT has"
2021.findings-emnlp.200,S15-2038,0,0.0186345,"n and answer senapproaches.We broadly categorise such approaches tence selection which require detecting the semaninto input-related, external and internal. Input modifications (Zhao et al., 2020; Singh et al., tic similarity between text pairs (Peinelt et al., 2020). Early semantic similarity methods used 2020; Lai et al., 2020; Ruan et al., 2020) adapt the feature-engineering techniques, exploring various information that is fed to BERT – e.g. feeding text triples separated by [SEP] tokens instead of sen- syntactic (Filice et al., 2017), semantic (Balchev et al., 2016) and lexical features (Tran et al., 2015; tence pairs as in Lai et al. (2020) – while leaving Almarwani and Diab, 2017). Subsequent work the architecture unchanged. Output modifications tried to model text pair relationships either based on (Xuan et al., 2020; Zhang et al., 2020) build on increasingly complex neural architectures (Deriu BERT’s pre-trained representation by adding external information after the encoding step – e.g. and Cieliebak, 2017; Wang et al., 2017; Tan et al., 2018) or by combining both approaches through combining it with additional semantic information hybrid techniques (Wu et al., 2017a; Feng et al., as in Z"
2021.newsum-1.7,D18-1443,0,0.0264201,"g it first as an extractive summariser and then as an abstractive one. The BertSumExtAbs pretrained model3 has been used. PG (Pointer-Generator with Coverage Penalty) (See et al., 2017)4 . This uses a 1-layer bidirectional LSTM encoder and a 1-layer unidirectional LSTM decoder with attention, with the possibility of switching between copying words or generating them (Pointer-Generator) and including a coverage mechanism adding up attention distributions of previous steps to minimise repetitions. The ’OpenNMT BRNN (2 layer, emb 256, hid 1024)’ pre-trained model4 has been used. CopyTransformer (Gehrmann et al., 2018)5 . This uses the transformer architecture, but one attention head defines the copy distribution. The ’OpenNMT Transformer’ pretrained model4 has been used. FastAbsRL (Chen and Bansal, 2018)6 . An extractor agent is used to select sentences (using LSTM layers to represent and copy sentences) and an abstractor network is used to compress and paraphrase the selected sentences. Both are trained separately and then the full model is trained with reinforcement learning by using A2C (Mnih et al., 2016). The reported Rouge scores of these models (Lin, 2004) are shown in Table 1. None of the pre-train"
2021.newsum-1.7,N19-1204,0,0.0268445,"ge Penalty) (See et al., 2017), CopyTransformer (Gehrmann et al., 2018), and FastAbsRL (Chen and Bansal, 2018). Those models are applied in combination with the machine translation system MarianMT (JunczysDowmunt et al., 2018) using the Opus-MT models (Tiedemann and Thottingal, 2020). We have evaluated the quality of the summaries for each model and their comparison. Early research on the problem of text summarisation in low resourced languages (although not focused on deliberation) Orˇasan and Chiorean (2008) demonstrated the limitations of machine translation systems at that time. Recently, Ouyang et al. (2019) revisited the problem of low quality translations in low resourced languages and successfully demonstrated the possibility of using abstractive summarisation by retraining their model on corpora that have gone through the same machine translation process. In this study, we complete the cycle, translating from the original language to English, summarising, and translating back to the original language, thus avoiding the need for retraining. Using other approaches, Yao et al. (2015) studied English-to-Chinese summarisation combining an extractive approach with a process of sentence compression"
C02-1105,P00-1065,0,\N,Missing
C02-1105,C92-2117,0,\N,Missing
C02-1105,H94-1020,0,\N,Missing
C02-1105,P98-1013,0,\N,Missing
C02-1105,C98-1013,0,\N,Missing
C02-1105,palmer-etal-2000-semantic,0,\N,Missing
C02-1105,H01-1010,0,\N,Missing
C04-1027,H90-1021,0,0.0181915,"about the way we view the structure of the world. Our method for inducing domain theories relies on this inversion, since in the general case it is a much easier job to disambiguate sentences than to directly encode the theory that we are drawing on in so doing. Our strategy for trying to build a domain theory is to try to capitalise on the information that is tacitly contained in those disambiguation decisions. 2 Some background (Pulman, 2000) showed that it was possible to learn a simple domain theory from a disambiguated corpus: a subset of the ATIS (air travel information service) corpus (Doddington and Godfrey, 1990). Ambiguous sentences were annotated as shown to indicate the preferred reading: [i,would,like, [the,cheapest,flight, from,washington,to,atlanta] ] [do,they,[serve,a,meal],on, [the,flight,from,san_francisco,to,atlanta]] [i,would,like, [a,flight,from,boston, to,san_francisco, [that,leaves,before,’8:00’] ] ] The ‘good’ and the ‘bad’ parses were used to produce simplified first order logical forms representing the semantic content of the various readings of the sentences. The ‘good’ readings were used as positive evidence, and the ‘bad’ readings (or more accurately, the bad parts of some of the r"
C04-1027,C02-1105,1,0.622497,"the notion of logical consistency is too strong a test in many cases. Note also that the results of the theory induction process are perfectly comprehensible - the outcome is a theory with some logical structure, rather than a black box. The method requires a fully parsed corpus with corresponding logical forms. Using a similar technique, we have experimented with slightly larger datasets, using the Penn Tree Bank (Marcus et al., 1994) since the syntactic annotations for sentences given there are intended to be complete enough for semantic interpretation, in principle, at least. In practice, (Liakata and Pulman, 2002) report, it is by no means easy to do this. It is possible to recover partial logical forms from a large proportion of the treebank, but these are not complete or accurate enough to simply replicate the ATIS experiment. In the work reported here, we selected about 40 texts containing the verb ‘resign’, all reporting, among other things, ‘company succession’ events, a scenario familiar from the Message Understanding Conference (MUC) task (Grishman and Sundheim, 1995). The texts amounted to almost 4000 words in all. Then we corrected and completed some automatically produced logical forms by han"
C04-1027,H94-1020,0,0.0387888,"s involved in that experiment were too small for the results to be statistically meaningful, the experiment proved that the method works in principle, although of course in reality the notion of logical consistency is too strong a test in many cases. Note also that the results of the theory induction process are perfectly comprehensible - the outcome is a theory with some logical structure, rather than a black box. The method requires a fully parsed corpus with corresponding logical forms. Using a similar technique, we have experimented with slightly larger datasets, using the Penn Tree Bank (Marcus et al., 1994) since the syntactic annotations for sentences given there are intended to be complete enough for semantic interpretation, in principle, at least. In practice, (Liakata and Pulman, 2002) report, it is by no means easy to do this. It is possible to recover partial logical forms from a large proportion of the treebank, but these are not complete or accurate enough to simply replicate the ATIS experiment. In the work reported here, we selected about 40 texts containing the verb ‘resign’, all reporting, among other things, ‘company succession’ events, a scenario familiar from the Message Understan"
C04-1027,C96-1079,0,\N,Missing
C16-1146,W15-1516,0,0.0328934,"Missing"
C16-1146,S14-2145,0,0.105602,"ave a mention of a location entity name and discard other sentences. 2.2 Categories The Number of location mentions in a single sentence in our dataset varies from one to over 50. To simplify the task, we only annotate sentences that contain one or two location mentions. These sentences were divided into two groups: sentences containing one location mention — Single, and sentences containing two location mentions — Multi. This is to observe the difficulty of annotating two groups by human annotators and by the models. 2.3 Aspects Like existing work in the aspect-based sentiment analysis task (Brychcın et al., 2014), a pre-defined list of aspects is provided for annotators to choose from. These aspects are: live, safety, price, quiet, dining, nightlife, transit-location, touristy, shopping, green-culture and multicultural. Adding an additional aspect of misc was considered. However in the initial round of annotations, we realised that it had a negative effect on the decisiveness of annotators and it led to a lower overall agreement. Aspect general refers to a generic opinion about a location, e.g. “I love Camden Town”. 2.4 Sentiment For each selected aspect, annotators were required to select a polarity"
C16-1146,P14-2009,0,0.0585927,"Missing"
C16-1146,P11-1016,0,0.0374303,"unit of text, and recognizing the polarity associated with each aspect separately. The datasets for this task were mostly based on specialized review platforms such as Yelp where it is assumed that only one entity is discussed in one review snippet, but the opinion on multiple aspects can be expressed. This task is particularly useful because a user can assess the aggregated sentiment for each individual aspect of a given product or service and get a more fine-grained understanding of its quality. Another line of research in this field is targeted (a.k.a. target-dependent) sentiment analysis (Jiang et al., 2011; Vo and Zhang, 2015). Targeted sentiment analysis investigates the classification of opinion polarities towards certain target entity mentions in given sentences (often a tweet). For instance in the sentence “People everywhere love Windows & vista. Bill Gates”, polarity towards Bill Gates is “Neutral” but the positive sentiment towards Windows & vista will interfere with identifying it if the usual methods for sentiment analysis task are employed. However this task assumes only the overall sentiment for each entity. Moreover, the existing corpora for this task so far has contained only a sing"
C16-1146,liakata-etal-2010-corpora,1,0.807761,"Missing"
C16-1146,W02-1011,0,0.0224414,"Missing"
C16-1146,S15-2082,0,0.118495,"Missing"
C16-1146,E12-2021,0,0.0228055,"Missing"
C16-1146,P02-1053,0,0.00968963,"Missing"
C16-1146,N10-1122,0,\N,Missing
C16-1146,D13-1171,0,\N,Missing
C16-1230,R13-1011,0,0.060746,"Missing"
C16-1230,W16-0403,0,0.0471791,"disputed beliefs that Barack Obama is reportedly Muslim. The authors use tweets observed in the past to train a classifier, which is then applied to new tweets discussing the same rumour. In recent work, rule-based methods have been put forward as a way to improve on the performance of the Qazvinian et al. (2011) baseline. This is the approach followed by Liu et al. (2015), who introduced a simple rule-based method that looks for the presence of positive or negative words in a tweet. One draw back of such rule-based approaches is that they may not generalise to new, unseen rumours. Similarly, Hamidian and Diab (2016) have recently studied the extent to which a model trained from historical tweets can be used for classifying new tweets discussing the same rumour. While Zhao et al. (2015) did not study stance classification, they showed that tweets that trigger questioning responses from others are likely to report disputed rumours, which reinforces the motivation of our work of determining the stance of tweets to then deal with rumours. Classification of stance towards a target on Twitter has been addressed in SemEval-2016 task 6 (Mohammad et al., 2016). Task A had to determine the stance of tweets towards"
C16-1230,D15-1311,1,0.852704,"ised or unsupervised approach. The dataset of this competition was not related to rumours or breaking news, it only considered a 3-way classification and did not provide any relations between tweets, which were treated as individual instances. Our work presents different objectives in three aspects. First, we aim to classify the stance of tweets towards rumours that emerge while breaking news unfold; these rumours are unlikely to have been observed before, and hence rumours from previously observed events, which are likely to diverge, need to be leveraged for training. As far as we know, only Lukasik et al. (2015; 2016a; 2016b) have tackled stance classification in the context of breaking news applied to new rumours. Lukasik et al. (2015; 2016a) used Gaussian Processes to perform 3-way stance classification into supporting, denying or questioning, while comments where not considered as part of the task. Lukasik et al. (2016b) did include comments to perform 4-way stance classification; they used Hawkes Processes to exploit the temporal sequence 2439 of stances towards rumours to classify new tweets discussing rumours. Work by Zeng et al. (Zeng et al., 2016) has also performed stance classification for"
C16-1230,P16-2064,1,0.838462,"classify the stance of tweets towards rumours that emerge while breaking news unfold; these rumours are unlikely to have been observed before, and hence rumours from previously observed events, which are likely to diverge, need to be leveraged for training. As far as we know, only Lukasik et al. (2015; 2016a; 2016b) have tackled stance classification in the context of breaking news applied to new rumours. Lukasik et al. (2015; 2016a) used Gaussian Processes to perform 3-way stance classification into supporting, denying or questioning, while comments where not considered as part of the task. Lukasik et al. (2016b) did include comments to perform 4-way stance classification; they used Hawkes Processes to exploit the temporal sequence 2439 of stances towards rumours to classify new tweets discussing rumours. Work by Zeng et al. (Zeng et al., 2016) has also performed stance classification for rumours around breaking news, but overlapping rumours were used for training and testing. Second, recent research has posited that a 4-way classification is needed to capture responses seen in the unfolding of breaking news (Procter et al., 2013b; Zubiaga et al., 2016). Moving away from the 2-way classification abo"
C16-1230,S15-2047,0,0.0728214,"Missing"
C16-1230,S16-1003,0,0.0730401,"y not generalise to new, unseen rumours. Similarly, Hamidian and Diab (2016) have recently studied the extent to which a model trained from historical tweets can be used for classifying new tweets discussing the same rumour. While Zhao et al. (2015) did not study stance classification, they showed that tweets that trigger questioning responses from others are likely to report disputed rumours, which reinforces the motivation of our work of determining the stance of tweets to then deal with rumours. Classification of stance towards a target on Twitter has been addressed in SemEval-2016 task 6 (Mohammad et al., 2016). Task A had to determine the stance of tweets towards five targets as ‘favor’, ‘against’ or ‘none’. Task B tested stance detection towards an unlabelled target, which required a weakly supervised or unsupervised approach. The dataset of this competition was not related to rumours or breaking news, it only considered a 3-way classification and did not provide any relations between tweets, which were treated as individual instances. Our work presents different objectives in three aspects. First, we aim to classify the stance of tweets towards rumours that emerge while breaking news unfold; thes"
C16-1230,D11-1147,0,0.823403,"lassification is a task that is increasingly gaining popularity in its application to tweets. While Twitter is a generous source of reports of breaking news, outpacing even news outlets (Kwak et al., 2010), it also comes with the caveat that some of those reports are still rumours at the time of posting and so are yet to be verified and corroborated (Mendoza et al., 2010; Procter et al., 2013b; Procter et al., 2013a). The rumour stance classification task intends to assist in this verification process by determining the type of support expressed in different tweets discussing the same rumour (Qazvinian et al., 2011). Aggregation of the stance of multiple tweets discussing a rumour can then be of help to determine its likely veracity, enabling – among other benefits – the flagging of highly disputed rumours that are likely to be false. Previous research on rumour stance classification for tweets has been limited to the tweet as the unit to be classified. However, such approaches ignore the additional context and knowledge that can be gained from the structure of Twitter interactions within conversational threads (Zubiaga et al., 2016; Procter et al., 2013b; Tolmie et al., 2015). The latter are formed as T"
C16-1230,I11-1164,0,0.0309058,"ructure of Twitter postings for stance classification, and hence its utility remains unexplored. A work that is related is that of Lukasik et al. (2016b), who exploited the temporal sequence of tweets, although the conversational structure was ignored and each tweet was treated as a separate unit. In other domains where debates or conversations are involved, the sequence of responses has been exploited to make the most of the evolving discourse and perform an improved classification of each individual post after learning the structure and dynamics of the conversation as a whole. For instance, Qu and Liu (2011) found Hidden Markov Models to be an effective approach to classify threads in on-line fora as successfully solving or not the question raised in the initial post. This was later further studied in a SemEval shared task, where each post in a forum thread had to also be classified as good, potential or bad (M`arquez et al., 2015). FitzGerald et al. (2011) used a linear-chain CRF to identify high-quality comments in threads responding to blog posts. In a task that is related to stance classification, researchers have also studied the identification of agreement and disagreement in on-line conver"
C16-1230,N10-1020,0,0.0438559,"pproach takes into account the interaction between users on social media, whether it is about appealing for more information in order to corroborate a rumourous post (querying) or to say something that does not contribute to the resolution of the rumour’s veracity (commenting). Finally, instead of dealing with tweets as single units in isolation, we exploit the conversational structure of Twitter replies, building a classifier that learns the dynamics of stance in tree-structured conversational threads. The closest work when it comes to exploiting conversational structure in tweets is that of Ritter et al. (2010) who modelled linear sequences of replies in Twitter conversations with Hidden Markov Models for dialogue act tagging, but the structure of the tree as a whole was not exploited. As far as we know, no work has leveraged the conversational structure of Twitter postings for stance classification, and hence its utility remains unexplored. A work that is related is that of Lukasik et al. (2016b), who exploited the temporal sequence of tweets, although the conversational structure was ignored and each tweet was treated as a separate unit. In other domains where debates or conversations are involved"
C16-1230,W15-4625,0,0.0206378,"the initial post. This was later further studied in a SemEval shared task, where each post in a forum thread had to also be classified as good, potential or bad (M`arquez et al., 2015). FitzGerald et al. (2011) used a linear-chain CRF to identify high-quality comments in threads responding to blog posts. In a task that is related to stance classification, researchers have also studied the identification of agreement and disagreement in on-line conversations. To classify agreement between question-answer (Q-A) message pairs in fora, Abbott et al. (2011) used Naive Bayes as the classifier, and Rosenthal and McKeown (2015) used a logistic regression classifier. However, in both cases only pairs of messages were considered, and the entire sequence of responses in the tree was not used. CRF has also been used to detect agreement and disagreement between speakers in broadcast debates (Wang et al., 2011), which our task differs from in that it solely focuses on text. It is also worthwhile to emphasise that stance classification is different to agreement/disagreement detection, given that in stance classification one has to determine the orientation of a user towards a rumour. Instead, in agreement/disagreement dete"
C16-1230,P11-2065,0,0.0248998,"o blog posts. In a task that is related to stance classification, researchers have also studied the identification of agreement and disagreement in on-line conversations. To classify agreement between question-answer (Q-A) message pairs in fora, Abbott et al. (2011) used Naive Bayes as the classifier, and Rosenthal and McKeown (2015) used a logistic regression classifier. However, in both cases only pairs of messages were considered, and the entire sequence of responses in the tree was not used. CRF has also been used to detect agreement and disagreement between speakers in broadcast debates (Wang et al., 2011), which our task differs from in that it solely focuses on text. It is also worthwhile to emphasise that stance classification is different to agreement/disagreement detection, given that in stance classification one has to determine the orientation of a user towards a rumour. Instead, in agreement/disagreement detection, one has to determine if a pair of posts share the same view. In stance classification, one might agree with another user who is denying a rumour, and hence they are denying the rumour as well, irrespective of the pairwise agreement. To the best of our knowledge Twitter conver"
C16-1283,P11-2008,0,0.0210257,"Missing"
C16-1283,D09-1063,0,0.0245672,"d to our problem. The unigrams of every text were matched against those vectors and seven functions were applied on every dimension of the resulting matrix (mean, median, min, max, stdev, first and third quartile). • Lexicons: We employed several lexicons that have been effectively used in sentiment- or emotionrelated works. Those were the Opinion Lexicon (Hu and Liu, 2004), NRC Hashtag, NRC Hashtag Emotion (Mohammad, 2012), Unigram and Bigram NRC Hashtag Sentiment and Sentiment 140 lexicons (Zhu et al., 2014), MaxDiff Twitter Sentiment Lexicon (Svetlana Kiritchenko and Mohammad, 2014), MSOL (Mohammad et al., 2009) and AFINN (Nielsen, 2011). For lexicons providing binary values (pos/neg), we counted the number of ngrams matching each of the positive and negative classes; for those lexicons with score values, we used the simple counts and the total summation of the corresponding scores from each ngram in the text matched against the lexicons. • Topics: In order to better categorise the content that a subject has shared and to accommodate the sparse representations of the ngrams, we used the word clusters created by Preot¸iuc-Pietro et al. (2015), which were based on word2vec representations of the most c"
C16-1283,S12-1033,0,0.0264598,"noise reduction purposes. • Word embeddings: We used the word embeddings created by (Tang et al., 2014), which have been used successfully before for the task of sentiment analysis, related to our problem. The unigrams of every text were matched against those vectors and seven functions were applied on every dimension of the resulting matrix (mean, median, min, max, stdev, first and third quartile). • Lexicons: We employed several lexicons that have been effectively used in sentiment- or emotionrelated works. Those were the Opinion Lexicon (Hu and Liu, 2004), NRC Hashtag, NRC Hashtag Emotion (Mohammad, 2012), Unigram and Bigram NRC Hashtag Sentiment and Sentiment 140 lexicons (Zhu et al., 2014), MaxDiff Twitter Sentiment Lexicon (Svetlana Kiritchenko and Mohammad, 2014), MSOL (Mohammad et al., 2009) and AFINN (Nielsen, 2011). For lexicons providing binary values (pos/neg), we counted the number of ngrams matching each of the positive and negative classes; for those lexicons with score values, we used the simple counts and the total summation of the corresponding scores from each ngram in the text matched against the lexicons. • Topics: In order to better categorise the content that a subject has"
C16-1283,P14-1146,0,0.0191881,"Missing"
C16-1283,S14-2077,0,0.0126045,"ang et al., 2014), which have been used successfully before for the task of sentiment analysis, related to our problem. The unigrams of every text were matched against those vectors and seven functions were applied on every dimension of the resulting matrix (mean, median, min, max, stdev, first and third quartile). • Lexicons: We employed several lexicons that have been effectively used in sentiment- or emotionrelated works. Those were the Opinion Lexicon (Hu and Liu, 2004), NRC Hashtag, NRC Hashtag Emotion (Mohammad, 2012), Unigram and Bigram NRC Hashtag Sentiment and Sentiment 140 lexicons (Zhu et al., 2014), MaxDiff Twitter Sentiment Lexicon (Svetlana Kiritchenko and Mohammad, 2014), MSOL (Mohammad et al., 2009) and AFINN (Nielsen, 2011). For lexicons providing binary values (pos/neg), we counted the number of ngrams matching each of the positive and negative classes; for those lexicons with score values, we used the simple counts and the total summation of the corresponding scores from each ngram in the text matched against the lexicons. • Topics: In order to better categorise the content that a subject has shared and to accommodate the sparse representations of the ngrams, we used the word clu"
C18-1288,W17-4419,0,0.0193297,"d verification, comparing the improvement brought by each of the two auxiliary tasks, and (4) performing multi-task learning that combines rumour detection, stance classification and veracity prediction to improve the performance for veracity. We employ a deep learning architecture that views each subtask as having a sequential nature. We compare our results with a state-of-the-art veracity classification approach introduced by Enayet and El-Beltagy (2017). While a lot of work reports positive outcomes of the application of multi-task learning to various NLP tasks (Collobert and Weston, 2008; Aguilar et al., 2017; Lan et al., 2017), there are also studies showing that this is not always the case (Alonso and Plank, 2017; Bingel and Søgaard, 2017). Alonso and Plank (2017) were the first to demonstrate that multi-task learning brings benefits only for some combinations of main and auxiliary tasks. They also investigate the relationship between the multi-task learning outcome and the properties of the dataset. We perform a similar analysis to examine the link between the properties of our rumour datasets and the results of the multi-task learning approach used. Our results show that a multi-task learning"
C18-1288,E17-1005,0,0.339922,"ulti-task learning that combines rumour detection, stance classification and veracity prediction to improve the performance for veracity. We employ a deep learning architecture that views each subtask as having a sequential nature. We compare our results with a state-of-the-art veracity classification approach introduced by Enayet and El-Beltagy (2017). While a lot of work reports positive outcomes of the application of multi-task learning to various NLP tasks (Collobert and Weston, 2008; Aguilar et al., 2017; Lan et al., 2017), there are also studies showing that this is not always the case (Alonso and Plank, 2017; Bingel and Søgaard, 2017). Alonso and Plank (2017) were the first to demonstrate that multi-task learning brings benefits only for some combinations of main and auxiliary tasks. They also investigate the relationship between the multi-task learning outcome and the properties of the dataset. We perform a similar analysis to examine the link between the properties of our rumour datasets and the results of the multi-task learning approach used. Our results show that a multi-task learning scenario that leverages all three subtasks, where veracity classification is the main task and stance classi"
C18-1288,E17-2026,0,0.0229713,"combines rumour detection, stance classification and veracity prediction to improve the performance for veracity. We employ a deep learning architecture that views each subtask as having a sequential nature. We compare our results with a state-of-the-art veracity classification approach introduced by Enayet and El-Beltagy (2017). While a lot of work reports positive outcomes of the application of multi-task learning to various NLP tasks (Collobert and Weston, 2008; Aguilar et al., 2017; Lan et al., 2017), there are also studies showing that this is not always the case (Alonso and Plank, 2017; Bingel and Søgaard, 2017). Alonso and Plank (2017) were the first to demonstrate that multi-task learning brings benefits only for some combinations of main and auxiliary tasks. They also investigate the relationship between the multi-task learning outcome and the properties of the dataset. We perform a similar analysis to examine the link between the properties of our rumour datasets and the results of the multi-task learning approach used. Our results show that a multi-task learning scenario that leverages all three subtasks, where veracity classification is the main task and stance classification and rumour detecti"
C18-1288,S17-2082,0,0.282763,"on, determining the attitude of the sources or users towards the truthfulness of the rumour, and (4) rumour verification, as the ultimate step where the veracity value of the rumour is predicted. These steps can be performed at different times in the life-cycle of a rumour, making this a time-sensitive process. Ideally, rumours can be resolved as either true or false. However, they can also remain unverified when there is no sufficient evidence to determine their veracity (Caplow, 1947). A recent body of work studies each of these four tasks separately (Lukasik et al., 2016; Liu et al., 2016; Enayet and El-Beltagy, 2017). However, the way these subtasks interact and their integration into a complete rumour resolution system is yet to be explored. In this work we assume that veracity classification is the crucial component of a rumour resolution system, as the final output that determines, given information collected about a rumour at a certain point in time, if the rumour is true, false, or remains unverified. We express the rumour resolution process as a multi-task problem that needs to address a number of challenges, where the veracity classification task is the main task and the rest of the components are"
C18-1288,S17-2083,1,0.905653,"ing body of work has focused on the stance classification component, i.e. determining whether different posts associated with a rumour support it, deny it, query it, or just comment on it. The stance expressed by users towards a particular rumour can be indicative of the veracity of a rumour, furthermore, it has been shown that rumours attracting higher levels of skepticism in the form of denials and questioning responses are more likely to be proven false later (Mendoza et al., 2010; Procter et al., 2013; Derczynski et al., 2014). Previous work on stance classification (Lukasik et al., 2016; Kochkina et al., 2017; Zubiaga et al., 2017; Zubiaga et al., 2018b) has explored the use of sequential classifiers, treating the task as one that evolves over time; it has been shown that these sequential classifiers substantially outperform standard, non-sequential classifiers. Rumour detection. Work on rumour detection is more scarce. One of the first approaches was introduced by Zhao et al. (2015), who built a rule-based approach to identify skepticism (e.g. is this true?) and therefore determine that the associated information is a rumour. The limitations of this approach consist in having to wait for response"
C18-1288,D17-1134,0,0.0294424,"ing the improvement brought by each of the two auxiliary tasks, and (4) performing multi-task learning that combines rumour detection, stance classification and veracity prediction to improve the performance for veracity. We employ a deep learning architecture that views each subtask as having a sequential nature. We compare our results with a state-of-the-art veracity classification approach introduced by Enayet and El-Beltagy (2017). While a lot of work reports positive outcomes of the application of multi-task learning to various NLP tasks (Collobert and Weston, 2008; Aguilar et al., 2017; Lan et al., 2017), there are also studies showing that this is not always the case (Alonso and Plank, 2017; Bingel and Søgaard, 2017). Alonso and Plank (2017) were the first to demonstrate that multi-task learning brings benefits only for some combinations of main and auxiliary tasks. They also investigate the relationship between the multi-task learning outcome and the properties of the dataset. We perform a similar analysis to examine the link between the properties of our rumour datasets and the results of the multi-task learning approach used. Our results show that a multi-task learning scenario that lever"
C18-1288,P16-2064,1,0.866256,"r as it unfolds; (3) stance classification, determining the attitude of the sources or users towards the truthfulness of the rumour, and (4) rumour verification, as the ultimate step where the veracity value of the rumour is predicted. These steps can be performed at different times in the life-cycle of a rumour, making this a time-sensitive process. Ideally, rumours can be resolved as either true or false. However, they can also remain unverified when there is no sufficient evidence to determine their veracity (Caplow, 1947). A recent body of work studies each of these four tasks separately (Lukasik et al., 2016; Liu et al., 2016; Enayet and El-Beltagy, 2017). However, the way these subtasks interact and their integration into a complete rumour resolution system is yet to be explored. In this work we assume that veracity classification is the crucial component of a rumour resolution system, as the final output that determines, given information collected about a rumour at a certain point in time, if the rumour is true, false, or remains unverified. We express the rumour resolution process as a multi-task problem that needs to address a number of challenges, where the veracity classification task is t"
C18-1288,P17-2067,0,0.0555658,". The limitations of this approach consist in having to wait for responses to arrive, as well as lack of generalisability due to manually-defined rules. Zubiaga et al (2017) proposed a sequential approach to leverage context from earlier posts during an event. The sequential approach achieved significant improvements, especially in terms of recall, where the rule-based approach proved limited. Rumour verification. Common approaches to rumour verification involve first the collection of corpora of resolved rumours from rumour debunking websites such as snopes.com, emergent.com, politifact.com. Wang (2017) created a dataset based on claims from politifact.com that are annotated for degrees of truthfulness. To resolve these they propose a hybrid convolutional neural network that integrates metadata with text. Twitter is a popular platform to study rumours, while often seeds and annotations for rumours are still taken from rumour debunking websites. Giasemidis et al. (2016) collected a dataset of 72 rumours from Twitter. This work measured trustworthiness of a claim at varying time windows. Boididou et al. (2014; 2017) have focused on tweets that are related to fake images, bringing in the multim"
D13-1070,C12-1041,0,0.014322,"was given to the sentence selection criteria for the extractive summaries. 747 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 747–757, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics The sentences chosen for the rhetorical extracts were either all sentences of a particular category (in the case of rare categories) (Teufel and Moens, 2002), selected according to a classifier trained on a relevance gold standard (Teufel and Moens, 2002), manually or randomly selected (Teufel, 2010, p.60). More recently Contractor et al. (2012) have used automatically annotated argumentative zones (Guo et al., 2011) to guide the creation of extractive summaries of scientific articles. Here argumentative zones are used as features for the summariser, along with verbs, tf-idf values and sentence location. They use a standard approach to summarisation, with a binary classification recognising candidate sentences which are then fed into a clustering mechanism. Extracts can be created to summarise the entire paper or focus on specific user-specified aspects. The number of sentences to include in the summary is prespecified (either direct"
D13-1070,D11-1025,0,0.015484,"roceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 747–757, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics The sentences chosen for the rhetorical extracts were either all sentences of a particular category (in the case of rare categories) (Teufel and Moens, 2002), selected according to a classifier trained on a relevance gold standard (Teufel and Moens, 2002), manually or randomly selected (Teufel, 2010, p.60). More recently Contractor et al. (2012) have used automatically annotated argumentative zones (Guo et al., 2011) to guide the creation of extractive summaries of scientific articles. Here argumentative zones are used as features for the summariser, along with verbs, tf-idf values and sentence location. They use a standard approach to summarisation, with a binary classification recognising candidate sentences which are then fed into a clustering mechanism. Extracts can be created to summarise the entire paper or focus on specific user-specified aspects. The number of sentences to include in the summary is prespecified (either directly or using a compression ratio). Our approach also makes use of the scie"
D13-1070,W09-1401,0,0.0425779,"Missing"
D13-1070,W11-1801,0,0.0279145,"Missing"
D13-1070,liakata-etal-2010-corpora,1,0.838702,"ects. The number of sentences to include in the summary is prespecified (either directly or using a compression ratio). Our approach also makes use of the scientific discourse for summarisation purposes. We use the scientific discourse to create a content model for extractive summarisation, with a focus on representing the content of the full paper, while keeping the cohesion of the narrative. We first automatically annotate the articles with a scheme which captures fine-grained aspects of the content and conceptual structure of the papers, namely the Core Scientific Concepts (CoreSC) scheme (Liakata et al., 2010; Liakata et al., 2012). The CoreSC scheme is “uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence” (White et al., 2011), which are not readily identifiable by other annotation schemes. Also, when compared to argumentative zoning and more specifically its extension for chemistry papers, AZII (Teufel et al., 2009), it was shown to provide a greater level of detail in terms of categories denoting objectives, methods and outcomes whereas AZII focusses on the attribution of knowledge claims and the relation with previous work (Liakata et"
D13-1070,D12-1106,0,0.0431961,"Missing"
D13-1070,J13-2002,0,0.0312578,"are more likely to be found in the abstracts. We expect that a refinement in the sentence selection criterion, which would also take sentence length into account, will help to showcase further the benefits of using a CoreSC-based content model. Analysis using ROUGE showed that while summaries C had a slightly higher ROUGE-1 measure than summaries B (0.75 vs 0.73), with respect to abstracts, ROUGE-L was the same for the two (0.70). In table 5 we also report measurements on summary informativeness based on divergence (Kullback Leibler (KL) divergence and Jensen Shannon (JS) divergence), as in (Louis and Nenkova, 2013). KL divergence is asymmetric and reflects the average number of bits wasted by coding samples of a distribution P using another distribution Q. JS divergence is an information-theoretic measure, reflecting the average distance of the KL divergence between summary and input (the full paper in our case) from the mean vocabulary distributions. Compared to other measures, JS divergence has been found to produce the best predictions of summary quality (Louis and Nenkova, 2013). In practice, what JS divergence tells us is how ‘different’/divergent the summary is from the original paper. Low diverge"
D13-1070,W98-1124,0,0.0600264,"ce, such as significant n-grams, verbs and word triples, as well as global features such as the position of the sentence within the document and within a paragraph and section headers. The following subsections give details about the creation of extractive summaries from CoreSC categories. 3.1 A content model for CoreSC extractive summaries Building an extractive summary using a computational model of document structure is an idea shared by many previous approaches, whether the model is hand-crafted, based on rhetorical elements (McKeown, 1985; Teufel and Moens, 2002) or rhetorical relations (Marcu, 1998b; Marcu, 1998a) or whether it is a content model, learnt automatically from text as in (Barzilay and Lee, 2004), focussing on the local content or a combination of the local content and global structure (Sauper et al., 2010). Our document content model is primarily based on the global discourse of the article as provided by the type and number of CoreSC categories. However, unlike (Teufel and Moens, 2002), who take a fixed number of AZ categories of specific type to create rhetorical extracts, the number of categories used from each CoreSC category depends on their distribution in the origina"
D13-1070,C08-1087,0,0.195241,"Missing"
D13-1070,P10-1057,0,0.0283668,"Missing"
D13-1070,C10-1101,0,0.0607388,"Missing"
D13-1070,D10-1037,0,0.0205315,"details about the creation of extractive summaries from CoreSC categories. 3.1 A content model for CoreSC extractive summaries Building an extractive summary using a computational model of document structure is an idea shared by many previous approaches, whether the model is hand-crafted, based on rhetorical elements (McKeown, 1985; Teufel and Moens, 2002) or rhetorical relations (Marcu, 1998b; Marcu, 1998a) or whether it is a content model, learnt automatically from text as in (Barzilay and Lee, 2004), focussing on the local content or a combination of the local content and global structure (Sauper et al., 2010). Our document content model is primarily based on the global discourse of the article as provided by the type and number of CoreSC categories. However, unlike (Teufel and Moens, 2002), who take a fixed number of AZ categories of specific type to create rhetorical extracts, the number of categories used from each CoreSC category depends on their distribution in the original article. Any and all types of CoreSC category could potentially appear in a summary, as our summaries are meant to be representative of the entire content of the paper. Also, the ordering of the categories in the summary is"
D13-1070,J02-4002,0,0.821325,"mphasis of this approach was the identification of the argumentative zones, less attention was given to the sentence selection criteria for the extractive summaries. 747 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 747–757, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics The sentences chosen for the rhetorical extracts were either all sentences of a particular category (in the case of rare categories) (Teufel and Moens, 2002), selected according to a classifier trained on a relevance gold standard (Teufel and Moens, 2002), manually or randomly selected (Teufel, 2010, p.60). More recently Contractor et al. (2012) have used automatically annotated argumentative zones (Guo et al., 2011) to guide the creation of extractive summaries of scientific articles. Here argumentative zones are used as features for the summariser, along with verbs, tf-idf values and sentence location. They use a standard approach to summarisation, with a binary classification recognising candidate sentences which are then fed into a clustering mechanism. Extracts can be created to summarise the entire paper or focus on specific user-specifi"
D13-1070,D09-1155,1,0.841377,"sion of the narrative. We first automatically annotate the articles with a scheme which captures fine-grained aspects of the content and conceptual structure of the papers, namely the Core Scientific Concepts (CoreSC) scheme (Liakata et al., 2010; Liakata et al., 2012). The CoreSC scheme is “uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence” (White et al., 2011), which are not readily identifiable by other annotation schemes. Also, when compared to argumentative zoning and more specifically its extension for chemistry papers, AZII (Teufel et al., 2009), it was shown to provide a greater level of detail in terms of categories denoting objectives, methods and outcomes whereas AZII focusses on the attribution of knowledge claims and the relation with previous work (Liakata et al., 2010). We then use the distribution of CoreSC categories observed in abstracts to create a content model which provides a skeleton for extractive summaries. The reasoning behind this is to try to preserve cohesion within the summaries and we hypothesise 748 that the sequence of CoreSC categories is a good proxy for cohesion (see section 3.1). In creating the summary,"
D13-1070,W11-0217,0,0.0162261,"ummarisation purposes. We use the scientific discourse to create a content model for extractive summarisation, with a focus on representing the content of the full paper, while keeping the cohesion of the narrative. We first automatically annotate the articles with a scheme which captures fine-grained aspects of the content and conceptual structure of the papers, namely the Core Scientific Concepts (CoreSC) scheme (Liakata et al., 2010; Liakata et al., 2012). The CoreSC scheme is “uniquely suited to recovering common types of scientific arguments about hypotheses, explanations, and evidence” (White et al., 2011), which are not readily identifiable by other annotation schemes. Also, when compared to argumentative zoning and more specifically its extension for chemistry papers, AZII (Teufel et al., 2009), it was shown to provide a greater level of detail in terms of categories denoting objectives, methods and outcomes whereas AZII focusses on the attribution of knowledge claims and the relation with previous work (Liakata et al., 2010). We then use the distribution of CoreSC categories observed in abstracts to create a content model which provides a skeleton for extractive summaries. The reasoning behi"
D13-1070,W04-3252,0,\N,Missing
D13-1070,N04-1015,0,\N,Missing
demiros-etal-2000-named,W98-1120,0,\N,Missing
demiros-etal-2000-named,M98-1004,0,\N,Missing
demiros-etal-2000-named,M98-1012,0,\N,Missing
demiros-etal-2000-named,M98-1014,0,\N,Missing
demiros-etal-2000-named,M98-1021,0,\N,Missing
demiros-etal-2000-named,M98-1018,0,\N,Missing
demiros-etal-2000-named,M98-1019,0,\N,Missing
demiros-etal-2000-named,M98-1020,0,\N,Missing
demiros-etal-2000-named,M95-1014,0,\N,Missing
demiros-etal-2000-named,M95-1012,0,\N,Missing
demiros-etal-2000-named,C96-1072,0,\N,Missing
demiros-etal-2000-named,A97-1031,0,\N,Missing
demiros-etal-2000-named,M95-1017,0,\N,Missing
demiros-etal-2000-named,A97-1029,0,\N,Missing
E17-1046,S15-2107,0,0.0230441,"neural network layer between the left and right context in a deep neural network structure but require a combination of three corpora for training and evaluation. Results show that conventional neural network models like LSTM are incapable of explicitly capturing important context information of a target (Tang et al., 2016b). Tang et al. (2016a) also experiment with adding attention layers for LSTM but Related Work: Target-dependent Sentiment Classification on Twitter The 2015 Semeval challenge introduced a task on target-specific Twitter sentiment (Rosenthal et al., 2015) which most systems (Boag et al., 2015; Plotnikova et al., 2015) treated in the same way as tweet level sentiment. The best performing system in the 2016 Semeval Twitter challenge substask B (Nakov et al., 2016), named Tweester, also performs on tweet level sentiment classification. This is unsurprising since tweets in both tasks only contain a single predefined target entity and as a result often a tweet-level approach is sufficient. An exception to tweet level approaches for this task, showing promise, is Townsend et al. (2015), who trained a SVM classifier for tweet segmentation, then used a phrase-based sentiment classifier fo"
E17-1046,P14-2009,0,0.368796,"e of election polls in both referenda and general elections (Burnap et al., 2016), it is important to understand not only the overall mood of the electorate, but also to distinguish and identify sentiment towards different key issues and entities, many of which are discussed on social media on the run up to elections. Recent developments on target-specific Twitter sentiment classification have explored different ways of modelling the association between target entities and their contexts. Jiang et al. (2011) propose a rule-based approach that utilises dependency parsing and contextual tweets. Dong et al. (2014), Tang et al. (2016a) and Zhang et al. (2016) have studied the use of different recurrent neural network models for such a task but the gain in performance from the complex neural architectures is rather unclear1 In this work we introduce the multi-targetspecific sentiment recognition task, building a corpus of tweets from the 2015 UK general election campaign suited to the task. In this dataset, target entities have been semi-automatically selected, and sentiment expressed towards multiple target entities as well as high-level topics in a tweet have been manually annotated. Unlike all existin"
E17-1046,P11-1016,0,0.242107,"te-of-the-art performance of TDParse over existing approaches for tweets with multiple targets, which encourages further research on the multi-target-specific sentiment recognition task.2 2 is very different from that in tweets. Recently Vargas et al. (2016) analysed the differences between the overall and target-dependent sentiment of tweets for three events containing 30 targets, showing many significant differences between the corresponding overall and target-dependent sentiment labels, thus confirming that these are distinct tasks. Early work tackling target-dependent sentiment in tweets (Jiang et al., 2011) designed targetdependent features manually, relying on the syntactic parse tree and a set of grammar-based rules, and incorporating the sentiment labels of related tweets to improve the classification performance. Recent work (Dong et al., 2014) used recursive neural networks and adaptively chose composition functions to combine child feature vectors according to their dependency type, to reflect sentiment signal propagation to the target. Their datadriven composition selection approach replies on the dependency types as features and a small set of rules for constructing target-dependent tree"
E17-1046,D14-1108,0,0.0309014,"15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 483–493, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics each tweet mentions a single target; we introduce a more realistic and challenging task of identifying sentiment towards multiple targets within a tweet. To tackle this task, we propose TDParse, a method that divides a tweet into different segments building on the approach introduced by Vo and Zhang (2015). TDParse exploits a syntactic dependency parser designed explicitly for tweets (Kong et al., 2014), and combines syntactic information for each target with its left-right context. We evaluate and compare our proposed system both on our new multi-target UK election dataset, as well as on the benchmarking dataset for single-target dependent sentiment (Dong et al., 2014). We show a clear state-of-the-art performance of TDParse over existing approaches for tweets with multiple targets, which encourages further research on the multi-target-specific sentiment recognition task.2 2 is very different from that in tweets. Recently Vargas et al. (2016) analysed the differences between the overall and"
E17-1046,P04-3020,0,0.131916,"rom tweetspecific parsing, in conjunction with the left-right contexts, we show the state-of-the-art performance in both single and multi-target classification tasks. We also show that the tweet level approach that many sentiment systems adopted in both Semeval challenges, fail to capture all target-sentiments in a multi-target scenario (Section 5.1). 3 the set of N words that appear in the tweet: Si = Wi1 , Wi2 , ..., WiN and our list of curated topic keywords T , the ranking function is defined as: log(|Si |) ∗ |Wi ∈ Si ∩ Wi ∈ T | where |Si |is the total number of words in the tweet; unlike Mihalcea (2004) we prefer longer tweets. We used exact matching with flexibility on the special characters at either end. TF-IDF normalisation and cosine similarity were then applied to the dataset to remove very similar tweets (empirically we set the cosine similarity threshold to 0.6). We also collected all external URLs mentioned in our dataset and their web content throughout the data harvesting period, filtering out tweets that only contain an external link or snippets of a web page. Finally we sampled 4,500 top-ranked tweets keeping the representation of tweets mentioning each election issue proportion"
E17-1046,P14-1146,0,0.0604675,"able 2 on the single-target benchmarking corpus (Dong et al., 2014), with three model categories: 1) tweet-level target-independent models, 2) targetdependent models without considering the ‘sametarget-multi-appearance’ scenario and 3) targetdependent models incorporating the ‘same-targetmulti-appearance’ scenario. We include the models presented in the previous section as well as models for target specific sentiment from the literature where possible. Among the target-independent baseline models Target-ind (Vo and Zhang, 2015) and Semevalbest have shown strong performance compared with SSWE (Tang et al., 2014) and SVM-ind (Jiang et al., 2011) as they use more features, especially rich automatic features using the embeddings of Mikolov et al. (2013). Interestingly they also perform better than some of the targetdependent baseline systems, namely SVM-dep • TDPWindow-N: the same as TDParse+ with a window to constrain the left-right context. 8 Note that this isn’t a binary classification task; the F1 score is still effected by the neutral tweets. 488 Model SSWE SVM-ind LSTM Target-ind Semeval-best SVM-dep Recursive NN AdaRNN Target-dep Target-dep+ TD-LSTM TC-LSTM GRNN G3 GRNN+G3 TDParse+ Target-dep+ (m"
E17-1046,C16-1311,0,0.374746,"in both referenda and general elections (Burnap et al., 2016), it is important to understand not only the overall mood of the electorate, but also to distinguish and identify sentiment towards different key issues and entities, many of which are discussed on social media on the run up to elections. Recent developments on target-specific Twitter sentiment classification have explored different ways of modelling the association between target entities and their contexts. Jiang et al. (2011) propose a rule-based approach that utilises dependency parsing and contextual tweets. Dong et al. (2014), Tang et al. (2016a) and Zhang et al. (2016) have studied the use of different recurrent neural network models for such a task but the gain in performance from the complex neural architectures is rather unclear1 In this work we introduce the multi-targetspecific sentiment recognition task, building a corpus of tweets from the 2015 UK general election campaign suited to the task. In this dataset, target entities have been semi-automatically selected, and sentiment expressed towards multiple target entities as well as high-level topics in a tweet have been manually annotated. Unlike all existing studies on target"
E17-1046,S16-1001,0,0.0105992,"show that conventional neural network models like LSTM are incapable of explicitly capturing important context information of a target (Tang et al., 2016b). Tang et al. (2016a) also experiment with adding attention layers for LSTM but Related Work: Target-dependent Sentiment Classification on Twitter The 2015 Semeval challenge introduced a task on target-specific Twitter sentiment (Rosenthal et al., 2015) which most systems (Boag et al., 2015; Plotnikova et al., 2015) treated in the same way as tweet level sentiment. The best performing system in the 2016 Semeval Twitter challenge substask B (Nakov et al., 2016), named Tweester, also performs on tweet level sentiment classification. This is unsurprising since tweets in both tasks only contain a single predefined target entity and as a result often a tweet-level approach is sufficient. An exception to tweet level approaches for this task, showing promise, is Townsend et al. (2015), who trained a SVM classifier for tweet segmentation, then used a phrase-based sentiment classifier for assigning sentiment around the target. The Semeval aspect-based sentiment analysis task (Pontiki et al., 2015; Pateria and Choubey, 2016) aims to identify sentiment toward"
E17-1046,D16-1021,0,0.268296,"in both referenda and general elections (Burnap et al., 2016), it is important to understand not only the overall mood of the electorate, but also to distinguish and identify sentiment towards different key issues and entities, many of which are discussed on social media on the run up to elections. Recent developments on target-specific Twitter sentiment classification have explored different ways of modelling the association between target entities and their contexts. Jiang et al. (2011) propose a rule-based approach that utilises dependency parsing and contextual tweets. Dong et al. (2014), Tang et al. (2016a) and Zhang et al. (2016) have studied the use of different recurrent neural network models for such a task but the gain in performance from the complex neural architectures is rather unclear1 In this work we introduce the multi-targetspecific sentiment recognition task, building a corpus of tweets from the 2015 UK general election campaign suited to the task. In this dataset, target entities have been semi-automatically selected, and sentiment expressed towards multiple target entities as well as high-level topics in a tweet have been manually annotated. Unlike all existing studies on target"
E17-1046,pak-paroubek-2010-twitter,0,0.0533404,"Missing"
E17-1046,S15-2110,1,0.823377,"allenge introduced a task on target-specific Twitter sentiment (Rosenthal et al., 2015) which most systems (Boag et al., 2015; Plotnikova et al., 2015) treated in the same way as tweet level sentiment. The best performing system in the 2016 Semeval Twitter challenge substask B (Nakov et al., 2016), named Tweester, also performs on tweet level sentiment classification. This is unsurprising since tweets in both tasks only contain a single predefined target entity and as a result often a tweet-level approach is sufficient. An exception to tweet level approaches for this task, showing promise, is Townsend et al. (2015), who trained a SVM classifier for tweet segmentation, then used a phrase-based sentiment classifier for assigning sentiment around the target. The Semeval aspect-based sentiment analysis task (Pontiki et al., 2015; Pateria and Choubey, 2016) aims to identify sentiment towards entityattribute pairs in customer reviews. This differs from our goal in the following way: both the entities and attributes are limited to a predefined inventory of limited size; they are aspect categories reflected in the reviews rather than specific targets, while each review only has one target entity, e.g. a laptop"
E17-1046,S16-1051,0,0.0171394,"16 Semeval Twitter challenge substask B (Nakov et al., 2016), named Tweester, also performs on tweet level sentiment classification. This is unsurprising since tweets in both tasks only contain a single predefined target entity and as a result often a tweet-level approach is sufficient. An exception to tweet level approaches for this task, showing promise, is Townsend et al. (2015), who trained a SVM classifier for tweet segmentation, then used a phrase-based sentiment classifier for assigning sentiment around the target. The Semeval aspect-based sentiment analysis task (Pontiki et al., 2015; Pateria and Choubey, 2016) aims to identify sentiment towards entityattribute pairs in customer reviews. This differs from our goal in the following way: both the entities and attributes are limited to a predefined inventory of limited size; they are aspect categories reflected in the reviews rather than specific targets, while each review only has one target entity, e.g. a laptop or a restaurant. Also sentiment classification in formal text such as product reviews 2 The data and code can be found at https://goo.gl/ S2T1GO 484 fail to achieve competitive results possibly due to the small training corpus. Going beyond t"
E17-1046,S15-2103,0,0.0136551,"r between the left and right context in a deep neural network structure but require a combination of three corpora for training and evaluation. Results show that conventional neural network models like LSTM are incapable of explicitly capturing important context information of a target (Tang et al., 2016b). Tang et al. (2016a) also experiment with adding attention layers for LSTM but Related Work: Target-dependent Sentiment Classification on Twitter The 2015 Semeval challenge introduced a task on target-specific Twitter sentiment (Rosenthal et al., 2015) which most systems (Boag et al., 2015; Plotnikova et al., 2015) treated in the same way as tweet level sentiment. The best performing system in the 2016 Semeval Twitter challenge substask B (Nakov et al., 2016), named Tweester, also performs on tweet level sentiment classification. This is unsurprising since tweets in both tasks only contain a single predefined target entity and as a result often a tweet-level approach is sufficient. An exception to tweet level approaches for this task, showing promise, is Townsend et al. (2015), who trained a SVM classifier for tweet segmentation, then used a phrase-based sentiment classifier for assigning sentiment arou"
E17-1046,S15-2082,0,0.155122,"ibly because they usually require large amount of training data. 483 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 483–493, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics each tweet mentions a single target; we introduce a more realistic and challenging task of identifying sentiment towards multiple targets within a tweet. To tackle this task, we propose TDParse, a method that divides a tweet into different segments building on the approach introduced by Vo and Zhang (2015). TDParse exploits a syntactic dependency parser designed explicitly for tweets (Kong et al., 2014), and combines syntactic information for each target with its left-right context. We evaluate and compare our proposed system both on our new multi-target UK election dataset, as well as on the benchmarking dataset for single-target dependent sentiment (Dong et al., 2014). We show a clear state-of-the-art performance of TDParse over existing approaches for tweets with multiple targets, which encourages further research on the multi-target-specific sentiment recognition task.2 2 is very different"
E17-1046,D11-1141,0,0.0224338,"filtering out tweets that only contain an external link or snippets of a web page. Finally we sampled 4,500 top-ranked tweets keeping the representation of tweets mentioning each election issue proportionate to the original dataset. For annotation we considered sentiment towards two types of targets: entities and topic keywords. Entities were processed in two ways: firstly, named entities (people, locations, and organisations) were automatically annotated by combining the output of Stanford Named Entity Recognition (NER) (Finkel et al., 2005), NLTK NER (Bird, 2006) and a Twitter-specific NER (Ritter et al., 2011). All three were combined for a more complete coverage of entities mentioned in tweets and subsequently corrected by removing wrongly marked entities through manual annotation. Secondly, to make sure we covered all key entities in the tweets, we also matched tweets against a manually curated list of 7 political-party names and added users mentioned therein as entities. The second type of targets matched the topic keywords from our curated list. Creating a Corpus for Target Specific Sentiment in Twitter We describe the design, collection and annotation of a corpus of tweets about the 2015 UK el"
E17-1046,P12-3020,0,0.0643767,"Missing"
E17-1046,P06-4018,0,\N,Missing
E17-1046,S15-2078,0,\N,Missing
E17-1046,P05-1045,0,\N,Missing
I17-3006,P11-1016,0,0.118666,"Missing"
I17-3006,E17-1046,1,0.64875,"Missing"
I17-3011,Q17-1010,0,0.0083609,"en words (Morgado da Costa et al., 2016). 2.2.3 Context-based models Previous approaches predominantly rely on ontological resources, which require a lot of human effort to build and maintain, resulting in limited coverage for new words and domains. We use distributed representations to capture word similarity based on syntactic behaviour, as they can be trained unsupervised on a large scale and are easily adapted on new language material. We train word embeddings with word2vec (Mikolov et al., 2013) on sentences from the original three corpora and also obtain pre-trained word embeddings from Bojanowski et al. (2017). The pre-trained embeddings consistently achieve better results and are hence used in all subsequent experiments. Since the head word is linguistically the most important factor for classifier selection, we first train two widely used machine learning models (SVM, Logistic Regression) on the embedding vector of the head word (head). In order to investigate to which extend context may help with classifier prediction, we then gradually add more contextual features to the models: With the motivation of reducing head word ambiguity, we include embedding vectors of words within window size n=2 of"
I17-3011,I05-3004,0,0.59573,"r Prediction Models Interactive Web Interface Figure 1: Overview of proposed system 41 The Companion Volume of the IJCNLP 2017 Proceedings: System Demonstrations, pages 41–44, c Taipei, Taiwan, November 27 – December 1, 2017. 2017 AFNLP Figure 1 gives an overview of our system. It comprises data collection, pre-processing and the compilation of the Chinese Classifier Database (section 2.1), the training of classifier prediction models (section 2.2), and the interactive online interface (section 3). 2.1 2.2 Classifier Prediction 2.2.1 Task Following the only previous machine learning approach (Guo and Zhong, 2005), we frame classifier prediction as a multi-class classification problem. However, in contrast to previous work that focused on word-based classifier prediction, we adapt the prediction task for a sentence-based scenario, which is a more natural and less ambiguous task than predicting classifiers without context. Not all sentences in the Chinese classifier database contain head words, due to co-referential and anaphoric usage. Hence, we query the database for sentences in which both the head word and corresponding classifier were identified, resulting in 681,102 sentences. This subset is rando"
I17-3011,P03-1056,0,0.0453445,"Missing"
I17-3011,mcenery-xiao-2004-lancaster,0,0.0206318,"Missing"
I17-3011,2016.gwc-1.36,0,0.535359,"quantity or frequency of its head word and requires a certain degree of shared properties between classifier and head. Although native speakers select classifiers intuitively, language learners often struggle with the correct usage of classifiers due to the lack of a similar word class in their native language. Moreover, no dictionary or finite set of rules covers all possible classifier-head combinations exhaustively. Previous research has focused on associations between classifiers and nominal head words in isolation and included approaches based on ontologies (Mok et al., 2012; Morgado da Costa et al., 2016), databases with semantic features of Chinese classifiers (Gao, (2) 一 场 精彩 的 球 one chang exciting DE ball ‘an exciting match’ This study introduces a large-scale dataset of everyday Chinese classifier usage for machine learning experiments. We present a model that outperforms previous frequency and ontology baselines for classifier prediction without the need for extensive linguistic preprocessing and head word identification. We further demonstrate the usefulness of the database and our models in use cases. 2 System Design Preprocessing sentence extraction Corpora 1 Following Huang (1998) and"
L16-1274,P14-2059,1,0.797242,"work in this area is He et al. (2010), who built an experimental citation recommendation system using the documents indexed by the CiteSeerX search engine as a test collection (over 450,000 documents), now available for testing online (Huang et al., 2014). Recently, Huang et al. (2015) improved all metrics on this task and dataset by applying multi-layered neural networks. Other techniques have been applied to this task, such as collaborative filtering (Caragea et al., 2013) and translation models (He et al., 2012), and other aspects of it have been explored, such as document representation (Duma and Klein, 2014) and context extraction (Ritchie, 2009). The work we present here is to our knowledge the first to apply the classification of rhetorical function of sentences to the CBCR task. 3. Classification of Rhetorical Function Scientific papers follow a formal structure, and the language of academia requires clear argumentation (Hyland, 2009). This has led to the creation of classification schemes for the rhetorical and argumentative structure of scientific papers. To date, the standard approach has been to take the sentence as the minimum unit of annotation, and we maintain this approach in this work"
L16-1274,liakata-etal-2010-corpora,1,0.768139,"the classification of rhetorical function of sentences to the CBCR task. 3. Classification of Rhetorical Function Scientific papers follow a formal structure, and the language of academia requires clear argumentation (Hyland, 2009). This has led to the creation of classification schemes for the rhetorical and argumentative structure of scientific papers. To date, the standard approach has been to take the sentence as the minimum unit of annotation, and we maintain this approach in this work. Two of the most prominent are Argumenative Zoning (Teufel, 2000) and Core Scientific Concepts (CoreSC, Liakata et al. (2010)). These are among the first approaches to incorporate successful automatic classification of sentences in a paper, using a supervised machine learning approach. CoreSC (Table 1) was specifically developed for the domain of biomedical science and treats papers as “humanreadable representations of scientific investigations”, aiming to retrieve the structure of the investigation from the paper (Liakata et al., 2010). Rhetorical and argumentation schemes like these have found application in experimental academic retrieval tools (Sch¨afer and Kasterka (2010), Ravenscroft et al. (2013), Angrosh et"
L16-1274,P10-1057,0,0.0141507,"r cited. We use two metrics to measure accuracy: Normalized Discounted Cumulative Gain (NDCG), a smooth discounting scheme over ranks, and top-1 accuracy, which is just the number of times the original paper was retrieved in the first position. Context extraction: Lacking a more sophisticated method, we extract the context of a citation using a symmetric window of 3 sentences: 1 before the citation, the sentence containing the citation and 1 after. This is a frequently applied method (Huang et al., 2015) and is close to what has been assumed to be the optimal window of 2 sentences up, 2 down (Qazvinian and Radev, 2010), while yielding fewer query terms and therefore allowing us more experimental freedom through faster queries. Similarity: We use the default Lucene similarity formula for assessing the relevance of a document to a context (Figure 2). In this formula, the coord term is an absolute multiplier of 1738 Figure 1: The intuition behind our approach. Depending on the function of the citation, we search for key terms in different classes of sentences, as automatically labelled. In practice, it is more finely grained by applying different weights to different document fields. score(q, d) = coord(q, d)"
L16-1274,L16-1650,1,0.889979,"Missing"
L16-1274,W10-0402,0,0.042938,"Missing"
L16-1274,W06-1613,0,0.0498216,"vestigation Means by which authors seek to achieve a goal of the investigation A method mentioned pertaining to previous work An experimental method A statement about a theoretical model or framework The data/phenomena recorded in an investigation Factual statements about the outputs of an investigation Statements inferred from observations & results relating to research hypothesis Table 1: CoreSC classes and their description. deal with the function of a citation within its argumentative context. Specific schemes for classifying the function of a citation have been developed, notably that of Teufel et al. (2006), specifically developed for Citation Function Classification. However, we are not aware of a scheme particularly tailored to our domain of biomedical science, so instead we employ CoresSC classes as proxies for citation function, which we hypothesize is valid in our domain. 4. Methodology Given that we can classify each sentence according to its rhetorical status in the document using Core Scientific Concepts, we aim to find whether a) giving higher weight to terms found in particular classes of sentences in the document collection will increase the retrieval accuracy and b) whether we can in"
L16-1650,bhowmick-etal-2010-determining,0,0.0316128,"metrics suitable for a multi-label annotation scenario include Krippendorff’s α (Krippendorff, 1970; Krippendorff, 2004), which considers difference/distance in annotation on all possible annotation units, irrespective of the number of annotators or labels and the type of annotation (categorical, numeric, ordinal). More recently (Dou and others, 2007) introduced fuzzy Kappa, a version of Cohen’s Kappa to allow for annotations with a high degree of subjectivity and which defines an agreement between fuzzy classifiers, incorporating a user-dependent probability distribution on selected values. (Bhowmick et al., 2010) extend this to multiple annotations and change the composition function within the agreement function to make it more suitable for including information on annotator confidence. We report on simple, variants on the pairwise kappa suitable for multi-label annotation and demonstrating a different degree of strictness. The rest of the paper is structured as follows. Section 2. describes the papers that were chosen for this corpus and the guidelines which have been employed by the annotators to assign CoreSC concepts. Section 3. describes various measures for inter-annotator agreement and how the"
L16-1650,W09-1325,1,0.878119,"ct that there are indeed more than one CoreSC present in a sentence”. This ensures that we avoid the unnecessary addition of multiple annotations. Annotation was performed according to the revised set of guidelines by three biology curators, after a first round of annotation of another set of three papers, to calibrate annotations and align the annotators’ understanding of the guidelines. Each of the three annotators then proceeded to annotate the same 50 CRA papers and details about this corpus are reported in the next section. To apply the annotations, they used the SAPIENT annotation tool (Liakata et al., 2009). Each of the 50 papers is annotated at the sentence level with at least one CoreSC (to a maximum of three). 3 http://repository.jisc.ac.uk/88/ C 8171 274 2 see http://www.sapientaproject.com/ Figure 1: Experimental design for this study. Three annotators independently annotate the corpus, annotations are merged into a gold standard, which is then further evaluated through building a automated classifier from this corpus. Figure 2: Example of the annotation of multiple CoreSC concepts per sentence using the SAPIENT Tool. 3. 3.1. Building the multi-CoreSC CRA corpus annotated sentences across t"
L16-1650,liakata-etal-2010-corpora,1,0.942383,"ground The CoreSC scheme, corpus and applications In order to extract semantic information from textone needs to pay attention to different aspects of the discourse such as how different sentences or clauses interconnect, alternative mentions of the same entities or concepts, change of theme or topic, communication roles served by different discourse segments (Webber et al., 2012). The largest resource for discourse annotation remains the Penn Discourse TreeBank which contains over 18k explicitly signalled relations (Prasad et al., 2014; Prasad et al., 2011). Core Scientific Concepts (CoreSC)(Liakata et al., 2010; Liakata and others, 2012) is a three-layer functional discourse scheme used to annotate scientific publications at the sentence level. The first layer corresponds to 11 categories (Hypothesis, Motivation, Background, Goal, Object, Method, Experiment, Model, Observation, Result, Conclusion), deemed suitable in expressing the structure of a scientific investigation while the second layer provides for the annotation of properties of the concepts (e.g. “New”, “Old”). A depiction of the first and second layer in a single flattened representation can be found in Table 1. The third layer of the sch"
L16-1650,D13-1070,1,0.804049,"annotate the discourse structure, at the sentence level, of 265 full text publications in the domains of chemistry and biochemistry (ART corpus) (Liakata and Soldatova, 2009; Liakata et al., 2010) (Liakata and Soldatova, 2009). The annotation was conducted by following a set of 45 page guidelines1 allocating a single CoreSC to each sentence. To date the first layer of the scheme (11 CoreSC concepts) has been used to train automated classifiers (Liakata and others, 2012) and the automatically produced annotations have been utilised to create extractive summaries, as alternatives to abstracts (Liakata et al., 2013), to allow fine-grained searches of papers and recognise paper types (e.g. reviews, research papers etc.) (Ravenscroft et al., 2013) and identify drug-drug interactions (Boyce and others, 2013). A limitation of the previous guidelines and annotation was the provision for a single CoreSC concept per sentence. However, it may be the case that more than one CoreSC concept (e.g. Goal and Method) are expressed within the same sentence. Here we address this shortcoming by adapting the annotation guidelines to make it possible to assign multiple CoreSC anno1 {http://repository.jisc.ac.uk/88/} Categor"
L16-1650,J14-4007,0,0.0267236,"ific discourse, Core Scientific Concepts, Cancer Risk Assessment 1. 1.1. Background The CoreSC scheme, corpus and applications In order to extract semantic information from textone needs to pay attention to different aspects of the discourse such as how different sentences or clauses interconnect, alternative mentions of the same entities or concepts, change of theme or topic, communication roles served by different discourse segments (Webber et al., 2012). The largest resource for discourse annotation remains the Penn Discourse TreeBank which contains over 18k explicitly signalled relations (Prasad et al., 2014; Prasad et al., 2011). Core Scientific Concepts (CoreSC)(Liakata et al., 2010; Liakata and others, 2012) is a three-layer functional discourse scheme used to annotate scientific publications at the sentence level. The first layer corresponds to 11 categories (Hypothesis, Motivation, Background, Goal, Object, Method, Experiment, Model, Observation, Result, Conclusion), deemed suitable in expressing the structure of a scientific investigation while the second layer provides for the annotation of properties of the concepts (e.g. “New”, “Old”). A depiction of the first and second layer in a singl"
L16-1650,N04-4020,0,0.127915,"version of the corpus as the final annotations are the results of discussion rather than being independently annotated. 4116 1.2. Inter annotator agreement for multiple labels Inter-annotator agreement (IAA) is commonly calculated to assess the quality of annotations in corpus linguistics as well as highlight potential outliers within a document set. Standard IAA scoring mechanisms (such as Cohen’s pairwise κ (Cohen, 1960)) assume the assignment of one category label per unit of annotation, which is not directly applicable in this case due to allowing multiple CoreSC categories per sentence. (Rosenberg and Binkowski, 2004) have suggested an extension to the pairwise κ to allow for multiple labels in a setting which assumes an ordering between multiple chosen labels, so that different weights are assigned to agreement on the first, second etc. chosen category, where all weights per annotation unit sum up to 1. We have followed this approach in one of the IAA metrics we report in the following section together with other alternatives for computing kappa. Other IAA metrics suitable for a multi-label annotation scenario include Krippendorff’s α (Krippendorff, 1970; Krippendorff, 2004), which considers difference/di"
L16-1650,W01-0708,0,0.0948932,"Missing"
liakata-etal-2010-corpora,W08-0606,0,\N,Missing
liakata-etal-2010-corpora,W09-1325,1,\N,Missing
liakata-etal-2010-corpora,D09-1155,1,\N,Missing
liakata-etal-2010-corpora,P07-1125,0,\N,Missing
liakata-etal-2010-corpora,I08-1050,0,\N,Missing
O06-5001,W02-1503,0,0.044789,"Missing"
O06-5001,P06-1020,0,0.0615077,"Missing"
O06-5001,Y05-1008,1,0.814454,"Missing"
P18-4004,C08-1075,0,0.0391122,"Missing"
P18-4004,councill-etal-2008-parscit,0,0.122543,"Missing"
P19-1268,E17-2115,0,0.0213507,"ow do you learn to program? 193190 No 268368 Nn what are the range of careers in biotechnology in indonesia? how do you tenderize beef stew meat? what is meant by ‘e‘ in mathematics? what is meant by mathematics? Table 1: Examples for difficulty cases from the development set of the Quora dataset. o=obvious, n=nonobvious, N=negative label, P=positive label Introduction Modelling semantic similarity between a pair of texts is a fundamental task in NLP with a wide range of applications (Baudiˇs et al., 2016). One area of active research is Community Question Answering (CQA) (Nakov et al., 2017; Bonadiman et al., 2017), which is concerned with the automatic answering of questions based on user generated content from Q&A websites (e.g. StackExchange) and requires modelling the semantic similarity between question and answer pairs. Another well-studied task is paraphrase detection (Socher et al., 2011; He et al., 2015; Tomar et al., 2017), which models the semantic equivalence between a pair of sentences. Evaluation for such tasks has primarily focused on metrics, such as mean average precision (MAP), F1 or accuracy, which give equal weights to all examples, regardless of their difficulty. However, as illustr"
P19-1268,S17-2003,0,0.13502,"Missing"
P19-1268,S17-2001,0,0.0263167,"paraphrase detection vs. similarity scoring), see Table 2. Semeval A Type Table 2: Selected text pair similarity data sets. Size as number of text pairs. rank=ranking task, class=classification task, regr=regression task. Datasets and Tasks 2250 2000 1750 1500 1250 1000 750 500 250 0 Task Quora MSRP STS 3. We propose alternative evaluation metrics based on example difficulty (section 5) and provide a reference implementation at https://github.com/wuningxi/LexSim. 2 Name 1.0 STS. In the case of STS, we convert the scores into binary labels. Based on the description of the relatedness scores in Cer et al. (2017), we assign a positive label if relatedness ≥ 4 and a negative one otherwise to use a similar criterion as in the other datasets. 3 Distinguishing between obvious and non-obvious examples As shown, pairs with high lexical divergence tend to have a negative label in the above datasets (e.g. No in Table 1), while low lexical divergence is associated with a positive label (e.g. Po in Table 1). Intuitively, these are cases which should be relatively easy to identify. More difficult are text pairs with a positive label but high lexical divergence (e.g. Pn in Table 1), or a negative label despite lo"
P19-1268,S15-2047,0,0.0626189,"n how well they can resolve difficult cases. We make the following contributions: 1. We propose a criterion to distinguish between obvious and non-obvious examples in text 2792 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2792–2798 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics pair similarity datasets (section 4). 2. We characterise current datasets in terms of the extent to which they contain obvious vs. non-obvious items (section 4). SemEval The SemEval Community Question Answering (CQA) dataset (Nakov et al., 2015, 2016, 2017) contains posts from the online forum Qatar Living. The task is to rank relevant posts above non-relevant ones. Each subtask involves an initial post and 10 possibly relevant posts with binary annotations. Task A contains questions and comments from the same thread, task B involves question paraphrases, and task C is similar to A but contains comments from an external thread. 0.8 In this paper, we focus on predicting the semantic similarity between two text snippets in a binary classification scenario, as the ranking scenario is only applicable to some of the datasets. Binary labe"
P19-1268,I05-5002,0,0.43093,") answer ranking (B) paraphrase ranking (C) answer ranking paraphrase detection paraphrase detection similarity scoring Semeval B 500 Negative Positive 0.2 SemEval STS The Semantic Textual Similarity Benchmark (STS) dataset (Cer et al., 2017) consists of a selection of STS SemEval shared tasks (2012-2017). It contains sentence pairs annotated with continuous semantic relatedness scores on a scale from 0 (low similarity) to 5 (high similarity). MSRP The Microsoft Research Paraphrase corpus (MSRP) is a popular paraphrase detection dataset, consisting of pairs of sentences with binary judgments (Dolan and Brockett, 2005). 0.0 Size Quora The Quora duplicate questions dataset contains a large number of question pairs with binary labels1 . The task is to predict whether two questions are paraphrases, similar to Task B of SemEval, but it is framed as a classification rather than a ranking problem. We use the same training / development / test set partition as Wang et al. (2017). We selected well-known benchmark datasets differing in size (small vs. large), document length (single sentence vs. multi-sentence), document types (declarative vs. interrogative) and tasks (answer ranking vs. paraphrase detection vs. sim"
P19-1268,P18-2124,0,0.0383673,"her et al., 2011; He et al., 2015; Tomar et al., 2017), which models the semantic equivalence between a pair of sentences. Evaluation for such tasks has primarily focused on metrics, such as mean average precision (MAP), F1 or accuracy, which give equal weights to all examples, regardless of their difficulty. However, as illustrated by the examples in Table 1, not all items within text pair similarity datasets are equally difficult to resolve. Recent work has shown the need to better understand limitations of current models and datasets in natural language understanding (Wadhwa et al., 2018a; Rajpurkar et al., 2018). For example, Kaushik and Lipton (2018) showed that models sometimes exploit dataset properties to achieve high performance even when crucial task information is withheld, and Gururangan et al. (2018) demonstrated that model performance is inflated by annotation artefacts in natural language inference tasks. In this paper, we analyse current datasets and recently proposed models by focusing on item difficulty based on shallow lexical overlap. Rodrigues et al. (2018) found declarative CQA sentence pairs to be more difficult to resolve than interrogative pairs as the latter contain more cases o"
P19-1268,N18-2017,0,0.0724551,"Missing"
P19-1268,D15-1181,0,0.0334398,"=nonobvious, N=negative label, P=positive label Introduction Modelling semantic similarity between a pair of texts is a fundamental task in NLP with a wide range of applications (Baudiˇs et al., 2016). One area of active research is Community Question Answering (CQA) (Nakov et al., 2017; Bonadiman et al., 2017), which is concerned with the automatic answering of questions based on user generated content from Q&A websites (e.g. StackExchange) and requires modelling the semantic similarity between question and answer pairs. Another well-studied task is paraphrase detection (Socher et al., 2011; He et al., 2015; Tomar et al., 2017), which models the semantic equivalence between a pair of sentences. Evaluation for such tasks has primarily focused on metrics, such as mean average precision (MAP), F1 or accuracy, which give equal weights to all examples, regardless of their difficulty. However, as illustrated by the examples in Table 1, not all items within text pair similarity datasets are equally difficult to resolve. Recent work has shown the need to better understand limitations of current models and datasets in natural language understanding (Wadhwa et al., 2018a; Rajpurkar et al., 2018). For exam"
P19-1268,L18-1513,0,0.0209613,"he need to better understand limitations of current models and datasets in natural language understanding (Wadhwa et al., 2018a; Rajpurkar et al., 2018). For example, Kaushik and Lipton (2018) showed that models sometimes exploit dataset properties to achieve high performance even when crucial task information is withheld, and Gururangan et al. (2018) demonstrated that model performance is inflated by annotation artefacts in natural language inference tasks. In this paper, we analyse current datasets and recently proposed models by focusing on item difficulty based on shallow lexical overlap. Rodrigues et al. (2018) found declarative CQA sentence pairs to be more difficult to resolve than interrogative pairs as the latter contain more cases of superficial overlap. In addition, Wadhwa et al. (2018b) showed that competitive neural reading comprehension models are susceptible to shallow patterns (e.g. lexical overlap). Our study digs deeper into these findings to investigate the properties of current text pair similarity datasets with respect to different levels of difficulty and evaluates models based on how well they can resolve difficult cases. We make the following contributions: 1. We propose a criteri"
P19-1268,W17-4121,0,0.168316,"Missing"
P19-1268,W18-2610,0,0.0414507,"Missing"
P19-1268,W18-1001,0,0.0250996,"aphrase detection (Socher et al., 2011; He et al., 2015; Tomar et al., 2017), which models the semantic equivalence between a pair of sentences. Evaluation for such tasks has primarily focused on metrics, such as mean average precision (MAP), F1 or accuracy, which give equal weights to all examples, regardless of their difficulty. However, as illustrated by the examples in Table 1, not all items within text pair similarity datasets are equally difficult to resolve. Recent work has shown the need to better understand limitations of current models and datasets in natural language understanding (Wadhwa et al., 2018a; Rajpurkar et al., 2018). For example, Kaushik and Lipton (2018) showed that models sometimes exploit dataset properties to achieve high performance even when crucial task information is withheld, and Gururangan et al. (2018) demonstrated that model performance is inflated by annotation artefacts in natural language inference tasks. In this paper, we analyse current datasets and recently proposed models by focusing on item difficulty based on shallow lexical overlap. Rodrigues et al. (2018) found declarative CQA sentence pairs to be more difficult to resolve than interrogative pairs as the l"
S14-2136,esuli-sebastiani-2006-sentiwordnet,0,0.0100656,"e level was addressed by taking the majority of annotations following the precedence neutral > pos > neg > other. This is in contrast to the approach used by Nakov et al. for SemEval which intersects annotations. Annotations at the tweet level were aggregated using a majority vote. We found that using the proportion of positive, negative and neutral words in a tweet is a surprisingly robust feature for cross-domain classification, and boosted our performance when using bigrams. 3 Subjectivity Detection and Contextual Polarity Disambiguation (Subtask A) Sentiment lexicons such as SentiWordNet (Esuli and Sebastiani, 2006), the NRC emotions lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., 2005) and the Bing Liu Lexicon (Hu and Liu, 2004) have been used for determining whether a phrase should be labelled as positive, negative or neutral within a tweet or sentence (contextual polarity). However, lexical resources are by nature non-contextual and may not have good coverage over a given domain. We instead considered how to infer contextual polarity purely from the data available. To address the problem of class imbalance in the tweets, we break the problem of contextual polarity detection into"
S14-2136,P11-2008,0,0.127678,"Missing"
S14-2136,P07-1056,0,0.134442,"Missing"
S14-2136,W10-0204,0,0.0210932,"ions following the precedence neutral > pos > neg > other. This is in contrast to the approach used by Nakov et al. for SemEval which intersects annotations. Annotations at the tweet level were aggregated using a majority vote. We found that using the proportion of positive, negative and neutral words in a tweet is a surprisingly robust feature for cross-domain classification, and boosted our performance when using bigrams. 3 Subjectivity Detection and Contextual Polarity Disambiguation (Subtask A) Sentiment lexicons such as SentiWordNet (Esuli and Sebastiani, 2006), the NRC emotions lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., 2005) and the Bing Liu Lexicon (Hu and Liu, 2004) have been used for determining whether a phrase should be labelled as positive, negative or neutral within a tweet or sentence (contextual polarity). However, lexical resources are by nature non-contextual and may not have good coverage over a given domain. We instead considered how to infer contextual polarity purely from the data available. To address the problem of class imbalance in the tweets, we break the problem of contextual polarity detection into two stages: (i) we first determine whether a given wor"
S14-2136,H05-2018,0,0.0550097,"neg > other. This is in contrast to the approach used by Nakov et al. for SemEval which intersects annotations. Annotations at the tweet level were aggregated using a majority vote. We found that using the proportion of positive, negative and neutral words in a tweet is a surprisingly robust feature for cross-domain classification, and boosted our performance when using bigrams. 3 Subjectivity Detection and Contextual Polarity Disambiguation (Subtask A) Sentiment lexicons such as SentiWordNet (Esuli and Sebastiani, 2006), the NRC emotions lexicon (Mohammad and Turney, 2010), the MPQA lexicon (Wilson et al., 2005) and the Bing Liu Lexicon (Hu and Liu, 2004) have been used for determining whether a phrase should be labelled as positive, negative or neutral within a tweet or sentence (contextual polarity). However, lexical resources are by nature non-contextual and may not have good coverage over a given domain. We instead considered how to infer contextual polarity purely from the data available. To address the problem of class imbalance in the tweets, we break the problem of contextual polarity detection into two stages: (i) we first determine whether a given word should be assigned a positive, negativ"
S14-2136,S13-2052,0,\N,Missing
S15-2110,S14-2115,0,0.0199126,"tational Linguistics and negative sentiment, which conforms to the distribution of training data in task A, as well as phrasebased classifiers trained on a balanced set of positive, negative and neutral tweets. We use the latter to identify sentiment in the vicinity of topic words in task C, for targeted sentiment assignment. In previous work (Tang et al., 2014a; Tang et al., 2014b) sentiment-specific word embeddings have been used as features for identification of tweet-level sentiment but not phrase-level sentiment. Other work which considered word embeddings for phrase level sentiment (dos Santos, 2014) did not focus on producing sentiment-specific representations and the embeddings learnt were a combination of character and word embeddings, where the relative contribution of the word embeddings is not clear. In this work we present two different strategies for learning phrase level sentiment specific word embeddings. 2.1 Feature Extraction for Task A Here we provide a detailed description of data preprocessing and feature extraction for phrase-level sentiment. Working on the training set (7,643 tweets), we replaced URLs with “URLINK”, converted everything to lower case, removed special char"
S15-2110,P11-2008,0,0.106022,"Missing"
S15-2110,P03-1054,0,0.00706247,"nding the reasons behind the manifestation of different reactions. We develop several strategies for selecting a topic-relevant portion of a tweet and use it to produce a sentiment annotation. A driving force of our approach has been to use phrase-based sentiment identification from subtask A to annotate the topicrelevant selections. 4.1 Topic Relevance Through Syntactic Relations A syntactic parser generates possible grammatical relations between words in unstructured text, which are potentially useful for capturing the context around a target topic. We experimented with the Stanford parser (Klein and Manning, 2003) and the recently released TweeboParser (Kong et al., 2014). TweeboParser is explicitly designed to parse tweets – supporting multi-word annotations and multiple roots – but instead of the popular Penn Treebank annotation it uses a simpler annotation scheme and outputs much less dependency type information and was therefore not deemed suitable for our purpose. We used the Stanford parser with a caseless parsing model, expected to work better for short documents. We define the topic-relevant portion of a tweet as the weakly connected components of the dependency graph containing a given topic w"
S15-2110,D14-1108,0,0.0534553,". We develop several strategies for selecting a topic-relevant portion of a tweet and use it to produce a sentiment annotation. A driving force of our approach has been to use phrase-based sentiment identification from subtask A to annotate the topicrelevant selections. 4.1 Topic Relevance Through Syntactic Relations A syntactic parser generates possible grammatical relations between words in unstructured text, which are potentially useful for capturing the context around a target topic. We experimented with the Stanford parser (Klein and Manning, 2003) and the recently released TweeboParser (Kong et al., 2014). TweeboParser is explicitly designed to parse tweets – supporting multi-word annotations and multiple roots – but instead of the popular Penn Treebank annotation it uses a simpler annotation scheme and outputs much less dependency type information and was therefore not deemed suitable for our purpose. We used the Stanford parser with a caseless parsing model, expected to work better for short documents. We define the topic-relevant portion of a tweet as the weakly connected components of the dependency graph containing a given topic word. 4.2 Generating Per-Token Annotations Our four differen"
S15-2110,W10-0204,0,0.0252243,"cial characters and tokenised on whitespace, as in (Brody and Diakopoulos, 2011). We decided to keep user mentions, as potentially sentiment-revealing features. We then extracted features both for the target (the designated highlighted phrase) and its context (the whole tweet): Ngrams: For a target at the position n in a tweet, we created binary unigram and bigram features of the sequence between {n 4, n + 4}, as suggested by Saif et al. (Mohammad et al., 2013). Lexicons: We used four different lexica: Bing Liu’s lexicon (Hu and Liu, 2004) (about 6,800 polarised terms), NRC’s Emotion Lexicon (Mohammad and Turney, 2010) (about 14,000 words annotated based on 10 emotional dimensions), the Sentiment140 Lexicon (62,468 unigrams, 677,968 bigrams and 480,010 non-contiguous pairs) and NRC’s Hashtag Sentiment Lexicon (Mohammad et al., 2013) (54,129 unigrams, 316,531 bigrams and 308,808 non-contiguous pairs). We extracted the number of words in the text that appear in every dimension of the Bing Liu and NRC Emotion Lexica. For every 658 lexicon, we extracted features indicating the number of positive unigrams, bigrams and pairs, their maximum sentimental value as indicated by each lexicon, the sum of their sentiment"
S15-2110,S13-2053,0,0.248377,"rases relevant to the designated topic/target and assign sentiment according to our subtask A classifier. Our submitted subtask A classifier ranked fourth in the SemEval official results while our BASELINE and µPARSE classifiers for subtask C would have ranked second. 1 Alexandra Cristea University of Warwick Bo Wang University of Warwick 2 Introduction Twitter holds great potential for analyses in the social sciences both due to its explosive popularity, increasing accessibility to large amounts of data and its dynamic nature. For sentiment analysis on twitter the best performing approaches (Mohammad et al., 2013; Zhu et al., 2014) have used a set of rich lexical features. However, the development of lexica can be time consuming and is not always suitable when shifting between domains, which examine new topics and user populations (Thelwall and Buckley, 2013). Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts (Tang et al., 2014b). Moreover, it is important to be able to identify sentiment in relation to particular entities, topics or events (aspect-based sentiment). P"
S15-2110,S14-2004,0,0.0205211,"assifier’s performance. 4 Target-Specific Sentiment: Subtask C Experiment S UBMISSION S UBMISSION -S ENTIMENT S UBMISSION -R ETOKENIZED CONLL-P ROPAGATION BASELINE µPARSE Score 22.79 29.37 27.88 31.84 46.59 46.87 Table 3: Summary of the performance of our subtask C classifiers. Table 2: The scores obtained on the test set with different features. In Table 2 we list the average F1 scores of positive and negative tweets in the test data set when 660 In subtask C the goal is to identify the sentiment targeted towards a particular topic or entity. This is closely linked to aspect-based sentiment (Pontiki et al., 2014) and is very important for understanding the reasons behind the manifestation of different reactions. We develop several strategies for selecting a topic-relevant portion of a tweet and use it to produce a sentiment annotation. A driving force of our approach has been to use phrase-based sentiment identification from subtask A to annotate the topicrelevant selections. 4.1 Topic Relevance Through Syntactic Relations A syntactic parser generates possible grammatical relations between words in unstructured text, which are potentially useful for capturing the context around a target topic. We expe"
S15-2110,E12-1049,0,0.124522,"t Lexicon (Mohammad et al., 2013) (54,129 unigrams, 316,531 bigrams and 308,808 non-contiguous pairs). We extracted the number of words in the text that appear in every dimension of the Bing Liu and NRC Emotion Lexica. For every 658 lexicon, we extracted features indicating the number of positive unigrams, bigrams and pairs, their maximum sentimental value as indicated by each lexicon, the sum of their sentiment values and the value of the last non-zero (non-neutral) token. All features were extracted both from the tweet as well as the target. Word Embeddings: We used the tweets collected by (Purver and Battersby, 2012) as training data for sentiment-specific word embeddings. These tweets contain emoticons and hashtags for six different emotions, which we group together to compile positive and negative subsets. To create phrase-level word embeddings, we applied two strategies: (i) we searched for positive and negative words (as defined in Bing Liu’s lexicon) in the corpus; (ii) we performed chi-squared feature selection and extracted the 5,000 most important tokens to be used as our index; for both strategies, we extracted the phrase included in the 2-token-length, two-sided window. The embeddings were learn"
S15-2110,S14-2033,0,0.303377,"ncreasing accessibility to large amounts of data and its dynamic nature. For sentiment analysis on twitter the best performing approaches (Mohammad et al., 2013; Zhu et al., 2014) have used a set of rich lexical features. However, the development of lexica can be time consuming and is not always suitable when shifting between domains, which examine new topics and user populations (Thelwall and Buckley, 2013). Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts (Tang et al., 2014b). Moreover, it is important to be able to identify sentiment in relation to particular entities, topics or events (aspect-based sentiment). Phrase-Based Sentiment Analysis (Subtask A) as a Means to an End (subtask C) Phrase-based sentiment analysis (subtask A) in tweets is a long standing task where the goal is to classify the sentiment of a designated expression within the tweet as either positive, negative or neutral. The state of the art for subtask A achieves high performance usually based on methodologies employing features obtained from either manually or automatically generated lexica"
S15-2110,P14-1146,0,0.641522,"ncreasing accessibility to large amounts of data and its dynamic nature. For sentiment analysis on twitter the best performing approaches (Mohammad et al., 2013; Zhu et al., 2014) have used a set of rich lexical features. However, the development of lexica can be time consuming and is not always suitable when shifting between domains, which examine new topics and user populations (Thelwall and Buckley, 2013). Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts (Tang et al., 2014b). Moreover, it is important to be able to identify sentiment in relation to particular entities, topics or events (aspect-based sentiment). Phrase-Based Sentiment Analysis (Subtask A) as a Means to an End (subtask C) Phrase-based sentiment analysis (subtask A) in tweets is a long standing task where the goal is to classify the sentiment of a designated expression within the tweet as either positive, negative or neutral. The state of the art for subtask A achieves high performance usually based on methodologies employing features obtained from either manually or automatically generated lexica"
S15-2110,H05-1044,0,0.0523721,"(<3, :DD, ;), :D, 8), :-), :), (-:) and the number of negative emoticons (:(, :’(, :/, :-(, :<). Lexicon-based features: For lexica that only provided the polarities of sentiment bearing words, we used the numbers of matched positive words and negative words in a tweet as features; for lexica that provided sentiment scores for words or ngrams, we included the sum of positive scores of matched words and the sum of negative scores of matched words as two separate features. The lexica we utilised fell into two categories: manually generated sentiment lexica like the AFINN (Nielsen, 2011), MPQA (Wilson et al., 2005), and Bing Liu’s lexica (Liu, 2010); and automatically generated sentiment lexica like the Sentiment140 (Mohammad et al., 2013) and NRC Hashtag Sentiment lexica (Mohammad et al., 2013). Word embeddings representations features: We learned positive and negative word embeddings separately by training on the HAPPY and NON - HAPPY tweets from Purver & Battersby’s multi-class Twitter emoticon and hashtag corpus (Purver and Battersby, 2012), as with subtask A. The difference with subtask A is that here we used the whole tweet as our input (compared to the two-sided window around a polarised word in"
S15-2110,S14-2077,0,0.013131,"esignated topic/target and assign sentiment according to our subtask A classifier. Our submitted subtask A classifier ranked fourth in the SemEval official results while our BASELINE and µPARSE classifiers for subtask C would have ranked second. 1 Alexandra Cristea University of Warwick Bo Wang University of Warwick 2 Introduction Twitter holds great potential for analyses in the social sciences both due to its explosive popularity, increasing accessibility to large amounts of data and its dynamic nature. For sentiment analysis on twitter the best performing approaches (Mohammad et al., 2013; Zhu et al., 2014) have used a set of rich lexical features. However, the development of lexica can be time consuming and is not always suitable when shifting between domains, which examine new topics and user populations (Thelwall and Buckley, 2013). Excitingly, the state of the art has recently shifted toward novel semi-supervised techniques such as the incorporation of word embeddings to represent the context of words and concepts (Tang et al., 2014b). Moreover, it is important to be able to identify sentiment in relation to particular entities, topics or events (aspect-based sentiment). Phrase-Based Sentime"
S15-2110,D11-1052,0,\N,Missing
S17-2006,S17-2082,0,0.162573,"Missing"
S17-2006,S17-2084,0,0.0230442,"Missing"
S17-2006,S17-2083,1,0.818965,"hin the conversations were performed through crowdsourcing – as performed to satisfactory quality already with the existing training data (Zubiaga et al., 2015a). 3 Score 0.635 0.778 0.641 0.701 0.749 0.709 0.784 0.780 0.741 0.391 Table 2: Results for Task A: port/deny/query/comment classification. supTask A, we also introduce a baseline excluding the common, low-impact “comment” class, considering accuracy over only support, deny and query. This is included as the SDQ baseline. 4 Participant Systems and Results We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc´ıa Lozano et al., 2017; Enayet and ElBeltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour veracity classification task, with participant teams coming from four continents (Europe: Germany, Sweden, UK; North America: Canada; Asia: China, India, Taiwan; Africa: Egypt), showing the global reach of the issue of"
S17-2006,W11-1701,0,0.0223707,"ce a subtask where the goal is to label the type of interaction between a given statement (rumourous tweet) and a reply tweet (the latter can be either direct or nested replies). Secondly, participants need to determine the type of response towards a rumourous tweet from a tree-structured conversation, where each tweet is not necessarily sufficiently descriptive on its own, but needs to be viewed in the context of an aggregate discussion consisting of tweets preceding it in the thread. This is more closely aligned with stance classification as defined in other domains, such as public debates (Anand et al., 2011). The latter also relates somewhat to the SemEval-2015 Task 3 on Answer Selection in Community Question Answering (Moschitti et al., 2015), where the task was to determine the quality of responses in tree-structured threads in CQA platforms. Responses to questions are classified as ‘good’, ‘potential’ or ‘bad’. Both tasks are related to textual entailment and textual similarity. However, Semeval-2015 Task3 is clearly a question answering task, the platform itself supporting a QA format in contrast with the more free-form format of conversations in Twitter. Moreover, as a question answering tas"
S17-2006,D15-1311,1,0.840265,"gorised into one of the following four categories, following Procter et al. (2013b): • Support: the author of the response supports the veracity of the rumour. • Deny: the author of the response denies the veracity of the rumour. • Query: the author of the response asks for additional evidence in relation to the veracity of the rumour. • Comment: the author of the response makes their own comment without a clear contribution to assessing the veracity of the rumour. Prior work in the area has found the task difficult, compounded by the variety present in language use between different stories (Lukasik et al., 2015; Zubiaga et al., 2017). This indicates it is challenging enough to make for an interesting SemEval shared task. 1.2 1.3 Impact Identifying the veracity of claims made on the web is an increasingly important task (Zubiaga et al., 2015b). Decision support, digital journalism and disaster response already rely on picking out such claims (Procter et al., 2013b). Additionally, web and social media are a more challenging environment than e.g. newswire, which has traditionally provided the mainstay of similar tasks (such as RTE (Bentivogli et al., 2011)). Last year we ran a workshop at WWW 2015, Rum"
S17-2006,S16-1003,0,0.0771763,"Missing"
S17-2006,S17-2080,0,0.0322263,"Missing"
S17-2006,D11-1147,0,0.479793,"y participants on these challenges. 1 Introduction and Motivation Rumours are rife on the web. False claims affect people’s perceptions of events and their behaviour, sometimes in harmful ways. With the increasing reliance on the Web – social media, in particular – as a source of information and news updates by individuals, news professionals, and automated systems, the potential disruptive impact of rumours is further accentuated. The task of analysing and determining veracity of social media content has been of recent interest to the field of natural language processing. After initial work (Qazvinian et al., 2011), increasingly advanced systems and annotation schemas have been developed to support the analysis of rumour and misinformation in text (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016b). Veracity judgment can be decomposed intuitively in terms of a comparison between assertions made in – and entailments from – a candidate text, and external world knowledge. Intermediate linguistic cues have also been 1.1 Subtask A - SDQC Support/ Rumour stance classification Related to the objective of predicting a rumour’s veracity, Subtask A deals with the complemen"
S17-2006,S17-2087,0,0.0127388,"lready with the existing training data (Zubiaga et al., 2015a). 3 Score 0.635 0.778 0.641 0.701 0.749 0.709 0.784 0.780 0.741 0.391 Table 2: Results for Task A: port/deny/query/comment classification. supTask A, we also introduce a baseline excluding the common, low-impact “comment” class, considering accuracy over only support, deny and query. This is included as the SDQ baseline. 4 Participant Systems and Results We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc´ıa Lozano et al., 2017; Enayet and ElBeltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour veracity classification task, with participant teams coming from four continents (Europe: Germany, Sweden, UK; North America: Canada; Asia: China, India, Taiwan; Africa: Egypt), showing the global reach of the issue of rumour veracity on social media. Most participants tackled Subtask A, which involves classifying"
S17-2006,S17-2085,0,0.0202189,"ing – as performed to satisfactory quality already with the existing training data (Zubiaga et al., 2015a). 3 Score 0.635 0.778 0.641 0.701 0.749 0.709 0.784 0.780 0.741 0.391 Table 2: Results for Task A: port/deny/query/comment classification. supTask A, we also introduce a baseline excluding the common, low-impact “comment” class, considering accuracy over only support, deny and query. This is included as the SDQ baseline. 4 Participant Systems and Results We have had 13 system submissions at RumourEval, eight submissions for Subtask A (Kochkina et al., 2017; Bahuleyan and Vechtomova, 2017; Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Garc´ıa Lozano et al., 2017; Enayet and ElBeltagy, 2017), the identification of stance towards rumours, and five submissions for Subtask B (Srivastava et al., 2017; Wang et al., 2017; Singh et al., 2017; Chen et al., 2017; Enayet and El-Beltagy, 2017), the rumour veracity classification task, with participant teams coming from four continents (Europe: Germany, Sweden, UK; North America: Canada; Asia: China, India, Taiwan; Africa: Egypt), showing the global reach of the issue of rumour veracity on social media. Most participants tackle"
S17-2006,S17-2086,0,0.0386461,"Missing"
S17-2083,D16-1084,1,0.910464,"Missing"
S17-2083,S16-1003,0,0.205816,"Missing"
S17-2083,D11-1147,0,0.523447,"ant step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours. In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours. We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Subtask A. 1 2 Related Work Single Tweet Stance Classification Stance classification for rumours was pioneered by Qazvinian et al. (2011) as a binary classification task (support/denial). Zeng et al. (2016) perform stance classification for rumours emerging during crises. Both works use tweets related to the same rumour during training and testing. A model based on bidirectional LSTM encoding of tweets conditioned on targets has been shown to achieve state-of-the-art on the SemEval2016 task 6 dataset (Augenstein et al., 2016). However the RumourEval task is different as it addresses conversation threads. Introduction In stance classification one is concerned with determining the attitude of the author of a text towards a target"
S17-2083,W13-4008,0,0.0272014,"training and testing. A model based on bidirectional LSTM encoding of tweets conditioned on targets has been shown to achieve state-of-the-art on the SemEval2016 task 6 dataset (Augenstein et al., 2016). However the RumourEval task is different as it addresses conversation threads. Introduction In stance classification one is concerned with determining the attitude of the author of a text towards a target (Mohammad et al., 2016). Targets can range from abstract ideas, to concrete entities and events. Stance classification is an active research area that has been studied in different domains (Ranade et al., 2013; Chuang and Hsieh, 2015). Here we focus on stance classification of tweets towards the truthfulness of rumours circulating in Twitter conversations in the context of breaking news. Each conversation is defined by a tweet that initiates the conversation and a set of nested replies to it that form a conversation thread. The goal is to classify each of the tweets in the Sequential Stance Classification Lukasik et al. (2016) and Zubiaga et al. (2016a) consider the sequential nature of tweet threads in their works. Lukasik et al. (2016) employ Hawkes processes to classify temporal sequences of twe"
S17-2083,C16-1230,1,0.878808,"stract conversation thread as either supporting, denying, querying or commenting (SDQC) on the rumour initiated by the source tweet. Being able to detect stance automatically is very useful in the context of events provoking public resonance and associated rumours, as a first step towards verification of early reports (Zhao et al., 2015). For instance, it has been shown that rumours that are later proven to be false tend to spark significantly larger numbers of denying tweets than rumours that are later confirmed to be true (Mendoza et al., 2010; Procter et al., 2013; Derczynski et al., 2014; Zubiaga et al., 2016b). Here we focus on exploiting the conversational structure of social media threads for stance classification and introduce a novel LSTM-based approach to harness conversations. This paper describes team Turing’s submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A). Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing. Stance classification is considered to be an important step towards rumour ve"
S17-2083,P16-2064,0,\N,Missing
S19-2147,N16-1138,0,0.0499239,"n propagation. Initial work on rumour detection and stance classification (Qazvinian et al., 2011) was succeeded by more elaborate systems and annotation schemas (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016). Vosoughi (2015) demonstrated the value of making use of propagation information, i.e. the ensuing discussion, in rumour verification. Stance detection is the task of classifying a text according to the position it takes with respect to a statement. Research supports the importance of this subtask as a first step to 6 veracity identification. (Ferreira and Vlachos, 2016; Enayet and El-Beltagy, 2017). Crowd response, stance and the details of rumour propagation feature in the work by Chen et al. (2016) as well as the most successful system in RumourEval 2017 (Enayet and El-Beltagy, 2017), and the highest performing systems in RumourEval 2019. 1.2 Datasets for rumour verification The UK fact-checking charity Full Fact provides a roadmap7 for development of automated fact checking. They cite open and shared evaluation as one of their five principles for international collaboration, demonstrating the continuing relevance of shared tasks in this area. Shared data"
S19-2147,S17-2083,1,0.672765,"4 (15) 0.4895 (4) 0.1272 (19) 0.3267 (16) 0.3537 (14) 0.3875 (10) 0.4384 (6) 0.3927 (9) 0.6067 (2) 0.4792 (5) 0.3699 (12) 0.4298 (8) 0.3326 0.6846 0.2165 0.1845 0.7857 0.2530 0.3364 0.7806 0.4929 0.3089 0.7698 - 0.2241 0.7115 0.2234 uses the same features as the stance classification system but produces a single output per branch. The veracity prediction for the thread is then decided using majority voting over per-branch outcomes. Stance classification baseline For subtask A we released a Keras (Chollet et al., 2015) implementation of branchLSTM, the winning system of RumourEval 2017 Task A (Kochkina et al., 2017). This system uses the conversation structure by splitting it into linear branches. It is a neural network architecture that uses LSTM layer(s) to process sequences of tweets, outputting a stance label at each time step. Each tweet is represented by the average of its word vectors 11 concatenated with a number of extra features. This baseline was outperformed by 3 submitted systems (BLCU NLP, BUT-FIT, eventAI). 4.2 Subtask B, RMSE 0.6078 (1) 0.7642 (2) 0.8012 (3) 0.8179 (5) 0.8081 (4) 0.8623 (7) 0.8623 (7) 0.8623 (7) 0.8678 (8) 0.8264 (6) 0.9148 (9) - Table 5: Results table. Ranking is in brac"
S19-2147,C18-1288,1,0.869514,"Missing"
S19-2147,P18-1184,0,0.0196785,"cite open and shared evaluation as one of their five principles for international collaboration, demonstrating the continuing relevance of shared tasks in this area. Shared datasets are a crucial part of the joint endeavour. Datasets for rumour resolution are still relatively few, and likely to be in increasing demand. In addition to the data from RumourEval 2017, the dataset released by Kwon et al. (2017) is also suitable for veracity classification. It includes 51 true rumours and 60 false rumours, where each rumour includes a stream of tweets associated with it. Twitter 15 and 16 datasets (Ma et al., 2018) contain claim propagation trees and combine tasks of rumour detection and verification in one four-way classification task (Non-rumour, True, False, Unverified). A Sina Weibo corpus is also available (Wu et al., 2015), in which 5000 posts are classified for veracity, but responses are not available. Partially generated statistical claim checking data is now becoming available in the context of the FEVER shared task, mentioned above, but is not suitable for this type of work. Twitter continues to be a highly relevant platform for rumour verification, being popular with the public as well as po"
S19-2147,N18-1202,0,0.0133727,"t al., 2018) 10 https://github.com/kochkinaelena/ RumourEval2019 11 We are using word2vec (Mikolov et al., 2013) model pretrained on the GoogleNews dataset (300d) 851 training of bidirectional representations to provide additional context. They experiment with different parameter settings and if the model increased overall performance it was added to the classifier. Interestingly the best performing system in task A (BLCU-NLP) and the third best (CLEARumor) also use pre-trained contextual embedding representations with BLCU-NLP using OpenAI GPT (Radford et al., 2018) and ClEARumor using ELMo (Peters et al., 2018). While most systems use single tweets or pairs of tweets (sourceresponse) as their underlying structure to operate on, BLCU-NLP employ an inference chain-based system for this paper. Thus they consider the conversation thread starting with a source tweet, followed by replies, in which each one responds to an earlier one in time sequence. They take each conversation thread as an inference chain and concentrate on utilizing it to solve the problem of class imbalance in subtask A and training data scarcity in subtask B. They also have augmented the training data with external public datasets. Ot"
S19-2147,D11-1147,0,0.461343,"ient skepticism and/or anxiety so as to motivate finding out the actual truth” (Zubiaga et al., 2016). One can distinguish several component to a rumour resolution pipeline such as rumour detection, rumour tracking and stance classification, leading to the final outcome of determining the veracity of a rumour (Zubiaga et al., 2018). Thus what characterises rumour verification compared to other types of fact checking is time sensitivity and the importance of dynamic interactions between users, their stance and information propagation. Initial work on rumour detection and stance classification (Qazvinian et al., 2011) was succeeded by more elaborate systems and annotation schemas (Kumar and Geethakumari, 2014; Zhang et al., 2015; Shao et al., 2016; Zubiaga et al., 2016). Vosoughi (2015) demonstrated the value of making use of propagation information, i.e. the ensuing discussion, in rumour verification. Stance detection is the task of classifying a text according to the position it takes with respect to a statement. Research supports the importance of this subtask as a first step to 6 veracity identification. (Ferreira and Vlachos, 2016; Enayet and El-Beltagy, 2017). Crowd response, stance and the details o"
S19-2147,D15-1312,0,0.023098,"Hostage-taker in supermarket siege killed, reports say. #ParisAttacks LINK [true] Veracity prediction. Example 2: u1: OMG. #Prince rumoured to be performing in Toronto today. Exciting! [false] Table 1: Examples of source tweets with veracity value of local features. Fact checking is a broad complex task, challenging the resourcefulness of even a human expert. Claims such as ”we send the EU 350 million a week” which is partially true would need to be decomposed into statements to be checked against knowledge bases and multiple sources. Ways of automating fact checking has inspired researchers (Vlachos and Riedel, 2015) and has resulted in a new shared task FEVER.6 Other research has focused on stylistic tells of untrustworthiness in the source itself (Conroy et al., 2015; Singhania et al., 2017). Rumour verification is a particular case of fact checking. Rumours are “circulating stories of questionable veracity, which are apparently credible but hard to verify, and produce sufficient skepticism and/or anxiety so as to motivate finding out the actual truth” (Zubiaga et al., 2016). One can distinguish several component to a rumour resolution pipeline such as rumour detection, rumour tracking and stance classi"
W08-2212,P98-1013,0,0.028901,"is syntactically valid combination. Similarly, an important component of reference resolution is the knowledge of what semantic category an entity falls under. For example, in ‘The crop can be used to produce ethanol. This can be used to power trucks or cars’, knowledge that ethanol is the kind of thing that can be subject of ‘power’, whereas ‘crop’ is not, is required to successfully resolve the reference of ‘this’. When considering division into semantic categories one’s immediate thought would be to take advantage of existing semantic resources (such as WordNet (Miller, 1995)) or FrameNet (Baker et al., 1998). For example, Clark and Weir (2002) calculate the probability of a noun sense appearing as a particular argument by using WordNet to generalise over the noun sense. However, even though WordNet has been extremely useful in numerous applications, many researchers have found that the fact that it is largely developed via the intuitions of lexicographers, rather than being empirically based, means that the semantic information often is poorly matched with word usage in a particular domain. Pantel and Lin (2002) and Phillips and Riloff (2002) have pointed out that WordNet often includes many rare"
W08-2212,C04-1180,0,0.029691,"ng domain specific verb senses (either in terms of verb classes or through the assignment of semantic types to the verb arguments) is applied as a proof of concept to the domain of financial news. We chose this domain since the WSJ section of the Penn Treebank II is already available in the form of predicate-argument structures, obtained according to the method described in Liakata and Pulman (2002). However, the same approach can apply to predicate-arg structures from non-treebank data such as the QLFs derived from LFG structures in Cahill et al. (2003) or semantic representations such as in Bos et al. (2004). The WSJ corpus consists of 2,454 articles with a total of 2,798 distinct verb predicates, 62 prepositional predicates and 221 copular predicates containing the verb to ‘be’. Here we are only dealing with the non-prepositional predicates. The latter follow an uneven distribution of occurrences; there is a minority of very frequent verbs whereas the majority are rather sparse. The problem with infrequent predicates is that the number of instances is often too small to allow for meaningful clustering of the verb-argument slots. To circumvent this, we pre-process predicates with low frequencies"
W08-2212,A97-1052,0,0.0601583,"ber are counted as arguments of the 1 arg1 is subject; arg2 is direct object; arg3 is indirect object, roughly 142 Liakata and Pulman less frequent semantically related predicate, so that the latter receives a count boost. For example, the words that appear as subjects of the verb ‘hit’ are also considered subjects of the verb ‘clobber’, which belongs to the same synset as ‘hit’ but is underrepresented in the corpus. This is making use of the knowledge that semantically similar verbs are similar in terms of subcategorisation (Korhonen and Preiss, 2003) and is in agreement with the approach in Briscoe and Carroll (1997) where the subcategorisation frames (SCFs) of representative verbs are merged together to form SCFs of the rest of the verbs belonging to the same semantic class. We understand that the above process may be indirectly adding false positives to the verb senses. It would be interesting in the future to examine the trade-off between boosting the counts of infrequent verbs and the addition of false positives. A second pre-processing stage was applied to the arguments of the 2,798 verb predicates. The idea underlying this process was to create a version of the predicates where obvious semantic grou"
W08-2212,J07-4004,0,0.0145586,"patterns. 3 Results and Evaluation To evaluate the semantic types assigned by the automatically derived classes as well as the transferability of the derived CPA-like patterns to unseen instances, we performed a pilot study where we applied the patterns to two randomly selected articles from the on-line versions of the WSJ and the FT from March 2008. We believe this to be a useful test for the validity of the patterns since the new articles are guaranteed to be distinct from the training WSJ data of the 90s, while still belonging to the same domain. We parsed the article using the CCG parser (Clark and Curran, 2007) and concentrated on its RASP option (Briscoe et al., 1997) output, consisting of dependency relations. Since our patterns concern the semantic typing of verb arguments, we focussed on the relations ncsubj (non-clausal subject), dobj (direct obj) and iobj (indirect obj) between a verb and the respective argument position. We ignored erroneous parses5 as well as copular predicates with the verb to ‘be’, since the CCG parser’s dependency relations did not maintain the connection between ‘be’ and the adjective or participle, making it clumsy to automatically link arguments in the way we need to."
W08-2212,J02-2003,0,0.131085,". Similarly, an important component of reference resolution is the knowledge of what semantic category an entity falls under. For example, in ‘The crop can be used to produce ethanol. This can be used to power trucks or cars’, knowledge that ethanol is the kind of thing that can be subject of ‘power’, whereas ‘crop’ is not, is required to successfully resolve the reference of ‘this’. When considering division into semantic categories one’s immediate thought would be to take advantage of existing semantic resources (such as WordNet (Miller, 1995)) or FrameNet (Baker et al., 1998). For example, Clark and Weir (2002) calculate the probability of a noun sense appearing as a particular argument by using WordNet to generalise over the noun sense. However, even though WordNet has been extremely useful in numerous applications, many researchers have found that the fact that it is largely developed via the intuitions of lexicographers, rather than being empirically based, means that the semantic information often is poorly matched with word usage in a particular domain. Pantel and Lin (2002) and Phillips and Riloff (2002) have pointed out that WordNet often includes many rare senses while missing out domainspec"
W08-2212,J05-1005,0,0.0301045,"xicosyntactic contexts, are aggregated to form intermediate clusters to which hierarchical clustering is applied for further generalisation. A very interesting aspect of this work is that concept-clusters have a dual nature, consisting both of words-terms (extension) and their lexico-syntactic contexts (intension). As is the case in our approach, cluster formation is twofold, by grouping together words according to the contexts they appear in but also by clustering contexts based on the words they share though this is mentioned as future work in Gamallo et al. (2007). However, in earlier work Gamallo et al. (2005) cluster together similar syntactic positions in Portuguese derived automatically and each cluster represents a semantic condition. Words-fillers of the common position are used to extensionally define the particular condition. Clusters are formed in two stages, where first the similarity between any two positions is calculated in terms of their common word fillers, the 20 most similar ones for each position are aggreggated and the intersection of common words kept as features. Next, basic clusters are agglomerated according to the amount Automatic Fine-Grained Semantic Classification for Doma"
W08-2212,P06-1044,0,0.116905,"clustering nouns to derive semantic classes. Work more directly comparable to ours includes Schulte im Walde (2003, 2006) who presents a method for clustering German verbs by linguistically motivated feature selection. Evaluation against a manually annotated gold standard showed that syntactic subcategorisation features were most informative whereas selectional preferences added noise to the clustering. However, the author concludes that there is no perfect choice of verb features and that some verbs can be distinguished on a coarse feature level while others require fine-grained information. Korhonen et al. (2006) also use syntactically motivated features to cluster together verbs from the biomedical domain and in more recent work (Sun et al., 2008) showed that rich syntactic information about both arguments and adjuncts of verbs constitute the best performing feature set for verb clustering. Gamallo et al. (2007) follow a similar approach to Pantel and Lin (2002) where an initial set of specific clusters, containing manually chosen terms representative of the domain as well as their lexicosyntactic contexts, are aggregated to form intermediate clusters to which hierarchical clustering is applied for f"
W08-2212,P03-1007,0,0.0247603,". Thus, words featuring as arguments of the most frequent synset member are counted as arguments of the 1 arg1 is subject; arg2 is direct object; arg3 is indirect object, roughly 142 Liakata and Pulman less frequent semantically related predicate, so that the latter receives a count boost. For example, the words that appear as subjects of the verb ‘hit’ are also considered subjects of the verb ‘clobber’, which belongs to the same synset as ‘hit’ but is underrepresented in the corpus. This is making use of the knowledge that semantically similar verbs are similar in terms of subcategorisation (Korhonen and Preiss, 2003) and is in agreement with the approach in Briscoe and Carroll (1997) where the subcategorisation frames (SCFs) of representative verbs are merged together to form SCFs of the rest of the verbs belonging to the same semantic class. We understand that the above process may be indirectly adding false positives to the verb senses. It would be interesting in the future to examine the trade-off between boosting the counts of infrequent verbs and the addition of false positives. A second pre-processing stage was applied to the arguments of the 2,798 verb predicates. The idea underlying this process w"
W08-2212,C02-1105,1,0.877323,"used to assign semantic types to arguments of verbs. We were pleasantly surprised to find almost perfect recall, and respectable precision figures. 2 Method The method of clustering together verb argument slots for obtaining domain specific verb senses (either in terms of verb classes or through the assignment of semantic types to the verb arguments) is applied as a proof of concept to the domain of financial news. We chose this domain since the WSJ section of the Penn Treebank II is already available in the form of predicate-argument structures, obtained according to the method described in Liakata and Pulman (2002). However, the same approach can apply to predicate-arg structures from non-treebank data such as the QLFs derived from LFG structures in Cahill et al. (2003) or semantic representations such as in Bos et al. (2004). The WSJ corpus consists of 2,454 articles with a total of 2,798 distinct verb predicates, 62 prepositional predicates and 221 copular predicates containing the verb to ‘be’. Here we are only dealing with the non-prepositional predicates. The latter follow an uneven distribution of occurrences; there is a minority of very frequent verbs whereas the majority are rather sparse. The p"
W08-2212,C02-1144,0,0.231079,"advantage of existing semantic resources (such as WordNet (Miller, 1995)) or FrameNet (Baker et al., 1998). For example, Clark and Weir (2002) calculate the probability of a noun sense appearing as a particular argument by using WordNet to generalise over the noun sense. However, even though WordNet has been extremely useful in numerous applications, many researchers have found that the fact that it is largely developed via the intuitions of lexicographers, rather than being empirically based, means that the semantic information often is poorly matched with word usage in a particular domain. Pantel and Lin (2002) and Phillips and Riloff (2002) have pointed out that WordNet often includes many rare senses while missing out domainspecific senses and terminology. Some authors, Kilgariff (1997) and Hanks and Pustejovsky (2004), among others, reject the basic idea shared by WordNet and FrameNet (as well as traditional dictionaries) that there is a fixed list of senses for many verbs, arguing that individual senses will often be domain specific and should be discovered empirically by examining the syntactic and semantic contexts they occur in. We are highly sympathetic to this view and in this work we assum"
W08-2212,P93-1024,0,0.15897,"uggest that the verb patterns provide reasonably full coverage of the domain, while we can assign informative fine-grained semantic types to arguments with a reasonable degree of precision. Of course, a larger evaluation would be desirable, as would some task-related measure of how much this semantic typing helps in accurate processing. We hope to do this in future work. 4 Related Work The literature on acquiring semantic classes of words is very extensive. It is mostly motivated by WSD and WSI where the aim is to discover or be able to differentiate between different senses of a target word. Pereira et al. (1993) describes a method for clustering words according to their distributions in particular syntactic contexts. Nouns for instance are classified according to their distribution as direct objects of verbs, where it is assumed that the classification of verbs and nouns co-varies. In our approach we also make this assumption and nouns are clustered indirectly by first grouping together the verb argument slots they fill. Clustering in both cases is probabilistic with the assumptions that members of the same cluster follow similar distributions or in our case a joint distribution. Phillips and Riloff"
W08-2212,W02-1017,0,0.0595792,"antic resources (such as WordNet (Miller, 1995)) or FrameNet (Baker et al., 1998). For example, Clark and Weir (2002) calculate the probability of a noun sense appearing as a particular argument by using WordNet to generalise over the noun sense. However, even though WordNet has been extremely useful in numerous applications, many researchers have found that the fact that it is largely developed via the intuitions of lexicographers, rather than being empirically based, means that the semantic information often is poorly matched with word usage in a particular domain. Pantel and Lin (2002) and Phillips and Riloff (2002) have pointed out that WordNet often includes many rare senses while missing out domainspecific senses and terminology. Some authors, Kilgariff (1997) and Hanks and Pustejovsky (2004), among others, reject the basic idea shared by WordNet and FrameNet (as well as traditional dictionaries) that there is a fixed list of senses for many verbs, arguing that individual senses will often be domain specific and should be discovered empirically by examining the syntactic and semantic contexts they occur in. We are highly sympathetic to this view and in this work we assume, as Hanks and Pustejovsky do,"
W08-2212,W04-1908,0,0.100684,"wing form when replacing class IDs with tentative semantic labels: [company_organisation] report [percentage_money_income_revenue_stock_share_asset] [proposition_stake_rate_percentage] However, this does not mean that a person cannot be the 1st argument of report; there is overlap between classes 1 and 6 (the major person class) and Figure 1 shows they are closely linked. Such proximity of classes is considered during pattern evaluation (Section 3). 148 Liakata and Pulman The patterns were stored in a MySQL database. They are partly modelled on the ‘Corpus Pattern Analysis’ model described in Pustejovsky et al. (2004). These are syntagmatic patterns representing a selection context for the predicate they include, which determines the sense of the latter although CPA Patterns as defined by Pustejovsky et al. (2004) and Rumshisky and Pustejovsky (2006) are in fact rather more detailed than our patterns. 3 Results and Evaluation To evaluate the semantic types assigned by the automatically derived classes as well as the transferability of the derived CPA-like patterns to unseen instances, we performed a pilot study where we applied the patterns to two randomly selected articles from the on-line versions of the"
W08-2212,rumshisky-pustejovsky-2006-inducing,0,0.013115,"not be the 1st argument of report; there is overlap between classes 1 and 6 (the major person class) and Figure 1 shows they are closely linked. Such proximity of classes is considered during pattern evaluation (Section 3). 148 Liakata and Pulman The patterns were stored in a MySQL database. They are partly modelled on the ‘Corpus Pattern Analysis’ model described in Pustejovsky et al. (2004). These are syntagmatic patterns representing a selection context for the predicate they include, which determines the sense of the latter although CPA Patterns as defined by Pustejovsky et al. (2004) and Rumshisky and Pustejovsky (2006) are in fact rather more detailed than our patterns. 3 Results and Evaluation To evaluate the semantic types assigned by the automatically derived classes as well as the transferability of the derived CPA-like patterns to unseen instances, we performed a pilot study where we applied the patterns to two randomly selected articles from the on-line versions of the WSJ and the FT from March 2008. We believe this to be a useful test for the validity of the patterns since the new articles are guaranteed to be distinct from the training WSJ data of the 90s, while still belonging to the same domain. W"
W08-2212,E03-1037,0,0.0574143,"Missing"
W08-2212,J06-2001,0,0.0357816,"Missing"
W08-2212,C04-1133,0,\N,Missing
W08-2212,C04-1027,1,\N,Missing
W08-2212,C98-1013,0,\N,Missing
W08-2212,P06-4020,0,\N,Missing
W09-1325,W07-1008,0,0.156855,"tence annotation of full papers, as one would need to highlight entire sentences manually. Also these systems work mainly with plain text, so they do not necessarily interpret the structural information already available in the paper, which can be crucial to annotation decisions for the type of high level annotation mentioned 2 3 http://opentextmining.org/wiki/Main Page 193 http://callisto.mitre.org/manual/use.html http://protege.stanford.edu/ Proceedings of the Workshop on BioNLP, pages 193–200, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics above. The OSCAR3 (Corbett et al., 2007) tool for the recognition and annotation of chemical named entities fully displays underlying paper information in XML but is not suited to sentence by sentence annotation. To address the above issues, we present a system (SAPIENT) for sentence by sentence annotation of scientific papers which supports ontologymotivated concepts representing the core information about scientific papers (CISP) (Soldatova and Liakata, 2007). An important aspect of the system is that although annotation is sentence based, the system caters for identifiers, which link together sentences pertaining to the same conc"
W09-1325,P07-1125,0,0.0264007,"erves XML markup and identifies sentences through the addition of in-line markup. SAPIENT has been used in a systematic study for the annotation of scientific papers with concepts representing the Core Information about Scientific Papers (CISP) to create a corpus of 225 annotated papers. 1 Introduction Given the rapid growth in the quantity of scientific literature, particularly in the Biosciences, there is an increasing need to work with full papers rather than abstracts, both to identify their key contributions and to provide some automated assistance to researchers (Karamanis et al., 2008; Medlock and Briscoe, 2007). Initiatives like OTMI1 , which aim to make full papers available to researchers for text mining purposes is further evidence that relying solely on abstracts presents important limitations for such tasks. A recent study on whether information retrieval from full text is more effective than searching abstracts alone (Lin Jimmy, 2009) showed that 1 the former is indeed the case. Their experimental results suggested that span-level analysis is a promising strategy for taking advantage of the full papers, where spans are defined as paragraphs of text assessed by humans and deemed to be relevant"
W09-1325,N06-4006,0,0.027609,"search, sentence based annotation has been used to identify text regions with scientific content of interest to the user (Wilbur et al., 2006; Shatkay et al., 2008) or zones of different rhetorical status (AZ) (Teufel and Moens, 2002). Sentences are the structural units of paragraphs and can be more flexible than paragraphs for text mining purposes other than information retrieval. Current general purpose systems for linguistic annotation such as Callisto2 allow the creation of a simple annotation schema that is a tag set augmented with simple (e.g. string) attributes for each tag. Knowtator (Ogren, 2006) is a plug-in of the knowledge representation tool Prot´eg´e3 , which works as a general purpose text annotation tool and has the advantage that it can work with complex ontologyderived schemas. However, these systems are not particularly suited to sentence by sentence annotation of full papers, as one would need to highlight entire sentences manually. Also these systems work mainly with plain text, so they do not necessarily interpret the structural information already available in the paper, which can be crucial to annotation decisions for the type of high level annotation mentioned 2 3 http"
W09-1325,J02-4002,0,0.168465,"deed the case. Their experimental results suggested that span-level analysis is a promising strategy for taking advantage of the full papers, where spans are defined as paragraphs of text assessed by humans and deemed to be relevant to one of 36 pre-defined topics. Therefore, when working with full papers, it is important to be able to identify and annotate spans of text. In previous research, sentence based annotation has been used to identify text regions with scientific content of interest to the user (Wilbur et al., 2006; Shatkay et al., 2008) or zones of different rhetorical status (AZ) (Teufel and Moens, 2002). Sentences are the structural units of paragraphs and can be more flexible than paragraphs for text mining purposes other than information retrieval. Current general purpose systems for linguistic annotation such as Callisto2 allow the creation of a simple annotation schema that is a tag set augmented with simple (e.g. string) attributes for each tag. Knowtator (Ogren, 2006) is a plug-in of the knowledge representation tool Prot´eg´e3 , which works as a general purpose text annotation tool and has the advantage that it can work with complex ontologyderived schemas. However, these systems are"
W09-1325,P06-4020,0,\N,Missing
W10-1913,P07-2009,0,0.0101004,"l verb, we used all the verbs instead. Verb Class. Because individual verbs can result in sparse data problems, we also experimented with a novel feature: verb class (e.g. the class of EXPERI MENT verbs for verbs such as measure and inject). We obtained 60 classes by clustering verbs appearing in full cancer risk assessment articles using the approach of Sun and Korhonen (2009). POS. Tense tends to vary from one category to another, e.g. past is common in RES and past partici103 7 Experimental evaluation ple in CON. We used the part-of-speech (POS) tag of each verb assigned by the C&C tagger (Curran et al., 2007) as a feature. GR. Structural information about heads and dependents has proved useful in text classification. We used grammatical relations (GRs) returned by the C&C parser as features. They consist of a named relation, a head and a dependent, and possibly extra parameters depending on the relation involved, e.g. (dobj investigate mouse). We created features for each subject (ncsubj), direct object (dobj), indirect object (iobj) and second object (obj2) relation in the corpus. Subj and Obj. As some GR features may suffer from data sparsity, we collected all the subjects and objects (appearing"
W10-1913,I08-1050,0,0.649211,"ambridge, UK yg244@cam.ac.uk alk23@cam.ac.uk Maria Liakata Aberystwyth University, UK mal@aber.ac.uk Ilona Silins Lin Sun Ulla Stenius Karolinska Institutet, SWEDEN University of Cambridge, UK Karolinska Institutet, SWEDEN Ilona.Silins@ki.se ls418@cam.ac.uk Ulla.Stenius@ki.se Abstract Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are richer in information"
W10-1913,D09-1067,1,0.195817,"of sentences, and can vary from one category to another. For example, experiment is frequent in METH and conclude in CON. Previous works have used the matrix verb of each sentence as a feature. Because the matrix verb is not the only meaningful verb, we used all the verbs instead. Verb Class. Because individual verbs can result in sparse data problems, we also experimented with a novel feature: verb class (e.g. the class of EXPERI MENT verbs for verbs such as measure and inject). We obtained 60 classes by clustering verbs appearing in full cancer risk assessment articles using the approach of Sun and Korhonen (2009). POS. Tense tends to vary from one category to another, e.g. past is common in RES and past partici103 7 Experimental evaluation ple in CON. We used the part-of-speech (POS) tag of each verb assigned by the C&C tagger (Curran et al., 2007) as a feature. GR. Structural information about heads and dependents has proved useful in text classification. We used grammatical relations (GRs) returned by the C&C parser as features. They consist of a named relation, a head and a dependent, and possibly extra parameters depending on the relation involved, e.g. (dobj investigate mouse). We created feature"
W10-1913,J02-4002,0,0.815757,"n Guo Anna Korhonen University of Cambridge, UK University of Cambridge, UK yg244@cam.ac.uk alk23@cam.ac.uk Maria Liakata Aberystwyth University, UK mal@aber.ac.uk Ilona Silins Lin Sun Ulla Stenius Karolinska Institutet, SWEDEN University of Cambridge, UK Karolinska Institutet, SWEDEN Ilona.Silins@ki.se ls418@cam.ac.uk Ulla.Stenius@ki.se Abstract Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for ful"
W10-1913,W09-1325,1,0.898833,"Missing"
W10-1913,D09-1155,0,0.337523,"ac.uk alk23@cam.ac.uk Maria Liakata Aberystwyth University, UK mal@aber.ac.uk Ilona Silins Lin Sun Ulla Stenius Karolinska Institutet, SWEDEN University of Cambridge, UK Karolinska Institutet, SWEDEN Ilona.Silins@ki.se ls418@cam.ac.uk Ulla.Stenius@ki.se Abstract Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are richer in information and also considered t"
W10-1913,liakata-etal-2010-corpora,1,0.825214,"wyth University, UK mal@aber.ac.uk Ilona Silins Lin Sun Ulla Stenius Karolinska Institutet, SWEDEN University of Cambridge, UK Karolinska Institutet, SWEDEN Ilona.Silins@ki.se ls418@cam.ac.uk Ulla.Stenius@ki.se Abstract Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are richer in information and also considered to be more in need of the definition of inform"
W10-1913,W06-3309,0,0.326688,"UK University of Cambridge, UK yg244@cam.ac.uk alk23@cam.ac.uk Maria Liakata Aberystwyth University, UK mal@aber.ac.uk Ilona Silins Lin Sun Ulla Stenius Karolinska Institutet, SWEDEN University of Cambridge, UK Karolinska Institutet, SWEDEN Ilona.Silins@ki.se ls418@cam.ac.uk Ulla.Stenius@ki.se Abstract Moens, 2002; Mizuta et al., 2005; Tbahriti et al., 2006; Ruch et al., 2007). To date, a number of different schemes and techniques have been proposed for sentence-based classification of scientific literature according to information structure, e.g. (Teufel and Moens, 2002; Mizuta et al., 2005; Lin et al., 2006; Hirohata et al., 2008; Teufel et al., 2009; Shatkay et al., 2008; Liakata et al., 2010). Some of the schemes are coarse-grained and merely classify sentences according to typical section names seen in scientific documents (Lin et al., 2006; Hirohata et al., 2008). Others are finer-grained and based e.g. on argumentative zones (Teufel and Moens, 2002; Mizuta et al., 2005; Teufel et al., 2009), qualitative dimensions (Shatkay et al., 2008) or conceptual structure (Liakata et al., 2010) of documents. The majority of such schemes have been developed for full scientific journal articles which are"
W10-1913,W09-3603,0,0.332738,"Missing"
W10-1913,W04-1205,0,\N,Missing
W10-3101,W10-1913,1,0.845906,"definition, the two schemes focus on different aspects of the papers, with CoreSCs providing more detail with respect to different types of methods and results and AZ-II looking mostly at the appropriation of knowledge claims. Based on a set of 36 papers annotated with both schemes, we were able to confirm that the two schemes are indeed complementary [Liakata et al. (2010)]. CoreSC categories provide a greater level of granularity when it comes to the content-related categories whereas AZ-II categories cover aspects of the knowledge claims that permeate across different CoreSC concepts. In [Guo et al. (2010)] we followed a similar methodology for annotating abstracts with CoreSCs and an independently produced annotation scheme for abstract sections [Hirohata et al. (2008)]. We found a subsumption relation between the schemes, with CoreSCs providing the finer granularity. To obtain the mapping between annotation schemes, which allows annotation schemes to be defined in a wider context, we ideally require annotations from different schemes to be made available for the same set of papers. However, a first interpretation of the relation between schemes can be made by mapping between annotation guidel"
W10-3101,I08-1050,0,0.154353,"nd AZ-II looking mostly at the appropriation of knowledge claims. Based on a set of 36 papers annotated with both schemes, we were able to confirm that the two schemes are indeed complementary [Liakata et al. (2010)]. CoreSC categories provide a greater level of granularity when it comes to the content-related categories whereas AZ-II categories cover aspects of the knowledge claims that permeate across different CoreSC concepts. In [Guo et al. (2010)] we followed a similar methodology for annotating abstracts with CoreSCs and an independently produced annotation scheme for abstract sections [Hirohata et al. (2008)]. We found a subsumption relation between the schemes, with CoreSCs providing the finer granularity. To obtain the mapping between annotation schemes, which allows annotation schemes to be defined in a wider context, we ideally require annotations from different schemes to be made available for the same set of papers. However, a first interpretation of the relation between schemes can be made by mapping between annotation guidelines. 5 Thoughts on using CoreSCs for Negation and Speculation Current work of ours involves automating the recognition of CoreSCs and we plan to use them to produce e"
W10-3101,W09-1325,1,0.850286,"estigation factual statements about the outputs of an investigation statements inferred from observations & results relating to research hypothesis give flat labels, we cater for the categories in table 1. The CoreSC scheme was accompanied by a set of 45 page guidelines which contain a decision tree, detailed description of the semantics of the categories, 6 rules for pairwise distinction and examples from chemistry papers. These guidelines are available from http://ie-repository.jisc.ac.uk/88/. 3 The CoreSC corpus We used the CoreSC annotation scheme and the semantic annotation tool SAPIENT [Liakata et al. (2009)] to construct a corpus of 265 annotated papers [Liakata and Soldatova (2009)] from physical chemistry and biochemistry. The CoreSC corpus was developed in two different phases. During phase I, fifteen Chemistry experts were split into five groups of three, each of which annotated eight different papers; A 16th expert annotated across groups as a consistency check. This resulted in a total of 41 papers being annotated, all of which received multiple annotations. We ranked annotators according to median success in terms of inter-annotator agreement (as measured by Cohen’s kappa) both within the"
W10-3101,liakata-etal-2010-corpora,1,0.611411,"components within the paper, and AZ-II [Teufel et al. (2009)], which assumes a paper is the attempt of claiming ownership for a new piece of knowledge and aims to recover the rhetorical structure and the relevant stages in the argumentation. By definition, the two schemes focus on different aspects of the papers, with CoreSCs providing more detail with respect to different types of methods and results and AZ-II looking mostly at the appropriation of knowledge claims. Based on a set of 36 papers annotated with both schemes, we were able to confirm that the two schemes are indeed complementary [Liakata et al. (2010)]. CoreSC categories provide a greater level of granularity when it comes to the content-related categories whereas AZ-II categories cover aspects of the knowledge claims that permeate across different CoreSC concepts. In [Guo et al. (2010)] we followed a similar methodology for annotating abstracts with CoreSCs and an independently produced annotation scheme for abstract sections [Hirohata et al. (2008)]. We found a subsumption relation between the schemes, with CoreSCs providing the finer granularity. To obtain the mapping between annotation schemes, which allows annotation schemes to be def"
W10-3101,W04-3103,0,0.0332959,"n, while at the same time it can be used in combination with a CoreSC to infer the type of knowledge obtained (e.g. a positive or negative result). We plan to use automatic methods for recognising negation patterns in CoreSCs and relate them to specific CoreSC categories. There is a consensus that identifying speculation is a harder task than identifying negation. Part of the problem is that “speculative assertions are to be identified on the basis of the judgements about the author’s intended meaning, rather than on the presence of certain designated hedge terms” [Medlock and Briscoe (2007); Light et al. (2004)]. When annotating papers with CoreSCs, annotators are required to understand the paper content rather than base category assignments en3 tirely on linguistic patterns. This is why we have chosen experts as annotators for the creation of the CoreSC corpus. So both speculation and CoreSC annotation appear to be higher level annotation tasks requiring comprehension of the intended meaning. Looking at the annotation guidelines for hedges [Medlock and Briscoe (2007)], it would seem that cases of hedge type 1 correspond to to CoreSC Conclusion, hedge type 2 pertains to Background, hedge type 3 woul"
W10-3101,morante-2010-descriptive,0,0.0153553,"oreSCs and we plan to use them to produce extractive summaries for papers. We are also in the process of evaluating the usefulness of CoreSCs for Cancer Risk Assessment (CRA). An important aspect of the latter is being able to distinguish between positive and negative results and assess the confidence in any conclusions drawn. This naturally leads us to the need for exploring negation and speculation, both of which are prominent in scientific papers, as well as how these two phenomena correlate to CoreSCs. While it seems that negation can be identified by means of certain linguistic patterns [Morante (2010)], different types of negation can appear throughout the paper, some pertaining to background work, problems serving as the motivation of the paper, others referring to intermediate results or conclusions. It is interesting to look at these different types of negation in the context of each of the different CoreSCs, the type of linguistic patterns used to express it and their distribution across CoreSCs. This can provide a more targetted approach to negation, while at the same time it can be used in combination with a CoreSC to infer the type of knowledge obtained (e.g. a positive or negative"
W10-3101,N07-1040,0,0.0309319,"ighlighted the need for automatic processing methods. Work by [Lin (2009)] has shown that methods such as information retrieval are more effective if zones of interest are specified within the papers. Various corpora and annotation schemes have been proposed for designating a variety of linguistic phenomena permeating scientific papers, including negation, hedges, dependencies and semantic relations [Vincze et al. (2008); Pyysalo et al. (2007); Medlock and Briscoe (2007); McIntosh and Curran (2009)]. Other schemes follow the argumentation and citation flow within papers [Teufel et al. (2009); Teufel and Siddharthan (2007)] or indeed a combination of some of the above along multiple dimensions [Shatkay et al. (2008)]. 1 The CoreSC scheme The CoreSC annotation scheme adopts the view that a scientific paper is the human-readable representation of a scientific investigation and therefore seeks to mark the components of a scientific investigation as expressed in the text. CoreSC is ontology-motivated and originates from the CISP meta-data [Soldatova and Liakata (2007)], a subset of classes from EXPO [Soldatova and King (2006)], an ontology for the description of scientific investigations. CISP consists of the conce"
W10-3101,D09-1155,0,0.173634,"the biosciences, has highlighted the need for automatic processing methods. Work by [Lin (2009)] has shown that methods such as information retrieval are more effective if zones of interest are specified within the papers. Various corpora and annotation schemes have been proposed for designating a variety of linguistic phenomena permeating scientific papers, including negation, hedges, dependencies and semantic relations [Vincze et al. (2008); Pyysalo et al. (2007); Medlock and Briscoe (2007); McIntosh and Curran (2009)]. Other schemes follow the argumentation and citation flow within papers [Teufel et al. (2009); Teufel and Siddharthan (2007)] or indeed a combination of some of the above along multiple dimensions [Shatkay et al. (2008)]. 1 The CoreSC scheme The CoreSC annotation scheme adopts the view that a scientific paper is the human-readable representation of a scientific investigation and therefore seeks to mark the components of a scientific investigation as expressed in the text. CoreSC is ontology-motivated and originates from the CISP meta-data [Soldatova and Liakata (2007)], a subset of classes from EXPO [Soldatova and King (2006)], an ontology for the description of scientific investigati"
W10-3101,W08-0606,0,\N,Missing
W10-3101,P07-1125,0,\N,Missing
W12-4305,W09-3742,1,0.893764,"Missing"
W12-4305,W10-3001,0,0.0345169,"Missing"
W12-4305,W10-1913,1,0.871066,"kas et al (2010), Morante & Sporleder (2012)). Finally, Shatkay et al (2008) define a multidimensional scheme, which combines several of the above-mentioned aspects. Recent work has compared schemes to discover mappings and relative merits. Liakata et al. (2010) compared AZ-II and CoreSC on 36 papers annotated with both schemes and found that CoreSC provides finer granularity in distinguishing content categories (e.g. methods, goals and outcomes) while the strength of AZ-II lies in detecting the attribution of knowledge claims and identifying the different functions of background information. Guo et al. (2010) compared three schemes for the identification of discourse structure in scientific abstracts from cancer research assessment articles. The work showed a subsumption relation between the scheme of Hirohata et al. (2008), a cut-down version of the 5 http://www.nactem.ac.uk/medie/ 44 scheme proposed by Teufel et al. (2009) and CoreSC (1st layer), from general to specific. 8 Conclusion We have compared three different schemes, each taking a different perspective to the annotation of scientific discourse. The comparison shows that the three schemes are complementary, with different strengths and p"
W12-4305,I08-1050,1,0.952295,"0 Result 51 0 14 0 20 0 0 0 6 18 0 0 0 0 1 103 7 OtherResult 11 0 1 0 0 1 0 1 0 10 0 0 0 0 0 12 47 OtherFact 4 0 1 0 0 2 5 3 0 7 0 0 0 0 0 2 54 RegResult 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Intertextual 13 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 Intratextual 17 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 280 4 35 0 28 3 34 4 29 127 0 24 2 0 2 178 136 Table 5: Segments vs Event Meta-Knowledge 43 7 Related Work A number of schemes for annotating scientific discourse elements at the sentence level have been proposed. Certain schemes have been aimed at abstracts, e.g., (McKnight & Srinivasan, 2003; Ruch et al., 2007; Hirohata et al., 2008; Björne et al., 2009). The work of Hirohata et al. (2009) has been integrated with the MEDIE service5 (Miyao et al., 2006), allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al., 1999; Teufel & Moens, 2002; Teufel et al., 2009; Teufel, 2010). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers. AZ was modified for the"
W12-4305,W08-0607,0,0.0612837,"Missing"
W12-4305,liakata-etal-2010-corpora,1,0.879485,") extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. Scientific discourse annotation has also targeted the retrieval of speculative text to help improve curation. For a recent overview see de Waard and Pander Maat (2012). Modality and negation in text have also been the focus of recent workshops (Farkas et al (2010), Morante & Sporleder (2012)). Finally, Shatkay et al (2008) define a multidimensional scheme, which combines several of the above-mentioned aspects. Recent work has compared schemes to discover mappings and relative merits. Liakata et al. (2010) compared AZ-II and CoreSC on 36 papers annotated with both schemes and found that CoreSC provides finer granularity in distinguishing content categories (e.g. methods, goals and outcomes) while the strength of AZ-II lies in detecting the attribution of knowledge claims and identifying the different functions of background information. Guo et al. (2010) compared three schemes for the identification of discourse structure in scientific abstracts from cancer research assessment articles. The work showed a subsumption relation between the scheme of Hirohata et al. (2008), a cut-down version of th"
W12-4305,W04-3103,0,0.0971683,"Missing"
W12-4305,P06-1128,0,0.0451617,"Missing"
W12-4305,J12-2001,0,0.0160302,"nd summarisation tasks with Computational Linguistics papers. AZ was modified for the annotation of biology papers by Mizuta et al. (2005) in order to facilitate information extraction, and more recently Teufel et al. (2009) extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. Scientific discourse annotation has also targeted the retrieval of speculative text to help improve curation. For a recent overview see de Waard and Pander Maat (2012). Modality and negation in text have also been the focus of recent workshops (Farkas et al (2010), Morante & Sporleder (2012)). Finally, Shatkay et al (2008) define a multidimensional scheme, which combines several of the above-mentioned aspects. Recent work has compared schemes to discover mappings and relative merits. Liakata et al. (2010) compared AZ-II and CoreSC on 36 papers annotated with both schemes and found that CoreSC provides finer granularity in distinguishing content categories (e.g. methods, goals and outcomes) while the strength of AZ-II lies in detecting the attribution of knowledge claims and identifying the different functions of background information. Guo et al. (2010) compared three schemes for"
W12-4305,nawaz-etal-2010-meta,1,0.821695,"rd event annotated corpora exist; examples include the GENIA Event Corpus (Kim et al., 2008), GREC (Thompson et al., 2009) and BioInfer (Pyysalo et al., 2007), in addition to the corpora produced for the shared tasks. Figure 1. Bio-Event Representation 3.2 Meta-knowledge Annotation Until recently, the only attempts to recognise information relating to the correct interpretation of events were restricted to sparse details regarding negation and speculation (Kim et al., 2011). In order to address this problem, a multidimensional annotation scheme especially tailored to bio-events was developed (Nawaz et al., 2010; Thompson et al., 2011). The scheme identifies and categorises several different types of contextual details regarding events (termed meta-knowledge), including discourse information. Different types of meta-knowledge are encoded through five distinct dimensions (Figure 2). The advantage of using multiple dimensions is that the interplay between the assigned values in each dimension can reveal both subtle and substantial differences in the types of meta-knowledge expressed. In the majority of cases, meta-knowledge is expressed through the presence of particular “clue” words or phrases, althou"
W12-4305,J02-4002,0,0.330601,"24 2 0 2 178 136 Table 5: Segments vs Event Meta-Knowledge 43 7 Related Work A number of schemes for annotating scientific discourse elements at the sentence level have been proposed. Certain schemes have been aimed at abstracts, e.g., (McKnight & Srinivasan, 2003; Ruch et al., 2007; Hirohata et al., 2008; Björne et al., 2009). The work of Hirohata et al. (2009) has been integrated with the MEDIE service5 (Miyao et al., 2006), allowing the user to query facts using conclusions, results, etc. For full papers, the most notable work has focussed on argumentative zoning (AZ) (Teufel et al., 1999; Teufel & Moens, 2002; Teufel et al., 2009; Teufel, 2010). An important aspect of AZ involves capturing the attribution of knowledge claims and citation function, and the scheme has been tested on information extraction and summarisation tasks with Computational Linguistics papers. AZ was modified for the annotation of biology papers by Mizuta et al. (2005) in order to facilitate information extraction, and more recently Teufel et al. (2009) extended the AZ scheme to better accommodate the life sciences and chemistry in particular, producing AZ-II. Scientific discourse annotation has also targeted the retrieval of"
W12-4305,D09-1155,0,0.509488,"types that seem to be exclusive and useful (de Waard & Pander Maat, 2009). Three classes of segment types are defined: − Basic segment types: segments referring directly to the topic of study – see Table 2. − ‘Other’-segment types: segments referring to conceptual or experimental work in other research papers than the current one − Regulatory segment types: ‘regulatory’ clauses that control and introduce other segments. A list of segment types is presented in Table 2; further details, including a list of all segment types and correlations with verb tense can be found in de Waard & Pander Maat (2009). The focus of this work is to identify linguistic features that characterise these discourse segment types, according to three aspects: − Verb tense, aspect, mood and voice − Semantic verb class − Epistemic modality markers So far, 6 full-text papers (comprising about 2300 segments) have been manually annotated with segment types and correlated with the above features. A first automated validation was promising (de Waard, Buitelaar and Eigener, 2009). The need for parsing at a clause level is especially prominent in biological text, since specific semantic roles are played by particular claus"
W12-4305,W08-0606,0,0.0653538,"Missing"
W12-4305,W12-4306,1,\N,Missing
W12-4305,nawaz-etal-2012-identification,1,\N,Missing
W12-4305,W11-1825,0,\N,Missing
W12-4305,E99-1015,0,\N,Missing
W12-4305,W09-1402,0,\N,Missing
W12-4305,L12-1000,0,\N,Missing
W15-3814,W13-2001,0,0.0348375,"embedding is a collective name for a set of language modelling and feature learning techniques, by which words in a vocabulary 2 2.1 Methods and results BioNLP GENIA task A series of efforts has been initiated to evaluate the available solutions and investigate potentials in event extraction technologies. Among them, the 121 Proceedings of the 2015 Workshop on Biomedical Natural Language Processing (BioNLP 2015), pages 121–126, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics periments, we use LibSVM as the implementation of SVM. BioNLP Shared Tasks (BioNLP-ST) [7] have been consistently conducted since 2009 and attracted community-wide support. BioNLP-ST GENIA task is a core task and had the third edition in 2013. The task gradually increased its difficulties and complexities, for example, by upgrading from abstract-only text to full-text articles and subsuming co-reference tasks. In the latest GENIA 2013 task, EVEX achieves the best performance (F-score: 50.97; recall: 45.44; precision: 58.03) [8]. Our system achieves a comparable result with a higher precision (Fscore 47.33; recall: 37.14; precision: 65.21). 2.2 2.3 Word embedding for trigger and arg"
W15-3814,W11-1805,1,\N,Missing
W15-3814,W13-2002,0,\N,Missing
W16-0307,W15-1201,0,0.15769,"Missing"
W16-0307,W15-1202,0,0.11317,"Missing"
W16-0307,W15-1210,0,0.0218553,"ve vocabulary while low performance scores suggest a shared vocabulary across both the subreddits. The results of this pairwise comparison are illustrated in Figure 1. More details are provided in the supplementary materials, covering the algorithm and randomisation steps. 2.4 Detecting sentiment and happiness in posts One additional aspect that can be assessed when looking at the linguistic aspect of communications on social media is the expression of sentiment. Sentiment has been noted as a crucial indicator of how much involved someone is in a specific event (Tausczik and Pennebaker, 2010; Murphy et al., 2015), and therefore can also play a role in the expression of mental illness. Some of the conditions investigated here may have characteristic mood patterns, e.g. it is likely that someone suffering from 67 depression will use negative sentiment and express unhappiness, while someone suffering from Bipolar Disorder may change between positive and negative mood expressions over time. However, by assessing sentiment and happiness for a large population of individuals, novel patterns for individual mental health problems may evolve. As part of our investigation, we used two different methods, one to"
W16-0307,D08-1020,0,0.0141667,"Understanding these features and consequently the nature and content of the posts will allow us to better design useful classification systems and predictive models. Through discussion, we determined an initial feature set of linguistic characteristics that draws on previously established measures of psychological relevance, such as LIWC and Coh-Metrix (Graesser et al., 2004). However, we note here that in order to not overload our initial feature set, we selected a subset of all the available possibilities. In our feature set, we included linguistic features introduced by Pitler and Nenkova (Pitler and Nenkova, 2008) and partially overlapping with those used in CohMetrix for predicting text quality. More specifically, we adopt features that aim at assessing the readability of textual content. Readability is a measurement that aims to assess the required education level for a reader to fully appreciate a certain content. The task of understanding textual content and assessing its quality encompasses various factors that are captured through the features that we also propose here (see supplemental material for more information about the implementation). A subset of these features have been used successfully"
W16-0310,P03-1054,0,0.0282628,"ses nodes as grammatical 98 constituents (e.g. NP, VP) using the Penn Treebank tagset (Marcus et al., 1993). Nodes are classified either as leaf nodes with terminal categories (such as noun, verb, adjective etc.) or interior nodes with non-terminal categories (e.g. verb phrase, sentence etc.). Therefore, constituency trees are quite expressive and provide us with rich information concerning the roles of elements and chunks of elements found in written natural language. In this study, we used the Probabilistic Context Free Grammar (PCFG) parser that is built into the Stanford Core NLP toolkit (Klein and Manning, 2003), a variant on the probabilistic CKY algorithm, which produces a constituency parse tree representation for each sentence. As will become clear in the sections below, we found constituency parse trees particularly useful in modelling global grammatical constraints on the scope of negation and in the context of surface mentions of the word “suicide”. Such constraints would have been harder to express using dependency parsers, although we do plan to incorporate dependency triples in future analysis. In addition, the target token was also searched for in the sentence tree, in a reduced form (by a"
W16-0310,J93-2004,0,0.0549886,"3.2 Proposed method for negation detection Our proposed methodology consisted of two steps: 1) preprocessing and formatting the data, and 2) execution of the negation resolution task. 3.2.1 Preprocessing Each sentence was preprocessed in order to prepare the input for the negation resolution algorithm in a suitable format: a syntactic representation (parse tree) and the target token (“suicide”). Our proposed methodology makes use of constituency-based parse trees. A constituency tree is a tree that categorises nodes as grammatical 98 constituents (e.g. NP, VP) using the Penn Treebank tagset (Marcus et al., 1993). Nodes are classified either as leaf nodes with terminal categories (such as noun, verb, adjective etc.) or interior nodes with non-terminal categories (e.g. verb phrase, sentence etc.). Therefore, constituency trees are quite expressive and provide us with rich information concerning the roles of elements and chunks of elements found in written natural language. In this study, we used the Probabilistic Context Free Grammar (PCFG) parser that is built into the Stanford Core NLP toolkit (Klein and Manning, 2003), a variant on the probabilistic CKY algorithm, which produces a constituency parse"
W16-0310,W08-0606,0,0.0957604,"Missing"
W17-4210,P13-2080,0,0.0273234,"ment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claims in the text. Further, we could approach incongruence as a semantic issue and look to existing work on contradiction (De Marneffe et al., 2008), contrast (Harabagiu et al., 2006) and entailment recognition (Levy et al., 2013). In doing so, we may well discover several sub-types of incongruence which may fall into different semantic categories. (13) A sausage a day could lead to cancer: Pancreatic cancer warning over processed meat (14) Rise of the hugger mugger: Sociable thieves who cuddle while they rob (15) £100 to play truant! Schools accused of bribing worst pupils to stay away when Ofsted inspectors call Molek-Kozakowska (2013) views sensationalism as a discourse strategy used to repackage information in a more exciting, extraordinary or interesting way, via the presence of several discourse illocutions (e.g."
W17-4210,P00-1041,0,0.0467958,"of headline incongruence detection is best approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claim"
W17-4210,S16-1003,0,0.0362797,"eadlines this paper aims to highlight: incongruent headlines do not necessarily adhere to an identifiable style in their surface form, but rather must be identified in relation to the text they represent. This presents significant problems for the NLP approaches so far discussed. 13 As Molek-Kozakowska (2013) used only one news source (the Daily Mail), this list may be specific to this particular newspaper’s voice and/or the knowledge, subjectivity and demographic range of the annotators. 14 See Hoffman and Justicz (2016, Appendices 1-4). 59 Finally, stance detection (Augenstein et al., 2016; Mohammad et al., 2016) has been applied in the Fake News Challenge (FNC-1)15 as a means of exploring whether different articles agree or disagree with a given headline or claim, to aid in the task of fact checking. Stance is certainly relevant to task of incongruence detection, but we argue that it is not sufficient for our task, as the headlinearticle relation may be incongruent in ways separate from (dis)agreement. Beyond the headlinearticle pair itself, however, stance detection could be used to analyse engagement and interaction with an article on social media, given that early indications suggest that users ar"
W17-4210,E17-1092,0,0.0219404,"quen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claims in the text. Further, we could approach incongruence as a semantic issue and look to existing work on contradiction (De Marneffe et al., 2008), contrast (Harabagiu et al., 2006) and entailment recognition (Levy et al., 2013). In doing so, we may well discover several sub-types of incongruence which may fall into different semantic categories. ("
W17-4210,E17-3010,0,0.094443,"ethodology, the source of the headline-article pair may well prove to be a useful feature in the broader classification process, which we will explore experimentally in future work. Arguably, the task of headline incongruence detection is best approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supp"
W17-4210,P08-1118,0,0.0662678,"Missing"
W17-4210,W03-0501,0,0.299577,"approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the identification of arguments supported by insufficient evidence. This could be viewed as very close to the task of the detection of incongruent headlines, where the headline represents an argument which is not supported by claims in the text. Further, we could approach"
W17-4210,D15-1312,0,0.129872,"n conjunction with other methodology, the source of the headline-article pair may well prove to be a useful feature in the broader classification process, which we will explore experimentally in future work. Arguably, the task of headline incongruence detection is best approached in parts: to analyse complex relationships between a headline and an entire news article is likely to be extremely difficult, not least because of their very different lengths and levels of linguistic complexity. This could therefore be facilitated with the extraction of key quotes (Pouliquen et al., 2007) or claims (Vlachos and Riedel, 2015; Thorne and Vlachos, 2017). Alternatively, one could automatically generate the statistically ‘best’ headline for an article using existing title and headline generation and summarisation methods (e.g. Banko et al. (2000); Zajic et al. (2002); Dorr et al. (2003)), and evaluate how far away the existing headline is from this in terms of a number of criteria, such as lexical choices, syntactic structure, length, tonality (sentiment or emotion), and so on. It may also be interesting to explore existing work on argument analysis: for example, Stab and Gurevych (2017) explore methods for the ident"
W17-4210,N16-1138,0,0.0443085,"not sufficient for our task, as the headlinearticle relation may be incongruent in ways separate from (dis)agreement. Beyond the headlinearticle pair itself, however, stance detection could be used to analyse engagement and interaction with an article on social media, given that early indications suggest that users are compelled to alert others when they notice that a headline is misleading. 4 Discusses: The body text discuss the same topic as the headline, but does not take a position. Unrelated: The body text discusses a different topic than the headline. Built on the data set described in Ferreira and Vlachos (2016), which is collected from rumour tracking website, Emergent17 , the corpus contains approximately 50,000 annotated headlinebody pairs. A manual analysis of the first 50 body IDs led to a number of observations on the applicability of this data set to the problem of headline incongruence. Firstly, the ‘headline’ in a pair is the claim from the original post on the website, and is as such not necessarily a gold-standard headline. In addition, a single ‘headline’ can occur with multiple article bodies, and vice versa, which means that the original relation between the two is not captured. In our"
W17-4210,E17-4007,0,\N,Missing
Y05-1008,W00-1801,0,0.0128869,"ominal and verbal derivational morphology. In the following, we describe our treatment of affixal verbal morphology and genitive compounding. These phenomena are treated within the framework of two-level morphology and implemented using the Xerox finite-state tools LEXC and XFST (Beesley and Karttunen, 2003). Malagasy also has reduplicative morphological processes, in which a new root is formed by reduplicating part or all of a basic root. It is well-known that reduplication requires special treatment in a finite-state morphological model. Although the COMPILE - REPLACE algorithm described by Beesley and Karttunen (2000; 2003) provides a means of treating these cases, we have not yet addressed the problem of reduplication in our morphological analyzer. Malagasy roots may have one or more syllables; most roots are regular or ‘strong’, and have penultimate stress if they are multisyllabic. Three-syllable roots take penultimate stress unless they end in one of the ‘weak syllables’ na/ny, ka, tra, in which case they usually receive antepenultimate stress and are called ‘weak roots’ (Keenan and Polinsky, 1998). In our analysis roots are assumed to be strong unless they are either guessed trisyllabic roots ending"
Y05-1008,W02-1503,0,0.03838,"ctive patterns of nominal and verbal morphology, describing genitive compounding and suffixation for nouns, and various derivational processes involving compounding and affixation for verbs. 1 Overview of Malagasy Morphology Malagasy is an Austronesian language spoken by about six million people on the island of Madagascar. With Welsh, it is a focus of the Verb-Initial Grammars subproject (users.ox.ac.uk/˜cpgl0015/pargram/) within the PARGRAM initiative, a collaborative project to develop computational lexicons and grammars within the shared linguistic framework of Lexical Functional Grammar (Butt et al., 2002). Because of the complicated and productive patterns of Malagasy verbal and nominal morphology, the development of such a grammar relies heavily on a computational component for morphological analysis. As with any finite-state morphological transducer, our Malagasy morphological analyzer is bidirectional: it can be used in grammatical analysis to produce morphologically analyzed input to a parser, or in generation to produce a surface form from a specification of lexical properties (Beesley and Karttunen, 2003). The research reported here is supported by a grant from the Economic and Social Re"
