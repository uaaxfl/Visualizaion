2020.acl-main.17,D17-1040,0,0.0226972,"eeds the execution of attention, which is of complexity O(M ). Usually, N is much larger than M . 4.5 Baseline We consider a baseline method using the encoderdecoder architecture, which takes the set of triples and the draft text as input and generates a revised text. We refer to the method as E NC D EC E DITOR. The encoder of E NC D EC E DITOR is the same as that of FACT E DITOR. The decoder is the standard attention and copy model, which creates and utilizes a context vector and predicts the next word at each time. The time complexity of inference in E NC D E C E DITOR is O(N 2 + N M ) (cf.,Britz et al. (2017)). Note that in fact-based text editing, usually N is very large. That means that E NC D EC E DITOR is less efficient than FACT E DITOR. 5 Experiment We conduct experiments to make comparison between FACT E DITOR and the baselines using the two datasets W EB E DIT and ROTO E DIT. 5.1 We utilize ExactMatch (EM), B LEU (Papineni et al., 2002) and S ARI (Xu et al., 2016) scores5 as evaluation metrics for fluency. We also utilize precision, recall, and F1 score as evaluation metrics for fidelity. For W EB E DIT, we extract the entities from the generated text and the reference text and then calcul"
2020.acl-main.17,D14-1179,0,0.0104714,"Missing"
2020.acl-main.17,Q18-1031,0,0.0545548,"-2 . ˆ 0 share subsequences. The set of triple templates (b) Example for deletion. The revised template y 0 and the reference template x Tˆ T is {(AGENT-1, fullName, PATIENT-1)}. Our method copies “whose full name is PATIENT-1” from the reference template x0 to create the draft template x0 . Table 2: Examples for insertion and deletion, where words in green are matched, words in gray are not matched, words in blue are copied, and words in orange are removed. Best viewed in color. searchers have developed methods to deal with the problem using other texts as templates (Hashimoto et al., 2018; Guu et al., 2018; Peng et al., 2019). The difference between the approach and factbased text editing is that the former is about tableto-text generation based on other texts, while the latter is about text-to-text generation based on structured data. 3 as t = (subj, pred, obj). For simplicity, we refer to the nouns or noun phrases of subject and object simply as entities. The revised text is a sequence of words denoted as y. The draft text is a sequence of words denoted as x. Given the set of triples T and the revised text y, we aim to create a draft text x, such that x is not in accordance with T , in contra"
2020.acl-main.17,I05-5002,0,0.0321315,"en a sequence of words, where the edits include KEEP, DROP, and ADD. Malmi et al. (2019) propose a sequential tagging model that assigns a tag (KEEP or DELETE) to each word in the input sequence and also decides whether to add a phrase before the word. Our proposed approach is also based on sequential tagging of actions. It is designed for fact-based text editing, not text-to-text generation, however. 2.2 Related Work Text Editing Text editing has been studied in different settings such as automatic post-editing (Knight and Chander, 1994; Simard et al., 2007; Yang et al., 2017), paraphrasing (Dolan and Brockett, 2005), sentence simplification (Inui et al., 2003; Wubben et al., 2012), grammar error correction (Ng et al., 2014), and text style transfer (Shen et al., 2017; Hu et al., 2017). The rise of encoder-decoder models (Cho et al., 2014; Sutskever et al., 2014) as well as the attention (Bahdanau et al., 2015; Vaswani et al., 2017) 1 The datasets are publicly available at https:// github.com/isomap/factedit Table-to-Text Generation Table-to-text generation is the task which aims to generate a text from structured data (Reiter and Dale, 2000; Gatt and Krahmer, 2018), for example, a text from an infobox ab"
2020.acl-main.17,P19-1331,0,0.0881889,"and an evaluator. Yin et al. (2019) formalize the problem of text edit as learning and utilization of edit representations and propose an encoder-decoder model for the task. Zhao et al. (2018) integrate paraphrasing rules with the Transformer model for text simplification. Zhao et al. (2019) proposes a method for English grammar correction using a Transformer and copy mechanism. Another approach to text editing is to view the problem as sequential tagging instead of encoderdecoder. In this way, the efficiency of learning and prediction can be significantly enhanced. Vu and Haffari (2018) and Dong et al. (2019) conduct automatic post-editing and text simplification on the basis of edit operations and employ Neural Programmer-Interpreter (Reed and De Freitas, 2016) to predict the sequence of edits given a sequence of words, where the edits include KEEP, DROP, and ADD. Malmi et al. (2019) propose a sequential tagging model that assigns a tag (KEEP or DELETE) to each word in the input sequence and also decides whether to add a phrase before the word. Our proposed approach is also based on sequential tagging of actions. It is designed for fact-based text editing, not text-to-text generation, however. 2."
2020.acl-main.17,P15-1033,0,0.0134306,"ents, a buffer for storing the draft text and its representations, a stream for storing the revised text and its representations, and a memory for storing the triples and their representations, as shown in Figure 1. FACT E DITOR scans the text in the buffer, copies the parts of text from the buffer into the stream if they are described in the triples in the memory, deletes the parts of the text if they are not mentioned in the triples, and inserts new parts of next into the stream which is only presented in the triples. The architecture of FACT E DITOR is inspired by those in sentence parsing Dyer et al. (2015); Watanabe and Sumita (2015). The actual processing of FACT E DITOR is to generate a sequence of words into the stream from the given sequence of words in the buffer and the set of triples in the memory. A neural network is employed to control the entire editing process. 174 4.2 Neural Network Initialization FACT E DITOR first initializes the representations of content in the buffer, stream, and memory. There is a feed-forward network associated with the memory, utilized to create the embeddings of triples. Let M denote the number of triples. The embedding of triple tj , j = 1, · · · , M is ca"
2020.acl-main.17,P17-1017,0,0.171981,"s 171–182 c July 5 - 10, 2020. 2020 Association for Computational Linguistics First, we devise a method for automatically creating a dataset for fact-based text editing. Recently, several table-to-text datasets have been created and released, consisting of pairs of facts and corresponding descriptions. We leverage such kind of data in our method. We first retrieve facts and their descriptions. Next, we take the descriptions as revised texts and automatically generate draft texts based on the facts using several rules. We build two datasets for fact-based text editing on the basis of W EB NLG (Gardent et al., 2017) and ROTOW IRE, consisting of 233k and 37k instances respectively (Wiseman et al., 2017) 1 . Second, we propose a model for fact-based text editing called FACT E DITOR. One could employ an encoder-decoder model, such as an encoderdecoder model, to perform the task. The encoderdecoder model implicitly represents the actions for transforming the draft text into a revised text. In contrast, FACT E DITOR explicitly represents the actions for text editing, including Keep, Drop, and Gen, which means retention, deletion, and generation of word respectively. The model utilizes a buffer for storing the"
2020.acl-main.17,W18-2501,0,0.0133057,"he embeddings of lower-cased words from the table, and use averaged embedding to deal with the OOV problem (Moryossef et al., 2019). We tune the hyperparameters based on the B LEU score on a development set. For W EB E DIT, we set the sizes of embeddings, buffers, and triples to 300, and set the size of the stream to 600. For ROTO E DIT, we set the size of embeddings to 100 and set the sizes of buffers, triples, and stream to 200. The initial learning rate is 2e-3, and AMSGrad is used for automatically adjusting the learning rate (Reddi et al., 2018). Our implementation makes use of AllenNLP (Gardner et al., 2018). 5.2 Experimental Results Quantitative evaluation Experiment Setup The main baseline is the encoder-decoder model E NC D EC E DITOR, as explained above. We further consider three baselines, No-Editing, Table-to-Text, and Text-to-Text. In No-Editing, the draft text is directly used. In Table-to-Text, a revised text is generated from the triples using encoder-decoder. In Text-to-Text, a revised text is created from the draft text using the encoder-decoder model. Figure 2 gives illustrations of the baselines. We evaluate the results of revised texts by the models from the viewpoint of fluency an"
2020.acl-main.17,P16-1154,1,0.813421,"tively. The model utilizes a buffer for storing the draft text, a stream to store the revised text, and a memory for storing the facts. It also employs a neural network to control the entire editing process. FACT E DITOR has a lower time complexity than the encoder-decoder model, and thus it can edit a text more efficiently. Experimental results show that FACT E DITOR outperforms the baseline model of using encoderdecoder for text editing in terms of fidelity and fluency, and also show that FACT E DITOR can perform text editing faster than the encoder-decoder model. 2 2.1 and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) has dramatically changed the landscape, and now one can perform the task relatively easily with an encoder-decoder model such as Transformer provided that a sufficient amount of data is available. For example, Li et al. (2018) introduce a deep reinforcement learning framework for paraphrasing, consisting of a generator and an evaluator. Yin et al. (2019) formalize the problem of text edit as learning and utilization of edit representations and propose an encoder-decoder model for the task. Zhao et al. (2018) integrate paraphrasing rules with the Transformer model for t"
2020.acl-main.17,P16-1014,0,0.0245223,"utilizes a buffer for storing the draft text, a stream to store the revised text, and a memory for storing the facts. It also employs a neural network to control the entire editing process. FACT E DITOR has a lower time complexity than the encoder-decoder model, and thus it can edit a text more efficiently. Experimental results show that FACT E DITOR outperforms the baseline model of using encoderdecoder for text editing in terms of fidelity and fluency, and also show that FACT E DITOR can perform text editing faster than the encoder-decoder model. 2 2.1 and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) has dramatically changed the landscape, and now one can perform the task relatively easily with an encoder-decoder model such as Transformer provided that a sufficient amount of data is available. For example, Li et al. (2018) introduce a deep reinforcement learning framework for paraphrasing, consisting of a generator and an evaluator. Yin et al. (2019) formalize the problem of text edit as learning and utilization of edit representations and propose an encoder-decoder model for the task. Zhao et al. (2018) integrate paraphrasing rules with the Transformer model for text simplification. Zhao"
2020.acl-main.17,W03-1602,0,0.0119774,", DROP, and ADD. Malmi et al. (2019) propose a sequential tagging model that assigns a tag (KEEP or DELETE) to each word in the input sequence and also decides whether to add a phrase before the word. Our proposed approach is also based on sequential tagging of actions. It is designed for fact-based text editing, not text-to-text generation, however. 2.2 Related Work Text Editing Text editing has been studied in different settings such as automatic post-editing (Knight and Chander, 1994; Simard et al., 2007; Yang et al., 2017), paraphrasing (Dolan and Brockett, 2005), sentence simplification (Inui et al., 2003; Wubben et al., 2012), grammar error correction (Ng et al., 2014), and text style transfer (Shen et al., 2017; Hu et al., 2017). The rise of encoder-decoder models (Cho et al., 2014; Sutskever et al., 2014) as well as the attention (Bahdanau et al., 2015; Vaswani et al., 2017) 1 The datasets are publicly available at https:// github.com/isomap/factedit Table-to-Text Generation Table-to-text generation is the task which aims to generate a text from structured data (Reiter and Dale, 2000; Gatt and Krahmer, 2018), for example, a text from an infobox about a term in biology in wikipedia (Lebret e"
2020.acl-main.17,D16-1128,0,0.0610668,"Missing"
2020.acl-main.17,D18-1421,1,0.840181,"han the encoder-decoder model, and thus it can edit a text more efficiently. Experimental results show that FACT E DITOR outperforms the baseline model of using encoderdecoder for text editing in terms of fidelity and fluency, and also show that FACT E DITOR can perform text editing faster than the encoder-decoder model. 2 2.1 and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) has dramatically changed the landscape, and now one can perform the task relatively easily with an encoder-decoder model such as Transformer provided that a sufficient amount of data is available. For example, Li et al. (2018) introduce a deep reinforcement learning framework for paraphrasing, consisting of a generator and an evaluator. Yin et al. (2019) formalize the problem of text edit as learning and utilization of edit representations and propose an encoder-decoder model for the task. Zhao et al. (2018) integrate paraphrasing rules with the Transformer model for text simplification. Zhao et al. (2019) proposes a method for English grammar correction using a Transformer and copy mechanism. Another approach to text editing is to view the problem as sequential tagging instead of encoderdecoder. In this way, the e"
2020.acl-main.17,D19-1510,0,0.0187564,"et al. (2019) proposes a method for English grammar correction using a Transformer and copy mechanism. Another approach to text editing is to view the problem as sequential tagging instead of encoderdecoder. In this way, the efficiency of learning and prediction can be significantly enhanced. Vu and Haffari (2018) and Dong et al. (2019) conduct automatic post-editing and text simplification on the basis of edit operations and employ Neural Programmer-Interpreter (Reed and De Freitas, 2016) to predict the sequence of edits given a sequence of words, where the edits include KEEP, DROP, and ADD. Malmi et al. (2019) propose a sequential tagging model that assigns a tag (KEEP or DELETE) to each word in the input sequence and also decides whether to add a phrase before the word. Our proposed approach is also based on sequential tagging of actions. It is designed for fact-based text editing, not text-to-text generation, however. 2.2 Related Work Text Editing Text editing has been studied in different settings such as automatic post-editing (Knight and Chander, 1994; Simard et al., 2007; Yang et al., 2017), paraphrasing (Dolan and Brockett, 2005), sentence simplification (Inui et al., 2003; Wubben et al., 20"
2020.acl-main.17,N19-1236,0,0.0435106,"Missing"
2020.acl-main.17,W14-1701,0,0.0252046,"odel that assigns a tag (KEEP or DELETE) to each word in the input sequence and also decides whether to add a phrase before the word. Our proposed approach is also based on sequential tagging of actions. It is designed for fact-based text editing, not text-to-text generation, however. 2.2 Related Work Text Editing Text editing has been studied in different settings such as automatic post-editing (Knight and Chander, 1994; Simard et al., 2007; Yang et al., 2017), paraphrasing (Dolan and Brockett, 2005), sentence simplification (Inui et al., 2003; Wubben et al., 2012), grammar error correction (Ng et al., 2014), and text style transfer (Shen et al., 2017; Hu et al., 2017). The rise of encoder-decoder models (Cho et al., 2014; Sutskever et al., 2014) as well as the attention (Bahdanau et al., 2015; Vaswani et al., 2017) 1 The datasets are publicly available at https:// github.com/isomap/factedit Table-to-Text Generation Table-to-text generation is the task which aims to generate a text from structured data (Reiter and Dale, 2000; Gatt and Krahmer, 2018), for example, a text from an infobox about a term in biology in wikipedia (Lebret et al., 2016) and a description of restaurant from a structured rep"
2020.acl-main.17,W17-5525,0,0.0599925,"Missing"
2020.acl-main.17,P02-1040,0,0.106938,"E DITOR is the same as that of FACT E DITOR. The decoder is the standard attention and copy model, which creates and utilizes a context vector and predicts the next word at each time. The time complexity of inference in E NC D E C E DITOR is O(N 2 + N M ) (cf.,Britz et al. (2017)). Note that in fact-based text editing, usually N is very large. That means that E NC D EC E DITOR is less efficient than FACT E DITOR. 5 Experiment We conduct experiments to make comparison between FACT E DITOR and the baselines using the two datasets W EB E DIT and ROTO E DIT. 5.1 We utilize ExactMatch (EM), B LEU (Papineni et al., 2002) and S ARI (Xu et al., 2016) scores5 as evaluation metrics for fluency. We also utilize precision, recall, and F1 score as evaluation metrics for fidelity. For W EB E DIT, we extract the entities from the generated text and the reference text and then calculate the precision, recall, and F1 scores. For ROTO E DIT, we use the information extraction tool provided by Wiseman et al. (2017) for calculation of the scores. For the embeddings of subject and object for both datasets and the embedding of the predicate for ROTO E DIT, we simply use the embedding lookup table. For the embedding of the pre"
2020.acl-main.17,N19-1263,0,0.0155411,"sequences. The set of triple templates (b) Example for deletion. The revised template y 0 and the reference template x Tˆ T is {(AGENT-1, fullName, PATIENT-1)}. Our method copies “whose full name is PATIENT-1” from the reference template x0 to create the draft template x0 . Table 2: Examples for insertion and deletion, where words in green are matched, words in gray are not matched, words in blue are copied, and words in orange are removed. Best viewed in color. searchers have developed methods to deal with the problem using other texts as templates (Hashimoto et al., 2018; Guu et al., 2018; Peng et al., 2019). The difference between the approach and factbased text editing is that the former is about tableto-text generation based on other texts, while the latter is about text-to-text generation based on structured data. 3 as t = (subj, pred, obj). For simplicity, we refer to the nouns or noun phrases of subject and object simply as entities. The revised text is a sequence of words denoted as y. The draft text is a sequence of words denoted as x. Given the set of triples T and the revised text y, we aim to create a draft text x, such that x is not in accordance with T , in contrast to y, and therefo"
2020.acl-main.17,P19-1195,0,0.0356342,"Missing"
2020.acl-main.17,D18-1341,0,0.027164,"consisting of a generator and an evaluator. Yin et al. (2019) formalize the problem of text edit as learning and utilization of edit representations and propose an encoder-decoder model for the task. Zhao et al. (2018) integrate paraphrasing rules with the Transformer model for text simplification. Zhao et al. (2019) proposes a method for English grammar correction using a Transformer and copy mechanism. Another approach to text editing is to view the problem as sequential tagging instead of encoderdecoder. In this way, the efficiency of learning and prediction can be significantly enhanced. Vu and Haffari (2018) and Dong et al. (2019) conduct automatic post-editing and text simplification on the basis of edit operations and employ Neural Programmer-Interpreter (Reed and De Freitas, 2016) to predict the sequence of edits given a sequence of words, where the edits include KEEP, DROP, and ADD. Malmi et al. (2019) propose a sequential tagging model that assigns a tag (KEEP or DELETE) to each word in the input sequence and also decides whether to add a phrase before the word. Our proposed approach is also based on sequential tagging of actions. It is designed for fact-based text editing, not text-to-text"
2020.acl-main.17,P15-1113,0,0.0141355,"toring the draft text and its representations, a stream for storing the revised text and its representations, and a memory for storing the triples and their representations, as shown in Figure 1. FACT E DITOR scans the text in the buffer, copies the parts of text from the buffer into the stream if they are described in the triples in the memory, deletes the parts of the text if they are not mentioned in the triples, and inserts new parts of next into the stream which is only presented in the triples. The architecture of FACT E DITOR is inspired by those in sentence parsing Dyer et al. (2015); Watanabe and Sumita (2015). The actual processing of FACT E DITOR is to generate a sequence of words into the stream from the given sequence of words in the buffer and the set of triples in the memory. A neural network is employed to control the entire editing process. 174 4.2 Neural Network Initialization FACT E DITOR first initializes the representations of content in the buffer, stream, and memory. There is a feed-forward network associated with the memory, utilized to create the embeddings of triples. Let M denote the number of triples. The embedding of triple tj , j = 1, · · · , M is calculated as bt st Stream Buf"
2020.acl-main.17,D17-1239,0,0.298694,"devise a method for automatically creating a dataset for fact-based text editing. Recently, several table-to-text datasets have been created and released, consisting of pairs of facts and corresponding descriptions. We leverage such kind of data in our method. We first retrieve facts and their descriptions. Next, we take the descriptions as revised texts and automatically generate draft texts based on the facts using several rules. We build two datasets for fact-based text editing on the basis of W EB NLG (Gardent et al., 2017) and ROTOW IRE, consisting of 233k and 37k instances respectively (Wiseman et al., 2017) 1 . Second, we propose a model for fact-based text editing called FACT E DITOR. One could employ an encoder-decoder model, such as an encoderdecoder model, to perform the task. The encoderdecoder model implicitly represents the actions for transforming the draft text into a revised text. In contrast, FACT E DITOR explicitly represents the actions for text editing, including Keep, Drop, and Gen, which means retention, deletion, and generation of word respectively. The model utilizes a buffer for storing the draft text, a stream to store the revised text, and a memory for storing the facts. It"
2020.acl-main.17,P12-1107,0,0.0238106,"Missing"
2020.acl-main.17,D17-1213,0,0.073264,"in the draft text, but absent from the triples. The facts in blue do not appear in the draft text, but in the triples. The task of fact-based text editing is to edit the draft text on the basis of the triples, by deleting unsupported facts and inserting missing facts while retaining supported facts. Introduction Automatic editing of text by computer is an important application, which can help human writers to write better documents in terms of accuracy, fluency, etc. The task is easier and more practical than the automatic generation of texts from scratch and is attracting attention recently (Yang et al., 2017; Yin et al., 2019). In this paper, we consider a new and specific setting of it, referred to as fact-based text editing, in which a draft text and several facts (represented in triples) are given, and the system ∗ The work was done when Hayate Iso was a research intern at ByteDance AI Lab. aims to revise the text by adding missing facts and deleting unsupported facts. Table 1 gives an example of the task. As far as we know, no previous work did address the problem. In a text-to-text generation, given a text, the system automatically creates another text, where the new text can be a text in an"
2020.acl-main.17,D18-1355,0,0.0175406,"aster than the encoder-decoder model. 2 2.1 and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) has dramatically changed the landscape, and now one can perform the task relatively easily with an encoder-decoder model such as Transformer provided that a sufficient amount of data is available. For example, Li et al. (2018) introduce a deep reinforcement learning framework for paraphrasing, consisting of a generator and an evaluator. Yin et al. (2019) formalize the problem of text edit as learning and utilization of edit representations and propose an encoder-decoder model for the task. Zhao et al. (2018) integrate paraphrasing rules with the Transformer model for text simplification. Zhao et al. (2019) proposes a method for English grammar correction using a Transformer and copy mechanism. Another approach to text editing is to view the problem as sequential tagging instead of encoderdecoder. In this way, the efficiency of learning and prediction can be significantly enhanced. Vu and Haffari (2018) and Dong et al. (2019) conduct automatic post-editing and text simplification on the basis of edit operations and employ Neural Programmer-Interpreter (Reed and De Freitas, 2016) to predict the seq"
2020.acl-main.17,N19-1014,0,0.0183747,"016) has dramatically changed the landscape, and now one can perform the task relatively easily with an encoder-decoder model such as Transformer provided that a sufficient amount of data is available. For example, Li et al. (2018) introduce a deep reinforcement learning framework for paraphrasing, consisting of a generator and an evaluator. Yin et al. (2019) formalize the problem of text edit as learning and utilization of edit representations and propose an encoder-decoder model for the task. Zhao et al. (2018) integrate paraphrasing rules with the Transformer model for text simplification. Zhao et al. (2019) proposes a method for English grammar correction using a Transformer and copy mechanism. Another approach to text editing is to view the problem as sequential tagging instead of encoderdecoder. In this way, the efficiency of learning and prediction can be significantly enhanced. Vu and Haffari (2018) and Dong et al. (2019) conduct automatic post-editing and text simplification on the basis of edit operations and employ Neural Programmer-Interpreter (Reed and De Freitas, 2016) to predict the sequence of edits given a sequence of words, where the edits include KEEP, DROP, and ADD. Malmi et al."
2020.acl-main.82,L16-1153,0,0.474633,"ds. Wrong: 他的求胜欲很强，为了越狱在挖洞。 He has a strong desire to win and is digging for prison breaks Correct: 他的求生欲很强，为了越狱在挖洞。 He has a strong desire to survive and is digging for prison breaks. Introduction Spelling error correction is an important task which aims to correct spelling errors in a text either at word-level or at character-level (Yu and Li, 2014; Yu et al., 2014; Zhang et al., 2015; Wang et al., 2018b; Hong et al., 2019; Wang et al., 2019). It is crucial for many natural language applications such as search (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016; Wang et al., 2018b), and essay scoring (Burstein and Chodorow, 1999). In this paper, we consider Chinese spelling error correction (CSC) at character-level. Spelling error correction is also a very challenging task, because to completely solve the problem the system needs to have human-level language understanding ability. There are at least two challenges here, as shown in Table 1. First, world knowledge is needed for spelling error correction. Character 字 in the first sentence is mistakenly written as 子, where 金子塔 means golden tower and 金字塔 means pyramid. Humans can correct the typo by ref"
2020.acl-main.82,W99-0411,0,0.558788,"nd is digging for prison breaks Correct: 他的求生欲很强，为了越狱在挖洞。 He has a strong desire to survive and is digging for prison breaks. Introduction Spelling error correction is an important task which aims to correct spelling errors in a text either at word-level or at character-level (Yu and Li, 2014; Yu et al., 2014; Zhang et al., 2015; Wang et al., 2018b; Hong et al., 2019; Wang et al., 2019). It is crucial for many natural language applications such as search (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016; Wang et al., 2018b), and essay scoring (Burstein and Chodorow, 1999). In this paper, we consider Chinese spelling error correction (CSC) at character-level. Spelling error correction is also a very challenging task, because to completely solve the problem the system needs to have human-level language understanding ability. There are at least two challenges here, as shown in Table 1. First, world knowledge is needed for spelling error correction. Character 字 in the first sentence is mistakenly written as 子, where 金子塔 means golden tower and 金字塔 means pyramid. Humans can correct the typo by referring to world knowledge. Second, sometimes inference is also require"
2020.acl-main.82,C10-1041,0,0.762647,"solely based on BERT. 1 Correct: 埃及有金字塔。Egypt has pyramids. Wrong: 他的求胜欲很强，为了越狱在挖洞。 He has a strong desire to win and is digging for prison breaks Correct: 他的求生欲很强，为了越狱在挖洞。 He has a strong desire to survive and is digging for prison breaks. Introduction Spelling error correction is an important task which aims to correct spelling errors in a text either at word-level or at character-level (Yu and Li, 2014; Yu et al., 2014; Zhang et al., 2015; Wang et al., 2018b; Hong et al., 2019; Wang et al., 2019). It is crucial for many natural language applications such as search (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016; Wang et al., 2018b), and essay scoring (Burstein and Chodorow, 1999). In this paper, we consider Chinese spelling error correction (CSC) at character-level. Spelling error correction is also a very challenging task, because to completely solve the problem the system needs to have human-level language understanding ability. There are at least two challenges here, as shown in Table 1. First, world knowledge is needed for spelling error correction. Character 字 in the first sentence is mistakenly written as 子, where 金子塔 means golden tower a"
2020.acl-main.82,D19-5522,0,0.476247,"on two datasets demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT. 1 Correct: 埃及有金字塔。Egypt has pyramids. Wrong: 他的求胜欲很强，为了越狱在挖洞。 He has a strong desire to win and is digging for prison breaks Correct: 他的求生欲很强，为了越狱在挖洞。 He has a strong desire to survive and is digging for prison breaks. Introduction Spelling error correction is an important task which aims to correct spelling errors in a text either at word-level or at character-level (Yu and Li, 2014; Yu et al., 2014; Zhang et al., 2015; Wang et al., 2018b; Hong et al., 2019; Wang et al., 2019). It is crucial for many natural language applications such as search (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016; Wang et al., 2018b), and essay scoring (Burstein and Chodorow, 1999). In this paper, we consider Chinese spelling error correction (CSC) at character-level. Spelling error correction is also a very challenging task, because to completely solve the problem the system needs to have human-level language understanding ability. There are at least two challenges here, as shown in Table 1. First, world knowledge"
2020.acl-main.82,2021.ccl-1.108,0,0.100573,"Missing"
2020.acl-main.82,W15-3106,0,0.405248,"子塔 means golden tower and 金字塔 means pyramid. Humans can correct the typo by referring to world knowledge. Second, sometimes inference is also required. In the second sentence, the 4-th character 生 is mistakenly written as 胜. In fact, 胜 and the surrounding characters form a new valid word 求胜欲 (desire to win), rather than the intended word 求生欲 (desire to survive). Many methods have been proposed for CSC or more generally spelling error correction. Previous approaches can be mainly divided into two categories. One employs traditional machine learning and the other deep learning (Yu et al., 2014; Tseng et al., 2015; Wang et al., 2018b). Zhang et al. (2015), for example, proposed a unified framework for CSC consisting of a pipeline of error de882 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 882–890 c July 5 - 10, 2020. 2020 Association for Computational Linguistics tection, candidate generation, and final candidate selection using traditional machine learning. Wang et al. (2019) proposed a Seq2Seq model with copy mechanism which transforms an input sentence into a new sentence with spelling errors corrected. More recently, BERT (Devlin et al., 2018), the"
2020.acl-main.82,W18-5446,0,0.101098,"Missing"
2020.acl-main.82,D18-1273,0,0.298657,"xperimental results on two datasets demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT. 1 Correct: 埃及有金字塔。Egypt has pyramids. Wrong: 他的求胜欲很强，为了越狱在挖洞。 He has a strong desire to win and is digging for prison breaks Correct: 他的求生欲很强，为了越狱在挖洞。 He has a strong desire to survive and is digging for prison breaks. Introduction Spelling error correction is an important task which aims to correct spelling errors in a text either at word-level or at character-level (Yu and Li, 2014; Yu et al., 2014; Zhang et al., 2015; Wang et al., 2018b; Hong et al., 2019; Wang et al., 2019). It is crucial for many natural language applications such as search (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016; Wang et al., 2018b), and essay scoring (Burstein and Chodorow, 1999). In this paper, we consider Chinese spelling error correction (CSC) at character-level. Spelling error correction is also a very challenging task, because to completely solve the problem the system needs to have human-level language understanding ability. There are at least two challenges here, as shown in Table 1. Fir"
2020.acl-main.82,P19-1578,0,0.327743,"onstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT. 1 Correct: 埃及有金字塔。Egypt has pyramids. Wrong: 他的求胜欲很强，为了越狱在挖洞。 He has a strong desire to win and is digging for prison breaks Correct: 他的求生欲很强，为了越狱在挖洞。 He has a strong desire to survive and is digging for prison breaks. Introduction Spelling error correction is an important task which aims to correct spelling errors in a text either at word-level or at character-level (Yu and Li, 2014; Yu et al., 2014; Zhang et al., 2015; Wang et al., 2018b; Hong et al., 2019; Wang et al., 2019). It is crucial for many natural language applications such as search (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016; Wang et al., 2018b), and essay scoring (Burstein and Chodorow, 1999). In this paper, we consider Chinese spelling error correction (CSC) at character-level. Spelling error correction is also a very challenging task, because to completely solve the problem the system needs to have human-level language understanding ability. There are at least two challenges here, as shown in Table 1. First, world knowledge is needed for spelli"
2020.acl-main.82,W14-6835,0,0.229037,"oyed in other language detectioncorrection problems. Experimental results on two datasets demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT. 1 Correct: 埃及有金字塔。Egypt has pyramids. Wrong: 他的求胜欲很强，为了越狱在挖洞。 He has a strong desire to win and is digging for prison breaks Correct: 他的求生欲很强，为了越狱在挖洞。 He has a strong desire to survive and is digging for prison breaks. Introduction Spelling error correction is an important task which aims to correct spelling errors in a text either at word-level or at character-level (Yu and Li, 2014; Yu et al., 2014; Zhang et al., 2015; Wang et al., 2018b; Hong et al., 2019; Wang et al., 2019). It is crucial for many natural language applications such as search (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016; Wang et al., 2018b), and essay scoring (Burstein and Chodorow, 1999). In this paper, we consider Chinese spelling error correction (CSC) at character-level. Spelling error correction is also a very challenging task, because to completely solve the problem the system needs to have human-level language understanding ability. There ar"
2020.acl-main.82,W14-6820,0,0.563459,"guage detectioncorrection problems. Experimental results on two datasets demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT. 1 Correct: 埃及有金字塔。Egypt has pyramids. Wrong: 他的求胜欲很强，为了越狱在挖洞。 He has a strong desire to win and is digging for prison breaks Correct: 他的求生欲很强，为了越狱在挖洞。 He has a strong desire to survive and is digging for prison breaks. Introduction Spelling error correction is an important task which aims to correct spelling errors in a text either at word-level or at character-level (Yu and Li, 2014; Yu et al., 2014; Zhang et al., 2015; Wang et al., 2018b; Hong et al., 2019; Wang et al., 2019). It is crucial for many natural language applications such as search (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016; Wang et al., 2018b), and essay scoring (Burstein and Chodorow, 1999). In this paper, we consider Chinese spelling error correction (CSC) at character-level. Spelling error correction is also a very challenging task, because to completely solve the problem the system needs to have human-level language understanding ability. There are at least two ch"
2020.acl-main.82,W15-3107,0,0.357616,"rrection problems. Experimental results on two datasets demonstrate that the performance of our proposed method is significantly better than the baselines including the one solely based on BERT. 1 Correct: 埃及有金字塔。Egypt has pyramids. Wrong: 他的求胜欲很强，为了越狱在挖洞。 He has a strong desire to win and is digging for prison breaks Correct: 他的求生欲很强，为了越狱在挖洞。 He has a strong desire to survive and is digging for prison breaks. Introduction Spelling error correction is an important task which aims to correct spelling errors in a text either at word-level or at character-level (Yu and Li, 2014; Yu et al., 2014; Zhang et al., 2015; Wang et al., 2018b; Hong et al., 2019; Wang et al., 2019). It is crucial for many natural language applications such as search (Martins and Silva, 2004; Gao et al., 2010), optical character recognition (OCR) (Afli et al., 2016; Wang et al., 2018b), and essay scoring (Burstein and Chodorow, 1999). In this paper, we consider Chinese spelling error correction (CSC) at character-level. Spelling error correction is also a very challenging task, because to completely solve the problem the system needs to have human-level language understanding ability. There are at least two challenges here, as sh"
2020.acl-main.82,O15-2002,0,\N,Missing
2021.acl-long.135,N19-1423,0,0.0254899,"rified in other language understanding tasks (Paolini et al., 2021). Seq2Seq-DU comprises a BERT-based encoder to encode the utterances in the dialogue, a BERT based encoder to encode the schema descriptions, an attender to calculate attentions between the utterance embeddings and schema embeddings, and a decoder to generate pointers of items representing the intents and slots-value pairs of state. Seq2Seq-DU has the following advantages. (1) Global: it relies on the sequence to sequence framework to simultaneously model the intents, slots, and slot-values. (2) Representable: It employs BERT (Devlin et al., 2019) to learn and utilize better representations of not only the current utterance but also the previous utterances in the dialogue. If schema descriptions are available, it also employs BERT for the learning and utilization of their representations. (3) Scalable: It uses the pointer generation mechanism, as in the Pointer Network (Vinyals et al., 2015), to create representations of intents, slots, and slot-values, no matter whether the slots are categorical or non-categorical, and whether the schemas are unseen or not. Experimental results on benchmark datasets show that Seq2Seq-DU1 performs much"
2021.acl-long.135,N18-2118,0,0.407487,"ots, and slot values. (2) Represenable, it has strong capa1714 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1714–1725 August 1–6, 2021. ©2021 Association for Computational Linguistics bility to represent knowledge for the task, on top of a pre-trained language model like BERT. (3) Scalable, the model can deal with categorical and non-categorical slots and unseen schemas. Many methods have been proposed for DST (Wu et al., 2019; Zhong et al., 2018; Mrkˇsi´c et al., 2017; Goo et al., 2018). There are two lines of relevant research. (1) To enhance the scalability of DST, a problem formulation, referred to as schemaguided dialogue, is proposed. In the setting, it is assumed that descriptions on schemas in natural language across multiple domains are given and utilized. Consequently, a number of methods are developed to make use of schema descriptions to increase the scalability of DST (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020). The methods regard DST as a classification and/or an extraction problem and independently infer the intent and slot value pairs for t"
2021.acl-long.135,2020.sigdial-1.4,0,0.0226057,"Missing"
2021.acl-long.135,W14-4337,0,0.0385242,"f generating a sequence. 4 4.1 Experiments Datasets We conduct experiments using the benchmark datasets on task-oriented dialogue. SGD (Rastogi et al., 2019) and MultiWOZ2.2 (Zang et al., 2020) are datasets for DST; they include schemas with categorical slots and non-categorical slots in multiple domains and natural language descriptions on the schemas, as shown in Table 2. In particular, SGD includes unseen schemas in the test set. MultiWOZ2.1 (Eric et al., 2020) is the previous version of MultiWOZ2.2, which only has categorical slots in multiple domains. WOZ2.0 (Wen et al., 2017) and DSTC2 (Henderson et al., 2014) are datasets for DST; they contain schemas with only categorical slots in a single domain. M2M (Shah et al., 2018) is a dataset for DST and it has span annotations for slot values in multiple domains. ATIS (Tur et al., 2010) and SNIPS (Coucke et al., 2018) are datasets for NLU in single-turn dialogues in a single domain. Table 3 gives the statics of datasets in the experiments. 4.2 Baselines and Variants We make comparison between our approach and the state-of-the-art methods on the datasets. SGD, MultiWOZ2.2 and MultiWOZ2.1: We compare Seq2SeqDU with six state-of-the-art methods on SGD, Mult"
2021.acl-long.135,P18-1133,0,0.263645,"lity of DST (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020). The methods regard DST as a classification and/or an extraction problem and independently infer the intent and slot value pairs for the current turn. Therefore, the proposed models are generally representable and scalable, but not global. (2) There are also a few methods which view DST as a sequence to sequence problem. Some methods sequentially infer the intent and slot value pairs for the current turn on the basis of dialogue history and usually employ a hierarchical structure (not based on BERT) for the inference (Lei et al., 2018; Ren et al., 2019; Chen et al., 2020b). Recently, a new approach is proposed which formalizes the tasks in dialogue as sequence prediction problems using a unified language model (based on GPT-2) (Hosseini-Asl et al., 2020). The method cannot deal with unseen schemas and intents, however, and thus is not scalable. We propose a novel approach to DST, referred to as Seq2Seq-DU (sequence-to-sequence for dialogue understanding), which combines the advantages of the existing approaches. To the best of our knowledge, there was no previous work which studied the approach. We think that DST should be"
2021.acl-long.135,P17-1163,0,0.0563017,"Missing"
2021.acl-long.135,P18-2069,0,0.0407306,"Missing"
2021.acl-long.135,W18-5045,0,0.133016,"rkˇsi´c et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020). The approaches 1 The code is available at https://github.com/ sweetalyssum/Seq2Seq-DU. 1715 Model FastSGD (Noroozi et al., 2020) SGD Baseline (Rastogi et al., 2019) TripPy (Heck et al., 2020) TRADE (Wu et al., 2019) DS-DST (Zhang et al., 2019) BERT-DST (Chao and Lane, 2019) StateNet (Ren et al., 2018) GLAD (Zhong et al., 2018) Belief Tracking (Ramadan et al., 2018) Neural Belief Tracker (Mrkˇsi´c et al., 2017) DST+LU (Rastogi et al., 2018) Joint BERT (Chen et al., 2019) Slot-Gated (Goo et al., 2018) Atten.-BiRNN (Liu and Lane, 2016) RNN-LSTM (Hakkani-T¨ur et al., 2016) Sequicity (Lei et al., 2018) COMER (Ren et al., 2019) CREDIT (Chen et al., 2020b) SimpleTOD (Hosseini-Asl et al., 2020) Characteristics Data Sets BERT-based model, employs two carry-over procedures and multi-head attenSGD tions to model schema descriptions. BERT-based model, predictions are made over a dynamic set of intents and SGD and MultiWOZ2.2 slots, using their descriptions. BERT-based model, make use of various copy mechanisms to fill slots with MultiWOZ2."
2021.acl-long.135,D19-1196,0,0.160365,"gi et al., 2019; Zang et al., 2020; Noroozi et al., 2020). The methods regard DST as a classification and/or an extraction problem and independently infer the intent and slot value pairs for the current turn. Therefore, the proposed models are generally representable and scalable, but not global. (2) There are also a few methods which view DST as a sequence to sequence problem. Some methods sequentially infer the intent and slot value pairs for the current turn on the basis of dialogue history and usually employ a hierarchical structure (not based on BERT) for the inference (Lei et al., 2018; Ren et al., 2019; Chen et al., 2020b). Recently, a new approach is proposed which formalizes the tasks in dialogue as sequence prediction problems using a unified language model (based on GPT-2) (Hosseini-Asl et al., 2020). The method cannot deal with unseen schemas and intents, however, and thus is not scalable. We propose a novel approach to DST, referred to as Seq2Seq-DU (sequence-to-sequence for dialogue understanding), which combines the advantages of the existing approaches. To the best of our knowledge, there was no previous work which studied the approach. We think that DST should be formalized as a s"
2021.acl-long.135,D18-1299,0,0.0582859,"ere has been a large amount of work on task-oriented dialogue, especially dialogue state tracking and natural language understanding (eg., (Zhang et al., 2020; Huang et al., 2020; Chen et al., 2017)). Table 1 makes a summary of existing methods on DST. We also indicate the methods on which we make comparison in our experiments. 2.1 Dialogue State Tracking Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkˇsi´c et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020). The approaches 1 The code is available at https://github.com/ sweetalyssum/Seq2Seq-DU. 1715 Model FastSGD (Noroozi et al., 2020) SGD Baseline (Rastogi et al., 2019) TripPy (Heck et al., 2020) TRADE (Wu et al., 2019) DS-DST (Zhang et al., 2019) BERT-DST (Chao and Lane, 2019) StateNet (Ren et al., 2018) GLAD (Zhong et al., 2018) Belief Tracking (Ramadan et al., 2018) Neural Belief Tracker (Mrkˇsi´c et al., 2017) DST+LU (Rastogi et al., 2018) Joint BERT (Chen et al., 2019) Slot-Gated (Goo et al."
2021.acl-long.135,E17-1042,0,0.0271808,"tilized to measure the loss of generating a sequence. 4 4.1 Experiments Datasets We conduct experiments using the benchmark datasets on task-oriented dialogue. SGD (Rastogi et al., 2019) and MultiWOZ2.2 (Zang et al., 2020) are datasets for DST; they include schemas with categorical slots and non-categorical slots in multiple domains and natural language descriptions on the schemas, as shown in Table 2. In particular, SGD includes unseen schemas in the test set. MultiWOZ2.1 (Eric et al., 2020) is the previous version of MultiWOZ2.2, which only has categorical slots in multiple domains. WOZ2.0 (Wen et al., 2017) and DSTC2 (Henderson et al., 2014) are datasets for DST; they contain schemas with only categorical slots in a single domain. M2M (Shah et al., 2018) is a dataset for DST and it has span annotations for slot values in multiple domains. ATIS (Tur et al., 2010) and SNIPS (Coucke et al., 2018) are datasets for NLU in single-turn dialogues in a single domain. Table 3 gives the statics of datasets in the experiments. 4.2 Baselines and Variants We make comparison between our approach and the state-of-the-art methods on the datasets. SGD, MultiWOZ2.2 and MultiWOZ2.1: We compare Seq2SeqDU with six st"
2021.acl-long.135,P19-1078,0,0.119666,"ies. (1) Global, the model can jointly represent intents, slots, and slot values. (2) Represenable, it has strong capa1714 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1714–1725 August 1–6, 2021. ©2021 Association for Computational Linguistics bility to represent knowledge for the task, on top of a pre-trained language model like BERT. (3) Scalable, the model can deal with categorical and non-categorical slots and unseen schemas. Many methods have been proposed for DST (Wu et al., 2019; Zhong et al., 2018; Mrkˇsi´c et al., 2017; Goo et al., 2018). There are two lines of relevant research. (1) To enhance the scalability of DST, a problem formulation, referred to as schemaguided dialogue, is proposed. In the setting, it is assumed that descriptions on schemas in natural language across multiple domains are given and utilized. Consequently, a number of methods are developed to make use of schema descriptions to increase the scalability of DST (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020). The methods regard DST as a classification and/or an extraction problem"
2021.acl-long.135,P18-1134,0,0.0143361,"tes to Joint BERT). 2 Related Work There has been a large amount of work on task-oriented dialogue, especially dialogue state tracking and natural language understanding (eg., (Zhang et al., 2020; Huang et al., 2020; Chen et al., 2017)). Table 1 makes a summary of existing methods on DST. We also indicate the methods on which we make comparison in our experiments. 2.1 Dialogue State Tracking Previous approaches mainly focus on encoding of the dialogue context and employ deep neural networks such as CNN, RNN, and LSTMRNN to independently infer the values of slots in DST (Mrkˇsi´c et al., 2017; Xu and Hu, 2018; Zhong et al., 2018; Ren et al., 2018; Rastogi et al., 2017; Ramadan et al., 2018; Wu et al., 2019; Zhang et al., 2019; Heck et al., 2020). The approaches 1 The code is available at https://github.com/ sweetalyssum/Seq2Seq-DU. 1715 Model FastSGD (Noroozi et al., 2020) SGD Baseline (Rastogi et al., 2019) TripPy (Heck et al., 2020) TRADE (Wu et al., 2019) DS-DST (Zhang et al., 2019) BERT-DST (Chao and Lane, 2019) StateNet (Ren et al., 2018) GLAD (Zhong et al., 2018) Belief Tracking (Ramadan et al., 2018) Neural Belief Tracker (Mrkˇsi´c et al., 2017) DST+LU (Rastogi et al., 2018) Joint BERT (Che"
2021.acl-long.135,2020.nlp4convai-1.13,0,0.197388,"ategorical and non-categorical slots and unseen schemas. Many methods have been proposed for DST (Wu et al., 2019; Zhong et al., 2018; Mrkˇsi´c et al., 2017; Goo et al., 2018). There are two lines of relevant research. (1) To enhance the scalability of DST, a problem formulation, referred to as schemaguided dialogue, is proposed. In the setting, it is assumed that descriptions on schemas in natural language across multiple domains are given and utilized. Consequently, a number of methods are developed to make use of schema descriptions to increase the scalability of DST (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020). The methods regard DST as a classification and/or an extraction problem and independently infer the intent and slot value pairs for the current turn. Therefore, the proposed models are generally representable and scalable, but not global. (2) There are also a few methods which view DST as a sequence to sequence problem. Some methods sequentially infer the intent and slot value pairs for the current turn on the basis of dialogue history and usually employ a hierarchical structure (not based on BERT) for the inference (Lei et al., 2018; Ren et al., 2019; Chen et al., 202"
2021.acl-long.135,P18-1135,0,0.276548,"the model can jointly represent intents, slots, and slot values. (2) Represenable, it has strong capa1714 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1714–1725 August 1–6, 2021. ©2021 Association for Computational Linguistics bility to represent knowledge for the task, on top of a pre-trained language model like BERT. (3) Scalable, the model can deal with categorical and non-categorical slots and unseen schemas. Many methods have been proposed for DST (Wu et al., 2019; Zhong et al., 2018; Mrkˇsi´c et al., 2017; Goo et al., 2018). There are two lines of relevant research. (1) To enhance the scalability of DST, a problem formulation, referred to as schemaguided dialogue, is proposed. In the setting, it is assumed that descriptions on schemas in natural language across multiple domains are given and utilized. Consequently, a number of methods are developed to make use of schema descriptions to increase the scalability of DST (Rastogi et al., 2019; Zang et al., 2020; Noroozi et al., 2020). The methods regard DST as a classification and/or an extraction problem and independently i"
2021.emnlp-main.323,N19-1423,0,0.166856,"ent perspectives. Please notice that this orthogonal regularization mechanism is general and not limited to audio-language tasks. The main contributions of our paper are listed as follows: EMNLP2021. 2 2.1 Related Work Unimodal Pre-training There has been a long interest around selfsupervised representation learning. Previous works have explored alternative approaches to improve word embedding (Mikolov et al., 2013; Le and Mikolov, 2014; Pennington et al., 2014), which is a low-level linguistic representation. After that, pre-trained NLP models based on multi-layer Transformers, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019) and GPT2 (Radford et al., 2019), benefit from context-sensitive representation learning on large-scale corpus, showing significant improvements in various downstream language understanding tasks. Self-supervised learning in speech processing has also shown increasing promise. Following BERT, many approaches (Jiang et al., 2019; Liu et al., 2021, 2020; Chi et al., 2021) are proposed to learn high-level acoustic representations rather than surface features such as log Mel-spectrograms or waveform, which can reveal the abundant information w"
2021.emnlp-main.323,D17-1082,0,0.04066,"Missing"
2021.emnlp-main.323,2021.ccl-1.108,0,0.0363175,"Missing"
2021.emnlp-main.323,D14-1162,0,0.0965902,"hogonality is introduced during the model fine-tuning stage, which is designed to ensure that features of different modalities provide information from different perspectives. Please notice that this orthogonal regularization mechanism is general and not limited to audio-language tasks. The main contributions of our paper are listed as follows: EMNLP2021. 2 2.1 Related Work Unimodal Pre-training There has been a long interest around selfsupervised representation learning. Previous works have explored alternative approaches to improve word embedding (Mikolov et al., 2013; Le and Mikolov, 2014; Pennington et al., 2014), which is a low-level linguistic representation. After that, pre-trained NLP models based on multi-layer Transformers, such as BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019) and GPT2 (Radford et al., 2019), benefit from context-sensitive representation learning on large-scale corpus, showing significant improvements in various downstream language understanding tasks. Self-supervised learning in speech processing has also shown increasing promise. Following BERT, many approaches (Jiang et al., 2019; Liu et al., 2021, 2020; Chi et al., 2021) are proposed to le"
2021.emnlp-main.323,D19-1514,0,0.0196301,"a- and-language tasks on visual question answering tions with Transformer, which considers both (Antol et al., 2015) and visual commonsense reaintra- and inter- modalities connections. To soning (Zellers et al., 2019) datasets. In general, the best of our knowledge, we are the first these vision-and-language pre-training methods to introduce the pre-training cross audio-and- can be divided into two categories, according to language modalities. their different encoder architectures as follows: (a) prior works like ViLBERT (Lu et al., 2019) and • We propose a novel cross-modality fusion LXMERT (Tan and Bansal, 2019), apply two unimechanism at the fine-tuning stage, which forces our pre-trained model learn compos- modal networks to encode input text and images respectively and adapt cross-modal interactions in a ite features from different views. symmetric fusion manner; (b) the other category of pre-training frameworks like VisualBert (Li et al., • Comprehensive empirical results demonstrate 2019), Unicoder-VL (Li et al., 2020a) and UNITER that our CTAL achieves the state-of-the-art (Chen et al., 2020), concatenate vision and lanresults on various downstream SP tasks, such as emotion classification, sent"
2021.emnlp-main.323,P19-1656,0,0.0175419,"the sentiment analysis task is to predict the degree of positive and negative sentiment. Compared to the emotion classification task, sentiment analysis is a regression task rather than a classification task. We adopt CMU-MOSEI (Zadeh et al., 2018) dataset for evaluation, which contains 23,454 movie review video clips from YouTube. We use only audio and corresponding transcriptions as input in our experiments. Each sample in the dataset is labeled with a sentiment score from -3 (strongly negative) to 3 (strongly positive) by human annotators. We follow the same experimental protocol as MuIT (Tsai et al., 2019), with the same train/test data split and the same evaluation metrics, which includes two classification metrics: (1) binary accuracy (i.e., Acc2 : accuracy over positive/negative sentiments classification), and F1 score; (2) two regression metrics: mean absolute error (MAE), and the Pearson correlation coefficient (Corr) between model’s predictions and human annotations. Since the prior top results reported on the CMU-MOSEI dataset are all achieved using all three modalities, so does MulT3 , we prune the vision-related components in MulT and re-train the model using only audio and text inform"
2021.emnlp-main.348,2020.coling-main.166,1,0.731363,"tions x+y=27 2x+4y=86 Solutions x=11 y=16 Table 1: An illustrative example of an MWP. Introduction ∗ Math Word Problem In this paper, our objective is to automatically generate well-formed MWPs. Such automation will not only reduce the teachers’ burden of manually designing MWPs, but provide students with a sufficiently large number of practice exercises, which help students avoid rote memorization (Williams, 2011; Wang and Su, 2016). A large spectrum of models have been developed and successfully applied in a broad area of natural question generation (NQG) (Pan et al., 2019; Li et al., 2018; Liu et al., 2020; Sun et al., 2018; Zhang and Bansal, 2019; Kurdi et al., 2020; Guan et al., 2021a,b) and there has been a recent movement from the NQG community towards automatic generation of MWPs (Koncel-Kedziorski et al., 2016a; Polozov et al., 2015; Zhou and Huang, 2019). For example, Koncel-Kedziorski et al. (2016a) proposed a two-stage rewriting approach to edit existing human-authored MWPs. Polozov et al. (2015) conducted the MWP generation as a constrained synthesis of labeled logical graphs that represent abstract plots. In general, there exists a large number of NQG models representing various text"
2021.emnlp-main.348,2020.lrec-1.579,0,0.0315107,"d approaches are more flexible compared with templates based approaches. However, there are several drawbacks that prevent them from providing the large number of MWPs. First, the generation process is based on existing MWPs, which significantly limits the generation ability. Second, students easily fall into rote memorization since it is too trivial to notice that the underlying mathematical equations are still unchanged. Recent attempts have been focused on exploiting neural network based approaches that generating MWPs from equations and topics in an end-toend manner (Zhou and Huang, 2019; Liyanage and Ranathunga, 2020). Zhou and Huang (2019) designed a neural network with two encoders to fuse information of both equations and topics and dualattention mechanism to generate relevant MWPs. Liyanage and Ranathunga (2020) tackled the generation problem by using the long short term memory network with enhanced input features, such as character embeddings, word embeddings and part-of-speech tag embeddings. The closest work to our approach is Zhou and Huang (2019) and the main differences are as follows: (1) Zhou and Huang (2019) directly encode the equation by a single-layer bidirectional gated recurrent unit (GRU"
2021.emnlp-main.348,P16-1170,0,0.022836,"ct and fuse information from both equations and commonsense knowledge, which improves syntax structure of generated MWP sentences. Overall this paper makes the following contributions: 2 2.1 Related Work Natural Question Generation Previous research has directly approached the task of automatically generating questions for many useful applications such as augmenting data for the QA tasks (Li et al., 2018; Sun et al., 2018; Zhang and Bansal, 2019), helping semantic parsing (Guo et al., 2018) and machine reading comprehension (Yu et al., 2020; Yuan et al., 2017), improving conversation quality (Mostafazadeh et al., 2016; Dong et al., 2019), and providing student exercises for education purposes (Koncel-Kedziorski et al., 2016a). Various NQG methods are developed which can be divided into two categories: heuristic based approaches and neural network based approaches (Pan et al., 2019; Kurdi et al., 2020). The former generates questions in two stages: it first obtains intermediate symbolic representations and then constructs the natural language questions by either rearranging the surface form of the input sentence or generating with pre-defined question templates. The latter neural approaches view the NQG tas"
2021.emnlp-main.348,P02-1040,0,0.109706,"e that different from previous work of automatically solving the MWPs such as MAWPS (Koncel-Kedziorski et al., 2016b) and MathQA (Amini et al., 2019), we focus on the generation task of MWPs of more than one linear equations in students’ real-life scenarios by using topics in our CSKG. Both MAWPS and MathQA datasets do not contain MWPs that have two or three equations and variables. Furthermore, there is no explicit topics associated with the MWPs in these publicly available MWP datasets. We use following evaluation metrics: (1) BLEU4: the 4-gram overlap score against gold-standard sentences (Papineni et al., 2002); (2) METEOR: n-gram overlap with paraphrase and languagespecific considerations (Denkowski and Lavie, 2014); (3) ROUGE-L: the overlap of longest common subsequence between candidate and goldstandard sentences (Lin, 2004); (4) Self-BLEU: the diversity measurement of averaging BLEU scores of four generated MWP pairs given the same input (Zhu et al., 2018). Meanwhile, we conduct two human evaluation studies to comprehensively evaluate the quality of the generated MWPs. First, we ask three evaluators to rate from the following aspects ranging from 1 to 3: (1) Equation Relevance: how relevant is M"
2021.emnlp-main.348,D18-1427,0,0.12872,"=86 Solutions x=11 y=16 Table 1: An illustrative example of an MWP. Introduction ∗ Math Word Problem In this paper, our objective is to automatically generate well-formed MWPs. Such automation will not only reduce the teachers’ burden of manually designing MWPs, but provide students with a sufficiently large number of practice exercises, which help students avoid rote memorization (Williams, 2011; Wang and Su, 2016). A large spectrum of models have been developed and successfully applied in a broad area of natural question generation (NQG) (Pan et al., 2019; Li et al., 2018; Liu et al., 2020; Sun et al., 2018; Zhang and Bansal, 2019; Kurdi et al., 2020; Guan et al., 2021a,b) and there has been a recent movement from the NQG community towards automatic generation of MWPs (Koncel-Kedziorski et al., 2016a; Polozov et al., 2015; Zhou and Huang, 2019). For example, Koncel-Kedziorski et al. (2016a) proposed a two-stage rewriting approach to edit existing human-authored MWPs. Polozov et al. (2015) conducted the MWP generation as a constrained synthesis of labeled logical graphs that represent abstract plots. In general, there exists a large number of NQG models representing various text data and their sy"
2021.emnlp-main.348,W17-2603,0,0.0189361,"e develop a self-planning module to dynamically select and fuse information from both equations and commonsense knowledge, which improves syntax structure of generated MWP sentences. Overall this paper makes the following contributions: 2 2.1 Related Work Natural Question Generation Previous research has directly approached the task of automatically generating questions for many useful applications such as augmenting data for the QA tasks (Li et al., 2018; Sun et al., 2018; Zhang and Bansal, 2019), helping semantic parsing (Guo et al., 2018) and machine reading comprehension (Yu et al., 2020; Yuan et al., 2017), improving conversation quality (Mostafazadeh et al., 2016; Dong et al., 2019), and providing student exercises for education purposes (Koncel-Kedziorski et al., 2016a). Various NQG methods are developed which can be divided into two categories: heuristic based approaches and neural network based approaches (Pan et al., 2019; Kurdi et al., 2020). The former generates questions in two stages: it first obtains intermediate symbolic representations and then constructs the natural language questions by either rearranging the surface form of the input sentence or generating with pre-defined questi"
2021.emnlp-main.348,D19-1253,0,0.0878984,"y=16 Table 1: An illustrative example of an MWP. Introduction ∗ Math Word Problem In this paper, our objective is to automatically generate well-formed MWPs. Such automation will not only reduce the teachers’ burden of manually designing MWPs, but provide students with a sufficiently large number of practice exercises, which help students avoid rote memorization (Williams, 2011; Wang and Su, 2016). A large spectrum of models have been developed and successfully applied in a broad area of natural question generation (NQG) (Pan et al., 2019; Li et al., 2018; Liu et al., 2020; Sun et al., 2018; Zhang and Bansal, 2019; Kurdi et al., 2020; Guan et al., 2021a,b) and there has been a recent movement from the NQG community towards automatic generation of MWPs (Koncel-Kedziorski et al., 2016a; Polozov et al., 2015; Zhou and Huang, 2019). For example, Koncel-Kedziorski et al. (2016a) proposed a two-stage rewriting approach to edit existing human-authored MWPs. Polozov et al. (2015) conducted the MWP generation as a constrained synthesis of labeled logical graphs that represent abstract plots. In general, there exists a large number of NQG models representing various text data and their syntax and semantics (Pan"
2021.emnlp-main.348,P17-1061,0,0.058336,"Missing"
2021.emnlp-main.348,W19-8661,0,0.0521128,"manually designing MWPs, but provide students with a sufficiently large number of practice exercises, which help students avoid rote memorization (Williams, 2011; Wang and Su, 2016). A large spectrum of models have been developed and successfully applied in a broad area of natural question generation (NQG) (Pan et al., 2019; Li et al., 2018; Liu et al., 2020; Sun et al., 2018; Zhang and Bansal, 2019; Kurdi et al., 2020; Guan et al., 2021a,b) and there has been a recent movement from the NQG community towards automatic generation of MWPs (Koncel-Kedziorski et al., 2016a; Polozov et al., 2015; Zhou and Huang, 2019). For example, Koncel-Kedziorski et al. (2016a) proposed a two-stage rewriting approach to edit existing human-authored MWPs. Polozov et al. (2015) conducted the MWP generation as a constrained synthesis of labeled logical graphs that represent abstract plots. In general, there exists a large number of NQG models representing various text data and their syntax and semantics (Pan et al., 2019). However, automatic generation of MWPs still presents numerous challenges that come from special characteristics of real-world educational scenarios as follows: (1) MWP generation models need to not only"
2021.findings-acl.37,P19-1139,0,0.0240898,"e-grained tokens masked. ELECTRA (Clark et al., 2020) has a GAN-style architecture for efficiently utilizing all tokens in pre-training. It has been found that the use of coarse-grained tokens is beneficial for pre-trained language models. (Devlin et al., 2018) point out that ‘whole word masking’ is effective for training of BERT. It is also observed that whole word masking is useful for building a Chinese BERT (Cui et al., 2019). In ERNIE (Sun et al., 2019b), entity level masking is employed as a strategy for pre-training and proved to be effective for language understanding tasks (see also (Zhang et al., 2019)). In SpanBERT (Joshi et al., 2020), text spans are masked in pre-training and the learned model can substantially enhance the accuracies of span selection tasks. It is indicated that word segmentation is especially important for Chinese and a BERT-based Chinese text encoder is proposed with n-gram representations (Diao et al., 2019). All existing work focuses on the use of single-grained tokens in learning and utilization of pre-trained language models. In this work, we propose a general technique of exploiting multi-grained tokens for pre-trained language models and apply it to BERT. 3 gener"
2021.findings-acl.37,P19-1075,0,0.0409595,"Missing"
2021.findings-emnlp.396,P19-1425,0,0.0197578,"E and Secoco-Edit is very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study tran"
2021.findings-emnlp.396,2020.acl-main.529,0,0.0281867,"use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al."
2021.findings-emnlp.396,C18-1055,0,0.0198205,"ng findings. First, the performance of Secoco-E2E and Secoco-Edit is very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors."
2021.findings-emnlp.396,W18-6317,0,0.0137347,"real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw"
2021.findings-emnlp.396,W18-1807,0,0.102508,"Missing"
2021.findings-emnlp.396,W18-6453,0,0.0172586,"Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a recon"
2021.findings-emnlp.396,D19-5506,0,0.0151559,"a for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, w"
2021.findings-emnlp.396,P19-1291,0,0.0169515,"l., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, we iteratively edit the input until the input is unchanged and the"
2021.findings-emnlp.396,N19-1314,0,0.0159315,"very close. Therefore, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caus"
2021.findings-emnlp.396,N19-4009,0,0.0150201,"hod enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness of self-correcting encoding. It is worth noting that the BLEU scores here are results on noisy test sets, so they are certainly lower 4641 Dialogue BLEU 4 Methods Speech BLEU 4 WMT En-De BLEU 4 AVG BLEU 4 Latency (ms/sent) BASE BASE +s"
2021.findings-emnlp.396,W18-6319,0,0.0117715,"roduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness of self-correcting encoding. It is worth noting that the BLEU scores here are results on noisy test sets, so they are certainly lower 4641 Dialogue BL"
2021.findings-emnlp.396,P18-2037,0,0.0182766,"s potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthe"
2021.findings-emnlp.396,P16-1162,0,0.0421259,"et al. (2019) to develop a multi-task based method to solve the robustness problem. We construct triples (clean input, noisy input, target translation), and introduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al., 2016) to process all these data and restricted merge operations to a maximum of 30k separately. For evaluation, we used the standard Sacrebleu (Post, 2018) to calculate BLEU-4. All models were implemented based on Fairseq (Ott et al., 2019). 3.4 Results Table 2 shows the translation results on Dialogue, Speech and WMT En-De. Clearly, all competitors substantially improve the baseline model in terms of BLEU. Secoco achieves the best performance on all three test sets, gaining improvements of 2.2, 0.7, and 0.4 BLEU-4 points over BASE+synthetic respectively. The improvements suggest the effectiveness"
2021.findings-emnlp.396,N19-1190,0,0.0237898,"et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture to deal with natural noise for NMT. Different from ours, most of these works use the synthetic data in a coarsegrained and implicit way (i.e. simply combining the synthetic and raw data). As described in Section 2.3, we iteratively edit the input until the input is unchanged and then translate it. We present examples in Table 3. We 5 Conclusions can see that m"
2021.findings-emnlp.396,P19-1583,0,0.0203659,"ss of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two deco"
2021.findings-emnlp.396,D17-1319,0,0.0238081,"tion. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2"
2021.findings-emnlp.396,D18-1316,0,0.0167893,"e, it is better to use SecocoE2E for its simplification and efficiency. Second, Secoco is more effective on the real-world test sets, showing its potential in real-world application. 3.5 Iterative Editing Related Work Approaches to the robustness of NMT can be roughly divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recogn"
2021.findings-emnlp.396,W19-5368,0,0.0796658,"ethod against the following three baseline systems. BASE One widely-used way to achieve NMT robustness is to mix raw clean data with noisy data to train NMT models. We refer to models trained with/without synthetic data as BASE/BASE+synthetic. R EPAIR To deal with noisy inputs, one might train a repair model to transform noisy inputs into clean inputs that a normally trained translation model can deal with. Both the repair and translation model are transformer-based models. As a pipeline model (repairing before translating), R EPAIR may suffer from error propagation. R ECONSTRUCTION We follow Zhou et al. (2019) to develop a multi-task based method to solve the robustness problem. We construct triples (clean input, noisy input, target translation), and introduce an additional decoder to obtain clean inputs from noisy inputs. This method enables NMT to transform a noisy input into a clean input and pass this knowledge into the translation decoder. 3.3 Data Baselines Settings In our studies, all translation models were Transformer-base. They were trained with a batch size of 32,000 tokens. The beam size was set to 5 during decoding. We used byte pair encoding compression algorithm (BPE) (Sennrich et al"
2021.findings-emnlp.396,D17-1147,0,0.060923,"Missing"
2021.findings-emnlp.396,P18-2048,0,0.0159752,"y divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture t"
2021.findings-emnlp.396,2021.naacl-industry.14,1,0.749888,"-Edit), as illustrated in the right part of Figure 2. In general, Secoco-E2E provides better robustness without sacrificing decoding speed. For Secoco-Edit, iterative editing enables better interpretability. Detailed editing operations provide a different perspective on how the model resists noise. 3 Experiments 3.1 We conducted our experiments on three test sets, including Dialogue, Speech, and WMT14 En-De, to examine the effectiveness of Secoco. Dialogue is a real-world Chinese-English dialogue test set constructed based on TV drama subtitles1 , which contains three types of natural noises (Wang et al., 2021). Speech is an in-house ChineseEnglish speech translation test set which contains various noise from ASR. To evaluate Secoco on different language pairs, we also used WMT14 EnDe test sets to build a noisy test set with random deletion and insertion operations. Table 1 shows the details of the three test sets. For Chinese-English translation, we used WMT2020 Chinese-English data2 (48M) for Dialogue, and CCMT3 (9M) for Speech. For WMT En-De, we adopted the widely-used WMT14 training data4 (4.5M). We synthesized corresponding 1 https://github.com/rgwt123/DialogueMT http://www.statmt.org/wmt20/tra"
2021.findings-emnlp.396,P19-1123,0,0.0345329,"Missing"
2021.findings-emnlp.396,W18-6314,0,0.0205705,"y divided into three categories. In the first research line, adversarial examples are generated with black- or white-box methods. The generated adversarial examples are then used to combine with original training data for adversarial training (Ebrahimi et al., 2018; Chaturvedi et al., 2019; Cheng et al., 2019; Michel et al., 2019; Zhao et al., 2018; Cheng et al., 2020). In the second strand, a wide variety of methods have been proposed to deal with noise in training data (Schwenk, 2018; Guo et al., 2018; Xu and Koehn, 2017; Koehn et al., 2018; van der Wees et al., 2017; Wang and Neubig, 2019; Wang et al., 2018a,b, 2019). Finally, efforts have been also explored to directly cope with naturally occurring noise in texts, which are closely related to our work. Heigold et al. (2018); Belinkov and Bisk (2018); Levy et al. (2019) focus on word spelling errors. Sperber et al.; Liu et al. (2019) study translation problems caused by speech recognition. Vaibhav et al. (2019) introduce back-translation to generate more natural synthetic data, and employ extra tags to distinguish synthetic data from raw data. Zhou et al. (2019) propose a reconstruction method based on one encoder and two decoders architecture t"
C02-1011,J93-2003,0,0.00559474,"Missing"
C02-1011,P98-1069,0,0.302277,"of the baseline methods relying on existing technologies. The higher performance of our method can be attributed to the enormity of the web data used and the employment of the EM Algorithm. 2. Related Work 2.1 Translation Corpora with Non-parallel A straightforward approach to word or phrase translation is to perform the task by using parallel bilingual corpora (e.g., Brown et al, 1993). Parallel corpora are, however, difficult to obtain in practice. To deal with this difficulty, a number of methods have been proposed, which make use of relatively easily obtainable non-parallel corpora (e.g., Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000). Within these methods, it is usually assumed that a number of translation candidates for a word or phrase are given (or can be easily collected) and the problem is focused on translation selection. All of the proposed methods manage to find out the translation(s) of a given word or phrase, on the basis of the linguistic phenomenon that the contexts of a translation tend to be similar to the contexts of the given word or phrase. Fung and Yee (1998), for example, proposed to represent the contexts of a word or phrase with a real-valued vector (e.g., a TF-IDF v"
C02-1011,W99-0905,0,0.0108204,"omain there is only one-to-one mapping relationship between words in the two languages. The assumption is reasonable in a specific domain, but is too strict in the general domain, in which we presume to perform translation here. A straightforward extension of Fung and Yee’s assumption to the general domain is to restrict the many-to-many relationship to that of many-to-one mapping (or one-to-one mapping). This approach, however, has a drawback of losing information in vector transformation, as will be described. For other methods using non-parallel corpora, see also (Tanaka and Iwasaki, 1996; Kikui, 1999, Koehn and Kevin 2000; Sumita 2000; Nakagawa 2001; Gao et al, 2001). 2.2 Translation Using Web Data Web is an extremely rich source of data for natural language processing, not only in terms of data size but also in terms of data type (e.g., multilingual data, link data). Recently, a new trend arises in natural language processing, which tries to bring some new breakthroughs to the field by effectively using web data (e.g., Brill et al, 2001). Nagata et al (2001), for example, proposed to collect partial parallel corpus data on the web to create a translation dictionary. They observed that th"
C02-1011,W01-1413,0,0.317534,"formation in vector transformation, as will be described. For other methods using non-parallel corpora, see also (Tanaka and Iwasaki, 1996; Kikui, 1999, Koehn and Kevin 2000; Sumita 2000; Nakagawa 2001; Gao et al, 2001). 2.2 Translation Using Web Data Web is an extremely rich source of data for natural language processing, not only in terms of data size but also in terms of data type (e.g., multilingual data, link data). Recently, a new trend arises in natural language processing, which tries to bring some new breakthroughs to the field by effectively using web data (e.g., Brill et al, 2001). Nagata et al (2001), for example, proposed to collect partial parallel corpus data on the web to create a translation dictionary. They observed that there are many partial parallel corpora between English and Japanese on the web, and most typically English translations of Japanese terms (words or phrases) are parenthesized and inserted immediately after the Japanese terms in documents written in Japanese. 3. Base Noun Phrase Translation Our method for Base NP translation comprises of two steps: translation candidate collection and translation selection. In translation candidate collection, we look for translatio"
C02-1011,A00-2009,0,0.0259506,"Missing"
C02-1011,P99-1067,0,0.307779,"ods relying on existing technologies. The higher performance of our method can be attributed to the enormity of the web data used and the employment of the EM Algorithm. 2. Related Work 2.1 Translation Corpora with Non-parallel A straightforward approach to word or phrase translation is to perform the task by using parallel bilingual corpora (e.g., Brown et al, 1993). Parallel corpora are, however, difficult to obtain in practice. To deal with this difficulty, a number of methods have been proposed, which make use of relatively easily obtainable non-parallel corpora (e.g., Fung and Yee, 1998; Rapp, 1999; Diab and Finch, 2000). Within these methods, it is usually assumed that a number of translation candidates for a word or phrase are given (or can be easily collected) and the problem is focused on translation selection. All of the proposed methods manage to find out the translation(s) of a given word or phrase, on the basis of the linguistic phenomenon that the contexts of a translation tend to be similar to the contexts of the given word or phrase. Fung and Yee (1998), for example, proposed to represent the contexts of a word or phrase with a real-valued vector (e.g., a TF-IDF vector), in w"
C02-1011,P00-1054,0,0.0522038,"Missing"
C02-1011,C96-2098,0,0.022818,"sumed that in a specific domain there is only one-to-one mapping relationship between words in the two languages. The assumption is reasonable in a specific domain, but is too strict in the general domain, in which we presume to perform translation here. A straightforward extension of Fung and Yee’s assumption to the general domain is to restrict the many-to-many relationship to that of many-to-one mapping (or one-to-one mapping). This approach, however, has a drawback of losing information in vector transformation, as will be described. For other methods using non-parallel corpora, see also (Tanaka and Iwasaki, 1996; Kikui, 1999, Koehn and Kevin 2000; Sumita 2000; Nakagawa 2001; Gao et al, 2001). 2.2 Translation Using Web Data Web is an extremely rich source of data for natural language processing, not only in terms of data size but also in terms of data type (e.g., multilingual data, link data). Recently, a new trend arises in natural language processing, which tries to bring some new breakthroughs to the field by effectively using web data (e.g., Brill et al, 2001). Nagata et al (2001), for example, proposed to collect partial parallel corpus data on the web to create a translation dictionary. They obs"
C02-1011,C98-1066,0,\N,Missing
C02-1011,P00-1015,0,\N,Missing
C16-1205,D14-1179,0,0.0570165,"Missing"
C16-1205,N16-1102,0,0.0151591,"ce representation than just history of attention, and is therefore a more powerful model for machine translation. 2181 We also provide some actual translation examples (see Appendix) to show that our I NTERACTIVE ATTENTION can get better performance then baselines, especially on solving under-translation problem. We think the interactive mechanism of NMTIA is helpful for the decoder to automatically distinguish which parts have been translated and which parts are under-translated. 5 Related Work Our work is related to recent works that focus on improving attention models (Luong et al., 2015a; Cohn et al., 2016; Feng et al., 2016). Luong et al. (2015a) proposed to use global and local attention models to improve translation performance. They use a global one to attend to all source words and a local one to look at a subset of source words at a time. Cohn et al. (2016) extended the attention-based NMT to include structural biases from word-based alignment models, which achieved improvements across several language pairs. Feng et al. (2016) added implicit distortion and fertility models to attentionbased NMT to achieve translation improvements. These works are different with our I NTERACTIVE ATTENTION"
C16-1205,P05-1066,0,0.21663,"Missing"
C16-1205,P15-1001,0,0.0579261,"can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with"
C16-1205,N03-1017,0,0.0445318,"T baselines: Groundhog and RNNsearch? (our implementation of improved attention model as described in Section 2.2), and our I NTERACTIVE ATTENTION model (NMTIA ). The “*” indicates that the results are significantly (p&lt;0.01) better than those of all the baseline systems. 4.3 Comparison Systems We compare our NMTIA with four systems: • Moses (Koehn et al., 2007): an open source phrase-based translation system5 with default configuration. The word alignments are obtained with GIZA++ (Och and Ney, 2003) on the training corpora in both directions, using the “grow-diag-final-and” balance strategy (Koehn et al., 2003). The 4-gram language model with modified Kneser-Ney smoothing is trained on the target portion of training data with the SRILM toolkit (Stolcke and others, 2002), • Groundhog: an open source NMT system6 implemented with the conventional attention model (Bahdanau et al., 2015). • RNNsearch? : our in-house implementation of NMT system with the improved conventional attention model as described in Section 2.2. • Coverage Model: state-of-the-art variants of attention-based NMT model (Tu et al., 2016) which improve the attention mechanism through modeling a soft coverage on the source representati"
C16-1205,D15-1166,0,0.701706,"NTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comp"
C16-1205,P15-1002,0,0.384963,"NTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comp"
C16-1205,J03-1002,0,0.0114395,"Missing"
C16-1205,P16-1159,0,0.0609123,"rt variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 2015). This superiority in efficiency comes mainly from"
C16-1205,P16-1008,1,0.724987,"paper, we propose a new attention mechanism, called I NTERACTIVE ATTENTION, which models the interaction between the decoder and the representation of source sentence during translation by both reading and writing operations. I NTERACTIVE ATTENTION can keep track of the interaction history and therefore improve the translation performance. Experiments on NIST Chinese-English translation task show that I NTERACTIVE ATTENTION can achieve significant improvements over both the previous attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an in"
C16-1205,D16-1027,1,0.848213,"attention-based NMT baseline and some state-of-the-art variants of attention-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 20"
C16-1205,Q16-1027,0,0.0182714,"ntion-based NMT (i.e., coverage models (Tu et al., 2016)). And neural machine translator with our I NTERACTIVE ATTENTION can outperform the open source attention-based NMT system Groundhog by 4.22 BLEU points and the open source phrase-based system Moses by 3.94 BLEU points averagely on multiple test sets. 1 Introduction Neural Machine Translation (NMT) has made promising progress in recent years (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Jean et al., 2015; Luong et al., 2015b; Tang et al., 2016; Wang et al., 2016; Li et al., 2016; Tu et al., 2016; Shen et al., 2016; Zhou et al., 2016), in which attention model plays an increasingly important role. Attention-based NMT represents the source sentence as a sequence of vectors after a RNN or bi-directional RNN (Schuster and Paliwal, 1997), and then simultaneously conducts dynamic alignment with a gating neural network and generation of the target sentence with another RNN. Usually NMT with attention model is more efficient than its attention-free counterpart: it can achieve comparable results with far less parameters and training instances (Jean et al., 2015). This superiority in efficiency comes mainly from the mechanism of dy"
C16-1205,P07-2045,0,\N,Missing
C96-1003,J93-2004,0,\N,Missing
C96-1003,P93-1022,0,\N,Missing
C96-1003,P91-1030,0,\N,Missing
C96-1003,J92-4003,0,\N,Missing
C96-1003,P90-1034,0,\N,Missing
C96-1003,H90-1056,0,\N,Missing
C96-1003,P93-1024,0,\N,Missing
C96-1004,C94-2195,0,0.0576731,"een the case frames. Word senses are in general difficult to define precisely, however, and in language processing, they would have to be disambiguated Dora the context ~nyway, which is essentially equivalent to assuming that the dependencies between case slots exist. Thus, our proposed method can in effect &apos;discover&apos; implicit word senses fi&apos;om corpus data. *Real World Computing Partnership 20 and methods of resolving ambiguity are also based on the assuml:ition th.at case slots are independent (llindle and Rooth, 1991), or dependencies lmtween at most two case slots are considered (Brill and Resnik, 1994). Thus, provision of an efl&apos;ective method of learning de,pendencies between (;as(; slots, as well as investigation of the usefulness of the acquired dependencies in disambiguation and other natural language processing tasks would be an inll)ortant contributiota to the fie.ld. In this paper, wc view the problem of learning (;as(? frame patterns as that of learning a lnultidimensional discrete joint distribution, where raw doni variables represent case slots. We then formalize the dependencies between case slots as the probabilistic dependencies betweeit these ralldoiil variables. Since the illl"
C96-1004,P91-1030,0,0.103752,"Missing"
C96-1004,J93-2004,0,\N,Missing
C96-1004,J98-2002,1,\N,Missing
C96-1004,J93-1005,0,\N,Missing
C96-1004,J93-1007,0,\N,Missing
C96-1004,A92-1014,0,\N,Missing
C96-1004,H94-1048,0,\N,Missing
C96-1004,H90-1056,0,\N,Missing
C96-1004,C94-2119,0,\N,Missing
C96-1004,H93-1054,0,\N,Missing
C98-2119,J92-4003,0,0.0339763,"Missing"
C98-2119,C96-1003,1,0.900199,") principle. Our clustering algorithm iteratively merges noun classes and verb classes in turn, in a bottom u p foMfion. For 749 each merge it performs, it calculates the increase in data description length resulting from merging any noun (or verb) class pair, and performs the merge having the least increase in data description length, provided that the increase in data description length is less than the reduction in model description length. There have been a number of methods proposed in the Literature to address the word clustering problem (e.g., (Brown et at., 1992; Pereira et al., 1993; Li and Abe, 1996)). The method proposed in this paper is a natural extension of t)oth Li & Abe&apos;s and Brown et al&apos;s methods, and is an attempt to overcome their drawbacks while retaining their advantages. The method of Brown et aI, which is based on the Maximum Likelihood Estimation (MLE), performs a merge which would result in the least reduction in (average) mutual information. Our method turns out to be equivalent to performing the merge with the least reduction in mutual information, provided that the reduction is below a certain threshold which depends on the size of the co-occurrence data and the number o"
C98-2119,P93-1024,0,0.454798,"Missing"
C98-2119,J98-2002,1,\N,Missing
C98-2119,C94-2195,0,\N,Missing
D13-1096,J93-2003,0,0.0605106,"Figure 2: Content of the dataset. ferent levels. For example, one may discover from the data that when the word “Hawaii” occurs in the post, the response are more likely to contain words like “trip”, “flight”, or “Honolulu”. On a slightly more abstract level, one may learn that when an entity name is mentioned in the post, it tends to be mentioned again in the response. More complicated matching pattern could also be learned. For example, the response to a post asking “how to” is statistically longer than average responses. As a particular case, Ritter et al. (2011) applied translation model (Brown et al., 1993) on similar parallel data extracted from Twitter in order to extract the word-to-word correlation. Please note that with more sophisticated natural language processing, we can go beyond bag-of-words for more complicated correspondence between post and response. Training Automatic Response Models Although the original (post, response) pairs are rather abundant, they are not enough for discriminative training and testing of retrieval models, for the following reasons. In the labeled pairs, both positive and negative ones are ranked high by some baseline models, and hence more difficult to tell a"
D13-1096,W00-0304,0,0.0278445,"ular style. Introduction Natural language conversation is one of the holy grail of artificial intelligence, and has been taken as the original form of the celebrated Turing test. Previous effort in this direction has largely focused on analyzing the text and modeling the state of the conversation through dialogue models, while in this paThis task is much simpler than modeling a complete dialogue session (e.g., as proposed in Turing test), and probably not enough for real conversation scenario which requires often several rounds of interactions (e.g., automatic question answering system as in (Litman et al., 2000)). However it can shed important light on understanding the complicated mechanism of the interaction between an utterance and its response. The research in this direction will not only instantly help the applications of short session dialogue such as automatic message replying on mobile phone and the chatbot employed in voice assistant like Siri1 , but also it will eventually benefit the modeling of dialogues in a more general setting. Previous effort in modeling lengthy dialogues focused either on rule-based or learning-based models (Carpenter, 1997; Litman et al., 2000; Williams and Young, 2"
D13-1096,W12-1611,0,0.0127283,"nt light on understanding the complicated mechanism of the interaction between an utterance and its response. The research in this direction will not only instantly help the applications of short session dialogue such as automatic message replying on mobile phone and the chatbot employed in voice assistant like Siri1 , but also it will eventually benefit the modeling of dialogues in a more general setting. Previous effort in modeling lengthy dialogues focused either on rule-based or learning-based models (Carpenter, 1997; Litman et al., 2000; Williams and Young, 2007; Schatzmann et al., 2006; Misu et al., 2012). This category of approaches require relatively less data (e.g. reinforcement learning based) for ∗ The work is done when the first author worked as intern at Noah’s Ark Lab, Huawei Techologies. 1 http://en.wikipedia.org/wiki/Siri 935 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 935–945, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics training or no training at all, but much manual effort in designing the rules or the particular learning algorithms. In this paper, we propose to attack this problem"
D13-1096,D11-1054,0,0.125203,"esponse. These matching patterns could be of difFigure 2: Content of the dataset. ferent levels. For example, one may discover from the data that when the word “Hawaii” occurs in the post, the response are more likely to contain words like “trip”, “flight”, or “Honolulu”. On a slightly more abstract level, one may learn that when an entity name is mentioned in the post, it tends to be mentioned again in the response. More complicated matching pattern could also be learned. For example, the response to a post asking “how to” is statistically longer than average responses. As a particular case, Ritter et al. (2011) applied translation model (Brown et al., 1993) on similar parallel data extracted from Twitter in order to extract the word-to-word correlation. Please note that with more sophisticated natural language processing, we can go beyond bag-of-words for more complicated correspondence between post and response. Training Automatic Response Models Although the original (post, response) pairs are rather abundant, they are not enough for discriminative training and testing of retrieval models, for the following reasons. In the labeled pairs, both positive and negative ones are ranked high by some base"
D13-1096,W03-1730,0,0.0207579,"Missing"
D16-1027,D16-1053,0,0.025072,"Missing"
D16-1027,P15-2088,1,0.891904,"Missing"
D16-1027,D15-1166,0,0.0416826,"Missing"
D16-1027,P02-1040,0,0.097279,"Missing"
D16-1027,P16-1008,1,0.249335,"Missing"
D16-1027,D11-1020,1,0.66588,"Missing"
D17-1221,P15-1153,1,0.892052,"he word level to the clause level, which include news headers such as “BEIJING, Nov. 24 (Xinhua) –”, intra-sentential attribution such as “, police said Thursday”, “, he said”, etc. The information filtered by the rules will be processed according to the word salience score. Information with smaller salience score (&lt; ) will be removed. 2084 2.3.2 Phrase-based Optimization for Summary Construction After coarse-grained compression on each single sentence as described above, we design a unified optimization method for summary generation. We refine the phrase-based summary construction model in (Bing et al., 2015) by adjusting the goal as compressive summarization. We consider the salience information obtained by our neural attention model and the compressed sentences in the coarse-grained compression component. Based on the parsed constituency tree for each input sentence as described in Section 2.3.1, we extract the noun-phrases (NPs) and verb-phrases (VPs). The salience Si of a phrase Pi is defined as: X X Si = { tf (t)/ tf (t)} × ai (15) t∈Pi t∈T opic where ai is the salience of the sentence containing Pi . tf (t) is the frequency of the concept t (unigram/bigram) in the whole topic. Thus, Si inher"
D17-1221,P16-1046,0,0.0226281,"mpression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience. By adding sparsity constraints on the number of out"
D17-1221,W04-3247,0,0.565526,"nts on the number of output vectors, we can generate condensed information which can be treated as word salience. Fine-grained and coarse-grained sentence compression strategies are incorporated to produce compressive summaries. Experiments on some benchmark data sets show that our framework achieves better results than the state-of-the-art methods. 1 Introduction The goal of Multi-Document Summarization (MDS) is to automatically produce a succinct summary, preserving the most important information of a set of documents describing a topic1 (Luhn, 1958; Edmundson, 1969; Goldstein et al., 2000; Erkan and Radev, 2004b; Wan et al., 2007; Nenkova and McKeown, 2012). Considering the procedure of summary writing by humans, when people read, they will remember and forget part ∗ The work described in this paper is supported by grants from the Research and Development Grant of Huawei Technologies Co. Ltd (YB2015100076/TH1510257) and the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). 1 A topic represents a real event, e.g., “AlphaGo versus Lee Sedol”. of the content. Information which is more important may make a deep impression easily. When people recall and digest"
D17-1221,W00-0405,0,0.241849,"adding sparsity constraints on the number of output vectors, we can generate condensed information which can be treated as word salience. Fine-grained and coarse-grained sentence compression strategies are incorporated to produce compressive summaries. Experiments on some benchmark data sets show that our framework achieves better results than the state-of-the-art methods. 1 Introduction The goal of Multi-Document Summarization (MDS) is to automatically produce a succinct summary, preserving the most important information of a set of documents describing a topic1 (Luhn, 1958; Edmundson, 1969; Goldstein et al., 2000; Erkan and Radev, 2004b; Wan et al., 2007; Nenkova and McKeown, 2012). Considering the procedure of summary writing by humans, when people read, they will remember and forget part ∗ The work described in this paper is supported by grants from the Research and Development Grant of Huawei Technologies Co. Ltd (YB2015100076/TH1510257) and the Grant Council of the Hong Kong Special Administrative Region, China (Project Code: 14203414). 1 A topic represents a real event, e.g., “AlphaGo versus Lee Sedol”. of the content. Information which is more important may make a deep impression easily. When pe"
D17-1221,P16-1154,1,0.918562,"rained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights are learned automatically by an unsupervised data reconstruction framework which can capture the sentence salience. By adding sparsity constra"
D17-1221,D13-1047,0,0.0188099,"hat aoi = Aoi,: ∈ Rm is the attention weight vector for si . According to Equation 9, a large value in aoi conveys a meaning that the corresponding sentence should contribute more when generating si . We also use the magnitude of the columns in Ao to represent the salience of sentences. 2.3 2.3.1 Compressive Summary Generation Phase Coarse-grained Sentence Compression Using the information distillation result from the cascaded neural attention model, we conduct coarse-grained compression for each individual sentence. Such strategy has been adopted in some multi-document summarization methods (Li et al., 2013; Wang et al., 2013; Yao et al., 2015). Our coarse-grained sentence compression jointly considers word salience obtained from the neural attention model and linguistically-motivated rules. The linguistically-motivated rules are designed based on the observed obvious evidence for uncritical information from the word level to the clause level, which include news headers such as “BEIJING, Nov. 24 (Xinhua) –”, intra-sentential attribution such as “, police said Thursday”, “, he said”, etc. The information filtered by the rules will be processed according to the word salience score. Information wit"
D17-1221,W04-1013,0,0.0249676,"Missing"
D17-1221,P98-2222,0,0.0532758,"ESU4 (R-SU4) are reported. 4 R-1 0.280 0.308 0.360 0.373 0.340 0.377 0.391 0.392 0.393* R-1 0.302 0.312 0.378 0.403 0.353 0.398 0.408 0.419 0.423* R-2 0.046 0.058 0.075 0.083 0.055 0.087 0.097 0.103 0.107* R-SU4 0.088 0.102 0.130 0.144 0.112 0.137 0.150 0.156 0.161* tence compression (Section 2.3.1) show that the compression can indeed improve the sumamrization performance. 4.2 Main Results of Compressive MDS We compare our system C-Attention with several unsupervised summarization baselines and state-of-the-art models. Random baseline selects sentences randomly for each topic. Lead baseline (Wasson, 1998) ranks the news chronologically and extracts the leading sentences one by one. TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004a) estimate sentence salience by applying the PageRank algorithm to the sentence graph. PKUTM (Li et al., 2011) employs manifold-ranking for sentence scoring and selection; ABS-Phrase (Bing et al., 2015) generates abstractive summaries using phrase-based optimization framework. Three other unsupervised methods based on sparse coding are also compared, namely, DSDR (He et al., 2012), MDS-Sparse (Liu et al., 2015), and RAMDS (Li et al., 2015). As sh"
D17-1221,D12-1022,0,0.028573,"der to obtain coherent summaries with good readability, we add some constraints into the ILP framework such as sentence generation constraint: Let βk denote the selection indicator of the sentence xk . If any phrase from xk is selected, βk = 1. Otherwise, βk = 0. For generating a compressed summary sentence, it is required that if βk = 1, at least one NP and at lease one VP of the sentence should be selected. It is expressed as: ∀Pi ∈ xk , αi ≤ βk ∧ X i αi ≥ βk , Other constraints include sentence number, summary length, phrase co-occurrence, etc. For details, please refer to McDonald (2007), Woodsend and Lapata (2012), and Bing et al. (2015). The objective function and constraints are linear. Therefore the optimization can be solved by existing ILP solvers such as the simplex algorithm (Dantzig and Thapa, 2006). In the implementation, we use a package called lp solve3 . In the post-processing, the phrases and sentences in a summary are ordered according to their natural order if they come from the same document. Otherwise, they are ordered according to the timestamps of the corresponding documents. 3 Experimental Setup 3.1 Datasets DUC: Both DUC 2006 and DUC 2007 are used in our evaluation. DUC 2006 and DU"
D17-1221,D15-1166,0,0.250292,"is preserved. Precisely, the recaller outputs fewer vectors s than that of the input UHFRQVWUXFW ܺ ܿଶ ܪ ܿଶ Dec ݏଶ ܿଵ ݏଵ ܿଵ UHFDOOHU ܵ ܣ ܣ ܿ Enc UHDGHU supervised manner. Thereafter, the word salience is fed into a coarse-grained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model tha"
D17-1221,C12-1128,0,0.047742,"Missing"
D17-1221,K16-1028,0,0.165303,"ଶ ܿଵ ݏଵ ܿଵ UHFDOOHU ܵ ܣ ܣ ܿ Enc UHDGHU supervised manner. Thereafter, the word salience is fed into a coarse-grained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights are learned automatically b"
D17-1221,D15-1044,0,0.299409,"W ܺ ܿଶ ܪ ܿଶ Dec ݏଶ ܿଵ ݏଵ ܿଵ UHFDOOHU ܵ ܣ ܣ ܿ Enc UHDGHU supervised manner. Thereafter, the word salience is fed into a coarse-grained sentence compression component. Finally, the attention weights are integrated into a phrase-based optimization framework for compressive summary generation. In fact, the notion of “attention” has gained popularity recently in neural network modeling, which has improved the performance of many tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015). However, very few previous works employ attention mechanism to tackle MDS. Rush et al. (2015) and Nallapati et al. (2016) employed attention-based sequenceto-sequence (seq2seq) framework only for sentence summarization. Gu et al. (2016), Cheng and Lapata (2016), and Nallapati et al. (2016) also utilized seq2seq based framework with attention modeling for short text or single document summarization. Different from their works, our framework aims at conducting multi-document summarization in an unsupervised manner. Our contributions are as follows: (1) We propose a cascaded attention model that captures salient information in different semantic representations. (2) The attention weights"
D17-1221,P13-1136,0,0.0304866,"Rm is the attention weight vector for si . According to Equation 9, a large value in aoi conveys a meaning that the corresponding sentence should contribute more when generating si . We also use the magnitude of the columns in Ao to represent the salience of sentences. 2.3 2.3.1 Compressive Summary Generation Phase Coarse-grained Sentence Compression Using the information distillation result from the cascaded neural attention model, we conduct coarse-grained compression for each individual sentence. Such strategy has been adopted in some multi-document summarization methods (Li et al., 2013; Wang et al., 2013; Yao et al., 2015). Our coarse-grained sentence compression jointly considers word salience obtained from the neural attention model and linguistically-motivated rules. The linguistically-motivated rules are designed based on the observed obvious evidence for uncritical information from the word level to the clause level, which include news headers such as “BEIJING, Nov. 24 (Xinhua) –”, intra-sentential attribution such as “, police said Thursday”, “, he said”, etc. The information filtered by the rules will be processed according to the word salience score. Information with smaller salience"
D17-1221,W04-3252,0,\N,Missing
D17-1221,C98-2217,0,\N,Missing
D18-1421,D17-1091,0,0.0713909,"generator within the Seq2Seq framework. The main difference lies in that we use another trainable neural network, referred to as evaluator, to guide the training of the generator through reinforcement learning. There is also work on paraphrasing generation in different settings. For example, Mallinson et al. (2017) leverage bilingual data to produce paraphrases by pivoting over a shared translation in another language. Wieting et al. (2017); Wieting and Gimpel (2018) use neural machine translation to generate paraphrases via back-translation of bilingual sentence pairs. Buck et al. (2018) and Dong et al. (2017) tackle the problem of QA-specific paraphrasing with the guidance from an external 3872 Figure 3: Examples of the generated paraphrases by different models on Quora-II. QA system and an associated evaluation metric. Inverse reinforcement learning (IRL) aims to learn a reward function from expert demonstrations. Abbeel and Ng (2004) propose apprenticeship learning, which uses a feature based linear reward function and learns to match feature expectations. Ratliff et al. (2006) cast the problem as structured maximum margin prediction. Ziebart et al. (2008) propose max entropy IRL in order to sol"
D18-1421,N18-1170,0,0.0918222,"k Neural paraphrase generation recently draws attention in different application scenarios. The task is often formalized as a sequence-to-sequence (Seq2Seq) learning problem. Prakash et al. (2016) employ a stacked residual LSTM network in the Seq2Seq model to enlarge the model capacity. Cao et al. (2017) utilize an additional vocabulary to restrict word candidates during generation. Gupta et al. (2018) use a variational auto-encoder framework to generate more diverse paraphrases. Ma et al. (2018) utilize an attention layer instead of a linear mapping in the decoder to pick up word candidates. Iyyer et al. (2018) harness syntactic information for controllable paraphrase generation. Zhang and Lapata (2017) tackle a similar task of sentence simplification withe Seq2Seq model coupled with deep reinforcement learning, in which the reward function is manually defined for the task. Similar to these works, we also pretrain the paraphrase generator within the Seq2Seq framework. The main difference lies in that we use another trainable neural network, referred to as evaluator, to guide the training of the generator through reinforcement learning. There is also work on paraphrasing generation in different setti"
D18-1421,N06-1058,0,0.352798,"task in which given a sentence the system creates paraphrases of it. Paraphrase generation is an important task in NLP, which can be a key technology in many applications such as retrieval based question answering, semantic parsing, query reformulation in web search, data augmentation for dialogue system. However, due to the complexity of natural language, automatically generating accurate and diverse paraphrases is still very challenging. Traditional symbolic approaches to paraphrase generation include rule-based methods (McKeown, 1983), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), grammar-based methods (Narayan et al., 2016), and statistical machine translation (SMT) based methods (Quirk et al., 2004; Zhao et al., 2008, 2009). Recently, neural network based sequence-tosequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g., Cho et al. (2014); Wu et al. (2016); Shang et al. (2015); Vinyals and Le (2015); Rush et al. (2015); Yin et al. (2016)). Paraphrase generation can naturally be formulated as a Seq2Seq problem (Cao et al., 2017; Prakash et a"
D18-1421,D17-1126,0,0.111715,"(up to at most bi-grams) and METEOR (Lavie and Agarwal, 2007). As pointed out, ideally it would be better not to merely use a lexical measure like ROUGE or BLEU for evaluation of paraphrasing. We choose to use them for 1 We directly present the results reported in Gupta et al. (2018) on the same dateset and settings. reproducibility of our experimental results by others. For the manual evaluation, we conduct evaluation on the generated paraphrases in terms of relevance and fluency. 4.2 Datasets We evaluate our methods with the Quora question pair dataset 2 and Twitter URL paraphrasing corpus (Lan et al., 2017). Both datasets contain positive and negative examples of paraphrases so that we can evaluate the RbM-SL and RbM-IRL methods. We randomly split the Quora dataset in two different ways obtaining two experimental settings: Quora-I and Quora-II. In Quora-I, we partition the dataset by question pairs, while in Quora-II, we partition by question ids such that there is no shared question between the training and test/validation datasets. In addition, we sample a smaller training set in Quora-II to make the task more challenging. Twitter URL paraphrasing corpus contains two subsets, one is labeled by"
D18-1421,W07-0734,0,0.0766939,"neural network based methods, we choose five baseline models: the attentive Seq2Seq model (Bahdanau et al., 2015), the stacked Residual LSTM networks (Prakash et al., 2016), the variational auto-encoder (VAESVG-eq) (Gupta et al., 2018) 1 , the pointergenerator (See et al., 2017), and the reinforced pointer-generator with ROUGE-2 as reward (RLROUGE) (Ranzato et al., 2016). We conduct both automatic and manual evaluation on the models. For the automatic evaluation, we adopt four evaluation measures: ROUGE-1, ROUGE-2 (Lin, 2004), BLEU (Papineni et al., 2002) (up to at most bi-grams) and METEOR (Lavie and Agarwal, 2007). As pointed out, ideally it would be better not to merely use a lexical measure like ROUGE or BLEU for evaluation of paraphrasing. We choose to use them for 1 We directly present the results reported in Gupta et al. (2018) on the same dateset and settings. reproducibility of our experimental results by others. For the manual evaluation, we conduct evaluation on the generated paraphrases in terms of relevance and fluency. 4.2 Datasets We evaluate our methods with the Quora question pair dataset 2 and Twitter URL paraphrasing corpus (Lan et al., 2017). Both datasets contain positive and negativ"
D18-1421,D17-1230,0,0.0288573,"dversarial inverse reinforcement learning in visual story telling. To the best of our knowledge, our work is the first that applies deep IRL into a Seq2Seq task. Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) is a family of unsupervised generative models. GAN contains a generator and a discriminator, respectively for generating examples from random noises and distinguishing generated examples from real examples, and they are trained in an adversarial way. There are applications of GAN on NLP, such as text generation (Yu et al., 2017; Guo et al., 2018) and dialogue generation (Li et al., 2017). RankGAN (Lin et al., 2017) is the one most similar to RbM-IRL that employs a ranking model as the discriminator. However, RankGAN works for text generation rather than sequenceto-sequence learning, and training of generator in RankGAN relies on parallel data while the training of RbM-IRL can use non-parallel data. There are connections between GAN and IRL as pointed by Finn et al. (2016a); Ho and Ermon (2016). However, there are significant differences between GAN and our RbM-IRL model. GAN employs the discriminator to distinguish generated examples from real examples, while RbMIRL employs t"
D18-1421,W04-1013,0,0.367993,"llenge in paraphrase generation lies in the definition of the evaluation measure. Ideally the measure is able to calculate the semantic similarity between a generated paraphrase and the given sentence. In a straightforward application of Seq2Seq to paraphrase generation one would make use of cross entropy as evaluation measure, which can only be a loose approximation of semantic similarity. To tackle this problem, Ranzato et al. (2016) propose employing reinforcement learning (RL) to guide the training of Seq2Seq and using lexical-based measures such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as a reward function. However, these lexical measures may not perfectly represent semantic similarity. It is likely that a correctly generated sequence gets a low ROUGE score due to lexical mismatch. For instance, an input sentence “how far is Earth from Sun” can be paraphrased as “what is the distance between Sun and Earth”, but it will 3865 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3865–3878 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics get a very low ROUGE score given “how many miles is"
D18-1421,E17-1083,0,0.0747494,"information for controllable paraphrase generation. Zhang and Lapata (2017) tackle a similar task of sentence simplification withe Seq2Seq model coupled with deep reinforcement learning, in which the reward function is manually defined for the task. Similar to these works, we also pretrain the paraphrase generator within the Seq2Seq framework. The main difference lies in that we use another trainable neural network, referred to as evaluator, to guide the training of the generator through reinforcement learning. There is also work on paraphrasing generation in different settings. For example, Mallinson et al. (2017) leverage bilingual data to produce paraphrases by pivoting over a shared translation in another language. Wieting et al. (2017); Wieting and Gimpel (2018) use neural machine translation to generate paraphrases via back-translation of bilingual sentence pairs. Buck et al. (2018) and Dong et al. (2017) tackle the problem of QA-specific paraphrasing with the guidance from an external 3872 Figure 3: Examples of the generated paraphrases by different models on Quora-II. QA system and an associated evaluation metric. Inverse reinforcement learning (IRL) aims to learn a reward function from expert d"
D18-1421,J83-1001,0,0.472573,"ween Sun and Earth” are paraphrases. Paraphrase generation refers to a task in which given a sentence the system creates paraphrases of it. Paraphrase generation is an important task in NLP, which can be a key technology in many applications such as retrieval based question answering, semantic parsing, query reformulation in web search, data augmentation for dialogue system. However, due to the complexity of natural language, automatically generating accurate and diverse paraphrases is still very challenging. Traditional symbolic approaches to paraphrase generation include rule-based methods (McKeown, 1983), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), grammar-based methods (Narayan et al., 2016), and statistical machine translation (SMT) based methods (Quirk et al., 2004; Zhao et al., 2008, 2009). Recently, neural network based sequence-tosequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g., Cho et al. (2014); Wu et al. (2016); Shang et al. (2015); Vinyals and Le (2015); Rush et al. (2015); Yin et al. (2016)). Paraphrase generati"
D18-1421,W16-6625,0,0.0488204,"araphrases of it. Paraphrase generation is an important task in NLP, which can be a key technology in many applications such as retrieval based question answering, semantic parsing, query reformulation in web search, data augmentation for dialogue system. However, due to the complexity of natural language, automatically generating accurate and diverse paraphrases is still very challenging. Traditional symbolic approaches to paraphrase generation include rule-based methods (McKeown, 1983), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), grammar-based methods (Narayan et al., 2016), and statistical machine translation (SMT) based methods (Quirk et al., 2004; Zhao et al., 2008, 2009). Recently, neural network based sequence-tosequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g., Cho et al. (2014); Wu et al. (2016); Shang et al. (2015); Vinyals and Le (2015); Rush et al. (2015); Yin et al. (2016)). Paraphrase generation can naturally be formulated as a Seq2Seq problem (Cao et al., 2017; Prakash et al., 2016; Gupta et al., 2018; Su and Yan, 2017"
D18-1421,P18-1083,0,0.0241289,"on from expert demonstrations. Abbeel and Ng (2004) propose apprenticeship learning, which uses a feature based linear reward function and learns to match feature expectations. Ratliff et al. (2006) cast the problem as structured maximum margin prediction. Ziebart et al. (2008) propose max entropy IRL in order to solve the problem of expert suboptimality. Recent work involving deep learning in IRL includes Finn et al. (2016b) and Ho et al. (2016). There does not seem to be much work on IRL for NLP. In Neu and Szepesv´ari (2009), parsing is formalized as a feature expectation matching problem. Wang et al. (2018) apply adversarial inverse reinforcement learning in visual story telling. To the best of our knowledge, our work is the first that applies deep IRL into a Seq2Seq task. Generative Adversarial Networks (GAN) (Goodfellow et al., 2014) is a family of unsupervised generative models. GAN contains a generator and a discriminator, respectively for generating examples from random noises and distinguishing generated examples from real examples, and they are trained in an adversarial way. There are applications of GAN on NLP, such as text generation (Yu et al., 2017; Guo et al., 2018) and dialogue gene"
D18-1421,P02-1040,0,0.104001,"8; Su and Yan, 2017). The main challenge in paraphrase generation lies in the definition of the evaluation measure. Ideally the measure is able to calculate the semantic similarity between a generated paraphrase and the given sentence. In a straightforward application of Seq2Seq to paraphrase generation one would make use of cross entropy as evaluation measure, which can only be a loose approximation of semantic similarity. To tackle this problem, Ranzato et al. (2016) propose employing reinforcement learning (RL) to guide the training of Seq2Seq and using lexical-based measures such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) as a reward function. However, these lexical measures may not perfectly represent semantic similarity. It is likely that a correctly generated sequence gets a low ROUGE score due to lexical mismatch. For instance, an input sentence “how far is Earth from Sun” can be paraphrased as “what is the distance between Sun and Earth”, but it will 3865 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3865–3878 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics get a very low ROUGE score gi"
D18-1421,P18-1042,0,0.0669772,"with deep reinforcement learning, in which the reward function is manually defined for the task. Similar to these works, we also pretrain the paraphrase generator within the Seq2Seq framework. The main difference lies in that we use another trainable neural network, referred to as evaluator, to guide the training of the generator through reinforcement learning. There is also work on paraphrasing generation in different settings. For example, Mallinson et al. (2017) leverage bilingual data to produce paraphrases by pivoting over a shared translation in another language. Wieting et al. (2017); Wieting and Gimpel (2018) use neural machine translation to generate paraphrases via back-translation of bilingual sentence pairs. Buck et al. (2018) and Dong et al. (2017) tackle the problem of QA-specific paraphrasing with the guidance from an external 3872 Figure 3: Examples of the generated paraphrases by different models on Quora-II. QA system and an associated evaluation metric. Inverse reinforcement learning (IRL) aims to learn a reward function from expert demonstrations. Abbeel and Ng (2004) propose apprenticeship learning, which uses a feature based linear reward function and learns to match feature expectat"
D18-1421,D16-1244,0,0.105742,"Missing"
D18-1421,D17-1026,0,0.0726079,"e Seq2Seq model coupled with deep reinforcement learning, in which the reward function is manually defined for the task. Similar to these works, we also pretrain the paraphrase generator within the Seq2Seq framework. The main difference lies in that we use another trainable neural network, referred to as evaluator, to guide the training of the generator through reinforcement learning. There is also work on paraphrasing generation in different settings. For example, Mallinson et al. (2017) leverage bilingual data to produce paraphrases by pivoting over a shared translation in another language. Wieting et al. (2017); Wieting and Gimpel (2018) use neural machine translation to generate paraphrases via back-translation of bilingual sentence pairs. Buck et al. (2018) and Dong et al. (2017) tackle the problem of QA-specific paraphrasing with the guidance from an external 3872 Figure 3: Examples of the generated paraphrases by different models on Quora-II. QA system and an associated evaluation metric. Inverse reinforcement learning (IRL) aims to learn a reward function from expert demonstrations. Abbeel and Ng (2004) propose apprenticeship learning, which uses a feature based linear reward function and learn"
D18-1421,C16-1275,0,0.363662,"zilay, 2006), grammar-based methods (Narayan et al., 2016), and statistical machine translation (SMT) based methods (Quirk et al., 2004; Zhao et al., 2008, 2009). Recently, neural network based sequence-tosequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g., Cho et al. (2014); Wu et al. (2016); Shang et al. (2015); Vinyals and Le (2015); Rush et al. (2015); Yin et al. (2016)). Paraphrase generation can naturally be formulated as a Seq2Seq problem (Cao et al., 2017; Prakash et al., 2016; Gupta et al., 2018; Su and Yan, 2017). The main challenge in paraphrase generation lies in the definition of the evaluation measure. Ideally the measure is able to calculate the semantic similarity between a generated paraphrase and the given sentence. In a straightforward application of Seq2Seq to paraphrase generation one would make use of cross entropy as evaluation measure, which can only be a loose approximation of semantic similarity. To tackle this problem, Ranzato et al. (2016) propose employing reinforcement learning (RL) to guide the training of Seq2Seq and using lexical-based meas"
D18-1421,W04-3219,0,0.488528,"e a key technology in many applications such as retrieval based question answering, semantic parsing, query reformulation in web search, data augmentation for dialogue system. However, due to the complexity of natural language, automatically generating accurate and diverse paraphrases is still very challenging. Traditional symbolic approaches to paraphrase generation include rule-based methods (McKeown, 1983), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), grammar-based methods (Narayan et al., 2016), and statistical machine translation (SMT) based methods (Quirk et al., 2004; Zhao et al., 2008, 2009). Recently, neural network based sequence-tosequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g., Cho et al. (2014); Wu et al. (2016); Shang et al. (2015); Vinyals and Le (2015); Rush et al. (2015); Yin et al. (2016)). Paraphrase generation can naturally be formulated as a Seq2Seq problem (Cao et al., 2017; Prakash et al., 2016; Gupta et al., 2018; Su and Yan, 2017). The main challenge in paraphrase generation lies in the definition of the"
D18-1421,1983.tc-1.13,0,0.232839,"Missing"
D18-1421,D15-1044,0,0.0435915,"hrase generation include rule-based methods (McKeown, 1983), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), grammar-based methods (Narayan et al., 2016), and statistical machine translation (SMT) based methods (Quirk et al., 2004; Zhao et al., 2008, 2009). Recently, neural network based sequence-tosequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g., Cho et al. (2014); Wu et al. (2016); Shang et al. (2015); Vinyals and Le (2015); Rush et al. (2015); Yin et al. (2016)). Paraphrase generation can naturally be formulated as a Seq2Seq problem (Cao et al., 2017; Prakash et al., 2016; Gupta et al., 2018; Su and Yan, 2017). The main challenge in paraphrase generation lies in the definition of the evaluation measure. Ideally the measure is able to calculate the semantic similarity between a generated paraphrase and the given sentence. In a straightforward application of Seq2Seq to paraphrase generation one would make use of cross entropy as evaluation measure, which can only be a loose approximation of semantic similarity. To tackle this proble"
D18-1421,W16-0106,1,0.8036,"lude rule-based methods (McKeown, 1983), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), grammar-based methods (Narayan et al., 2016), and statistical machine translation (SMT) based methods (Quirk et al., 2004; Zhao et al., 2008, 2009). Recently, neural network based sequence-tosequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g., Cho et al. (2014); Wu et al. (2016); Shang et al. (2015); Vinyals and Le (2015); Rush et al. (2015); Yin et al. (2016)). Paraphrase generation can naturally be formulated as a Seq2Seq problem (Cao et al., 2017; Prakash et al., 2016; Gupta et al., 2018; Su and Yan, 2017). The main challenge in paraphrase generation lies in the definition of the evaluation measure. Ideally the measure is able to calculate the semantic similarity between a generated paraphrase and the given sentence. In a straightforward application of Seq2Seq to paraphrase generation one would make use of cross entropy as evaluation measure, which can only be a loose approximation of semantic similarity. To tackle this problem, Ranzato et al. ("
D18-1421,P17-1099,0,0.728416,"in Natural Language Processing, pages 3865–3878 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics get a very low ROUGE score given “how many miles is it from Earth to Sun” as a reference. In this work, we propose taking a data-driven approach to train a model that can conduct evaluation in learning for paraphrasing generation. The framework contains two modules, a generator (for paraphrase generation) and an evaluator (for paraphrase evaluation). The generator is a Seq2Seq learning model with attention and copy mechanism (Bahdanau et al., 2015; See et al., 2017), which is first trained with cross entropy loss and then fine-tuned by using policy gradient with supervisions from the evaluator as rewards. The evaluator is a deep matching model, specifically a decomposable attention model (Parikh et al., 2016), which can be trained by supervised learning (SL) when both positive and negative examples are available as training data, or by inverse reinforcement learning (IRL) with outputs from the generator as supervisions when only positive examples are available. In the latter setting, for the training of evaluator using IRL, we develop a novel algorithm b"
D18-1421,P15-1152,1,0.813274,"ng. Traditional symbolic approaches to paraphrase generation include rule-based methods (McKeown, 1983), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), grammar-based methods (Narayan et al., 2016), and statistical machine translation (SMT) based methods (Quirk et al., 2004; Zhao et al., 2008, 2009). Recently, neural network based sequence-tosequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g., Cho et al. (2014); Wu et al. (2016); Shang et al. (2015); Vinyals and Le (2015); Rush et al. (2015); Yin et al. (2016)). Paraphrase generation can naturally be formulated as a Seq2Seq problem (Cao et al., 2017; Prakash et al., 2016; Gupta et al., 2018; Su and Yan, 2017). The main challenge in paraphrase generation lies in the definition of the evaluation measure. Ideally the measure is able to calculate the semantic similarity between a generated paraphrase and the given sentence. In a straightforward application of Seq2Seq to paraphrase generation one would make use of cross entropy as evaluation measure, which can only be a loose approximation of"
D18-1421,D17-1062,0,0.019294,". The task is often formalized as a sequence-to-sequence (Seq2Seq) learning problem. Prakash et al. (2016) employ a stacked residual LSTM network in the Seq2Seq model to enlarge the model capacity. Cao et al. (2017) utilize an additional vocabulary to restrict word candidates during generation. Gupta et al. (2018) use a variational auto-encoder framework to generate more diverse paraphrases. Ma et al. (2018) utilize an attention layer instead of a linear mapping in the decoder to pick up word candidates. Iyyer et al. (2018) harness syntactic information for controllable paraphrase generation. Zhang and Lapata (2017) tackle a similar task of sentence simplification withe Seq2Seq model coupled with deep reinforcement learning, in which the reward function is manually defined for the task. Similar to these works, we also pretrain the paraphrase generator within the Seq2Seq framework. The main difference lies in that we use another trainable neural network, referred to as evaluator, to guide the training of the generator through reinforcement learning. There is also work on paraphrasing generation in different settings. For example, Mallinson et al. (2017) leverage bilingual data to produce paraphrases by pi"
D18-1421,P09-1094,0,0.449927,"Missing"
D18-1421,D17-1127,0,0.0547969,"an et al., 2016), and statistical machine translation (SMT) based methods (Quirk et al., 2004; Zhao et al., 2008, 2009). Recently, neural network based sequence-tosequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g., Cho et al. (2014); Wu et al. (2016); Shang et al. (2015); Vinyals and Le (2015); Rush et al. (2015); Yin et al. (2016)). Paraphrase generation can naturally be formulated as a Seq2Seq problem (Cao et al., 2017; Prakash et al., 2016; Gupta et al., 2018; Su and Yan, 2017). The main challenge in paraphrase generation lies in the definition of the evaluation measure. Ideally the measure is able to calculate the semantic similarity between a generated paraphrase and the given sentence. In a straightforward application of Seq2Seq to paraphrase generation one would make use of cross entropy as evaluation measure, which can only be a loose approximation of semantic similarity. To tackle this problem, Ranzato et al. (2016) propose employing reinforcement learning (RL) to guide the training of Seq2Seq and using lexical-based measures such as BLEU (Papineni et al., 200"
D18-1421,P08-1116,0,0.180763,"n many applications such as retrieval based question answering, semantic parsing, query reformulation in web search, data augmentation for dialogue system. However, due to the complexity of natural language, automatically generating accurate and diverse paraphrases is still very challenging. Traditional symbolic approaches to paraphrase generation include rule-based methods (McKeown, 1983), thesaurus-based methods (Bolshakov and Gelbukh, 2004; Kauchak and Barzilay, 2006), grammar-based methods (Narayan et al., 2016), and statistical machine translation (SMT) based methods (Quirk et al., 2004; Zhao et al., 2008, 2009). Recently, neural network based sequence-tosequence (Seq2Seq) learning has made remarkable success in various NLP tasks, including machine translation, short-text conversation, text summarization, and question answering (e.g., Cho et al. (2014); Wu et al. (2016); Shang et al. (2015); Vinyals and Le (2015); Rush et al. (2015); Yin et al. (2016)). Paraphrase generation can naturally be formulated as a Seq2Seq problem (Cao et al., 2017; Prakash et al., 2016; Gupta et al., 2018; Su and Yan, 2017). The main challenge in paraphrase generation lies in the definition of the evaluation measure."
J04-1001,P01-1005,0,0.0147574,"to A, because the misclassified instances (crosses) with D are those mistakenly classified from C, and they will not have much negative effect on classification to A, even though the translation from Chinese into English can introduce some noise. Similar explanations can be given for other classification decisions. In contrast, MB uses only the instances in A and B to construct a classifier. When the number of misclassified instances increases (as is inevitable in bootstrapping), its performance will stop improving. This phenomenon has also been observed when MB is applied to other tasks (cf. Banko and Brill 2001; Pierce and Cardie 2001). 12 Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping 3.4 Relationship between BB and Co-training We note that there are similarities between BB and co-training. Both BB and co-training execute two bootstrapping processes in parallel and make the two processes collaborate with one another in order to improve their performance. The two processes look at different types of information in data and exchange the information in learning. However, there are also significant differences between BB and co-training. In co-training, the two processes use di"
J04-1001,P91-1034,0,0.138266,"Missing"
J04-1001,P94-1020,0,0.0396289,"ation-based learning (Mangu and Brill 1997), neural networks (Towell and Voorhees 1998), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier (i.e., a classification program) using features representing context information (e.g., surrounding context words). For other related work on translation disambiguation, see Brown et al. (1991), Bruce and Weibe (1994), Dagan and Itai (1994), Lin (1997), Pedersen and Bruce (1997), Schutze (1998), Kikui (1999), Mihalcea and Moldovan (1999), Koehn and Knight (2000), and Zhou, Ding, and Huang (2001). Let us formulate the problem of word sense (translation) disambiguation as follows. Let E denote a set of words. Let ε denote an ambiguous word in E, and let e 2 Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping denote a context word in E. (Throughout this article, we use Greek letters to represent ambiguous words and italic letters to represent context words.) Let Tε denote the set of senses"
J04-1001,W99-0613,0,0.0254781,"Missing"
J04-1001,J94-4003,0,0.600747,"ngu and Brill 1997), neural networks (Towell and Voorhees 1998), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier (i.e., a classification program) using features representing context information (e.g., surrounding context words). For other related work on translation disambiguation, see Brown et al. (1991), Bruce and Weibe (1994), Dagan and Itai (1994), Lin (1997), Pedersen and Bruce (1997), Schutze (1998), Kikui (1999), Mihalcea and Moldovan (1999), Koehn and Knight (2000), and Zhou, Ding, and Huang (2001). Let us formulate the problem of word sense (translation) disambiguation as follows. Let E denote a set of words. Let ε denote an ambiguous word in E, and let e 2 Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping denote a context word in E. (Throughout this article, we use Greek letters to represent ambiguous words and italic letters to represent context words.) Let Tε denote the set of senses of ε, and let tε denot"
J04-1001,H92-1045,0,0.113973,"Missing"
J04-1001,W99-0905,0,0.0174338,"ng and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier (i.e., a classification program) using features representing context information (e.g., surrounding context words). For other related work on translation disambiguation, see Brown et al. (1991), Bruce and Weibe (1994), Dagan and Itai (1994), Lin (1997), Pedersen and Bruce (1997), Schutze (1998), Kikui (1999), Mihalcea and Moldovan (1999), Koehn and Knight (2000), and Zhou, Ding, and Huang (2001). Let us formulate the problem of word sense (translation) disambiguation as follows. Let E denote a set of words. Let ε denote an ambiguous word in E, and let e 2 Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping denote a context word in E. (Throughout this article, we use Greek letters to represent ambiguous words and italic letters to represent context words.) Let Tε denote the set of senses of ε, and let tε denote a sense in Tε . Let eε stand for an instance representing a context"
J04-1001,P97-1009,0,0.0410233,"ural networks (Towell and Voorhees 1998), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier (i.e., a classification program) using features representing context information (e.g., surrounding context words). For other related work on translation disambiguation, see Brown et al. (1991), Bruce and Weibe (1994), Dagan and Itai (1994), Lin (1997), Pedersen and Bruce (1997), Schutze (1998), Kikui (1999), Mihalcea and Moldovan (1999), Koehn and Knight (2000), and Zhou, Ding, and Huang (2001). Let us formulate the problem of word sense (translation) disambiguation as follows. Let E denote a set of words. Let ε denote an ambiguous word in E, and let e 2 Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping denote a context word in E. (Throughout this article, we use Greek letters to represent ambiguous words and italic letters to represent context words.) Let Tε denote the set of senses of ε, and let tε denote a sense in"
J04-1001,P99-1020,0,0.00730641,"99), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier (i.e., a classification program) using features representing context information (e.g., surrounding context words). For other related work on translation disambiguation, see Brown et al. (1991), Bruce and Weibe (1994), Dagan and Itai (1994), Lin (1997), Pedersen and Bruce (1997), Schutze (1998), Kikui (1999), Mihalcea and Moldovan (1999), Koehn and Knight (2000), and Zhou, Ding, and Huang (2001). Let us formulate the problem of word sense (translation) disambiguation as follows. Let E denote a set of words. Let ε denote an ambiguous word in E, and let e 2 Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping denote a context word in E. (Throughout this article, we use Greek letters to represent ambiguous words and italic letters to represent context words.) Let Tε denote the set of senses of ε, and let tε denote a sense in Tε . Let eε stand for an instance representing a context of ε, that is, a sequence of"
J04-1001,P96-1006,0,0.0929503,") can be viewed as a problem of classification and can be addressed by employing various supervised learning methods. For example, with such a learning method, an English sentence containing an ambiguous English word corresponds to an instance, and the Chinese translation of the word in the context (i.e., the word sense) corresponds to a classification decision (a label). Many methods for word sense disambiguation based on supervised learning technique have been proposed. They include those using naive Bayes (Gale, Church, and Yarowsky 1992a), decision lists (Yarowsky 1994), nearest neighbor (Ng and Lee 1996), transformation-based learning (Mangu and Brill 1997), neural networks (Towell and Voorhees 1998), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier (i.e., a classification program) using features representing context information (e.g., surrounding context words). For other related work on translation disambiguation, see Brown et a"
J04-1001,A00-2009,0,0.266863,"ponds to an instance, and the Chinese translation of the word in the context (i.e., the word sense) corresponds to a classification decision (a label). Many methods for word sense disambiguation based on supervised learning technique have been proposed. They include those using naive Bayes (Gale, Church, and Yarowsky 1992a), decision lists (Yarowsky 1994), nearest neighbor (Ng and Lee 1996), transformation-based learning (Mangu and Brill 1997), neural networks (Towell and Voorhees 1998), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier (i.e., a classification program) using features representing context information (e.g., surrounding context words). For other related work on translation disambiguation, see Brown et al. (1991), Bruce and Weibe (1994), Dagan and Itai (1994), Lin (1997), Pedersen and Bruce (1997), Schutze (1998), Kikui (1999), Mihalcea and Moldovan (1999), Koehn and Knight (2000), and Zhou, Ding, and Huang (2001). Let u"
J04-1001,W97-0322,0,0.0463379,"s (Towell and Voorhees 1998), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier (i.e., a classification program) using features representing context information (e.g., surrounding context words). For other related work on translation disambiguation, see Brown et al. (1991), Bruce and Weibe (1994), Dagan and Itai (1994), Lin (1997), Pedersen and Bruce (1997), Schutze (1998), Kikui (1999), Mihalcea and Moldovan (1999), Koehn and Knight (2000), and Zhou, Ding, and Huang (2001). Let us formulate the problem of word sense (translation) disambiguation as follows. Let E denote a set of words. Let ε denote an ambiguous word in E, and let e 2 Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping denote a context word in E. (Throughout this article, we use Greek letters to represent ambiguous words and italic letters to represent context words.) Let Tε denote the set of senses of ε, and let tε denote a sense in Tε . Let eε stand for an i"
J04-1001,W01-0501,0,0.0148567,"Missing"
J04-1001,J98-1004,0,0.242809,"), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier (i.e., a classification program) using features representing context information (e.g., surrounding context words). For other related work on translation disambiguation, see Brown et al. (1991), Bruce and Weibe (1994), Dagan and Itai (1994), Lin (1997), Pedersen and Bruce (1997), Schutze (1998), Kikui (1999), Mihalcea and Moldovan (1999), Koehn and Knight (2000), and Zhou, Ding, and Huang (2001). Let us formulate the problem of word sense (translation) disambiguation as follows. Let E denote a set of words. Let ε denote an ambiguous word in E, and let e 2 Li and Li Word Translation Disambiguation Using Bilingual Bootstrapping denote a context word in E. (Throughout this article, we use Greek letters to represent ambiguous words and italic letters to represent context words.) Let Tε denote the set of senses of ε, and let tε denote a sense in Tε . Let eε stand for an instance represen"
J04-1001,J98-1005,0,0.189408,"supervised learning methods. For example, with such a learning method, an English sentence containing an ambiguous English word corresponds to an instance, and the Chinese translation of the word in the context (i.e., the word sense) corresponds to a classification decision (a label). Many methods for word sense disambiguation based on supervised learning technique have been proposed. They include those using naive Bayes (Gale, Church, and Yarowsky 1992a), decision lists (Yarowsky 1994), nearest neighbor (Ng and Lee 1996), transformation-based learning (Mangu and Brill 1997), neural networks (Towell and Voorhees 1998), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier (i.e., a classification program) using features representing context information (e.g., surrounding context words). For other related work on translation disambiguation, see Brown et al. (1991), Bruce and Weibe (1994), Dagan and Itai (1994), Lin (1997), Pedersen and Bruce (1997), S"
J04-1001,P94-1013,0,0.10202,"general, word sense disambiguation) can be viewed as a problem of classification and can be addressed by employing various supervised learning methods. For example, with such a learning method, an English sentence containing an ambiguous English word corresponds to an instance, and the Chinese translation of the word in the context (i.e., the word sense) corresponds to a classification decision (a label). Many methods for word sense disambiguation based on supervised learning technique have been proposed. They include those using naive Bayes (Gale, Church, and Yarowsky 1992a), decision lists (Yarowsky 1994), nearest neighbor (Ng and Lee 1996), transformation-based learning (Mangu and Brill 1997), neural networks (Towell and Voorhees 1998), Winnow (Golding and Roth 1999), boosting (Escudero, Marquez, and Rigau 2000), and naive Bayesian ensemble (Pedersen 2000). The assumption behind these methods is that it is nearly always possible to determine the sense of an ambiguous word by referring to its context, and thus all of the methods build a classifier (i.e., a classification program) using features representing context information (e.g., surrounding context words). For other related work on transl"
J04-1001,P95-1026,0,0.957495,"a (i.e., in Chinese, either to [gongchang] or to [zhiwu]), our goal would be to determine the correct Chinese translation. That is, word translation disambiguation is essentially a special case of word sense disambiguation (in the above example, gongchang would correspond to the sense of factory and zhiwu to the sense of flora).1 We could view word translation disambiguation as a problem of classification. To perform the task, we could employ a supervised learning method, but since to do so would require human labeling of data, which would be expensive, bootstrapping would be a better choice. Yarowsky (1995) has proposed a bootstrapping method for word sense disambiguation. When applied to translation from English to Chinese, his method starts learning with a small number of English sentences that contain ambiguous English words and that are labeled with correct Chinese translations of those words. It then uses these classified sentences as training data to create a classifier (e.g., a decision list), which it uses to classify unclassified sentences containing the same ambiguous words. The output of this process is then used as additional training data. It also adopts the one-sense-per-discourse"
J04-1001,O01-2001,0,0.16831,"Missing"
J98-2002,J94-4005,0,0.00957789,"24, Number 2 Table 9 Example input data as doubles. see in see with girl with man with Table 10 Example input data as triples. see in park see with telescope girl with scarf see with friend man with hat Table 11 Example input data as quadruples and labels. see girl in park see man with telescope see girl with scarf ADV ADV ADN then the prepositional phrase is attached to verb, if the latter probability is significantly larger, it is attached to nounl, and otherwise no decision is made. The second approach (Sekine et al. 1992; Chang, Luo, and Su 1992; Resnik 1993a; Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2) and (nounl, prep, noun2), like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples. For example, Resnik (1993a) proposes the use of the selectional association measure calculated based on such triples, as described in Section 2. More specifically, his method compares maxclassi~noun2 A(Classi [ verb, prep) and maxclassi~no,m2 A(Classi I nounl,prep) to make disambiguation decisions. The third approach (Brill and Resnik 1994; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995) re"
J98-2002,J93-2002,0,0.12329,"the latter issue. The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * C&C Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{|ihang,abe}@ccm.cl.nec.co.jp @ 1998Associationfor Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994;"
J98-2002,J95-4004,0,0.0149198,"Missing"
J98-2002,C94-2195,0,0.128629,"ng, Luo, and Su 1992; Resnik 1993a; Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2) and (nounl, prep, noun2), like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples. For example, Resnik (1993a) proposes the use of the selectional association measure calculated based on such triples, as described in Section 2. More specifically, his method compares maxclassi~noun2 A(Classi [ verb, prep) and maxclassi~no,m2 A(Classi I nounl,prep) to make disambiguation decisions. The third approach (Brill and Resnik 1994; Ratnaparkhi, Reynar, and Roukos 1994; Collins and Brooks 1995) receives quadruples (verb, noun1, prep, noun2) and labels indicating which way the PP-attachment goes, like those in Table 11, and learns a disambiguation rule for resolving PP-attachment ambiguities. For example, Brill and Resnik, (1994) propose a method they call transformation-based error-driven learning (see also Brill [1995]). Their method first learns IF-THEN type rules, where the IF parts represent conditions like (prep is with) and (verb is see), and the THEN parts represent transformations from (attach to verb) to (attac"
J98-2002,A97-1052,0,0.0696611,"pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * C&C Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{|ihang,abe}@ccm.cl.nec.co.jp @ 1998Associationfor Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1"
J98-2002,J92-4003,0,0.0405878,"Missing"
J98-2002,W94-0208,0,0.0107139,"ity model that specifies the conditional probability P(n I v, r) for each n in the set of nouns .M = {nl, n2. . . . . nN}, V in the set of verbs V = {vl, v2. . . . . Vv}, and r in the set of slot names T~ = {rl, r2. . . . . rR}, satisfying: P(n Iv, r) = 1. (1) nGM This type of probability model is often referred to as a word-based model. Since the number of probability parameters in word-based models is large (O(N. V. R)), accurate 1 Recently, MDL and related techniques have become popular in corpus-based natural language processing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke and Omohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright 1996; Grunwald 1996). In this paper, we introduce MDL into the context of case frame pattern acquisition. 218 Li and Abe Generalizing Case Frames Table 1 Example (verb, slot_name, slot_value) triple data. verb slot_name slot_value fly fly fly fly fly fly fly fly fly fly argl argl argl argl argl argl argl argl argl argl bee bird bird crow bird eagle bee eagle bird crow &quot;Freq.&quot; - - swallow crow eagle bird bug bee insect Figure 1 Frequency data for the subject slot of verb fly. estimation"
J98-2002,C92-2099,0,0.0149218,"ssue, and refer the interested reader to Li and Abe (1996), which deals with the latter issue. The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * C&C Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{|ihang,abe}@ccm.cl.nec.co.jp @ 1998Associationfor Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the for"
J98-2002,C94-2119,0,0.063501,"solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * C&C Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{|ihang,abe}@ccm.cl.nec.co.jp @ 1998Associationfor Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based on the Minimum Description Length principle (MDL): a principle of data compression and statistical estimation from information theory. 1 In order to assist with efficiency,"
J98-2002,P92-1023,0,0.0437824,"Missing"
J98-2002,C96-2190,0,0.0252061,"Missing"
J98-2002,A97-1055,0,0.019832,"Missing"
J98-2002,P94-1038,0,0.00924096,"Missing"
J98-2002,P91-1030,0,0.373201,"rselves to the former issue, and refer the interested reader to Li and Abe (1996), which deals with the latter issue. The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * C&C Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{|ihang,abe}@ccm.cl.nec.co.jp @ 1998Associationfor Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use o"
J98-2002,J93-1005,0,0.250353,"Missing"
J98-2002,C90-3029,0,0.015361,"One can see that a significant amount of generalization is performed by our method--the resulting tree cut is about 5 levels higher than the starting cut, on the average. 4.2 Experiment 2: PP-Attachment Disambiguation Case frame patterns obtained by our method can be used in various tasks in natural language processing. In this paper, we test its effectiveness in a structural (PPattachment) disambiguation experiment. Disambiguation Methods. It has been empirically verified that the use of lexical semantic knowledge is effective in structural disambiguation, such as the PP-attachment problem (Hobbs and Bear 1990; Whittemore, Ferrara, and Brunner 1990). There have been 232 Li and Abe Generalizing Case Frames Table 7 Examples of generalization results. Class Probability Example Words Direct Object of eat (food,nutrient) (life_form,organism,being,living_thing) /measure,quantity,amount,quantum) (artifact,article,artefact) 0.39 0.11 0.10 0.08 pizza, egg lobster, horse amount of as if eat rope 0.30 0.10 0.07 0.05 computer, painting stock, share company, bank security, ticket 0.35 0.28 0.08 airplane, flag, executive mile delegation 0.13 0.13 0.12 0.11 0.06 company, fleet flight, operation center service, un"
J98-2002,C96-1004,1,0.845436,"tion to this problem would have a great impact on various tasks in natural language processing, including the structural disambiguation problem in parsing. The acquired knowledge would also be helpful for building a lexicon, as it would provide lexicographers with word usage descriptions. In our view, the problem of acquiring case frame patterns involves the following two issues: (a) acquiring patterns of individual case frame slots; and (b) learning dependencies that may exist between different slots. In this paper, we confine ourselves to the former issue, and refer the interested reader to Li and Abe (1996), which deals with the latter issue. The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumot"
J98-2002,J93-2004,0,0.0495194,"Missing"
J98-2002,C92-2107,0,0.0225412,"r example, Rissanen (1995) has devised an algorithm for learning decision trees. 8 Consider, for example, the case w h e n the co-occurrence data is given as f ( s w a l l o w ) = 2,f(crow) = 2,f(eagle) = 2,f(bird) = 2 for the problem in Section 2. 228 Li and Abe Generalizing Case Frames like our system to judge that the class BIRD and the noun bee can be the subject slot of the verb fly. The problem of deciding whether to stop generalizing at BIRD and bee, or generalizing further to ANIMAL has been addressed by a number of authors (Webster and Marcus 1989; Velardi, Pazienza, and Fasolo 1991; Nomiyama 1992). Minimization of the total description length provides a disciplined criterion to do this. A remarkable fact about MDL is that theoretical findings have indeed verified that MDL, as an estimation strategy, is near optimal in terms of the rate of convergence of its estimated models to the true model as data size increases. When the true model is included in the class of models considered, the models selected by MDL converge to the true model at the rate of O/~C:~9~_i~!~ 2.1Sl J&apos; where k* is the number of parameters in the true model, and [S] the data size, which is near optimal (Barron and Cov"
J98-2002,P93-1024,0,0.306125,"Missing"
J98-2002,H94-1048,0,0.0917121,"Missing"
J98-2002,H93-1054,0,0.0704552,"umoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * C&C Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{|ihang,abe}@ccm.cl.nec.co.jp @ 1998Associationfor Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generali"
J98-2002,P95-1030,0,0.00993138,"n in the set of nouns .M = {nl, n2. . . . . nN}, V in the set of verbs V = {vl, v2. . . . . Vv}, and r in the set of slot names T~ = {rl, r2. . . . . rR}, satisfying: P(n Iv, r) = 1. (1) nGM This type of probability model is often referred to as a word-based model. Since the number of probability parameters in word-based models is large (O(N. V. R)), accurate 1 Recently, MDL and related techniques have become popular in corpus-based natural language processing and other related fields (Ellison 1991, 1992; Cartwright and Brent 1994; Stolcke and Omohundro 1994; Brent, Murthy, and Lundberg 1995; Ristad and Thomas 1995; Brent and Cartwright 1996; Grunwald 1996). In this paper, we introduce MDL into the context of case frame pattern acquisition. 218 Li and Abe Generalizing Case Frames Table 1 Example (verb, slot_name, slot_value) triple data. verb slot_name slot_value fly fly fly fly fly fly fly fly fly fly argl argl argl argl argl argl argl argl argl argl bee bird bird crow bird eagle bee eagle bird crow &quot;Freq.&quot; - - swallow crow eagle bird bug bee insect Figure 1 Frequency data for the subject slot of verb fly. estimation of a word-based model is difficult with the data size that is available in practice--a"
J98-2002,A92-1014,0,0.033702,"at the former probability is significantly larger, 233 Computational Linguistics Volume 24, Number 2 Table 9 Example input data as doubles. see in see with girl with man with Table 10 Example input data as triples. see in park see with telescope girl with scarf see with friend man with hat Table 11 Example input data as quadruples and labels. see girl in park see man with telescope see girl with scarf ADV ADV ADN then the prepositional phrase is attached to verb, if the latter probability is significantly larger, it is attached to nounl, and otherwise no decision is made. The second approach (Sekine et al. 1992; Chang, Luo, and Su 1992; Resnik 1993a; Grishman and Sterling 1994; Alshawi and Carter 1994) takes triples (verb, prep, noun2) and (nounl, prep, noun2), like those in Table 10, as training data for acquiring semantic knowledge and performs PP-attachment disambiguation on quadruples. For example, Resnik (1993a) proposes the use of the selectional association measure calculated based on such triples, as described in Section 2. More specifically, his method compares maxclassi~noun2 A(Classi [ verb, prep) and maxclassi~no,m2 A(Classi I nounl,prep) to make disambiguation decisions. The third appro"
J98-2002,J93-1007,0,0.0182384,"ssue. The case frame (case slot) pattern acquisition process consists of two phases: extraction of case frame instances from corpus data, and generalization of those instances to case frame patterns. The generalization step is needed in order to represent the input case frame instances more compactly as well as to judge the (degree of) acceptability of unseen case frame instances. For the extraction problem, there have been various methods proposed to date, which are quite adequate (Hindle and Rooth 1991; Grishman and Sterling 1992; Manning 1992; Utsuro, Matsumoto, and Nagao 1992; Brent 1993; Smadja 1993; Grefenstette 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * C&C Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{|ihang,abe}@ccm.cl.nec.co.jp @ 1998Associationfor Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et"
J98-2002,C94-2116,0,0.0196397,"of methods for generalizing values of a case frame slot for a verb have been * C&C Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{|ihang,abe}@ccm.cl.nec.co.jp @ 1998Associationfor Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based on the Minimum Description Length principle (MDL): a principle of data compression and statistical estimation from information theory. 1 In order to assist with efficiency, our method mak"
J98-2002,C96-2159,0,0.0152856,"tte 1994; Briscoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * C&C Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{|ihang,abe}@ccm.cl.nec.co.jp @ 1998Associationfor Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based on the Minimum Description Length pri"
J98-2002,A97-1053,0,0.0601857,"scoe and Carroll 1997). The generalization problem, in contrast, is a more challenging one and has not been solved completely. A number of methods for generalizing values of a case frame slot for a verb have been * C&C Media Res. Labs., NEC Corporation, 4-1-1 Miyazaki Miyamae-ku, Kawasaki 216, Japan. E-mail:{|ihang,abe}@ccm.cl.nec.co.jp @ 1998Associationfor Computational Linguistics Computational Linguistics Volume 24, Number 2 proposed. Some of these methods make use of prior knowledge in the form of an existing thesaurus (Resnik 1993a, 1993b; Framis 1994; Almuallim et al. 1994; Tanaka 1996; Utsuro and Matsumoto 1997), while others do not rely on any prior knowledge (Pereira, Tishby, and Lee 1993; Grishman and Sterling 1994; Tanaka 1994). In this paper, we propose a new generalization method, belonging to the first of these two categories, which is both theoretically well-motivated and computationally efficient. Specifically, we formalize the problem of generalizing values of a case frame slot for a given verb as that of estimating a conditional probability distribution over a partition of words, and propose a new generalization method based on the Minimum Description Length principle (MDL): a principle of"
J98-2002,C92-2088,0,0.056792,"Missing"
J98-2002,J91-2002,0,0.0211412,"Missing"
J98-2002,P89-1022,0,0.0486963,"Missing"
J98-2002,P90-1004,0,0.0364766,"Missing"
J98-2002,C92-2070,0,0.0110375,"for each class (subset) C in that partition, such that P(CIv, r) = 1. (3) CEF Within a given class C, it is assumed that each n o u n is generated with equal probability, namely 1 Vn E C: P(n l v, r) = ~ x P(C I v, F). (4) Here, we assume that a word belongs to a single class. In practice, however, m a n y words have sense ambiguity and a word can belong to several different classes, e.g., bird is a member of both BIRD and MEAT. Thorough treatment of this problem is beyond the scope of the present paper; we simply note that one can employ an existing word-sense disambiguation technique (e.g.,Yarowsky 1992, 1994) in preprocessing, and use the disambiguated word senses as virtual words in the following 220 Li and Abe Generalizing Case Frames ANIMAL BIRD swallow crow INSECT eagle bird bug bee insect Figure 3 An example thesaurus. case-pattern acquisition process. It is also possible to extend our model so that each w o r d probabilistically belongs to several different classes, which w o u l d allow us to resolve both structural and word-sense ambiguities at the time of disambiguation. 2 Employing probabilistic membership, however, w o u l d make the estimation process significantly more computat"
J98-2002,P94-1013,0,0.0177904,"Missing"
J98-2002,J99-2007,1,\N,Missing
J98-2002,C94-1006,0,\N,Missing
J98-2002,P93-1022,0,\N,Missing
J98-2002,P93-1032,0,\N,Missing
J99-2007,W95-0103,0,0.0202069,"(person) )(from (place/)(to (place))) (company))(from (place))(to (place))) 3 2 1 1 1 1 that the slot of from and that of to should be considered dependent and the attachment site of one of the prepositional phrases (case slots) can be determined by that of the other with high accuracy and confidence. There has not been a general method proposed to date, however, that learns dependencies between case slots. Methods of resolving ambiguities have been based, for example, on the assumption that case slots are mutually independent (Hindle and Rooth 1991), or at most two case slots are dependent (Collins and Brooks 1995). In this article, we propose an efficient and general method of learning dependencies between case frame slots. 2. Learning Method Suppose that we have frequency data of the type shown in Table 1 automatically extracted from a corpus, in which words in the slots are replaced by the classes they belong to. We assume that case frame instances with a given verb are generated by a discrete joint probability distribution of the form Py(Xl, x2 . . . . . xn), where Py stands for the verb, and each of the random variables Xi, i = 1, 2 . . . . , n, represents a case slot. We then formulate the depende"
J99-2007,P91-1030,0,0.44531,"g2 (airplane))) (person))(arg2 (company))) (person))(to (place/)) (person) )(from (place/)(to (place))) (company))(from (place))(to (place))) 3 2 1 1 1 1 that the slot of from and that of to should be considered dependent and the attachment site of one of the prepositional phrases (case slots) can be determined by that of the other with high accuracy and confidence. There has not been a general method proposed to date, however, that learns dependencies between case slots. Methods of resolving ambiguities have been based, for example, on the assumption that case slots are mutually independent (Hindle and Rooth 1991), or at most two case slots are dependent (Collins and Brooks 1995). In this article, we propose an efficient and general method of learning dependencies between case frame slots. 2. Learning Method Suppose that we have frequency data of the type shown in Table 1 automatically extracted from a corpus, in which words in the slots are replaced by the classes they belong to. We assume that case frame instances with a given verb are generated by a discrete joint probability distribution of the form Py(Xl, x2 . . . . . xn), where Py stands for the verb, and each of the random variables Xi, i = 1, 2"
J99-2007,J98-2002,1,0.811428,"small. This is probably due to the fact that the verbs for which dependencies are detected are those for which the amount of training data is sufficient (relative to the inherent difficulty of disambiguation for that verb), and hence that are easy to disambiguate. 3.3 Experiment 3: Class-based Model We also used the proposed method to acquire case frame patterns as class-based dependency forest models, using the 354 verbs in Experiment 1. As before, we considered only the 12 most frequent slots. We generalized the values of the case slots within these case frames using the method proposed in Li and Abe (1998) to obtain class-based case frame data. We then used these data as input to the learning algorithm. The results were rather discouraging: very few case slots were determined to be dependent in the case frame patterns. To be more precise, there were on the average only 64/354 - 0.2 dependency links found for each verb. This is because the number of parameters in a class-based model was large as compared to the size of the data we had available. These experimental results seem to justify the commonly made assumption that class-based case slots, and hence word-based case slots, are mutually indep"
N16-1113,P11-2037,0,0.0300195,"Missing"
N16-1113,D13-1135,0,0.169061,"Missing"
N16-1113,D10-1062,0,0.769955,"hat there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. After building the training data for DP generation, we apply a supervised approach to build our"
N16-1113,P07-2045,0,0.0104744,"ing the approach described in Section 2.1. There are two different language models for the DP annotation (Section 2.1) and translation tasks, respectively: one is trained on the 2.13TB Chinese Web Page Collection Corpus5 while the other one is trained on all extracted 7M English subtitle data (Wang et al., 2016). Corpus Lang. Sentents Train Dev Test ZH EN ZH EN ZH EN 1,037,292 1,037,292 1,086 1,086 1,154 1,154 Pronouns 604,896 816,610 756 1,025 762 958 Ave. Len. 5.91 7.87 6.13 8.46 5.81 8.17 Table 3: Statistics of corpora. We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on a Chinese–English dialogue translation task. Furthermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002). To obtain a good word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with another larger parallel subtitle corpus that contains 6M sentence pairs.6 We use minimum error rate training (Och, 2003) to optimize the feature weights. The RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010). We use a pre-trained word embedding via a lookup table. We use the following settings: windows = 5,"
N16-1113,D10-1086,0,0.132,"Missing"
N16-1113,W10-1737,0,0.380169,"Missing"
N16-1113,D09-1106,1,0.87907,"Missing"
N16-1113,P13-2064,1,0.842545,"missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The difference to our task is that ZP cont"
N16-1113,C14-1003,0,0.0608436,"Missing"
N16-1113,J03-1002,0,0.0251063,"on. Related work is described in Section 3. The experimental results for both the DP generator and translation are reported in Section 4. Section 5 analyses some real examples which is followed by our conclusion in Section 6. 2 Methodology The architecture of our proposed method is shown in Figure 2, which can be divided into three phases: DP corpus annotation, DP generation, and SMT integration. 2.1 DP Training Corpus Annotation We propose an approach to automatically annotate DPs by utilizing alignment information. Given a parallel corpus, we first use an unsupervised word alignment method (Och and Ney, 2003; Tu et al., 2012) to produce a word alignment. From observing of the alignment matrix, we found it is possible to detect DPs by projecting misaligned pronouns from the non-pro-drop target side (English) to the pro-drop source side (Chinese). In this work, we focus on nominative and accusative pronouns including personal, possessive and reflexive instances, as listed in Table 1. Figure 2: Architecture of proposed method. Category Subjective Personal Objective Personal Possessive Objective Possessive Reflexive Pronouns 我 (I), 我们 (we), 你/你们 (you), 他 (he), 她 (she), 它 (it), 他们/她们/它 们 (they). 我 (me"
N16-1113,P03-1021,0,0.094528,"37,292 1,037,292 1,086 1,086 1,154 1,154 Pronouns 604,896 816,610 756 1,025 762 958 Ave. Len. 5.91 7.87 6.13 8.46 5.81 8.17 Table 3: Statistics of corpora. We carry out our experiments using the phrasebased SMT model in Moses (Koehn et al., 2007) on a Chinese–English dialogue translation task. Furthermore, we train 5-gram language models using the SRI Language Toolkit (Stolcke, 2002). To obtain a good word alignment, we run GIZA++ (Och and Ney, 2003) on the training data together with another larger parallel subtitle corpus that contains 6M sentence pairs.6 We use minimum error rate training (Och, 2003) to optimize the feature weights. The RNN models are implemented using the common Theano neural network toolkit (Bergstra et al., 2010). We use a pre-trained word embedding via a lookup table. We use the following settings: windows = 5, the size of the single hidden layer = 200, iterations = 10, embeddings = 200. The MLP classifier use random initialized embeddings, with the following settings: the size of the single hidden layer = 200, embeddings = 100, iterations = 200. For end-to-end evaluation, case-insensitive BLEU (Papineni et al., 2002) is used to measure 5 Available at http://www.sogou"
N16-1113,P02-1040,0,0.0971952,"on, we pre-process the input sentences by inserting possible DPs via the DP generation model. This makes the input sentences more consistent with the additional pronoun-complete rule table. To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007). To validate the effect of the proposed approach, we carried out experiments on a Chinese–English translation task. Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted corpus. Working together with DP-generated input sentences achieves a further improvement of nearly 1.0 984 BLEU point. Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points). Generally, the contributions of this paper include the following: • We propose an automatic method to build a large-scale DP training corpus. Given that the DPs are annotated in the parallel corpus, models trained on this data are more appropriate to the translation task; • Benefit"
N16-1113,W12-4501,0,0.0351916,"Missing"
N16-1113,N07-1029,0,0.0127834,"anslation of missing pronouns by explicitly recalling DPs for both parallel data and monolingual input sentences. More specifically, we extract an additional rule table from the DP-inserted parallel corpus to produce a “pronoun-complete” translation model. In addition, we pre-process the input sentences by inserting possible DPs via the DP generation model. This makes the input sentences more consistent with the additional pronoun-complete rule table. To alleviate the propagation of DP prediction errors, we feed the translation system N -best prediction results via confusion network decoding (Rosti et al., 2007). To validate the effect of the proposed approach, we carried out experiments on a Chinese–English translation task. Experimental results on a largescale subtitle corpus show that our approach improves translation performance by 0.61 BLEU points (Papineni et al., 2002) using the additional translation model trained on the DP-inserted corpus. Working together with DP-generated input sentences achieves a further improvement of nearly 1.0 984 BLEU point. Furthermore, translation performance with N -best integration is much better than its 1-best counterpart (i.e. +0.84 BLEU points). Generally, th"
N16-1113,W12-4213,0,0.348159,"Missing"
N16-1113,C10-1123,1,0.824671,"et language, so that the possibly missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The di"
N16-1113,I11-1145,1,0.817536,"hat the possibly missing pronouns in the translation might be recalled. This makes the input sentences and DP-inserted TM more consistent in terms of recalling DPs. 2.3.3 N-best inputs However, the above method suffers from a major drawback: it only uses the 1-best prediction result for decoding, which potentially introduces translation mistakes due to the propagation of prediction errors. To alleviate this problem, an obvious solution is to offer more alternatives. Recent studies have shown that SMT systems can benefit from widening the annotation pipeline (Liu et al., 2009; Tu et al., 2010; Tu et al., 2011; Liu et al., 2013). In the same direction, we propose to feed the decoder N -best prediction results, which allows the system to arbitrate between multiple ambiguous hypotheses from upstream processing so that the best translation can be produced. The general method is to make the input with N -best DPs into a confusion network. In our experiment, each prediction result in the N-best list is assigned a weight of 1/N . 3 Related Work There is some work related to DP generation. One is zero pronoun resolution (ZP), which is a subdirection of co-reference resolution (CR). The difference to our t"
N16-1113,C12-2122,1,0.905413,"Missing"
N16-1113,L16-1436,1,0.843791,"Missing"
N16-1113,P13-1081,0,0.692401,"se pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. After building the training data for DP generation, we apply a supervised approach to build our DP generator. We div"
N16-1113,N13-1125,0,0.0308356,"Missing"
N16-1113,C10-2158,0,0.0510881,"Missing"
N16-1113,P15-2051,0,0.709643,"consists of 1M sentence pairs extracted from movie and TV episode subtitles. We found that there are 6.5M Chinese pronouns and 9.4M English pronouns, which shows that more than 2.9 million Chinese pronouns are missing. In response to this problem, we propose to find a general and replicable way to improve translation quality. The main challenge of this research is that training data for DP generation are scarce. Most 983 Proceedings of NAACL-HLT 2016, pages 983–993, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics works either apply manual annotation (Yang et al., 2015) or use existing but small-scale resources such as the Penn Treebank (Chung and Gildea, 2010; Xiang et al., 2013). In contrast, we employ an unsupervised approach to automatically build a largescale training corpus for DP generation using alignment information from parallel corpora. The idea is that parallel corpora available in SMT can be used to project the missing pronouns from the target side (i.e. non-pro-drop language) to the source side (i.e. pro-drop language). To this end, we propose a simple but effective method: a bi-directional search algorithm with Language Model (LM) scoring. Aft"
N16-1113,zhang-etal-2014-dual,0,0.0371089,"Missing"
N16-1113,D07-1057,0,0.339977,"Missing"
P02-1024,J90-2002,0,0.064307,"ACM. We then describe in detail the methodology of constructing the ACM. The effectiveness of the ACM is evaluated on a realistic application, namely Japanese Kana-Kanji conversion. Experimental results show substantial improvements of the ACM in comparison with classical cluster models and word n-gram models at the same model size. Our analysis shows that the high-performance of the ACM lies in the asymmetry of the model. 1 Introduction The n-gram model has been widely applied in many applications such as speech recognition, machine translation, and Asian language text input [Jelinek, 1990; Brown et al., 1990; Gao et al., 2002]. It is a stochastic model, which predicts the next word (predicted word) given the previous n-1 words (conditional words) in a word sequence. 1 hangli@microsoft.com The cluster n-gram model is a variant of the word n-gram model in which similar words are classified in the same cluster. This has been demonstrated as an effective way to deal with the data sparseness problem and to reduce the memory sizes for realistic applications. Recent research [Yamamoto et al., 2001] shows that using different clusters for predicted and conditional words can lead to cluster models that ar"
P02-1024,J92-4003,0,0.924381,"ed word) given the previous n-1 words (conditional words) in a word sequence. 1 hangli@microsoft.com The cluster n-gram model is a variant of the word n-gram model in which similar words are classified in the same cluster. This has been demonstrated as an effective way to deal with the data sparseness problem and to reduce the memory sizes for realistic applications. Recent research [Yamamoto et al., 2001] shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models, which use the same clusters for both words [Brown et al., 1992]. This is the basis of the asymmetric cluster model (ACM), which will be formally defined and empirically studied in this paper. Although similar models have been used in previous studies [Goodman and Gao, 2000; Yamamoto et al., 2001], several issues have not been completely investigated. These include: (1) an effective methodology for constructing the ACM, (2) a thorough comparative study of the ACM with classical cluster models and word models when they are applied to a realistic application, and (3) an analysis of the reason why the ACM is superior. The goal of this study is to address the"
P02-1024,O01-2002,1,0.844613,"Missing"
P02-1024,P93-1024,0,0.107216,"model. This paper is organized as follows: Section 1 introduces our research topic, and then Section 2 reviews related work. Section 3 defines the ACM and describes in detail the method of model construction. Section 4 first introduces the Japanese Kana-Kanji conversion task; it then presents our main experiments and a discussion of our findings. Finally, conclusions are presented in Section 5. 2 Related Work A large amount of previous research on clustering has been focused on how to find the best clusters [Brown et al., 1992; Kneser and Ney, 1993; Yamamoto and Sagisaka, 1999; Ueberla, 1996; Pereira et al., 1993; Bellegarda et al., 1996; Bai et al., 1998]. Only small differences have been observed, however, in the performance of the different techniques for constructing clusters. In this study, we focused our research on novel techniques for using clusters – the ACM, in which different clusters are used for predicted and conditional words respectively. The discussion of the ACM in this paper is an extension of several studies below. The first similar cluster model was presented by Goodman and Gao [2000] in which the clustering techniques were combined with Stolcke’s [1998] pruning to reduce the langu"
P02-1024,W97-0309,0,0.0420057,"Missing"
P02-1024,P01-1068,0,0.387971,"ny applications such as speech recognition, machine translation, and Asian language text input [Jelinek, 1990; Brown et al., 1990; Gao et al., 2002]. It is a stochastic model, which predicts the next word (predicted word) given the previous n-1 words (conditional words) in a word sequence. 1 hangli@microsoft.com The cluster n-gram model is a variant of the word n-gram model in which similar words are classified in the same cluster. This has been demonstrated as an effective way to deal with the data sparseness problem and to reduce the memory sizes for realistic applications. Recent research [Yamamoto et al., 2001] shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models, which use the same clusters for both words [Brown et al., 1992]. This is the basis of the asymmetric cluster model (ACM), which will be formally defined and empirically studied in this paper. Although similar models have been used in previous studies [Goodman and Gao, 2000; Yamamoto et al., 2001], several issues have not been completely investigated. These include: (1) an effective methodology for constructing the ACM, (2) a thorough comparative st"
P02-1044,P91-1034,0,0.312149,"rd and which are respectively assigned with the correct Chinese translations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be"
P02-1044,J94-4003,0,0.311172,"spectively assigned with the correct Chinese translations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be not in parallel but f"
P02-1044,H92-1045,0,0.291484,"tly outperforms MB. 2 Related Work The problem of word translation disambiguation (in general, word sense disambiguation) can be viewed as that of classification and can be addressed by employing a supervised learning method. In such a learning method, for instance, an English sentence containing an ambiguous English word corresponds to an example, and the Chinese translation of the word under the context corresponds to a classification decision (a label). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best fo"
P02-1044,W99-0905,0,0.0323872,"the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be not in parallel but from the same domain. BB constructs classifiers for"
P02-1044,P99-1020,0,0.0132534,"t then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be not in parallel but from the same domain. BB constructs classifiers for English to Chinese translatio"
P02-1044,P96-1006,0,0.0293996,"sambiguation (in general, word sense disambiguation) can be viewed as that of classification and can be addressed by employing a supervised learning method. In such a learning method, for instance, an English sentence containing an ambiguous English word corresponds to an example, and the Chinese translation of the word under the context corresponds to a classification decision (a label). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Ped"
P02-1044,W97-0322,0,0.0134034,"ith the correct Chinese translations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be not in parallel but from the same domain. BB c"
P02-1044,A00-2009,0,0.224647,"l). Many methods for word sense disambiguation using a supervised learning technique have been proposed. They include those using Naïve Bayes (Gale et al. 1992a), Decision List (Yarowsky 1994), Nearest Neighbor (Ng and Lee 1996), Transformation Based Learning (Mangu and Brill 1997), Neural Network (Towell and 1 In this paper, we take English-Chinese translation as example; it is a relatively easy process, however, to extend the discussions to translations between other language pairs. Voorhess 1998), Winnow (Golding and Roth 1999), Boosting (Escudero et al. 2000), and Naïve Bayesian Ensemble (Pedersen 2000). Among these methods, the one using Naïve Bayesian Ensemble (i.e., an ensemble of Naïve Bayesian Classifiers) is reported to perform the best for word sense disambiguation with respect to a benchmark data set (Pedersen 2000). The assumption behind the proposed methods is that it is nearly always possible to determine the translation of a word by referring to its context, and thus all of the methods actually manage to build a classifier (i.e., a classification program) using features representing context information (e.g., co-occurring words). Since preparing supervised learning data is expens"
P02-1044,J98-1004,0,0.0625172,"ranslations of the word. It then uses the classified sentences as training data to learn a classifier (e.g., a decision list) and uses the constructed classifier to classify some unclassified sentences containing the ambiguous word as additional training data. It also adopts the heuristics of ‘one sense per discourse’ (Gale et al. 1992b) to further classify unclassified sentences. By repeating the above processes, it can create an accurate classifier for word translation disambiguation. For other related work, see, for example, (Brown et al. 1991; Dagan and Itai 1994; Pedersen and Bruce 1997; Schutze 1998; Kikui 1999; Mihalcea and Moldovan 1999). 3 Bilingual Bootstrapping 3.1 Overview Instead of using Monolingual Bootstrapping, we propose a new method for word translation disambiguation using Bilingual Bootstrapping. In translation from English to Chinese, for instance, BB makes use of not only unclassified data in English, but also unclassified data in Chinese. It also uses a small number of classified data in English and, optionally, a small number of classified data in Chinese. The data in English and in Chinese are supposed to be not in parallel but from the same domain. BB constructs clas"
P02-1044,J98-1005,0,0.107655,"Missing"
P02-1044,P94-1013,0,0.0827766,"Missing"
P02-1044,P95-1026,0,0.791721,"translation disambiguation. For instance, we are concerned with an ambiguous word in English (e.g., ‘plant’), which has multiple translations in Chinese (e.g., ‘ (gongchang)’ and ‘ (zhiwu)’). Our goal is to determine the correct Chinese translation of the ambiguous English word, given an English sentence which contains the word. Word translation disambiguation is actually a special case of word sense disambiguation (in the example above, ‘gongchang’ corresponds to the Ꮉॖ ỡ⠽ Hang Li Microsoft Research Asia 5F Sigma Center, No.49 Zhichun Road, Haidian Beijing, China, 100080 hangli@microsoft.com Yarowsky (1995) proposes a method for word sense (translation) disambiguation that is based on a bootstrapping technique, which we refer to here as ‘Monolingual Bootstrapping (MB)’. In this paper, we propose a new method for word translation disambiguation using a bootstrapping technique we have developed. We refer to the technique as ‘Bilingual Bootstrapping (BB)’. In order to evaluate the performance of BB, we conducted some experiments on word translation disambiguation using the BB technique and the MB technique. All of the results indicate that BB consistently and significantly outperforms MB. 2 Related"
P02-1044,W01-0501,0,\N,Missing
P02-1044,P94-1020,0,\N,Missing
P02-1044,P97-1009,0,\N,Missing
P02-1044,P01-1005,0,\N,Missing
P02-1044,W99-0613,0,\N,Missing
P02-1044,O01-2001,0,\N,Missing
P03-1042,W99-0613,0,0.0334017,"Missing"
P03-1042,P02-1044,1,0.924281,"the degree of uncertainty correlation of the two classifiers in collaborative bootstrapping and uses the measure in analysis of collaborative bootstrapping. Furthermore, it proposes a new algorithm of collaborative bootstrapping on the basis of uncertainty reduction. Experimental results have verified the correctness of the analysis and have demonstrated the significance of the new algorithm. 1 Introduction We consider here the problem of collaborative bootstrapping. It includes co-training (Blum and Mitchell, 1998; Collins and Singer, 1998; Nigam and Ghani, 2000) and bilingual bootstrapping (Li and Li, 2002). Collaborative bootstrapping begins with a small number of labelled data and a large number of unlabelled data. It trains two (types of) classifiers from the labelled data, uses the two classifiers to label some unlabelled data, trains again two new classifiers from all the labelled data, and repeats the above process. During the process, the two Li Lian Computer Science Department Fudan University No. 220 Handan Road Shanghai, China, 200433 leelix@yahoo.com classifiers help each other by exchanging the labelled data. In co-training, the two classifiers have different feature structures, and"
P03-1042,W01-0501,0,0.0146949,"ult is based on the view independence assumption, which is strict in practice. Abney (2002) refined Dasgupta et al’s result by relaxing the view independence assumption with a new constraint. He also proposed a new co-training algorithm on the basis of the constraint. Nigam and Ghani (2000) empirically demonstrated that bootstrapping with a random feature split (i.e. co-training), even violating the view independence assumption, can still work better than bootstrapping without a feature split (i.e., bootstrapping with a single classifier). For other work on co-training, see (Muslea et al 200; Pierce and Cardie 2001). Li and Li (2002) proposed an algorithm for word sense disambiguation in translation between two languages, which they called ‘bilingual bootstrapping’. Instead of making an assumption on the features, bilingual bootstrapping makes an assumption on the classes. Specifically, it assumes that the classes of the classifiers in bootstrapping do not overlap. Thus, bilingual bootstrapping is different from co-training. Because the notion of agreement is not involved in bootstrapping in (Nigam & Ghani 2000) and bilingual bootstrapping, Dasgupta et al and Abney’s analyses cannot be directly used on t"
P03-1042,P95-1026,0,0.0546948,"repeatedly trains two classifiers from the labelled data, labels some unlabelled data with the two classifiers, and exchanges the newly labelled data between the two classifiers. Blum and Mitchell assume that the two classifiers are based on two subsets of the entire feature set and the two subsets are conditionally independent with one another given a class. This assumption is called ‘view independence’. In their algorithm of co-training, one classifier always asks the other classifier to label the most certain instances for the collaborator. The word sense disambiguation method proposed in Yarowsky (1995) can also be viewed as a kind of co-training. Since the assumption of view independence cannot always be met in practice, Collins and Singer (1998) proposed a co-training algorithm based on ‘agreement’ between the classifiers. As for theoretical analysis, Dasgupta et al. (2001) gave a bound on the generalization error of co-training within the framework of PAC learning. The generalization error is a function of ‘disagreement’ between the two classifiers. Dasgupta et al’s result is based on the view independence assumption, which is strict in practice. Abney (2002) refined Dasgupta et al’s resu"
P03-1042,J04-1001,1,\N,Missing
P03-1042,P02-1046,0,\N,Missing
P07-1087,P00-1037,0,0.0497676,"nly local information but also global information in a document in case restoration. Spelling error correction can be formalized as a classification problem. Golding and Roth (1996) propose using the Winnow algorithm to address the issue. The problem can also be formalized as that of data conversion using the source channel model. The source model can be built as an n-gram language model and the channel model can be constructed with confusing words measured by edit distance. Brill and Moore, Church and Gale, and Mayes et al. have developed different techniques for confusing words calculation (Brill and Moore, 2000; Church and Gale, 1991; Mays et al., 1991). Sproat et al. (1999) have investigated normalization of non-standard words in texts, including numbers, abbreviations, dates, currency amounts, and acronyms. They propose a taxonomy of nonstandard words and apply n-gram language models, decision trees, and weighted finite-state transducers to the normalization. 3 Text Normalization In this paper we define text normalization at three levels: paragraph, sentence, and word level. The subtasks at each level are listed in Table 1. For example, at the paragraph level, there are two subtasks: extra line-br"
P07-1087,P03-1020,0,0.337923,"ils using two machine-learning based methods: Conditional Random Fields and Perceptron for learning HMMs. See also (Carvalho and Cohen, 2004). Tang et al. (2005) propose a cascaded approach for email data cleaning by employing Support Vector Machines and rules. Their method can detect email headers, signatures, program codes, and extra line breaks in emails. See also (Wong et al., 2007). Palmer and Hearst (1997) propose using a Neural Network model to determine whether a period in a sentence is the ending mark of the sentence, an abbreviation, or both. See also (Mikheev, 2000; Mikheev, 2002). Lita et al. (2003) propose employing a language modeling approach to address the case restoration problem. They define four classes for word casing: all letters in lower case, first letter in uppercase, all letters in upper case, and mixed case, and formalize the problem as assigning class labels to words in natural language texts. Mikheev (2002) proposes using not only local information but also global information in a document in case restoration. Spelling error correction can be formalized as a classification problem. Golding and Roth (1996) propose using the Winnow algorithm to address the issue. The proble"
P07-1087,J02-3002,0,0.0262953,"cognition in emails using two machine-learning based methods: Conditional Random Fields and Perceptron for learning HMMs. See also (Carvalho and Cohen, 2004). Tang et al. (2005) propose a cascaded approach for email data cleaning by employing Support Vector Machines and rules. Their method can detect email headers, signatures, program codes, and extra line breaks in emails. See also (Wong et al., 2007). Palmer and Hearst (1997) propose using a Neural Network model to determine whether a period in a sentence is the ending mark of the sentence, an abbreviation, or both. See also (Mikheev, 2000; Mikheev, 2002). Lita et al. (2003) propose employing a language modeling approach to address the case restoration problem. They define four classes for word casing: all letters in lower case, first letter in uppercase, all letters in upper case, and mixed case, and formalize the problem as assigning class labels to words in natural language texts. Mikheev (2002) proposes using not only local information but also global information in a document in case restoration. Spelling error correction can be formalized as a classification problem. Golding and Roth (1996) propose using the Winnow algorithm to address t"
P07-1087,H05-1056,0,0.0346425,"zation is usually viewed as an engineering issue and is addressed in an ad-hoc manner. Much of the previous work focuses on processing texts in clean form, not texts in informal form. Also, prior work mostly focuses on processing one type or a small number of types of errors, whereas this paper deals with many different types of errors. Clark (2003) has investigated the problem of preprocessing noisy texts for natural language processing. He proposes identifying token boundaries and sentence boundaries, restoring cases of words, and correcting misspelled words by using a source channel model. Minkov et al. (2005) have investigated the problem of named entity recognition in informally in689 putted texts. They propose improving the performance of personal name recognition in emails using two machine-learning based methods: Conditional Random Fields and Perceptron for learning HMMs. See also (Carvalho and Cohen, 2004). Tang et al. (2005) propose a cascaded approach for email data cleaning by employing Support Vector Machines and rules. Their method can detect email headers, signatures, program codes, and extra line breaks in emails. See also (Wong et al., 2007). Palmer and Hearst (1997) propose using a N"
P07-1087,J97-2002,0,\N,Missing
P11-1006,H05-1120,0,0.0230713,"date generation is only concerned with a single word. For single-word candidate generation, rule-based approach is commonly used. The use of edit distance is a typical example, which exploits operations of character deletion, insertion and substitution. Some methods generate candidates within a fixed range of edit distance or different ranges for strings with different lengths (Li et al., 2006; Whitelaw et al., 2009). Other methods make use of weighted edit distance to enhance the representation power of edit distance (Ristad and Yianilos, 1998; Oncina and Sebban, 2005; McCallum et al., 2005; Ahmad and Kondrak, 2005). Conventional edit distance does not take in consideration context information. For example, people tend to misspell “c” to “s” or “k” depending on contexts, and a straightforward application of edit distance cannot deal with the problem. To address the challenge, some researchers proposed using a large number of substitution rules containing context information (at character level). For example, Brill and Moore (2000) developed a generative model including contextual substitution rules; and Toutanova and Moore (2002) further improved the model by adding pronunciation factors into the model."
P11-1006,P00-1037,0,0.567004,"ndidate generation in spelling error correction. Candidate generation is to find the most possible corrections of a misspelled word. In such a problem, strings are words, and the operators represent insertion, deletion, and substitution of characters with or without surrounding characters, for example, “a”→“e” and “lly”→“ly”. Note that candidate generation is concerned with a single word; after candidate generation, the words surrounding it in the text can be further leveraged to make the final candidate selection, e.g., Li et al. (2006), Golding and Roth (1999). In spelling error correction, Brill and Moore (2000) proposed employing a generative model for candidate generation and a hierarchy of trie structures for fast candidate retrieval. Our approach is a discriminative approach and is aimed at improving Brill and Moore’s method. Okazaki et al. (2008) proposed using a logistic regression model for approximate dictionary matching. Their method is also a discriminative approach, but it is largely different from our approach in the following points. It formalizes the problem as binary classification and 52 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 52–"
P11-1006,D07-1019,0,0.0249006,"del. Okazaki et al.’s model is largely different from the model proposed in this paper, although both of them are discriminative models. Their model is a binary classification model and it is assumed that only a single rule is applied in candidate generation. Since users’ behavior of misspelling and correction can be frequently observed in web search log data, it has been proposed to mine spelling-error and correction pairs by using search log data. The mined pairs can be directly used in spelling error correction. Methods of selecting spelling and correction pairs with maximum entropy model (Chen et al., 2007) or similarity functions (Islam and Inkpen, 2009; Jones et al., 2006) have been developed. The mined pairs can only be used in candidate generation of high frequency typos, however. In this paper, we work on candidate generation at the character level, which can be applied to spelling error correction for both high and low frequency words. 3 Model for Candidate Generation As an example of approximate string search, we consider candidate generation in spelling correction. Suppose that there is a vocabulary V and a misspelled word, the objective of candidate generation is to select the best corr"
P11-1006,D09-1129,0,0.0334138,"rent from the model proposed in this paper, although both of them are discriminative models. Their model is a binary classification model and it is assumed that only a single rule is applied in candidate generation. Since users’ behavior of misspelling and correction can be frequently observed in web search log data, it has been proposed to mine spelling-error and correction pairs by using search log data. The mined pairs can be directly used in spelling error correction. Methods of selecting spelling and correction pairs with maximum entropy model (Chen et al., 2007) or similarity functions (Islam and Inkpen, 2009; Jones et al., 2006) have been developed. The mined pairs can only be used in candidate generation of high frequency typos, however. In this paper, we work on candidate generation at the character level, which can be applied to spelling error correction for both high and low frequency words. 3 Model for Candidate Generation As an example of approximate string search, we consider candidate generation in spelling correction. Suppose that there is a vocabulary V and a misspelled word, the objective of candidate generation is to select the best corrections from the vocabulary V. We care about bot"
P11-1006,P06-1129,0,0.0444301,"Missing"
P11-1006,J04-4003,0,0.0263459,"h as large rule sets and large vocabulary sizes. The efficiency of our method is also very high in different experimental settings. 2 Related Work Approximate string search has been studied by many researchers. Previous work mainly focused on efficiency rather than model. Usually, it is assumed that the model (similarity distance) is fixed and the goal is to efficiently find all the strings in the collection whose similarity distances are within a threshold. Most existing methods employ n-gram based algo53 rithms (Behm et al., 2009; Li et al., 2007; Yang et al., 2008) or filtering algorithms (Mihov and Schulz, 2004; Li et al., 2008). Instead of finding all the candidates in a fixed range, methods for finding the top k candidates have also been developed. For example, the method by Vernica and Li (2009) utilized n-gram based inverted lists as index structure and a similarity function based on n-gram overlaps and word frequencies. Yang et al. (2010) presented a general framework for top k retrieval based on ngrams. In contrast, our work in this paper aims to learn a ranking function which can achieve both high accuracy and efficiency. Spelling error correction normally consists of candidate generation and"
P11-1006,D08-1047,0,0.253815,"cters with or without surrounding characters, for example, “a”→“e” and “lly”→“ly”. Note that candidate generation is concerned with a single word; after candidate generation, the words surrounding it in the text can be further leveraged to make the final candidate selection, e.g., Li et al. (2006), Golding and Roth (1999). In spelling error correction, Brill and Moore (2000) proposed employing a generative model for candidate generation and a hierarchy of trie structures for fast candidate retrieval. Our approach is a discriminative approach and is aimed at improving Brill and Moore’s method. Okazaki et al. (2008) proposed using a logistic regression model for approximate dictionary matching. Their method is also a discriminative approach, but it is largely different from our approach in the following points. It formalizes the problem as binary classification and 52 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 52–61, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics assumes that there is only one rule applicable each time in candidate generation. Efficiency is also not a major concern for them, because it is for offlin"
P11-1006,P02-1019,0,0.0285783,"e (Ristad and Yianilos, 1998; Oncina and Sebban, 2005; McCallum et al., 2005; Ahmad and Kondrak, 2005). Conventional edit distance does not take in consideration context information. For example, people tend to misspell “c” to “s” or “k” depending on contexts, and a straightforward application of edit distance cannot deal with the problem. To address the challenge, some researchers proposed using a large number of substitution rules containing context information (at character level). For example, Brill and Moore (2000) developed a generative model including contextual substitution rules; and Toutanova and Moore (2002) further improved the model by adding pronunciation factors into the model. Schaback and Li (2007) proposed a multi-level feature-based framework for spelling error correction including a modification of Brill and Moore’s model (2000). Okazaki et al. (2008) utilized substring substitution rules and incorporated the rules into a L1 -regularized logistic regression model. Okazaki et al.’s model is largely different from the model proposed in this paper, although both of them are discriminative models. Their model is a binary classification model and it is assumed that only a single rule is appli"
P11-1006,D09-1093,0,0.0129184,"gh accuracy and efficiency. Spelling error correction normally consists of candidate generation and candidate final selection. The former task is an example of approximate string search. Note that candidate generation is only concerned with a single word. For single-word candidate generation, rule-based approach is commonly used. The use of edit distance is a typical example, which exploits operations of character deletion, insertion and substitution. Some methods generate candidates within a fixed range of edit distance or different ranges for strings with different lengths (Li et al., 2006; Whitelaw et al., 2009). Other methods make use of weighted edit distance to enhance the representation power of edit distance (Ristad and Yianilos, 1998; Oncina and Sebban, 2005; McCallum et al., 2005; Ahmad and Kondrak, 2005). Conventional edit distance does not take in consideration context information. For example, people tend to misspell “c” to “s” or “k” depending on contexts, and a straightforward application of edit distance cannot deal with the problem. To address the challenge, some researchers proposed using a large number of substitution rules containing context information (at character level). For exam"
P12-1047,N03-1003,0,0.206645,"natural language processing and information retrieval, which includes paraphrasing, textual entailment and transformation between query and document title in search. The key question here is how to represent the rewriting of sentences. In previous research on sentence re-writing learning such as paraphrase identification and recognizing textual entailment, most representations are based on the lexicons (Zhang and Patrick, 2005; Lintean and Rus, 2011; de Marneffe et al., 2006) or the syntactic trees (Das and Smith, 2009; Heilman and Smith, 2010) of the sentence pairs. In (Lin and Pantel, 2001; Barzilay and Lee, 2003), re-writing rules serve as underlying representations for paraphrase generation/discovery. Motivated by the work, we represent re-writing of sentences by all possible re-writing rules that can be applied into it. For example, in Fig. 1, (A) is one re-writing rule that can be applied into the sentence re-writing (B). Specifically, we propose a new class of kernel functions (Sch¨olkopf and Smola, 2002), called string rewriting kernel (SRK), which defines the similarity between two re-writings (pairs) of strings as the inner product between them in the feature space induced by all the re-writing"
P12-1047,P08-1077,0,0.0171766,"ncluding the wildcard kernel to facilitate inexact matching between the strings. The string kernels defined on two pairs of objects (including strings) were also developed, which decompose the similarity into product of similarities between individual objects using tensor product (Basilico and Hofmann, 2004; Ben-Hur and Noble, 2005) or Cartesian product (Kashima et al., 2009). The task of paraphrasing usually consists of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by t"
P12-1047,I05-5002,0,0.164065,"English Stemmer (http://www.iveonik.com/ ). 455 window sizes k. We also tried to combine the kernels with two lexical features “unigram precision and recall” proposed in (Wan et al., 2006), referred to as PR. For each kernel K, we tested the window size settings of K1 + ... + Kkmax (kmax ∈ {1, 2, 3, 4}) together with the combination with PR and we report the best accuracies of them in Tab 1 and Tab 2. 6.1 Paraphrase Identification The task of paraphrase identification is to examine whether two sentences have the same meaning. We trained and tested all the methods on the MSR Paraphrase Corpus (Dolan and Brockett, 2005; Quirk et al., 2004) consisting of 4,076 sentence pairs for training and 1,725 sentence pairs for testing. The experimental results on different SRKs are shown in Table 1. It can be seen that kb-SRK outperforms ps-SRK and pw-SRK. The results by the state-of-the-art methods reported in previous work are also included in Table 1. kb-SRK outperforms the existing lexical approach (Zhang and Patrick, 2005) and kernel approach (Lintean and Rus, 2011). It also works better than the other approaches listed in the table, which use syntactic trees or dependency relations. Fig. 7 gives detailed results"
P12-1047,W07-1401,0,0.0352189,"Missing"
P12-1047,W07-1423,0,0.106742,"9) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency la450 bels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual entailment, de Marneffe et al. (2006) classified sentences pairs on the basis of word alignments. MacCartney and Manning (2008) used an inference procedure based on natural logic and combined it with the methods by de Marneffe et al. (2006). Harmeling (2007) and Heilman and Smith (2010) classified sequence pairs based on transformation on syntactic trees. Zanzotto et al. (2007) used a kernel method on syntactic tree pairs (Moschitti and Zanzotto, 2007). 3 Kernel Approach to Sentence Re-Writing Learning We formalize sentence re-writing learning as a kernel method. Following the literature of string kernel, we use the terms “string” and “character” instead of “sentence” and “word”. Suppose that we are given training data consisting of re-writings of strings and their responses ((s1 ,t1 ), y1 ), ..., ((sn ,tn ), yn ) ∈ (Σ∗ × Σ∗ ) × Y i where Σ denot"
P12-1047,N10-1145,0,0.659008,"e. Introduction Learning for sentence re-writing is a fundamental task in natural language processing and information retrieval, which includes paraphrasing, textual entailment and transformation between query and document title in search. The key question here is how to represent the rewriting of sentences. In previous research on sentence re-writing learning such as paraphrase identification and recognizing textual entailment, most representations are based on the lexicons (Zhang and Patrick, 2005; Lintean and Rus, 2011; de Marneffe et al., 2006) or the syntactic trees (Das and Smith, 2009; Heilman and Smith, 2010) of the sentence pairs. In (Lin and Pantel, 2001; Barzilay and Lee, 2003), re-writing rules serve as underlying representations for paraphrase generation/discovery. Motivated by the work, we represent re-writing of sentences by all possible re-writing rules that can be applied into it. For example, in Fig. 1, (A) is one re-writing rule that can be applied into the sentence re-writing (B). Specifically, we propose a new class of kernel functions (Sch¨olkopf and Smola, 2002), called string rewriting kernel (SRK), which defines the similarity between two re-writings (pairs) of strings as the inne"
P12-1047,C08-1066,0,0.0153732,"cation task. Among the most successful methods, Wan et al. (2006) enriched the feature set by the BLEU metric and dependency relations. Das and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency la450 bels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual entailment, de Marneffe et al. (2006) classified sentences pairs on the basis of word alignments. MacCartney and Manning (2008) used an inference procedure based on natural logic and combined it with the methods by de Marneffe et al. (2006). Harmeling (2007) and Heilman and Smith (2010) classified sequence pairs based on transformation on syntactic trees. Zanzotto et al. (2007) used a kernel method on syntactic tree pairs (Moschitti and Zanzotto, 2007). 3 Kernel Approach to Sentence Re-Writing Learning We formalize sentence re-writing learning as a kernel method. Following the literature of string kernel, we use the terms “string” and “character” instead of “sentence” and “word”. Suppose that we are given training dat"
P12-1047,W06-1603,0,0.011707,"of paraphrase pattern generation and paraphrase identification. Paraphrase pattern generation is to automatically extract semantically equivalent patterns (Lin and Pantel, 2001; Bhagat and Ravichandran, 2008) or sentences (Barzilay and Lee, 2003). Paraphrase identification is to identify whether two given sentences are a paraphrase of each other. The methods proposed so far formalized the problem as classification and used various types of features such as bag-of-words feature, edit distance (Zhang and Patrick, 2005), dissimilarity kernel (Lintean and Rus, 2011) predicate-argument structure (Qiu et al., 2006), and tree edit model (which is based on a tree kernel) (Heilman and Smith, 2010) in the classification task. Among the most successful methods, Wan et al. (2006) enriched the feature set by the BLEU metric and dependency relations. Das and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency la450 bels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007). In recognizing textual e"
P12-1047,W04-3219,0,0.0541647,"Missing"
P12-1047,U06-1019,0,0.733099,"Missing"
P12-1047,W07-1412,0,0.4284,"Missing"
P12-1047,U05-1023,0,0.130747,"ritten by Shakespeare. (B) Figure 1: Example of re-writing. (A) is a re-writing rule and (B) is a re-writing of sentence. Introduction Learning for sentence re-writing is a fundamental task in natural language processing and information retrieval, which includes paraphrasing, textual entailment and transformation between query and document title in search. The key question here is how to represent the rewriting of sentences. In previous research on sentence re-writing learning such as paraphrase identification and recognizing textual entailment, most representations are based on the lexicons (Zhang and Patrick, 2005; Lintean and Rus, 2011; de Marneffe et al., 2006) or the syntactic trees (Das and Smith, 2009; Heilman and Smith, 2010) of the sentence pairs. In (Lin and Pantel, 2001; Barzilay and Lee, 2003), re-writing rules serve as underlying representations for paraphrase generation/discovery. Motivated by the work, we represent re-writing of sentences by all possible re-writing rules that can be applied into it. For example, in Fig. 1, (A) is one re-writing rule that can be applied into the sentence re-writing (B). Specifically, we propose a new class of kernel functions (Sch¨olkopf and Smola, 2002), c"
P12-1047,P09-1053,0,\N,Missing
P12-2037,P05-1074,0,0.0600309,"Missing"
P12-2037,P01-1008,0,0.0138383,"o Seattle”) S2 = {Seattle}:(“how far is it from Boston to X1 ” ,“distance from Boston to X1 ”) S3 = {Boston, Seattle}:(“how far is it from X1 to X2 ” ,“distance from X1 to X2 ”) P= { (p,pr)} Generating Reformulation Patterns Pattern Base O nline Phase New Question new q Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction"
P12-2037,D07-1086,0,0.0340618,"et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to automatically mine 5w1h question reformulation patterns from query logs. Recently, query reformulation (Boldi et al., 2009; Jansen et al., 2009) has been studied in web search. Different techniques have been developed for query segmentation (Bergsma and Wang, 2007; Tan and Peng, 2008) and query substitution (Jones et al., 2006; Wang and Zhai, 2008). Yet, most previous research focused on keyword queries without considering 5w1h questions. 3 Mining Question Reformulation Patterns for Web Search Our framework consists of three major components, which is illustrated in Fig. 1. 188 Generating Reformulation Patterns From the search log, we extract all successive query pairs issued by the same user within a certain time period where the first query is a 5w1h question. In such query pair, the second query is considered as a question reformulation. Our method"
P12-2037,P08-1077,0,0.0143149,"on, Seattle}:(“how far is it from X1 to X2 ” ,“distance from X1 to X2 ”) P= { (p,pr)} Generating Reformulation Patterns Pattern Base O nline Phase New Question new q Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). L"
P12-2037,N06-1003,0,0.016163,"lations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to automatically mine 5w1h question reformulation patterns from query logs. Recently, query reformulation (Boldi et al., 2009; Jansen et al., 2009) has been s"
P12-2037,D08-1021,0,0.0149006,"om X1 to X2 ” ,“distance from X1 to X2 ”) P= { (p,pr)} Generating Reformulation Patterns Pattern Base O nline Phase New Question new q Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has bee"
P12-2037,N06-1058,0,0.0229005,"Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to automatically mine 5w1h question reformulation patterns from query logs. Recently, query reformulation (Boldi et al., 2009; J"
P12-2037,N03-1024,0,0.0107922,"“how far is it from Boston to X1 ” ,“distance from Boston to X1 ”) S3 = {Boston, Seattle}:(“how far is it from X1 to X2 ” ,“distance from X1 to X2 ”) P= { (p,pr)} Generating Reformulation Patterns Pattern Base O nline Phase New Question new q Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Dur"
P12-2037,I05-1011,0,0.0229,"Missing"
P12-2037,P08-1003,0,0.0544106,"Missing"
P12-2037,P02-1006,0,0.0468385,"Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to automatically mine 5w1h question reformulation patterns from query logs. Recently, query reformulation (Boldi et al., 2009; Jansen et al., 2009) has been studied in web search. Different techniques have be"
P12-2037,P08-1089,0,0.0156806,"ce from X1 to X2 ”) P= { (p,pr)} Generating Reformulation Patterns Pattern Base O nline Phase New Question new q Generating Question Reformulations Question Reformulation { qrnew} Retrieval Model Retrieved Documents { D } 3.1 Figure 1: The framework of reformulating questions. 2 Related Work In the Natural Language Processing (NLP) area, different expressions that convey the same meaning are referred as paraphrases (Lin and Pantel, 2001; Barzilay and McKeown, 2001; Pang et al., 2003; Pas¸ca and Dienes, 2005; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to autom"
P12-2037,C10-1148,0,0.0537288,"; Bannard and CallisonBurch, 2005; Bhagat and Ravichandran, 2008; Callison-Burch, 2008; Zhao et al., 2008). Paraphrases have been studied in a variety of NLP applications such as machine translation (Kauchak and Barzilay, 2006; Callison-Burch et al., 2006), question answering (Ravichandran and Hovy, 2002) and document summarization (McKeown et al., 2002). Yet, little research has considered improving web search performance using paraphrases. Query logs have become an important resource for many NLP applications such as class and attribute extraction (Pas¸ca and Van Durme, 2008), paraphrasing (Zhao et al., 2010) and language modeling (Huang et al., 2010). Little research has been conducted to automatically mine 5w1h question reformulation patterns from query logs. Recently, query reformulation (Boldi et al., 2009; Jansen et al., 2009) has been studied in web search. Different techniques have been developed for query segmentation (Bergsma and Wang, 2007; Tan and Peng, 2008) and query substitution (Jones et al., 2006; Wang and Zhai, 2008). Yet, most previous research focused on keyword queries without considering 5w1h questions. 3 Mining Question Reformulation Patterns for Web Search Our framework cons"
P15-1003,P14-1062,0,0.207783,"Missing"
P15-1003,J07-2003,0,0.064903,"Missing"
P15-1003,D14-1179,0,0.0842528,"Missing"
P15-1003,P05-1066,0,0.106045,"Missing"
P15-1003,P14-1129,0,0.0614801,"Missing"
P15-1003,N04-1035,0,0.0736712,"Missing"
P15-1003,P07-1019,0,0.0144132,"Missing"
P15-1003,D13-1176,0,0.131209,"Missing"
P15-1003,N03-1017,0,0.00644014,"Missing"
P15-1003,P07-2045,0,0.0132544,"Missing"
P15-1003,D13-1108,1,0.848069,"Missing"
P15-1003,P02-1038,0,0.428481,"Missing"
P15-1003,J03-1002,0,0.00890853,"Missing"
P15-1003,P03-1021,0,0.072223,"Missing"
P15-1003,P08-1066,0,0.0292075,"Missing"
P15-1003,D11-1020,1,0.715388,"Missing"
P15-1003,D13-1106,0,\N,Missing
P15-1151,D13-1106,0,0.0780662,"Missing"
P15-1151,D11-1033,0,0.0341918,"Missing"
P15-1151,J92-1002,0,0.559939,"Missing"
P15-1151,P96-1041,0,0.351522,"Missing"
P15-1151,P14-1129,0,0.0225834,"Missing"
P15-1151,D13-1176,0,0.0950312,"Missing"
P15-1151,P14-1062,0,0.0830922,"Missing"
P15-1151,P02-1038,0,0.155749,"Missing"
P15-1151,D13-1140,0,0.0268192,"Missing"
P15-1152,D13-1106,0,0.00434304,"od’s sake, it is 11 in the morning Enhhhh... sounds yummy which restaurant exactly? For god&apos;s sake, it is 11 in the morning Enhhhh... sounds yummy which restaurant exactly? Decoder vector Encoder Having my fish sandwich right now Figure 1: The diagram of encoder-decoder framework for automatic response generation. NRM essentially estimates the likelihood of a response given a post. Clearly the estimated probability should be complex enough to represent all the suitable responses. Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Note that in machine trans2 The Dataset for STC Our models are trained on a corpus of roughly 4.4 million pairs of conversations from Weibo 3 . 2.1 Conversations on Sina Weibo Weibo is a popular Twitter-like microblogging service in China, on which a user can post short messages (referred to as post in the reminder of this paper) visible to the public or a group of users following her/him. Other users make comment on a published post, which will be referred to as a response. Just like Twitter, Weibo also has the length limit of 140 Chinese char"
P15-1152,J03-1002,0,0.0168354,"taset. In comparison to NRM, only the top one response is considered in the evaluation process. SMT-based: In SMT-based models, the postresponse pairs are directly used as parallel data for training a translation model. We use the most widely used open-source phrase-based translation model-Moses (Koehn et al., 2007). Another parallel data consisting of 3000 post-response pairs is used to tune the system. In (Ritter et al., 2011), the authors used a modified SMT model to obtain the “Response” of Twitter “Stimulus”. The main modification is in replacing the standard GIZA++ word alignment model (Och and Ney, 2003) with a new phrase-pair selection method, in which all the 6 we use the default similarity function of Lucene 7 possible phrase-pairs in the training data are considered and their associated probabilities are estimated by the Fisher’s Exact Test, which yields performance slightly better than default setting8 . Compared to retrieval-based methods, the generated responses by SMT-based methods often have fluency or even grammatical problems. In this work, we choose the Moses with default settings as our SMT model. 5 Results and Analysis Automatic evaluation of response generation is still an open"
P15-1152,P02-1040,0,0.109003,"milarity function of Lucene 7 possible phrase-pairs in the training data are considered and their associated probabilities are estimated by the Fisher’s Exact Test, which yields performance slightly better than default setting8 . Compared to retrieval-based methods, the generated responses by SMT-based methods often have fluency or even grammatical problems. In this work, we choose the Moses with default settings as our SMT model. 5 Results and Analysis Automatic evaluation of response generation is still an open problem. The widely accepted evaluation methods in translation (e.g. BLEU score (Papineni et al., 2002)) do not apply, since the range of the suitable responses is so large that it is practically impossible to give reference with adequate coverage. It is also not reasonable to evaluate with Perplexity, a generally used measurement in statistical language modeling, because the naturalness of response and the relatedness to the post can not be well evaluated. We therefore resort to human judgement, similar to that taken in (Ritter et al., 2011) but with an important difference. 5.1 Evaluation Methods We adopt human annotation to compare the performance of different models. Five labelers with at l"
P15-1152,D11-1054,0,0.921604,"Misu et al., 2012; Litman et al., 2000). These types of methods often rely on manual effort in designing rules or automatic training of model with a particular learning algorithm and a small amount of data, which makes it difficult to develop an extensible open domain conversation system. Recently due to the explosive growth of microblogging services such as Twitter1 and Weibo2 , the amount of conversation data available on the web has tremendously increased. This makes a 1 2 https://twitter.com/. http://www.weibo.com/. data-driven approach to attack the conversation problem (Ji et al., 2014; Ritter et al., 2011) possible. Instead of multiple rounds of conversation, the task at hand, referred to as Short-Text Conversation (STC), only considers one round of conversation, in which each round is formed by two short texts, with the former being an input (referred to as post) from a user and the latter a response given by the computer. The research on STC may shed light on understanding the complicated mechanism of natural language conversation. Previous methods for STC fall into two categories, 1) the retrieval-based method (Ji et al., 2014), and 2) the statistical machine translation (SMT) based method ("
P15-1152,N15-1020,0,0.888274,"possible. Instead of multiple rounds of conversation, the task at hand, referred to as Short-Text Conversation (STC), only considers one round of conversation, in which each round is formed by two short texts, with the former being an input (referred to as post) from a user and the latter a response given by the computer. The research on STC may shed light on understanding the complicated mechanism of natural language conversation. Previous methods for STC fall into two categories, 1) the retrieval-based method (Ji et al., 2014), and 2) the statistical machine translation (SMT) based method (Sordoni et al., 2015; Ritter et al., 2011). The basic idea of retrievalbased method is to pick a suitable response by ranking the candidate responses with a linear or non-linear combination of various matching features (e.g. number of shared words). The main drawbacks of the retrieval-based method are the following • the responses are pre-existing and hard to customize for the particular text or requirement from the task, e.g., style or attitude. • the use of matching features alone is usually not sufficient for distinguishing positive responses from negative ones, even after time consuming feature engineering. ("
P15-1152,D13-1096,1,0.661855,"user can post short messages (referred to as post in the reminder of this paper) visible to the public or a group of users following her/him. Other users make comment on a published post, which will be referred to as a response. Just like Twitter, Weibo also has the length limit of 140 Chinese characters on both posts and responses, making the post-response pair an ideal surrogate for short-text conversation. 2.2 Dataset Description To construct this million scale dataset, we first crawl hundreds of millions of post-response pairs, and then clean the raw data in a similar way as suggested in (Wang et al., 2013), including 1) removing trivial responses like “wow”, 2) filtering out potential advertisements, and 3) removing the responses after first 30 ones for topic consistency. Table 1 shows some statistics of the dataset used 1578 3 http://www.noahlab.com.hk/topics/ShortTextConversation Training Test Data Labeled Dataset (retrieval-based) Fine Tuning (SMT-based) #posts #responses #pairs #test posts #posts #responses #labeled pairs #posts #responses #pairs 219,905 4,308,211 4,435,959 110 225 6,017 6,017 2,925 3,000 3,000 Decoder Attention Signal Encoder Table 1: Some statistics of the dataset. Labele"
P15-1152,D13-1176,0,0.0655918,"my fish sandwich right now For god’s sake, it is 11 in the morning Enhhhh... sounds yummy which restaurant exactly? For god&apos;s sake, it is 11 in the morning Enhhhh... sounds yummy which restaurant exactly? Decoder vector Encoder Having my fish sandwich right now Figure 1: The diagram of encoder-decoder framework for automatic response generation. NRM essentially estimates the likelihood of a response given a post. Clearly the estimated probability should be complex enough to represent all the suitable responses. Similar framework has been used for machine translation with a remarkable success (Kalchbrenner and Blunsom, 2013; Auli et al., 2013; Sutskever et al., 2014; Bahdanau et al., 2014). Note that in machine trans2 The Dataset for STC Our models are trained on a corpus of roughly 4.4 million pairs of conversations from Weibo 3 . 2.1 Conversations on Sina Weibo Weibo is a popular Twitter-like microblogging service in China, on which a user can post short messages (referred to as post in the reminder of this paper) visible to the public or a group of users following her/him. Other users make comment on a published post, which will be referred to as a response. Just like Twitter, Weibo also has the length limit"
P15-1152,P07-2045,0,0.0130996,"n (Ji et al., 2014), we pick 225 posts and about 30 retrieved responses for each of them given by a baseline retriever6 from the 4.4M repository, and manually label them to obtain labeled 6,017 post-response pairs. We use ranking SVM model (Joachims, 2006) for the parameters ωi based on the labeled dataset. In comparison to NRM, only the top one response is considered in the evaluation process. SMT-based: In SMT-based models, the postresponse pairs are directly used as parallel data for training a translation model. We use the most widely used open-source phrase-based translation model-Moses (Koehn et al., 2007). Another parallel data consisting of 3000 post-response pairs is used to tune the system. In (Ritter et al., 2011), the authors used a modified SMT model to obtain the “Response” of Twitter “Stimulus”. The main modification is in replacing the standard GIZA++ word alignment model (Och and Ney, 2003) with a new phrase-pair selection method, in which all the 6 we use the default similarity function of Lucene 7 possible phrase-pairs in the training data are considered and their associated probabilities are estimated by the Fisher’s Exact Test, which yields performance slightly better than defaul"
P15-1152,W00-0304,0,0.0213671,"Missing"
P15-1152,W12-1611,0,0.0120233,"Missing"
P15-2088,N03-1017,0,0.028722,"pture context-dependent semantic similarities of translation pairs. We adopt a curriculum learning strategy to train the model: we classify the training examples into easy, medium, and difficult categories, and gradually build the ability of representing phrases and sentencelevel contexts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points. 1 Introduction Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms (Koehn et al., 2003), which often fail to capture translation pairs which are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. ∗ * Corresponding author 536 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th"
P15-2088,P07-2045,0,0.0101836,"00. We set the sliding window k = 3, and the learning rate η = 0.02. All the parameters are selected based on the development data. We train the word embeddings using a bilingual strategy similar to Yang et al. (2013), and set the dimension of the word embeddings be 50. To produce high-quality bilingual phrase pairs to train the CDCM model, we perform forced decoding on the bilingual training sentences and collect the used phrase pairs. 5.2 Evaluation of Translation Quality We have two baseline systems: • Baseline: The baseline system is an opensource system of the phrase-based model – Moses (Koehn et al., 2007) with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model. 6 Conclusion In this paper, we propose a context-dependent convolutional matching model to capture semantic similarities between phrase pairs that are sensitive to contexts. Experimental results show that our approach significantly improves the translation performance and obtains improvement of 1.0 BLEU scores on the overall test data. Integrating deep architecture into contextdependent translation selection is a promising way"
P15-2088,D08-1010,0,0.0187228,"4, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the 1 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models Baseline CICM CDCM1 CDCM2 CDCM3 MT04 34.86 35.82α 35.87α 35.97α 36.26αβ MT05 33.18 33.51α 33.58 33.80α 33.94αβ All 34.40 34.95α 35.01α 35.21α"
P15-2088,P08-1114,0,0.0247046,"tion test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the 1 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models Baseline CICM CDCM1 CDCM2 CDCM3 MT04 34.86 35.82α 35.87α 35.97α 36.26αβ MT05 33.18 33.51α 33.58 33.80α 33.94αβ All 34.40 34.95α 35.01α 35.21α 35.40αβ Table 1 summaries the"
P15-2088,P15-1003,1,0.855887,"ases with a similar meaning across different languages. Based on that translation equivalents share the same semantic meaning, they can supervise each other to learn their semantic phrase embeddings in a continuous space (Gao et al., 2014; Zhang et al., 2014). However, these models focused on capturing semantic similarities between phrase pairs in the global contexts, and neglected the local contexts, thus ignored the useful discriminative information. Alternatively, we integrate the local contexts into our convolutional matching architecture to obtain context-dependent semantic similarities. Meng et al. (2015) and Zhang (2015) have proposed independently to summary source sentences with convolutional neural networks. However, they both extend the neural network joint model (NNJM) of Devlin et al. (2014) to include the whole source sentence, while we focus on capturing context-dependent semantic similarities of translation pairs. 1: procedure CURRICULUM-TRAINING(T , W ) 2: N1 ← easy negative(T ) 3: N2 ← medium negative(T ) 4: N3 ← difficult negative(T ) 5: T ← N1 6: CURRICULUM(T , n · t) . CUR. easy 7: T ← MIX([N1 , N2 ]) 8: CURRICULUM(T , n · t) . CUR. medium 9: for step ← 1 . . . n do 10: T ← MIX("
P15-2088,P05-1066,0,0.0850318,"Missing"
P15-2088,P03-1021,0,0.00843428,"terations reach the predefined number, we terminate this curriculum. 4 5 Experiments 5.1 Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use the 2002 NIST MT evaluation test data as the development data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of disc"
P15-2088,P02-1040,0,0.0960297,"Setup We carry out our experiments on the NIST Chinese-English translation tasks. Our training data contains 1.5M sentence pairs coming from LDC dataset.1 We train a 4-gram language model on the Xinhua portion of the GIGAWORD corpus using the SRI Language Toolkit (Stolcke, 2002) with modified Kneser-Ney Smoothing (Kneser and Ney, 1995). We use the 2002 NIST MT evaluation test data as the development data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the so"
P15-2088,P14-1129,0,0.0895301,"Missing"
P15-2088,D14-1015,0,0.0151334,"or evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the 1 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models Baseline CICM CDCM1 CDCM2 CDCM3 MT04 34.86 35.82α 35.87α 35.97α 36.26αβ MT05 33.18 33.51α 33.58 33.80α 33.94αβ All 34.40 34.95α 35.01α 35.21α 35.40αβ Table 1 summaries the results of CDCMs trained from different curriculums. No matter from which curriculum it is trained, the C"
P15-2088,P14-1066,0,0.386412,"ntencelevel contexts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points. 1 Introduction Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms (Koehn et al., 2003), which often fail to capture translation pairs which are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. ∗ * Corresponding author 536 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 536–541, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Matching))Score) matching(model( convolu6onal( sentence(model( representation representat"
P15-2088,P13-1017,0,0.0539848,"pendent counterpart, and “All” is the combined test sets. The superscripts α and β indicate statistically significant difference (p &lt; 0.05) from Baseline and CICM, respectively. For training the neural networks, we use 4 convolution layers for source sentences and 3 convolution layers for target phrases. For both of them, 4 pooling layers (pooling size is 2) are used, and all the feature maps are 100. We set the sliding window k = 3, and the learning rate η = 0.02. All the parameters are selected based on the development data. We train the word embeddings using a bilingual strategy similar to Yang et al. (2013), and set the dimension of the word embeddings be 50. To produce high-quality bilingual phrase pairs to train the CDCM model, we perform forced decoding on the bilingual training sentences and collect the used phrase pairs. 5.2 Evaluation of Translation Quality We have two baseline systems: • Baseline: The baseline system is an opensource system of the phrase-based model – Moses (Koehn et al., 2007) with a set of common features, including translation models, word and phrase penalties, a linear distortion model, a lexicalized reordering model, and a language model. 6 Conclusion In this paper,"
P15-2088,C08-1041,0,0.0247345,"data, and the 2004, 2005 NIST MT evaluation test data as the test data. We use minimum error rate training (Och, 2003) to optimize the feature weights. For evaluation, case-insensitive NIST BLEU (Papineni et al., 2002) is used to measure translation performance. We perform a significance test using the sign-test approach (Collins et al., 2005). Related Work Our research builds on previous work in the field of context-dependent rule matching and bilingual phrase representations. There is a line of work that employs local contexts over discrete representations of words or phrases. For example, He et al. (2008), Liu et al. (2008) and Marton and Resnik (2008) employed within-sentence contexts that consist of discrete words to guide rule matching. Wu et al. (2014) exploited discrete contextual features in the source sentence (e.g. words and part-of-speech tags) to learn better bilingual word embeddings for SMT. In this study, we take into account all the 1 The corpus includes LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 539 Models Baseline CICM CDCM1 CDCM2 CDCM3 MT04 34.86 35.82α 35.87α 35.97α 36.26αβ MT05 33.18 33.51α 33.58 33.80α 33.94αβ All 34.40 34"
P15-2088,P14-1011,0,0.391613,"ts by using training examples from easy to difficult. Experimental results show that our approach significantly outperforms the baseline system by up to 1.4 BLEU points. 1 Introduction Conventional statistical machine translation (SMT) systems extract and estimate translation pairs based on their surface forms (Koehn et al., 2003), which often fail to capture translation pairs which are grammatically and semantically similar. To alleviate the above problems, several researchers have proposed learning and utilizing semantically similar translation pairs in a continuous space (Gao et al., 2014; Zhang et al., 2014; Cho et al., 2014). The core idea is that the two phrases in a translation pair should share the same semantic meaning and have similar (close) feature vectors in the continuous space. ∗ * Corresponding author 536 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 536–541, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Matching))Score) matching(model( convolu6onal( sentence(model( representation representation … … pooling Laye"
P15-2088,D13-1176,0,\N,Missing
P15-2088,P12-1079,0,\N,Missing
P15-2088,D13-1141,0,\N,Missing
P15-2088,P14-1013,0,\N,Missing
P16-1008,J93-2003,0,0.0568553,"Missing"
P16-1008,P15-1001,0,0.0941512,"Missing"
P16-1008,N03-1017,0,0.0513269,"Missing"
P16-1008,P07-2045,0,0.0228456,"Missing"
P16-1008,N06-1014,0,0.143259,"Missing"
P16-1008,D15-1166,0,0.112,"Missing"
P16-1008,J07-2003,0,0.0902386,"Missing"
P16-1008,J03-1002,0,0.011396,"Missing"
P16-1008,W14-4012,0,0.186136,"Missing"
P16-1008,D14-1179,0,0.0785675,"Missing"
P16-1008,P05-1066,0,0.0753936,"Missing"
P16-1008,P03-1021,0,0.0087897,"Missing"
P16-1008,P02-1040,0,0.100194,"Missing"
P16-1008,P16-1159,1,0.746732,"Missing"
P16-1008,koen-2004-pharaoh,0,\N,Missing
P16-1008,D08-1060,0,\N,Missing
P16-1008,P05-1033,0,\N,Missing
P16-1008,P09-1065,1,\N,Missing
P16-1008,N13-1073,0,\N,Missing
P16-1008,D11-1125,0,\N,Missing
P16-1008,D13-1052,0,\N,Missing
P16-1154,P16-1046,0,0.0524333,"references except for the input sequence, which will help in applications with heterogeneous source and target sequences such as machine translation. The copying mechanism can also be viewed as carrying information over to the next stage without any nonlinear transformation. Similar ideas are proposed for training very deep neural networks in (Srivastava et al., 2015; He et al., 2015) for classification tasks, where shortcuts are built between layers for the direct carrying of information. Recently, we noticed some parallel efforts towards modeling mechanisms similar to or related to copying. Cheng and Lapata (2016) devised a neural summarization model with the ability to extract words/sentences from the source. Gulcehre et al. (2016) proposed a pointing method to handle the OOV words for summarization and MT. In contrast, C OPY N ET is more general, and not limited to a specific task or OOV words. Moreover, the softmaxC OPY N ET is more flexible than gating in the related work in handling the mixture of two modes, due to its ability to adequately model the content of copied segment. 7 Conclusion and Future Work We proposed C OPY N ET to incorporate copying into the sequence-to-sequence learning framewor"
P16-1154,P16-1014,0,0.659184,"uch as machine translation. The copying mechanism can also be viewed as carrying information over to the next stage without any nonlinear transformation. Similar ideas are proposed for training very deep neural networks in (Srivastava et al., 2015; He et al., 2015) for classification tasks, where shortcuts are built between layers for the direct carrying of information. Recently, we noticed some parallel efforts towards modeling mechanisms similar to or related to copying. Cheng and Lapata (2016) devised a neural summarization model with the ability to extract words/sentences from the source. Gulcehre et al. (2016) proposed a pointing method to handle the OOV words for summarization and MT. In contrast, C OPY N ET is more general, and not limited to a specific task or OOV words. Moreover, the softmaxC OPY N ET is more flexible than gating in the related work in handling the mixture of two modes, due to its ability to adequately model the content of copied segment. 7 Conclusion and Future Work We proposed C OPY N ET to incorporate copying into the sequence-to-sequence learning framework. For future work, we will extend this idea to the task where the source and target are in heterogeneous types, for exam"
P16-1154,D15-1229,0,0.187358,"e case where strict replication is needed. A closer look (see Figure 3 for example) reveals that the decoder is dominated by copy-mode when moving into the subsequence to replicate, and switch to generate-mode after leaving this area, showing C OPY N ET can achieve a rather precise coordination of the two modes. The Target Sequence Figure 3: Example output of C OPY N ET on the synthetic dataset. The heatmap represents the activations of the copy-mode over the input sequence (left) during the decoding process (bottom). 5.2 Dataset: We evaluate our model on the recently published LCSTS dataset (Hu et al., 2015), a large scale dataset for short text summarization. The dataset is collected from the news medias on Sina Weibo1 including pairs of (short news, summary) in Chinese. Shown in Table 2, PART II and III are manually rated for their quality from 1 to 5. Following the setting of (Hu et al., 2015) we use Part I as the training set and and the subset of Part III scored from 3 to 5 as the testing set. Dataset no. of pairs no. of score ≥ 3 PART II PART III 2,400,591 - 10,666 8685 1106 725 Table 2: Some statistics of the LCSTS dataset. Experimental Setting: We try C OPY N ET that is based on character"
P16-1154,D15-1044,0,0.65022,"Electrical and Electronic Engineering, The University of Hong Kong {jiataogu, vli}@eee.hku.hk ‡ Huawei Noah’s Ark Lab, Hong Kong {lu.zhengdong, hangli.hl}@huawei.com Abstract Seq2Seq is essentially an encoder-decoder model, in which the encoder first transform the input sequence to a certain representation which can then transform the representation into the output sequence. Adding the attention mechanism (Bahdanau et al., 2014) to Seq2Seq, first proposed for automatic alignment in machine translation, has led to significant improvement on the performance of various tasks (Shang et al., 2015; Rush et al., 2015). Different from the canonical encoderdecoder architecture, the attention-based Seq2Seq model revisits the input sequence in its raw form (array of word representations) and dynamically fetches the relevant piece of information based mostly on the feedback from the generation of the output sequence. In this paper, we explore another mechanism important to the human language communication, called the “copying mechanism”. Basically, it refers to the mechanism that locates a certain segment of the input sentence and puts the segment into the output sequence. For example, in the following two dial"
P16-1154,P15-1152,1,0.291496,"Missing"
P16-1154,N15-1020,0,0.0454578,"Missing"
P16-1154,W04-1013,0,0.0862979,"I PART III 2,400,591 - 10,666 8685 1106 725 Table 2: Some statistics of the LCSTS dataset. Experimental Setting: We try C OPY N ET that is based on character (+C) and word (+W). For the word-based variant the word-segmentation is obtained with jieba2 . We set the vocabulary size to 3,000 (+C) and 10,000 (+W) respectively, which are much smaller than those for models in (Hu et al., 2015). For both variants we set the embedding dimension to 350 and the size of hidden layers to 500. Following (Hu et al., 2015), we evaluate the test performance with the commonly used ROUGE-1, ROUGE-2 and ROUGE-L (Lin, 2004), and compare it against the two models in (Hu et al., 2015), which are essentially canonical Encoder-Decoder and its variant with attention. Models Text Summarization Automatic text summarization aims to find a condensed representation which can capture the core meaning of the original document. It has been recently formulated as a Seq2Seq learning problem in (Rush et al., 2015; Hu et al., 2015), which essentially gives abstractive summarization since the summary is generated based on a representation of the document. In contrast, extractive summarization extracts sentences or phrases from th"
P16-1154,P15-1002,0,0.0328719,"ause of the difficulty brought by the unseen words. 6 Related Work Our work is partially inspired by the recent work of Pointer Networks (Vinyals et al., 2015a), in which a pointer mechanism (quite similar with the proposed copying mechanism) is used to predict the output sequence directly from the input. In addition to the difference with ours in application, (Vinyals et al., 2015a) cannot predict outside of the set of input sequence, while C OPY N ET can naturally combine generating and copying. C OPY N ET is also related to the effort to solve the OOV problem in neural machine translation. Luong et al. (2015) introduced a heuristics to postprocess the translated sentence using annotations on the source sentence. In contrast C OPY N ET addresses the OOV problem in a more systemic way with an end-to-end model. However, as C OPYN ET copies the exact source words as the output, it cannot be directly applied to machine translation. However, such copying mechanism can be naturally extended to any types of references except for the input sequence, which will help in applications with heterogeneous source and target sequences such as machine translation. The copying mechanism can also be viewed as carryin"
P17-2092,P07-2045,0,0.006204,"on in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a chunk state with a chunk attention, based on which multiple word states are generated withIn typical neural machine translation (NMT), the decoder generates a sentence word by word, packing all linguistic granular"
P17-2092,P05-1033,0,0.138079,"raged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguisti"
P17-2092,N03-1017,0,0.0576836,"ularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computat"
P17-2092,D14-1179,0,0.106361,"Missing"
P17-2092,P16-1160,0,0.309404,"ut the phrasal component may only change once. The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, o"
P17-2092,P06-1077,0,0.211161,"Missing"
P17-2092,P16-2058,0,0.0225803,"Missing"
P17-2092,P16-1078,0,0.311835,"h global reordering of phrases and local translation inside phrases. Our model has following benefits: 1. The chunk-based NMT model explicitly splits the lexical and phrasal components of the decode state for different time-scales, which addresses the issue of inconsistent updating speeds of different components, making the model more flexible. 2. Our model recognizes phrase structures explicitly. Phrase information are then used for word predictions, the representations of which are then used to help predict corresponding words. 3. Instead of incorporating source side linguistic information (Eriguchi et al., 2016; Sennrich and Haddow, 2016), our model incorporates linguistic knowledges in the target side (for deciding chunks), which will guide the translation more in line with linguistic grammars. 4. Given the predicted phrase representation, our NMT model could extract attentive source context by chunk attention, which is more specific and thus more useful compared to the word-level counterpart. Experiments show that our proposed model obtains considerable BLEU score improvements upon an attention-based NMT baseline on the Chinese to English and the German to English datasets simultaneously. 2 bush s"
P17-2092,D15-1166,0,0.0422838,"ine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work was done when Hao Zhou was interning and Zhaopeng Tu was working at Huawei Noah’s Ark Lab. 580 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 580–586 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2092 out attention. The word state is updated at every step, while the chunk state is only updated when the chunk boundary is detected by a boundary"
P17-2092,P08-1114,0,0.19138,"Missing"
P17-2092,P17-1174,0,0.0518004,"f previous work explore to incorporate the syntactic information in NMT, which shows the effectiveness of the syntactic information (Stahlberg et al., 2016). Shi et al. (2016) give some empirical results that the deep networks of NMT are able to capture some useful syntactic information implicitly. Luong et al. (2016) propose to use a multi-task framework for NMT and neural parsing, achieving promising results. Eriguchi et al. (2016) propose a string-totree NMT system by end-to-end training. Different to previous work, we try to incorporate the syntactic information in the target side of NMT. Ishiwatari et al. (2017) concurrently propose to use chunk-based decoder to cope with the problem of free word-order languages. Differently, they adopt word-level attention, and predict the end of chunk by generating end-of-chunk tokens instead of using boundary gate. Table 4: Subjective evaluation results. System dl4mt This Work DE-14 16.53 17.40 DE-1213 16.78 17.45 Table 5: Results on German-English the translation is translated by. The human evaluator is asked to give 4 scores: adequacy score and fluency score, which are between 0 and 5, the larger, the better; under-translation score and overtranslation score, wh"
P17-2092,P02-1040,0,0.102378,"from LDC corpora, with 25.1M Chinese words and 27.1M English words, respectively. We choose the NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) datasets as our test sets. We also evaluate our model on the WMT translation task of German-English, newstest2014 (DE14) is adopted as development set and newstest2012, newstest2013 (DE1213) are adopted as testing set. The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002). In training, we limit the source and target vocabularies to the most frequent 30K words. We train each model with the sentences of length up to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag sequence and boundary sequence on yn , respectively. In the C OPY operation, the chunk state is kept the"
P17-2092,D13-1176,0,0.0742459,"updates them in two different time-scales. Specifically, we first predict a chunk time-scale state for phrasal modeling, on top of which multiple word time-scale states are generated. In this way, the target sentence is translated hierarchically from chunks to words, with information in different granularities being leveraged. Experiments show that our proposed model significantly improves the translation performance over the state-of-the-art NMT model. 1 Introduction Recent work of neural machine translation (NMT) models propose to adopt the encoder-decoder framework for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), which employs a recurrent neural network (RNN) encoder to model the source context information and a RNN decoder to generate translations, which is significantly different from previous statistical machine translation systems (Koehn et al., 2003; Chiang, 2005). This framework is then extended by an attention mechanism, which acquires source sentence context dynamically at different decoding steps (Bahdanau et al., 2014; Luong et al., 2015). 1 In this work, we focus on chunk-based well-formed phrases, which generally contain two to five words. ∗ Work"
P17-2092,W16-2209,0,0.184245,"ponent may only change once. The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a ch"
P17-2092,Q17-1007,1,0.89772,"Missing"
P17-2092,P16-1162,0,0.0186724,". The inconsistent varying speed of the two components may cause translation errors. Typical NMT model generates target sentences in the word level, packing the phrasal and lexical information in one hidden state, which is not necessarily the best for translation. Much previous work propose to improve the NMT model by adopting fine-grained translation levels such as the character or sub-word levels, which can learn the intermediate information inside words (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Luong et al., 2016; Lee et al., 2016; Sennrich and Haddow, 2016; Sennrich et al., 2016; Garc´ıa-Mart´ınez et al., 2016). However, high level structures such as phrases has not been explicitly explored in NMT, which is very useful for machine translation (Koehn et al., 2007). We propose a chunk-based bi-scale decoder for NMT, which explicitly splits the lexical and phrasal components into different time-scales.1 The proposed model generates target words in a hierarchical way, which deploys a standard word time-scale RNN (lexical modeling) on top of an additional chunk time-scale RNN (phrasal modeling). At each step of decoding, our model first predict a chunk state with a chunk"
P17-2092,P16-1008,1,0.910907,"Missing"
P17-2092,P08-1066,0,0.0644196,"Missing"
P17-2092,P16-1218,0,0.0232133,"to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag sequence and boundary sequence on yn , respectively. In the C OPY operation, the chunk state is kept the same as the previous step. In the U PDATE operation, ept−1 is the representation of last chunk, which is computed by the LSTM-minus approach (Wang and Chang, 2016): Ts ! log P (yn |xn ) + log P (ln |xn ) + log P (bn |xn ) bt will be 0 or 1, where 1 denotes this is the boundary of a new chunk while 0 denotes not. Two different operations would be executed: # pt−1 , bt = 0 (C OPY ) pt = g(pt−1 , ept−1 , pct ), bt = 1 (U PDATE ) ept−1 = m(st−1 , eyt−1 ) − m(st′ , eyt′ ) N & ! (10) The chunk attention model differs from the standard word attention model (i.e., Equation 3) at: 1) it reads chunk state pt−1 rather than word state st−1 , and 2) it is only executed at boundary of each chunk rather than at each decoding step. In this way, our model only extracts"
P17-2092,D16-1159,0,0.0691766,"Missing"
P17-2092,1983.tc-1.13,0,0.186514,"Missing"
P17-2092,P16-2049,0,0.0402049,"Missing"
P17-2092,P15-1117,1,0.838643,"Chinese-English translation task. Our training data consists of 1.16M2 sentence pairs extracted from LDC corpora, with 25.1M Chinese words and 27.1M English words, respectively. We choose the NIST 2002 (MT02) dataset as our development set, and the NIST 2003 (MT03), 2004 (MT04) 2005 (MT05) datasets as our test sets. We also evaluate our model on the WMT translation task of German-English, newstest2014 (DE14) is adopted as development set and newstest2012, newstest2013 (DE1213) are adopted as testing set. The English sentences are labeled by a neural chunker, which is implemented according to Zhou et al. (2015). We use the case-insensitive 4-gram NIST BLEU score as our evaluation metric (Papineni et al., 2002). In training, we limit the source and target vocabularies to the most frequent 30K words. We train each model with the sentences of length up to 50 words. Sizes of the chunk representation and chunk hidden state are set to 1000. All the other settings are the same as in Bahdanau et al. (2014). (9) here t′ is the boundary of last chunk and m(·) is a linear function. pct is the context vector for chunk pt , which is calculated by a chunk attention model: pct = &apos; where ln and bn are chunk tag seq"
P17-2092,Q17-1026,0,\N,Missing
P97-1006,P93-1024,0,0.141656,"omplish hypothesis testing, we employ the EM algorithm to efficiently and approximately calculate from training data the maximum likelihood estimates of parameters in a finite mixture model. Our method overcomes the major drawbacks of the method using word-based distributions and the method based on hard clustering, while retaining their merits; it in fact includes those two methods as special cases. Experimental results indicate that our method outperforrrLs them. Although the finite mixture model has already been used elsewhere in natural language processing (e.g. (Jelinek and Mercer, 1980; Pereira, Tishby, and Lee, 1993)), this is the first work, to the best of knowledge, that uses it in the context of document classification. Abstract We propose a new method of classifying documents into categories. We define for each category a finite mixture model based on soft clustering of words. We treat the problem of classifying documents as that of conducting statistical hypothesis testing over finite mixture models, and employ the EM algorithm to efficiently estimate parameters in a finite mixture model. Experimental results indicate that our method outperforms existing methods. 1 Introduction We are concerned here"
P97-1006,H90-1056,0,0.114404,"nto the category for which it has the largest probability is equivalent to classifying it into the category having the largest likelihood with respect to it. Hereafter, we will use only the term likelihood and denote it as L(dlci). Notice that in practice the parameters in a distribution must be estimated from training data. In the case of WBM, the number of parameters is large; the training data size, however, is usually not sufficiently large for accurately estimating them. This is the data .sparseness problem that so often stands in the way of reliable statistical language processing (e.g.(Gale and Church, 1990)). Moreover, the number of parameters in word-based distributions is too large to be efficiently stored. Table 3: Frequencies of clusters kl ks k3 c1 7 0 3 c2 0 2 5 There are any number of ways to create clusters in hard clustering, but the method employed is crucial to the accuracy of document classification. Guthrie et. al. have devised a way suitable to documentation classification. Suppose that there are two categories cl =&apos;tennis&apos; and c2=&apos;soccer,&apos; and we obtain from the training data (previously classified documents) the frequencies of words in each category, such as those in Tab. 1. Lett"
P97-1006,C94-2172,0,\N,Missing
P98-2124,J92-4003,0,0.0376377,"Missing"
P98-2124,C96-1003,1,0.90702,"L) principle. Our clustering algorithm iteratively merges noun classes and verb classes in turn, in a bottom up fashion. For 749 each merge it performs, it calculates the increase in data description length resulting from merging any noun (or verb) class pair, and performs the merge having the least increase in data description length, provided that the increase in data description length is less than the reduction in model description length. There have been a number of methods proposed in the literature to address the word clustering problem (e.g., (Brown et al., 1992; Pereira et al., 1993; Li and Abe, 1996)). The method proposed in this paper is a natural extension of both Li & Abe&apos;s and Brown et al&apos;s methods, and is an attempt to overcome their drawbacks while retaining their advantages. The method of Brown et al, which is based on the Maximum Likelihood Estimation (MLE), performs a merge which would result in the least reduction in (average) mutual information. Our method turns out to be equivalent to performing the merge with the least reduction in mutual information, provided that the reduction is below a certain threshold which depends on the size of the co-occurrence data and the number of"
P98-2124,P93-1024,0,0.523326,"Missing"
P98-2124,J98-2002,1,\N,Missing
P98-2124,C94-2195,0,\N,Missing
Q17-1007,J93-2003,0,0.0952604,"ments in Section 5.2 show that our gating mechanism significantly outperforms linear interpolation when combining contexts. Comparison to Handling Null-Generated Words in SMT: In machine translation, there are certain syntactic elements of the target language that are missing in the source (i.e., null-generated words). In fact this was the preliminary motivation for our approach: current attention models lack a mechanism to control the generation of words that do not have a strong correspondence on the source side. The model structure of NMT is quite similar to the traditional word-based SMT (Brown et al., 1993). Therefore, techniques that have proven effective in SMT may also be applicable to NMT. Toutanova et al. (2002) extend the calculation of translation probabilities to include null-generated target words in word-based SMT. These words are generated based on both the special source token null and the neighbouring word in the target language by a mixture model. We have simplified and generalized their approach: we use context gates to dynamically control the contribution of source context. When producing null-generated words, the context gate can as93 sign lower weights to the source context, by"
Q17-1007,P05-1066,0,0.0240416,"Missing"
Q17-1007,P15-1001,0,0.0454042,"SMT and NMT6 models: • Moses (Koehn et al., 2007): an open source phrase-based translation system with default configuration and a 4-gram language model trained on the target portion of training data; • GroundHog (Bahdanau et al., 2015): an open source attention-based NMT model with default setting. We have two variants that differ in the activation function used in the decoder 5 The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06. 6 There is some recent progress on aggregating multiple models or enlarging the vocabulary(e.g.,, in (Jean et al., 2015)), but here we focus on the generic models. # 1 2 3 4 5 6 7 8 9 System Moses GroundHog (vanilla) 2 + Context Gate (both) GroundHog (GRU ) 4 + Context Gate (source) 4 + Context Gate (target) 4 + Context Gate (both) GroundHog-Coverage (GRU ) 8 + Context Gate (both) #Parameters – 77.1M 80.7M 84.3M 87.9M 87.9M 87.9M 84.4M 88.0M MT05 31.37 26.07 30.86∗ 30.61 31.96∗ 32.38∗ 33.52∗ 32.73 34.13∗ MT06 30.85 27.34 30.85∗ 31.12 32.29∗ 32.11∗ 33.46∗ 32.47 34.83∗ MT08 23.01 20.38 24.71∗ 23.23 24.97∗ 23.78 24.85∗ 25.23 26.22∗ Ave. 28.41 24.60 28.81 28.32 29.74 29.42 30.61 30.14 31.73 Table 2: Evaluation of t"
Q17-1007,D13-1176,0,0.0685886,"ear , the export of new high level technology product was UNK - billion us dollars china ’s guangdong hi - tech exports hit 58 billion dollars china ’s export of high and new hi - tech exports of the export of the export of the export of the export of the export of the export of the export of the export of · · · Table 1: Source and target contexts are highly correlated to translation adequacy and fluency, respectively. 5src and 5tgt denote halving the contributions from the source and target contexts when generating the translation, respectively. Introduction Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has made significant progress in the past several years. Its goal is to construct and utilize a single large neural network to accomplish the entire translation task. One great advantage of NMT is that the translation system can be completely constructed by learning from data without human involvement (cf., feature engineering in statistical machine translation (SMT)). The encoderdecoder architecture is widely employed (Cho et al., 2014; Sutskever et al., 2014), in which the encoder summarizes the source sentence into a vector representation, an"
Q17-1007,P07-2045,0,0.0649955,"Missing"
Q17-1007,D15-1166,0,0.200096,"Missing"
Q17-1007,J03-1002,0,0.0544157,"Missing"
Q17-1007,P02-1040,0,0.118366,"Missing"
Q17-1007,W09-0441,0,0.0117211,"t words in the target sentence are more related to the translation adequacy, and thus should depend more on the source context. In contrast, function words in the target sentence are often more related to the translation fluency (e.g., “of” after “is fond”), and thus should depend more on the target context. In this work, we propose to use context gates to control the contributions of source and target contexts on the generation of target words (decoding) 1 Fluency measures whether the translation is fluent, while adequacy measures whether the translation is faithful to the original sentence (Snover et al., 2009). 88 Figure 1: Architecture of decoder RNN. in NMT. Context gates are non-linear gating units which can dynamically select the amount of context information in the decoding process. Specifically, at each decoding step, the context gate examines both the source and target contexts, and outputs a ratio between zero and one to determine the percentages of information to utilize from the two contexts. In this way, the system can balance the adequacy and fluency of the translation with regard to the generation of a word at each position. Experimental results show that introducing context gates lead"
Q17-1007,W02-1012,0,0.167148,"Missing"
Q17-1007,P16-1008,1,0.93311,"ted.4 The decoding state implicitly models the notion of “coverage” by recurrently reading the time-dependent source context si . Lowering its contribution weakens the “coverage” effect and encourages the decoder to regenerate phrases multiple times to achieve the desired translation length. 2. The translation is incomplete. As shown in Table 1, NMT can get stuck in an infinite loop repeatedly generating a phrase due to the overwhelming influence of the source context. As a result, generation terminates early because 4 The recently proposed coverage based technique can alleviate this problem (Tu et al., 2016). In this work, we consider another approach, which is complementary to the coverage mechanism. 90 Figure 3: Architecture of context gate. the translation reaches the maximum length allowed by the implementation, even though the decoding procedure is not finished. The quantitative (Figure 2) and qualitative (Table 1) results confirm our hypothesis, i.e., source and target contexts are highly correlated to translation adequacy and fluency. We believe that a mechanism that can dynamically select information from source context and target context would be useful for NMT models, and this is exactl"
Q17-1007,P16-1125,0,0.0156616,"context gate controls the effect of the source context based on its relative importance. Experiments in Section 5.2 show that combining the two methods can further improve translation performance. There is another difference as well: the coverage mechanism is only applicable to attention-based NMT models, while the context gate is applicable to all NMT models. Comparison to Exploiting Auxiliary Contexts in Language Modeling: A thread of work in language modeling (LM) attempts to exploit auxiliary sentence-level or document-level context in an RNN LM (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2016). Independent of our work, Wang and Cho (2016) propose “early fusion” models of RNNs where additional information from an intersentence context is “fused” with the input to the RNN. Closely related to Wang and Cho (2016), our approach aims to dynamically control the contributions of required source and target contexts for machine translation, while theirs focuses on integrating auxiliary corpus-level contexts for language modelling to better approximate the corpus-level probability. In addition, we employ a gating mechanism to produce a dynamic weight at different decoding steps to combine sou"
S17-1008,W11-0609,0,0.0352407,"lity, but its responses are still short and dull. To tackle this issue, we initiate an online AL process where our model interacts with real users and learns incrementally from their feedback at each turn of dialogue. The CA−human interaction for online AL is set up as follows (pseudocode in Algorithm 1, example interaction in Figure 1). Offline Two-Phase Supervised Learning To establish an offline baseline, we train our network sequentially on two datasets, one for generic dialogue, and the other specially curated for short-text conversation. Phase 1: We use the Cornell Movie Dialogs Corpus (Danescu-Niculescu-Mizil and Lee, 2011), consisting of 300K message-response pairs. Each pair is treated as an input and target sequence during training with the joint cross-entropy (XENT) loss function, which maximizes the likelihood of generating the target sequence given its input. 1. The user sends a message ui at time step i. 2. CA generates K responses ci,1 , ci,2 , ..., ci,K using hamming-diverse Beam Search. These are displayed to the user in order of decreasing generation likelihood. 3. The user provides feedback by selecting one of the K responses as the ‘best’ one or suggesting a (K+1)’th response, denoted by c∗i,j . The"
S17-1008,E17-2075,0,0.00738157,"learning mechanism based on hamming-diverse beam search for response generation and one-character userfeedback at each step. Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train agents with customized personas, moods and conversational styles. 1 Introduction 2 Several recent works propose neural generative conversational agents (CAs) for open-domain and task-oriented dialogue (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016, 2017; Wen et al., 2016; Shen et al., 2017; Eric and Manning, 2017a,b). These models typically use LSTM encoder-decoder architectures (e.g. the sequence-to-sequence (Seq2Seq) framework (Sutskever et al., 2014)), which are linguistically robust but can often generate short, dull and inconsistent responses (Serban et al., 2016; Li et al., 2016a). Researchers are now exploring Deep Reinforcement Learning (DRL) to address the hard problems of NLU and NLG in dialogue generation. In most of the existing works, the reward function is hand-crafted, and is either specific to the task to be completed, or is based on a few desirable developer-defined conversational Rel"
S17-1008,N03-1017,0,0.00523602,"eration is a relatively new research paradigm that is most relevant to our work. For task-specific dialogue (Su et al., 2016; Zhao and Eskenazi, 2016; Cuay´ahuitl et al., 2016; Williams and Zweig, 2016; Li et al., 2017b,c; Peng et al., 2017), the reward function is usually based on task completion rate, and thus is easy to define. For the much harder problem of open-domain dialogue generation (Li et al., 2016e; Yu et al., 2016; Weston, 2016), hand-crafted reward functions are used to capture desirable conversation properties. Li et al. (2016d) propose DRL-based diversitypromoting Beam Search (Koehn et al., 2003) for response generation. Very recently, new approaches have been pro1 The user has the option to provide longer feedback. 78 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 78–83, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics Algorithm 1 Online Active Learning posed to incorporate online human feedback into neural conversation models (Li et al., 2016c; Abel et al., 2017; Li et al., 2017a). Our work falls in this line of research, and is distinguished from existing approaches in the following key ways. 1:"
S17-1008,P16-1094,0,0.032973,"Missing"
S17-1008,N15-1020,0,0.0750545,"Missing"
S17-1008,D16-1127,0,0.200105,"stomized personas, moods and conversational styles. 1 Introduction 2 Several recent works propose neural generative conversational agents (CAs) for open-domain and task-oriented dialogue (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016, 2017; Wen et al., 2016; Shen et al., 2017; Eric and Manning, 2017a,b). These models typically use LSTM encoder-decoder architectures (e.g. the sequence-to-sequence (Seq2Seq) framework (Sutskever et al., 2014)), which are linguistically robust but can often generate short, dull and inconsistent responses (Serban et al., 2016; Li et al., 2016a). Researchers are now exploring Deep Reinforcement Learning (DRL) to address the hard problems of NLU and NLG in dialogue generation. In most of the existing works, the reward function is hand-crafted, and is either specific to the task to be completed, or is based on a few desirable developer-defined conversational Related Work & Contributions DRL-based dialogue generation is a relatively new research paradigm that is most relevant to our work. For task-specific dialogue (Su et al., 2016; Zhao and Eskenazi, 2016; Cuay´ahuitl et al., 2016; Williams and Zweig, 2016; Li et al., 2017b,c; Peng e"
S17-1008,I17-1074,0,0.0304494,"al., 2016; Li et al., 2016a). Researchers are now exploring Deep Reinforcement Learning (DRL) to address the hard problems of NLU and NLG in dialogue generation. In most of the existing works, the reward function is hand-crafted, and is either specific to the task to be completed, or is based on a few desirable developer-defined conversational Related Work & Contributions DRL-based dialogue generation is a relatively new research paradigm that is most relevant to our work. For task-specific dialogue (Su et al., 2016; Zhao and Eskenazi, 2016; Cuay´ahuitl et al., 2016; Williams and Zweig, 2016; Li et al., 2017b,c; Peng et al., 2017), the reward function is usually based on task completion rate, and thus is easy to define. For the much harder problem of open-domain dialogue generation (Li et al., 2016e; Yu et al., 2016; Weston, 2016), hand-crafted reward functions are used to capture desirable conversation properties. Li et al. (2016d) propose DRL-based diversitypromoting Beam Search (Koehn et al., 2003) for response generation. Very recently, new approaches have been pro1 The user has the option to provide longer feedback. 78 Proceedings of the 6th Joint Conference on Lexical and Computational Sema"
S17-1008,D17-1237,0,0.014666,", 2016a). Researchers are now exploring Deep Reinforcement Learning (DRL) to address the hard problems of NLU and NLG in dialogue generation. In most of the existing works, the reward function is hand-crafted, and is either specific to the task to be completed, or is based on a few desirable developer-defined conversational Related Work & Contributions DRL-based dialogue generation is a relatively new research paradigm that is most relevant to our work. For task-specific dialogue (Su et al., 2016; Zhao and Eskenazi, 2016; Cuay´ahuitl et al., 2016; Williams and Zweig, 2016; Li et al., 2017b,c; Peng et al., 2017), the reward function is usually based on task completion rate, and thus is easy to define. For the much harder problem of open-domain dialogue generation (Li et al., 2016e; Yu et al., 2016; Weston, 2016), hand-crafted reward functions are used to capture desirable conversation properties. Li et al. (2016d) propose DRL-based diversitypromoting Beam Search (Koehn et al., 2003) for response generation. Very recently, new approaches have been pro1 The user has the option to provide longer feedback. 78 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), page"
S17-1008,W16-3649,0,0.012032,"is hand-crafted, and is either specific to the task to be completed, or is based on a few desirable developer-defined conversational Related Work & Contributions DRL-based dialogue generation is a relatively new research paradigm that is most relevant to our work. For task-specific dialogue (Su et al., 2016; Zhao and Eskenazi, 2016; Cuay´ahuitl et al., 2016; Williams and Zweig, 2016; Li et al., 2017b,c; Peng et al., 2017), the reward function is usually based on task completion rate, and thus is easy to define. For the much harder problem of open-domain dialogue generation (Li et al., 2016e; Yu et al., 2016; Weston, 2016), hand-crafted reward functions are used to capture desirable conversation properties. Li et al. (2016d) propose DRL-based diversitypromoting Beam Search (Koehn et al., 2003) for response generation. Very recently, new approaches have been pro1 The user has the option to provide longer feedback. 78 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 78–83, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics Algorithm 1 Online Active Learning posed to incorporate online human feedback into neural conv"
S17-1008,W16-3601,0,0.0107648,"ust but can often generate short, dull and inconsistent responses (Serban et al., 2016; Li et al., 2016a). Researchers are now exploring Deep Reinforcement Learning (DRL) to address the hard problems of NLU and NLG in dialogue generation. In most of the existing works, the reward function is hand-crafted, and is either specific to the task to be completed, or is based on a few desirable developer-defined conversational Related Work & Contributions DRL-based dialogue generation is a relatively new research paradigm that is most relevant to our work. For task-specific dialogue (Su et al., 2016; Zhao and Eskenazi, 2016; Cuay´ahuitl et al., 2016; Williams and Zweig, 2016; Li et al., 2017b,c; Peng et al., 2017), the reward function is usually based on task completion rate, and thus is easy to define. For the much harder problem of open-domain dialogue generation (Li et al., 2016e; Yu et al., 2016; Weston, 2016), hand-crafted reward functions are used to capture desirable conversation properties. Li et al. (2016d) propose DRL-based diversitypromoting Beam Search (Koehn et al., 2003) for response generation. Very recently, new approaches have been pro1 The user has the option to provide longer feedback. 78 Proc"
S17-1008,P15-1152,1,0.543163,"g research proposes offline supervision or hand-crafted reward functions for online reinforcement, we devise a novel interactive learning mechanism based on hamming-diverse beam search for response generation and one-character userfeedback at each step. Experiments show that our model inherently promotes the generation of semantically relevant and interesting responses, and can be used to train agents with customized personas, moods and conversational styles. 1 Introduction 2 Several recent works propose neural generative conversational agents (CAs) for open-domain and task-oriented dialogue (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016, 2017; Wen et al., 2016; Shen et al., 2017; Eric and Manning, 2017a,b). These models typically use LSTM encoder-decoder architectures (e.g. the sequence-to-sequence (Seq2Seq) framework (Sutskever et al., 2014)), which are linguistically robust but can often generate short, dull and inconsistent responses (Serban et al., 2016; Li et al., 2016a). Researchers are now exploring Deep Reinforcement Learning (DRL) to address the hard problems of NLU and NLG in dialogue generation. In most of the existing works, the reward function is h"
W00-1305,P97-1006,1,0.510729,"; Weiss et al., 1999; Nigam et al., 2000)). While text classification uses a number of pre-determined categories, topic analysis includes no notion of category. The output of topic analysis is a topic structure, while the output o f text clas40 sification is a label representing a category. Furthermore, text classification is generally based on supervised learning, which uses labeled text data 6. By way of contrast, topic analysis is based on unsupervised learning, which uses only unlabeled text data. Finite mixture models have been used in a variety of applications in text processing (e.g., (Li and Yamanishi, 1997; Nigam et al., 2000; Hofmann, 1999)), indicating that they are essential to text processing. We should note, however, that their definitions and the ways they use them axe different from those for STM in this paper. For example, Li and Yamanishi propose to employ in text classification a mixture model (Li and Yamanishi, 1997) defined over categories: P(WIC) = ~ split of the data &apos;Apte split,&apos; which consists of 9603 texts for training and 3299 texts for test. All of the texts had already been classified into 90 categories by human subjects. For each text, we used the Oxford Learner&apos;s Dictionar"
W00-1305,W99-0908,0,0.0676574,"Missing"
W00-1305,P99-1046,0,0.146426,"Missing"
W00-1305,J92-4003,0,0.0236055,"= 1 , . - . , r n ) , 1 denotes the presence of a word, while 0 the absence of it. We further denote s TM = sl...sm, and • m.~/j (I) For a fixed seed word s, we take a word w as a frequently co-occurring word if the presence of s is a statistically significant indicator of the presence of w. W TM ~.. W 1 m 4.2 O u t l i n e Our topic analysis consists of three processes: a pre-process called &apos;topic spotting,&apos; text segmentation, and topic identification. In topic log) SNote that the quantity within [---] in (1) is ( e m - which is an effective measure for word co-occurrence calculation (cf.,(Brown et al., 1992)). When the sample size is small, mutual information values tend to be undesirably large. The quantity within {-..} in (1) can help avoid this undesirable tendency because its value will become large when data size is small. pirical) mutual inyormation, where ms+ denotes the number of l&apos;s in wm&apos;, and w~+s the number of l&apos;s in w m~,. 2For an introduction to M D L , see (Li, 1998). 37 ASIAI SXPOITERS PSAk DAEAOS Fit05 U.S.-IAPA |RIFT (25-HAE-1987) block 0 . . . . . . . . t r a d e - e x p o r ~ - c a r i ~ t - i m p o : r t ( O , 1 2 ) Japan-Japa.l~ese(O.07) U$(0.06) 0 Sountin S t r a d e f r i"
W00-1305,J97-1003,0,0.490901,"for purposes of information retrieval. With it, one can understand what the main topics and subtopics of a text are, and where those subtopics lie within the text. To the best of our knowledge, however, no previous study has so far dealt with the topic analysis problem in the above sense. The most closely related are key word extraction and text segmentation. A keyword extraction method (e.g., that using tf-idf (Salton and Yang, 1973)) generally extracts from a text key words which represent topics within the text, but it does not conduct segmentation. A segmentation method (e.g., TextTiling (Hearst, 1997)) generally segments a text into blocks (paragraphs) in accord with topic changes within the text, but it does not identify (or label) by itself the topics discussed in 35 mentation method (e.g., TextTiling). Specifically, one can extract key words from a text using tf-idf, view these extracted key words as topics, segment the text into blocks using TextTiling, and estimate the distribution of topics (key words) within each block. Experimental results indicate, :however, that our method significantly outper~brms such a combined method in topic identification and outperforms it in text segmenta"
W16-0105,D13-1160,0,0.238756,"Missing"
W16-0105,W10-2903,0,0.0525007,"Missing"
W16-0105,P11-1060,0,0.0963396,"Missing"
W16-0105,P15-1142,0,0.115044,"Missing"
W16-0105,D15-1199,0,0.0950147,"Missing"
W16-0105,D12-1040,0,\N,Missing
W16-0105,P14-1133,0,\N,Missing
W16-0105,D15-1198,0,\N,Missing
W16-0105,P15-1096,0,\N,Missing
W16-0106,D14-1067,0,0.0218583,"Missing"
W16-0106,P15-1152,1,0.31684,"Missing"
W16-0106,D14-1179,0,\N,Missing
W16-0106,W14-4012,0,\N,Missing
W16-0106,D14-1071,0,\N,Missing
W96-0112,J93-1002,0,0.0445654,"t of the conditional probabilities of the rules which are applied in the derivation of that tree. Other methods have also been proposed. Magerman ~ Marcus, for instance, have proposed making use of a conditional probability model specifying a conditional probability of a CFG rule, given the part-of-speech trigram it dominates and its parent rule (Magerman and Marcus, 1991). Black et al. have defined a richer model to utilize all the information in the top-down derivation of a non-terminal (Black et al., 1992). Briscoe & Carroll have proposed using a probabilistic model specific to LR parsing (Briscoe and Carroll, 1993). 148 the the block on block the table in the P:I room A number of companies sell end buy by computer N P:5 by computer II the II table in the sell and It' buy room NP:~ NP:2 the Ii block PP:3 P:I PP:3 NP:2 il on the ii in the sell and room fl buy by computer table Non-terminel: length Non-terminal: length (a) (b) Figure 2: Examples of syntactic parsing The advantage of the syntactic parsing approach is that it mGv embody heuristics (principles) effective in disambiguation, which would not have been thought of by humans, but it also risks not embodying heuristics (principles) already known to"
W96-0112,P92-1023,0,0.339201,"g., (Shieber, 1983; Wermter, 1989; Wilks et al., 1985)). An alternative approach is to view language as a stochastic phenomenon, particularly from the viewpoint of information theory and statistics. If we could properly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also resolve ambiguities quite well. There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective (Alshawi and Carter, 1995; Black et al., 1992; Briscoe and Carroll. 1993; Chang et al., 1992; Collins and Brooks, 1995; Fujisaki, 1989; Hindle and Rooth, 1991; Hindle and Rooth, 1993; Jelinek et al., 1990; Magerman and Marcus, 1991; Magerman, 1995; Ratnaparkhi et al., 1994; Resnik, 1993; Su and Chang, 1988). Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily. We feel that it is necessary to devise a new method that unifies the above two approaches, i.e., to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology. Most psycholinguistic principles hav"
W96-0112,J82-3004,0,0.091186,"Missing"
W96-0112,W95-0103,0,0.0705354,"Wermter, 1989; Wilks et al., 1985)). An alternative approach is to view language as a stochastic phenomenon, particularly from the viewpoint of information theory and statistics. If we could properly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also resolve ambiguities quite well. There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective (Alshawi and Carter, 1995; Black et al., 1992; Briscoe and Carroll. 1993; Chang et al., 1992; Collins and Brooks, 1995; Fujisaki, 1989; Hindle and Rooth, 1991; Hindle and Rooth, 1993; Jelinek et al., 1990; Magerman and Marcus, 1991; Magerman, 1995; Ratnaparkhi et al., 1994; Resnik, 1993; Su and Chang, 1988). Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily. We feel that it is necessary to devise a new method that unifies the above two approaches, i.e., to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology. Most psycholinguistic principles have 1A r e p r e s e n t a t"
W96-0112,W89-0209,0,0.289404,"., 1985)). An alternative approach is to view language as a stochastic phenomenon, particularly from the viewpoint of information theory and statistics. If we could properly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also resolve ambiguities quite well. There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective (Alshawi and Carter, 1995; Black et al., 1992; Briscoe and Carroll. 1993; Chang et al., 1992; Collins and Brooks, 1995; Fujisaki, 1989; Hindle and Rooth, 1991; Hindle and Rooth, 1993; Jelinek et al., 1990; Magerman and Marcus, 1991; Magerman, 1995; Ratnaparkhi et al., 1994; Resnik, 1993; Su and Chang, 1988). Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily. We feel that it is necessary to devise a new method that unifies the above two approaches, i.e., to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology. Most psycholinguistic principles have 1A r e p r e s e n t a t i o n of a p r"
W96-0112,P91-1030,0,0.400207,"ternative approach is to view language as a stochastic phenomenon, particularly from the viewpoint of information theory and statistics. If we could properly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also resolve ambiguities quite well. There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective (Alshawi and Carter, 1995; Black et al., 1992; Briscoe and Carroll. 1993; Chang et al., 1992; Collins and Brooks, 1995; Fujisaki, 1989; Hindle and Rooth, 1991; Hindle and Rooth, 1993; Jelinek et al., 1990; Magerman and Marcus, 1991; Magerman, 1995; Ratnaparkhi et al., 1994; Resnik, 1993; Su and Chang, 1988). Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily. We feel that it is necessary to devise a new method that unifies the above two approaches, i.e., to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology. Most psycholinguistic principles have 1A r e p r e s e n t a t i o n of a p r o b a b i l i t y d i s"
W96-0112,J93-1005,0,0.0353288,"view language as a stochastic phenomenon, particularly from the viewpoint of information theory and statistics. If we could properly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also resolve ambiguities quite well. There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective (Alshawi and Carter, 1995; Black et al., 1992; Briscoe and Carroll. 1993; Chang et al., 1992; Collins and Brooks, 1995; Fujisaki, 1989; Hindle and Rooth, 1991; Hindle and Rooth, 1993; Jelinek et al., 1990; Magerman and Marcus, 1991; Magerman, 1995; Ratnaparkhi et al., 1994; Resnik, 1993; Su and Chang, 1988). Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily. We feel that it is necessary to devise a new method that unifies the above two approaches, i.e., to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology. Most psycholinguistic principles have 1A r e p r e s e n t a t i o n of a p r o b a b i l i t y d i s t r i b u t i o n is cal"
W96-0112,C90-3029,0,0.387809,"guage processing. To completely resolve ambiguities, we would need to construct a human-like language understanding system (c.f.(Altmann and Steedman, 1988; Johnson-Laird, 1983)). The construction of such a system is extremely difficult, however, and we need to adopt a more realistic approach. In psycholinguistics, a number of principles have been proposed which attempt to modelize the human disambiguation process. The Lexical Preference Rule (LPR) (Ford et al., 1982), the Right Association Principle (RAP) (Kimball, 1973), and the Attach Low and Parallel Principle (ALPP, an extension of RAP) (Hobbs and Bear, 1990) have been proposed, and it is thought that we might resolve ambiguities quite satisfactorily if we could implement these principles sufficiently (Hobbs and Bear, 1990; Whittemore et al., 1990). Methods of implementing these principles have also been proposed (e.g., (Shieber, 1983; Wermter, 1989; Wilks et al., 1985)). An alternative approach is to view language as a stochastic phenomenon, particularly from the viewpoint of information theory and statistics. If we could properly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also"
W96-0112,J94-4001,0,0.0423296,"Missing"
W96-0112,E91-1004,0,0.505228,"ticularly from the viewpoint of information theory and statistics. If we could properly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also resolve ambiguities quite well. There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective (Alshawi and Carter, 1995; Black et al., 1992; Briscoe and Carroll. 1993; Chang et al., 1992; Collins and Brooks, 1995; Fujisaki, 1989; Hindle and Rooth, 1991; Hindle and Rooth, 1993; Jelinek et al., 1990; Magerman and Marcus, 1991; Magerman, 1995; Ratnaparkhi et al., 1994; Resnik, 1993; Su and Chang, 1988). Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily. We feel that it is necessary to devise a new method that unifies the above two approaches, i.e., to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology. Most psycholinguistic principles have 1A r e p r e s e n t a t i o n of a p r o b a b i l i t y d i s t r i b u t i o n is called a ' p r o b a b i l i t y m o d e l , ' or si"
W96-0112,J93-2004,0,0.0358587,"G rules as our grammar to be used by a parser which calculates a preference for each partial interpretation, and always retains the N most preferable partial interpretations 7. We have not yet actually constructed such a parser, however, and use a parser called 'SAX,' previously developed by Matsumoto & Sugimura (Matsumoto and Sugimura, 1986), which calculates a preference for each interpretation after it obtains each interpretation. We then trained the parameters of probability models. We extracted 181,250 case frames from the WSJ (Wall Street Journal) bracketed corpus of the Penn Tree Bank (Marcus et al., 1993). We used these data to estimate three-word probabilities and two-word probabilities• Furthermore, we extracted 963 sentences from the WSJ tagged corpus of the Penn Tree Bank. We used SAX to analyze the sentences and selected the correct syntactic trees by hand. We then employed the Maximum Likelihood Estimator to estimate length probabilities using the selected syntactic trees, e.g., if CFG rule N P ~ NP, P P is applied x times, and among the attachments obtained by applying this rule, xi of them have the lengths of 2 and 3, then the length probability P(2, 31(NP NP, PP)) is estimated as ~ ."
W96-0112,H94-1048,0,0.240214,"theory and statistics. If we could properly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also resolve ambiguities quite well. There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective (Alshawi and Carter, 1995; Black et al., 1992; Briscoe and Carroll. 1993; Chang et al., 1992; Collins and Brooks, 1995; Fujisaki, 1989; Hindle and Rooth, 1991; Hindle and Rooth, 1993; Jelinek et al., 1990; Magerman and Marcus, 1991; Magerman, 1995; Ratnaparkhi et al., 1994; Resnik, 1993; Su and Chang, 1988). Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily. We feel that it is necessary to devise a new method that unifies the above two approaches, i.e., to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology. Most psycholinguistic principles have 1A r e p r e s e n t a t i o n of a p r o b a b i l i t y d i s t r i b u t i o n is called a ' p r o b a b i l i t y m o d e l , ' or simplely a ' m o d e l . ' 141 been develope"
W96-0112,H93-1054,0,0.166987,"we could properly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also resolve ambiguities quite well. There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective (Alshawi and Carter, 1995; Black et al., 1992; Briscoe and Carroll. 1993; Chang et al., 1992; Collins and Brooks, 1995; Fujisaki, 1989; Hindle and Rooth, 1991; Hindle and Rooth, 1993; Jelinek et al., 1990; Magerman and Marcus, 1991; Magerman, 1995; Ratnaparkhi et al., 1994; Resnik, 1993; Su and Chang, 1988). Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily. We feel that it is necessary to devise a new method that unifies the above two approaches, i.e., to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology. Most psycholinguistic principles have 1A r e p r e s e n t a t i o n of a p r o b a b i l i t y d i s t r i b u t i o n is called a ' p r o b a b i l i t y m o d e l , ' or simplely a ' m o d e l . ' 141 been developed on the basis"
W96-0112,P83-1017,0,0.29042,"oach. In psycholinguistics, a number of principles have been proposed which attempt to modelize the human disambiguation process. The Lexical Preference Rule (LPR) (Ford et al., 1982), the Right Association Principle (RAP) (Kimball, 1973), and the Attach Low and Parallel Principle (ALPP, an extension of RAP) (Hobbs and Bear, 1990) have been proposed, and it is thought that we might resolve ambiguities quite satisfactorily if we could implement these principles sufficiently (Hobbs and Bear, 1990; Whittemore et al., 1990). Methods of implementing these principles have also been proposed (e.g., (Shieber, 1983; Wermter, 1989; Wilks et al., 1985)). An alternative approach is to view language as a stochastic phenomenon, particularly from the viewpoint of information theory and statistics. If we could properly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also resolve ambiguities quite well. There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective (Alshawi and Carter, 1995; Black et al., 1992; Briscoe and Carroll. 1993; Chang et al., 1992"
W96-0112,C88-2133,0,0.03453,"erly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also resolve ambiguities quite well. There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective (Alshawi and Carter, 1995; Black et al., 1992; Briscoe and Carroll. 1993; Chang et al., 1992; Collins and Brooks, 1995; Fujisaki, 1989; Hindle and Rooth, 1991; Hindle and Rooth, 1993; Jelinek et al., 1990; Magerman and Marcus, 1991; Magerman, 1995; Ratnaparkhi et al., 1994; Resnik, 1993; Su and Chang, 1988). Although each of the disambiguation methods proposed to date has its merits, none resolves the disambiguation problem completely satisfactorily. We feel that it is necessary to devise a new method that unifies the above two approaches, i.e., to implement psycholinguistic principles of disambiguation on the basis of a probabilistic methodology. Most psycholinguistic principles have 1A r e p r e s e n t a t i o n of a p r o b a b i l i t y d i s t r i b u t i o n is called a ' p r o b a b i l i t y m o d e l , ' or simplely a ' m o d e l . ' 141 been developed on the basis of a vast data base"
W96-0112,P90-1004,0,0.133216,"uction of such a system is extremely difficult, however, and we need to adopt a more realistic approach. In psycholinguistics, a number of principles have been proposed which attempt to modelize the human disambiguation process. The Lexical Preference Rule (LPR) (Ford et al., 1982), the Right Association Principle (RAP) (Kimball, 1973), and the Attach Low and Parallel Principle (ALPP, an extension of RAP) (Hobbs and Bear, 1990) have been proposed, and it is thought that we might resolve ambiguities quite satisfactorily if we could implement these principles sufficiently (Hobbs and Bear, 1990; Whittemore et al., 1990). Methods of implementing these principles have also been proposed (e.g., (Shieber, 1983; Wermter, 1989; Wilks et al., 1985)). An alternative approach is to view language as a stochastic phenomenon, particularly from the viewpoint of information theory and statistics. If we could properly define a probability model 1 and calculate the likelihood value of each interpretation using the model, we might also resolve ambiguities quite well. There have been a number of methods proposed to perform structural disambiguation using probability models, many of which have proved to be quite effective (Als"
W96-0112,J94-4005,0,\N,Missing
W96-0112,J98-2002,1,\N,Missing
W96-0112,H92-1026,0,\N,Missing
W96-0112,P95-1037,0,\N,Missing
W96-0112,P93-1005,0,\N,Missing
W96-0112,1991.iwpt-1.22,0,\N,Missing
