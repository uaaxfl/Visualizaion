1999.mtsummit-1.54,C94-1020,1,0.828788,"Missing"
1999.mtsummit-1.54,1995.tmi-1.27,0,0.089845,"Missing"
1999.mtsummit-1.54,J93-2003,0,\N,Missing
1999.mtsummit-1.54,C96-2185,1,\N,Missing
1999.tmi-1.23,C96-1030,0,\N,Missing
1999.tmi-1.23,J93-2003,0,\N,Missing
1999.tmi-1.23,C94-1020,1,\N,Missing
1999.tmi-1.23,1995.tmi-1.21,0,\N,Missing
2001.mtsummit-papers.35,bohan-etal-2000-evaluating,0,0.165096,"Missing"
2001.mtsummit-papers.35,1995.mtsummit-1.35,0,0.366415,"and a comprehension test to see their practicability. 2. Construction of Test-sets 2.1 Characteristics of Test-sets Two important problems concerning the construction of the test-sets were coverage and objectivity. In order to collect examples that cover a variety of linguistic phenomena, we initially classified linguistic phenomena, which will be described later. And we attempted to collect a variety of examples that can be assigned to specific linguistic phenomena. In order to perform an objective evaluation method, we prepared one yes/no question for one example sentence. As pointed out in Isahara (1995), this yes/no question about a linguistic phenomenon enabled us to evaluate MT systems objectively. This objectivity can be a basis of a fair comparison between MT systems. One example of our test-sets is presented in figure 1. [Serial Id]100 [Grammar Id] 10102080000 [English] August 15 is an unforgettable day to us Koreans. [Korean] 8wol 15il-eun uri hangugin-egenun ijeul su eobs-neun nal-ida. (August 15-TOPIC we Korean DATIVE-TOPIC forgettable-NEG-MOD day-FIN. [Question] Are two nouns in &quot;us Korean&quot; translated into an appositive? [Source] English High School Textbook- ii-a-1 Figure 1. A Samp"
2001.mtsummit-papers.35,1995.mtsummit-1.37,0,0.10801,"Missing"
2018.gwc-1.27,choi-etal-2004-korean,1,0.598636,"’ and ‘refinery’ is closer to the actual semantic distance centered on the homonym of ‘plant’. To solve this problem, several papers (Neelakantan et al., 2015; Rothe and Schütze, 2015) have been published that learn word embedding by the actual meaning of words using a method is called multisense word embedding. We learn multi-sense word embedding using a WSD module to distinguish the meaning of words in advance. Our WSD module is based on the unsupervised learning approach and uses the Markov Random Field (MRF) algorithm which resolves the ambiguity based on the semantic category of CoreNet (Choi et al., 2004). In MRF, the node is composed of common noun, verb, and adjective, and the edge between the nodes is set as long as the distance is only one on the dependency path, in a similar way to this paper (Chaplot et al., 2015). Figure 5. Example of Tokenization The tokenization example for the input sentence is shown in Figure 5. The second word ‘leave’ is tokenized with a POS tag and a sense number. In addition, to make a word embedding suitable for relation extraction, the multiword entity was grouped into one token. As shown in Figure 5, ‘Man Utd’ and ‘Wayne Rooney’, a multiword entity, was bundle"
2018.gwc-1.27,P11-1055,0,0.0204866,"abeled data using the distant supervision assumption. Figure 1. Example of labeled data collection based on distant supervision The distant supervision method is relatively efficient in that it automatically generates training/labeled data between a large corpus and a large knowledge base, but the veracity of the labeled data is sometimes ambiguous. As shown in Figure 1, among the collected sentences that contain both ‘Facebook’ and ‘Mark Zuckerberg’, the first sentence means that Mark Zuckerberg is a founder of Facebook, but the second sentence does not. Various studies (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) have been introduced to solve this problem. However, they use traditional natural language processing (NLP) features such as part of speech (POS) tagging and dependency tree, so the errors occurring in NLP tools propagate to the relation extraction system. Therefore, these papers (Kim, 2014; Zeng et al., 2014) proposed a relation extraction system that used word embedding and deep neural network (DNN) approaches without the above NLP features, and showed improved performance than previous studies. Especially, the piecewise max pooling convolution neural network (PCNN)"
2018.gwc-1.27,D14-1181,0,0.0257466,"is sometimes ambiguous. As shown in Figure 1, among the collected sentences that contain both ‘Facebook’ and ‘Mark Zuckerberg’, the first sentence means that Mark Zuckerberg is a founder of Facebook, but the second sentence does not. Various studies (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) have been introduced to solve this problem. However, they use traditional natural language processing (NLP) features such as part of speech (POS) tagging and dependency tree, so the errors occurring in NLP tools propagate to the relation extraction system. Therefore, these papers (Kim, 2014; Zeng et al., 2014) proposed a relation extraction system that used word embedding and deep neural network (DNN) approaches without the above NLP features, and showed improved performance than previous studies. Especially, the piecewise max pooling convolution neural network (PCNN) model introduced in (Zeng et al., 2015) transforms the convolution neural network (CNN) model into a form more suitable for relation extraction task. However, these studies have a disadvantage in not reflecting the sense of words in word embedding. For example, the word ‘bow’ could be divided into various meanings"
2018.gwc-1.27,P09-1113,0,0.102705,"Missing"
2018.gwc-1.27,P15-1173,0,0.0254103,"multi-sense word embedding ‘apple-fruit’ and ‘apple-company’. Thus, the triangle inequality problem (Neelakantan et al., 2015) can occur. ????????(?, ?) ≤ ????????(?, ?) + ????????(?, ?) For example, there is a problem that the distance between ‘(a) pollen – (c) refinery’ is smaller than the sum of the distances between ‘(a) pollen – (b) plant’ and ‘(c) refinery – (b) plant’. In other words, the similarity between the two words ‘pollen’ and ‘refinery’ is closer to the actual semantic distance centered on the homonym of ‘plant’. To solve this problem, several papers (Neelakantan et al., 2015; Rothe and Schütze, 2015) have been published that learn word embedding by the actual meaning of words using a method is called multisense word embedding. We learn multi-sense word embedding using a WSD module to distinguish the meaning of words in advance. Our WSD module is based on the unsupervised learning approach and uses the Markov Random Field (MRF) algorithm which resolves the ambiguity based on the semantic category of CoreNet (Choi et al., 2004). In MRF, the node is composed of common noun, verb, and adjective, and the edge between the nodes is set as long as the distance is only one on the dependency path,"
2018.gwc-1.27,D12-1042,0,0.0329638,"istant supervision assumption. Figure 1. Example of labeled data collection based on distant supervision The distant supervision method is relatively efficient in that it automatically generates training/labeled data between a large corpus and a large knowledge base, but the veracity of the labeled data is sometimes ambiguous. As shown in Figure 1, among the collected sentences that contain both ‘Facebook’ and ‘Mark Zuckerberg’, the first sentence means that Mark Zuckerberg is a founder of Facebook, but the second sentence does not. Various studies (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) have been introduced to solve this problem. However, they use traditional natural language processing (NLP) features such as part of speech (POS) tagging and dependency tree, so the errors occurring in NLP tools propagate to the relation extraction system. Therefore, these papers (Kim, 2014; Zeng et al., 2014) proposed a relation extraction system that used word embedding and deep neural network (DNN) approaches without the above NLP features, and showed improved performance than previous studies. Especially, the piecewise max pooling convolution neural network (PCNN) model introduced in (Zen"
2018.gwc-1.27,C14-1220,0,0.0845575,"Missing"
2018.gwc-1.27,D15-1203,0,0.0861415,"12) have been introduced to solve this problem. However, they use traditional natural language processing (NLP) features such as part of speech (POS) tagging and dependency tree, so the errors occurring in NLP tools propagate to the relation extraction system. Therefore, these papers (Kim, 2014; Zeng et al., 2014) proposed a relation extraction system that used word embedding and deep neural network (DNN) approaches without the above NLP features, and showed improved performance than previous studies. Especially, the piecewise max pooling convolution neural network (PCNN) model introduced in (Zeng et al., 2015) transforms the convolution neural network (CNN) model into a form more suitable for relation extraction task. However, these studies have a disadvantage in not reflecting the sense of words in word embedding. For example, the word ‘bow’ could be divided into various meanings such as ‘baU – greeting’ and ‘boU – archer’s weapon’. Therefore, if a relation extraction model is learned with lexical ambiguity, it may result in not properly reflecting the characteristics of the homograph. Thus, it is necessary to apply multi-sense word embedding to the relation extraction model. However, to the best"
2020.coling-main.147,E17-1057,0,0.0567026,"Missing"
2020.coling-main.147,C18-1283,0,0.0272415,"uth scoring task by up to 0.05 AUC-ROC than the existing negative sampling methods. (4) A novel evaluation dataset for a fact checking problem, which comprises the factual statements missing in a knowledge graph. It makes our dataset more suitable than the existing datasets for evaluating the ability to validate additional facts missing in a knowledge graph. 2 Related Work Approaches for truth scoring can be broadly classified into two types: (1) approaches that use unstructured textual data to find supporting evidential sentences for a given statement (Gerber et al., 2015; Syed et al., 2018; Thorne and Vlachos, 2018), and (2) approaches that use a knowledge graph to find supporting evidential paths for a given statement (Shiralkar et al., 2017; Syed et al., 2019; Ortona et al., 2018; Shi and Weninger, 2016). The latter approaches are more relevant to our approach presented herein. There are mainly three sub-types of work using a knowledge graph for truth scoring: (2-1) The first type of works uses only positive evidential paths for truth scoring. For example, the factual statement (Leonardo da Vinci, knownFor, Mona Lisa) will be truthful if there is the supporting positive evidential fact (Leonardo da Vin"
2020.coling-main.147,C18-2005,1,0.88157,"Missing"
2020.coling-main.147,2020.lrec-1.27,1,0.799555,"Missing"
2020.lrec-1.27,D14-1164,0,0.0314973,"Missing"
2020.lrec-1.27,L16-1323,0,0.118998,"ercome the drawbacks of expert-driven training data generation, crowdsourcing-based data collection has recently been increasingly applied. With the crowdsourcing method, a vast amount of data is gathered from an undefined large group of the general public and refined to be used as input data for training deep learning neural network models. In particular, Snow et al. (Snow et al., 2008) showed that, for many NLP tasks, crowdsourced data is as good as or better than that annotated by experts. Many previous studies have used shared data or publicized their crowdsourcing data (Liu et al., 2016; Chamberlain et al., 2016; Demartini et al., 2012), but they have the drawback of lacking a corpus that enables a sequential training and evaluation process consisting of entity linking, coreference resolution, and relation extraction from the same document. Although those data were constructed in the same language (i.e., English), they performed tests and evaluations with data collected from different document in each step, such as entity linking from a twitter data and relation extraction from Wikipedia or the new york times corpus. This makes it difficult to analyze the overall knowledge extraction performance acco"
2020.lrec-1.27,P16-1061,0,0.0274918,"Missing"
2020.lrec-1.27,D17-1284,0,0.0237132,"ple”] is given as the mention set M of this text. Then, the entity linking task is to correctly link m “Apple” to “Apple (company)” instead of “Apple (fruit).” This task is conducted in two sub-steps. First, the entity linking model detects entity candidates that may match the mentions to be linked in the knowledge base. Then, the entity to be linked is searched from the set of entity candidates through a comparison of the candidates’ linking scores. Recent models are related to word embedding, context word embedding, and entity embedding implemented with the output of the entity description (Gupta et al., 2017); modeling a mention set assuming latent relations among entities (Le and Titov, 2018); and a mixed model of jointly learning mention detection and entity linking (Kolitsas et al., 2018). 2.2. Coreference Resolution Because a previously mentioned word (antecedent) is usually referred to in various other forms in a natural language (e.g., a pronoun, demonstrative determiner, or abbreviation), understanding the text is necessary to verify whether such expressions are referring to the co-referring entity. This process of grouping all expressions referring to the co-referring entity is termed core"
2020.lrec-1.27,D11-1072,0,0.0135338,"Missing"
2020.lrec-1.27,P11-1055,0,0.167461,"Missing"
2020.lrec-1.27,C16-1139,0,0.0378362,"Missing"
2020.lrec-1.27,K18-1050,0,0.0124626,"in two sub-steps. First, the entity linking model detects entity candidates that may match the mentions to be linked in the knowledge base. Then, the entity to be linked is searched from the set of entity candidates through a comparison of the candidates’ linking scores. Recent models are related to word embedding, context word embedding, and entity embedding implemented with the output of the entity description (Gupta et al., 2017); modeling a mention set assuming latent relations among entities (Le and Titov, 2018); and a mixed model of jointly learning mention detection and entity linking (Kolitsas et al., 2018). 2.2. Coreference Resolution Because a previously mentioned word (antecedent) is usually referred to in various other forms in a natural language (e.g., a pronoun, demonstrative determiner, or abbreviation), understanding the text is necessary to verify whether such expressions are referring to the co-referring entity. This process of grouping all expressions referring to the co-referring entity is termed coreference resolution. It plays an important role in knowledge extraction because entity linking alone is insufficient for catching all information. For example, consider knowledge extracti"
2020.lrec-1.27,P18-1148,0,0.0544495,"correctly link m “Apple” to “Apple (company)” instead of “Apple (fruit).” This task is conducted in two sub-steps. First, the entity linking model detects entity candidates that may match the mentions to be linked in the knowledge base. Then, the entity to be linked is searched from the set of entity candidates through a comparison of the candidates’ linking scores. Recent models are related to word embedding, context word embedding, and entity embedding implemented with the output of the entity description (Gupta et al., 2017); modeling a mention set assuming latent relations among entities (Le and Titov, 2018); and a mixed model of jointly learning mention detection and entity linking (Kolitsas et al., 2018). 2.2. Coreference Resolution Because a previously mentioned word (antecedent) is usually referred to in various other forms in a natural language (e.g., a pronoun, demonstrative determiner, or abbreviation), understanding the text is necessary to verify whether such expressions are referring to the co-referring entity. This process of grouping all expressions referring to the co-referring entity is termed coreference resolution. It plays an important role in knowledge extraction because entity"
2020.lrec-1.27,N18-2108,0,0.0241515,"Missing"
2020.lrec-1.27,N16-1104,0,0.0701476,"e-consuming. To overcome the drawbacks of expert-driven training data generation, crowdsourcing-based data collection has recently been increasingly applied. With the crowdsourcing method, a vast amount of data is gathered from an undefined large group of the general public and refined to be used as input data for training deep learning neural network models. In particular, Snow et al. (Snow et al., 2008) showed that, for many NLP tasks, crowdsourced data is as good as or better than that annotated by experts. Many previous studies have used shared data or publicized their crowdsourcing data (Liu et al., 2016; Chamberlain et al., 2016; Demartini et al., 2012), but they have the drawback of lacking a corpus that enables a sequential training and evaluation process consisting of entity linking, coreference resolution, and relation extraction from the same document. Although those data were constructed in the same language (i.e., English), they performed tests and evaluations with data collected from different document in each step, such as entity linking from a twitter data and relation extraction from Wikipedia or the new york times corpus. This makes it difficult to analyze the overall knowledge e"
2020.lrec-1.27,P09-1113,0,0.0388105,"Missing"
2020.lrec-1.27,C18-2005,1,0.412477,"used for DS collection had short English labels as defined in the ontology schema. Because the property could be interpreted differently by different workers, we provided it in the format of a natural language yes/no question. The meaning of each property was taken from the Wikidata description information. For example, we converted birthPlace to “birth location of a person, animal or fictional character.” We paid 0.4$ per hit (15 questions on average) for the work in this phase, and each set was assigned to one worker. 4. Resources We built crowdsourcing data from Korean Wikipedia and KBox (Nam et al., 2018). As far as we know, Amazon Mechanical Turk and Crowdflower are widely used in English. However, there is not a Korean related work there, and it is not easy to gather Korean worker. Therefore, we use Crowdworks1 platform, which is the representative of Korean crowdsourcing companies. Ontological knowledge extraction requires a reference knowledge base. This knowledge base is indispensable for entity linking and relation extraction and plays a pivotal role in extracting correct information that meets the schema of the knowledge base. For the reference knowledge base, we selected KBox (Nam et a"
2020.lrec-1.27,P14-2119,0,0.0493172,"Missing"
2020.lrec-1.27,N18-1202,0,0.0149023,"Missing"
2020.lrec-1.27,W12-4501,0,0.108161,"Missing"
2020.lrec-1.27,P18-1199,0,0.0218008,"Missing"
2020.lrec-1.27,D08-1027,0,0.272331,"Missing"
2020.lrec-1.27,P19-1279,0,0.029496,"Missing"
2020.lrec-1.27,D12-1042,0,0.123206,"Missing"
2020.lrec-1.27,N16-1114,0,0.0289269,"Missing"
2020.lrec-1.27,D17-1187,0,0.0263724,"Missing"
2020.lrec-1.27,D15-1203,0,0.0407111,"Missing"
2020.lrec-1.27,P12-1087,0,0.031881,"Missing"
2020.lrec-1.27,P16-2034,0,0.0180409,"Missing"
2020.lrec-1.30,P98-1013,0,0.19547,"t or translated definitions. We then evaluated whether the crowdsourced results accurately captured the meaning of frames both cross-culturally and cross-linguistically, and found that by allowing the crowd workers to make intuitive choices, they achieved a quality comparable to that of trained FrameNet experts (F1 &gt; 0.75). The outcomes of this work are now publicly available as a new release of Korean FrameNet 1.1. Keywords: FrameNet, Multilingual, Crowdsourcing 1. Introduction FrameNet is a large linguistic resource in which the meaning of text is represented using Frame Semantics (frames) (Baker et al., 1998; Fillmore et al., 2003). An English language version of FrameNet was created by the FrameNet Project at the International Computer Science Institute1 (ICSI), which defined English frames and assigned them to English vocabularies in order to construct lexical units (LUs) consisting of a word and frame pair. During annotation, the process of which was formalized by Ruppenhofer et al. (2006), annotators chose a proper frame (framing) for each target word and corresponding frame elements for the arguments. Since being developed, FrameNet has become a useful resource in a variety of fields, and re"
2020.lrec-1.30,C18-3003,0,0.339918,"Science Institute1 (ICSI), which defined English frames and assigned them to English vocabularies in order to construct lexical units (LUs) consisting of a word and frame pair. During annotation, the process of which was formalized by Ruppenhofer et al. (2006), annotators chose a proper frame (framing) for each target word and corresponding frame elements for the arguments. Since being developed, FrameNet has become a useful resource in a variety of fields, and researchers are now showing increasing interest in not only English FrameNet, but also in multilingual FrameNets for other languages (Baker et al., 2018; Borin et al., 2010; Meurs et al., 2008; sub, 2003; Burchardt et al., 2006; Yang et al., 2018; Virk and Prasad, 2018). However, despite this interest, the construction of multilingual FrameNets remains challenging due to the expense and complexity of annotating over raw sentences, including those in English. As an alternative, crowdsourcing can be used to construct multilingual FrameNets. Several studies have investigated the viability of constructing FrameNets via crowdsourcing and found that annotations obtained via a crowd of 10 people are reliable when compared to annotations provided by"
2020.lrec-1.30,burchardt-etal-2006-salsa,0,0.198453,"Missing"
2020.lrec-1.30,candito-etal-2014-developing,0,0.0441876,"Missing"
2020.lrec-1.30,P13-2130,0,0.227532,"t al., 2010; Meurs et al., 2008; sub, 2003; Burchardt et al., 2006; Yang et al., 2018; Virk and Prasad, 2018). However, despite this interest, the construction of multilingual FrameNets remains challenging due to the expense and complexity of annotating over raw sentences, including those in English. As an alternative, crowdsourcing can be used to construct multilingual FrameNets. Several studies have investigated the viability of constructing FrameNets via crowdsourcing and found that annotations obtained via a crowd of 10 people are reliable when compared to annotations provided by experts (Fossati et al., 2013; Dumitrache et al., 2018). Similar studies have also been conducted for non-English languages. For example, Ohara et al. (2018) employed a Kyoto case frame dictionary when performing framing via crowdsourcing on Japanese sentences. In terms of multilingual FrameNets, while a few studies have explored cross-lingual issues for target-languages, such as the absence of a suitable English frame for target-language words (Candito et al., 1 http://framenet.icsi.berkeley.ed 2014; Pedersen et al., 2018; Ohara et al., 2018; NeˇsporeB¯erzkalne et al., 2018), to the best of our knowledge, no studies to d"
2020.lrec-1.30,I05-6002,0,0.0632767,"ing the stimulus that provoked an emotion. In (10), an event when the boys yelled provoked an emotion be surprised. For the emotional frames, the crowd workers chose more specific frames and considered the context of sentences that focused on the experience or stimulus. 5. Discussions 1) Why are some transferred FrameNet annotations invalid? Many multilingual FrameNet studies use the transferring approach, which utilizes English FrameNet as a seed and transfers it to the target-language FrameNet. Translating English lexical units (LUs) into the target-language by using a bilingual dictionary (Kanamaru et al., 2005; Borin et al., 2010) and/or a bilingual corpus (Park et al., 2014; Kim et al., 2016; Pedersen et al., 2018; Yang et al., 2018) would be considered as a starting point of target-language words, and its frame. Generally, these works are based on the assumption that the translated target-language words have the same frames as the original-language words. However, because of the differences in culture and linguistics (Baker et al., 2018), frames are shown in different manners in bilingual corpora (Torrent et al., 2018). More specifically, even if a pair of sentences in a bilingual corpus are sema"
2020.lrec-1.30,C16-2037,1,0.919031,"tion (e.g., predicate-argument structure) to non-native English speaking crowd workers, nor have there been any studies that evaluated whether crowdsourced annotations effectively capture the ICSI meaning of English frames both cross-culturally and cross-linguistically. This is important because even for parallel sentences that are translations of each other, the corresponding frames may differ for cultural and/or linguistic reasons (Baker et al., 2018; Torrent et al., 2018). The focus of the current study was to address these issues in the context of Korean FrameNet (KFN) (Park et al., 2014; Kim et al., 2016), which was created using the transferring approach by translating existing FrameNets and using the original frame annotations as the original-language sentences. In this approach, parallel sentences have the same frame annotations for each aligned word. In this paper, the term transferred annotation is used to refer to the original frame annotation of translated words. It should be noted that some transferred annotations are not valid as they do not consider cross-lingual issues in translation, such as cases where a translation changes the meanings of words. For example, in KFN, while the gen"
2020.lrec-1.30,meurs-etal-2008-semantic,0,0.0119331,"English frames and assigned them to English vocabularies in order to construct lexical units (LUs) consisting of a word and frame pair. During annotation, the process of which was formalized by Ruppenhofer et al. (2006), annotators chose a proper frame (framing) for each target word and corresponding frame elements for the arguments. Since being developed, FrameNet has become a useful resource in a variety of fields, and researchers are now showing increasing interest in not only English FrameNet, but also in multilingual FrameNets for other languages (Baker et al., 2018; Borin et al., 2010; Meurs et al., 2008; sub, 2003; Burchardt et al., 2006; Yang et al., 2018; Virk and Prasad, 2018). However, despite this interest, the construction of multilingual FrameNets remains challenging due to the expense and complexity of annotating over raw sentences, including those in English. As an alternative, crowdsourcing can be used to construct multilingual FrameNets. Several studies have investigated the viability of constructing FrameNets via crowdsourcing and found that annotations obtained via a crowd of 10 people are reliable when compared to annotations provided by experts (Fossati et al., 2013; Dumitrach"
2020.lrec-1.30,L18-1378,0,0.0471223,"ions obtained via a crowd of 10 people are reliable when compared to annotations provided by experts (Fossati et al., 2013; Dumitrache et al., 2018). Similar studies have also been conducted for non-English languages. For example, Ohara et al. (2018) employed a Kyoto case frame dictionary when performing framing via crowdsourcing on Japanese sentences. In terms of multilingual FrameNets, while a few studies have explored cross-lingual issues for target-languages, such as the absence of a suitable English frame for target-language words (Candito et al., 1 http://framenet.icsi.berkeley.ed 2014; Pedersen et al., 2018; Ohara et al., 2018; NeˇsporeB¯erzkalne et al., 2018), to the best of our knowledge, no studies to date have evaluated the effectiveness of unifying the crowdsourcing settings that specify the information provided to workers. Instead, each crowdsourced approach has employed different settings to guide crowd workers during framing tasks, such as the definition and corresponding exemplar sentences for a particular frame. Also, no studies have evaluated the benefit or lack thereof of providing English ICSI frame definitions or additional semantic information (e.g., predicate-argument structure)"
2020.lrec-1.30,L18-1139,0,0.107025,"es in order to construct lexical units (LUs) consisting of a word and frame pair. During annotation, the process of which was formalized by Ruppenhofer et al. (2006), annotators chose a proper frame (framing) for each target word and corresponding frame elements for the arguments. Since being developed, FrameNet has become a useful resource in a variety of fields, and researchers are now showing increasing interest in not only English FrameNet, but also in multilingual FrameNets for other languages (Baker et al., 2018; Borin et al., 2010; Meurs et al., 2008; sub, 2003; Burchardt et al., 2006; Yang et al., 2018; Virk and Prasad, 2018). However, despite this interest, the construction of multilingual FrameNets remains challenging due to the expense and complexity of annotating over raw sentences, including those in English. As an alternative, crowdsourcing can be used to construct multilingual FrameNets. Several studies have investigated the viability of constructing FrameNets via crowdsourcing and found that annotations obtained via a crowd of 10 people are reliable when compared to annotations provided by experts (Fossati et al., 2013; Dumitrache et al., 2018). Similar studies have also been conduc"
2020.paclic-1.25,W18-5503,0,0.0632011,"Missing"
2020.paclic-1.25,N06-2015,0,0.220931,"Missing"
2020.paclic-1.25,D19-1588,0,0.0458678,"Missing"
2020.paclic-1.25,P16-1101,0,0.110295,"Missing"
2020.paclic-1.25,W18-6013,1,0.812314,"gold set can only be performed on a limited part of the evaluation set sharing the same part of raw corpus to be annotated. notation: morphological analysis, lexical sense analysis, named entity analysis, subject anaphora resolution, co-reference resolution, dependency analysis, and semantic roles analysis. The evaluation sets are also constructed by the same layers. 2 In Korean, the word segment divided by white space is called ”Eojeol”, this is composed of a noun or verb stem combined with a postposition (”Josa”) or ending (”Eomi”) that function as inflectional and derivational particles. (Noh et al., 2018) 3 In this project, we are constructed 7 linguistic layers of corpus annotations as gold set to validate 7 linguistic layers of corpus annotation (evaluation set) constructed by other project groups. The evaluation sets after validation can be downloaded at https://corpus.korean.go.kr/. Figure 1: The flow of the corpus annotation and validation process in this paper. Blue-coloureds indicate our process and result, and green-coloureds indicate evaluatee group’s process and result. That’s why we present an additional method to validate in the range of evaluation set without a gold set. Thus, we"
2020.paclic-1.25,N19-1176,0,0.053472,"Missing"
C02-1086,J90-1003,0,0.0662542,"1996; Eichmann et al, 1998; Yang et al, 1998; Jang et al, 1999; Chun, 2000) based on bilingual dictionaries, multilingual ontology or thesaurus are much more practical. Many researches adopt dictionary-based query translation because it is simpler and practical, given the wide availability of bilingual or multilingual dictionaries. In order to achieve a high performance CLIR using dictionary-based query translation, however, it is necessary to solve the problem of increased ambiguities of query terms. One way of resolving query ambiguities is to use the statistics, such as mutual information (Church and Hanks, 1990), to measure associations of query terms, on the basis of existing corpora (Jang et al, 1999). Document clusters, widely adopted in various applications such as browsing and viewing of document results (Hearst and Pedersen, 1996) or topic detection (Allan et al, 1998), also reflect the association of terms and documents. Lee et al (2001) showed that incorporating a document re-ranking method based on document clusters into the vector space retrieval achieved the significant improvement in monolingual IR, as it contributed to resolving ambiguities caused by polysemous query terms. The noise or"
C02-1086,P99-1029,0,0.0662692,"ategories: statistical approaches and translation approaches. Statistical methods establish cross-lingual associations without language translation (Dumais et al, 1997; Rehder et al, 1997; Yang et al, 1998). They require large-scale bilingual corpora. In translation approach, either queries or documents are translated. Though document translation is possible when high quality machine translation systems are available (Kwon et al, 1997; Oard and Hackett, 1997), it is not very practical. Query translation methods (Hull and Grefenstette, 1996; Davis, 1996; Eichmann et al, 1998; Yang et al, 1998; Jang et al, 1999; Chun, 2000) based on bilingual dictionaries, multilingual ontology or thesaurus are much more practical. Many researches adopt dictionary-based query translation because it is simpler and practical, given the wide availability of bilingual or multilingual dictionaries. In order to achieve a high performance CLIR using dictionary-based query translation, however, it is necessary to solve the problem of increased ambiguities of query terms. One way of resolving query ambiguities is to use the statistics, such as mutual information (Church and Hanks, 1990), to measure associations of query term"
C02-1086,1998.amta-tutorials.5,0,\N,Missing
C02-1088,P00-1011,0,0.246401,"Missing"
C02-1088,A97-1029,0,0.611097,"Missing"
C02-1088,M98-1018,0,0.0576915,"Missing"
C02-1088,C00-2167,0,0.0199097,"Missing"
C02-1088,P95-1026,0,0.169801,"g condition is satisfied and the learning is over. We set it to be 3/4 of the number of test examples. The last method is a mixed voting. We use two voting methods mentioned above one after another. First, we use probability voting. After the learning is over we use majority voting. The threshold of the probability voting is 1/2 of the number of test examples here. 2.4 Post-Processing After the learning, the system modifies test examples by using a rule, one sense per discourse. One sense per discourse means that the sense of a target word is highly consistent within any given document. David Yarowsky (1995) showed it was accurate in the word sense disambiguation. We label the examples that are not labeled yet as the category of the labeled word in the discourse as following example and we output named entity tagged corpus. Example after the ensemble learning i Z α (h) Z α (h) = ∑∏α igi ( h , f ) f i ... ... KIA&lt;type=organization&gt; reul ji-won-han-da. KIA neon ... ... after post-processing We use MEMT, Maximum Entropy Modeling Toolkit (Ristad, 1998), to compute the parameter for the features. ... ... KIA&lt;type=organization&gt; reul ji-won-han-da. KIA&lt;type=organization&gt; neon ... ... 2.3.2 3 Combining T"
C02-1088,A97-1030,0,\N,Missing
C02-1088,P00-1042,0,\N,Missing
C02-1088,C00-2124,0,\N,Missing
C02-1088,W99-0613,0,\N,Missing
C02-1097,C96-1005,0,0.51815,"Missing"
C02-1097,J01-3001,0,0.0274655,"Missing"
C02-1097,J98-4002,0,0.0467749,"Missing"
C02-1097,J98-1004,0,0.160369,"target word. They are called dynamic sense vectors because they are changed according to a target word and its context. Finally, a word sense of a target word is determined using static and dynamic sense vectors. The English SENSEVAL test suit is used for this experimentation and our method produces relatively good results. 1 ‘Contextual words’ is defined as a list of content words in context. 2 In this paper, a target word ‘Wt’ is a semantically 1. Introduction It is popular in WSD to use contextual information in training data (Agirre, et al., 1996 3 ; Escudero, et al., 2000; Gruber, 1991; Schutze, 1998). Co-occurring words within a limited window-sized context support one sense among the semantically ambiguous ones of the word. The problem is to find the most effective patterns in order to capture the right sense. It is true that they have similar context and co-occurrence information when words are used with the same sense (Rigau, et al., 1997). It is also true that contextual words nearby an ambiguous word give more effective patterns or features than those far from it (Chen, et al., 1998). In this paper, we represent each sense of a word as a vector in word space. First, contextual words"
C02-1097,P98-1037,0,0.0524887,"Missing"
C02-1097,kilgarriff-rosenzweig-2000-english,0,0.0675454,"Missing"
C02-1097,P97-1007,0,0.379565,"rds’ is defined as a list of content words in context. 2 In this paper, a target word ‘Wt’ is a semantically 1. Introduction It is popular in WSD to use contextual information in training data (Agirre, et al., 1996 3 ; Escudero, et al., 2000; Gruber, 1991; Schutze, 1998). Co-occurring words within a limited window-sized context support one sense among the semantically ambiguous ones of the word. The problem is to find the most effective patterns in order to capture the right sense. It is true that they have similar context and co-occurrence information when words are used with the same sense (Rigau, et al., 1997). It is also true that contextual words nearby an ambiguous word give more effective patterns or features than those far from it (Chen, et al., 1998). In this paper, we represent each sense of a word as a vector in word space. First, contextual words in the training sense tagged data4 are represented as context vectors. Then, ambiguous word in a given context of ‘Wt’. This context may consist of several sentences and it is represented by ‘contextual words’. 3 Agirre et al., (1996) defines a term ‘conceptual density’ based on how many nodes are hit between WordNet node and target words+contexts"
C02-1097,P95-1026,0,0.16604,"cannot contain relevant information consistently (Kilgarriff et al., 2000). Words in this context window6 can be classified into nouns, verbs, and adjectives. The classified words within the context window are assumed to show the co-occurring behaviour with the target word. They provide a supporting vector for a certain sense. Contextual words nearby a target word give more relevant information to decide its sense than those far from it. Distance from a target word is used for this purpose and it is calculated by the assumption that the target words in the context window have the same sense (Yarowsky, 1995). Each word in the training samples can be weighted by formula (1). Let Wij(tk) represent a weighting function for a term tk, which appears in the jth training sample for the ith sense, tfijk 5 POS, collocations, semantic word associations, subcategorization information, semantic roles, selectional preferences and frequency of senses are useful for WSD (Agirre et al., 2001). 6 Since, the length of context window was considered when SENSEVAL-2 lexical sample data were constructed, we use a training sample itself as context window. represent the frequency of a term tk in the jth training sample"
C02-1097,P91-1019,0,\N,Missing
C02-1097,C98-1037,0,\N,Missing
C02-1099,J96-4002,0,0.0129396,"ed into three words : ‘broad’, ‘cast’ and ‘ing’. But from the training corpus and pronunciation dictionary, all of complex word is divided into two words like ‘broad’ and ‘casting’. 4 (www.cs.cmu.edu/~laura/pages/arpabet.ps): ARPAbet symbol will be used for representing phonemes. ARPAbet English b oa r d | | | | Pronunciation /B/ /AO/ /R/ /D/ Table 2. One possible alignment between English word ‘board’ and its pronunciation For automatic EPU-P alignment, we used the modified version of Kang’s E-K alignment algorithm (Kang et al., 2000; Kang et al., 2001). It is based on Covington’s algorithm (Covington, 1996). Covington views an alignment as a way of stepping through two words – a word in one side and a word in the other side – while performing ‘match’ or ‘skip’ operation on each step. Kang added ‘forward bind’ and ‘backward bind’ operations to consider one-to-many, many-to- one and many-to-many alignments Operation Match Condition Penalty Similar C/CP 0 V/VP 0 V/SVP or C/SVP 30 Dissimilar C/CP 240 V/CP or C/VP 250 Bind Similar C/CP 0 V/VP 0 V/SVP or C/SVP 30 Dissimilar C/CP 190 V/CP or C/VP 200 Table 3. Penalty metrics: C, V, CP, VP, and SVP represent consonants, vowels, consonant phonemes, vowel"
C02-1099,kang-choi-2000-automatic,1,0.843158,"Missing"
C02-1099,P97-1017,0,0.242054,"Missing"
C02-1099,J90-2002,0,\N,Missing
C04-1178,W97-0313,0,\N,Missing
C04-1178,W02-2023,0,\N,Missing
C04-1178,W02-1028,0,\N,Missing
C04-1178,P98-2182,0,\N,Missing
C04-1178,C98-2177,0,\N,Missing
C16-2037,P98-1013,0,0.323794,"Missing"
C16-2037,W12-2425,0,0.0698446,"Missing"
C16-2037,tonelli-pianta-2008-frame,0,0.488594,"an be widely used as Semantic Role Labeling training set for Machine Translation, Information Extraction, Event Recognition and etc. Since characteristics of target language must be considered to apply on the other NLP tasks, FrameNets have been developed in several languages. Most FrameNets like Japanese annotated sentences one by one (Ohara et al., 2003), but this procedure requires much time and effort of frame experts. On the other hand, utilizing existing corpus to easily generate FrameNet was also researched. For example, projection algorithm of English frame semantic data into Italian (Tonelli and Pianta, 2008) was suggested. However, comparing to the abundant corpus in English FrameNet, FrameNets in other languages are containing relatively small dataset, or even missing. There was no Korean FrameNet until importing 4025 English FrameNet sentences into Korean using trained translators (Park et al., 2014). The translated sentences with frame information might be used for NLP in Korean with secured quality, but the quantity is rather too small for machine learning training data. Focusing on the lack of frame annotated sentence, this study proposes much cost efficient method for expanding size of Kore"
C18-2005,W16-4409,1,0.78949,"ot achieve satisfactory performance with just one RE module, we have configured an ensemble with multiple RE models. In the relation extraction step, our system considers not only the entities provided by the entity linking system but also the results of named entity recognition (NER) module as the entity. A new entity that does not exist in KBox cannot be identified by entity linking system, therefore we consider the result of NER as a new entity. Of the many types of NER, only three types of Person, Location, and Organization are considered to be new entities. 21 The Pattern-based RE model (Choi et al., 2016) aims to extract knowledge with high reliability. Human annotators use this model to generate patterns using lexical and syntactic features such as POS, dependency tree, and named entity recognition. This model shows a high precision but low recall, and therefore, scalability is a problem. Sentence-level RE consists of both convolutional neural network (CNN) (Nam et al., 2018) and LSTM models to address scalability issues and increase recall. These models use distant supervision (Mintz et al., 2009) as a way to collect training data. Distant supervision assumes to collect all the sentences tha"
C18-2005,K15-1014,1,0.827017,"kaist.ac.kr License details: http:// 20 Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, pages 20–24 Santa Fe, New Mexico, USA, August 20-26, 2018. Figure 1: Architecture of the proposed demo system. quentially by each main parts to extract knowledge and this knowledge is stored in KBox immediately. Details of each part are as follows. 2.1 Preprocessing Preprocessing involves the following three steps in sequence. NLP Tool extracts features such as part-ofspeech (POS) tags, dependency parsing, and named entity of input text. Entity Linking (Kim and Choi, 2015) links entity mentions in the text with their corresponding entities in KBox. This entity linking system consists of two modules: the entity boundary detection module finds out entity candidates from the text using a bidirectional long-short term memory (LSTM) model with inside-outside-beginning (IBO) and POS tags as features, and the entity disambiguation module takes entity candidates extracted from the entity boundary detection module and selects the most appropriate entity candidate. The system uses a support vector machine (SVM) with entity boundary information and semantic relations betw"
C18-2005,L18-1563,1,0.841626,"cally create labeled data between a large-scale KB and a corpus. Both CNN and LSTM models use entity-embedded Korean word embedding as input vectors; the CNN model additionally uses vectors for position and POS. The sentence-level RE model is used to reveal the relation between two entities in a sentence; therefore, it is weak at extracting facts that can be found across sentences (paragraph). One of the differences between Korean and English is the zero anaphora. In Korean, repeated subjects are frequently omitted in the latter sentence. To address this problem, the Paragraph-level RE model (Kim and Choi, 2018), which is useful for estimating omitted subjects and predicting relations, explores the incorporation of global contexts derived from paragraph-into-sentence embedding as a means of compensating for the shortage of training data in distantly supervised RE. This model specifically performs zero subject resolution through entity-relation-based graph analysis to find a central entity. The central entities are selected from each paragraph by calculating the out-degree centrality based on the network model of the entity graph using the knowledge base triples. This allows us to learn RE models for"
C18-2005,P17-1004,0,0.013801,"a(Auer et al., 2007), and YAGO(Suchanek et al., 2007) are widely used in many NLP tasks. These KBs store knowledge in the form of a triple; for example, (Les Miserables, author, Victor Hugo). However, because even large-scale KBs do not contain all the possible knowledge, the knowledge completion task remains crucial in the NLP field. Various approaches can be used for constructing knowledge completion systems, such as knowledge reasoning and extraction. Among them, the task of extracting factual knowledge from unstructured text, such as natural language sentences, is important. In addition, (Lin et al., 2017) mentioned that certain knowledge is described only in a certain language. For example, the Korean Wikipedia contains much information about Korean culture; similarly, the English Wikipedia contains information about English culture. Moreover, as far as we know, no knowledge extraction system is available for all languages. In addition, building a KB for a specific language requires an ontology schema definition and a knowledge extraction system that is appropriate for that language, as if creating a WordNet (Miller et al., 1990) for each language. This paper describes a work-in-progress (demo"
C18-2005,P09-1113,0,0.0251319,"on, Location, and Organization are considered to be new entities. 21 The Pattern-based RE model (Choi et al., 2016) aims to extract knowledge with high reliability. Human annotators use this model to generate patterns using lexical and syntactic features such as POS, dependency tree, and named entity recognition. This model shows a high precision but low recall, and therefore, scalability is a problem. Sentence-level RE consists of both convolutional neural network (CNN) (Nam et al., 2018) and LSTM models to address scalability issues and increase recall. These models use distant supervision (Mintz et al., 2009) as a way to collect training data. Distant supervision assumes to collect all the sentences that contain both entities of a triple. Thus, it is widely used as an effective way to automatically create labeled data between a large-scale KB and a corpus. Both CNN and LSTM models use entity-embedded Korean word embedding as input vectors; the CNN model additionally uses vectors for position and POS. The sentence-level RE model is used to reveal the relation between two entities in a sentence; therefore, it is weak at extracting facts that can be found across sentences (paragraph). One of the diff"
C18-2011,P15-1136,0,0.0259175,"al., 2017; Quirk and Poon, 2017); these are basically done in a way that increases the number of possible paths between the entities present in other sentences by integrating dependency graphs generated in a single sentence. The dependency graph–the key element of these studies–is known to be effective in relation extraction. However, it is difficult to acquire a highly efficient parser for all languages; thus, the practical application cannot extract relationships in various language environments. As another solution, we can apply a pipelined model to first perform a co-reference resolution (Clark and Manning, 2015) or zero-anaphora resolution (Mitkov, 1999) and then perform relation extraction, but error propagation between processes has been pointed to as a common problem in many natural language processing (NLP) tasks (Quirk and Corston-Oliver, 2006; Yang and Cardie, 2013; Han et al., 2013; Zeng et al., 2015). This demo aims to overcome these issues by means of a projection in the context of the paragraph into the relationship between tuples in the KB. A paragraph is a series of sentences that fleshes out a coherent theme and maintains a consistent flow, so if an omitted entity exists, it is clear tha"
C18-2011,P14-1079,0,0.0215853,"and Navigli, 2013), and question-answering (Yahya et al., 2012) has increased, there has been considerable interest in extracting relationships for a large number of documents written in natural language. Relation extraction aims to identify and recognize the semantic relationships between pairs of entities (persons, locations, organizations, etc.) from sentences written in free text and to create them in a structured form. Most studies in relationship extraction are distantly supervised and only take into account intrasentence relationships that contain pairs of entities (Mintz et al., 2009; Fan et al., 2014; Zeng et al., 2015). For example, suppose that the following paragraph is given with entities marked by parentheses:“[Cristiano Ronaldo] was born in Madeira. He plays for the Spanish club [Real Madrid C.F.] and the position is a [Forward].” Although the entity mentions do not occur in the same sentence, these sentences convey the team to which “Cristiano Ronaldo” belongs and his position, but this cannot be inferred from each individual sentence. In particular, it is very common for an entity to be omitted from a sentence in Wikipedia–a popular corpus for relation extraction–because Wikipedia"
C18-2011,Y13-1026,0,0.0432411,"Missing"
C18-2011,J13-3008,0,0.0735571,"Missing"
C18-2011,P09-1113,0,0.0302984,"013), search (Marco and Navigli, 2013), and question-answering (Yahya et al., 2012) has increased, there has been considerable interest in extracting relationships for a large number of documents written in natural language. Relation extraction aims to identify and recognize the semantic relationships between pairs of entities (persons, locations, organizations, etc.) from sentences written in free text and to create them in a structured form. Most studies in relationship extraction are distantly supervised and only take into account intrasentence relationships that contain pairs of entities (Mintz et al., 2009; Fan et al., 2014; Zeng et al., 2015). For example, suppose that the following paragraph is given with entities marked by parentheses:“[Cristiano Ronaldo] was born in Madeira. He plays for the Spanish club [Real Madrid C.F.] and the position is a [Forward].” Although the entity mentions do not occur in the same sentence, these sentences convey the team to which “Cristiano Ronaldo” belongs and his position, but this cannot be inferred from each individual sentence. In particular, it is very common for an entity to be omitted from a sentence in Wikipedia–a popular corpus for relation extraction"
C18-2011,Q17-1008,0,0.018621,"these sentences convey the team to which “Cristiano Ronaldo” belongs and his position, but this cannot be inferred from each individual sentence. In particular, it is very common for an entity to be omitted from a sentence in Wikipedia–a popular corpus for relation extraction–because Wikipedia pages each focus on only one entity in most cases. This is also a very common phenomenon in text that is written in a language that can omit a subject or object even if it is not a Wikipedia article. There have been studies into tackling these constraints on relation extraction in two or more sentences (Peng et al., 2017; Quirk and Poon, 2017); these are basically done in a way that increases the number of possible paths between the entities present in other sentences by integrating dependency graphs generated in a single sentence. The dependency graph–the key element of these studies–is known to be effective in relation extraction. However, it is difficult to acquire a highly efficient parser for all languages; thus, the practical application cannot extract relationships in various language environments. As another solution, we can apply a pipelined model to first perform a co-reference resolution (Clark and"
C18-2011,W06-1608,0,0.0468022,"ency graph–the key element of these studies–is known to be effective in relation extraction. However, it is difficult to acquire a highly efficient parser for all languages; thus, the practical application cannot extract relationships in various language environments. As another solution, we can apply a pipelined model to first perform a co-reference resolution (Clark and Manning, 2015) or zero-anaphora resolution (Mitkov, 1999) and then perform relation extraction, but error propagation between processes has been pointed to as a common problem in many natural language processing (NLP) tasks (Quirk and Corston-Oliver, 2006; Yang and Cardie, 2013; Han et al., 2013; Zeng et al., 2015). This demo aims to overcome these issues by means of a projection in the context of the paragraph into the relationship between tuples in the KB. A paragraph is a series of sentences that fleshes out a coherent theme and maintains a consistent flow, so if an omitted entity exists, it is clear that the reader can This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 48 Proceedings of the 27th International Conference on Computational Linguis"
C18-2011,D12-1035,0,0.0466639,"Missing"
C18-2011,P13-1161,0,0.0134918,"ese studies–is known to be effective in relation extraction. However, it is difficult to acquire a highly efficient parser for all languages; thus, the practical application cannot extract relationships in various language environments. As another solution, we can apply a pipelined model to first perform a co-reference resolution (Clark and Manning, 2015) or zero-anaphora resolution (Mitkov, 1999) and then perform relation extraction, but error propagation between processes has been pointed to as a common problem in many natural language processing (NLP) tasks (Quirk and Corston-Oliver, 2006; Yang and Cardie, 2013; Han et al., 2013; Zeng et al., 2015). This demo aims to overcome these issues by means of a projection in the context of the paragraph into the relationship between tuples in the KB. A paragraph is a series of sentences that fleshes out a coherent theme and maintains a consistent flow, so if an omitted entity exists, it is clear that the reader can This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 48 Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrat"
C18-2011,D15-1203,0,0.0352296,", and question-answering (Yahya et al., 2012) has increased, there has been considerable interest in extracting relationships for a large number of documents written in natural language. Relation extraction aims to identify and recognize the semantic relationships between pairs of entities (persons, locations, organizations, etc.) from sentences written in free text and to create them in a structured form. Most studies in relationship extraction are distantly supervised and only take into account intrasentence relationships that contain pairs of entities (Mintz et al., 2009; Fan et al., 2014; Zeng et al., 2015). For example, suppose that the following paragraph is given with entities marked by parentheses:“[Cristiano Ronaldo] was born in Madeira. He plays for the Spanish club [Real Madrid C.F.] and the position is a [Forward].” Although the entity mentions do not occur in the same sentence, these sentences convey the team to which “Cristiano Ronaldo” belongs and his position, but this cannot be inferred from each individual sentence. In particular, it is very common for an entity to be omitted from a sentence in Wikipedia–a popular corpus for relation extraction–because Wikipedia pages each focus on"
C94-1020,J87-1004,0,\N,Missing
C94-1087,P86-1009,0,\N,Missing
C94-2138,J93-1002,0,0.0383551,"Missing"
C94-2138,H91-1046,0,\N,Missing
C96-1040,P91-1034,0,0.0633685,"Missing"
C96-1041,H92-1022,0,0.0378313,"Missing"
C96-1041,A88-1019,0,0.0216635,"Missing"
C96-1041,J93-2004,0,0.0280948,"Missing"
C96-1041,J94-2001,0,0.0816182,"Missing"
C96-1041,C90-3038,0,0.0656793,"Missing"
C96-1041,J93-2006,0,0.0646272,"Missing"
C96-2185,C96-1041,1,0.874945,"Missing"
C96-2185,J93-2004,0,\N,Missing
C98-1039,A94-1005,0,\N,Missing
C98-1039,C94-1020,1,\N,Missing
C98-1115,J92-4003,0,0.0470488,"Missing"
C98-1115,W97-0107,1,0.835121,"ted by C(w v -4 -4 wq)w,)&apos; Tile l)rocess p~(w,, -4 w~) = Y:w, c(w,, fl[(i,m)fli~(m,J) • m=i+l continues until the entropy of the tra.ining cot-pus becomes the nfinimunl. The frequency of occurrence, C(wi -4 wj), is ca.h:ulated t)y The basis t)robabilities are: f4l~(i,i + 1 ) = p(wi -4 wi+l) fl[(i,i + 1 ) = p ( w i ¢--wi+,) fl,1(i, i) = fli&apos;(i, i) = 1 /Jis(i, ~&apos;(),S&apos;) fl,~(i,i+ 1) = p(L,.(i, i+ 1)) = p(wi ~ wi+l) fliP(i, i + 1 ) = p(Lt(i, i + l)) = p(wi +-- wi+~). fli~(1,EOS) is the sentence probability bec(,,~f -~ w ) = 2A lit&apos;t&apos;le nlore detailed explanation of the expressions can be fmmd in (Lee and Choi, 1997}. 725 -~ w , ~ , ,,;,,,,) &quot;D - = p(Wl,n) y]v(r&apos;l,-,,.)o..(.,, 1 p(wl,,,) a&apos;~(i,j)fl~.(i,j) w h e r e Occ(Wi -4 Wj, &quot;19, &apos;u.&apos;l,n) is 1 if t h e (tet)endency rela, tion, (wi -4 wj), is used in the &quot;D, and 0 otherwise. Similarly, the occurrence frequency of the dependency relation, (wi ~ w5), where tile test corpus contains a total of IV[ words and is composed of S sentences. is computed by P(!,,dw, °~l(i&apos;J)fll(i&apos;J)&quot; 4 Preliminary 3.4 experiments &gt;&apos;,~ We have experimented with three language models, tri-gram model (TRI), bi-gram model (BI), and the proposed model (DEP) on a raw corpus extracted"
C98-1115,J92-1004,0,0.0756352,"Missing"
chae-choi-2000-design,J98-2002,0,\N,Missing
choi-chae-2000-terminology,kang-choi-2000-automatic,1,\N,Missing
E95-1008,J93-1003,0,0.0650911,"Missing"
E95-1008,P93-1024,0,0.0247565,"a Bayesian model for lexical variables induced graceful approximation over unobserved and infrequent events. There are two known methods to deal with the data sparseness problem. They are smoothing and class based methods (Dagan 1992). Smoothing methods (Church and Gale 1991) readjust the distribution of frequencies of word occurrences obtained from sample texts, and verify the distribution through held-out texts. As Dagan (1992) pointed out, however, the values from the smoothing methods closely agree with the probability of a bigram consisting of two independent words. Class based methods (Pereira et al. 1993) approximate the likelihood of unobserved words based on similar words. Dagan and et al. (1992) proposed a non-hierarchical class based method. The two approaches report limited successes of purely experimental nature. This is so because they are based on strong assumptions. In the case of smoothing methods, frequency readjustment is somewhat arbitrary and will not be good for heavily dependent bigrams. As to the class based methods, the notion of similar words differs across different methods, and the association of probabilistic dependency with the similarity (class) of words is too strong t"
E95-1008,P93-1022,0,\N,Missing
hahm-etal-2014-named,W09-3302,0,\N,Missing
hahm-etal-2014-named,P03-2031,0,\N,Missing
hahm-etal-2014-named,H92-1045,0,\N,Missing
hahm-etal-2014-named,D07-1074,0,\N,Missing
hahm-etal-2014-named,C96-1079,0,\N,Missing
hahm-etal-2014-named,P05-1045,0,\N,Missing
I05-1013,A97-1012,0,0.0312044,"ssing unrestricted texts, because of ungrammatical sentences, the unavoidable incompleteness of lexicon and grammar, and other reasons like long sentences. Partial parsing is an alternative technique developed in response to these problems. This technique aims to recover syntactic information eﬃciently and reliably from unrestricted texts by sacriﬁcing completeness and depth of analysis, and relying on local information to resolve ambiguities [1]. Partial parsing techniques can be roughly classiﬁed into two groups. The ﬁrst group of techniques involves partial parsing via ﬁnite state machines [2,3,9,10]. These approaches apply the sequential regular expression recognizer to an input sentence. When multiple rules match an input string at a given position,  This research was supported in part by the Ministry of Science and Technology, the Ministry of Culture and Tourism, and the Korea Science and Engineering Foundation in Korea. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 143–154, 2005. c Springer-Verlag Berlin Heidelberg 2005  144 M.-S. Choi, C.S. Lim, and K.-S. Choi the longest-matching rule is selected. Therefore, these parsers always produce a single best analysis and operate very"
I05-1013,H91-1060,0,0.0921432,"Missing"
I05-1013,P98-1034,0,0.0328435,"1, pp. 143–154, 2005. c Springer-Verlag Berlin Heidelberg 2005  144 M.-S. Choi, C.S. Lim, and K.-S. Choi the longest-matching rule is selected. Therefore, these parsers always produce a single best analysis and operate very fast. In general, these approaches use a hand-written regular grammar. As would be expected, manually writing a grammar is both very time consuming and prone to have inconsistencies. The other group of partial parsing techniques is text chunking, that is, recognition of non-overlapping and non-recursive cores of major phrases (chunks), by using machine learning techniques [4,7,8,13,15,17]. Since Ramshaw and Marcus [15] ﬁrst proposed formulating the chunking task as a tagging task, most chunking methods have followed this word-tagging approach. In base noun phrase chunking, for instance, each word is marked with one of three chunk tags: I (for a word inside an NP), O (for outside of an NP), and B (for between the end of one NP and the start of another) as follows1 : In ( early trading ) in ( Hong Kong ) ( Monday ), ( gold ) was quoted at ( $ 366.50 ) ( an ounce ). InO earlyI tradingI inO HongI KongI MondayB ,O goldI wasO quotedO atO $I 366.50I anB ounceI .O With respect to thes"
I05-1013,W99-0621,0,0.014539,"1, pp. 143–154, 2005. c Springer-Verlag Berlin Heidelberg 2005  144 M.-S. Choi, C.S. Lim, and K.-S. Choi the longest-matching rule is selected. Therefore, these parsers always produce a single best analysis and operate very fast. In general, these approaches use a hand-written regular grammar. As would be expected, manually writing a grammar is both very time consuming and prone to have inconsistencies. The other group of partial parsing techniques is text chunking, that is, recognition of non-overlapping and non-recursive cores of major phrases (chunks), by using machine learning techniques [4,7,8,13,15,17]. Since Ramshaw and Marcus [15] ﬁrst proposed formulating the chunking task as a tagging task, most chunking methods have followed this word-tagging approach. In base noun phrase chunking, for instance, each word is marked with one of three chunk tags: I (for a word inside an NP), O (for outside of an NP), and B (for between the end of one NP and the start of another) as follows1 : In ( early trading ) in ( Hong Kong ) ( Monday ), ( gold ) was quoted at ( $ 366.50 ) ( an ounce ). InO earlyI tradingI inO HongI KongI MondayB ,O goldI wasO quotedO atO $I 366.50I anB ounceI .O With respect to thes"
I05-1013,W95-0107,0,0.131546,"1, pp. 143–154, 2005. c Springer-Verlag Berlin Heidelberg 2005  144 M.-S. Choi, C.S. Lim, and K.-S. Choi the longest-matching rule is selected. Therefore, these parsers always produce a single best analysis and operate very fast. In general, these approaches use a hand-written regular grammar. As would be expected, manually writing a grammar is both very time consuming and prone to have inconsistencies. The other group of partial parsing techniques is text chunking, that is, recognition of non-overlapping and non-recursive cores of major phrases (chunks), by using machine learning techniques [4,7,8,13,15,17]. Since Ramshaw and Marcus [15] ﬁrst proposed formulating the chunking task as a tagging task, most chunking methods have followed this word-tagging approach. In base noun phrase chunking, for instance, each word is marked with one of three chunk tags: I (for a word inside an NP), O (for outside of an NP), and B (for between the end of one NP and the start of another) as follows1 : In ( early trading ) in ( Hong Kong ) ( Monday ), ( gold ) was quoted at ( $ 366.50 ) ( an ounce ). InO earlyI tradingI inO HongI KongI MondayB ,O goldI wasO quotedO atO $I 366.50I anB ounceI .O With respect to thes"
I05-1013,C98-1034,0,\N,Missing
I05-1040,P02-1051,0,0.398681,"n Machine transliteration is an automatic method to generate characters or words in one alphabetical system for the corresponding characters in another alphabetical system. For example, English word data is transliterated into Korean ‘deita’ 1 and Japanese ‘deeta’. Transliteration is used to phonetically translate proper names and technical terms especially from languages in Roman alphabets to languages in non-Roman alphabets such as from English to Korean, Japanese, and Chinese and so on. There has been increasing concern on machine transliteration as an assistant of Machine Translation (MT) [2], [10], mono-lingual information retrieval (MLIR) [8], [11] and cross-lingual information retrieval (CLIR) [6]. In the area of MLIR and CLIR, machine transliteration bridges the gap between a transliterated localized form and its original form by generating all possible transliterated forms from each original form. Especially for CLIR, machine transliteration gives a help to query translation where proper names and technical terms frequently appear in source language queries. In the area of MT, machine transliteration prevents translation failure when translations of 1 In this paper, target la"
I05-1040,J96-1002,0,0.0216036,"’, δt(gp2)= ‘o’, δt(gp3)=‘~’, δt(gp4)=‘~’, and δt(gp5)=‘deu’ for board. Finally, the target language transliteration of board as ‘bodeu’ can be acquired by concatenating the sequence of produced target graphemes. 3 Machine Learning Algorithms for Each Component Function In this section we will describe a way of modeling component functions using three machine learning algorithms (maximum entropy model, decision tree, and memorybased learning). 3.1 Maximum Entropy Model The maximum entropy model (MEM) is a widely used probability model that can incorporate heterogeneous information effectively [3]. In the maximum entropy model, an event ev is usually composed of a target event (te) and a history event (he), say ev=&lt;te, he&gt;. Event ev is represented by a bundle of feature functions, fei(ev), which represent the existence of a certain characteristic in event ev. A feature function is a binary valued function. It is activated (fei(ev)=1) when it meets its activating condition, otherwise it is deactivated (fei(ev)=0) [3]. δp and δt based on the maximum entropy model can be represented as formula (1). History events in each component function are made from the left, right and current context"
I05-1040,2003.mtsummit-papers.17,0,0.693417,"ations of 1 In this paper, target language transliterations are represented with their Romanization form in a quotation mark (‘’) . R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 450 – 461, 2005. © Springer-Verlag Berlin Heidelberg 2005 An Ensemble of Grapheme and Phoneme for Machine Transliteration 451 proper names and technical terms are not registered in a translation dictionary. A machine transliteration system, therefore, may affect the performance of MT, MLIR, and CLIR system. Three machine transliteration models have been studied: called “grapheme2-based transliteration model (ψG)” [7], [8], [9], [11], [12], [13], “phoneme3-based transliteration model (ψP)” [10], [12], and “hybrid transliteration model (ψH)” [2], [4], [12]. ψG and ψP are classified in terms of units to be transliterated. ψG is referred to the direct model because it directly transforms source language graphemes to target language graphemes without any phonetic knowledge of source language words. ψP is called the pivot model because it makes use of phonemes as a pivot during a transliteration process. Therefore ψP usually needs two steps; the first step is to produce phonemes from source language graphemes,"
I05-1040,kang-choi-2000-automatic,1,0.940178,"enerate characters or words in one alphabetical system for the corresponding characters in another alphabetical system. For example, English word data is transliterated into Korean ‘deita’ 1 and Japanese ‘deeta’. Transliteration is used to phonetically translate proper names and technical terms especially from languages in Roman alphabets to languages in non-Roman alphabets such as from English to Korean, Japanese, and Chinese and so on. There has been increasing concern on machine transliteration as an assistant of Machine Translation (MT) [2], [10], mono-lingual information retrieval (MLIR) [8], [11] and cross-lingual information retrieval (CLIR) [6]. In the area of MLIR and CLIR, machine transliteration bridges the gap between a transliterated localized form and its original form by generating all possible transliterated forms from each original form. Especially for CLIR, machine transliteration gives a help to query translation where proper names and technical terms frequently appear in source language queries. In the area of MT, machine transliteration prevents translation failure when translations of 1 In this paper, target language transliterations are represented with their Ro"
I05-1040,C00-1061,0,0.789043,"1 In this paper, target language transliterations are represented with their Romanization form in a quotation mark (‘’) . R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 450 – 461, 2005. © Springer-Verlag Berlin Heidelberg 2005 An Ensemble of Grapheme and Phoneme for Machine Transliteration 451 proper names and technical terms are not registered in a translation dictionary. A machine transliteration system, therefore, may affect the performance of MT, MLIR, and CLIR system. Three machine transliteration models have been studied: called “grapheme2-based transliteration model (ψG)” [7], [8], [9], [11], [12], [13], “phoneme3-based transliteration model (ψP)” [10], [12], and “hybrid transliteration model (ψH)” [2], [4], [12]. ψG and ψP are classified in terms of units to be transliterated. ψG is referred to the direct model because it directly transforms source language graphemes to target language graphemes without any phonetic knowledge of source language words. ψP is called the pivot model because it makes use of phonemes as a pivot during a transliteration process. Therefore ψP usually needs two steps; the first step is to produce phonemes from source language graphemes, and the se"
I05-1040,P97-1017,0,0.321709,"hine transliteration is an automatic method to generate characters or words in one alphabetical system for the corresponding characters in another alphabetical system. For example, English word data is transliterated into Korean ‘deita’ 1 and Japanese ‘deeta’. Transliteration is used to phonetically translate proper names and technical terms especially from languages in Roman alphabets to languages in non-Roman alphabets such as from English to Korean, Japanese, and Chinese and so on. There has been increasing concern on machine transliteration as an assistant of Machine Translation (MT) [2], [10], mono-lingual information retrieval (MLIR) [8], [11] and cross-lingual information retrieval (CLIR) [6]. In the area of MLIR and CLIR, machine transliteration bridges the gap between a transliterated localized form and its original form by generating all possible transliterated forms from each original form. Especially for CLIR, machine transliteration gives a help to query translation where proper names and technical terms frequently appear in source language queries. In the area of MT, machine transliteration prevents translation failure when translations of 1 In this paper, target language"
I05-1040,P04-1021,0,0.109877,"target language transliterations are represented with their Romanization form in a quotation mark (‘’) . R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 450 – 461, 2005. © Springer-Verlag Berlin Heidelberg 2005 An Ensemble of Grapheme and Phoneme for Machine Transliteration 451 proper names and technical terms are not registered in a translation dictionary. A machine transliteration system, therefore, may affect the performance of MT, MLIR, and CLIR system. Three machine transliteration models have been studied: called “grapheme2-based transliteration model (ψG)” [7], [8], [9], [11], [12], [13], “phoneme3-based transliteration model (ψP)” [10], [12], and “hybrid transliteration model (ψH)” [2], [4], [12]. ψG and ψP are classified in terms of units to be transliterated. ψG is referred to the direct model because it directly transforms source language graphemes to target language graphemes without any phonetic knowledge of source language words. ψP is called the pivot model because it makes use of phonemes as a pivot during a transliteration process. Therefore ψP usually needs two steps; the first step is to produce phonemes from source language graphemes, and the second step is to pr"
I05-2012,J93-2003,0,0.0451378,"tural language applications to properly handle technical terms, it is necessary to give information about conceptual units and their meaning including homonym, synonym and domain dependency. In this paper, we propose a term constituent alignment algorithm, which extracts such information from bilingual technical term pairs. In our algorithm, one or more than one English term constituents are regarded as a conceptual unit. Therefore, the main objec2 Related Works One of the well-known alignment techniques is the one based on statistical machine translation models. It was initially proposed by (Brown et al., 1993) and, more recently, have been intensively studied by several research groups (Germann et al., 2001; Och et al., 2003). It is used for finding sentence, phrase, and word-level correspondences from parallel texts. It can be formulated as equation (1). For the give source text, S, it finds the most probable alignment set, A, and target text, T. (1) p(T |S ) = ∑ p (T , a |S ) a∈A Brown (Brown et al., 1993) proposed five alignment models, called IBM Model, for an English-French alignment task based on equa68 constituents are ‘jak-mul’, ‘seng-jang’, and ‘yul’ where the first two are a noun and the"
I05-2012,P96-1018,0,\N,Missing
I05-2012,J93-1006,0,\N,Missing
I05-2012,J03-1002,0,\N,Missing
I05-2012,P93-1002,0,\N,Missing
I05-2012,P01-1030,0,\N,Missing
J96-3007,J93-1002,0,0.0242609,"Missing"
J96-3007,H91-1046,0,0.0314729,"an be done more efficiently with charts. 1. Introduction Though hidden Markov models have been successful in some applications such as corpus tagging, they are limited to the problems of regular languages. There have been attempts to associate probabilities with context-free grammar formalisms. Recently Briscoe and Carroll (1993) have reported work on generalized probabilistic LR parsing, and others have tried different formalisms such as LTAG (Schabes, Roth, and Osborne 1993) and Link grammar (Lafferty, Sleator, and Temperley 1992). Kupiec extended a SCFG that worked on CNF to a general CFG (Kupiec 1991). The re-estimation algorithm presented in this paper may be seen as another version for general CFG. One significant problem of most probabilistic approaches is the computational burden of estimating the parameters (Lari and Young 1990). In this paper, we consider a probabilistic recursive transition network (PRTN) as an underlying grammar representation, and present an algorithm for training the probabilistic parameters, then suggest an improved version that works with reduced redundant computations. The key point is to save intermediate results and avoid the same computation later on. Moreo"
J96-3007,E93-1040,0,0.0628823,"Missing"
K15-1014,C14-1147,0,0.0343471,"Missing"
K15-1014,P12-1073,0,0.0485354,"Missing"
K15-1014,Q14-1019,0,\N,Missing
kang-choi-2000-automatic,W98-1005,0,\N,Missing
kang-choi-2000-automatic,J96-4002,0,\N,Missing
kang-choi-2000-automatic,P98-2220,0,\N,Missing
kang-choi-2000-automatic,C98-2215,0,\N,Missing
kang-choi-2000-automatic,P97-1017,0,\N,Missing
L16-1055,W09-3417,0,0.280275,"own annotation languages of temporal information, Time Markup Language (TimeML) (Pustejovsky et al., 2003) and ISO-TimeML. Although these annotation languages define many tags and attributes for representing various types of temporal information, they do not incorporate language diversity. For example, they assume that annotation is performed in the token level. However, Korean is an agglutinative language whose words are formed by joining morphemes together, so it can not be annotated properly in the token level. As an annotation language for Korean, the Korean TimeML (KTimeML) was proposed (Im et al., 2009), and its contributions can be summarized as follows: (1) it employs a morpheme-level standoff annotation scheme, (2) it takes a surface-based annotation scheme, (3) it suggests to cancel the head-only markup policy of TimeML, (4) it addresses several Korean-specific issues (e.g., the usage of signal tag for only temporal connectives), and (5) it introduces the TARSQI Toolkit for the annotation process following the KTimeML. In this paper, we argue that the KTimeML has some limitations, and propose a revised version of the KTimeML. For example, the previous KTimeML did not consider some charac"
L16-1055,S13-2001,0,0.0157604,"ecause the annotators have to count the number of preceding characters. Furthermore, it will also be more difficult to read or check whether the annotated timex3 tag is correct or not, due to the same reason. Thus, the two attributes are necessary to help human annotators. 357 Figure 2: Sample annotated sentences of Korean TimeBank. To address the fourth limitation, we just use the same attribute name id for every tag, as ISO-TimeML does. To address the fifth limitation, we employ a makeinstance tag, which is also adopted by TempEval shared tasks (Verhagen et al., 2009; Verhagen et al., 2010; UzZaman et al., 2013). The makeinstance tag takes the role of an event instances, while the event tag has only the role of an event token. This clear separation of the two roles will help the further applications to easily analyze event tags. As there is at least one instance for each event token, the number of event tags is always smaller than or equal to the number of makeinstance tags. 3. Table 1: The statistics of Korean TimeBank. Item document sentence timex3 event makeinstance tlink Table 2: Kappa coefficient of Korean TimeBank. Korean TimeBank There are some existing Korean datasets of temporal information."
L16-1055,S10-1010,0,\N,Missing
L18-1013,P14-1136,0,0.0168243,"and then 4) chooses the most similar context vector and LU to get its frame-semantics. Figure 2 shows a snapshot of these operations. In this paper, the context words are defined as 1-hop connected nodes with a target word in the dependency path, and the context vector is generated by averaging the sum of the word embeddings of each context word. The similarity score is calculated as the cosine similarity between the context vector of the target word and the frame embeddings. Ongoing studies The method described above uses a limited scope of the surrounding context of a target word and an LU. Hermann et al. (2014) uses a joint model with word embedding and syntactic structure, and Swayamdipta et al. (2017) uses rich syntactic features to identify the frame-semantics. Generating the frame embeddings from rich features would be the next step. Manual Annotation Correction To publish the resource, the FrameNet-annotated KAIST treebank corpus would be validated as a gold standard corpus to prevent error propagation issues in the training process. The result of the annotation correction is beyond the scope of this paper; however, the preparatory work for manual annotation correction is reported in this secti"
L18-1013,kingsbury-palmer-2002-treebank,0,0.063262,"Missing"
L18-1013,meurs-etal-2008-semantic,0,0.0786215,"Missing"
L18-1013,P15-1173,0,0.0294009,"3.1. In the frame identification task, the frame-semantics of the target words would be disambiguated by their surrounding contexts (Baker et al., 1998). It means that if a given target word has a similar context as an LU in the Korean FrameNet, its proper frame-semantics would be a framesemantics of the LU. Figure 2: Identify the frame-semantics for a target word by comparing a context vector of the LU, which was learned from the Korean FrameNet annotations. In this paper, to select a suitable frame-semantics for the target words, we have borrowed ideas from the concepts of synset embedding (Rothe and Schütze, 2015) and doc2vec (Le and Mikolov, 2014). Our model 1) learns the context vectors of each LU (called the frame embedding) from the full-text annotation in the Korean FrameNet, 2) generates the context vector for the given target word from the input text in the same way to generate the frame embeddings, 3) determines a similarity score between the context vectors of the target word and the frame embeddings, and then 4) chooses the most similar context vector and LU to get its frame-semantics. Figure 2 shows a snapshot of these operations. In this paper, the context words are defined as 1-hop connect"
L18-1013,D07-1002,0,0.0620226,"er describes the approach and its expected results. As a first step, we built a lexical database of the Korean FrameNet, and used it to learn the model for automatic annotation. Its current scope, status, and limitations are discussed in this paper. Keywords: FrameNet, Semantic Role Labeling, Corpus Annotation 1. Introduction FrameNet is a large lexical database that has rich annotations to represent the meanings of text using semantic frames (Baker et al., 1998; Fillmore et al., 2003). FrameNet has been considered a useful resource for various applications such as question answering systems (Shen and Lapata, 2007, Hahm et al., 2016), information extraction (Surdeanu et al., 2003), and dialog systems (Chen et al., 2013). Lately, researchers have shown increasing interest in multilingual FrameNet (Borin et al., 2010; You and Lui, 2005; Meurs et al., 2008; Subirats and Petruck, 2003; Burchardt et al., 2006). A Korean FrameNet project built the Korean FrameNet resource by translating English FrameNet annotations and Japanese FrameNet into its equivalent in Korean (Park et al., 2014; Kim et al., 2016). One of the purpose of the FrameNet annotation is to build a frame-semantic parser to understand the meani"
L18-1013,P03-1002,0,0.107484,", we built a lexical database of the Korean FrameNet, and used it to learn the model for automatic annotation. Its current scope, status, and limitations are discussed in this paper. Keywords: FrameNet, Semantic Role Labeling, Corpus Annotation 1. Introduction FrameNet is a large lexical database that has rich annotations to represent the meanings of text using semantic frames (Baker et al., 1998; Fillmore et al., 2003). FrameNet has been considered a useful resource for various applications such as question answering systems (Shen and Lapata, 2007, Hahm et al., 2016), information extraction (Surdeanu et al., 2003), and dialog systems (Chen et al., 2013). Lately, researchers have shown increasing interest in multilingual FrameNet (Borin et al., 2010; You and Lui, 2005; Meurs et al., 2008; Subirats and Petruck, 2003; Burchardt et al., 2006). A Korean FrameNet project built the Korean FrameNet resource by translating English FrameNet annotations and Japanese FrameNet into its equivalent in Korean (Park et al., 2014; Kim et al., 2016). One of the purpose of the FrameNet annotation is to build a frame-semantic parser to understand the meaning of a text. Some studies have built frame-semantic parser for Engl"
L18-1013,Q15-1003,0,0.0179638,"hown at the top of the list of frame-semantics candidates, and it would prevent the annotators from wasting time searching for suitable framesemantics tags. We generate the constraints from the Korean FrameNet database. For example, if a given word exist in the Korean FrameNet, its corresponding framesemantics tags (i.e., annotated frame-semantics in Korean FrameNet full-text annotations) are shown at the top of the list of candidate tags. Ongoing studies Identifying a boundary of frame elements and its type (i.e., the frame element tag) is still a challenge in the framesemantic parsing task. Täckström et al. (2015) relied on the dependency features and some heuristic rules, and Yang and Mitchell (2017) used a joint model to jointly assign the frame-semantics and frame elements. In our project, our purpose of using automatic FrameNet annotation is to construct a silver standard corpus that would be corrected manually. Next, we are focused on the task that can identify frame elements well. To accomplish this, we are studying methods to generate valence patterns with richer syntactic features than using only grammatical functions and phrase types for high recall performance. 4. Evaluation For the evaluatio"
L18-1013,D17-1128,0,0.0448696,"nd Lui, 2005; Meurs et al., 2008; Subirats and Petruck, 2003; Burchardt et al., 2006). A Korean FrameNet project built the Korean FrameNet resource by translating English FrameNet annotations and Japanese FrameNet into its equivalent in Korean (Park et al., 2014; Kim et al., 2016). One of the purpose of the FrameNet annotation is to build a frame-semantic parser to understand the meaning of a text. Some studies have built frame-semantic parser for English by using full-text annotation and partially annotated exemplar sentences to train their models (Das et al., 2010; Swayamdipta et al., 2017; Yang and Mitchell, 2017). For example, the state-of-the-art frame-semantic parser uses nearly 139k exemplar sentences for training data and it generally introduces a 3–4 F1 gain for parsing (Yang and Mitchell., 2017). In comparison, the Korean FrameNet has full-text annotations for 5,025 sentences and 8,200 lexical units (LUs). In this paper, we report an ongoing project to construct Korean FrameNet annotations to scale up the amount of annotations. A task to annotate the frame-semantics manually over raw sentences has been formalized by Ruppenhofer et al. (2006). It is an expensive and complex task, because the anno"
L18-1165,C14-1220,0,0.0683396,"Missing"
L18-1165,choi-etal-2004-korean,1,0.605488,"ords that have the same form can have different meanings. Word Sense Disambiguation(WSD) is to select the correct meaning of a word in context. It is an important problem that can be utilized in many problems of natural language processing such as machine translation and information extraction (Chaplot et al., 2015). In English, studies are typically conducted on the basis of the senses listed in the Princeton WordNet(PWN) as candidates for the meaning of words (Navigli et al., 2007; Chaplot et al., 2015). In this study, we resolve the ambiguity of words based on the senses listed in CoreNet (Choi et al, 2004), a Korean lexical semantic network. Both PWN and CoreNet are fine-grained resources, so it is difficult for even human annotators to correctly identify the senses of words. In order for WSD to become an enabling technique for end-to-end applications, it requires the ability to make reasonable sense distinctions (Navigli et al., 2007). In English, coarse-grained WSD is performed by semiautomatically clustering PWN senses (Navigli et al., 2007). However, Korean dictionaries also list homograph and polyseme numbers for each sense. Homographs are words that have the same form but completely diffe"
L18-1165,P16-1085,0,0.0320158,"Missing"
L18-1165,C96-1041,1,0.311587,"ch is the term frequency– inverse document frequency (TF-IDF) vector similarity method, which will be described in Section 3.1, and can be used to obtain the frequency values of concepts necessary to implement the MRF based method, as described in Section 3.2. 3.1 MRF Method A MRF is an undirected graphical model that consists of set of random variables. Each node in the graph represent a random variable, and each random variable is only dependent on another random variable that represents another node that is directly connected by an edge. This model has been used to solve many NLP problems (Jung et al., 1996; Chaplot et al., 2015) In this study, we applied an MRF based WSD method (Chaplot et al., 2015) to the Korean language using the concept hierarchy in CoreNet. In this method, target words for the WSD in a sentence are selected as nodes in the MRF, and edges are only generated for two directly connected words in the dependency tree. Finally, the senses of all the words are jointly disambiguated by way of a MAP query on this MRF model. We adopted the detailed methods described by Chaplot et al. (2015), and outline how they were applied to the Korean language in the following text. TF-IDF Vector"
L18-1165,S07-1006,0,0.0705813,"Missing"
L18-1230,W00-1318,0,0.0615211,"Missing"
L18-1563,P11-1055,0,0.314162,"sating for the shortage of training data in distantly supervised RE. Experiments on RE from Korean Wikipedia show that the presented approach can learn an exact RE from sentences (including grammatically incoherent sentences) without syntactic parsing. Keywords: Relation Extraction, Sentence Embedding, Pro-drop Languages 1. Introduction Text Corpus As the demand for structured knowledge has increased, considerable interest has emerged in relation extraction (RE) from large collections of documents written in natural language. In particular, with “distant supervision” (DS) (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013), it is possible to extract the relationships between pairs of entities without human manual annotation using a knowledge base (KB); this heuristically aligns entities in texts to a given KB and then uses this alignment to train an RE system. Although the DS strategy is a more effective method of automatically labeling training data than directly supervised labeling, DS-based approaches can extract only relations that are limited to a “single complete sentence” that contains two target entities. This makes it difficult to obtain both the subjects and object entities that"
L18-1563,C82-1032,0,0.395952,"d to a “single complete sentence” that contains two target entities. This makes it difficult to obtain both the subjects and object entities that participate in the KB in a single sentence, particularly in null subject (or object) languages such as Korean, Japanese, Arabic, and Swedish, that can leave the subject of a sentence unexpressed, unlike English which allows neither. It is also difficult to utilize DS-based approaches for English data when sentences have an informal, grammatically incoherent style, such as the style popularly used on Twitter, in discharge summaries of clinical texts (Marsh and Sager, 1982), or in a text shortened to bulleted lists in a Wikipedia article. This point can be illustrated by considering the examples in Figure 1. S1 contains a subject, object, and predicate, whereas the subject is omitted in S2 because it is obvious in adjacent sentences in Korean, resulting in differences between the same sentence written in Korean and in English. Therein, we know S2 is obviously a positive example for tuple founderOf(Steve Jobs, Apple Inc.), but we cannot label the training instance S2 according to the traditional paradigm of an existing DS-based approach. We propose a novel approa"
L18-1563,P09-1113,0,0.20356,"Missing"
L18-1563,Q17-1008,0,0.0272248,"was inspired by Mintz et al. (2009), who adopted the Freebase for the distant supervision of the Wikipedia corpus. Unlike existing methods, we performed RE across sentences at the paragraph-level by extending the possibility of labeling incomplete sentences that were unavailable in the traditional DS-based approach. To the best of our knowledge, ours is the first DS-based approach to solve the problem of data sparseness by applying DS to the RE of informal sentences and alleviating DS assumptions. Quirk and Poon (2016) introduced the RE method in two adjacent sentences using the DS approach. Peng et al. (2017) explored a framework for cross-sentence n-ary RE based on graph long short-term memory networks; they used a graph formula to provide a unified method of integrating various intra- and inter-sentential dependencies such as sequential, syntactic, and discourse relationships. According to their experiments on biomedical domains, use of RE beyond sentence boundaries can yield much more knowledge. In this context, we intend to find more information by spanning multiple sentences. While they are based on the various linguistic analyses, our proposed method can be differentiated by using contexts w"
L18-1563,N13-1008,0,0.0216329,"of training data in distantly supervised RE. Experiments on RE from Korean Wikipedia show that the presented approach can learn an exact RE from sentences (including grammatically incoherent sentences) without syntactic parsing. Keywords: Relation Extraction, Sentence Embedding, Pro-drop Languages 1. Introduction Text Corpus As the demand for structured knowledge has increased, considerable interest has emerged in relation extraction (RE) from large collections of documents written in natural language. In particular, with “distant supervision” (DS) (Mintz et al., 2009; Hoffmann et al., 2011; Riedel et al., 2013), it is possible to extract the relationships between pairs of entities without human manual annotation using a knowledge base (KB); this heuristically aligns entities in texts to a given KB and then uses this alignment to train an RE system. Although the DS strategy is a more effective method of automatically labeling training data than directly supervised labeling, DS-based approaches can extract only relations that are limited to a “single complete sentence” that contains two target entities. This makes it difficult to obtain both the subjects and object entities that participate in the KB"
L18-1563,Q16-1011,0,0.0206937,"the sentence embedding, is more effective for language-independent extraction because it avoids high-level natural language processing (NLP) tools. Therefore, the present approach can be generally used for RE, even in languages for which NLP tools are lacking. 3562 2. Related Work S1 We often encounter a lack of explicitly annotated text in RE, instead finding richly structured KBs such as DBpedia (Bizer et al., 2009) or Freebase (Bollacker et al., 2008), which has raised significant interest in learning RE using DS. Many DS-based approaches (Hoffmann et al., 2011; Roller and Stevenson, 2014; Tsai and Roth, 2016; Craven and Kumlien, 1999; Mintz et al., 2009) use simple but effective heuristics to align existing facts with unlabeled text. This automatically generated labeled text can be used as training data for supervised learners. Our work was inspired by Mintz et al. (2009), who adopted the Freebase for the distant supervision of the Wikipedia corpus. Unlike existing methods, we performed RE across sentences at the paragraph-level by extending the possibility of labeling incomplete sentences that were unavailable in the traditional DS-based approach. To the best of our knowledge, ours is the first"
melz-etal-2006-compiling,C92-2082,0,\N,Missing
melz-etal-2006-compiling,P00-1063,1,\N,Missing
melz-etal-2006-compiling,C02-1007,0,\N,Missing
melz-etal-2006-compiling,N03-1032,0,\N,Missing
melz-etal-2006-compiling,J02-1004,0,\N,Missing
melz-etal-2006-compiling,P99-1016,0,\N,Missing
melz-etal-2006-compiling,J06-1003,0,\N,Missing
melz-etal-2006-compiling,W99-0609,0,\N,Missing
oh-etal-2002-word,kilgarriff-rosenzweig-2000-english,0,\N,Missing
oh-etal-2002-word,J98-4002,0,\N,Missing
oh-etal-2002-word,C96-1005,0,\N,Missing
oh-etal-2002-word,P95-1026,0,\N,Missing
oh-etal-2002-word,P91-1019,0,\N,Missing
oh-etal-2002-word,P98-1037,0,\N,Missing
oh-etal-2002-word,C98-1037,0,\N,Missing
oh-etal-2002-word,P91-1034,0,\N,Missing
oh-etal-2002-word,J98-1004,0,\N,Missing
oh-etal-2002-word,P97-1007,0,\N,Missing
P00-1005,1999.tmi-1.15,0,\N,Missing
P00-1005,C96-2211,0,\N,Missing
P00-1005,J94-4003,0,\N,Missing
P00-1005,P91-1034,0,\N,Missing
P00-1005,P98-2223,0,\N,Missing
P00-1005,C98-2218,0,\N,Missing
P00-1050,J96-1001,0,\N,Missing
P00-1050,J93-2003,0,\N,Missing
P00-1050,P98-1069,0,\N,Missing
P00-1050,C98-1066,0,\N,Missing
P00-1050,P98-2212,0,\N,Missing
P00-1050,C98-2207,0,\N,Missing
P00-1050,P95-1032,0,\N,Missing
P00-1050,P93-1001,0,\N,Missing
P00-1050,P91-1017,0,\N,Missing
P00-1050,P91-1023,0,\N,Missing
P00-1050,J93-1006,0,\N,Missing
P00-1050,J97-2004,0,\N,Missing
P00-1050,P97-1039,0,\N,Missing
P00-1050,P94-1012,0,\N,Missing
P00-1063,P97-1004,0,\N,Missing
P00-1063,A94-1006,0,\N,Missing
P00-1063,P98-1112,0,\N,Missing
P00-1063,C98-1108,0,\N,Missing
P00-1063,C92-3150,0,\N,Missing
P00-1072,P99-1004,0,\N,Missing
P00-1072,P99-1005,0,\N,Missing
P04-3027,2003.mtsummit-papers.26,0,0.0475454,"Missing"
P98-1039,C94-1020,1,0.878837,"Missing"
P98-1039,A94-1005,0,\N,Missing
P98-1119,W97-0107,1,0.837196,"nd the occurrence frequency of each dependency relation is calculated. Based on the frequencies, probabilities of dependency relations are recalculated by C(wp --+ w~) The process w,) = C(w continues until the entropy of the training corpus becomes the minimum. The frequency of occurrence, C(wi --+ wj), is calculated by /3[(i,m)t3?(m,j). m=i+l The basis probabilities are: /31r(i,i + 1) = p(wi ""~ wi+l) /3[(i,i + 1) = p(wi (-"" wi+l) /3~(i, i) = fl?(i, i) = 1 /37(1, EO S) = p( wL, ) w) = -+ 1 t • • t = p(wt,.)a.(,,3)/3~(i,j) ~A little more detailed explanation of the expressions can be found in (Lee and Choi, 1997). 725 where O~(wi ~ wj, D, wl,n) is 1 if the dependency relation, (wi --+ wj), is used in the D, and 0 otherwise. Similarly, the occurrence frequency of the dependency relation, (wi +- wj), where the test corpus contains a total of IV] words and is composed of S sentences. ~----L---o~l(i,j)~[(i,j ). is computed by 3.4 i | | i | ! I 3.23 4 Preliminary experiments We have experimented with three language models, tri-gram model (TRI), bi-gram model (BI), and the proposed model (DEP) on a raw corpus extracted from KAIST corpus 3. The raw corpus consists of 1,589 sentences with 13,139 words, descri"
P98-1119,J92-1004,0,0.0276754,"ms(Brown et al., 1992; Chang and Chen, 1996). N-gram model has been widely used so far, but it has always been clear that n-gram can not represent long distance dependencies. In contrast with n-gram model, grammarbased approach assigns syntactic structures to a sentence and computes the probability of the sentence using the probabilities of the structures. Long distance dependencies can be represented well by means of the structures. The 723 approach usually makes use of phrase structure grammars such as probabilistic context-free grammar and recursive transition network(Lari and Young, 1991; Sneff, 1992; Chen, 1996). In the approach, however, a sentence which is not accepted by the grammar is assigned zero probability. Thus, the grammar must have broadcoverage so that any sentence will get non-zero probability. But acquisition of such a robust grammar has been known to be very difficult. Due to the difficulty, some works try to use an integrated model of grammar and n-gram compensating each other(McCandless, 1994; Meteer and Rohlicek, 1993). Given a robust grammar, grammar-based language modeling is expected to be more powerful and compact in model size than n-gram-based one. In this paper w"
P98-1119,J92-4003,0,\N,Missing
W03-1122,C02-1167,0,0.0710645,"Missing"
W03-1122,P88-1012,0,\N,Missing
W03-1122,1995.mtsummit-1.17,0,\N,Missing
W04-1111,C86-1067,0,0.265237,"Missing"
W04-1111,P79-1000,0,0.130436,"Missing"
W04-1111,C88-2157,0,\N,Missing
W04-1111,C00-1056,0,\N,Missing
W04-1111,P00-1031,0,\N,Missing
W04-1813,W99-0609,0,0.051775,"Missing"
W06-0506,C92-2082,0,\N,Missing
W06-0506,C00-1022,0,\N,Missing
W06-0506,J06-1003,0,\N,Missing
W06-0506,W99-0609,0,\N,Missing
W09-3426,W09-3426,1,0.0512755,"Missing"
W10-3306,C92-2082,0,0.0730063,"ance set of B. In addition, we define the following terms for algorithm description: Definition 4: intrinsic token. Token 2 T is an intrinsic token of B iff T represents the intrinsic property of B. For example, when B is “Pioneer 11”, the intrinsic tokens of B are “spacecraft”, “escape3 ”, “Jupiter”, etc. Related Works Methods of taxonomic relation extraction can be divided into two broad categories depending on the input: unstructured or structured data. The extraction of taxonomic relations from unstructured text is mainly carried out using lexical patterns on the text. The Hearst pattern (Hearst, 1992) is used in many pattern-based approaches, such as Cimiano (2005). In addition, there has been research that attempted to use existing structured data, like the Wikipedia category structure or the contents of a thesaurus. The system of Ponzetto (2007) determines whether or not the given Wikipedia category link is an isa/instanceOf relation by applying a set of rules to the category names, while Nastase (2008) defined lexical patterns on category names, in addition to Ponzetto (2007). The YAGO system (Suchanek et al., 2007) attempts to classify whether the given article-category link represents"
W12-3411,W11-3801,0,0.157775,"e Sejong treebank, and the converted treebanks are used to train and test three different well-known statistical parsers, namely Stanford parser (Klein and Manning, 2003), Bikel-Collins parser (Bikel, 2012) and Berkeley parser (Petrov et al., 2006). To figure out the effect of each method, all six methods are sequentially applied one by one, and each version of the treebank is used to train and test each parser. The baseline treebank is the original Sejong treebank without any transformations. For the Korean head word extraction which will be used during parsing, the head percolation rule of (Choi and Palmer, 2011) is adapted. According to that paper, particles and endings were the most useful morphemes to determine dependencies between eojeols. Based on the observation, their rules are changed so that they give the best priorities on those morphemes. We use the preprocessing method described in (Park, 2006) for training trees. It replaces symboles with PennTreebank-like tags and corrects wrong morpheme 5 Berk. See Figure 1 for its transcription and translation. 83 Corpus Baseline M1 M 1-2 M 1-3 M 1-4 M 1-5 M 1-6 Baseline M1 M 1-2 M 1-3 M 1-4 M 1-5 M 1-6 Baseline M1 M 1-2 M 1-3 M 1-4 M 1-5 M 1-6 P 67.88"
W12-3411,W10-1406,0,0.371321,"Missing"
W12-3411,damljanovic-etal-2010-identification,0,0.0363748,"Missing"
W12-3411,N09-1037,0,0.0577934,"Missing"
W12-3411,J02-3001,0,0.0902237,"Missing"
W12-3411,P03-1054,0,0.00667126,"d cumulatively.6 System Stan. Figure 7: Example of retagging phrase tags: VP-MOD to DP, NP-MOD to JSP-MOD, and NP-SBJ to JSP-SBJ. 5 Bikel. Evaluations In this section, several experiment results using the standard F1 metric (2P R/(P + R)) are introduced to show the effect of each transforming method, and the most frequently shown error cases are explained. 5.1 Experiments using the Sejong Treebank The proposed transformation methods are applied to the Sejong treebank, and the converted treebanks are used to train and test three different well-known statistical parsers, namely Stanford parser (Klein and Manning, 2003), Bikel-Collins parser (Bikel, 2012) and Berkeley parser (Petrov et al., 2006). To figure out the effect of each method, all six methods are sequentially applied one by one, and each version of the treebank is used to train and test each parser. The baseline treebank is the original Sejong treebank without any transformations. For the Korean head word extraction which will be used during parsing, the head percolation rule of (Choi and Palmer, 2011) is adapted. According to that paper, particles and endings were the most useful morphemes to determine dependencies between eojeols. Based on the o"
W12-3411,P06-3013,1,0.887804,"Missing"
W12-3411,P06-1055,0,0.0614181,"o DP, NP-MOD to JSP-MOD, and NP-SBJ to JSP-SBJ. 5 Bikel. Evaluations In this section, several experiment results using the standard F1 metric (2P R/(P + R)) are introduced to show the effect of each transforming method, and the most frequently shown error cases are explained. 5.1 Experiments using the Sejong Treebank The proposed transformation methods are applied to the Sejong treebank, and the converted treebanks are used to train and test three different well-known statistical parsers, namely Stanford parser (Klein and Manning, 2003), Bikel-Collins parser (Bikel, 2012) and Berkeley parser (Petrov et al., 2006). To figure out the effect of each method, all six methods are sequentially applied one by one, and each version of the treebank is used to train and test each parser. The baseline treebank is the original Sejong treebank without any transformations. For the Korean head word extraction which will be used during parsing, the head percolation rule of (Choi and Palmer, 2011) is adapted. According to that paper, particles and endings were the most useful morphemes to determine dependencies between eojeols. Based on the observation, their rules are changed so that they give the best priorities on t"
W12-3411,W04-3207,0,0.148066,"Missing"
W12-3411,N07-1051,0,\N,Missing
W12-5201,chiarcos-2012-ontologies,0,0.0216234,"lmann et al., 2012; Rizzo et al., 2012) is underway in NLP2RDF as a sub-project of LOD2. NIF suggests a standard for several different NLP outputs. Korean NLP results are also different from other NLP application results based on other languages. So these metadata (for the results of NLP tools) are described by using interoperable RDF. Ontology for Korean POS tags is defined, and the whole process is complying with the specifications of NIF. The results from Section 3 are converted to RDF triples. 4.1.1 Ontology for Korean Linguistic Annotations The Ontologies of Linguistic Annotation (OLiA) (Chiarcos, 2012) are used to describe POS tags, which are different between languages. In Korean, the Sejong POS tagset and the KAIST POS tagset (Choi et al., 1994) are used for POS tagging. OLiA are offering an annotation model for Penn tag set10 which is mainly used in English. In the Penn tagset annotation model, POS tag information is the subpart structure of Linguistic Annotation domain. And OLiA are offering linking model also which is mapped between Penn annotation model and OLiA reference model. In this linking model, OLiA information is the subpart structure of Linguistic Concept domain and it is map"
W12-5201,W12-3411,1,0.913189,"ults like POS tagging, CFG parsing, DG parsing and so on for one-time input. And, by using wrapper3 which implemented by the NLP2RDF4 project team, those results are converted to RDF in compliance with NIF. Actually, sharing results in Korean NLP fields is still in its early stage. Researches on Korean parsers have been focused on DG parsing e.g. (Chung, 2004) because Korean word order is relatively free compared to other languages. Phrase-structured Sejong Treebank is transformed into the form of DG in (Choi and Palmer, 2011). Research for CFG parsing by using Sejong Treebank has progressed (Choi et al., 2012), but it is not active and disclosure of its research results and it also true that the lack of interoperability because a variety of tools put out different format results. In English the minimal unit for parsing is a word, but, in Korean, eojeol is a basic space unit 1 http://lod2.eu http://nlp.stanford.edu/software/corenlp.shtml http://nlp2rdf.org/implementations/stanford-corenlp 4 http://nlp2rdf.org 2 3 2 Figure 1: An example morpheme analysing Korean sentences by using HanNanum which separated from another eojeol with white-space. An eojeol is a word or its variant word form agglutinated"
W12-5201,W11-3801,0,0.0262299,"or linked data is Stanford Core-NLP2 . Stanford Core-NLP puts out various NLP analysis results like POS tagging, CFG parsing, DG parsing and so on for one-time input. And, by using wrapper3 which implemented by the NLP2RDF4 project team, those results are converted to RDF in compliance with NIF. Actually, sharing results in Korean NLP fields is still in its early stage. Researches on Korean parsers have been focused on DG parsing e.g. (Chung, 2004) because Korean word order is relatively free compared to other languages. Phrase-structured Sejong Treebank is transformed into the form of DG in (Choi and Palmer, 2011). Research for CFG parsing by using Sejong Treebank has progressed (Choi et al., 2012), but it is not active and disclosure of its research results and it also true that the lack of interoperability because a variety of tools put out different format results. In English the minimal unit for parsing is a word, but, in Korean, eojeol is a basic space unit 1 http://lod2.eu http://nlp.stanford.edu/software/corenlp.shtml http://nlp2rdf.org/implementations/stanford-corenlp 4 http://nlp2rdf.org 2 3 2 Figure 1: An example morpheme analysing Korean sentences by using HanNanum which separated from anoth"
W13-5714,candito-etal-2010-statistical,0,0.0627122,"Missing"
W13-5714,W11-3801,0,0.234471,"tep enables arguments for the verb to be correctly dependent on the main verb and the main verb to be dependent on the auxiliary verb in the ambiguous annotation scheme in the SJTree.1 Figure 1 shows the original SJTree phrase structure and its corresponding converted representation in dependency grammars. 3 Parsing Model Our parsing model gives a probability to each possible dependency tree T for a sentence S = e1 , e2 , ..., en , where ei is a Korean word. The model finally selects the dependency tree T ∗ that maximizes P (T |S) as follows: T ∗ = arg max P (T |S). (1) T 1 (Oh and Cha, 2010; Choi and Palmer, 2011) also introduced an conversion algorithm of dependency grammars for the SJTree. (Choi and Palmer, 2011) proposed head percolation rules for the SJTREE. However, we found some errors such as S related rules, where it gives lower priority to S than VP. It would fail to assign a head node correctly for S → VP S. Moreover, they did not consider auxiliary verb constructions annotated as VP in the SJTREE. According to their head rules, arguments for the main verb are dependent on the auxiliary verb instead of the main verb because of the annotation of the corpus (in general, VP → VP VP where the for"
W13-5714,P99-1062,1,0.536574,"inbu has 3 occurrences. 3 common = 0 if either i1 or i2 is not included in CoreNet. P ratioci = P p |ex | p |ey | j=1 i=1 (8) where i is the number of the occurrences of the instance examples of the same case marker and j is the number of the occurrences of the instance examples of the all case marker. 5.2 Constructing the database of N1 ui N2 structures We also integrate lexical information on Korean noun phrases of the form N1 ui N2 , which roughly corresponds to N2 of N1 in English. Even though Korean genitive marker ui does not have a broad usage as much as no in Japanese as described in (Kurohashi and Sakai, 1999), it sometime does not modify the immediate constituent such as Kyungjiui meylonhyang binwuleul (‘Melon-flavored soap of Kyungji’) where Kyungjiui modifies binwuleul instead of meylonhyang. The N1 ui N2 structure is very useful to recognize the meaning of natural language text can improve head-modifier relationships between genitive nouns. 6 Experiment and Results 6.1 Parsing results We use the Sejong Treebank corpus (SJTree) in our experiment.4 We use standard dataset split for training, development and testing. We report here final evaluation results on the baseline unlexicalized parsing and"
W13-5714,W12-3411,1,0.423407,"Missing"
W13-5714,W02-2207,0,0.597908,"Missing"
W13-5714,W10-1406,0,0.243484,"Missing"
W13-5714,N06-1023,1,0.788493,"ng. P (T |S) is defined as the product of probabilities as follows: P (T |S) = Y P (Epa , dist|eh ), (2) Epa ∈T where Epa represents a clause dominated by a predicate or a genitive nominal phrase, eh represents the head Korean word of Epa , and dist is the distance between Epa and eh . Instead of specifying the actual distance, it is classified into three bins: 1, 2 – 5, and 6 –. If the dependent Korean word appears right next to the head, the distance is 1. If it appears between 2 and 5, the distance is 2. If it appears past 6, the distance is 6. P (T |S) is calculated in the similar way as (Kawahara and Kurohashi, 2006a). We describe the outline of this model below. Each probability in equation (2) is decomposed into two ways according to the type of Epa . If Epa is a clause dominated by a predicate, it is decomposed into a predicate-argument structure (content part) P Am and a function part fm . eh is also decomposed into a content part ch and a function part fh . P (Epa , dist|eh ) = P (P Am , fm , dist|ch , fh ) = P (P Am |fm , dist, ch , fh ) × P (fm , dist|ch , fh ) ≈ P (P Am |fm , ch ) × P (fm , dist|fh ) (3) The first term in the last equation represents a fullylexicalized generative probability of t"
W13-5714,kawahara-kurohashi-2006-case,1,0.778624,"ng. P (T |S) is defined as the product of probabilities as follows: P (T |S) = Y P (Epa , dist|eh ), (2) Epa ∈T where Epa represents a clause dominated by a predicate or a genitive nominal phrase, eh represents the head Korean word of Epa , and dist is the distance between Epa and eh . Instead of specifying the actual distance, it is classified into three bins: 1, 2 – 5, and 6 –. If the dependent Korean word appears right next to the head, the distance is 1. If it appears between 2 and 5, the distance is 2. If it appears past 6, the distance is 6. P (T |S) is calculated in the similar way as (Kawahara and Kurohashi, 2006a). We describe the outline of this model below. Each probability in equation (2) is decomposed into two ways according to the type of Epa . If Epa is a clause dominated by a predicate, it is decomposed into a predicate-argument structure (content part) P Am and a function part fm . eh is also decomposed into a content part ch and a function part fh . P (Epa , dist|eh ) = P (P Am , fm , dist|ch , fh ) = P (P Am |fm , dist, ch , fh ) × P (fm , dist|ch , fh ) ≈ P (P Am |fm , ch ) × P (fm , dist|fh ) (3) The first term in the last equation represents a fullylexicalized generative probability of t"
W16-4409,P05-1053,0,0.0175063,"e been researched for RE. We are focusing on two different paradigms here. Fully supervised approaches require sufficiently many handcrafted training data. A triple is labeled to a sentence if the sentence expresses a certain relational fact between two entities. This kind of manually labeled data rarely has noise, however, it is very costly because annotation takes a lot of time and effort from experts. Manually annotated training data are good in quality but limited in quantity. Furthermore, a new training dataset may be needed for another corpus if the corpus has different characteristics (Zhou et al., 2005). Thus, this model is not scalable. Another type of approaches makes use of seed patterns. Using seed patterns, triples are first extracted with sentences they are extracted from then extracted triples and sentences are used to generate new patterns. By repeating this process, more and more patterns are collected. This kind of approaches can be an alternative solution when manually annotated training data are not available, however, the noise in triples and sentences tends to propagate over iterations (Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). This model is scalable because it"
W16-4409,P09-1113,0,0.234357,"tem uses feedback from a crowd. We will explain how easily a crowd feeds back and our system updates a model later. To maximize efficiency of crowd sourcing, we prefer to get as much feedback as possible per pattern. In other words, we want to have as few patterns as possible. We will also explain the method we used to regulate the number of patterns later in this section. First, we will give a brief explanation about how a pattern is generated in the following subsections. 2.1 Pattern generation For scalability, our systems uses distantly supervised data to train a relation extraction model (Mintz et al, 2009). Since Distant Supervision uses an existing knowledge graph to collect sentences for training, there is a possibility to gather more sentences as a knowledge graph grows. From a pair of sentence and triple, a pattern can be generated. If named-entities repeatedly appear in a sentence, all possible subject and object pairs are listed first. Figure 1 describes how a pattern is generated from the sentence ”A president of USA George W. Bush is the son of George H. W. Bush.” labeled with the subject and object pair (George W. Bush, George H. W. Bush) by Distant Supervision. A pattern is generated"
W16-4409,P06-1015,0,0.00889715,"us if the corpus has different characteristics (Zhou et al., 2005). Thus, this model is not scalable. Another type of approaches makes use of seed patterns. Using seed patterns, triples are first extracted with sentences they are extracted from then extracted triples and sentences are used to generate new patterns. By repeating this process, more and more patterns are collected. This kind of approaches can be an alternative solution when manually annotated training data are not available, however, the noise in triples and sentences tends to propagate over iterations (Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). This model is scalable because it can generate data by itself for the next training but the problem is low accuracy of extraction. We want a knowledge graph to contain only correct triples. Crowd sourcing is the first alternative to validate triples before they are uploaded into a knowledge graph. We want to take one more advantage in this triple validation process; not only for deciding whether to upload triples or not but also for training a relation extraction model. Needless to say, a better relation extraction model improves the efficiency of crowd sourcing; a larger portion of triples"
W16-4409,P07-1073,0,0.0265871,"be needed for another corpus if the corpus has different characteristics (Zhou et al., 2005). Thus, this model is not scalable. Another type of approaches makes use of seed patterns. Using seed patterns, triples are first extracted with sentences they are extracted from then extracted triples and sentences are used to generate new patterns. By repeating this process, more and more patterns are collected. This kind of approaches can be an alternative solution when manually annotated training data are not available, however, the noise in triples and sentences tends to propagate over iterations (Bunescu and Mooney, 2007; Pantel and Pennacchiotti, 2006). This model is scalable because it can generate data by itself for the next training but the problem is low accuracy of extraction. We want a knowledge graph to contain only correct triples. Crowd sourcing is the first alternative to validate triples before they are uploaded into a knowledge graph. We want to take one more advantage in this triple validation process; not only for deciding whether to upload triples or not but also for training a relation extraction model. Needless to say, a better relation extraction model improves the efficiency of crowd sourc"
W16-4411,W12-3010,0,0.0293846,"Missing"
W16-4411,D12-1048,0,0.0271564,"Introduction The web contains enormous information in the form of unstructured text. In recent years, Open Information Extraction (IE) based on self-supervised learning has become more strongly suggested to overcome limitations of traditional IE system, and it is now possible to process massive text corpora. However, early Open IE systems fall short of representing multiple relations between arguments within a sentence since they are designed to focus on binary extractions. This causes incomplete and insufficient extraction. To overcome this limitations, Kraken(Akbik and L¨oser, 2012), OLLIE(Mausam et al., 2012) and ClausIE(Del Corro and Gemulla, 2013) are designed to extract a set of arguments using dependency parsing and then represent the extracted knowledge as ternary or N-ary form. Consider, for example, the sentence “Marsel was established by the British government with the help of American policymakers in 1971 as the nation’s first research oriented science institution.”. Current Open IE systems focus on extracting triples; (Marsel, was established by, the British government) and (Marsel, was established in, 1971). Even if these systems extract multiple triples, there is still missing informat"
W16-4412,P98-1013,0,0.122621,"between the property dbo:knownFor and the concept of “first man”. Thus the specific KB-dependent approach has the limitation of the scope of representable knowledge, in our example, the attribute of the relation. By contrast, the semantic parsing (SP) is considered the KB-independent approach to analyse user’s intention and semantics of information in questions (Xu et al., 2014). The SP approach is not dependent on the specific KB, so that it is efficient on the open domain question answering (Yao et al., 2014a). In this paper, we propose SP approach based on the frame semantics in FrameNet (Baker et al., 1998) to interpret questions. FrameNet uses a PropBank-style predicate-argument structure to represent relations between each argument. Each relation evoked by target words, and each relation is disambiguated by assigning the target words to the frames. For instance, in our example question, the word “reached” roles as a target and evokes the frame Arriving (frame:Arriving), and the word “first” also evokes the frame First_experience (frame:First_experience). The frame is a lexicon to represent not only encyclopedia-like information similar to DBpedia Ontology, but also linguistic level semantics f"
W16-4412,P14-1133,0,0.07478,"Missing"
W16-4412,P14-1136,0,0.0203698,"Missing"
W16-4412,P09-1113,0,0.0386757,"eaningful patterns and rules by matching the syntactic structure of question with the schemas in KB, and the lexical features with the ontology vocabulary in KB (Yao et al., 2014b). This process is based on the traditional IE approach such as a distant supervision This work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 2 http://qald.sebastianwalter.org/ http://www.okbqa.org/ 82 Proceedings of the Open Knowledge Base and Question Answering (OKBQA) Workshop, pages 82–90, Osaka, Japan, December 11 2016. (Mintz et al., 2009), and it generates KB specified query. For example, to answer the example question “Who was the first person reached the South Pole?” over the target KB, the IE approach extracts triples from the question based on the schema underlying target KB. Let’s consider DBpedia as the target KB to answer the example question. In DBpedia, there is knowledge to answer the question that exists in the triple form <dbr:Roald_Amundsen, dbo:knownFor, dbr:South_Pole>. First, a IE-based system searches sentences which includes the entities, dbr:Roald_Amundsen and dbr:South_Pole. If a sentence “…Roald Amundsen w"
W16-4412,vossen-etal-2014-newsreader,0,0.021711,"Missing"
W16-4412,W14-2416,0,0.0469919,"Missing"
W97-0107,P95-1031,0,0.014866,"rammar induction are as follows(Lari and Young, 1990, Carroll, 1992b): (1) generating all the possible rules, (2) reestimating the probabilities of rules using the inside-outside algorithm, and (3) finally finding a stable grammar by eliminating the rules which have probability values close to 0. Generating all the rules is done by restricting the number of nonterminals and/or the number of the right hand side symbols in the rules and enumerating all the possible combinations. Chen extracts rules by some heuristics and reestimates the probabilities of rules using the inside-outside algorithm (Chen, 1995). The inside-outside algorithm learns a grammar by iteratively adjusting the rule probabilities to minimize the training corpus entropy. It is extensively used as reestimation algorithm for phrase structure grammars. Most of the works on phrase structure grammar induction, however, have partially succeeded. Estimating phrase structure grammars by minimizing the training corpus on41 tropy does not lead to the desired grammars which is consistent with human intuitions (de Marcken, 1995). To increase the correctness of the learned grammar, Marcken proposed to include lexical information to the ph"
W97-0107,W95-0102,0,0.028652,"Missing"
W97-0107,C96-1058,0,0.0797258,"CompleteSequence Lr/LI L ~&quot; I i IIF SI [ II [ Sl Figure 4: Best first parsing entries The PDG best-first parsing algorithm constructs the best dependency tree in bottomup manner, with dynamic programrrdng method using CYK style chart. It is based on complete-link and complete-sequence of non-constituent concept. The parsing algorithm constructs the complete-link.q and complete-sequences for substring, and merges incrementally the complete-links into larger complete-sequence and complete-sequences into larger complete-link until the Lr(BOS, EOS) with maximum probability is constructed. Eisner (Eisner, 1996) proposed an O(n 3) parsing algorithm for PDG. In their work, basic unit of chart entry is span which is also of non-constituent concept. But, the span slightly differs from our complete-sequence and complete-link. When two adjacent spans are merged into a larger span, some conditional tests must be satisfied. In our work, best-first parsing is done by inscribing the four entries with maximum probabilities, Lt (i, j), L~ (i, j), St(i,j), and Sr(i,j) to each chart positions in bottom-up/left-to-right manner without any extra condition checking. Figure 4 depicts the possible combinations of char"
W97-0107,P92-1017,0,0.0583102,"also to include lexiccal information to increase the correctness (Magerman, 1994, Collir~, 1996). This means that the lack of lexical information in phrase structure grammar is a major weak point for syntactic disambiguation. Besides the lack of lexical information, the induction of phrase structure grnmmar may suffer from structural data sparseness with medium sized training corpus. The structural data sparseness means the lack of information on the grammar rules. An approach to increase the correctness of grammar induction is to learn a grammar from a tree-tagged corpus or bracketed corpus (Pereira and Schabes, 1992, Black, Lafferty, and Roukos, 1992). But the construction of vast sized tree-corpus or bracketed corpus is very labour-intensive and manual construction of such corpus may produce serious inconsistencies. And the structural-data sparseness problem still remains. The problems of structural-data sparseness and lack of lexical information can be lessened with PDG. Dependency grammar defines a language as a set of dependency relations between any two words. The basic units of sentence structure in DG, the dependency relations are much simpler than the rules in phrase structure grnmmar. So, the se"
W97-0107,P96-1025,0,\N,Missing
W97-0107,P92-1024,0,\N,Missing
W97-0125,W93-0311,0,0.0398913,"Missing"
W97-0125,C94-1087,1,0.884519,"Missing"
W97-0125,E93-1023,0,0.0529094,"Missing"
W97-0125,E93-1035,0,\N,Missing
W99-0635,P87-1009,0,0.0697789,"Missing"
W99-0635,J93-1005,0,0.126539,"Missing"
W99-0635,P84-1109,0,0.0677284,"Missing"
W99-0635,C88-2110,0,0.0233297,"Missing"
W99-0635,P87-1020,0,0.0545987,"Missing"
W99-0635,C94-2139,0,\N,Missing
W99-0635,P95-1007,0,\N,Missing
Y00-1013,J93-1004,0,\N,Missing
Y00-1013,J93-2003,0,\N,Missing
Y00-1013,C94-1009,0,\N,Missing
Y00-1013,P98-2212,0,\N,Missing
Y00-1013,C98-2207,0,\N,Missing
Y00-1013,P93-1001,0,\N,Missing
Y00-1013,P93-1003,0,\N,Missing
Y00-1013,P91-1017,0,\N,Missing
Y00-1013,P94-1012,0,\N,Missing
Y02-1028,W98-0709,0,0.0569115,"Missing"
Y96-1014,J87-3001,0,\N,Missing
