2015.mtsummit-papers.23,J93-2003,0,0.0470688,"x, we note that the number of possible translation candidates Tx is much smaller compared to the target vocabulary size Ky . Instead of calculating the Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 303 Figure 3: Deep Output Layer of RNN Decoder translation probability for every word in the target vocabulary, we only calculate the probability for a limited set of target candidates which are relevant to the given x. The problem is how to get such a candidate set. Intuitively, we can make use of the word alignment information generated with IBM model (Brown et al., 1993). And we found that there are often too many candidates especially for the common words, which makes the improvement limited. Actually, we can get more accurate translation candidates from the phrase pairs of the phrase-based translation model (Koehn et al., 2003). Firstly, we train a phrase-based translation model with the word aligned bilingual sentence pairs. Then we segment the input sentence x into a number of continuous source phrases. Next, we search the corresponding target phrases for all source phrases from the phrase-based translation model, namely phrase table. After that, we build"
2015.mtsummit-papers.23,W14-4012,0,0.0860863,"Missing"
2015.mtsummit-papers.23,P14-1129,0,0.0728845,"Missing"
2015.mtsummit-papers.23,P07-1019,0,0.173351,"Missing"
2015.mtsummit-papers.23,koen-2004-pharaoh,0,0.21634,"existing system, Cho et al. (2014) proposed a whole new RNN Encoder-Decoder approach which generates the target translation with a neural network directly. Bahdanau et al. (2014) extended the above approach by allowing an RNN model to automatically (soft-)search for parts of a source sentence to predict a target word, and achieved a translation performance comparable to the existing state-of-the-art phrase-based systems (Koehn et al., 2003). Although NMT shows high potential, its decoding efficiency is still challenging. Cho et al. (2014) implemented a standard beam search decoding algorithm (Koehn, 2004) for an RNN Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 297 Encoder-Decoder system published as GroundHog 1 . The beam search algorithm successfully reduces the search space from exponential size to polynomial size, and it is able to translate about ten words per second. Even though the speed is acceptable for most research tasks, it is not yet efficient enough to meet the requirement of commercial systems for providing real-time translation service. Huang and Chiang (2007) proposed the forest rescoring algorithm which succeed to accelerate the dec"
2015.mtsummit-papers.23,P07-2045,0,0.0122377,"of corpus for NIST08 task, containing 67M Chinese words and 74M English words, to train our models. No monolingual corpus are used to help the model training. The NIST MT03 Chineseto-English test set is used as the development set, and NIST MT08 Chinese-to-English test set is used to evaluate our translation results. The development set is used to choose the best RNN Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 304 model in history, because the performance of RNN model will fluctuate during training. 4.2 Toolkits The open source SMT system Moses 2 (Koehn et al., 2007) is used to train a phrase-based machine translation system. The phrase table trained with Moses will be used to constrain the softmax translation. We use the open source RNN Encoder-Decoder toolkits GroundHog which is implemented with Theano (Bergstra et al., 2010; Bastien et al., 2012) to train a neural machine translation model. It is re-implemented with C++ in the GPU Environment and named NetTrans, which is well optimized and faster than GroundHog which is implemented in Python. Given the same model, there is no translation differences between GroundHog and NetTrans. We will report our ex"
2015.mtsummit-papers.23,N03-1017,0,0.112704,"models into the traditional translation decoders as additional features in a log-linear framework (Och and Ney, 2002). Rather than applying a neural network as a part of the existing system, Cho et al. (2014) proposed a whole new RNN Encoder-Decoder approach which generates the target translation with a neural network directly. Bahdanau et al. (2014) extended the above approach by allowing an RNN model to automatically (soft-)search for parts of a source sentence to predict a target word, and achieved a translation performance comparable to the existing state-of-the-art phrase-based systems (Koehn et al., 2003). Although NMT shows high potential, its decoding efficiency is still challenging. Cho et al. (2014) implemented a standard beam search decoding algorithm (Koehn, 2004) for an RNN Proceedings of MT Summit XV, vol.1: MT Researchers' Track Miami, Oct 30 - Nov 3, 2015 |p. 297 Encoder-Decoder system published as GroundHog 1 . The beam search algorithm successfully reduces the search space from exponential size to polynomial size, and it is able to translate about ten words per second. Even though the speed is acceptable for most research tasks, it is not yet efficient enough to meet the requiremen"
2015.mtsummit-papers.23,P02-1038,0,0.42432,"Missing"
2015.mtsummit-papers.23,D12-1089,0,0.0168434,"s. Finally, we constrain the softmax operation with the candidate word set for all the hypothesis extension of x. One disadvantage of above approach is that it is too costly to load the whole phrase table into memory. Fortunately, we can reduce the memory cost by pruning phrase table carefully. The phrase table filtering techniques have been widely used in machine translation. Instead of considering both the phrase coverage and translation features in the filtering technique, our system only considers the word coverage, which is much easier to implement. We investigated the histogram pruning (Zens et al., 2012) and length-based pruning method to reduce the memory used in our system. For histogram pruning, we preserves the X target phrases with highest probability for each source phrase. For length-based pruning, we prune the phrase pairs which contain more than X words in source or target side. Where X is the threshold. We found that even the basic length-based pruning method works well enough in our task. 4 Experimental Settings In this section, we evaluate the improved beam search algorithm on the task of Chinese-toEnglish translation. 4.1 Dataset In this paper, we use Chinese-to-English bilingual"
2020.acl-main.555,J05-3002,0,0.302043,"raph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al.,"
2020.acl-main.555,P11-1049,0,0.0446647,"Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (L"
2020.acl-main.555,P15-1153,0,0.155763,"lutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply tr"
2020.acl-main.555,N18-1150,0,0.0346674,"ed (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering th"
2020.acl-main.555,N13-1136,0,0.770568,"ecting redundancy and generating overall coherent summaries for MDS. Graphs that capture ∗ Corresponding author. relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of documents such as similarity graph based on lexical similarities (Erkan and Radev, 2004) and discourse graph based on discourse relations (Christensen et al., 2013), have been widely used in traditional graph-based extractive MDS models. However, they are not well studied by most abstractive approaches, especially the end-to-end neural approaches. Few work has studied the effectiveness of explicit graph representations on neural abstractive MDS. In this paper, we develop a neural abstractive MDS model which can leverage explicit graph representations of documents to more effectively process multiple input documents and distill abstractive summaries. Our model augments the end-toend neural architecture with the ability to incorporate well-established grap"
2020.acl-main.555,N19-1423,0,0.474382,"ontent. Benefiting from the graph modeling, our model can extract salient information from long documents and generate coherent summaries more effectively. We experiment with three types of graph representations, including similarity graph, topic graph and discourse graph, which all significantly improve the MDS performance. 6232 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6232–6243 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Additionally, our model is complementary to most pre-trained language models (LMs), like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLNet (Yang et al., 2019b). They can be easily combined with our model to process much longer inputs. The combined model adopts the advantages of both our graph model and pre-trained LMs. Our experimental results show that our graph model significantly improves the performance of pre-trained LMs on MDS. The contributions of our paper are as follows: • Our work demonstrates the effectiveness of graph modeling in neural abstractive MDS. We show that explicit graph representations are beneficial for both document representation and summary generation. • We propose"
2020.acl-main.555,N19-1409,0,0.022001,"ument relations, thus achieves significantly better performance.We also leverage the graph structure to guide the summary decoding process, which is beneficial for long summary generation. Additionally, we combine the advantages of pretrained LMs into our model. 2.3 Summarization with Pretrained LMs Pretrained LMs (Peters et al., 2018; Radford et al.; Devlin et al., 2019; Dong et al., 2019; Sun et al., 2019) have recently emerged as a key technology for achieving impressive improvements in a wide variety of natural language tasks, including both language understanding and language generation (Edunov et al., 2019; Rothe et al., 2019). Liu and Lapata (2019b) attempt to incorporate pre-trained BERT encoder into SDS model and achieves significant improvements. Dong et al. (2019) further propose a unified LM for both language understanding and language generation tasks, which achieves state-of-the-art results on several generation tasks including SDS. In this work, we propose an effective method to combine pretrained LMs with our graph model and make them be able to process much longer inputs effectively. 3 Model Description In order to process long source documents more effectively, we follow Liu and Lap"
2020.acl-main.555,P19-1102,0,0.415565,"ansfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) further propose to construct a local knowledge graph from documents and then linearize the graph into a sequence to better sale Seq2Seq models to multidocument inputs. Fabbri et al. (2019) also introduce a middle-scale (about 50K) MDS news dataset (namely MultiNews), and propose an end-to-end model by incorporating traditional MMR-based 6233 Graph Encoding Layer Graph Decoding Layer Add & Normalize Add & Normalize Feed Forward Feed Forward Feed Forward Feed Forward Add & Normalize Add & Normalize Graph-informed Self-Attention Hierarchical Graph Attention PARAGRAPH POSITION ENCODING Add & Normalize Transformer Masked Self-Attention Transformer TOKEN POSITION ENCODING POSITIONAL ENCODING first paragraph last paragraph token1 END Figure 1: Illustration of our model, which follows"
2020.acl-main.555,D19-1428,0,0.119197,"9a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) further propose to construct a local knowledge graph from documents and then linearize the graph into a sequence to better sale Seq2Seq models to multidocument inputs. Fabbri et al. (2019) also introduce a middle-scale (about 50K) MDS news dataset (namely MultiNews), and propose an end-to-end model by incorporating traditional MMR-based 6233 Graph Encoding Layer Graph Decoding Layer Add & Normalize Add & Normalize Feed Forward Feed Forward Feed Forward Feed Forward Add & Normalize Add & Normalize Graph-informed Self-Attention Hierarchical Graph Attention PARAGRAPH POSITION ENCODING Add & Norm"
2020.acl-main.555,D08-1019,0,0.248382,"l. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan"
2020.acl-main.555,D18-1443,0,0.0774954,"de: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2S"
2020.acl-main.555,W11-1608,0,0.0265118,"The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack o"
2020.acl-main.555,D18-1446,0,0.40232,"Missing"
2020.acl-main.555,D15-1219,1,0.902847,"structed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 20"
2020.acl-main.555,D18-1205,1,0.787509,"Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input"
2020.acl-main.555,D18-1441,1,0.688871,"Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input"
2020.acl-main.555,C18-1101,0,0.0528075,"s between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu"
2020.acl-main.555,P04-1077,0,0.0143827,"l Lead LexRank FT BERT+FT XLNet+FT RoBERTa+FT T-DMCA HT GraphSum GraphSum+RoBERTa R-1 38.22 36.12 40.56 41.49 40.85 42.05 40.77 41.53 42.63 42.99 R-2 16.85 11.67 25.35 25.73 25.29 27.00 25.60 26.52 27.70 27.83 Model Lead LexRank PG-BRNN HiMAP FT RoBERTa+FT HT GraphSum R-L 26.89 22.52 34.73 35.59 35.20 36.56 34.90 35.76 36.97 37.36 G.S.(Similarity)+RoBERTa G.S.(Topic)+RoBERTa G.S.(Discourse)+RoBERTa Table 1: Evaluation results on the WikiSum test set using ROUGE F1 . R-1, R-2 and R-L are abbreviations for ROUGE-1, ROUGE-2 and ROUGE-L, respectively. rization quality is evaluated using ROUGE F1 (Lin and Och, 2004). We report unigram and bigram overlap (ROUGE-1 and ROUGE-2) between system summaries and gold references as a means of assessing informativeness, and the longest common subsequence (ROUGE-L2 ) as a means of accessing fluency. Results on WikiSum Table 6 summarizes the evaluation results on the WikiSum dataset. Several strong extractive baselines and abstractive baselines are also evaluated and compared with our models. The first block in the table shows the results of extractive methods Lead and LexRank (Erkan and Radev, 2004). The second block shows the results of abstractive methods: (1) FT"
2020.acl-main.555,P19-1500,0,0.195307,"n Transformer TOKEN POSITION ENCODING POSITIONAL ENCODING first paragraph last paragraph token1 END Figure 1: Illustration of our model, which follows the encoder-deocder architecture. The encoder is a stack of transformer layers and graph encoding layers, while the decoder is a stack of graph decoding layers. We incorporate explicit graph representations into both the graph encoding layers and graph decoding layers. extractive model with a standard Seq2Seq model. The above Seq2Seq models haven’t study the importance of cross-document relations and graph representations in MDS. Most recently, Liu and Lapata (2019a) propose a hierarchical transformer model to utilize the hierarchical structure of documents. They propose to learn cross-document relations based on selfattention mechanism. They also propose to incorporate explicit graph representations into the model by simply replacing the attention weights with a graph matrix, however, it doesn’t achieve obvious improvement according to their experiments. Our work is partly inspired by this work, but our approach is quite different from theirs. In contrast to their approach, we incorporate explicit graph representations into the encoding process via a g"
2020.acl-main.555,D19-1387,0,0.0797698,"n Transformer TOKEN POSITION ENCODING POSITIONAL ENCODING first paragraph last paragraph token1 END Figure 1: Illustration of our model, which follows the encoder-deocder architecture. The encoder is a stack of transformer layers and graph encoding layers, while the decoder is a stack of graph decoding layers. We incorporate explicit graph representations into both the graph encoding layers and graph decoding layers. extractive model with a standard Seq2Seq model. The above Seq2Seq models haven’t study the importance of cross-document relations and graph representations in MDS. Most recently, Liu and Lapata (2019a) propose a hierarchical transformer model to utilize the hierarchical structure of documents. They propose to learn cross-document relations based on selfattention mechanism. They also propose to incorporate explicit graph representations into the model by simply replacing the attention weights with a graph matrix, however, it doesn’t achieve obvious improvement according to their experiments. Our work is partly inspired by this work, but our approach is quite different from theirs. In contrast to their approach, we incorporate explicit graph representations into the encoding process via a g"
2020.acl-main.555,2021.ccl-1.108,0,0.172335,"Missing"
2020.acl-main.555,C16-1143,0,0.164968,"Missing"
2020.acl-main.555,W04-3252,0,0.204168,"ing that graph modeling enables our model process longer inputs with better performance, and graphs with richer relations are more beneficial for MDS.1 2 Related Work 2.1 Graph-based MDS Most previous MDS approaches are extractive, which extract salient textual units from documents based on graph-based representations of sentences. Various ranking methods have been developed to rank textual units based on graphs to select most salient ones for inclusion in the final summary. Erkan and Radev (2004) propose LexRank to compute sentence importance based on a lexical similarity graph of sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been 1 Codes and results are in: https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2020-GraphSum further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009; Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neura"
2020.acl-main.555,P14-1084,0,0.061278,"ased on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straigh"
2020.acl-main.555,W00-1009,0,0.690742,"he summarization performance significantly. Empirical results on the WikiSum and MultiNews dataset show that the proposed architecture brings substantial improvements over several strong baselines. 1 Introduction Multi-document summarization (MDS) brings great challenges to the widely used sequence-tosequence (Seq2Seq) neural architecture as it requires effective representation of multiple input documents and content organization of long summaries. For MDS, different documents may contain the same content, include additional information, and present complementary or contradictory information (Radev, 2000). So different from single document summarization (SDS), cross-document links are very important in extracting salient information, detecting redundancy and generating overall coherent summaries for MDS. Graphs that capture ∗ Corresponding author. relations between textual units have great benefits to MDS, which can help generate more informative, concise and coherent summaries from multiple documents. Moreover, graphs can be easily constructed by representing text spans (e.g. sentences, paragraphs etc.) as graph nodes and the semantic links between them as edges. Graph representations of docu"
2020.acl-main.555,P17-1099,0,0.154693,"ss. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova and Strube, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely Wiki"
2020.acl-main.555,D19-1323,0,0.029316,"lay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) furth"
2020.acl-main.555,D18-1206,0,0.0452166,"e, 2008; Barzilay and McKeown, 2005; Barzilay, 2003), information extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenat"
2020.acl-main.555,D18-1108,0,0.0263741,"nd the Transformer with a hierarchical graph attention mechanism to utilize explicit graph structure to guide the summary decoding process. In the following, we will focus on the graph encoding layer and graph decoding layer of our model. 3.1 Graph Encoding Layer As shown in Figure 1, based on the output of the token-level transformer encoding layers, the graph encoding layer is used to encode all documents globally. Most existing neural work only utilizes attention mechanism to learn latent graph representations of documents where the graph edges are attention weights (Liu and Lapata, 2019a; Niculae et al., 2018; Fernandes et al., 2018). However, much work in traditional MDS has shown that explicit graph representations are very beneficial to MDS. Different types of graphs capture different kinds of semantic relations (e.g. lexical relations or discourse relations), which can help the model focus on different facets of the summarization task. In this work, we propose to incorporate explicit graph representations into the neural encoding process via a graph-informed attention mechanism. It takes advantage of the explicit relations in graphs to learn better inter-paragraph relations. Each paragraph can"
2020.acl-main.555,P19-1504,0,0.0828261,"on extractionbased (Li, 2015; Pighin et al., 2014; Wang and Cardie, 2013; Genest and Lapalme, 2011; Li and Zhuge, 2019) and paraphrasing-based (Bing et al., 2015; Berg-Kirkpatrick et al., 2011; Cohn and Lapata, 2009). More recently, some researches parse the source text into AMR representation and then generate summary based on it (Liao et al., 2018). Although neural abstractive models have achieved promising results on SDS (See et al., 2017; Paulus et al., 2018; Gehrmann et al., 2018; Celikyilmaz et al., 2018; Li et al., 2018a,b; Narayan et al., 2018; Yang et al., 2019a; Sharma et al., 2019; Perez-Beltrachini et al., 2019), it’s not straightforward to extend them to MDS. Due to the lack of sufficient training data, earlier approaches try to simply transfer SDS model to MDS task (Lebanoff et al., 2018; Zhang et al., 2018; Baumel et al., 2018) or utilize unsupervised models relying on reconstruction objectives (Ma et al., 2016; Chu and Liu, 2019). Later, Liu et al. (2018) propose to construct a large scale MDS dataset (namely WikiSum) based on Wikipedia, and develop a Seq2Seq model by considering the multiple input documents as a concatenated flat sequence. Fan et al. (2019) further propose to construct a local k"
2020.acl-main.555,N18-1202,0,0.0230266,"pproach is quite different from theirs. In contrast to their approach, we incorporate explicit graph representations into the encoding process via a graphinformed attention mechanism. Under the guidance of explicit relations in graphs, our model can learn better and richer cross-document relations, thus achieves significantly better performance.We also leverage the graph structure to guide the summary decoding process, which is beneficial for long summary generation. Additionally, we combine the advantages of pretrained LMs into our model. 2.3 Summarization with Pretrained LMs Pretrained LMs (Peters et al., 2018; Radford et al.; Devlin et al., 2019; Dong et al., 2019; Sun et al., 2019) have recently emerged as a key technology for achieving impressive improvements in a wide variety of natural language tasks, including both language understanding and language generation (Edunov et al., 2019; Rothe et al., 2019). Liu and Lapata (2019b) attempt to incorporate pre-trained BERT encoder into SDS model and achieves significant improvements. Dong et al. (2019) further propose a unified LM for both language understanding and language generation tasks, which achieves state-of-the-art results on several generat"
2020.acl-main.555,P13-1137,0,0.124191,"Missing"
2020.acl-main.555,K17-1045,0,0.0407222,"f sentences. Mihalcea and Tarau (2004) propose a graph-based ranking model to extract salient sentences from documents. Wan (2008) further proposes to incorporate documentlevel information and sentence-to-document relations into the graph-based ranking process. A series of variants of the PageRank algorithm has been 1 Codes and results are in: https://github.com/ PaddlePaddle/Research/tree/master/NLP/ ACL2020-GraphSum further developed to compute the salience of textual units recursively based on various graph representations of documents (Wan and Xiao, 2009; Cai and Li, 2012). More recently, Yasunaga et al. (2017) propose a neural graph-based model for extractive MDS. An approximate discourse graph is constructed based on discourse markers and entity links. The salience of sentences is estimated using features from graph convolutional networks (Kipf and Welling, 2016). Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes entity linking graph to capture the global dependencies between sentences. 2.2 Abstractive MDS Abstractive MDS approaches have met with limited success. Traditional approaches mainly include: sentence fusion-based (Banerjee et al., 2015; Filippova"
2020.acl-main.555,D08-1079,0,\N,Missing
2020.ccl-1.44,N16-1012,0,0.0671351,"Missing"
2020.ccl-1.44,K16-1028,0,0.0500478,"Missing"
2020.ccl-1.44,P17-1099,0,0.0779988,"Missing"
2021.acl-long.202,2020.acl-main.703,0,0.0295517,"rained on the ImageNet dataset. Recently, contrastive self-supervised learning like SimCLR (Chen et al., 2020a) and MoCo (He et al., 2020) also greatly improve the performance of visual representation learning. These pre-trained models only focus on visual tasks (e.g. image classification etc.), however, they cannot be used in textual or multimodal (i.e., with both text and image) tasks. The language pre-training methods based on the Transformer architecture are also very popular in NLP models, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and BART (Lewis et al., 2020). However, they mainly focus on textual tasks. They cannot effectively deal with the multi-modal tasks, such as image-text retrieval, image captioning, multimodal machine translation (Lin et al., 2020a; Su et al., 2021) and visual dialog (Murahari et al., 2020). Multi-Modal Pre-training Recently, multimodal pre-training methods have been more and more popular for solving the multi-modal tasks. All of them are trained on a corpus of image-text pairs, such as ViLBERT (Lu et al., 2019), VisualBERT (Li et al., 2019b), VL-BERT (Su et al., 2019), Unicoder-VL (Li et al., 2019a) and UNITER (Chen et al"
2021.acl-long.202,2021.ccl-1.108,0,0.0544888,"Missing"
2021.acl-long.202,D16-1264,0,0.0350289,"UNIMO-base by initializing from RoBERTa-base, and UNIMO-large by initializing from RoBERTa-large. Both UNIMObase and UNIMO-large are trained for at least 500K steps. An Adam optimizer with initial learning rate 3.3 Finetuning Tasks We fine-tune our model on two categories of downstream tasks: (1) single-modal language understanding and generation tasks; (2) multimodal vision-language understanding and generation tasks. The single-modal generation tasks include: generative conversational question answering on the CoQA dataset (Reddy et al., 2019), question generation on the SQuAD 1.1 dataset (Rajpurkar et al., 2016), abstractive summarization on the CNN/DailyMail (CNNDM) dataset (Hermann et al., 2015), and sentence compression on the Gigaword dataset (Rush et al., 2015). The single-modal understanding tasks include: sentiment classification on the SST-2 dataset (Socher et al., 2013), natural language inference on the MNLI dataset (Williams et al., 2017), linguistic acceptability analysis on the CoLA dataset (Warstadt et al., 2019) and semantic similarity analysis on the STS-B dataset (Cer et al., 2017). The multi-modal tasks include: visual question answering (VQA) on the VQA v2.0 dataset (Goyal et al.,"
2021.acl-long.202,Q19-1016,0,0.0140192,"-region features are set as 512 and 100, respectively. We pre-train UNIMO-base by initializing from RoBERTa-base, and UNIMO-large by initializing from RoBERTa-large. Both UNIMObase and UNIMO-large are trained for at least 500K steps. An Adam optimizer with initial learning rate 3.3 Finetuning Tasks We fine-tune our model on two categories of downstream tasks: (1) single-modal language understanding and generation tasks; (2) multimodal vision-language understanding and generation tasks. The single-modal generation tasks include: generative conversational question answering on the CoQA dataset (Reddy et al., 2019), question generation on the SQuAD 1.1 dataset (Rajpurkar et al., 2016), abstractive summarization on the CNN/DailyMail (CNNDM) dataset (Hermann et al., 2015), and sentence compression on the Gigaword dataset (Rush et al., 2015). The single-modal understanding tasks include: sentiment classification on the SST-2 dataset (Socher et al., 2013), natural language inference on the MNLI dataset (Williams et al., 2017), linguistic acceptability analysis on the CoLA dataset (Warstadt et al., 2019) and semantic similarity analysis on the STS-B dataset (Cer et al., 2017). The multi-modal tasks include:"
2021.acl-long.202,D15-1044,0,0.04454,"eps. An Adam optimizer with initial learning rate 3.3 Finetuning Tasks We fine-tune our model on two categories of downstream tasks: (1) single-modal language understanding and generation tasks; (2) multimodal vision-language understanding and generation tasks. The single-modal generation tasks include: generative conversational question answering on the CoQA dataset (Reddy et al., 2019), question generation on the SQuAD 1.1 dataset (Rajpurkar et al., 2016), abstractive summarization on the CNN/DailyMail (CNNDM) dataset (Hermann et al., 2015), and sentence compression on the Gigaword dataset (Rush et al., 2015). The single-modal understanding tasks include: sentiment classification on the SST-2 dataset (Socher et al., 2013), natural language inference on the MNLI dataset (Williams et al., 2017), linguistic acceptability analysis on the CoLA dataset (Warstadt et al., 2019) and semantic similarity analysis on the STS-B dataset (Cer et al., 2017). The multi-modal tasks include: visual question answering (VQA) on the VQA v2.0 dataset (Goyal et al., 2017), image caption on the Microsoft COCO Captions dataset (Chen et al., 2015), visual entailment on the SNLI-VE dataset (Xie et al., 2019) and image-text r"
2021.acl-long.202,P16-1162,0,0.0126841,"learn representations that capture modality-invariant information at the semantic level. Different from previous methods, UNIMO learns from different modalities of data, including images, texts and image-text pairs, thus achieving more robust and generalizable representations for both textual and visual input. As shown in Figure 2, UNIMO employs multi-layer self-attention Transformers to learn unified semantic representations for both textual and visual data. For a textual input W, it is firstly split into a sequence of subwords W = {[CLS], w1 , ..., wn , [SEP ]} by Byte-Pair Encoding (BPE) (Sennrich et al., 2016), and then the self-attention mechanism is leveraged to learn contextual token representations {h[CLS] , hw1 , ..., hwn , h[SEP ] }. The special tokens [CLS] and [SEP ] denote the start and end of the textual sequence, respectively. Similarly, for an image V, it is firstly converted to a sequence of region features V = {[IM G], v1 , ..., vt } ([IM G] denotes the representation of the entire image), and then the self-attention mechanism is leveraged to learn contextual region representations {h[IM G] , hv1 , ..., hvt }. Similar to previous work (Chen et al., 2020b), we use Faster R-CNN (Ren et"
2021.acl-long.202,P18-1238,0,0.0530716,"Missing"
2021.acl-long.365,N18-1165,0,0.0175884,"ouillon et al., 2016; Dettmers et al., 2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention. All of them attend to learn the distributed embeddings for entities and relations in KGs. Among them, some works (Schlichtkrull et al., 2018; Shang et al., 2019; Ye et al., 2019; Vashishth et al., 2019) extend GCN to relationaware GCN for the KGs. However, embedding based models underestimate the symbolic compositionality of relations in KGs, which limits their usage in more complex reasoning tasks. Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths. However, all the above methods cannot deal with the temporal dependencies among facts in TKGs. Temporal KG Reasoning. Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garc´ıaDur´an et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al., 2017, 2018; Han et al.,"
2021.acl-long.365,D18-1225,0,0.0282269,"sitionality of relations in KGs, which limits their usage in more complex reasoning tasks. Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths. However, all the above methods cannot deal with the temporal dependencies among facts in TKGs. Temporal KG Reasoning. Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garc´ıaDur´an et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al., 2017, 2018; Han et al., 2020b; Deng et al., 2020; Jin et al., 2019, 2020; Zhu et al., 2020; Li et al., 2021), as mentioned in Jin et al. (2020). Under the former setting, models attempt to infer missing facts at historical timestamps. While the latter setting, which this paper focuses on, attempts to predict facts in the future. Orthogonal to our work, Trivedi et al. (2017, 2018) estimate the conditional probability of observing a future fact via a te"
2021.acl-long.365,D18-1516,0,0.0526938,"Missing"
2021.acl-long.365,P17-1097,0,0.0296417,"yer Perceptron (MLP) parameterized with W1 and W2 as follows: π(ai |si ;Θ) = η(Ai W2 f (W1 [ei ⊕ hi ⊕ rq ]), (3) where η(·) is the softmax function, f (·) is the ReLU function (Glorot et al., 2011) and Θ is the set of all the learnable parameters in Stage 1. 3.3.3 Randomized Beam Search In the scenario of TKGs, the occurrence of a fact may result from multiple factors. Thus, multiple clue paths are necessary for the prediction. Besides, the intuitive candidates from Stage 1 should recall the right answers as many as possible. Therefore, we adopt randomized beam search (Sutskever et al., 2014; Guu et al., 2017; Wu et al., 2018) as the action sampling strategy of the agent, which injects random noise to the beam search in order to increase the exploration ability of the agent. Specifically, a beam contains B candidate clue paths at step i. For each candidate path, we append B most likely actions (according to Equation 3) to the end of the path, resulting in a new path pool with size B × B. Then we either pick the highestscoring paths with probability µ or uniformly sample a random path with probability 1 − µ repeatedly for B times. The score P of each candidate clue path at step i equals to ik=0 log"
2021.acl-long.365,2020.emnlp-main.593,0,0.264455,"us, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths. However, all the above methods cannot deal with the temporal dependencies among facts in TKGs. Temporal KG Reasoning. Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garc´ıaDur´an et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al., 2017, 2018; Han et al., 2020b; Deng et al., 2020; Jin et al., 2019, 2020; Zhu et al., 2020; Li et al., 2021), as mentioned in Jin et al. (2020). Under the former setting, models attempt to infer missing facts at historical timestamps. While the latter setting, which this paper focuses on, attempts to predict facts in the future. Orthogonal to our work, Trivedi et al. (2017, 2018) estimate the conditional probability of observing a future fact via a temporal point process taking all historical facts into consideration. Although Han et al."
2021.acl-long.365,2020.emnlp-main.305,0,0.0141068,"13). We evaluate CluSTeR on all these datasets. ICEWS14 and ICEWS05-15 are divided into training, validation, and test sets following the preprocessing on ICEWS18 in RE-NET (Jin et al., 2020). The details of the datasets are presented in Table 1. In the experiments, the widely used Mean Reciprocal Rank (MRR) and Hits@{1,10} are employed as the metrics. Without loss of generality, only the experimental results under the raw setting are 4736 reported. The filtered setting is not suitable for the reasoning task under the exploration setting, as mentioned in (Han et al., 2020b; Ding et al., 2021; Jain et al., 2020). The reason is explained in terms of an example as follows: Given a test quadruple (Barack Obama, visit,?, 2015-1-25) with the correct answer India. Assume there is a quadruple (Barack Obama, visit, Germany, 2013-1-18) in the training set. The filtered setting used in the previous studies ignores time information and considers (Barack Obama, visit, Germany, 20151-25) to be valid because (Barack Obama, visit, Germany, 2013-1-18) appears in the training set. It thus removes the quadruple from the corrupted ones. However, the fact (Barack Obama, visit, Germany) is temporally valid on 2013-1-18,"
2021.acl-long.365,2020.emnlp-main.541,0,0.489111,"and financial analysis (Bollen et al., 2011). Introduction Temporal Knowledge Graphs (TKGs) (Boschee et al., 2015; Gottschalk and Demidova, 2018, 2019; Zhao, 2020) have emerged as a very active research area over the last few years. Each fact in TKGs has a timestamp indicating its time of occurrence. For example, the fact, (COVID-19, New medical case occur, Shop, 2020-10-2), indicates that a new medical case of COVID-19 occurred in a shop on 2020-10-2. In this paper, reasoning on TKGs aims to predict future facts (events) for timestamp t &gt; tT , where tT is assumed to be the current timestamp (Jin et al., 2020). An example of the task is shown in Figure 1, which attempts to answer the query (COVID-19, New medical case occur, ?, 2020-12-23) with the given historical facts. Obviously, such a task may benefit many practical How do human beings predict future events? According to the dual process theory (Evans, 1984, 2003, 2008; Sloman, 1996), the first thing is to search the massive-capacity memories and find some related historical information (i.e., clues) intuitively. As shown in the left part of Figure 1, there are mainly three categories of clues vital to the query: 1) the 1-hop paths with the sam"
2021.acl-long.365,D19-1266,0,0.028132,"2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention. All of them attend to learn the distributed embeddings for entities and relations in KGs. Among them, some works (Schlichtkrull et al., 2018; Shang et al., 2019; Ye et al., 2019; Vashishth et al., 2019) extend GCN to relationaware GCN for the KGs. However, embedding based models underestimate the symbolic compositionality of relations in KGs, which limits their usage in more complex reasoning tasks. Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths. However, all the above methods cannot deal with the temporal dependencies among facts in TKGs. Temporal KG Reasoning. Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garc´ıaDur´an et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al., 2017, 2018; Han et al., 2020b; Deng et al., 2020; Jin et al., 2"
2021.acl-long.365,D18-1362,0,0.0162158,"g et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention. All of them attend to learn the distributed embeddings for entities and relations in KGs. Among them, some works (Schlichtkrull et al., 2018; Shang et al., 2019; Ye et al., 2019; Vashishth et al., 2019) extend GCN to relationaware GCN for the KGs. However, embedding based models underestimate the symbolic compositionality of relations in KGs, which limits their usage in more complex reasoning tasks. Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths. However, all the above methods cannot deal with the temporal dependencies among facts in TKGs. Temporal KG Reasoning. Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garc´ıaDur´an et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al., 2017"
2021.acl-long.365,D19-1264,0,0.0133813,"6; Dettmers et al., 2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention. All of them attend to learn the distributed embeddings for entities and relations in KGs. Among them, some works (Schlichtkrull et al., 2018; Shang et al., 2019; Ye et al., 2019; Vashishth et al., 2019) extend GCN to relationaware GCN for the KGs. However, embedding based models underestimate the symbolic compositionality of relations in KGs, which limits their usage in more complex reasoning tasks. Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths. However, all the above methods cannot deal with the temporal dependencies among facts in TKGs. Temporal KG Reasoning. Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garc´ıaDur´an et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al., 2017, 2018; Han et al., 2020b; Deng et al."
2021.acl-long.365,2020.emnlp-main.462,0,0.0376469,"asoning tasks. Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths. However, all the above methods cannot deal with the temporal dependencies among facts in TKGs. Temporal KG Reasoning. Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garc´ıaDur´an et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and extrapolation (Trivedi et al., 2017, 2018; Han et al., 2020b; Deng et al., 2020; Jin et al., 2019, 2020; Zhu et al., 2020; Li et al., 2021), as mentioned in Jin et al. (2020). Under the former setting, models attempt to infer missing facts at historical timestamps. While the latter setting, which this paper focuses on, attempts to predict facts in the future. Orthogonal to our work, Trivedi et al. (2017, 2018) estimate the conditional probability of observing a future fact via a temporal point process taking all historical facts into consideration. A"
2021.acl-long.365,D18-1397,0,0.0167893,"P) parameterized with W1 and W2 as follows: π(ai |si ;Θ) = η(Ai W2 f (W1 [ei ⊕ hi ⊕ rq ]), (3) where η(·) is the softmax function, f (·) is the ReLU function (Glorot et al., 2011) and Θ is the set of all the learnable parameters in Stage 1. 3.3.3 Randomized Beam Search In the scenario of TKGs, the occurrence of a fact may result from multiple factors. Thus, multiple clue paths are necessary for the prediction. Besides, the intuitive candidates from Stage 1 should recall the right answers as many as possible. Therefore, we adopt randomized beam search (Sutskever et al., 2014; Guu et al., 2017; Wu et al., 2018) as the action sampling strategy of the agent, which injects random noise to the beam search in order to increase the exploration ability of the agent. Specifically, a beam contains B candidate clue paths at step i. For each candidate path, we append B most likely actions (according to Equation 3) to the end of the path, resulting in a new path pool with size B × B. Then we either pick the highestscoring paths with probability µ or uniformly sample a random path with probability 1 − µ repeatedly for B times. The score P of each candidate clue path at step i equals to ik=0 log π(ak |sk ; Θ). No"
2021.acl-long.365,D17-1060,0,0.028403,"oning models (Bordes et al., 2013; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018; Shang et al., 2019; Sun et al., 2018) have drawn increasing attention. All of them attend to learn the distributed embeddings for entities and relations in KGs. Among them, some works (Schlichtkrull et al., 2018; Shang et al., 2019; Ye et al., 2019; Vashishth et al., 2019) extend GCN to relationaware GCN for the KGs. However, embedding based models underestimate the symbolic compositionality of relations in KGs, which limits their usage in more complex reasoning tasks. Thus, some recent works (Xiong et al., 2017; Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Wang et al., 2019; Li and Cheng, 2019) focus on multi-hop reasoning, which learns symbolic inference rules from relation paths. However, all the above methods cannot deal with the temporal dependencies among facts in TKGs. Temporal KG Reasoning. Reasoning on temporal KG can broadly be categorized into two settings, interpolation (Sadeghian et al., 2016; Garc´ıaDur´an et al., 2018; Leblay and Chekol, 2018; Dasgupta et al., 2018; Wu et al., 2019; Xu et al., 2020; Goel et al., 2020; Wu et al., 2020; Han et al., 2020a; Jung et al., 2020) and"
2021.acl-long.472,2020.acl-main.175,0,0.0125372,"odels as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection"
2021.acl-long.472,D17-1209,0,0.0607661,"Missing"
2021.acl-long.472,2020.acl-main.461,0,0.0198281,"Missing"
2021.acl-long.472,N18-1150,0,0.0212707,"ent representation and summary generation process of the Seq2Seq architecture by leveraging the graph structure. • Automatic and human evaluation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 201"
2021.acl-long.472,P18-1063,0,0.0157292,"emantic relevance of the generated summaries and references, as the BERTScore improvements of BASS is obvious. Results on SDS Table 3 shows our experiment results along with other SDS baselines. Similar to WikiSUM, we also report LexRank, TransS2S, and RoBERTaS2S. Besides, we report the performance of several other baselines. ORACLE is the upper-bound of current extrative models. Seq2seq is based on LSTM encoder-decoder with attention mechanism (Bahdanau et al., 2015). Pointer and Pointer+cov are pointer-generation (See et al., 2017) with and without coverage mechanism, respectively. FastAbs (Chen and Bansal, 2018) is an abstractive method by jointly training sentence extraction and compression. TLM (Pilault et al., 2020) is a recent long-document summarization method based on language model. We also report the performances of recent pretrianing-based SOTA 6058 text generation models BART (large) and Peaguasus (base) on BIGPATENT, which both contain a parameter size of 406M . The last block shows the results of our model, which contains a parameter size of 201M . The results show that BASS consistently outperforms RoBERTaS2S, and comparable with current large SOTA models with only half of the parameter"
2021.acl-long.472,D19-5412,0,0.0207023,"(Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summariz"
2021.acl-long.472,2020.acl-main.703,0,0.0383412,"hysics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Ill"
2021.acl-long.472,2020.acl-main.555,1,0.901075,"representing them as nodes and their relations as edges. This greatly benefits global structure learning and longdistance relation modeling. Several previous works have attempted to leverage sentence-relation graph to improve long sequence summarization, where nodes are sentences and edges are similarity or dis6052 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6052–6067 August 1–6, 2021. ©2021 Association for Computational Linguistics course relations between sentences (Li et al., 2020). However, the sentence-relation graph is not flexible for fine-grained (such as entities) information aggregation and relation modeling. Some other works also proposed to construct local knowledge graph by OpenIE to improve Seq2Seq models (Fan et al., 2019; Huang et al., 2020). However, the OpenIE-based graph only contains sparse relations between partially extracted phrases, which cannot reflect the global structure and rich relations of the overall sequence. For better modeling the long-distance relations and global structure of a long sequence, we propose to apply a phrase-level unified se"
2021.acl-long.472,D18-1205,1,0.790714,"ls to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient cont"
2021.acl-long.472,D18-1441,1,0.763064,"ls to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient cont"
2021.acl-long.472,C18-1101,0,0.0214588,"sing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficiency in processing long sequences. Huang et al. (2020) utilize OpenIEbased graph for boosting the faithfulness of the generated summaries. Compared with"
2021.acl-long.472,N15-1114,0,0.0238886,"rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficiency in processing long sequences. Huang et al. (2020) utilize OpenIEbased graph for boosting the faithfulness of the generated s"
2021.acl-long.472,P19-1500,0,0.47225,"luation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the r"
2021.acl-long.472,D19-1387,0,0.342053,"luation on both long-document summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the r"
2021.acl-long.472,N19-1173,0,0.398581,"et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al., 2020). Their works also emphasize the importance of modeling cross-document relations in MDS. 2.2 Structure Enhanced Summarization Explicit structures play an important role in recent deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax s"
2021.acl-long.472,2021.ccl-1.108,0,0.0427439,"Missing"
2021.acl-long.472,P19-1212,0,0.133693,"12 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Illustration of a unified semantic graph and its construction procedure for a document containing three sentences. In Graph Construction, underlined tokens represent phrases., co-referent phrases are represented in the same color. In The Unified Semantic Graph, nodes of different colors indic"
2021.acl-long.472,P14-5010,0,0.00266212,", two-hop meta-path represents more complex semantic relations in graph. For example, N-V-N like [Albert Einstein]-[won]-[the physics Nobel Prize] indicates SVO (subject–verb–object) relation. It is essential to effectively model the two-hop meta-path for complex semantic relation modeling. 3.2 Graph Construction In this section, we introduce the definition and construction of the unified semantic graph. To construct the semantic graph, we extract phrases and their relations from sentences by first merging tokens into phrases and then merging co-referent phrases into nodes. We employ CoreNLP (Manning et al., 2014) to obtain coreference chains of the input sequence and the dependency parsing tree of each sentence. Based on the dependency parsing tree, we merge consecutive tokens that form a complete semantic unit into a phrase. Afterwards, we merge the same phrases from different positions and phrases in the same coreference chain to form the nodes in the semantic graph. The final statistics of the unified semantic graph on WikiSUM are illustrated in table 1, which indicates that the scale of the graph expands moderately with the inputs. This also demonstrates how the unified semantic graph compresses l"
2021.acl-long.472,2020.emnlp-main.748,0,0.0134901,"Results on SDS Table 3 shows our experiment results along with other SDS baselines. Similar to WikiSUM, we also report LexRank, TransS2S, and RoBERTaS2S. Besides, we report the performance of several other baselines. ORACLE is the upper-bound of current extrative models. Seq2seq is based on LSTM encoder-decoder with attention mechanism (Bahdanau et al., 2015). Pointer and Pointer+cov are pointer-generation (See et al., 2017) with and without coverage mechanism, respectively. FastAbs (Chen and Bansal, 2018) is an abstractive method by jointly training sentence extraction and compression. TLM (Pilault et al., 2020) is a recent long-document summarization method based on language model. We also report the performances of recent pretrianing-based SOTA 6058 text generation models BART (large) and Peaguasus (base) on BIGPATENT, which both contain a parameter size of 406M . The last block shows the results of our model, which contains a parameter size of 201M . The results show that BASS consistently outperforms RoBERTaS2S, and comparable with current large SOTA models with only half of the parameter size. This further demonstrates the effectiveness of our graph-augmented model on long-document summarization"
2021.acl-long.472,2020.findings-emnlp.217,0,0.0113767,"stractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several large MDS datasets (Liu et al., 2018; Fabbri et al., 2019), some supervised abstractive models for MDS appear (Liu and Lapata, 2019a; Li et al.,"
2021.acl-long.472,W00-1009,0,0.480591,"or his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectric eﬀect Figure 1: Illustration of a unified semantic graph and its construction procedure for a document containing three sentences. In Graph Construction, underlined tokens represent phrases., co-referent phrases are represented in the same color. In The Unified Semantic Graph, nodes of different colors indicate different types, according to section 3.1. on the long source sequence (Shao et al., 2017). Thus"
2021.acl-long.472,2020.tacl-1.18,0,0.0241394,"cument summarization and MDS outperform several strong baselines and validate the effectiveness of our graph-based model. 2 2.1 Related Works Abstractive Summarization Abstractive summarization aims to generate a fluent and concise summary for the given input document (Rush et al., 2015). Most works apply Seq2Seq architecture to implicitly learn the summarization procedure (See et al., 2017; Gehrmann et al., 2018; Paulus et al., 2017; Celikyilmaz et al., 2018). More recently, significant improvements have been achieved by applying pre-trained language models as encoder (Liu and Lapata, 2019b; Rothe et al., 2020) or pre-training the generation process leveraging a large-scale of unlabeled corpus (Dong et al., 2019; Lewis et al., 2020; Qi et al., 2020; Zhang et al., 2020a). In MDS, most of the previous models apply extractive methods (Erkan and Radev, 2004; Cho et al., 2019). Due to the lack of large-scale datasets, some attempts on abstractive methods transfer single document summarization (SDS) models to MDS (Lebanoff et al., 2018; Yang et al., 2019) or unsupervised methods based on auto-encoder (Chu and Liu, 2019; Braˇzinskas et al., 2020; Amplayo and Lapata, 2020). After the release of several larg"
2021.acl-long.472,D19-1324,0,0.0156427,"deep learning-based extractive and abstractive summarization methods (Li et al., 2018a,b; Liu et al., 2019a). Different structures benefit summarization models from different aspects. Constituency parsing greatly benefits content selection 6053 Input Length #Nodes #Edges 800 140 154 1600 291 332 2400 467 568 3000 579 703 Table 1: Illustration of how the average number of nodes and edges in the graph changes when the input sequence becomes longer on WikiSUM. and compression for extractive models. Cao et al. (2015) propose to extract salient sentences based on their constituency parsing trees. Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax structure and syntax rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propo"
2021.acl-long.472,2020.acl-main.451,0,0.191789,"Xu and Durrett (2019) and Desai et al. (2020) jointly select and compress salient content based on syntax structure and syntax rules. Dependency parsing helps summarization models in semantic understanding. Jin et al. (2020) incorporate semantic dependency graphs of input sentences to help the summarization models generate sentences with better semantic relevance . Besides sentence-level structures, document-level structures also attract a lot of attention. Fernandes et al. (2019) build a simple graph consisting of sentences, tokens and POS for summary generation. By incorporating RST trees, Xu et al. (2020) propose a discourse-aware model to extract sentences. Similarly, structures from semantic analysis also help. Liu et al. (2015) and Liao et al. (2018) propose to guide summarization with Abstract Meaning Representation (AMR) for a better comprehension of the input context. (Li and Zhuge, 2019) propose semantic link networks based MDS but without graph neural networks. Recently, the local knowledge graph by OpenIE attracts great attention. Leveraging OpenIE extracted tuples, Fan et al. (2019) compress and reduce redundancy in multi-document inputs in MDS. Their work mainly focus on the efficie"
2021.acl-long.472,2020.acl-main.640,0,0.0218044,"d by the graph. Node Initialization Similar to graph construction in section 3.2, we initialize graph representations following the two-level merging, token merging and phrase merging. The token merging compresses and abstracts local token features into higher-level phrase representations. The phrase merging aggregates co-referent phrases in a wide context, which captures long-distance and crossdocument relations. To be simple, these two merging steps are implemented by average pooling. Graph Encoding Layer Following previous works in graph-to-sequence learning (KoncelKedziorski et al., 2019; Yao et al., 2020), we apply Transformer layers for graph modeling by applying the graph adjacent matrix as self-attention mask. Graph Augmentation Following previous works (Bastings et al., 2017; Koncel-Kedziorski et al., 2019), we add reverse edges and self-loop edges in graph as the original directed edges are 6055 not enough for learning backward information. For better utilizing the properties of the united semantic graph, we further propose two novel graph augmentation methods. Supernode As the graph becomes larger, noises introduced by imperfect graph construction also increase, which may cause disconnec"
2021.acl-long.472,D15-1044,0,0.251484,"ion tasks. 1 relativity. He won the physics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was fo"
2021.acl-long.472,P17-1099,0,0.353644,"vity. He won the physics Nobel Prize in 1921. The great prize was for his explanation of the photoelectric effect. The Unified Semantic Graph nomod:of Work is done during an internship at Baidu Inc. Corresponding author. cop obj won nsubj a German physicist appos nsubj published Albert Einstein dobj obl 1912 the theory of relativity Human Written Summary Albert Einstein received the physics Nobel Prize in 1912 for his discovery of the law of the photoelectric effect Nowadays, the sequence-to-sequence (Seq2Seq) based summarization models have gained unprecedented popularity (Rush et al., 2015; See et al., 2017; Lewis et al., 2020). However, complex summarization scenarios such as long-document or multi-document summarization (MDS), still bring great challenges to Seq2Seq models (Cohan et al., 2018; Liu et al., 2018). In a long document numerous details and salient content may distribute evenly (Sharma et al., 2019) while multiple documents may contain repeated, redundant or contradictory information (Radev, 2000). These problems make Seq2Seq models struggle with content selection and organization which mainly depend † the physics Nobel Prize nsubj explanation Introduction ∗ was for the photoelectri"
2021.emnlp-main.333,P19-1098,0,0.0852126,"2 for training, validation and testing and truncate each document to 768 tokens. DUC Dataset We use the benchmark datasets from the Document Understanding Conferences (DUC) containing clusters of English news articles and human reference summaries. We use DUC 2002, 2003 and 2004 datesets which contain 60, 30 and 50 clusters of nearly 10 documents respectively. Four human reference summaries have been created for each document cluster by NIST assessors. Our model is trained on DUC 2002, validated on DUC 2003, and tested on DUC 2004. We apply the similar preprocessing method with previous work (Cho et al., 2019) and truncate each document to 768 tokens Training Configuration We use the base version of RoBERTa (Liu et al., 2019b) to initialize our models in all experiments. The optimizer is Adam (Kingma and Ba, 2014) with β1=0.9 and β2=0.999, and the learning rate is 0.03 for MultiNews and 0.015 for DUC. We apply learning rate warmup over the first 10000 steps and decay as in (Kingma and Ba, 2014). Gradient clipping with maximum gradient norm 2.0 is also utilized during training. All models are trained on 4 GPUs (Tesla V100) for about 10 epochs. We apply dropout with probability 0.1 before all linear"
2021.emnlp-main.333,N13-1136,0,0.056924,"he graph structure is effective to model relations methods such as similarity graph and discourse between sentences which is an essential point to graph. Sentences are the basic information units select interrelated summary-worthy sentences in and represented as nodes in the graph. And relaextractive summarization. Erkan and Radev (2004) tions between sentences are represented as edges. utilize a similarity graph to construct an unsuper- For example, a similarity graph can be built based vised summarization methods called LexRank. G- on cosine similarities between tf-idf representations Flow (Christensen et al., 2013) and DISCOBERT of sentences. Let G denotes a graph representation (Xu et al., 2020) both use discourse graphs to gen- matrix of the input documents, where G[i][j] indierate concise and informative summaries. Li et al. cates the tf-idf weights between sentence Si and Sj . 4064 source documents. Figure 2 illustrates the overall architecture of SgSum. Sub-graph Ranking Layer Graph Pooling Graph Pooling Graph Pooling Graph Pooling Sub-graph Encoder Graph Encoder Transformer … Doc1 Transformer DocN Figure 2: Model architecture of SgSum. Graph-based multidocument encoder takes tokenized documents as"
2021.emnlp-main.333,P19-1102,0,0.103266,". Thus, our model can be viewed as a sub-graph selection framework which means selecting a proper sub-graph from a whole graph. Furthermore, the graph structure can help to reorder the sentences in the summary to obtain a more coherent summary (Christensen et al., 2013). We order the summary by placing sentences with discourse relations next to each other. 4 4.1 Experiments Experimental Setup stated, we use the similarity graph by default as it is the most widely used in previous work. MultiNews Dataset The MultiNews dataset is a large-scale multi-document summarization dataset introduced by (Fabbri et al., 2019). It contains 56,216 articles-summary pairs and each example consists of 2-10 source documents and a humanwritten summary. Following their experimental settings, we split the dataset into 44,972/5,622/5,622 for training, validation and testing and truncate each document to 768 tokens. DUC Dataset We use the benchmark datasets from the Document Understanding Conferences (DUC) containing clusters of English news articles and human reference summaries. We use DUC 2002, 2003 and 2004 datesets which contain 60, 30 and 50 clusters of nearly 10 documents respectively. Four human reference summaries h"
2021.emnlp-main.333,N10-1131,0,0.0435273,"(2013) build multi-document graphs to identify pairwise ordering constraints over the Different from above studies, some work focus sentences by accounting for discourse relation- on the summary-level selection. Wan et al. (2015) ships between sentences. More recently, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019) have attempted to a build two-stage document summarization. The first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Mendes et al. (2019) follow the extract-then-compress paradigm to train an extractor for content selection. Zhong et al. (2020) propose a novel extract-then-match framework which employs a sentence extractor to prune unnecessary information, then outputs a summary by matching models. These methods consider summary as a whole rather than individual sentences. Howev"
2021.emnlp-main.333,N18-1065,0,0.0474057,"Missing"
2021.emnlp-main.333,N09-1041,0,0.0748764,"30.95 34.52 34.02 35.41 Table 2: Evaluation results on the DUC2004 test set. We report R-1, R-2 and R-L scores, and follow the ROUGE setting of Cho et al. (2019).3 as an extra training resource to improve our model. The results show that single-document data boost the performance of our unified model a further step and achieve a new SOTA result on Multinews. Results on DUC Table 2 summarizes the evaluation results on the DUC2004 dataset. The first block shows four popular unsupervised baselines, and the second block shows several strong supervised baselines. We report the results of KSLSumm (Haghighi and Vanderwende, 2009), LexRank (Erkan and Radev, 2004), DPP (Kulesza and Taskar, 2011), Sim-DPP (Cho et al., 2019) following Cho et al. (2019). Besides, we also report the results of SubModular (Lin and Bilmes, 2010), StructSVM (Sipos et al., 2012) and PG (See et al., 2017) as strong baselines. The last block shows the results of our models. The results indicate that our model SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is com"
2021.emnlp-main.333,D18-1446,0,0.0384836,"Missing"
2021.emnlp-main.333,C16-1023,1,0.8911,"Missing"
2021.emnlp-main.333,2020.acl-main.555,1,0.918979,"blems. In this paper, we encode source documents by a Hierarchical Transformer, which consists of several sharedweight single Transformers (Vaswani et al., 2017) that process each document independently. Each Transformer takes a tokenized document as input and outputs its sentence representations. This architecture enables our model to process much longer input. Graph Encoding To effectively capture the relations between sentences in source documents, we incorporate explicit graph representations of documents into the neural encoding process via a graph-informed attention mechanism similar to Li et al. (2020). Each sentence can collect information from other related sentences to capture global information from the whole input. The graphinformed attention mechanism extends the vanilla self-attention mechanism to consider the pairwise relations in explicit graph representations as: αij = Softmax(eij + Rij ) (1) where eij denotes the origin self-attention weights between sentences Si and Sj , αij denotes the adjusted weights by graph structure. The key point of the graph-based self-attention is the additional pairwise relation bias Rij , which is computed as a Gaussian bias of the weights of graph re"
2021.emnlp-main.333,D18-1205,1,0.825796,"del SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commonly known that deep neural networks achieved great improvement on SDS task recently (Liu and Lapata, 2019b; Zhong et al., 2020; Li et al., 2018a,b). However, such supervised models can not work well on MDS task because parallel data for mulit-document are scarce and costly to obtain. For example, the DUC dataset only contains tens of parallel MDS data. There is a pressing need 3 -n 2 -m -w 1.2 -c 95 -r 1000 -l 250 4068 -n 2 -m -w 1.2 -c 95 -r 1000 -l 100 Models Lead LexRank BERTSUMEXT SgSum R-1 40.21 40.27 41.28 43.61 R-2 12.13 12.63 12.05 14.07 R-L 37.13 37.50 37.18 39.50 Table 3: Transfer performance on MultiNews dataset Figure 3: Results on different graph types. Models KLSumm LexRank Extract+Rewrite BERTSUMEXT PG-MMR SgSum R-1 31"
2021.emnlp-main.333,D18-1441,1,0.819301,"del SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commonly known that deep neural networks achieved great improvement on SDS task recently (Liu and Lapata, 2019b; Zhong et al., 2020; Li et al., 2018a,b). However, such supervised models can not work well on MDS task because parallel data for mulit-document are scarce and costly to obtain. For example, the DUC dataset only contains tens of parallel MDS data. There is a pressing need 3 -n 2 -m -w 1.2 -c 95 -r 1000 -l 250 4068 -n 2 -m -w 1.2 -c 95 -r 1000 -l 100 Models Lead LexRank BERTSUMEXT SgSum R-1 40.21 40.27 41.28 43.61 R-2 12.13 12.63 12.05 14.07 R-L 37.13 37.50 37.18 39.50 Table 3: Transfer performance on MultiNews dataset Figure 3: Results on different graph types. Models KLSumm LexRank Extract+Rewrite BERTSUMEXT PG-MMR SgSum R-1 31"
2021.emnlp-main.333,P02-1058,0,0.294435,"ified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-level extractor. Early models employ rule-based methods to score and select sentenecs (Lin and Hovy, 2002; Lin and Bilmes, 2011; Takamura and Okumura, 2009; 5 Related Work Schilder and Kondadadi, 2008). Recently, SUM5.1 Graph-based Summarization MARUNNER (Nallapati et al., 2017) adopt an encoder based on Recurrent Neural Networks which Most previous graph extractive MDS approaches aim to extract salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the"
2021.emnlp-main.333,P11-1052,0,0.0483401,", which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-level extractor. Early models employ rule-based methods to score and select sentenecs (Lin and Hovy, 2002; Lin and Bilmes, 2011; Takamura and Okumura, 2009; 5 Related Work Schilder and Kondadadi, 2008). Recently, SUM5.1 Graph-based Summarization MARUNNER (Nallapati et al., 2017) adopt an encoder based on Recurrent Neural Networks which Most previous graph extractive MDS approaches aim to extract salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the document. However, al"
2021.emnlp-main.333,P19-1500,0,0.0845786,"tion and a high-way layer normalization are 4065 applied after the graph-informed attention mechanism. These three components form the graph encoding layers. Graph Pooling In the MDS task, information is more massive and relations between sentences are much more complex. So it is necessary to have an overview of the central meaning of multi-document input. Zhong et al. (2020) generate a document representation with Siamese-BERT to guide the training and inference process. In this paper, based on the graph representation of documents, we apply a multi-head weighted-pooling operation similar to Liu and Lapata (2019a) to capture the global semantic information of source documents. It takes sentence representations in the source graph as input and outputs an overall representation of them (denoted as D), which provides global information of documents for both the sentence and summary selection processes. Let xi denotes the graph representation of sentence Si . For each head z ∈ {1, ..., nhead }, we first transform xi into attention scores azi and value vectors bzi , then we calculate an attention distribution a ˆzi over all sentences in the source graph based on attention scores: azi = Waz xi (3) bzi = Wb"
2021.emnlp-main.333,D19-1387,0,0.0905449,"tion and a high-way layer normalization are 4065 applied after the graph-informed attention mechanism. These three components form the graph encoding layers. Graph Pooling In the MDS task, information is more massive and relations between sentences are much more complex. So it is necessary to have an overview of the central meaning of multi-document input. Zhong et al. (2020) generate a document representation with Siamese-BERT to guide the training and inference process. In this paper, based on the graph representation of documents, we apply a multi-head weighted-pooling operation similar to Liu and Lapata (2019a) to capture the global semantic information of source documents. It takes sentence representations in the source graph as input and outputs an overall representation of them (denoted as D), which provides global information of documents for both the sentence and summary selection processes. Let xi denotes the graph representation of sentence Si . For each head z ∈ {1, ..., nhead }, we first transform xi into attention scores azi and value vectors bzi , then we calculate an attention distribution a ˆzi over all sentences in the source graph based on attention scores: azi = Waz xi (3) bzi = Wb"
2021.emnlp-main.333,P08-2052,0,0.0316995,"range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-level extractor. Early models employ rule-based methods to score and select sentenecs (Lin and Hovy, 2002; Lin and Bilmes, 2011; Takamura and Okumura, 2009; 5 Related Work Schilder and Kondadadi, 2008). Recently, SUM5.1 Graph-based Summarization MARUNNER (Nallapati et al., 2017) adopt an encoder based on Recurrent Neural Networks which Most previous graph extractive MDS approaches aim to extract salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the document. However, all LexRank to compute sentence importance based these models belong to sent"
2021.emnlp-main.333,P17-1099,0,0.03234,"he performance of our unified model a further step and achieve a new SOTA result on Multinews. Results on DUC Table 2 summarizes the evaluation results on the DUC2004 dataset. The first block shows four popular unsupervised baselines, and the second block shows several strong supervised baselines. We report the results of KSLSumm (Haghighi and Vanderwende, 2009), LexRank (Erkan and Radev, 2004), DPP (Kulesza and Taskar, 2011), Sim-DPP (Cho et al., 2019) following Cho et al. (2019). Besides, we also report the results of SubModular (Lin and Bilmes, 2010), StructSVM (Sipos et al., 2012) and PG (See et al., 2017) as strong baselines. The last block shows the results of our models. The results indicate that our model SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commonly known that deep neural"
2021.emnlp-main.333,N19-1173,0,0.112281,"sets from the Document Understanding Conferences (DUC) containing clusters of English news articles and human reference summaries. We use DUC 2002, 2003 and 2004 datesets which contain 60, 30 and 50 clusters of nearly 10 documents respectively. Four human reference summaries have been created for each document cluster by NIST assessors. Our model is trained on DUC 2002, validated on DUC 2003, and tested on DUC 2004. We apply the similar preprocessing method with previous work (Cho et al., 2019) and truncate each document to 768 tokens Training Configuration We use the base version of RoBERTa (Liu et al., 2019b) to initialize our models in all experiments. The optimizer is Adam (Kingma and Ba, 2014) with β1=0.9 and β2=0.999, and the learning rate is 0.03 for MultiNews and 0.015 for DUC. We apply learning rate warmup over the first 10000 steps and decay as in (Kingma and Ba, 2014). Gradient clipping with maximum gradient norm 2.0 is also utilized during training. All models are trained on 4 GPUs (Tesla V100) for about 10 epochs. We apply dropout with probability 0.1 before all linear layers. The number of hidden units in our models is set as 256, the feedforward hidden size is 1,024, and the number"
2021.emnlp-main.333,E12-1023,0,0.0352659,"single-document data boost the performance of our unified model a further step and achieve a new SOTA result on Multinews. Results on DUC Table 2 summarizes the evaluation results on the DUC2004 dataset. The first block shows four popular unsupervised baselines, and the second block shows several strong supervised baselines. We report the results of KSLSumm (Haghighi and Vanderwende, 2009), LexRank (Erkan and Radev, 2004), DPP (Kulesza and Taskar, 2011), Sim-DPP (Cho et al., 2019) following Cho et al. (2019). Besides, we also report the results of SubModular (Lin and Bilmes, 2010), StructSVM (Sipos et al., 2012) and PG (See et al., 2017) as strong baselines. The last block shows the results of our models. The results indicate that our model SgSum consistently outperforms most baselines, which further demonstrate the effectiveness of our model on different types of corpora. Additionally, we also test the performance of SgSum-extra which add CNN/DM data as a supplement. It is comparable to Sim-DPP baseline which also uses extra CNN/DM data to train a similarity model. And the results again show that singledocument data greatly improves the performance of our model. 4.3 Transfer Performances It is commo"
2021.emnlp-main.333,2021.ccl-1.108,0,0.045059,"Missing"
2021.emnlp-main.333,C18-1146,0,0.0222071,"e an end-to-end model which is trained on single-document data but can work well with multiple-document input. In this section we do further experiments to verify the transfer ability of our model from single to multi-document task. We follow the experiment setups of Lebanoff et al. (2018), and compare with several strong baseline models: (1) BERTSUMEXT (Liu and Lapata, 2019b), an extractive method with pre-trained LM model; (2) PG-MMR (Lebanoff et al., 2018), an encoder-decoder model which exploits the maximal marginal relevance method to select representative sentences; (3) Extract+Rewrite (Song et al., 2018), is a recent approach that scores sentences using LexRank and generates a title-like summary for each sentence using an encoder-decoder model. We follow the results of Lebanoff et al. (2018). Table 3 and Table 4 demonstrate the results on MultiNews and DUC2004 respectively. Models SgSum w/o s.g. enc w/o s.g. rank w/o s.g. enc&rank w/o graph enc w/o all R-1 47.36 46.87 46.91 46.69 46.21 45.43 R-2 18.61 17.93 17.97 17.64 17.12 16.62 R-L 43.13 42.67 42.80 42.48 42.11 41.32 Table 5: Ablation study on the MultiNews test set. s.g. is the abbreviation for sub-graph. fer ability which can reduce the"
2021.emnlp-main.333,N19-1397,0,0.0189253,"ly, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019) have attempted to a build two-stage document summarization. The first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Mendes et al. (2019) follow the extract-then-compress paradigm to train an extractor for content selection. Zhong et al. (2020) propose a novel extract-then-match framework which employs a sentence extractor to prune unnecessary information, then outputs a summary by matching models. These methods consider summary as a whole rather than individual sentences. However, they neglect the relations between sentences during both scoring and selecting. 5.3 Conclusion We propose a novel framework SgSum which transforms the MDS task into the problem of sub-graph selection. SgSum captures the relations between sentences by"
2021.emnlp-main.333,N18-1158,0,0.0213222,"salient textual units from docu- is the earliest neural summarization model. SUMO (Liu et al., 2019a) capitalizes on the notion of strucments based on graph structure representations tured attention to induce a multi-root dependency of sentences. Erkan and Radev (2004) introduce tree representation of the document. However, all LexRank to compute sentence importance based these models belong to sentence-level extractors on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Chris- which select high score sentences individually and might raise redundancy (Narayan et al., 2018). tensen et al. (2013) build multi-document graphs to identify pairwise ordering constraints over the Different from above studies, some work focus sentences by accounting for discourse relation- on the summary-level selection. Wan et al. (2015) ships between sentences. More recently, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Aly"
2021.emnlp-main.333,2020.acl-main.553,0,0.0666884,"sub-graph view is more appropriate to generate a coherent and concise summary. This is also the key point of our framework. Additionally, important sentences usually build up crucial sub-graphs. So it is a simple but efficient way to generate candidate sub-graphs based on those salient sentences. 3 3.1 Methodology Graph-based Multi-document Encoder Hierarchical Transformer Most previous works (Cao et al., 2017; Jin et al., 2020; Wang et al., 2017) did not consider the multi-document structure. They simply concatenate all documents together and treat the MDS as a special SDS with longer input. Wang et al. (2020) preprocess the multi-document input by truncating lead sentences averagely from each document, then concatenating them together as the MDS input. These preprocessing methods are simple ways to help the model encode multi-document inputs. But they do not make full use of the source document structures. Lead sentences extracted from each document might be similar with each other and result in redundant and incoherent problems. In this paper, we encode source documents by a Hierarchical Transformer, which consists of several sharedweight single Transformers (Vaswani et al., 2017) that process ea"
2021.emnlp-main.333,D17-1020,0,0.0195624,"he sub-graph structures, we can distinguish the quality of different candidate summaries and finally select the best one. Compared with the whole document graph view, sub-graph view is more appropriate to generate a coherent and concise summary. This is also the key point of our framework. Additionally, important sentences usually build up crucial sub-graphs. So it is a simple but efficient way to generate candidate sub-graphs based on those salient sentences. 3 3.1 Methodology Graph-based Multi-document Encoder Hierarchical Transformer Most previous works (Cao et al., 2017; Jin et al., 2020; Wang et al., 2017) did not consider the multi-document structure. They simply concatenate all documents together and treat the MDS as a special SDS with longer input. Wang et al. (2020) preprocess the multi-document input by truncating lead sentences averagely from each document, then concatenating them together as the MDS input. These preprocessing methods are simple ways to help the model encode multi-document inputs. But they do not make full use of the source document structures. Lead sentences extracted from each document might be similar with each other and result in redundant and incoherent problems. In"
2021.emnlp-main.333,2021.acl-long.472,1,0.782914,"our proposed sub-graph selection framework. et al. (2017) build on the approximate discourse graph model and account for macro-level features in sentences to improve sentence salience prediction. Yin et al. (2019) also propose a graph-based neural sentence ordering model, which utilizes an entity linking graph to capture the global dependencies between sentences. Li et al. (2020) incorporate explicit graph representations to the neural architecture based on a novel graph-informed selfattention mechanism. It is the first work to effectively combine graph structures with abstractive MDS model. Wu et al. (2021) present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. However, these works only consider the graph structure of source documents, but neglect the graph structures of summaries which are also important to generate coherent and informative summaries. 5.2 Sentence or Summary-level Extraction Extractive summarization methods usually produce a summary by selecting some original sentences in the document set by a sentence-le"
2021.emnlp-main.333,2020.acl-main.451,0,0.0429933,"ourse between sentences which is an essential point to graph. Sentences are the basic information units select interrelated summary-worthy sentences in and represented as nodes in the graph. And relaextractive summarization. Erkan and Radev (2004) tions between sentences are represented as edges. utilize a similarity graph to construct an unsuper- For example, a similarity graph can be built based vised summarization methods called LexRank. G- on cosine similarities between tf-idf representations Flow (Christensen et al., 2013) and DISCOBERT of sentences. Let G denotes a graph representation (Xu et al., 2020) both use discourse graphs to gen- matrix of the input documents, where G[i][j] indierate concise and informative summaries. Li et al. cates the tf-idf weights between sentence Si and Sj . 4064 source documents. Figure 2 illustrates the overall architecture of SgSum. Sub-graph Ranking Layer Graph Pooling Graph Pooling Graph Pooling Graph Pooling Sub-graph Encoder Graph Encoder Transformer … Doc1 Transformer DocN Figure 2: Model architecture of SgSum. Graph-based multidocument encoder takes tokenized documents as input and outputs sentence representations after graph encoding layers. Candidate"
2021.emnlp-main.333,K17-1045,0,0.0334523,"Missing"
2021.emnlp-main.333,K19-1074,0,0.0180512,"to identify pairwise ordering constraints over the Different from above studies, some work focus sentences by accounting for discourse relation- on the summary-level selection. Wan et al. (2015) ships between sentences. More recently, Yasunaga optimize the summarization performance directly 4070 based on the characteristics of summaries and rank summaries directly during inference. Bae et al. (2019), Paulus et al. (2017) and Celikyilmaz et al. (2018) use reinforcement learning to globally optimize summary-level performance. Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019) have attempted to a build two-stage document summarization. The first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Mendes et al. (2019) follow the extract-then-compress paradigm to train an extractor for content selection. Zhong et al. (2020) propose a novel extract-then-match framework which employs a sentence extractor to prune unnecessary information, then outputs a summary by matching models. These methods consider summary as a whole rather than individual sentences. However, they neglect the"
2021.emnlp-main.333,2020.acl-main.552,0,0.0746797,"hen,xiaoxinyan wu_hua,wanghaifeng}@baidu.com Abstract These models (called sentence-level extractors) do not consider summary as a whole but a combinaMost of existing extractive multi-document tion of independent sentences. This may cause summarization (MDS) methods score each incoherent and redundant problem, and result in sentence individually and extract salient sena poor summary even if the summary consists of tences one by one to compose a summary, high score sentences. Some works (Wan et al., which have two main drawbacks: (1) neglecting both the intra and cross-document relations 2015; Zhong et al., 2020) treat summary as a whole between sentences; (2) neglecting the coherunit and try to solve the weakness of sentenceence and conciseness of the whole summary. level extractors by using a summary-level extracIn this paper, we propose a novel MDS frametor. However, these models neglect the intra and work (SgSum) to formulate the MDS task as a cross-document relations between sentences which sub-graph selection problem, in which source also have benefits for extracting salient sentences, documents are regarded as a relation graph of detecting redundancy and generating overall cohersentences (e.g.,"
2021.emnlp-main.465,D18-1338,0,0.0215844,"single softmax operation by instead computing a lin- feedforward network with sparsely activated experts layers. The result is an example of ear combination over softmaxes, each weighted adaptive computation where parameters (exby learned coefficients. pert FFNs) are selected for each specific token. 2.7 Architectures This provides a way of scaling up the parameTransparent Attention One type of atten- ter count of a model independently from the FLOPs required for a forward pass. Some varition variant we experiment with is Transparent ants in Fedus et al. (2021) consider sparse selfAttention (Bapna et al., 2018). Transparent attention layers as well, but we only consider attention (Bapna et al., 2018) creates weighted the primary variant here. residual connections along encoder depth to Product Key Memory Similar to the facilitate gradient flow. In appendix A, we experiment with additional attention variants. expert model designs, product key memory networks (Lample et al., 2019) process inputs adapEvolved Transformer The Evolved tively, selecting sparse values. In contrast, the Transformer (So et al., 2019) was designed mechanism of sparse computation isn’t done via evolution-based architecture sear"
2021.emnlp-main.465,D13-1160,0,0.0145752,"of the total pre-training steps). We report the mean and standard deviation of the loss (log perplexity) on held-out data of these five experiments and also report the final loss at the end of pre-training (524, 288 steps). We do not use any regularization during pre-training. In the transfer learning setting, after pretraining we fine-tune each model on three different tasks: the SuperGLUE (Wang et al., 2019) natural language understanding metabenchmark, the XSum (Narayan et al., 2018) abstractive summarization dataset, and the closed-book variant (Roberts et al., 2020) of the WebQuestions (Berant et al., 2013) questionanswering task. With these tasks, we hope to capture a broad variety of NLP problems including language understanding and classification, language generation, and knowledge internalization. For SuperGLUE and XSum, each model is fine-tuned for 262,144 steps. Since the WebQuestions dataset is much smaller, we fine-tune the model for only 30,000 steps. We use a constant learning rate of 0.0005 with a linear warm-up of 20, 000 steps. Similar to pre-training, each batch contains 65, 536 tokens. We save a checkpoint every 2, 500 steps (1, 000 steps for WebQuestions) and report results on th"
2021.emnlp-main.465,D16-1053,0,0.0371801,"Missing"
2021.emnlp-main.465,W04-1013,0,0.044653,"Missing"
2021.emnlp-main.465,N18-2074,0,0.0612478,"Missing"
2021.semeval-1.97,2020.acl-main.747,0,0.0985724,"Missing"
2021.semeval-1.97,P19-1568,0,0.0219278,"d perceptron (MLP) when used in few-shot learning. To ensure reproducibility and so other researchers can build on this work, we release the program code, hyperparameters and experiments associated with this work1 . 2 Related work The first effective method that implemented Few Shot Learning in NLP was that by Koch et al. (2015), who introduced the application of the siamese network in one-shot learning. The siamese network is typically used to calculate semantic similarity between sentences and was shown to be powerful in the FSL setting. A recent use of zero-shot learning in WSD was work by Kumar et al. (2019), who proposed the 1 https://github.com/weilk/ SemEval-2021-Task-2 738 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 738–742 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics extension of WSD systems by incorporating Sense embeddings (EWISE). These sense embeddings are derived from a knowledge graph, namely WordNet (Miller, 1998), and graph embeddings. EWISE predicts over an embedding space instead of the discrete label space and allow generalized zero-shot learning capability. Instead of using annotated"
2021.semeval-1.97,2021.semeval-1.3,0,0.0668625,"Missing"
A00-1023,M98-1015,0,\N,Missing
A00-1023,M98-1027,0,\N,Missing
A00-1023,M98-1004,0,\N,Missing
A00-1023,A97-1029,0,\N,Missing
A00-1023,A97-1000,0,\N,Missing
C02-1127,J94-4003,0,0.0112306,"visualization of extracted events on *This work was partly supported by a grant from the Air Force Research Laboratory’s Information Directorate (AFRL/IF), Rome, NY, under contract F30602-00-C-0090. a map for an Information Extraction (IE) System. Location normalization is a special application of word sense disambiguation (WSD). There is considerable research on WSD. Knowledge-based work, such as (Hirst, 1987; McRoy, 1992; Ng and Lee, 1996) used hand-coded rules or supervised machine learning based on annotated corpus to perform WSD. Recent work emphasizes corpus-based unsupervised approach (Dagon and Itai, 1994; Yarowsky, 1992; Yarowsky, 1995) that avoids the need for costly truthed training data. Location normalization is different from general WSD in that the selection restriction often used for WSD in many cases is not sufficient to distinguish the correct sense from the other candidates. For example, in the sentence “The White House is located in Washington”, the selection restriction from the collocation ‘located in’ can only determine that “Washington” should be a location name, but is not sufficient to decide the actual sense of this location. Location normalization depends heavily on co-occu"
C02-1127,H92-1045,0,0.0184821,"weight assignment between two nodes: (1) More weight will be given to the edge between a city and the province (or the country) to which it belongs. (2) Distance between location names mentioned in the document is taken into consideration. The shorter the distance, the more we assign the weight between the nodes. (3) The number of word occurrences affects the weight calculation. For multiple mentions of a location name, only one node will be represented in the graph. We assume that all the same location mentions have the same meaning in a document following one sense per discourse principle (Gale, Church, and Yarowsky, 1992). When calculating the weight between two location names, the predefined similarity values shown in Table 1, the number of location name occurrences and the distance between them in a text are taken into consideration. After selecting each edge, the senses that are connected will be chosen, and other senses of the same location name will be discarded so that they will not be considered again in the MaxST calculation. A weight value is calculated with equation (1), where sij indicate the jth sense of word i , α reflects the number of location name occurrences in a text, and β refers to the dis"
C02-1127,M98-1015,0,0.0287115,"reference of locations are illustrated below. &lt;PersonProfile 001&gt; :: Name: Julian Werner Hill Position: Research chemist Age: 91 Birth-place: &lt;LocationProfile100&gt; Affiliation: Du Pont Co. Education: MIT &lt;LocationProfile 100&gt; :: Name: St. Louis State: Missouri Country: United States of America Zipcode: 63101 Lattitude : 90.191313 Longitude: 38.634616 Related_profiles: &lt;PersonProfile 001&gt; Several other applications such as question answering and classifying documents by location areas can also be enabled through LocNZ. 3 Lexical Grammar Processing in Local Context Named Entity tagging systems (Krupka and Hausman, 1998; Srihari et al., 2000) attempt to tag information such as names of people, organizations, locations, time, etc. in running text. In InfoXtract, we combine Maximum Entropy Model (MaxEnt) and Hidden Markov Model for NE tagging (Shrihari et al.,, 2000). The Maximum Entropy Models incorporate local contextual evidence in handling ambiguity of information from a location gazetteer. In the Tipster Location gazetteer used by InfoXtract, there are a lot of common words, such as I, A, June, Friendship , etc. Also, there is large overlap between person names and location names, such as Clinton, Jordan,"
C02-1127,J92-1001,0,0.0226059,"a Main Street or Broadway. Such ambiguity needs to be properly handled before converting location names into some normal form to support entity profile construction, event merging and visualization of extracted events on *This work was partly supported by a grant from the Air Force Research Laboratory’s Information Directorate (AFRL/IF), Rome, NY, under contract F30602-00-C-0090. a map for an Information Extraction (IE) System. Location normalization is a special application of word sense disambiguation (WSD). There is considerable research on WSD. Knowledge-based work, such as (Hirst, 1987; McRoy, 1992; Ng and Lee, 1996) used hand-coded rules or supervised machine learning based on annotated corpus to perform WSD. Recent work emphasizes corpus-based unsupervised approach (Dagon and Itai, 1994; Yarowsky, 1992; Yarowsky, 1995) that avoids the need for costly truthed training data. Location normalization is different from general WSD in that the selection restriction often used for WSD in many cases is not sufficient to distinguish the correct sense from the other candidates. For example, in the sentence “The White House is located in Washington”, the selection restriction from the collocation"
C02-1127,P96-1006,0,0.0292838,"t or Broadway. Such ambiguity needs to be properly handled before converting location names into some normal form to support entity profile construction, event merging and visualization of extracted events on *This work was partly supported by a grant from the Air Force Research Laboratory’s Information Directorate (AFRL/IF), Rome, NY, under contract F30602-00-C-0090. a map for an Information Extraction (IE) System. Location normalization is a special application of word sense disambiguation (WSD). There is considerable research on WSD. Knowledge-based work, such as (Hirst, 1987; McRoy, 1992; Ng and Lee, 1996) used hand-coded rules or supervised machine learning based on annotated corpus to perform WSD. Recent work emphasizes corpus-based unsupervised approach (Dagon and Itai, 1994; Yarowsky, 1992; Yarowsky, 1995) that avoids the need for costly truthed training data. Location normalization is different from general WSD in that the selection restriction often used for WSD in many cases is not sufficient to distinguish the correct sense from the other candidates. For example, in the sentence “The White House is located in Washington”, the selection restriction from the collocation ‘located in’ can o"
C02-1127,A00-1034,1,0.649805,"e illustrated below. &lt;PersonProfile 001&gt; :: Name: Julian Werner Hill Position: Research chemist Age: 91 Birth-place: &lt;LocationProfile100&gt; Affiliation: Du Pont Co. Education: MIT &lt;LocationProfile 100&gt; :: Name: St. Louis State: Missouri Country: United States of America Zipcode: 63101 Lattitude : 90.191313 Longitude: 38.634616 Related_profiles: &lt;PersonProfile 001&gt; Several other applications such as question answering and classifying documents by location areas can also be enabled through LocNZ. 3 Lexical Grammar Processing in Local Context Named Entity tagging systems (Krupka and Hausman, 1998; Srihari et al., 2000) attempt to tag information such as names of people, organizations, locations, time, etc. in running text. In InfoXtract, we combine Maximum Entropy Model (MaxEnt) and Hidden Markov Model for NE tagging (Shrihari et al.,, 2000). The Maximum Entropy Models incorporate local contextual evidence in handling ambiguity of information from a location gazetteer. In the Tipster Location gazetteer used by InfoXtract, there are a lot of common words, such as I, A, June, Friendship , etc. Also, there is large overlap between person names and location names, such as Clinton, Jordan, etc. Using MaxEnt, sys"
C02-1127,C92-2070,0,0.026624,"cted events on *This work was partly supported by a grant from the Air Force Research Laboratory’s Information Directorate (AFRL/IF), Rome, NY, under contract F30602-00-C-0090. a map for an Information Extraction (IE) System. Location normalization is a special application of word sense disambiguation (WSD). There is considerable research on WSD. Knowledge-based work, such as (Hirst, 1987; McRoy, 1992; Ng and Lee, 1996) used hand-coded rules or supervised machine learning based on annotated corpus to perform WSD. Recent work emphasizes corpus-based unsupervised approach (Dagon and Itai, 1994; Yarowsky, 1992; Yarowsky, 1995) that avoids the need for costly truthed training data. Location normalization is different from general WSD in that the selection restriction often used for WSD in many cases is not sufficient to distinguish the correct sense from the other candidates. For example, in the sentence “The White House is located in Washington”, the selection restriction from the collocation ‘located in’ can only determine that “Washington” should be a location name, but is not sufficient to decide the actual sense of this location. Location normalization depends heavily on co-occurrence constrain"
C02-1127,P95-1026,0,0.0323246,"This work was partly supported by a grant from the Air Force Research Laboratory’s Information Directorate (AFRL/IF), Rome, NY, under contract F30602-00-C-0090. a map for an Information Extraction (IE) System. Location normalization is a special application of word sense disambiguation (WSD). There is considerable research on WSD. Knowledge-based work, such as (Hirst, 1987; McRoy, 1992; Ng and Lee, 1996) used hand-coded rules or supervised machine learning based on annotated corpus to perform WSD. Recent work emphasizes corpus-based unsupervised approach (Dagon and Itai, 1994; Yarowsky, 1992; Yarowsky, 1995) that avoids the need for costly truthed training data. Location normalization is different from general WSD in that the selection restriction often used for WSD in many cases is not sufficient to distinguish the correct sense from the other candidates. For example, in the sentence “The White House is located in Washington”, the selection restriction from the collocation ‘located in’ can only determine that “Washington” should be a location name, but is not sufficient to decide the actual sense of this location. Location normalization depends heavily on co-occurrence constraints of geographica"
C02-1127,M98-1004,0,\N,Missing
C16-1023,W06-0901,0,0.0540376,"Missing"
C16-1023,J05-3002,0,0.155757,"Missing"
C16-1023,P15-1153,0,0.0223346,"Missing"
C16-1023,W07-1427,0,0.0296373,"Missing"
C16-1023,P13-1121,0,0.0558753,"Missing"
C16-1023,N16-1012,0,0.0979141,"Missing"
C16-1023,W02-1001,0,0.0699887,"Missing"
C16-1023,D08-1019,0,0.0793789,"Missing"
C16-1023,P14-1134,0,0.0722384,"Missing"
C16-1023,W09-0613,0,0.0274559,"Missing"
C16-1023,W11-1608,0,0.0534875,"Missing"
C16-1023,P11-1113,0,0.0418058,"Missing"
C16-1023,P13-1048,0,0.0439468,"Missing"
C16-1023,W15-4502,0,0.0344411,"Missing"
C16-1023,P14-1038,0,0.063692,"Missing"
C16-1023,P13-1008,0,0.0425695,"Missing"
C16-1023,D15-1219,1,0.839426,"Missing"
C16-1023,P10-1081,0,0.0288962,"Missing"
C16-1023,N03-1020,0,0.265211,"Missing"
C16-1023,N15-1114,0,0.238448,"Missing"
C16-1023,P14-5010,0,0.00594964,"Missing"
C16-1023,N04-1019,0,0.174386,"Missing"
C16-1023,I08-1016,0,0.0562322,"Missing"
C16-1023,P13-2026,0,0.0542914,"Missing"
C16-1023,P14-1084,0,0.0396286,"Missing"
C16-1023,P13-1132,0,0.0950994,"Missing"
C16-1023,W00-0403,0,0.15013,"Missing"
C16-1023,D15-1044,0,0.037938,"Missing"
C16-1023,W15-2812,0,0.0464819,"Missing"
C16-1023,N13-1135,0,0.0729064,"Missing"
C16-1023,J11-4007,0,0.0475917,"Missing"
C16-1023,W13-3508,0,0.0539083,"Missing"
C16-1023,P13-1137,0,0.0641792,"Missing"
C16-1023,N15-1040,0,0.0466394,"Missing"
C16-1098,D10-1111,0,0.0269956,"ation in Section 5.2. Topic Models for Documents Comparison. The other type of related work is the comparison of documents. Most existing studies for this goal focus on topic models to discover common and specific themes among document collections, referred to as cross-collection topic models (Paul, 2009). This idea was first explored with an initial topic model PLSI (Zhai et al., 2004), and later improved with LDA topic model (Blei, 2012; Pual, 2009) which inspires our dTM-Dirichlet. There are a number of real-world applications extending cross-collection topic models in different scenarios (Ahmed and Xing, 2010; Li et al., 2011). For example, Paul and Girju (2009) employed cross-collection LDA (ccLDA) for cross-cultural analysis of blogs and forums and later they proposed a two-dimensional topicaspect model (TAM) to jointly discover topics and aspects in scientific literature (Paul and Girju, 2010). The common idea behind these cross-collection topic models is that using latent topics capture the common and unique word usage among document collections. Cross-collection topic models neglect the correlations between each collection-specific topic and the common background topic, thus make it insuffici"
C16-1098,C08-2006,0,0.0837245,"Missing"
C16-1098,C10-1016,0,0.0218264,"rence on Computational Linguistics: Technical Papers, pages 1028–1038, Osaka, Japan, December 11-17 2016. tence discriminative capacity and a greedy sentence selection method to automatically generate summary for dTM-based comparative summarization. 2 Related Work Multi-document Summarization. Existing multi-document summarization can be either extractive or abstractive (Sekine and Nobata, 2003). Our work focuses on the extractive techniques which involve in assigning saliency scores to sentences and extracting high-scored sentences in a greedy manner to construct a summary (Wan et al., 2007; Cai et al., 2010; Gupta and Lehal, 2010; Celikyilmaz and Hakkani-Tur, 2011). Graph-based ranking techniques such as TextRank (Mihalcea and Tarau, 2004) and LexPageRank (ErKan and Radev, 2004) have been widely used in extractive summarization. A bigram based supervised method was proposed for extractive summarization in ILP framework (Li et al., 2013; Li, 2015). Jha et al. (2015) proposed an extractive algorithm that combines a content model with a discourse model to generate coherent summaries for scientific articles. A multi-dimensional summarization methodology was proposed to transform the paradigm of trad"
C16-1098,P10-1084,0,0.0739514,"Missing"
C16-1098,P11-1050,0,0.0618731,"Missing"
C16-1098,E12-1022,0,0.0843527,"nformation in a document set B under the assumption that users have already learnt the documents in set A, where documents in A chronologically precede the documents in B. The update summarization has been well studied. Most existing methods solve it as a redundancy removal problem by adding functionality to remove redundant sentences using filtering rules (Fisher and Roark, 2008), Maximal Marginal Relevance (Boudin et al., 2008), or graph-based algorithms (Shen and Li, 2010; Li et al., 2008). More related to this paper is the work of a topic-model based update summarization approach DualSum (Delort and Alfonseca, 2012), which learns a general background distribution across the corpus and a document-specific distribution for each document, but also learns two collection-specific distributions for each pair of update collection and base collection: the joint topic distribution and the update topic distribution. This paper revises DualSum as a baseline for evaluation in Section 5.2. Topic Models for Documents Comparison. The other type of related work is the comparison of documents. Most existing studies for this goal focus on topic models to discover common and specific themes among document collections, refe"
C16-1098,W04-3247,0,0.163208,"Missing"
C16-1098,N03-1020,0,0.3918,"Missing"
C16-1098,C08-1062,0,0.0261326,"The most similar task to comparative summarization is update summarization, which aims to detect and summarize novel information in a document set B under the assumption that users have already learnt the documents in set A, where documents in A chronologically precede the documents in B. The update summarization has been well studied. Most existing methods solve it as a redundancy removal problem by adding functionality to remove redundant sentences using filtering rules (Fisher and Roark, 2008), Maximal Marginal Relevance (Boudin et al., 2008), or graph-based algorithms (Shen and Li, 2010; Li et al., 2008). More related to this paper is the work of a topic-model based update summarization approach DualSum (Delort and Alfonseca, 2012), which learns a general background distribution across the corpus and a document-specific distribution for each document, but also learns two collection-specific distributions for each pair of update collection and base collection: the joint topic distribution and the update topic distribution. This paper revises DualSum as a baseline for evaluation in Section 5.2. Topic Models for Documents Comparison. The other type of related work is the comparison of documents."
C16-1098,D11-1105,0,0.0252479,"Topic Models for Documents Comparison. The other type of related work is the comparison of documents. Most existing studies for this goal focus on topic models to discover common and specific themes among document collections, referred to as cross-collection topic models (Paul, 2009). This idea was first explored with an initial topic model PLSI (Zhai et al., 2004), and later improved with LDA topic model (Blei, 2012; Pual, 2009) which inspires our dTM-Dirichlet. There are a number of real-world applications extending cross-collection topic models in different scenarios (Ahmed and Xing, 2010; Li et al., 2011). For example, Paul and Girju (2009) employed cross-collection LDA (ccLDA) for cross-cultural analysis of blogs and forums and later they proposed a two-dimensional topicaspect model (TAM) to jointly discover topics and aspects in scientific literature (Paul and Girju, 2010). The common idea behind these cross-collection topic models is that using latent topics capture the common and unique word usage among document collections. Cross-collection topic models neglect the correlations between each collection-specific topic and the common background topic, thus make it insufficient to capture dif"
C16-1098,P13-1099,0,0.0263398,"be either extractive or abstractive (Sekine and Nobata, 2003). Our work focuses on the extractive techniques which involve in assigning saliency scores to sentences and extracting high-scored sentences in a greedy manner to construct a summary (Wan et al., 2007; Cai et al., 2010; Gupta and Lehal, 2010; Celikyilmaz and Hakkani-Tur, 2011). Graph-based ranking techniques such as TextRank (Mihalcea and Tarau, 2004) and LexPageRank (ErKan and Radev, 2004) have been widely used in extractive summarization. A bigram based supervised method was proposed for extractive summarization in ILP framework (Li et al., 2013; Li, 2015). Jha et al. (2015) proposed an extractive algorithm that combines a content model with a discourse model to generate coherent summaries for scientific articles. A multi-dimensional summarization methodology was proposed to transform the paradigm of traditional summarization research through multi-disciplinary fundamental exploration on semantics, dimension, knowledge, computing and cyber-physical society (Zhuge, 2016). Comparative Summarization. Unlike the generic summarization that summarizes the common information in document collection, the comparative summarization aims to summ"
C16-1098,D15-1219,1,0.835062,"tive or abstractive (Sekine and Nobata, 2003). Our work focuses on the extractive techniques which involve in assigning saliency scores to sentences and extracting high-scored sentences in a greedy manner to construct a summary (Wan et al., 2007; Cai et al., 2010; Gupta and Lehal, 2010; Celikyilmaz and Hakkani-Tur, 2011). Graph-based ranking techniques such as TextRank (Mihalcea and Tarau, 2004) and LexPageRank (ErKan and Radev, 2004) have been widely used in extractive summarization. A bigram based supervised method was proposed for extractive summarization in ILP framework (Li et al., 2013; Li, 2015). Jha et al. (2015) proposed an extractive algorithm that combines a content model with a discourse model to generate coherent summaries for scientific articles. A multi-dimensional summarization methodology was proposed to transform the paradigm of traditional summarization research through multi-disciplinary fundamental exploration on semantics, dimension, knowledge, computing and cyber-physical society (Zhuge, 2016). Comparative Summarization. Unlike the generic summarization that summarizes the common information in document collection, the comparative summarization aims to summarize the d"
C16-1098,N10-1012,0,0.113966,"Missing"
C16-1098,D09-1146,0,0.211248,"arison. The other type of related work is the comparison of documents. Most existing studies for this goal focus on topic models to discover common and specific themes among document collections, referred to as cross-collection topic models (Paul, 2009). This idea was first explored with an initial topic model PLSI (Zhai et al., 2004), and later improved with LDA topic model (Blei, 2012; Pual, 2009) which inspires our dTM-Dirichlet. There are a number of real-world applications extending cross-collection topic models in different scenarios (Ahmed and Xing, 2010; Li et al., 2011). For example, Paul and Girju (2009) employed cross-collection LDA (ccLDA) for cross-cultural analysis of blogs and forums and later they proposed a two-dimensional topicaspect model (TAM) to jointly discover topics and aspects in scientific literature (Paul and Girju, 2010). The common idea behind these cross-collection topic models is that using latent topics capture the common and unique word usage among document collections. Cross-collection topic models neglect the correlations between each collection-specific topic and the common background topic, thus make it insufficient to capture differential word usage. More important"
C16-1098,W00-0403,0,0.396704,"Missing"
C16-1098,W03-0509,0,0.0605847,"o measure the senThis work is licenced under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1028 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1028–1038, Osaka, Japan, December 11-17 2016. tence discriminative capacity and a greedy sentence selection method to automatically generate summary for dTM-based comparative summarization. 2 Related Work Multi-document Summarization. Existing multi-document summarization can be either extractive or abstractive (Sekine and Nobata, 2003). Our work focuses on the extractive techniques which involve in assigning saliency scores to sentences and extracting high-scored sentences in a greedy manner to construct a summary (Wan et al., 2007; Cai et al., 2010; Gupta and Lehal, 2010; Celikyilmaz and Hakkani-Tur, 2011). Graph-based ranking techniques such as TextRank (Mihalcea and Tarau, 2004) and LexPageRank (ErKan and Radev, 2004) have been widely used in extractive summarization. A bigram based supervised method was proposed for extractive summarization in ILP framework (Li et al., 2013; Li, 2015). Jha et al. (2015) proposed an extr"
C16-1098,C10-1111,0,0.0974607,"summarization research through multi-disciplinary fundamental exploration on semantics, dimension, knowledge, computing and cyber-physical society (Zhuge, 2016). Comparative Summarization. Unlike the generic summarization that summarizes the common information in document collection, the comparative summarization aims to summarize the differences among document groups. Wang et al. (2012) proposed a discriminative sentence selection method to generate summary by selecting sentences in a greedy manner to minimize the generalized variance of a covariance matrix using a multivariate normal model. Shen and Li (2010) proposed a method by building the sentence graph for each document group and extracting a complementary minimum dominating set on each graph to form a discriminative summary. Update Summarization. The most similar task to comparative summarization is update summarization, which aims to detect and summarize novel information in a document set B under the assumption that users have already learnt the documents in set A, where documents in A chronologically precede the documents in B. The update summarization has been well studied. Most existing methods solve it as a redundancy removal problem b"
C16-1098,P07-1070,0,0.0232127,"nternational Conference on Computational Linguistics: Technical Papers, pages 1028–1038, Osaka, Japan, December 11-17 2016. tence discriminative capacity and a greedy sentence selection method to automatically generate summary for dTM-based comparative summarization. 2 Related Work Multi-document Summarization. Existing multi-document summarization can be either extractive or abstractive (Sekine and Nobata, 2003). Our work focuses on the extractive techniques which involve in assigning saliency scores to sentences and extracting high-scored sentences in a greedy manner to construct a summary (Wan et al., 2007; Cai et al., 2010; Gupta and Lehal, 2010; Celikyilmaz and Hakkani-Tur, 2011). Graph-based ranking techniques such as TextRank (Mihalcea and Tarau, 2004) and LexPageRank (ErKan and Radev, 2004) have been widely used in extractive summarization. A bigram based supervised method was proposed for extractive summarization in ILP framework (Li et al., 2013; Li, 2015). Jha et al. (2015) proposed an extractive algorithm that combines a content model with a discourse model to generate coherent summaries for scientific articles. A multi-dimensional summarization methodology was proposed to transform th"
C16-1100,D10-1051,0,0.018129,"scribes some previous work on poetry generation and compares our work with previous methods. Section 3 describes our planning based poetry generation framework. We introduce the datasets and experimental results in Section 4. Section 5 concludes the paper. 2 Related Work Poetry generation is a challenging task in NLP. Oliveira (2009; 2012) proposed a Spanish poem generation method based on semantic and grammar templates. Netzer et al. (2009) employed a method based on word association measures. Tosa et al. (2008) and Wu et al. (2009) used a phrase search approach for Japanese poem generation. Greene et al. (2010) applied statistical methods to analyze, generate and translate rhythmic poetry. Colton et al. (2012) described a corpus-based poetry generation system that uses templates to construct poems according to the given constrains. Yan et al. (2013) considered the poetry generation as an optimization problem based on a summarization framework with several constraints. Manurung (2004; 2012) and Zhou et al. (2010) used genetic algorithms for generating poems. An important approach to poem generation is based on statistical machine translation (SMT). Jiang and Zhou (2008) used an SMT-based model in gen"
C16-1100,C08-1048,0,0.154329,"ward tone); the last character of the second and last line in a quatrain must belong to the same rhyme category (Wang, 2002). With such strict restrictions, the well-written quatrain is full of rhythmic beauty. In recent years, the research of automatic poetry generation has received great attention. Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems. More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016). These methods usually generate the first line by selecting one line from the dataset of poems according to the user’s writing intents (usually a set of keywords), and the other three lines are generated based on the first line and the previous lines. The user’s writing intent can only affect the first line, and the rest three lines may ha"
C16-1100,D16-1230,0,0.00880819,"Missing"
C16-1100,D16-1126,0,0.139197,"Missing"
C16-1100,W04-3252,0,0.0142967,"according to one sub-topic and all the preceding lines. 3.2 Poem Planning 3.2.1 Keyword Extraction The user’s input writing intent can be represented as a sequence of words. There is an assumption in the Poem Planning stage that the number of keywords extracted from the input query Q must be equal to the number of lines N in the poem, which can ensure each line takes just one keyword as the sub-topic. If the user’s input query Q is too long, we need to extract the most important N words and keep the original order as the keywords sequence to satisfy the requirement. We use TextRank algorithm (Mihalcea and Tarau, 2004) to evaluate the importance of words. It is a graph-based ranking algorithm based on PageRank (Brin and Page, 1998). Each candidate word is represented by a vertex in the graph and edges are added between two words according to their cooccurrence; the edge weight is set according to the total count of co-occurrence strength of the two words. The TextRank score S(Vi ) is initialized to a default value (e.g. 1.0) and computed iteratively until convergence according to the following equation: S(Vi ) = (1 − d) + d X Vj ∈E(Vi ) P wji Vk ∈E(Vj ) wjk S(Vj ), (1) where wij is the weight of the edge be"
C16-1100,C16-1316,0,0.0286406,"ent hidden layers of the decoder and two encoders contained 512 hidden units. Parameters of our model were randomly initialized over a uniform distribution with support [-0.08,0.08]. The model was trained with the AdaDelta algorithm (Zeiler, 2012), where the minibatch was set to be 128. The final model is selected according to the perplexity on the validation set. 4.3 Evaluation 4.3.1 Evaluation Metrics It is well known that accurate evaluation of text generation system is difficult, such as the poetry generation and dialog response generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016). There are thousands of ways to generate an appropriate and relative poem or dialog response given a specific topic, the limited references are impossible to cover all the correct results. Liu et al. (2016) has recently shown that the overlap-based automatic evaluation metrics adapted for dialog responses, such 1 A collaborative online encyclopedia provided by Chinese search engine Baidu: http://baike.baidu.com. 1056 Poeticness Fluency Coherence Meaning Does the poem follow the rhyme and tone requirements ? Does the poem read smoothly and fluently? Is the poem coherent across lines? Does the"
C16-1100,W09-2005,0,0.0455963,"r genres of poetry in China. The principles of a quatrain include: The poem consists of four lines and each line has five or seven characters; every character has a particular tone, Ping (the level tone) or Ze (the downward tone); the last character of the second and last line in a quatrain must belong to the same rhyme category (Wang, 2002). With such strict restrictions, the well-written quatrain is full of rhythmic beauty. In recent years, the research of automatic poetry generation has received great attention. Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems. More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016). These methods usually generate the first line by selecting one line from the dataset of poems according to the user’s wr"
C16-1100,2005.sigdial-1.6,0,0.0113308,"et al., 2013). The recurrent hidden layers of the decoder and two encoders contained 512 hidden units. Parameters of our model were randomly initialized over a uniform distribution with support [-0.08,0.08]. The model was trained with the AdaDelta algorithm (Zeiler, 2012), where the minibatch was set to be 128. The final model is selected according to the perplexity on the validation set. 4.3 Evaluation 4.3.1 Evaluation Metrics It is well known that accurate evaluation of text generation system is difficult, such as the poetry generation and dialog response generation (Zhang and Lapata, 2014; Schatzmann et al., 2005; Mou et al., 2016). There are thousands of ways to generate an appropriate and relative poem or dialog response given a specific topic, the limited references are impossible to cover all the correct results. Liu et al. (2016) has recently shown that the overlap-based automatic evaluation metrics adapted for dialog responses, such 1 A collaborative online encyclopedia provided by Chinese search engine Baidu: http://baike.baidu.com. 1056 Poeticness Fluency Coherence Meaning Does the poem follow the rhyme and tone requirements ? Does the poem read smoothly and fluently? Is the poem coherent acro"
C16-1100,D14-1074,0,0.669329,"t years, the research of automatic poetry generation has received great attention. Most approaches employ rules or templates (Tosa et al., 2008; Wu et al., 2009; Netzer et al., 2009; Oliveira, 2009; Oliveira, 2012), genetic algorithms (Manurung, 2004; Zhou et al., 2010; Manurung et al., 2012), summarization methods (Yan et al., 2013) and statistical machine translation methods (Jiang and Zhou, 2008; He et al., 2012) to generate poems. More recently, deep learning methods have emerged as a promising discipline, which considers the poetry generation as a sequence-to-sequence generation problem (Zhang and Lapata, 2014; Wang et al., 2016; Yi et al., 2016). These methods usually generate the first line by selecting one line from the dataset of poems according to the user’s writing intents (usually a set of keywords), and the other three lines are generated based on the first line and the previous lines. The user’s writing intent can only affect the first line, and the rest three lines may have no association with the main topic of the poem, which may lead to semantic inconsistency when generating poems. In addition, topics of poems are usually represented by the words from the collected poems in the training"
C16-1185,P07-1071,0,0.0189292,"urther proposed to use CRF to deal with the problem, using both traditional bag of words features and new features such as dialog structures and dependencies between utterances. The common weakness of these methods is that they depend heavily on the features selected, and the feature construction process consumes much human effort. 2.2 Deep Learning models As deep learning becomes increasingly popular, researchers have been trying to apply deep learning frameworks to deal with natural language processing and understanding tasks, including sentence modelling, DA labelling and many other tasks. Collobert and Weston (2007), Collobert and Weston (2008) and Collobert et al. (2011) constructed deep neural network structures for natural language processing tasks, which project one-hot word representations into distributed representations with a look-up table (or a projection layer) and build either feed forward or convolutional neural network upon them. This type of models seek to free researchers from laborious feature engineering, and allow the systems to easily adapt to different tasks. Kalchbrenner et al. (2014) proposed a dynamic convolution neural network with multiple layers of convolution and k-max pooling"
C16-1185,J81-4005,0,0.75101,"Missing"
C16-1185,P13-2146,0,0.0263069,"pplying contextual information. However, this approach loses long distance dependency, thus giving very little improvement when compared with the CNN baseline. Zhou et al. (2015) tried to capture sequential information with the conditional random field (CRF) on the basis of a heterogeneous neural network. While their model works very well, we must also be keen to note that the RNN family models surpass CRF in sequence prediction tasks, as pointed out by Irsoy and Cardie (2014) and Yao et al. (2014). Apart from textual and contextual information, non-textual information can also be considered. Hu et al. (2013) applied a restricted Boltzmann machine to combine textual and non-textual features in a community question answering problem. Their work makes good use of the non-textual features by combining them with textual features in an unsupervised manner. To deal with the limitations of previous works, we propose a multi-level GRNN with non-textual features to predict the DAs. Our contributions can be highlighted in the following aspects: • We apply a two-level GRNN to predict the DA. The low level GRNN is designed for modelling textual information of each sentence, and the top level GRNN is designed"
C16-1185,D14-1080,0,0.0361244,"by feeding previous sentences in a fixed window together with the current one to a feed forward neural network. This makes a good attempt in applying contextual information. However, this approach loses long distance dependency, thus giving very little improvement when compared with the CNN baseline. Zhou et al. (2015) tried to capture sequential information with the conditional random field (CRF) on the basis of a heterogeneous neural network. While their model works very well, we must also be keen to note that the RNN family models surpass CRF in sequence prediction tasks, as pointed out by Irsoy and Cardie (2014) and Yao et al. (2014). Apart from textual and contextual information, non-textual information can also be considered. Hu et al. (2013) applied a restricted Boltzmann machine to combine textual and non-textual features in a community question answering problem. Their work makes good use of the non-textual features by combining them with textual features in an unsupervised manner. To deal with the limitations of previous works, we propose a multi-level GRNN with non-textual features to predict the DAs. Our contributions can be highlighted in the following aspects: • We apply a two-level GRNN to"
C16-1185,P14-1062,0,0.00928055,"rocessing and understanding tasks, including sentence modelling, DA labelling and many other tasks. Collobert and Weston (2007), Collobert and Weston (2008) and Collobert et al. (2011) constructed deep neural network structures for natural language processing tasks, which project one-hot word representations into distributed representations with a look-up table (or a projection layer) and build either feed forward or convolutional neural network upon them. This type of models seek to free researchers from laborious feature engineering, and allow the systems to easily adapt to different tasks. Kalchbrenner et al. (2014) proposed a dynamic convolution neural network with multiple layers of convolution and k-max pooling to model a sentence. As imagined, this model is computationally expensive due to the many layers. Conversely, the CNN model proposed by Kim (2014) takes just one convolution and pooling layer with multi-channel word embeddings, followed by a softmax classifier. This model succeeded in many NLP tasks, such as sentence classification, sentiment analysis and so on. Apart from CNN like architectures, researchers also applied recurrent neural network (RNN) and its variants to model sentences. Origin"
C16-1185,D10-1084,0,0.339457,"oss of information, thereby leading to a poor result. Louwerse and Crossley (2006) introduced n-gram features to predict the DA, which is widely used in NLP tasks. This model uncovers more information from the text, but it fails to capture long-distance dependency. Surendran and Levow (2006) used SVM on individual sentences then viterbi decoding to make use of contextual information in a HMM style. This model builds a rather good framework for sequential labelling, as it not only feeds each sentence to a strong classifier SVM, but also makes use of context information in a probability graph. (Kim et al., 2010) further proposed to use CRF to deal with the problem, using both traditional bag of words features and new features such as dialog structures and dependencies between utterances. The common weakness of these methods is that they depend heavily on the features selected, and the feature construction process consumes much human effort. 2.2 Deep Learning models As deep learning becomes increasingly popular, researchers have been trying to apply deep learning frameworks to deal with natural language processing and understanding tasks, including sentence modelling, DA labelling and many other tasks"
C16-1185,D14-1181,0,0.0255582,"ks, which project one-hot word representations into distributed representations with a look-up table (or a projection layer) and build either feed forward or convolutional neural network upon them. This type of models seek to free researchers from laborious feature engineering, and allow the systems to easily adapt to different tasks. Kalchbrenner et al. (2014) proposed a dynamic convolution neural network with multiple layers of convolution and k-max pooling to model a sentence. As imagined, this model is computationally expensive due to the many layers. Conversely, the CNN model proposed by Kim (2014) takes just one convolution and pooling layer with multi-channel word embeddings, followed by a softmax classifier. This model succeeded in many NLP tasks, such as sentence classification, sentiment analysis and so on. Apart from CNN like architectures, researchers also applied recurrent neural network (RNN) and its variants to model sentences. Originally proposed by Elman (1990), RNN is expected to propagate information through time, which means one can make use of past information as latent variables. Mikolov et al. (2010) applied RNN to language modelling and got some very interesting resul"
C16-1185,N16-1062,0,0.618196,"al License. creativecommons.org/licenses/by/4.0/ License details: http:// 1970 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 1970–1979, Osaka, Japan, December 11-17 2016. Utterance length(words) 51~100 1~2 >100 21~50 3~5 11~20 6~10 1~2 3~5 6~10 11~20 21~50 51~100 >100 Figure 1: Sentence length distribution in the SWDA corpus to facilitate the labelling process. In fact, the most important character of DA labelling that is different from simple sentence classification is that utterances appear sequentially in a conversation. Lee and Dernoncourt (2016) tried to make use of historical information by feeding previous sentences in a fixed window together with the current one to a feed forward neural network. This makes a good attempt in applying contextual information. However, this approach loses long distance dependency, thus giving very little improvement when compared with the CNN baseline. Zhou et al. (2015) tried to capture sequential information with the conditional random field (CRF) on the basis of a heterogeneous neural network. While their model works very well, we must also be keen to note that the RNN family models surpass CRF in"
C16-1185,J00-3003,0,0.871709,"Missing"
C18-1330,W17-2339,0,0.0495048,"ork models have also been used for the MLC task. Zhang and Zhou (2006) propose the BP-MLL that utilizes a fully-connected neural network and a pairwise ranking loss function. Nam et al. (2013) propose a neural network using cross-entropy loss instead of ranking loss. Benites and Sapozhnikova (2015) increase classification speed by adding an extra ART layer for clustering. Kurata et al. (2016) utilize word embeddings based on CNN to capture label correlations. Chen et al. (2017) propose to represent semantic information of text and model high-order label correlations by combining CNN with RNN. Baker and Korhonen (2017) initialize the final hidden layer with rows that map to co-occurrence of labels based on the CNN architecture to improve the performance of the model. Ma et al. (2018) propose to use the multi-label classification algorithm for machine translation to handle the situation where a sentence can be translated into more than one correct sentences. 5 Conclusions and Future Work In this paper, we propose to view the multi-label classification task as a sequence generation problem to model the correlations between labels. A sequence generation model with a novel decoder structure is proposed to impro"
C18-1330,D14-1181,0,0.00963567,"ines We compare our proposed methods with the following baselines: • Binary Relevance (BR) (Boutell et al., 2004) transforms the MLC task into multiple single-label classification problems by ignoring the correlations between labels. • Classifier Chains (CC) (Read et al., 2011) transforms the MLC task into a chain of binary classification problems and takes high-order label correlations into consideration. • Label Powerset (LP) (Tsoumakas and Katakis, 2006) transforms a multi-label problem to a multiclass problem with one multi-class classifier trained on all unique label combinations. • CNN (Kim, 2014) uses multiple convolution kernels to extract text features, which are then inputted to the linear transformation layer followed by a sigmoid function to output the probability distribution over the label space. The multi-label soft margin loss is optimized. • CNN-RNN (Chen et al., 2017) utilizes CNN and RNN to capture both the global and local textual semantics and model the label correlations. Following the previous work (Chen et al., 2017), we adopt the linear SVM as the base classifier in BR, CC and LP. We implement BR, CC and LP by means of Scikit-Multilearn (Szyma´nski, 2017), an opensou"
C18-1330,N16-1063,0,0.327329,"large datasets. Other methods such as ML-DT (Clare and King, 2001), Rank-SVM (Elisseeff and Weston, 2002), and ML-KNN (Zhang and Zhou, 2007) can only be used to capture the first or second order label correlations or are computationally intractable when high-order label correlations are considered. In recent years, neural networks have achieved great success in the field of NLP. Some neural network models have also been applied in the MLC task and achieved important progress. For instance, fully connected neural network with pairwise ranking loss function is utilized in Zhang and Zhou (2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style tran"
C18-1330,D15-1099,1,0.784573,"el data directly. Clare and King (2001) construct decision tree based on multi-label entropy to perform classification. Elisseeff and Weston (2002) optimize the empirical ranking loss by using maximum margin strategy and kernel tricks. Collective multi-label classifier (CML) (Ghamrawi and McCallum, 2005) adopts maximum entropy principle to deal with multi-label data by encoding label correlations as constraint conditions. Zhang and Zhou (2007) adopt k-nearest neighbor techniques to deal with multi-label data. F¨urnkranz et al. (2008) make ranking among labels by utilizing pairwise comparison. Li et al. (2015) propose a novel joint learning algorithm that allows the feedbacks to be propagated from the classifiers for latter labels to the classifier for the current label. Most methods, however, can only be used to capture the first or second order label correlations or are computationally intractable in considering high-order label correlations. Among ensemble methods, Tsoumakas et al. (2011) break the initial set of labels into a number of small random subsets and employ the LP algorithm to train a corresponding classifier. Szyma´nski et al. (2016) propose to construct a label co-occurrence graph a"
C18-1330,P18-2027,1,0.848082,"(2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style transfer (Shen et al., 2017; Xu et al., 2018) and other domains, we propose a sequence generation model with a novel decoder structure to solve the MLC task. The proposed sequence generation model consists of an encoder and a decoder with the attention 1 The datasets and code are available at https://github.com/lancopku/SGM This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3915 Proceedings of the 27th International Conference on Computational Linguistics, pages 3915–3926 Santa Fe, New Mexi"
C18-1330,D15-1166,0,0.0436376,"cted neural network with pairwise ranking loss function is utilized in Zhang and Zhou (2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style transfer (Shen et al., 2017; Xu et al., 2018) and other domains, we propose a sequence generation model with a novel decoder structure to solve the MLC task. The proposed sequence generation model consists of an encoder and a decoder with the attention 1 The datasets and code are available at https://github.com/lancopku/SGM This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3915 Proceedings of the 27th Inter"
C18-1330,P18-2053,1,0.840617,"m et al. (2013) propose a neural network using cross-entropy loss instead of ranking loss. Benites and Sapozhnikova (2015) increase classification speed by adding an extra ART layer for clustering. Kurata et al. (2016) utilize word embeddings based on CNN to capture label correlations. Chen et al. (2017) propose to represent semantic information of text and model high-order label correlations by combining CNN with RNN. Baker and Korhonen (2017) initialize the final hidden layer with rows that map to co-occurrence of labels based on the CNN architecture to improve the performance of the model. Ma et al. (2018) propose to use the multi-label classification algorithm for machine translation to handle the situation where a sentence can be translated into more than one correct sentences. 5 Conclusions and Future Work In this paper, we propose to view the multi-label classification task as a sequence generation problem to model the correlations between labels. A sequence generation model with a novel decoder structure is proposed to improve the performance of classification. Extensive experimental results show that the proposed methods outperform the baselines by a substantial margin. Further analysis o"
C18-1330,D15-1044,0,0.0499285,"d in Zhang and Zhou (2006). Kurata et al. (2016) propose to perform classification using the convolutional neural network (CNN). Chen et al. (2017) use CNN and recurrent neural network (RNN) to capture the semantic information of texts. However, they either neglect the correlations between labels or do not consider differences in the contributions of textual content when predicting labels. In this paper, inspired by the tremendous success of the sequence-to-sequence (Seq2Seq) model in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Sun et al., 2017), abstractive summarization (Rush et al., 2015; Lin et al., 2018), style transfer (Shen et al., 2017; Xu et al., 2018) and other domains, we propose a sequence generation model with a novel decoder structure to solve the MLC task. The proposed sequence generation model consists of an encoder and a decoder with the attention 1 The datasets and code are available at https://github.com/lancopku/SGM This work is licenced under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/ Licence details: http: 3915 Proceedings of the 27th International Conference on Computational Linguistics, pages 3915–3926"
C18-1330,D16-1137,0,0.0283203,"ity under the distribution yt−1 . yt−1 is the probability 3917 distribution over the label space L at time-step t − 1 and is computed as follows: ot = Wo f (Wd st + Vd ct ) (8) yt = sof tmax(ot + It ) (9) where Wo , Wd , and Vd are weight parameters, It ∈ RL is the mask vector that is used to prevent the decoder from predicting repeated labels, and f is a nonlinear activation function. ( −∞ if the label li has been predicted at previous t − 1 time steps. (It )i = 0 otherwise. (10) At the training stage, the loss function is the cross-entropy loss function. We employ the beam search algorithm (Wiseman and Rush, 2016) to find the top-ranked prediction path at inference time. The prediction paths ending with the eos are added to the candidate path set. 2.3 Global Embedding In the sequence generation model mentioned above, the embedding vector g(yt−1 ) in Equation (7) is the embedding of the label that has the highest probability under the distribution yt−1 . However, this calculation only takes advantage of the maximum value of yt−1 greedily. The proposed sequence generation model generates labels sequentially and predicts the next label conditioned on its previously predicted labels. Therefore, it is likel"
D15-1219,J05-3002,0,0.294057,"nces are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapalme, 2011). Statistics of words or phrases and syntactical analysis that have been widely used in existing summarization approaches are all shallow processing of text. It is necessary to explore su"
D15-1219,N07-4013,0,0.0516713,"Missing"
D15-1219,P15-1153,0,0.498511,"the source text into AMR graphs, and then transforms them into a summary graph and plans to generate text from it. This work only focuses on the graph-to-graph transformation. The module of text generation from AMR has not been developed. The nodes and edges of AMR graph are entities and relations between entities respectively, which are sufficiently different from the BSUs semantic link network. Moreover, texts can be generated efficiently from the BSUs network. Another recent abstractive summarization method generates new sentences by selecting and merging phrases from the input documents (Bing et al., 2015). It first extracts noun phrases and verb-object phrases from the input documents, and then calculates saliency scores for them. An ILP optimization framework is used to simultaneously select and merge informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. As the results show that the method is difficult to generate new informative sentences really different from the original sentences and may generate some none factual sentences since phrases from different sentences are merged. Open information extraction has been proposed by (Ban"
D15-1219,P05-1045,0,0.00715555,"Missing"
D15-1219,D08-1019,0,0.018646,"original documents directly by ranking or scoring and only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapalme, 2011). Statistics of words or phrases and syntactical analysis that have been widely used in existing summarization approaches are all shallow processing of text. It is necessary to explore summarization methods based on"
D15-1219,W11-1608,0,0.809758,"xts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapalme, 2011). Statistics of words or phrases and syntactical analysis that have been widely used in existing summarization approaches are all shallow processing of text. It is necessary to explore summarization methods based on deeper semantic analysis. We define the concept of Basic Semantic Unit (BSU) to express the semantics of texts. A BSU is an action indicator with its obligatory arguments which contain actor and receiver of the action. BSU is the most basic element of coherent information in texts, which can describe the semantics of an event or action. The semantic information of texts is represen"
D15-1219,N15-1114,0,0.0515888,"Missing"
D15-1219,N03-1020,0,0.213153,"and parameter tuning, and DUC2007 was used for testing. Based on the tuning set, the parameter λ is set as 10 and δ is set as 0.7 after tuning. Our system is compared with one state-of-theart graph-based extractive approach MultiMR (Wan and Xiao, 2009) and one abstractive approach TTG (Genest and Lapalme, 2011). In addition, we have implemented another baseline RankBSU which uses the graph-based ranking methods on the BSUs network to rank BSUs and select the top ranked BSUs to generate sentences. 4.2 Results ROUGE-1.5.5 toolkit was used to evaluate the quality of summary on DUC 2007 dataset (Lin and Hovy, 2003). The ROUGE scores of the NIST Baseline system (i.e. NIST Baseline) and average ROUGE scores of all the participating systems (i.e. AveDUC) for DUC 2007 main task were also listed. According to the results in Table 2, our system much outperforms the NIST Baseline and AveDUC, and achieves higher ROUGE scores than the abstractive approach TTG. So the abstract representation of texts and the information extraction process in our system are effective for multi-document summarization. Our system also achieves better performance than the baseline RankBSU, which demonstrates that the network reductio"
D15-1219,N04-1019,0,0.0773995,"more efficient than the popular graph-based ranking methods. As compared with the state-of-art graph-based extractive method MultiMR, our system also achieves better performance. Furthermore, our system is abstractive with abstract representation and sentence generation. Incorrect 1911 parser and co-reference resolution will lead to wrong extraction of BSU. If with more accurate parser and co-reference resolution, our system will be expected to achieve better performance. Since ROUGE metric evaluates summaries only from word overlapping perspective, we also use the pyramid evaluation metric (Nenkova and Passonneau, 2004) which can measure the summary quality beyond simply string matching. The pyramid evaluation metric involves semantic matching of summary content units (SCUs) so as to recognize alternate realizations of the same meaning, which is a better metric for the abstractive summary evaluation. Since the manual pyramid evaluation is time-consuming and the evaluation results can’t be reproducible with different groups of assessors, we use the automated version of pyramid proposed in (Passonneau et al., 2013) and adopt the same setting as in (Bing et al., 2015). Table 3 shows the evaluation results of ou"
D15-1219,P13-2026,0,0.0600126,"ummaries only from word overlapping perspective, we also use the pyramid evaluation metric (Nenkova and Passonneau, 2004) which can measure the summary quality beyond simply string matching. The pyramid evaluation metric involves semantic matching of summary content units (SCUs) so as to recognize alternate realizations of the same meaning, which is a better metric for the abstractive summary evaluation. Since the manual pyramid evaluation is time-consuming and the evaluation results can’t be reproducible with different groups of assessors, we use the automated version of pyramid proposed in (Passonneau et al., 2013) and adopt the same setting as in (Bing et al., 2015). Table 3 shows the evaluation results of our system and the three baseline systems on DUC 2007. The results show that the performance of our system is significantly better than the three baseline systems, which demonstrates that the summaries of our system contain more SCUs than summaries of other systems. So our system can generate more informative summary. In addition, large volumes of news texts for popular news events are crawled from the news websites. Figure 1 and 2 show the summaries for the “Malaysia MH370 Disappear” news event gene"
D15-1219,W09-2808,0,0.0109126,"d only little post-editing is made (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008). Abstractive summarization requires semantic analysis and abstract representation of texts, which need knowledge on and beyond the texts (Zhuge, 2015a). There are some abstractive approaches in recent years: sentence compression (Knight and Marcu, 2000; Knight and Marcu, 2002; Cohn and Lapata, 2009), sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008), and sentence revision (Tanaka et al., 2009). However, these approaches are sentence rewriting techniques based on syntactical analysis without semantic analysis and abstract representation. Fully abstractive summarization approach requires a separate process for the analysis of texts that serves as an intermediate step before the generation of sentences (Genest and Lapalme, 2011). Statistics of words or phrases and syntactical analysis that have been widely used in existing summarization approaches are all shallow processing of text. It is necessary to explore summarization methods based on deeper semantic analysis. We define the conce"
D15-1219,P13-1137,0,0.0206818,"nce to a coherent body of information based on the BSU semantic link network by summary structure planning. 2011). They extract binary relations from the web, which is different from our approach that extracts events or actions expressed in texts. 3 2 Related Work There are some abstractive summarization approaches in recent years. An approach TTG attempts to generate abstractive summary by using text-to-text generation to generate sentence for each subject-verb-object triple (Genest and Lapalme, 2011). A system that attempts to generate abstractive summaries for spoken meetings was proposed (Wang and Cardie, 2013). It identifies relation instances that are represented by a lexical indicator with an argument constituent from texts. Then the relation instances are filled into templates which are extracted by applying multiple sequence alignment. Both of these systems need to select a subset of the large volumes of generated sentences. However, our system generates summary directly by summary structure planning. It can generate well-organized and coherent summary more effectively. A recent work aims to generate abstractive summary based on Abstract Meaning Representation (AMR) (Liu et al., 2015). It first"
D15-1219,W11-1902,0,0.0107844,"f texts is obtained by extracting BSUs and constructing BSU semantic link network. A BSU is represented as an actoraction-receiver triple, which can both detects the crucial content and incorporates enough syntactic information to facilitate the downstream sentence generation. Some actions may not have the receiver argument. For example, “Flight MH370 – disappear” and “Flight MH370 - leave - Kuala Lumpur” are two BSUs. BSU Extraction. BSUs are extracted from the sentences of the documents. The texts are preprocessed by name entity recognition (Finkel et al., 2005) and co-reference resolution (Lee et al., 2011). Constituent and dependency parses are obtained by Stanford parser (Klein and Manning, 2003). The eligible action indicator is restricted to be a predicate verb; the eligible actor and receiver arguments are noun phrase. Both the actor and receiver arguments take the form of constituents in the parse tree. A valid BSU should have one action indicator and at least one actor argument, and satisfy the following constraints:  The actor argument is the nominal subject or external subject or the complement of a passive verb which is introduced by the preposition “by” and does the action.  The rec"
D15-1219,P03-1054,0,\N,Missing
D18-1205,P16-1154,0,0.0330196,"r abstractive sentence summarization (Takase et al., 2016; Rush et al., 2015; Chopra et al., 2016). These models are trained on a large corpus of news documents which are usually shortened to be the first one or two sentences, and their headlines. Later, some work explored the seq2seq models on document summarization, which produce a multi-sentence summary for a document. The seq2seq models usually exhibit some undesirable behaviors, such as inaccurately reproducing factual details, unable to deal with out-ofvocabulary (OOV) words and repetitions. To alleviate these issues, copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Nallapati et al., 2016) has been incorporated into the encoderdecoder architecture. Distraction-based attention model (Chen et al., 2016) and coverage mechanism (See et al., 2017) have also been investigated to alleviate the repetition problem. To better train the seq2seq model on tasks with long documents and multi-sentence summaries, a deep reinforced model was proposed to combine the standard words predication with teacher forcing learning and the global sequence prediction training with reinforcement learning (Paulus et al., 2017). Recently, Tan et al. (2017a) prop"
D18-1205,P15-2136,0,0.0218963,"an 75 words, rising to +10.68 Rouge-1, +6.05 Rouge-2 and +4.86 Rouge-L for summaries more than 125 words). The results also verify that our method is more effective in selecting salient information from documents, especially for long documents. 5 Related Work Existing exploration on document summarization mainly can be categorized to extractive methods and abstractive methods. 5.1 Extractive Summarization Methods Neural networks have been widely investigated on extractive document summarization task. Earlier work attempts to use deep learning techniques to improve sentence ranking or scoring (Cao et al., 2015a,b; Yin and Pei, 2015). Some recent work solves the sentence extraction and document modeling in an end-to-end framework. Cheng and Lapata (2016) propose an encoder-decoder approach where the encoder hierarchically learns the representation of sentences and documents while an attention-based sentence extractor extracts salient sentences sequentially from the original document. Nallapati et al. (2017) propose a recurrent neural network-based sequence-to-sequence model for sequential labelling of each sentence in the document. Neural models are able to leverage large-scale corpora and achieve b"
D18-1205,P16-1046,0,0.0382071,"method is more effective in selecting salient information from documents, especially for long documents. 5 Related Work Existing exploration on document summarization mainly can be categorized to extractive methods and abstractive methods. 5.1 Extractive Summarization Methods Neural networks have been widely investigated on extractive document summarization task. Earlier work attempts to use deep learning techniques to improve sentence ranking or scoring (Cao et al., 2015a,b; Yin and Pei, 2015). Some recent work solves the sentence extraction and document modeling in an end-to-end framework. Cheng and Lapata (2016) propose an encoder-decoder approach where the encoder hierarchically learns the representation of sentences and documents while an attention-based sentence extractor extracts salient sentences sequentially from the original document. Nallapati et al. (2017) propose a recurrent neural network-based sequence-to-sequence model for sequential labelling of each sentence in the document. Neural models are able to leverage large-scale corpora and achieve better performance than traditional methods. 5.2 Abstractive Summarization Methods As the seq2seq learning with neural networks achieve huge succes"
D18-1205,N16-1012,0,0.0304218,"rk-based sequence-to-sequence model for sequential labelling of each sentence in the document. Neural models are able to leverage large-scale corpora and achieve better performance than traditional methods. 5.2 Abstractive Summarization Methods As the seq2seq learning with neural networks achieve huge success in sequence generation tasks like machine translation, it also shows great potential in text summarization area, especially for abstractive methods. Some earlier researches studied the use of seq2seq learning for abstractive sentence summarization (Takase et al., 2016; Rush et al., 2015; Chopra et al., 2016). These models are trained on a large corpus of news documents which are usually shortened to be the first one or two sentences, and their headlines. Later, some work explored the seq2seq models on document summarization, which produce a multi-sentence summary for a document. The seq2seq models usually exhibit some undesirable behaviors, such as inaccurately reproducing factual details, unable to deal with out-ofvocabulary (OOV) words and repetitions. To alleviate these issues, copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Nallapati et al., 2016) has been incorporated into the enc"
D18-1205,P16-1014,0,0.0148855,"tence summarization (Takase et al., 2016; Rush et al., 2015; Chopra et al., 2016). These models are trained on a large corpus of news documents which are usually shortened to be the first one or two sentences, and their headlines. Later, some work explored the seq2seq models on document summarization, which produce a multi-sentence summary for a document. The seq2seq models usually exhibit some undesirable behaviors, such as inaccurately reproducing factual details, unable to deal with out-ofvocabulary (OOV) words and repetitions. To alleviate these issues, copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Nallapati et al., 2016) has been incorporated into the encoderdecoder architecture. Distraction-based attention model (Chen et al., 2016) and coverage mechanism (See et al., 2017) have also been investigated to alleviate the repetition problem. To better train the seq2seq model on tasks with long documents and multi-sentence summaries, a deep reinforced model was proposed to combine the standard words predication with teacher forcing learning and the global sequence prediction training with reinforcement learning (Paulus et al., 2017). Recently, Tan et al. (2017a) propose to leverage the hie"
D18-1205,W04-1013,0,0.00816359,"NNer (Nallapati et al., 2017), while SummaRuNNer-abs is similar to SummaRuNNer but is trained directly on the abstractive summaries. Lead-3 is a strong extractive baseline which uses the first 3 sentences of the document as summary. The abstractive models include: 1) Seq2seq-baseline, which uses the basic seq2seq encoder-decoder structure with attention mechanism and incorporates with the copy mechanism as in (See et al., 2017). 2) ABS-temp-attn (Nallapati et al., 2016), which uses Temporal Attention on the 3.4 Evaluation ROUGE Evaluation We evaluate our models with the standard ROUGE metric (Lin, 2004) and obtain ROUGE scores using the pyrouge package. Results in Table 1 show that our method has significant improvement over state-of-the-art neural abstractive baselines as well as extractive baselines. Note that, the Deep-reinforced model achieves the best ROUGE-L performance because it directly optimizes the ROUGE-L metric. Comparing with the current state-of-the-art model Coverage, our model achieves significant better performance on ROUGE-1 and ROUGE-2 metrics, and comparable performance on ROUGE-L metric, which demonstrates that our model is more effective in selecting salient informatio"
D18-1205,K16-1028,0,0.367396,"ze the selected sentences. In the following, we denote hi , hi,j as the hidden state of the i-th sentence and the j-th word of the i-th sentence in the document encoder part, respectively. In the information selection and summary decoder part, we denote h′t , h′t,k as the hidden state of the t-th summary sentence and the k-th word in the t-th summary sentence, respectively. 2.1 Document Encoder A document d is a sequence of sentences d = {si }, and each sentence is a sequence of words si = {wi,j }. A hierarchical encoder, which consists of two levels: word level and sentence level similar to (Nallapati et al., 2016), is used to encode the document from both word and sentence level. The word-level encoder is a bidirectional Gated Recurrent Unit (GRU) (Chung et al., 2014), which encodes the words of a sentence into sentence representation. The word encoder sequentially updates its hidden state after receiving a word, which is formulated as: hi,j = BiGRU (hi,j−1 , ei,j ) (1) where hi,j and ei,j denotes the hidden state and embedding of word wi,j , respectively. The concatenation of the forward and backward final hidden states in the word-level encoder is indicated as the vector representation xi of the sent"
D18-1205,D14-1162,0,0.0942026,"aulus et al., 2017; Tan et al., 2017a) use the anonymized version of data, which has been pre-processed to replace each named entity with an unique identifier. By contrast, we use the non-anonymized data similar to (See et al., 2017), which is a more favorable and challenging problem because it requires no pre-processing. 3.2 Implementation Details Model Parameters For all experiments, the word-level encoder and summary decoder both use 256-dimensional hidden states, and the sentence-level encoder and sentence selection network both use 512-dimensional hidden states. We use pre-trained Glove (Pennington et al., 2014) vector for initialization of word embeddings. The dimension of word embeddings is 100, which will be further trained in the model. We use a vocabulary of 50k words for both encoder and decoder. 1790 Method Lead-3 SummaRuNNer-abs SummaRuNNer Seq2seq-baseline ABS-temp-attn Graph-attention Deep-reinforced Coverage Our Model Rouge-1 40.34 37.5 39.6 36.64 35.46 38.1 39.87 39.53 41.54 Rouge-2 17.70 14.5 16.2 15.66 13.30 13.9 15.82 17.28 18.18 Rouge-L 36.57 33.4 35.3 33.42 32.65 34.0 36.90 36.38 36.47 Method Lead-3 Seq2seq-b. Coverage Our Model Informat. 3.49∗ 3.11∗ 3.41∗ 3.76 Concise 3.19∗ 2.95∗ 3."
D18-1205,D15-1044,0,0.0603604,"urrent neural network-based sequence-to-sequence model for sequential labelling of each sentence in the document. Neural models are able to leverage large-scale corpora and achieve better performance than traditional methods. 5.2 Abstractive Summarization Methods As the seq2seq learning with neural networks achieve huge success in sequence generation tasks like machine translation, it also shows great potential in text summarization area, especially for abstractive methods. Some earlier researches studied the use of seq2seq learning for abstractive sentence summarization (Takase et al., 2016; Rush et al., 2015; Chopra et al., 2016). These models are trained on a large corpus of news documents which are usually shortened to be the first one or two sentences, and their headlines. Later, some work explored the seq2seq models on document summarization, which produce a multi-sentence summary for a document. The seq2seq models usually exhibit some undesirable behaviors, such as inaccurately reproducing factual details, unable to deal with out-ofvocabulary (OOV) words and repetitions. To alleviate these issues, copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Nallapati et al., 2016) has been inc"
D18-1205,P17-1099,0,0.55795,"e the word attention distributions. Then the word-level context vector when generating the kth word at the tth sentence generation step can be computed as: ct,k = ∑ ∑ i,j i j βt,k hi,j , which is also incorporated into the word decoder. At each word generation step, the vocabulary distribution is calculated from the context vector ′ ct,k and the decoder state ht,k by: ′ ′ Pvocab (wt,k ) = sof tmax(Wv (Wc [ht,k , ct,k ] + bc ) + bv ) (11) where Wv and Wc are learned parameters. The copy mechanism based on the word attention is also imported into the decoder to alleviate the OOV problems as in (See et al., 2017). 2.4 Model Learning with Distant Supervision Despite the end-to-end training for the performance of generated summary, we also directly optimize the sentence selection decisions by importing supervision for the sentence selection vector αt in Equation 5. While there is no explicit supervision for sentence selection, we define a simple approach for labeling sentences based on the reference summaries. To simulate the sentence selection process on human-written abstracts, we compute the words-matching similarities (based on TF-IDF cosine similarity) between a reference-summary sentence and corre"
D18-1205,D16-1112,0,0.0519789,"(2017) propose a recurrent neural network-based sequence-to-sequence model for sequential labelling of each sentence in the document. Neural models are able to leverage large-scale corpora and achieve better performance than traditional methods. 5.2 Abstractive Summarization Methods As the seq2seq learning with neural networks achieve huge success in sequence generation tasks like machine translation, it also shows great potential in text summarization area, especially for abstractive methods. Some earlier researches studied the use of seq2seq learning for abstractive sentence summarization (Takase et al., 2016; Rush et al., 2015; Chopra et al., 2016). These models are trained on a large corpus of news documents which are usually shortened to be the first one or two sentences, and their headlines. Later, some work explored the seq2seq models on document summarization, which produce a multi-sentence summary for a document. The seq2seq models usually exhibit some undesirable behaviors, such as inaccurately reproducing factual details, unable to deal with out-ofvocabulary (OOV) words and repetitions. To alleviate these issues, copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Nallapati et al.,"
D18-1205,P17-1108,0,0.0621811,"Missing"
D18-1441,J05-3002,0,0.231578,"tructural-compression and structural-coverage properties based on the hierarchical encoder-decoder model by adding structural regularization during both the model learning phase and inference phase. 3.1 Structural Compression Compression is a basic property of document summarization, which has been widely explored in traditional document summarization research, such as sentence compression-based methods which shorten sentences by removing non-salient parts (Li et al., 2013; Durrett et al., 2016) and sentence fusion-based methods which merge information from several different source sentences (Barzilay and McKeown, 2005; Cheung and Penn, 2014). As shown in Figure 1, each summary sentence in the human-written reference summary is also created by compressing several specific source sentences. In this paper, we propose to model the structural-compression property of document summarization based on sentence-level attention distributions by: N 1 ∑ i strCom(αt ) = 1 − αt logαti logN (2) i=1 where αt denotes the sentence-level attention distribution when generating the tth summary sentence and N denotes the length of distribution αt . The right part in the above formula is actually the entropy of the distribution α"
D18-1441,I17-2071,0,0.0215189,"tsentence hierarchical structure, it can’t capture the basic structural properties of document summarization (see Figure 1(d) and Table 1). HowA good summary should have the ability to cover most of the important information of an input document. As shown in Figure 1, the humanwritten reference summary covers the information of many source sentences. Coverage has been 4081 used as a measure in many traditional document summarization research, such as the submodularbased methods which optimize the information coverage of the summary with similarity-based coverage metrics (Lin and Bilmes, 2011; Chali et al., 2017). In this work, we simply model the structuralcoverage property of summary based on the hierarchical architecture by encouraging different summary sentences to focus on different sets of source sentences so that the summary can cover more salient sentences of the input document. We measure the structural-coverage of summary based on the sentence-level attention distributions: strCov(αt ) = 1 − ∑ min(αti , t−1 ∑ αti′ ) (3) t′ =0 i which is used to encourage different summary sentences to focus on different sets of source sentences during the summary generation process. As the sentence-level att"
D18-1441,P17-1177,0,0.0242088,"set of sentences repeatedly. On the contrary, our method with structural regularizations focuses on different sets of source sentences when generating different summary sentences and discovers more salient information from the document. put. However, the extension of sentence abstractive methods to document summarization task is not straightforward. As long-distance dependencies are difficult to be captured in the recurrent framework (Bengio et al., 1994), the seq2seq models are not yet able to achieve convincing performance in encoding and decoding for a long sequence of multiple sentences (Chen et al., 2017; Koehn and Knowles, 4078 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4078–4087 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Original Text (truncated): the family of conjoined twin sisters who died 19 days after they were born have been left mortified(2) after they arrived at their gravesite to find cemetery staff had cleared the baby section of all mementos and tossed them in the rubbish(3) . faith and hope howie were dubbed the miracle twins when they were born on may 8 last year with one"
D18-1441,P16-1046,0,0.0302049,"generated by sentences fusion or compression. (Chen et al., 2016) and word-level coverage mechanism (See et al., 2017) have also been investigated to alleviate the repetition problem. Reinforcement learning has also been studied to improve the document summarization performance from global sequence level (Paulus et al., 2017). Hierarchical Encoder-Decoder architecture is first proposed by Li et al. (2015) to train an auto-encoder to reconstruct multi-sentence paragraphs. In summarization field, hierarchical encoder has first been used to alleviate the long dependency problem for long inputs (Cheng and Lapata, 2016; Nallapati et al., 2016). Tan et al. (2017b) also propose to use a hierarchical encoder to encode multiple summaries produced by several extractive summarization methods, and then decode them into a headline. However, these models don’t model the decoding process hierarchically. Tan et al. (2017a) first use the hierarchical encoder-decoder architecture on generating multisentences summaries. They mainly focus on incorporating sentence ranking into abstractive document summarization to help detect important sentences. Different from that, our work mainly tends to verify the necessity of levera"
D18-1441,D14-1085,0,0.0264546,"ructural-coverage properties based on the hierarchical encoder-decoder model by adding structural regularization during both the model learning phase and inference phase. 3.1 Structural Compression Compression is a basic property of document summarization, which has been widely explored in traditional document summarization research, such as sentence compression-based methods which shorten sentences by removing non-salient parts (Li et al., 2013; Durrett et al., 2016) and sentence fusion-based methods which merge information from several different source sentences (Barzilay and McKeown, 2005; Cheung and Penn, 2014). As shown in Figure 1, each summary sentence in the human-written reference summary is also created by compressing several specific source sentences. In this paper, we propose to model the structural-compression property of document summarization based on sentence-level attention distributions by: N 1 ∑ i strCom(αt ) = 1 − αt logαti logN (2) i=1 where αt denotes the sentence-level attention distribution when generating the tth summary sentence and N denotes the length of distribution αt . The right part in the above formula is actually the entropy of the distribution αt . As the attention dis"
D18-1441,N16-1012,0,0.0443263,"7 I18 I19 I 20 (e) Our Method Figure 1: Comparison of sentence-level attention distribuIntroduction Document summarization is the task of generating a fluent and condensed summary for a document while retaining the gist information. Recent success of neural sequence-to-sequence (seq2seq) architecture on text generation tasks like machine translation (Bahdanau et al., 2014) and image caption (Vinyals et al., 2015), has attracted growing attention to abstractive summarization research. Huge success has been witnessed in abstractive sentence summarization (Rush et al., 2015; Takase et al., 2016; Chopra et al., 2016; Cao et al., 2017; Zhou et al., 2017), which builds onesentence summaries from one or two-sentence in∗ This work was done while the first author was doing internship at Baidu Inc. tions for the summaries in Table 1 on a news article. (a) is the heatmap for the gold reference summary, (b) is for the Seq2seq-baseline system, (c) is for the Point-gen-cov (See et al., 2017) system, (d) is for the Hierarchical-baseline system and (e) is for our system. Ii and Oi indicate the i-th sentence of the input and output, respectively. Obviously, the seq2seq models, including the Seq2seq-baseline model and"
D18-1441,P16-1188,0,0.0169864,"to capture the sentence-level characteristics of document summarization process. In this work, we propose to model the structural-compression and structural-coverage properties based on the hierarchical encoder-decoder model by adding structural regularization during both the model learning phase and inference phase. 3.1 Structural Compression Compression is a basic property of document summarization, which has been widely explored in traditional document summarization research, such as sentence compression-based methods which shorten sentences by removing non-salient parts (Li et al., 2013; Durrett et al., 2016) and sentence fusion-based methods which merge information from several different source sentences (Barzilay and McKeown, 2005; Cheung and Penn, 2014). As shown in Figure 1, each summary sentence in the human-written reference summary is also created by compressing several specific source sentences. In this paper, we propose to model the structural-compression property of document summarization based on sentence-level attention distributions by: N 1 ∑ i strCom(αt ) = 1 − αt logαti logN (2) i=1 where αt denotes the sentence-level attention distribution when generating the tth summary sentence a"
D18-1441,P16-1154,0,0.0148804,"1) and quantitatively (Table 5 and Figure 4). (2) The model has the ability to shorten a long sentence to generate a more concise one or compress several different sentences to generate a more informative one by merging the information from them. Table 6 shows several examples of abstractive summaries produced by sentence compression in our model. 6 Related Work Recently some work explored the seq2seq models on document summarization, which exhibit some undesirable behaviors, such as inaccurately reproducing factual details, OOVs and repetitions. To alleviate these issues, copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Nallapati et al., 2016) has been incorporated into the encoderdecoder architecture to help generate information correctly. Distraction-based attention model Table 6: Examples of sentences compression or fusion by our model. The link-through denotes deleting the non-salient part of the original text. The italic denotes novel words or sentences generated by sentences fusion or compression. (Chen et al., 2016) and word-level coverage mechanism (See et al., 2017) have also been investigated to alleviate the repetition problem. Reinforcement learning has also been studied t"
D18-1441,P16-1014,0,0.0211915,"ively (Table 5 and Figure 4). (2) The model has the ability to shorten a long sentence to generate a more concise one or compress several different sentences to generate a more informative one by merging the information from them. Table 6 shows several examples of abstractive summaries produced by sentence compression in our model. 6 Related Work Recently some work explored the seq2seq models on document summarization, which exhibit some undesirable behaviors, such as inaccurately reproducing factual details, OOVs and repetitions. To alleviate these issues, copying mechanism (Gu et al., 2016; Gulcehre et al., 2016; Nallapati et al., 2016) has been incorporated into the encoderdecoder architecture to help generate information correctly. Distraction-based attention model Table 6: Examples of sentences compression or fusion by our model. The link-through denotes deleting the non-salient part of the original text. The italic denotes novel words or sentences generated by sentences fusion or compression. (Chen et al., 2016) and word-level coverage mechanism (See et al., 2017) have also been investigated to alleviate the repetition problem. Reinforcement learning has also been studied to improve the document"
D18-1441,W17-3204,0,0.0742551,"Missing"
D18-1441,D13-1047,0,0.0606625,"Missing"
D18-1441,P15-1107,0,0.045148,"Missing"
D18-1441,N16-1174,0,0.0783254,"from too much input content of a document (Tan et al., 2017a,b). The summary generated by the seq2seq models usually loses salient information of the original document or even contains repetitions (see Table 1). In fact, both document and summary naturally have document-sentence hierarchical structure, instead of being a flat sequence of words. It is widely aware that the hierarchical structure is necessary and useful for neural document modeling. Hierarchical neural models have already been successfully used in document-level language modeling (Lin et al., 2015) and document classification (Yang et al., 2016). However, few work makes use of the hierarchical structure of document and multi-sentence summary in document summarization. The basic hierarchical encoderdecoder model (Li et al., 2015) is also not yet able to capture the structural properties of both document and summary (see Figure 11 ), resulting in 1 To simulate the sentence-level attention mechanism on the gold reference summary, we compute the words-matching similarities (based on TF-IDF cosine similarity) between a reference-summary sentence and the corresponding source document sentences and normalize them into attention distribution"
D18-1441,W04-1013,0,0.0218291,"rom scratch during training. We use a vocabulary of 50k words for both the encoder and decoder. We trained our model on a single Tesla K40m GPU with a batch size of 16 and an epoch is set containing 10,000 randomly sampled documents. Convergence is reached within 300 epochs. After tuning on the validation set, parameters λ1 , λ2 , ζ1 and ζ2 , are set as -0.5, -1.0, 1.2 and 1.4, respectively. At the test time, we use the hierarchical decoding algorithm with sentence-level beam size 4 and word-level beam size 8. 4.2 Evaluation ROUGE Evaluation. We evaluate our models with the widely used ROUGE (Lin, 2004) toolkit. We compare our system’s results with the results of state-of-the-art neural summarization approaches reported in recent papers, including both abstractive models and extractive models. The extractive models include SummaRuNNer (Nallapati et al., 2017) and SummaRuNNer-abs which is similar to SummaRuNNer but is trained directly on the abstractive summaries. The abstractive models include: 1) Seq2seq-baseline, which uses the basic seq2seq encoder-decoder architecture with attention mechanism, and incorporates with copy mechanism (See et al., 2017) to alleviate the OOV problem. 2) ABS-te"
D18-1441,P17-1101,0,0.0200729,"Comparison of sentence-level attention distribuIntroduction Document summarization is the task of generating a fluent and condensed summary for a document while retaining the gist information. Recent success of neural sequence-to-sequence (seq2seq) architecture on text generation tasks like machine translation (Bahdanau et al., 2014) and image caption (Vinyals et al., 2015), has attracted growing attention to abstractive summarization research. Huge success has been witnessed in abstractive sentence summarization (Rush et al., 2015; Takase et al., 2016; Chopra et al., 2016; Cao et al., 2017; Zhou et al., 2017), which builds onesentence summaries from one or two-sentence in∗ This work was done while the first author was doing internship at Baidu Inc. tions for the summaries in Table 1 on a news article. (a) is the heatmap for the gold reference summary, (b) is for the Seq2seq-baseline system, (c) is for the Point-gen-cov (See et al., 2017) system, (d) is for the Hierarchical-baseline system and (e) is for our system. Ii and Oi indicate the i-th sentence of the input and output, respectively. Obviously, the seq2seq models, including the Seq2seq-baseline model and the Point-gen-cov model, lose much sa"
D18-1441,P11-1052,0,0.072881,"d based on the documentsentence hierarchical structure, it can’t capture the basic structural properties of document summarization (see Figure 1(d) and Table 1). HowA good summary should have the ability to cover most of the important information of an input document. As shown in Figure 1, the humanwritten reference summary covers the information of many source sentences. Coverage has been 4081 used as a measure in many traditional document summarization research, such as the submodularbased methods which optimize the information coverage of the summary with similarity-based coverage metrics (Lin and Bilmes, 2011; Chali et al., 2017). In this work, we simply model the structuralcoverage property of summary based on the hierarchical architecture by encouraging different summary sentences to focus on different sets of source sentences so that the summary can cover more salient sentences of the input document. We measure the structural-coverage of summary based on the sentence-level attention distributions: strCov(αt ) = 1 − ∑ min(αti , t−1 ∑ αti′ ) (3) t′ =0 i which is used to encourage different summary sentences to focus on different sets of source sentences during the summary generation process. As t"
D18-1441,D15-1106,0,0.0232576,"eq2seq models to discover important information from too much input content of a document (Tan et al., 2017a,b). The summary generated by the seq2seq models usually loses salient information of the original document or even contains repetitions (see Table 1). In fact, both document and summary naturally have document-sentence hierarchical structure, instead of being a flat sequence of words. It is widely aware that the hierarchical structure is necessary and useful for neural document modeling. Hierarchical neural models have already been successfully used in document-level language modeling (Lin et al., 2015) and document classification (Yang et al., 2016). However, few work makes use of the hierarchical structure of document and multi-sentence summary in document summarization. The basic hierarchical encoderdecoder model (Li et al., 2015) is also not yet able to capture the structural properties of both document and summary (see Figure 11 ), resulting in 1 To simulate the sentence-level attention mechanism on the gold reference summary, we compute the words-matching similarities (based on TF-IDF cosine similarity) between a reference-summary sentence and the corresponding source document sentence"
D18-1441,K16-1028,0,0.184912,"zing the accumulated score scoret of all the sentences generated, including the sentences generation score, structural-compression score and structural-coverage score, which are defined as: scoret = t ∑ {Pˆ (s′t′ )+ζ1 strCom(αt′ )+ζ2 strCov(αt′ )} t′ =0 (5) where ζ1 and ζ2 are factors introduced to control the influence of structural regularization during the decoding process. 4 Experiments 4.1 Experimental Settings We conduct our experiments on the CNN/Daily Mail dataset (Hermann et al., 2015), which has been widely used for exploration on summarizing documents with multi-sentence summaries (Nallapati et al., 2016; See et al., 2017; Tan et al., 2017a; Paulus et al., 2017). The CNN/DailyMail dataset contains input sequences of about 800 tokens in average and multi-sentence summaries of up to 200 tokens. The average number of sentences in documents and summaries are 42.1 and 3.8, respectively. We use the same version of non-anonymized data (the original text without pre-processing) as See et al. (2017), which has 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs. For all experiments, the word-level encoder and decoder both use 256-dimensional hidden states, and the sentence-level enco"
D18-1441,D15-1044,0,0.0742999,"7 I 8 I 9 I10 I11 I12 I13 I14 I15 I16 I17 I18 I19 I 20 (e) Our Method Figure 1: Comparison of sentence-level attention distribuIntroduction Document summarization is the task of generating a fluent and condensed summary for a document while retaining the gist information. Recent success of neural sequence-to-sequence (seq2seq) architecture on text generation tasks like machine translation (Bahdanau et al., 2014) and image caption (Vinyals et al., 2015), has attracted growing attention to abstractive summarization research. Huge success has been witnessed in abstractive sentence summarization (Rush et al., 2015; Takase et al., 2016; Chopra et al., 2016; Cao et al., 2017; Zhou et al., 2017), which builds onesentence summaries from one or two-sentence in∗ This work was done while the first author was doing internship at Baidu Inc. tions for the summaries in Table 1 on a news article. (a) is the heatmap for the gold reference summary, (b) is for the Seq2seq-baseline system, (c) is for the Point-gen-cov (See et al., 2017) system, (d) is for the Hierarchical-baseline system and (e) is for our system. Ii and Oi indicate the i-th sentence of the input and output, respectively. Obviously, the seq2seq models"
D18-1441,P17-1099,0,0.293624,"and image caption (Vinyals et al., 2015), has attracted growing attention to abstractive summarization research. Huge success has been witnessed in abstractive sentence summarization (Rush et al., 2015; Takase et al., 2016; Chopra et al., 2016; Cao et al., 2017; Zhou et al., 2017), which builds onesentence summaries from one or two-sentence in∗ This work was done while the first author was doing internship at Baidu Inc. tions for the summaries in Table 1 on a news article. (a) is the heatmap for the gold reference summary, (b) is for the Seq2seq-baseline system, (c) is for the Point-gen-cov (See et al., 2017) system, (d) is for the Hierarchical-baseline system and (e) is for our system. Ii and Oi indicate the i-th sentence of the input and output, respectively. Obviously, the seq2seq models, including the Seq2seq-baseline model and the Point-gen-cov model, lose much salient information of the input document and focus on the same set of sentences repeatedly. The Hierarchical-baseline model fails to detect several specific sentences that are salient and relevant for each summary sentence and focuses on the same set of sentences repeatedly. On the contrary, our method with structural regularizations"
D18-1441,D16-1112,0,0.0285496,"12 I13 I14 I15 I16 I17 I18 I19 I 20 (e) Our Method Figure 1: Comparison of sentence-level attention distribuIntroduction Document summarization is the task of generating a fluent and condensed summary for a document while retaining the gist information. Recent success of neural sequence-to-sequence (seq2seq) architecture on text generation tasks like machine translation (Bahdanau et al., 2014) and image caption (Vinyals et al., 2015), has attracted growing attention to abstractive summarization research. Huge success has been witnessed in abstractive sentence summarization (Rush et al., 2015; Takase et al., 2016; Chopra et al., 2016; Cao et al., 2017; Zhou et al., 2017), which builds onesentence summaries from one or two-sentence in∗ This work was done while the first author was doing internship at Baidu Inc. tions for the summaries in Table 1 on a news article. (a) is the heatmap for the gold reference summary, (b) is for the Seq2seq-baseline system, (c) is for the Point-gen-cov (See et al., 2017) system, (d) is for the Hierarchical-baseline system and (e) is for our system. Ii and Oi indicate the i-th sentence of the input and output, respectively. Obviously, the seq2seq models, including the Seq2s"
D18-1441,P17-1108,0,0.101184,"Missing"
D18-1481,S14-2010,0,0.0154627,") (Hu and Liu, 2004), subjectivity/objectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Li and Roth, 2002). We also consider paraphrase detection on the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004), where the evaluation metrics are accuracy and F1 score. We then evaluate on the SICK dataset (Marelli et al., 2014) for both textual entailment (SICK-E) and semantic relatedness (SICK-R). The evaluation metric is Pearson correlation for SICK-R. We also evaluate on the SemEval task of STS14 (Agirre et al., 2014), where the evaluation metrics are Pearson and Spearman correlations. The processing on each task is as follows: 1) Employ the pre-trained attention autoencoder to encode all sentences into the latent mean-max representations. 2) Using the representations as features, apply the open SentEval with a logistic regression classifier (Conneau et al., 2017) to automatically evaluate on all the tasks. For a fair comparison of the plain sentence embeddings, we adopt all the default settings. 4.2 Experiment Setup We train our model on the open Toronto Books Corpus (Zhu et al., 2015), which was also use"
D18-1481,D17-1070,0,0.19064,"example, it took two weeks to train the skipthought (Kiros et al., 2015). Moreover, the traditional RNN autoencoder generates words in sequence conditioning on the previous ground-truth words, i.e., teacher forcing training (Williams and Zipser, 1989). This teacher forcing strategy has been proven important because it forces the output of the RNN to stay close to the ground-truth sequence. However, at each time step, allowing the decoder solely to access the previous ground-truth words weakens the encoder’s ability to learn the global information of the input sequence. Some other approaches (Conneau et al., 2017; Cer et al., 2018; Subramanian et al., 2018) attempt to use the labelled data to build a generic sentence encoder, such as the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), but such large-scale high-quality labelled data appropriate for training sentence representations is generally not available in other languages. In this paper, we are interested in learning universal sentence representations based on a large amount of naturally occurring corpus, without using any labelled data. We propose a mean-max 4514 Proceedings of the 2018 Conference on Empirical Methods in"
D18-1481,C04-1051,0,0.0916931,"valuate the capability of sentence embeddings produced by our generic encoder. 4.1 Transfer Tasks We conduct extensive experiments on 10 transfer tasks. We first study the classification task on 6 benchmarks: movie review sentiment (MR, SST) (Pang and Lee, 2005; Socher et al., 2013), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Li and Roth, 2002). We also consider paraphrase detection on the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004), where the evaluation metrics are accuracy and F1 score. We then evaluate on the SICK dataset (Marelli et al., 2014) for both textual entailment (SICK-E) and semantic relatedness (SICK-R). The evaluation metric is Pearson correlation for SICK-R. We also evaluate on the SemEval task of STS14 (Agirre et al., 2014), where the evaluation metrics are Pearson and Spearman correlations. The processing on each task is as follows: 1) Employ the pre-trained attention autoencoder to encode all sentences into the latent mean-max representations. 2) Using the representations as features, apply the open Se"
D18-1481,D17-1254,0,0.242061,"at https://github. com/Zminghua/SentEncoding. 1 nization (Hill et al., 2015). In recent years, learning sentence representations has attracted much attention, which is to encode sentences into fixedlength vectors that could capture the semantic and syntactic properties of sentences and can then be transferred to a variety of other NLP tasks. The most widely used method is to employ an encoder-decoder architecture with recurrent neural networks (RNN) to predict the original input sentence or surrounding sentences given an input sentence (Kiros et al., 2015; Ba et al., 2016; Hill et al., 2016; Gan et al., 2017). However, the RNN becomes time consuming when the sequence is long. The problem becomes more serious when learning general sentence representations that needs training on a large amount of data. For example, it took two weeks to train the skipthought (Kiros et al., 2015). Moreover, the traditional RNN autoencoder generates words in sequence conditioning on the previous ground-truth words, i.e., teacher forcing training (Williams and Zipser, 1989). This teacher forcing strategy has been proven important because it forces the output of the RNN to stay close to the ground-truth sequence. However"
D18-1481,N16-1162,0,0.241136,"Missing"
D18-1481,J15-4004,0,0.0343445,"AAE greatly reduce the training time. 1 1 Introduction To automatically get the distributed representations of texts (words, phrases and sentences) is a fundamental task for natural language processing (NLP). There have been efficient learning algorithms to acquire the representations of words (Mikolov et al., 2013a), which have shown to provide useful features for various tasks. Interestingly, the acquired word representations reflect some observed aspects of human conceptual orga∗ Corresponding author. Our code is publicly available at https://github. com/Zminghua/SentEncoding. 1 nization (Hill et al., 2015). In recent years, learning sentence representations has attracted much attention, which is to encode sentences into fixedlength vectors that could capture the semantic and syntactic properties of sentences and can then be transferred to a variety of other NLP tasks. The most widely used method is to employ an encoder-decoder architecture with recurrent neural networks (RNN) to predict the original input sentence or surrounding sentences given an input sentence (Kiros et al., 2015; Ba et al., 2016; Hill et al., 2016; Gan et al., 2017). However, the RNN becomes time consuming when the sequence"
D18-1481,C02-1150,0,0.274168,", 2015; Gan et al., 2017; Conneau et al., 2017). We use the same benchmarks and follow the same procedure to evaluate the capability of sentence embeddings produced by our generic encoder. 4.1 Transfer Tasks We conduct extensive experiments on 10 transfer tasks. We first study the classification task on 6 benchmarks: movie review sentiment (MR, SST) (Pang and Lee, 2005; Socher et al., 2013), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Li and Roth, 2002). We also consider paraphrase detection on the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004), where the evaluation metrics are accuracy and F1 score. We then evaluate on the SICK dataset (Marelli et al., 2014) for both textual entailment (SICK-E) and semantic relatedness (SICK-R). The evaluation metric is Pearson correlation for SICK-R. We also evaluate on the SemEval task of STS14 (Agirre et al., 2014), where the evaluation metrics are Pearson and Spearman correlations. The processing on each task is as follows: 1) Employ the pre-trained attention autoencoder to encode all"
D18-1481,marelli-etal-2014-sick,0,0.0208506,"ive experiments on 10 transfer tasks. We first study the classification task on 6 benchmarks: movie review sentiment (MR, SST) (Pang and Lee, 2005; Socher et al., 2013), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Li and Roth, 2002). We also consider paraphrase detection on the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004), where the evaluation metrics are accuracy and F1 score. We then evaluate on the SICK dataset (Marelli et al., 2014) for both textual entailment (SICK-E) and semantic relatedness (SICK-R). The evaluation metric is Pearson correlation for SICK-R. We also evaluate on the SemEval task of STS14 (Agirre et al., 2014), where the evaluation metrics are Pearson and Spearman correlations. The processing on each task is as follows: 1) Employ the pre-trained attention autoencoder to encode all sentences into the latent mean-max representations. 2) Using the representations as features, apply the open SentEval with a logistic regression classifier (Conneau et al., 2017) to automatically evaluate on all the tasks. For a"
D18-1481,P04-1035,0,0.0583858,"ted the distributed representations of sentences by adding them as features in transfer tasks (Kiros et al., 2015; Gan et al., 2017; Conneau et al., 2017). We use the same benchmarks and follow the same procedure to evaluate the capability of sentence embeddings produced by our generic encoder. 4.1 Transfer Tasks We conduct extensive experiments on 10 transfer tasks. We first study the classification task on 6 benchmarks: movie review sentiment (MR, SST) (Pang and Lee, 2005; Socher et al., 2013), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Li and Roth, 2002). We also consider paraphrase detection on the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004), where the evaluation metrics are accuracy and F1 score. We then evaluate on the SICK dataset (Marelli et al., 2014) for both textual entailment (SICK-E) and semantic relatedness (SICK-R). The evaluation metric is Pearson correlation for SICK-R. We also evaluate on the SemEval task of STS14 (Agirre et al., 2014), where the evaluation metrics are Pearson and Spearman correlations."
D18-1481,P05-1015,0,0.0675117,"input sequence by optimizing the objective in Equation (22). 4 Evaluating Sentence Representations In the previous work, researchers evaluated the distributed representations of sentences by adding them as features in transfer tasks (Kiros et al., 2015; Gan et al., 2017; Conneau et al., 2017). We use the same benchmarks and follow the same procedure to evaluate the capability of sentence embeddings produced by our generic encoder. 4.1 Transfer Tasks We conduct extensive experiments on 10 transfer tasks. We first study the classification task on 6 benchmarks: movie review sentiment (MR, SST) (Pang and Lee, 2005; Socher et al., 2013), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Li and Roth, 2002). We also consider paraphrase detection on the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004), where the evaluation metrics are accuracy and F1 score. We then evaluate on the SICK dataset (Marelli et al., 2014) for both textual entailment (SICK-E) and semantic relatedness (SICK-R). The evaluation metric is Pearson correlation for SIC"
D18-1481,D13-1170,0,0.00541889,"ptimizing the objective in Equation (22). 4 Evaluating Sentence Representations In the previous work, researchers evaluated the distributed representations of sentences by adding them as features in transfer tasks (Kiros et al., 2015; Gan et al., 2017; Conneau et al., 2017). We use the same benchmarks and follow the same procedure to evaluate the capability of sentence embeddings produced by our generic encoder. 4.1 Transfer Tasks We conduct extensive experiments on 10 transfer tasks. We first study the classification task on 6 benchmarks: movie review sentiment (MR, SST) (Pang and Lee, 2005; Socher et al., 2013), customer product reviews (CR) (Hu and Liu, 2004), subjectivity/objectivity classification (SUBJ) (Pang and Lee, 2004), opinion polarity (MPQA) (Wiebe et al., 2005) and question type classification (TREC) (Li and Roth, 2002). We also consider paraphrase detection on the Microsoft Research Paraphrase Corpus (MRPC) (Dolan et al., 2004), where the evaluation metrics are accuracy and F1 score. We then evaluate on the SICK dataset (Marelli et al., 2014) for both textual entailment (SICK-E) and semantic relatedness (SICK-R). The evaluation metric is Pearson correlation for SICK-R. We also evaluate"
E17-2073,J92-4003,0,0.555437,"Missing"
E17-2073,W02-0109,0,0.0638675,"ing with n-gram term-frequency features whose good performance depend on a strong topic-genre correlation, bc is a relatively moderate lower-level feature. We believe that by combining high-level abstract features and lowerlevel features, our model may perform better in situations where the term-frequency based pattern is not entirely reliable for classification. Such is the case in the genre classification tasks of this paper, where term-frequency distribution can be confused by different topic-genre correlation. 4 4.1 Text pre-processing and SVM training The Natural Language Toolkit (NLTK) (Loper and Bird, 2002) was used for tokenization, and the WordNet Lemmatizer (Miller, 1994) was used for text pre-processing. The letters in the documents were also converted to lower cases to improve the TF-IDF baseline performance, and the word classes were determined by Brown clustering (Brown et al., 1992). During the unsupervised training of PV-DMs and DV-LSTMs, documents in a dataset were shuffled to eliminate the possibility that a classifier may simply use the position of documents for genre classification. All data were mean-zeroed before inputting to the classifier. For each type or combination of documen"
E17-2073,P09-1076,0,0.0253821,"ong-span sequential text information in a document; bc is to capture the word class statistics. The 3 biases are concatenated to the final 1000-dimensional DVLSTM document vector as follows: DV-LSTM = [n(b0ml ), n(b0c )]0 , • BNC Baby Corpus (BNCB) (Burnard, 2003): It is a subset of BNC, consisting of 182 documents written in 4 genres: fiction, newspapers, academic and conversation. Each genre consists of a total of about 1 million words. (1) • Penn Treebank Dataset (PTB): It was artificially extracted from the Penn Treebank Corpus by taking out the documents that have genre tags provided by (Webber, 2009; Plank, 2009). It has 5 genres: essays, highlights, letters, errata and news. The errata genre was removed as there are very few documents of that genre. We also removed short documents with fewer than 200 words from the dataset. At the end, the dataset has a total of 239 documents in 4 genres: 38 highlights, 95 essays, 42 letters, and 64 news. where bml is given by [n(b0mi ), n(b0mf ), n(b0mo ), n(b0mc ), n(b0l )]0 . (2) In Eq.(1) and Eq.(2), n(·) is the normalization operator which normalizes a vector to the unit norm. According to some previous researches in genre classification, it is fou"
E17-2073,H94-1111,0,0.277288,"rong topic-genre correlation, bc is a relatively moderate lower-level feature. We believe that by combining high-level abstract features and lowerlevel features, our model may perform better in situations where the term-frequency based pattern is not entirely reliable for classification. Such is the case in the genre classification tasks of this paper, where term-frequency distribution can be confused by different topic-genre correlation. 4 4.1 Text pre-processing and SVM training The Natural Language Toolkit (NLTK) (Loper and Bird, 2002) was used for tokenization, and the WordNet Lemmatizer (Miller, 1994) was used for text pre-processing. The letters in the documents were also converted to lower cases to improve the TF-IDF baseline performance, and the word classes were determined by Brown clustering (Brown et al., 1992). During the unsupervised training of PV-DMs and DV-LSTMs, documents in a dataset were shuffled to eliminate the possibility that a classifier may simply use the position of documents for genre classification. All data were mean-zeroed before inputting to the classifier. For each type or combination of document feature vectors, a linear SVM classifier was built from the trainin"
E17-2073,P10-1077,0,0.0806841,"ivation of DV-LSTM In our experiments, the LSTM neural network of Figure 1 has 200 units in the identity compression layer, 100 units in the sigmoid compression layer, 100 LSTM units, 500 word classes and V output units (where V is the vocabulary size). In the derivation of DV-LSTM, only the biases in the sigmoid layer bl ∈ R100 , LSTM layer bm ∈ R400 , and word-class layer bc ∈ R500 are adapted. The LSTM bias vector bm is further comprised of four 100-dimensional bias sub-vectors: input-gate biases bmi , forget-gate biases bmf , output-gate biases bmo , and cell biases bmc . 457 were merged (Wu et al., 2010) so that the total number of genres was reduced to 10. The 3 different biases are supposed to capture different and complementary information in a document: bl is to capture the abstract and distributed word embeddings; bm is to capture the long-span sequential text information in a document; bc is to capture the word class statistics. The 3 biases are concatenated to the final 1000-dimensional DVLSTM document vector as follows: DV-LSTM = [n(b0ml ), n(b0c )]0 , • BNC Baby Corpus (BNCB) (Burnard, 2003): It is a subset of BNC, consisting of 182 documents written in 4 genres: fiction, newspapers,"
E17-2073,J11-2004,0,0.0316648,"ocuments in 4 genres: 38 highlights, 95 essays, 42 letters, and 64 news. where bml is given by [n(b0mi ), n(b0mf ), n(b0mo ), n(b0mc ), n(b0l )]0 . (2) In Eq.(1) and Eq.(2), n(·) is the normalization operator which normalizes a vector to the unit norm. According to some previous researches in genre classification, it is found that models fitted on some lower-level features (e.g., term-frequency related feature, which is highly correlated to the topic and language) may actually hurt genre classification when they are tested on new documents of the same genre but of different topic or language (Petrenz and Webber, 2011; Petrenz, 2009; Petrenz, 2012). In our model, bm is a high-level abstract feature, which is relatively independent of the topic or language specific term-frequency distribution. bc is a lower-level feature that is related to the word clusters. Comparing with n-gram term-frequency features whose good performance depend on a strong topic-genre correlation, bc is a relatively moderate lower-level feature. We believe that by combining high-level abstract features and lowerlevel features, our model may perform better in situations where the term-frequency based pattern is not entirely reliable for"
E17-2073,E12-3002,0,0.014285,"ssays, 42 letters, and 64 news. where bml is given by [n(b0mi ), n(b0mf ), n(b0mo ), n(b0mc ), n(b0l )]0 . (2) In Eq.(1) and Eq.(2), n(·) is the normalization operator which normalizes a vector to the unit norm. According to some previous researches in genre classification, it is found that models fitted on some lower-level features (e.g., term-frequency related feature, which is highly correlated to the topic and language) may actually hurt genre classification when they are tested on new documents of the same genre but of different topic or language (Petrenz and Webber, 2011; Petrenz, 2009; Petrenz, 2012). In our model, bm is a high-level abstract feature, which is relatively independent of the topic or language specific term-frequency distribution. bc is a lower-level feature that is related to the word clusters. Comparing with n-gram term-frequency features whose good performance depend on a strong topic-genre correlation, bc is a relatively moderate lower-level feature. We believe that by combining high-level abstract features and lowerlevel features, our model may perform better in situations where the term-frequency based pattern is not entirely reliable for classification. Such is the ca"
E17-2073,P06-4018,0,\N,Missing
I05-1004,I05-1004,1,0.0512826,"Missing"
I05-1004,J96-1002,0,0.013769,"Missing"
I05-1004,W02-2018,0,\N,Missing
I05-1037,C02-1150,0,0.0766252,"Missing"
I05-1037,A00-1023,1,\N,Missing
I05-1037,W01-1203,0,\N,Missing
I05-1037,N01-1005,0,\N,Missing
I05-1037,W03-1208,0,\N,Missing
I05-1037,P04-1072,0,\N,Missing
I05-1037,H01-1069,0,\N,Missing
I05-1037,A00-1041,0,\N,Missing
li-etal-2006-mining,I05-1037,1,\N,Missing
li-etal-2006-mining,hutchinson-2004-mining,0,\N,Missing
N03-2025,A97-1029,0,0.186813,"Missing"
N03-2025,W99-0613,0,0.100879,"Three types of proper names are defined in the Message Understanding Conference (MUC) Named Entity (NE) standards, namely, PERSON (PER), ORGANIZATION (ORG), and LOCATION (LOC). [MUC-7 1998] There is considerable research on NE tagging using supervised machine learning [e.g. Bikel et al. 1997; Borthwick 1998]. To overcome the knowledge bottleneck of supervised learning, unsupervised machine learning has been applied to NE. [Cucchiarelli & Velardi 2001] discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsing structures. [Cucerzan & Yarowsky 1999], [Collins & Singer 1999] and [Kim et al. 2002] presented various techniques using co-training schemes for NE extraction seeded by a small list of proper names or hand-crafted NE rules. NE tagging has two tasks: (i) NE chunking; (ii) NE classification. Parsingsupported unsupervised NE learning systems including ours only need to focus on NE classification, assuming the NE chunks have been constructed by the parser. This paper presents a new bootstrapping approach using successive learning and conceptbased seeds. The successive learning is as follows. First, parsing-based NE rules are learned with high precision but l"
N03-2025,J01-1005,0,0.0213113,"ting NE system approaches supervised NE performance for some NE types. 1 Overview Recognizing and classifying proper names is a fundamental task for information extraction. Three types of proper names are defined in the Message Understanding Conference (MUC) Named Entity (NE) standards, namely, PERSON (PER), ORGANIZATION (ORG), and LOCATION (LOC). [MUC-7 1998] There is considerable research on NE tagging using supervised machine learning [e.g. Bikel et al. 1997; Borthwick 1998]. To overcome the knowledge bottleneck of supervised learning, unsupervised machine learning has been applied to NE. [Cucchiarelli & Velardi 2001] discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsing structures. [Cucerzan & Yarowsky 1999], [Collins & Singer 1999] and [Kim et al. 2002] presented various techniques using co-training schemes for NE extraction seeded by a small list of proper names or hand-crafted NE rules. NE tagging has two tasks: (i) NE chunking; (ii) NE classification. Parsingsupported unsupervised NE learning systems including ours only need to focus on NE classification, assuming the NE chunks have been constructed by the parser. This paper presents a new bootstrapping"
N03-2025,W99-0612,0,0.0173569,"for information extraction. Three types of proper names are defined in the Message Understanding Conference (MUC) Named Entity (NE) standards, namely, PERSON (PER), ORGANIZATION (ORG), and LOCATION (LOC). [MUC-7 1998] There is considerable research on NE tagging using supervised machine learning [e.g. Bikel et al. 1997; Borthwick 1998]. To overcome the knowledge bottleneck of supervised learning, unsupervised machine learning has been applied to NE. [Cucchiarelli & Velardi 2001] discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsing structures. [Cucerzan & Yarowsky 1999], [Collins & Singer 1999] and [Kim et al. 2002] presented various techniques using co-training schemes for NE extraction seeded by a small list of proper names or hand-crafted NE rules. NE tagging has two tasks: (i) NE chunking; (ii) NE classification. Parsingsupported unsupervised NE learning systems including ours only need to focus on NE classification, assuming the NE chunks have been constructed by the parser. This paper presents a new bootstrapping approach using successive learning and conceptbased seeds. The successive learning is as follows. First, parsing-based NE rules are learned"
N03-2025,H92-1045,0,0.017488,"IsA-based rules to the top of the decision list: IsA(seed)Æ tag of the seed, e.g. IsA(man) Æ PER The parsing-based first learner is used to tag a raw corpus. First, we retrieve all the named entity candidates associated with at least one of the five parsing relationships from the repository. After applying the decision list to the retrieved 1,607,709 NE candidates, 33,104 PER names, 16,426 LOC names, and 11,908 ORG names are tagged. In order to improve the bootstrapping performance, we use the heuristic one tag per domain for multiword NE in addition to the one sense per discourse principle [Gale et al 1992]. These heuristics are found to be very helpful in both increasing positive instances (i.e. tag propagation) and decreasing the spurious instances (i.e. tag elimination). The tag propagation/elimination scheme is adopted from [Yarowsky 1995]. After this step, a total of 367,441 proper names are classified, including 134,722 PER names, 186,488 LOC names, and 46,231 ORG names. The classified proper name instances lead to the construction of an automatically tagged training corpus, consisting of the NE instances and their two (left and right) neighboring words within the same sentence. In the fi"
N03-2025,C02-1088,0,0.0148433,"are defined in the Message Understanding Conference (MUC) Named Entity (NE) standards, namely, PERSON (PER), ORGANIZATION (ORG), and LOCATION (LOC). [MUC-7 1998] There is considerable research on NE tagging using supervised machine learning [e.g. Bikel et al. 1997; Borthwick 1998]. To overcome the knowledge bottleneck of supervised learning, unsupervised machine learning has been applied to NE. [Cucchiarelli & Velardi 2001] discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsing structures. [Cucerzan & Yarowsky 1999], [Collins & Singer 1999] and [Kim et al. 2002] presented various techniques using co-training schemes for NE extraction seeded by a small list of proper names or hand-crafted NE rules. NE tagging has two tasks: (i) NE chunking; (ii) NE classification. Parsingsupported unsupervised NE learning systems including ours only need to focus on NE classification, assuming the NE chunks have been constructed by the parser. This paper presents a new bootstrapping approach using successive learning and conceptbased seeds. The successive learning is as follows. First, parsing-based NE rules are learned with high precision but limited recall. Then, t"
N03-2025,P95-1026,0,0.0548132,"he five parsing relationships from the repository. After applying the decision list to the retrieved 1,607,709 NE candidates, 33,104 PER names, 16,426 LOC names, and 11,908 ORG names are tagged. In order to improve the bootstrapping performance, we use the heuristic one tag per domain for multiword NE in addition to the one sense per discourse principle [Gale et al 1992]. These heuristics are found to be very helpful in both increasing positive instances (i.e. tag propagation) and decreasing the spurious instances (i.e. tag elimination). The tag propagation/elimination scheme is adopted from [Yarowsky 1995]. After this step, a total of 367,441 proper names are classified, including 134,722 PER names, 186,488 LOC names, and 46,231 ORG names. The classified proper name instances lead to the construction of an automatically tagged training corpus, consisting of the NE instances and their two (left and right) neighboring words within the same sentence. In the final stage, a bi-gram HMM is trained based on the above training corpus. The HMM training process follows [Bikel 1997]. 3 Benchmarking We used the same blind testing corpus of 300,000 words containing 20,000 PER, LOC and ORG instances to meas"
N18-1018,P16-1046,0,0.0723454,"Missing"
N18-1018,N16-1012,0,0.0446173,"lt to understand. Compared with the statistic model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machi"
N18-1018,P16-1154,0,0.23065,"-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART"
N18-1018,D15-1229,0,0.48478,"contains 359 sentence pairs. Besides, each complex sentence is paired with 8 reference simplified sentences provided by Amazon Mechanical Turk workers. i We use Adam optimization method to train the model, with the default hyper-parameters: the learning rate α = 0.001, and β1 = 0.9, β2 = 0.999,  = 1e − 8. 3 Experiments Following the previous work (Cao et al., 2017), we test our model on the following two paraphrase orientated tasks: text simplification and short text abstractive summarization. 3.1 Text Simplification 3.1.2 Evaluation Metrics Following the previous work (Nisioi et al., 2017; Hu et al., 2015), we evaluate our model with different metrics on two tasks. 3.1.1 Datasets The datasets are both from the alignments between English Wikipedia website2 and Simple English Wikipedia website.3 The Simple English Wikipedia is built for “the children and adults who are learning the English language”, and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine trans"
N18-1018,P17-2100,1,0.925978,"f PBMTR and SBMT-SARI on EW-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, l"
N18-1018,N15-1022,0,0.189994,"ioi et al. (2017) and Zhang et al. (2017), we ask the human raters to rate the simplified text in three dimensions: Fluency, Adequacy and Simplicity. Fluency assesses whether the outputs are grammatically right and well formed. Adequacy represents the meaning preservation of the simplified text. Both the scores of fluency and adequacy range from 1 to 5 (1 is very bad and 5 is very good). Simplicity shows how simpler the model outputs are than the source text, which ranges from 1 to 5. • English Wikipedia and Simple English Wikipedia (EW-SEW). EW-SEW is a publicly available dataset provided by Hwang et al. (2015). To build the corpus, they first align the complex-simple sentence pairs, score the semantic similarity between the complex sentence and the simple sentence, and classify 2 3 3.1.3 Settings Our proposed model is based on the encoderdecoder framework. The encoder is implemented on LSTM, and the decoder is based on LSTM with Luong style attention (Luong et al., 2015). We http://en.wikipedia.org http://simple.wikipedia.org 199 PWKP PBMT (Wubben et al., 2012) Hybrid (Narayan and Gardent, 2014) EncDecA (Zhang and Lapata, 2017) DRESS (Zhang and Lapata, 2017) DRESS-LS (Zhang and Lapata, 2017) Seq2se"
N18-1018,P15-1001,0,0.0312775,"erate a fluent sentence, but the meaning is different from the source text, and even more difficult to understand. Compared with the statistic model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et"
N18-1018,K16-1028,0,0.124083,"SARI. 3.1.5 3.1.4 Baselines We compare our model with several neural text simplification systems. Results We compare WEAN with state-of-the-art models for text simplification. Table 1 and Table 2 summarize the results of the automatic evaluation. On PWKP dataset, we compare WEAN with PBMT, Hybrid, EncDecA, DRESS and DRESSLS. WEAN achieves a BLEU score of 54.54, outperforming all of the previous systems. On EWSEW dataset, we compare WEAN with PBMT-R, Hybrid, SBMT-SARI, and the neural models described above. We do not find any public release code of PBMT-R and SBMT-SARI. Fortunately, Xu et al. (2016) provides the predictions of PBMTR and SBMT-SARI on EW-SEW test set, so that we can compare our model with these systems. • Seq2seq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN"
N18-1018,P13-1151,0,0.0185862,"Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs. Kauchak (2013) proposes a text simplification corpus, and evaluates language modeling for text simplification on the proposed corpus. Narayan and Gardent (2014) propose a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation. Hwang et al. (2015) introduces a parallel simplification corpus by evaluating the similarity between the source text and the simplified ˇ text based on WordNet. Glavaˇs and Stajner (2015) propose an unsupervised approach to lexical simplification that makes use of word vectors and require only regular corpora. Xu et al. (2016) desi"
N18-1018,P14-1041,0,0.190927,". • English Wikipedia and Simple English Wikipedia (EW-SEW). EW-SEW is a publicly available dataset provided by Hwang et al. (2015). To build the corpus, they first align the complex-simple sentence pairs, score the semantic similarity between the complex sentence and the simple sentence, and classify 2 3 3.1.3 Settings Our proposed model is based on the encoderdecoder framework. The encoder is implemented on LSTM, and the decoder is based on LSTM with Luong style attention (Luong et al., 2015). We http://en.wikipedia.org http://simple.wikipedia.org 199 PWKP PBMT (Wubben et al., 2012) Hybrid (Narayan and Gardent, 2014) EncDecA (Zhang and Lapata, 2017) DRESS (Zhang and Lapata, 2017) DRESS-LS (Zhang and Lapata, 2017) Seq2seq (our implementation) WEAN (our proposal) BLEU 46.31 53.94 47.93 34.53 36.32 48.26 54.54 PWKP NTS-w2v DRESS-LS WEAN Reference EW-SEW Fluency Adequacy Simplicity All PBMT-R 3.36 2.92 3.37 3.22 SBMT-SARI 3.41 3.63 3.25 3.43 NTS-w2v 3.56 3.52 3.42 3.50 DRESS-LS 3.59 3.43 3.65 3.56 WEAN 3.61 3.56 3.65 3.61 Reference 3.71 3.64 3.45 3.60 Table 1: Automatic evaluation of our model and other related systems on PWKP datasets. The results are reported on the test sets. EW-SEW PBMT-R (Wubben et al.,"
N18-1018,P17-2014,0,0.141044,"e decoder input and the predicted output share the same vocabulary and word embeddings. Besides, we do not use any pretrained word embeddings in our model, so that all of the parameters are learned from scratch. 2.5 Training Although our generator is a retrieval style, WEAN is as differentiable as the sequence-to-sequence model. The objective of training is to minimize the cross entropy between the predicted word probability distribution and the golden one-hot distribution: X L=− yˆi log p(yi ) (7) each sentence pair as a good, good partial, partial, or bad match. Following the previous work (Nisioi et al., 2017), we discard the unclassified matches, and use the good matches and partial matches with a scaled threshold greater than 0.45. The corpus contains about 150K good matches and 130K good partial matches. We use this corpus as the training set, and the dataset provided by Xu et al. (Xu et al., 2016) as the validation set and the test set. The validation set consists of 2,000 sentence pairs, and the test set contains 359 sentence pairs. Besides, each complex sentence is paired with 8 reference simplified sentences provided by Amazon Mechanical Turk workers. i We use Adam optimization method to tra"
N18-1018,D17-1222,0,0.497871,"eq is our implementation of the sequence-to-sequence model with attention mechanism, which is the most popular neural model for text generation. • NTS and NTS-w2v (Nisioi et al., 2017) are two sequence-to-sequence model with extra mechanism like prediction ranking, and NTS-w2v uses a pretrain word2vec. • DRESS and DRESS-LS (Zhang and Lapata, 2017) are two deep reinforcement learning 200 LCSTS RNN-W(Hu et al., 2015) RNN(Hu et al., 2015) RNN-cont-W(Hu et al., 2015) RNN-cont(Hu et al., 2015) SRB(Ma et al., 2017) CopyNet-W(Gu et al., 2016) CopyNet(Gu et al., 2016) RNN-dist(Chen et al., 2016) DRGD(Li et al., 2017) Seq2seq WEAN R-1 17.7 21.5 26.8 29.9 33.3 35.0 34.4 35.2 37.0 32.1 37.8 R-2 8.5 8.9 16.1 17.4 20.0 22.3 21.6 22.6 24.2 19.9 25.6 R-L 15.8 18.6 24.1 27.2 30.1 32.0 31.3 32.5 34.2 29.2 35.2 website called Sina Weibo.4 It is split into three parts, with 2,400,591 pairs in PART I, 10,666 pairs in PART II and 1,106 pairs in PART III. All the text-summary pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015)"
N18-1018,P02-1040,0,0.104711,"implification 3.1.2 Evaluation Metrics Following the previous work (Nisioi et al., 2017; Hu et al., 2015), we evaluate our model with different metrics on two tasks. 3.1.1 Datasets The datasets are both from the alignments between English Wikipedia website2 and Simple English Wikipedia website.3 The Simple English Wikipedia is built for “the children and adults who are learning the English language”, and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine translation and text simplification, which measures the agreement between the model outputs and the gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug"
N18-1018,N03-1020,0,0.390902,"pairs in PART II and PART III are manually annotated with relevant scores ranged from 1 to 5. We only reserve pairs with scores no less than 3, leaving 8,685 pairs in PART II and 725 pairs in PART III. Following the previous work (Hu et al., 2015), we use PART I as training set, PART II as validation set, and PART III as test set. 3.2.2 Table 4: ROUGE F1 score on the LCSTS test set. R1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L, respectively. The models with a suffix of ‘W’ in the table are word-based, while the rest of models are character-based. Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary against the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. It shows that the neural models have better performance in BLEU, and WEAN achieves the best BLEU score with 94.45. We perform the human evaluation of WEAN and oth"
N18-1018,C16-1275,0,0.226628,"Missing"
N18-1018,D15-1044,0,0.294351,"idation set, and PART III as test set. 3.2.2 Table 4: ROUGE F1 score on the LCSTS test set. R1, R-2, and R-L denote ROUGE-1, ROUGE-2, and ROUGE-L, respectively. The models with a suffix of ‘W’ in the table are word-based, while the rest of models are character-based. Our evaluation metric is ROUGE score (Lin and Hovy, 2003), which is popular for summarization evaluation. The metrics compare an automatically produced summary against the reference summaries, by computing overlapping lexical units, including unigram, bigram, trigram, and longest common subsequence (LCS). Following previous work (Rush et al., 2015; Hu et al., 2015), we use ROUGE-1 (unigram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) as the evaluation metrics in the reported experimental results. It shows that the neural models have better performance in BLEU, and WEAN achieves the best BLEU score with 94.45. We perform the human evaluation of WEAN and other related systems, and the results are shown in Table 3. DRESS-LS is based on the reinforcement learning, and it encourages the fluency, simplicity and relevance of the outputs. Therefore, it achieves a high score in our human evaluation. WEAN gains a even better score than DRESS-LS. Beside"
N18-1018,D17-1062,0,0.19844,"he gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug. 17th, 2009). The dataset contains 108,016 sentence pairs, with 25.01 words on average per complex sentence and 20.87 words per simple sentence. Following the previous work (Zhang and Lapata, 2017), we remove the duplicate sentence pairs, and split the corpus with 89,042 pairs for training, 205 pairs for validation and 100 pairs for test. • Human evaluation. Human evaluation is essential to evaluate the quality of the model outputs. Following Nisioi et al. (2017) and Zhang et al. (2017), we ask the human raters to rate the simplified text in three dimensions: Fluency, Adequacy and Simplicity. Fluency assesses whether the outputs are grammatically right and well formed. Adequacy represents the meaning preservation of the simplified text. Both the scores of fluency and adequacy range from"
N18-1018,C10-1152,0,0.182865,", and the articles are composed with “easy words and short sentences”. Therefore, Simple English Wikipedia is a natural public simplified text corpus. • Automatic evaluation. We use the BLEU score (Papineni et al., 2002) as the automatic evaluation metric. BLEU is a widely used metric for machine translation and text simplification, which measures the agreement between the model outputs and the gold references. The references can be either single or multiple. In our experiments, the references are single on PWKP, and multiple on EW-SEW. • Parallel Wikipedia Simplification Corpus (PWKP). PWKP (Zhu et al., 2010) is a widely used benchmark for evaluating text simplification systems. It consists of aligned complex text from English WikiPedia (as of Aug. 22nd, 2009) and simple text from Simple Wikipedia (as of Aug. 17th, 2009). The dataset contains 108,016 sentence pairs, with 25.01 words on average per complex sentence and 20.87 words per simple sentence. Following the previous work (Zhang and Lapata, 2017), we remove the duplicate sentence pairs, and split the corpus with 89,042 pairs for training, 205 pairs for validation and 100 pairs for test. • Human evaluation. Human evaluation is essential to ev"
N18-1018,D16-1112,0,0.160819,"Missing"
N18-1018,D17-1020,0,0.0247035,"model, WEAN generates a more fluent sentence. Besides, WEAN can capture the semantic meaning of the word by querying the word embeddings, so the generated sentence is semantically correct, 5 Related Work Our work is related to the encoder-decoder framework (Cho et al., 2014) and the attention mechanism (Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs."
N18-1018,D11-1038,0,0.0306716,"(Bahdanau et al., 2014). Encoderdecoder framework, like sequence-to-sequence model, has achieved success in machine translation (Sutskever et al., 2014; Jean et al., 2015; Luong et al., 2015; Lin et al., 2018), text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Wang et al., 2017; Ma and Sun, 2017), and other natural language processing tasks (Liu et al., 2017). There are many other methods to improve neural attention model (Jean et al., 2015; Luong et al., 2015). Zhu et al. (2010) constructs a wikipedia dataset, and proposes a tree-based simplification model. Woodsend and Lapata (2011) introduces a datadriven model based on quasi-synchronous grammar, which captures structural mismatches and complex rewrite operations. Wubben et al. (2012) 203 Acknowledgements presents a method for text simplification using phrase based machine translation with re-ranking the outputs. Kauchak (2013) proposes a text simplification corpus, and evaluates language modeling for text simplification on the proposed corpus. Narayan and Gardent (2014) propose a hybrid approach to sentence simplification which combines deep semantics and monolingual machine translation. Hwang et al. (2015) introduces"
N18-1018,P12-1107,0,0.0647051,"Missing"
P03-1043,W99-0613,0,0.550644,"d supervised learning systems can reach near-human performance for NE tagging in a targeted domain. However, both approaches face a serious knowledge bottleneck, making rapid domain porting difficult. Such systems cannot effectively support user-defined named entities. That is the motivation for using unsupervised or weaklysupervised machine learning that only requires a raw corpus from a given domain for this NE research. (Cucchiarelli & Velardi 2001) discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsing structures. (Cucerzan & Yarowsky 1999), (Collins & Singer 1999) and (Kim 2002) presented various techniques using co-training schemes for NE extraction seeded by a small list of proper names or handcrafted NE rules. NE tagging has two tasks: (i) NE chunking; (ii) NE classification. Parsingsupported NE bootstrapping systems including ours only focus on NE classification, assuming NE chunks have been constructed by the parser. The key idea of co-training is the separation of features into several orthogonal views. In case of NE classification, usually one view uses the context evidence and the other relies on the lexicon evidence. Learners corresponding to"
P03-1043,M98-1015,0,0.0212186,"Missing"
P03-1043,J01-1005,0,0.0209421,"g supervised machine learning, such as the Hidden Markov Model (HMM) (Bikel 1997) and the Maximum Entropy Model (Borthwick 1998). The state-of-the-art rule-based systems and supervised learning systems can reach near-human performance for NE tagging in a targeted domain. However, both approaches face a serious knowledge bottleneck, making rapid domain porting difficult. Such systems cannot effectively support user-defined named entities. That is the motivation for using unsupervised or weaklysupervised machine learning that only requires a raw corpus from a given domain for this NE research. (Cucchiarelli & Velardi 2001) discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsing structures. (Cucerzan & Yarowsky 1999), (Collins & Singer 1999) and (Kim 2002) presented various techniques using co-training schemes for NE extraction seeded by a small list of proper names or handcrafted NE rules. NE tagging has two tasks: (i) NE chunking; (ii) NE classification. Parsingsupported NE bootstrapping systems including ours only focus on NE classification, assuming NE chunks have been constructed by the parser. The key idea of co-training is the separation of features into sever"
P03-1043,W99-0612,0,0.610863,"he-art rule-based systems and supervised learning systems can reach near-human performance for NE tagging in a targeted domain. However, both approaches face a serious knowledge bottleneck, making rapid domain porting difficult. Such systems cannot effectively support user-defined named entities. That is the motivation for using unsupervised or weaklysupervised machine learning that only requires a raw corpus from a given domain for this NE research. (Cucchiarelli & Velardi 2001) discussed boosting the performance of an existing NE tagger by unsupervised learning based on parsing structures. (Cucerzan & Yarowsky 1999), (Collins & Singer 1999) and (Kim 2002) presented various techniques using co-training schemes for NE extraction seeded by a small list of proper names or handcrafted NE rules. NE tagging has two tasks: (i) NE chunking; (ii) NE classification. Parsingsupported NE bootstrapping systems including ours only focus on NE classification, assuming NE chunks have been constructed by the parser. The key idea of co-training is the separation of features into several orthogonal views. In case of NE classification, usually one view uses the context evidence and the other relies on the lexicon evidence. L"
P03-1043,H92-1045,0,0.036297,"Missing"
P03-1043,C02-1088,0,0.024931,"Missing"
P03-1043,P98-2127,0,0.00256987,"ity with the corresponding NEs at structural level, not at string sequence level. For example, at string sequence level, PERSON names are often preceded by a set of prefixing title words Mr./Mrs./Miss/Dr. etc., but the corresponding common noun seeds man/woman etc. cannot appear in such patterns. However, at structural level, the concept-based seeds share the same or similar linguistic patterns (e.g. Subject-Verb-Object patterns) with the corresponding types of proper names. The rationale behind using concept-based seeds in NE bootstrapping is similar to that for parsingbased word clustering (Lin 1998): conceptually similar words occur in structurally similar context. In fact, the anaphoric function of pronouns and common nouns to represent antecedent NEs indicates the substitutability of proper names by the corresponding common nouns or pronouns. For example, this man can be substituted for the proper name John Smith in almost all structural patterns. Following the same rationale, a bootstrapping approach is applied to the semantic lexicon acquisition task [Thelen & Riloff. 2002]. The InfoXtract parser supports dependency parsing based on the linguistic units constructed by our shallow par"
P03-1043,W02-1028,0,0.0909905,"Missing"
P03-1043,A00-1034,1,0.648655,"Missing"
P03-1043,A97-1029,0,0.748092,"us proper names, time, or numerical expressions. Seven types of named entities are defined in the Message Understanding Conference (MUC) standards, namely, PERSON (PER), ORGANIZATION (ORG), LOCATION (LOC), TIME, DATE, MONEY, and PERCENT1 (MUC-7 1998). 1 This paper only focuses on classifying proper names. Time and numerical NEs are not yet explored using this method. There is considerable research on NE tagging using different techniques. These include systems based on handcrafted rules (Krupka 1998), as well as systems using supervised machine learning, such as the Hidden Markov Model (HMM) (Bikel 1997) and the Maximum Entropy Model (Borthwick 1998). The state-of-the-art rule-based systems and supervised learning systems can reach near-human performance for NE tagging in a targeted domain. However, both approaches face a serious knowledge bottleneck, making rapid domain porting difficult. Such systems cannot effectively support user-defined named entities. That is the motivation for using unsupervised or weaklysupervised machine learning that only requires a raw corpus from a given domain for this NE research. (Cucchiarelli & Velardi 2001) discussed boosting the performance of an existing NE"
P03-1043,P95-1026,0,0.170718,"Missing"
P03-1043,C98-2122,0,\N,Missing
P03-1043,M98-1004,0,\N,Missing
P03-1065,C96-2182,0,0.0476854,"Missing"
P03-1065,A88-1019,0,0.137279,"by the description of the implementation details. 3.1 System Architecture Figure 1 shows the system architecture that contains the PV Identification Module based on the PV Expert Lexicon. This is a pipeline system mainly based on pattern matching implemented in local grammars and/or expert lexicons [Srihari et al 2003]. 4 4 POS and NE tagging are hybrid systems involving both hand-crafted rules and statistical learning. English parsing is divided into two tasks: shallow parsing and deep parsing. The shallow parser constructs Verb Groups (VGs) and basic Noun Phrases (NPs), also called BaseNPs [Church 1988]. The deep parser utilizes syntactic subcategorization features and semantic features of a head (e.g., VG) to decode both syntactic and logical dependency relationships such as Verb-Subject, Verb-Object, Head-Modifier, etc. Lexical lookup General Lexicon Part-of-Speech (POS) Tagging Named Entity (NE) Taggig Shallow Parsing PV Identification General PV Expert Lexicon Lexicon Deep parsing Figure 1. System Architecture The general lexicon lookup component involves stemming that transforms regular or irregular inflected verbs into the base forms to facilitate the later phrasal verb matching. This"
P04-1076,P98-1012,0,0.909956,"iable discourse heuristics such as one sense per discourse [Gale et al 1992], cross-document name disambiguation is a much harder problem. Among major categories of named entities (NEs, which in this paper refer to entity names, excluding the MUC time and numerical NEs), company and product names are often trademarked or uniquely registered, and hence less subject to name ambiguity. This paper focuses on cross-document disambiguation of person names. Previous research for cross-document name disambiguation applies vector space model (VSM) for context similarity, only using co-occurring words [Bagga & Baldwin 1998]. A pre-defined threshold decides whether two context vectors are different enough to represent two different entities. This approach faces two challenges: i) it is difficult to incorporate natural language processing (NLP) results in the VSM framework; 1 ii) the algorithm focuses on the local pairwise context similarity, and neglects the global correlation in the data: this may cause inconsistent results, and hurts the performance. This paper presents a new algorithm that addresses these problems. A learning scheme with minimal supervision is developed within the Bayesian framework. Maximum"
P04-1076,C98-1012,0,\N,Missing
P04-1076,H92-1045,0,\N,Missing
P18-2079,I17-1064,1,0.60587,"Missing"
P18-2079,P16-1068,0,0.0230098,"itute of Big Data Research, Peking University {yang pc, xusun, liweitj47, shumingma}@pku.edu.cn Abstract lowing work applied similar methods by using various classiﬁers with more sophisticated features including grammar, vocabulary and style (Rudner and Liang, 2002; Attali and Burstein, 2004). These traditional methods can work almost as well as human raters. However, they all demand a large amount of feature engineering, which requires a lot of expertise. Recent studies turn to use deep neural networks, claiming that deep learning models can relieve the system from heavy feature engineering. Alikaniotis et al. (2016) proposed to use long short term memory network (Hochreiter and Schmidhuber, 1997) with a linear regression output layer to predict the score. They added a score prediction loss to the original C&W embedding (Collobert and Weston, 2008; Collobert et al., 2011), so that the word embeddings are related to the quality of the essay. Taghipour and Ng (2016) also applied recurrent neural networks to process the essay, except that they put a convolutional layer ahead of the recurrent layer to extract local features. Dong and Zhang (2016) proposed to apply a two-layer convolutional neural network (CNN"
P18-2079,D13-1180,0,0.0549068,"Missing"
P18-2079,D15-1049,0,0.0394781,"Missing"
P18-2079,D16-1115,0,0.0557963,"dels can relieve the system from heavy feature engineering. Alikaniotis et al. (2016) proposed to use long short term memory network (Hochreiter and Schmidhuber, 1997) with a linear regression output layer to predict the score. They added a score prediction loss to the original C&W embedding (Collobert and Weston, 2008; Collobert et al., 2011), so that the word embeddings are related to the quality of the essay. Taghipour and Ng (2016) also applied recurrent neural networks to process the essay, except that they put a convolutional layer ahead of the recurrent layer to extract local features. Dong and Zhang (2016) proposed to apply a two-layer convolutional neural network (CNN) to model the essay. The ﬁrst layer is responsible for encoding the sentence and the second layer is to encode the whole essay. Dong et al. (2017) further proposed to add attention mechanism to the pooling layer to automatically decide which part is more important in determining the quality of the essay. Although there has been a lot of work dealing with AES task, researchers have not attempted the AAPR task. Different from the essay in language capability tests, academic papers are much longer with much more information, and the"
P18-2079,K17-1017,0,0.0518291,"ct the score. They added a score prediction loss to the original C&W embedding (Collobert and Weston, 2008; Collobert et al., 2011), so that the word embeddings are related to the quality of the essay. Taghipour and Ng (2016) also applied recurrent neural networks to process the essay, except that they put a convolutional layer ahead of the recurrent layer to extract local features. Dong and Zhang (2016) proposed to apply a two-layer convolutional neural network (CNN) to model the essay. The ﬁrst layer is responsible for encoding the sentence and the second layer is to encode the whole essay. Dong et al. (2017) further proposed to add attention mechanism to the pooling layer to automatically decide which part is more important in determining the quality of the essay. Although there has been a lot of work dealing with AES task, researchers have not attempted the AAPR task. Different from the essay in language capability tests, academic papers are much longer with much more information, and the overall quality is affected by a variety of factors besides the writing. Therefore, we propose a model that considers the overall information of one academic paper, including the title, authors, abstract and th"
P18-2079,P17-1011,0,0.0134501,"n used for the AES task, which have achieved great success. Alikaniotis et al. (2016) proposed to use the LSTM model with a linear regression output layer to predict the score. Taghipour and Ng (2016) applied the CNN model followed by a recurrent layer to extract local features and model sequence dependencies. A twolayer CNN model was proposed by Dong and Zhang (2016) to cover more high-level and abstract information. Dong et al. (2017) further proposed to add attention mechanism to the pooling layer to automatically decide which part is more important in determining the quality of the essay. Song et al. (2017) proposed a multi-label neural sequence labeling approach for discourse mode identiﬁcation and showed that features extracted by this method can further improve the AES task. Table 4: Ablation Study. The symbol * indicates that the difference compared to full data is signiﬁcant with p ≤ 0.05 under t-test. model shows different degrees of decline when we remove different modules of the source paper. This shows that there are differences in the contribution of different modules of the source paper to its acceptance, which further illustrates the reasonableness of our use of modularized hierarchi"
P18-2079,D14-1181,0,0.0114005,"Missing"
P18-2079,P11-1019,0,0.103149,"Missing"
P18-2079,W15-0626,0,0.030822,"Missing"
P18-2079,D16-1193,0,\N,Missing
P19-1126,D18-1337,0,0.632017,"involves discrete decisions, learning via back-propagation is obstructed. Previous work on simultaneous NMT has thus far side-stepped this problem by making restrictive simplifications, either on the underlying NMT model or on the flexibility of the schedule. Cho and Esipova (2016) apply heuristics measures to estimate and then threshold the confidence of an NMT model trained on full sentences to adapt it at inference time to the streaming scenario. Several others use reinforcement learning (RL) to develop an agent to predict read and write decisions (Satija and Pineau, 2016; Gu et al., 2017; Alinejad et al., 2018). However, due to computational challenges, they pre-train an NMT model on full sentences and then train an agent that sees the fixed NMT model as part of its environment. Dalvi et al. (2018) and Ma et al. (2018) use fixed schedules and train their NMT systems accordingly. In particular, Ma et al. (2018) advocate for a wait-k strategy, wherein the system always waits for exactly k tokens before beginning to translate, and then alternates between reading and writing at a constant pre-specified emission rate. Due to the deterministic nature of their schedule, they can easily train the NMT system"
P19-1126,N12-1048,0,0.480757,"ntly-proposed Average Lagging latency metric (Ma et al., 2018), making it differentiable and calculable in expectation, which allows it to be used as a training objective. 3. We demonstrate favorable trade-offs to those of wait-k strategies at many latency values, and provide evidence that MILk’s advantage extends from its ability to adapt based on source content. 2 Background Much of the earlier work on simultaneous MT took the form of strategies to chunk the source sentence into partial segments that can be translated safely. These segments could be triggered by prosody (Fügen et al., 2007; Bangalore et al., 2012) or lexical cues (Rangarajan Sridhar et al., 2013), or optimized directly for translation quality (Oda et al., 2014). Segmentation decisions are surrogates for the core problem, which is deciding whether enough source content has been read to write the next target word correctly (Grissom II et al., 2014). However, since doing so involves discrete decisions, learning via back-propagation is obstructed. Previous work on simultaneous NMT has thus far side-stepped this problem by making restrictive simplifications, either on the underlying NMT model or on the flexibility of the schedule. Cho and E"
P19-1126,P18-1008,1,0.829926,"6.3M sentences) and WMT15 German-to-English (DeEn; 4.5M sentences) tasks. For EnFr we use a combination of newstest 2012 and newstest 2013 for development and report results on newstest 2014. For DeEn we validate on newstest 2013 and then report results on newstest 2015. Translation quality is measured using detokenized, cased BLEU (Papineni et al., 2002). For each data set, we use BPE (Sennrich et al., 2016) on the training data to construct a 32,000-type vocabulary that is shared between the source and target languages. 5.1 Model Our model closely follows the RNMT+ architecture described by Chen et al. (2018) with modifications to support streaming translation. It consists of a 6 layer LSTM encoder and an 8 layer LSTM decoder with additive attention (Bahdanau et al., 2014). All streaming models including waitk, MoChA and MILk use unidirectional encoders, while offline translation models use a bidirectional encoder. Both encoder and decoder LSTMs have 512 hidden units, per gate layer normalization (Ba et al., 2016), and residual skip connections after the second layer. The models are regularized using dropout with probability 0.2 and label smoothing with an uncertainty of 0.1 (Szegedy et al., 2016)"
P19-1126,N18-2079,0,0.391153,"to the beginning of the source. We show that MILk’s adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values. 1 • The schedule is learned and/or adaptive to the current context, but assumes a fixed MT system trained on complete source sentences, as typified by wait-if-* (Cho and Esipova, 2016) and reinforcement learning approaches (Grissom II et al., 2014; Gu et al., 2017). • The schedule is simple and fixed and can thus be easily integrated into MT training, as typified by wait-k approaches (Dalvi et al., 2018; Ma et al., 2018). Introduction Simultaneous machine translation (MT) addresses the problem of how to begin translating a source sentence before the source speaker has finished speaking. This capability is crucial for live or streaming translation scenarios, such as speech-tospeech translation, where waiting for one speaker to complete their sentence before beginning the translation would introduce an intolerable delay. In these scenarios, the MT engine must balance latency against quality: if it acts before the necessary source content arrives, translation quality degrades; but waiting for t"
P19-1126,D14-1140,0,0.493445,"Missing"
P19-1126,E17-1099,0,0.425863,"notonic attention head to schedule the reading of the source sentence, and a soft attention head that extends from the monotonic head back to the beginning of the source. We show that MILk’s adaptive schedule allows it to arrive at latency-quality trade-offs that are favorable to those of a recently proposed wait-k strategy for many latency values. 1 • The schedule is learned and/or adaptive to the current context, but assumes a fixed MT system trained on complete source sentences, as typified by wait-if-* (Cho and Esipova, 2016) and reinforcement learning approaches (Grissom II et al., 2014; Gu et al., 2017). • The schedule is simple and fixed and can thus be easily integrated into MT training, as typified by wait-k approaches (Dalvi et al., 2018; Ma et al., 2018). Introduction Simultaneous machine translation (MT) addresses the problem of how to begin translating a source sentence before the source speaker has finished speaking. This capability is crucial for live or streaming translation scenarios, such as speech-tospeech translation, where waiting for one speaker to complete their sentence before beginning the translation would introduce an intolerable delay. In these scenarios, the MT engine"
P19-1126,P14-2090,0,0.498658,"ich allows it to be used as a training objective. 3. We demonstrate favorable trade-offs to those of wait-k strategies at many latency values, and provide evidence that MILk’s advantage extends from its ability to adapt based on source content. 2 Background Much of the earlier work on simultaneous MT took the form of strategies to chunk the source sentence into partial segments that can be translated safely. These segments could be triggered by prosody (Fügen et al., 2007; Bangalore et al., 2012) or lexical cues (Rangarajan Sridhar et al., 2013), or optimized directly for translation quality (Oda et al., 2014). Segmentation decisions are surrogates for the core problem, which is deciding whether enough source content has been read to write the next target word correctly (Grissom II et al., 2014). However, since doing so involves discrete decisions, learning via back-propagation is obstructed. Previous work on simultaneous NMT has thus far side-stepped this problem by making restrictive simplifications, either on the underlying NMT model or on the flexibility of the schedule. Cho and Esipova (2016) apply heuristics measures to estimate and then threshold the confidence of an NMT model trained on ful"
P19-1126,P02-1040,0,0.106553,"able 2: DAL’s time-indexed lag DALi = gi0 − when |x |= |y |= 4 for a wait-3 system. i−1 γ DAL, alongside several examples of cases where DAL yields more intuitive results than AL. 5 Experiments We run our experiments on the standard WMT14 English-to-French (EnFr; 36.3M sentences) and WMT15 German-to-English (DeEn; 4.5M sentences) tasks. For EnFr we use a combination of newstest 2012 and newstest 2013 for development and report results on newstest 2014. For DeEn we validate on newstest 2013 and then report results on newstest 2015. Translation quality is measured using detokenized, cased BLEU (Papineni et al., 2002). For each data set, we use BPE (Sennrich et al., 2016) on the training data to construct a 32,000-type vocabulary that is shared between the source and target languages. 5.1 Model Our model closely follows the RNMT+ architecture described by Chen et al. (2018) with modifications to support streaming translation. It consists of a 6 layer LSTM encoder and an 8 layer LSTM decoder with additive attention (Bahdanau et al., 2014). All streaming models including waitk, MoChA and MILk use unidirectional encoders, while offline translation models use a bidirectional encoder. Both encoder and decoder L"
P19-1126,N13-1023,0,0.456647,"al., 2018), making it differentiable and calculable in expectation, which allows it to be used as a training objective. 3. We demonstrate favorable trade-offs to those of wait-k strategies at many latency values, and provide evidence that MILk’s advantage extends from its ability to adapt based on source content. 2 Background Much of the earlier work on simultaneous MT took the form of strategies to chunk the source sentence into partial segments that can be translated safely. These segments could be triggered by prosody (Fügen et al., 2007; Bangalore et al., 2012) or lexical cues (Rangarajan Sridhar et al., 2013), or optimized directly for translation quality (Oda et al., 2014). Segmentation decisions are surrogates for the core problem, which is deciding whether enough source content has been read to write the next target word correctly (Grissom II et al., 2014). However, since doing so involves discrete decisions, learning via back-propagation is obstructed. Previous work on simultaneous NMT has thus far side-stepped this problem by making restrictive simplifications, either on the underlying NMT model or on the flexibility of the schedule. Cho and Esipova (2016) apply heuristics measures to estimat"
P19-1126,P16-1162,0,0.20387,"|y |= 4 for a wait-3 system. i−1 γ DAL, alongside several examples of cases where DAL yields more intuitive results than AL. 5 Experiments We run our experiments on the standard WMT14 English-to-French (EnFr; 36.3M sentences) and WMT15 German-to-English (DeEn; 4.5M sentences) tasks. For EnFr we use a combination of newstest 2012 and newstest 2013 for development and report results on newstest 2014. For DeEn we validate on newstest 2013 and then report results on newstest 2015. Translation quality is measured using detokenized, cased BLEU (Papineni et al., 2002). For each data set, we use BPE (Sennrich et al., 2016) on the training data to construct a 32,000-type vocabulary that is shared between the source and target languages. 5.1 Model Our model closely follows the RNMT+ architecture described by Chen et al. (2018) with modifications to support streaming translation. It consists of a 6 layer LSTM encoder and an 8 layer LSTM decoder with additive attention (Bahdanau et al., 2014). All streaming models including waitk, MoChA and MILk use unidirectional encoders, while offline translation models use a bidirectional encoder. Both encoder and decoder LSTMs have 512 hidden units, per gate layer normalizatio"
P19-1479,W04-3252,0,0.0625855,"the article which serve as the topics of the news. These keywords are the most important words to understand the story of the article, most of which are named entities. Since keyword detection is not the main point of this paper, we do not go into the details of the extraction process. Given a news article D, we first do word segmentation and named entity recognition on the news articles with off-the-shelf tools such as Stanford CoreNLP.3 Since the named entities alone can be insufficient to cover the main focuses of the document, we further apply keyword extraction algorithms like TextRank (Mihalcea and Tarau, 2004) to obtain additional keywords. After we get the keywords κ of the news, we associate each sentence of the documents to its corresponding keywords. We adopt a simple strategy that assigns a sentence s to the keyword k if k appears in the sentence. Note that one sentence can be associated with multiple keywords, which implicitly indicates connection between the two topics. Sentences that do not contain any of the keywords are put into a special vertex called “Empty”. Because the title of the article is crucial to understand the news, we also add a special vertex called “Title” that contains the"
P19-1479,P18-1026,0,0.0494272,"ks (GCN) is applied to classify the documents. Liu et al. (2018) proposed a siamese GCN model in the text matching task by modelling two documents into one interaction graph. Zhang et al. (2018) adopted a similar strategy but used GCN to match the article with a short query. These works are inspiring to our work, however, they are only designed for the classification task, which are different from generation tasks. There are also some previous work dedicated to use GNN in the generation tasks. Xu et al. (2018a,b) proposed to use graph based model to encode SQL queries in the SQL-to-Text task. Beck et al. (2018) and Song et al. (2018) proposed to solve the AMR-to-Text problem with graph neural networks. Zhao et al. (2018) proposed to facilitate neural machine translation by fusing the dependency between words into the traditional sequence-to-sequence framework. Although these work apply GNN as the encoder, they are meant to take advantage of the information that are already in the form of graph (SQL query, AMR graph, dependency graph) and the input text is relatively short, while our work tries to model long text documents as graphs, which is more challenging. 3 Graph-to-Sequence Model In this sectio"
P19-1479,P18-1008,0,0.0544696,"). The length of the input sequence is truncated to 100. For the input of title together with content, we append the content to the back of the title. Evaluation Metrics We choose three metrics to evaluate the quality of generated comments. For all the metrics, we ask the raters to score the comments with three gears, the scores are then projected to 0 ∼ 10. • Coherence: This metric evaluates how Coherent (consistent) is the comment to the news document. It measures whether the comment is about the main story of the news, one side part of the news, or irrelevant to the news. • Self-attention (Chen et al., 2018): this model follows the encoder-decoder framework. We use multi-layer self-attention with multi-head as the encoder, and a RNN decoder with attention is applied. We use two kinds of input, the bag of words (B) and the keywords (K). Since the input is not sequential, positional encoding is not applied. A special ‘CLS’ label is inserted, the hidden vector of which serves as the initial state of decoder. For the bag of words input we use the words with top 100 term frequency (TF) in the news document. For the keywords input, we use the same extracted keywords (topic words) with the ones used in"
P19-1479,P16-1154,0,0.0727217,"− 21 A˜D ˜ − 12 H l W l ) H l+1 = σ(D A˜ = A + IN (4) (5) ˜ − 21 A˜D ˜ is the where IN is the identity matrix, D normalized symmetric adjacency matrix, W l is a learnable weight matrix. To avoid the oversmoothing problem of GCN, we add residual connections between layers, g l+1 = H l+1 + H l g out K = tanh(Wo g ) (7) Decoder ti = RN N (ti−1 , ei−1 ) X ci = αj × gj exp(δ(ti , gj ) αj = P exp(δ(ti , gk )) (8) (9) (10) where δ is the attention function. Since the topic words (name of the vertices) κ are important information for the article and may appear in the comment, we adopt copy mechanism (Gu et al., 2016) by merging the predicted word token probability distribution with the attention distribution. The probability pcopy of copying from the topic words is dynamically calculated with the decoding hidden state ti and the context vector ci , yi = sof tmax(Wo (tanh(W ([ti ; ci ]) + b))) pcopy = σ(Wcopy [ti ; ci ]) p = (1 − pcopy ) × y + pcopy × α content title comment keyword (11) (12) (13) comment # 287,889 378,677 ave word # Ent Sport 456.1 506.6 16.4 15.7 16.3 19.4 8.4 9.0 ave character # Ent Sport 754.0 858.7 28.1 27.4 26.2 31.2 - Table 3: Length of content, title, comment and keyword of the new"
P19-1479,P17-1099,0,0.0379121,"o the retrieved comments during generation, which is a combination of retrieval and generation based model. Pure generation based model remains challenging, yet is a more direct way to solve the problem. Additionally, when the article is very different from the historical ones, there may not be appropriate comments to refer to. In this work, we would like to explore a generation model that better exploits the news content to solve the problem. Different from the scenarios where sequenceto-sequence models achieve great success like machine translation (Bahdanau et al., 2014) and summarization (See et al., 2017), comment generation has several nontrivial challenges: 1 https://kuaibao.qq.com/ Code for the paper is available https://github.com/lancopku/ Graph-to-seq-comment-generation 2 at • The news articles can be very long, which makes it intractable for classic sequence-tosequence models. On the contrary, although the title is a very important information resource, it can be too short to provide sufficient information. • The title of the news sometimes uses hyperbolic expressions that are semantically different from the content of the article. For example, the title shown in the example (Table 1) p"
P19-1479,W18-6459,0,0.0272423,"ng task by modelling two documents into one interaction graph. Zhang et al. (2018) adopted a similar strategy but used GCN to match the article with a short query. These works are inspiring to our work, however, they are only designed for the classification task, which are different from generation tasks. There are also some previous work dedicated to use GNN in the generation tasks. Xu et al. (2018a,b) proposed to use graph based model to encode SQL queries in the SQL-to-Text task. Beck et al. (2018) and Song et al. (2018) proposed to solve the AMR-to-Text problem with graph neural networks. Zhao et al. (2018) proposed to facilitate neural machine translation by fusing the dependency between words into the traditional sequence-to-sequence framework. Although these work apply GNN as the encoder, they are meant to take advantage of the information that are already in the form of graph (SQL query, AMR graph, dependency graph) and the input text is relatively short, while our work tries to model long text documents as graphs, which is more challenging. 3 Graph-to-Sequence Model In this section, we introduce the proposed graphto-sequence model (shown in Figure 1). Our model follows the Encoder-Decoder f"
P19-1479,P18-1150,0,0.0615954,"Missing"
P19-1479,D18-1112,0,0.018799,"the more sentences comention two keywords together, the closer these based graph convolutional networks (GCN) is applied to classify the documents. Liu et al. (2018) proposed a siamese GCN model in the text matching task by modelling two documents into one interaction graph. Zhang et al. (2018) adopted a similar strategy but used GCN to match the article with a short query. These works are inspiring to our work, however, they are only designed for the classification task, which are different from generation tasks. There are also some previous work dedicated to use GNN in the generation tasks. Xu et al. (2018a,b) proposed to use graph based model to encode SQL queries in the SQL-to-Text task. Beck et al. (2018) and Song et al. (2018) proposed to solve the AMR-to-Text problem with graph neural networks. Zhao et al. (2018) proposed to facilitate neural machine translation by fusing the dependency between words into the traditional sequence-to-sequence framework. Although these work apply GNN as the encoder, they are meant to take advantage of the information that are already in the form of graph (SQL query, AMR graph, dependency graph) and the input text is relatively short, while our work tries to"
P19-1479,N16-1174,0,0.713898,"rs in Marvel movies. Based on the above observations, we propose a graph-to-sequence model that generates comments based on a graph constructed out of content of the article and the title. We propose to represent the long document as a topic interaction graph, which decomposes the text into several topic centered clusters of texts, each of which representing a key aspect (topic) of the article. Each cluster together with the topic form a vertex in the graph. The edges between vertices are calculated based on the semantic relation between the vertices. Compared with the hierarchical structure (Yang et al., 2016), which is designed for long articles, our graph based model is better able to understand the connection between different topics of the news. Our model jointly models the title • We collect and release a large scale (200,000) article-comment corpus that contains title, content and the comments of the news articles. 2 Related Work The Graph Neural Networks (GNN) model has attracted growing attention recently, which is good at modeling graph structure data. GNN is not only applied in structural scenarios, where the data are naturally performed in graph structure, such as social network predicti"
P19-1479,P18-2025,0,\N,Missing
W02-1905,A00-1041,0,0.294113,"Missing"
W02-1905,M98-1027,0,0.0138475,"as been significant progress and interest in QA research in recent years (Voorhees 2000, Pasca and Harabagiu 2001). QA is different than search engines in two aspects: (i) instead of a string of keyword search terms, the query is a natural language question, necessitating question parsing, (ii) instead of a list of documents or URLs, a list of candidate answers at phrase level or sentence level are expected to be returned in response to a query, hence the need for text processing beyond keyword indexing, typically supported by Natural Language Processing (NLP) and Information Extraction (IE) (Chinchor and Marsh 1998, Hovy, Hermjakob and Lin 2001, Li and Srihari 2000). Examples of the use of NLP and IE in Question Answering include shallow parsing (Kupiec, 1993), semantic parsing (Litkowski This work was partly supported by a grant from the Air Force Research Laboratory’s Information Directorate (AFRL/IF), Rome, NY, under contracts F30602-00-C-0037 and F30602-00-C-0090. Identifying exact or phrase-level answers is a much more challenging task than sentence-level answers. Good performance on the latter can be achieved by using sophisticated passage retrieval techniques and/or shallow level NLP/IE processin"
W02-1905,A00-1023,1,0.934502,"ntity type and in cases where there are multiple candidate answerpoints to select from. The rest of the paper is structured as follows: Section 1 presents the NLP/IE engine used, sections 2 discusses how to identify and formally represent what is being asked, section 3 presents the algorithm on identifying exact answers leveraging structural support, section 4 presents case studies and benchmarks, and section 5 is the conclusion. Input Tokenizer Kernel IE Modules Output(Entity, Phrase and Structural Links) 1999), Named Entity tagging (Abney et al. 2000, Srihari and Li 1999) and high-level IE (Srihari and Li, 2000). Named Entity Linguistic Modules Part-OfSpeech Shallow Parsing Asking-point Identification Semantic Parsing Entity Association Figure 1: InfoXtract™ NLP/IE System Architecture 1 NLP/IE Engine Description The NLP/IE engine used in the QA system described here is named InfoXtract™. It consists of an NLP component and IE component, each consisting of a set of pipeline modules (Figure 1). The NLP component serves as underlying support for IE. A brief description of these modules is given below. • Part-of-Speech Tagging: tagging syntactic categories such as noun, verb, adjective, etc. • Shallow Pa"
W02-1905,voorhees-tice-2000-trec,0,0.0848044,"ty constraints. The proposed methods are particularly effective in cases where the question-phrase does not correspond to a known named entity type and in cases where there are multiple candidate answer-phrases satisfying the named entity constraints. Introduction Natural language Question Answering (QA) is recognized as a capability with great potential. The NIST-sponsored Text Retrieval Conference (TREC) has been the driving force for developing this technology through its QA track since TREC-8 (Voorhees 1999). There has been significant progress and interest in QA research in recent years (Voorhees 2000, Pasca and Harabagiu 2001). QA is different than search engines in two aspects: (i) instead of a string of keyword search terms, the query is a natural language question, necessitating question parsing, (ii) instead of a list of documents or URLs, a list of candidate answers at phrase level or sentence level are expected to be returned in response to a query, hence the need for text processing beyond keyword indexing, typically supported by Natural Language Processing (NLP) and Information Extraction (IE) (Chinchor and Marsh 1998, Hovy, Hermjakob and Lin 2001, Li and Srihari 2000). Examples o"
W03-0106,J94-4003,0,0.0307905,"Such ambiguity needs to be properly handled before converting location names into normal form to support Entity Profile (EP) construction, information merging/consolidation as well as visualization of location-stamped extracted events on a map. Location normalization is a special application of word sense disambiguation (WSD). There is considerable research on WSD. Knowledge-based work, such as [Hirst 1987; McRoy 1992; Ng and Lee 1996] used hand-coded rules or supervised machine learning based on an annotated corpus to perform WSD. Recent work emphasizes a corpus-based unsupervised approach [Dagon and Itai 1994; Yarowsky 1992; Yarowsky 1995] that avoids the need for costly truthed training data. Location normalization is different from general WSD in that the selection restriction often used for WSD in many cases is not sufficient to distinguish the correct sense from the other candidates. For example, in the sentence “The White House is located in Washington”, the selection restriction from the collocation ‘located in’ can only determine that “Washington” should be a location name, but is not sufficient to decide the actual sense of this location. In terms of local context, we found that there are"
W03-0106,H92-1045,0,0.30056,"Missing"
W03-0106,C02-1127,1,0.196911,"t matching sense set within a document, we simply construct a graph where each node represents a sense of a location NE, and each edge represents the relationship between two location name senses. A graph spanning algorithm can be used to select the best senses from the graph. Last but not least, proper assignment of default senses is found to play a significant role in the performance of a location normalizer. This involves two issues: (i) determining default senses using heuristics and/or other methods, such as statistical processing for semi-automatic default sense extraction from the web [Li et al. 2002]; and (ii) setting the conditions/thresholds and the proper levels when assigning default senses, to coordinate with local and discourse evidence for enhanced performance. The second issue can be resolved through experimentation. In the light of the above overview, this paper presents an effective hybrid location normalization approach which consists of local pattern matching and discourse co-occurrence analysis as well as default senses. Multiple knowledge sources are used in a number of ways: (i) pattern matching driven by local context, (ii) maximum spanning tree search for discourse analy"
W03-0106,M98-1015,0,0.0252077,"entities, relationships and events from natural language text. Figure 1 shows the overall system architecture of InfoXtract, involving multiple modules in a pipeline structure. InfoXtract involves a spectrum of linguistic processing and relationship/event extraction. This engine, in its current state, involves over 100 levels of processing and 12 major components. Some components are based on hand-crafted pattern matching rules, some are statistical models or procedures, and others are hybrid (e.g. NE, Co-reference, Location Normalization). The basic information extraction task is NE tagging [Krupka and Hausman 1998; Srihari et al. 2000]. The NE tagger identifies and classifies proper names of type PERSON, ORGANIZATION, PRODUCT, NAMED-EVENTS, LOCATION (LOC) as well as numerical expressions such as MEASUREMENT (e.g. MONEY, LENGTH, WEIGHT, etc) and time expressions (TIME, DATE, MONTH, etc.). Parallel to location normalization, InfoXtract also involves time normalization and measurement normalization. Document Processor Document pool Process Manager Tokenizer Source Document Linguistic Processor(s) Lexicon Lookup Tokenlist POS Tagging Tokenlist IE Repository NE NE Tagging Output Manager CE Time Normalizatio"
W03-0106,P96-1006,0,0.0511539,"Missing"
W03-0106,A00-1034,1,0.67643,"nd events from natural language text. Figure 1 shows the overall system architecture of InfoXtract, involving multiple modules in a pipeline structure. InfoXtract involves a spectrum of linguistic processing and relationship/event extraction. This engine, in its current state, involves over 100 levels of processing and 12 major components. Some components are based on hand-crafted pattern matching rules, some are statistical models or procedures, and others are hybrid (e.g. NE, Co-reference, Location Normalization). The basic information extraction task is NE tagging [Krupka and Hausman 1998; Srihari et al. 2000]. The NE tagger identifies and classifies proper names of type PERSON, ORGANIZATION, PRODUCT, NAMED-EVENTS, LOCATION (LOC) as well as numerical expressions such as MEASUREMENT (e.g. MONEY, LENGTH, WEIGHT, etc) and time expressions (TIME, DATE, MONTH, etc.). Parallel to location normalization, InfoXtract also involves time normalization and measurement normalization. Document Processor Document pool Process Manager Tokenizer Source Document Linguistic Processor(s) Lexicon Lookup Tokenlist POS Tagging Tokenlist IE Repository NE NE Tagging Output Manager CE Time Normalization Location Normalizat"
W03-0106,C92-2070,0,0.0431262,"to be properly handled before converting location names into normal form to support Entity Profile (EP) construction, information merging/consolidation as well as visualization of location-stamped extracted events on a map. Location normalization is a special application of word sense disambiguation (WSD). There is considerable research on WSD. Knowledge-based work, such as [Hirst 1987; McRoy 1992; Ng and Lee 1996] used hand-coded rules or supervised machine learning based on an annotated corpus to perform WSD. Recent work emphasizes a corpus-based unsupervised approach [Dagon and Itai 1994; Yarowsky 1992; Yarowsky 1995] that avoids the need for costly truthed training data. Location normalization is different from general WSD in that the selection restriction often used for WSD in many cases is not sufficient to distinguish the correct sense from the other candidates. For example, in the sentence “The White House is located in Washington”, the selection restriction from the collocation ‘located in’ can only determine that “Washington” should be a location name, but is not sufficient to decide the actual sense of this location. In terms of local context, we found that there are certain fairly"
W03-0106,P95-1026,0,0.0934961,"handled before converting location names into normal form to support Entity Profile (EP) construction, information merging/consolidation as well as visualization of location-stamped extracted events on a map. Location normalization is a special application of word sense disambiguation (WSD). There is considerable research on WSD. Knowledge-based work, such as [Hirst 1987; McRoy 1992; Ng and Lee 1996] used hand-coded rules or supervised machine learning based on an annotated corpus to perform WSD. Recent work emphasizes a corpus-based unsupervised approach [Dagon and Itai 1994; Yarowsky 1992; Yarowsky 1995] that avoids the need for costly truthed training data. Location normalization is different from general WSD in that the selection restriction often used for WSD in many cases is not sufficient to distinguish the correct sense from the other candidates. For example, in the sentence “The White House is located in Washington”, the selection restriction from the collocation ‘located in’ can only determine that “Washington” should be a location name, but is not sufficient to decide the actual sense of this location. In terms of local context, we found that there are certain fairly predictable key"
W03-0106,J92-1001,0,\N,Missing
W03-0106,M98-1004,0,\N,Missing
W03-0430,W02-2018,0,0.0116257,"2 k , log P Λ (l(j) |o(j) ) − 2 2σ j=1 k where the second sum is a Gaussian prior over parameters (with variance σ) that provides smoothing to help cope with sparsity in the training data. When the training labels make the state sequence unambiguous (as they often do in practice), the likelihood function in exponential models such as CRFs is convex, so there are no local maxima, and thus finding the global optimum is guaranteed. It has recently been shown that quasi-Newton methods, such as L-BFGS, are significantly more efficient than traditional iterative scaling and even conjugate gradient (Malouf, 2002; Sha and Pereira, 2003). This method approximates the second-derivative of the likelihood by keeping a running, finite-sized window of previous first-derivatives. L-BFGS can simply be treated as a black-box optimization procedure, requiring only that one provide the first-derivative of the function to be optimized. Assuming that the training labels on instance j make its state path unambiguous, let s(j) denote that path, and then the first-derivative of the log-likelihood is   N X δL =  Ck (s(j) , o(j) ) − δλk j=1   N X X λk  P Λ (s|o(j) )Ck (s, o(j) ) − 2 σ j=1 s where Ck (s, o) is t"
W03-0430,W96-0213,0,0.0743788,"Missing"
W03-0430,N03-1028,0,0.496798,"odels, handle them well. There has been significant work with such models for greedy sequence modeling in NLP (Ratnaparkhi, 1996; Borthwick et al., 1998). Conditional Random Fields (CRFs) (Lafferty et al., 2001) are undirected graphical models, a special case of which correspond to conditionally-trained finite state machines. While based on the same exponential form as maximum entropy models, they have efficient procedures for complete, non-greedy finite-state inference and training. CRFs have shown empirical successes recently in POS tagging (Lafferty et al., 2001), noun phrase segmentation (Sha and Pereira, 2003) and Chinese word segmentation (McCallum and Feng, 2003). Given these models’ great flexibility to include a wide array of features, an important question that remains is what features should be used? For example, in some cases capturing a word tri-gram is important, however, there is not sufficient memory or computation to include all word tri-grams. As the number of overlapping atomic features increases, the difficulty and importance of constructing only certain feature combinations grows. This paper presents a feature induction method for CRFs. Founded on the principle of constructing only"
W03-0430,W98-1118,0,\N,Missing
W03-0430,W99-0613,0,\N,Missing
W03-0808,C80-1007,0,0.545039,"Missing"
W03-0808,A00-1011,0,0.0545589,"Missing"
W03-0808,H93-1026,0,0.0786332,"Missing"
W03-0808,M98-1015,0,0.33761,"Missing"
W03-0808,A00-1034,1,0.845605,"Missing"
W03-0808,C02-1127,1,\N,Missing
W03-0808,M98-1004,0,\N,Missing
W03-1211,A00-1041,0,0.0801795,"Missing"
W03-1211,A97-1029,0,0.0182672,"Missing"
W03-1211,P02-1061,0,0.0673689,"Missing"
W03-1211,M98-1015,0,0.0717234,"Missing"
W03-1211,W02-1905,1,0.854509,"mation is often ignored in keyword indexing and retrieval for the sake of efficiency and robustness/recall. However, QA requires fine-grained text processing beyond keyword indexing since, instead of a list of documents or URLs, a list of candidate answers at phrase level or sentence level is expected to be returned in response to a query. Typically QA is supported by Natural Language Processing (NLP) and IE [Chinchor & Marsh 1998] [Hovy et al. 2001] [Srihari & Li 2000]. Examples of using NLP and IE in Question Answering include shallow parsing [Kupiec 1993] [Srihari & Li 2000], deep parsing [Li et al. 2002] [Litkowski 1999] [Voorhees 1999], and IE [Abney et al. 2000] [Srihari & Li 2000]. Almost all state-of-the-art QA systems rely on NE in searching for candidate answers. For a system based on language models, a feature exclusion approach is used to re-train the models, excluding features related to the case information [Kubala et al. 1998] [Miller et al. 2000] [Palmer et al. 2000]. In particular, the DARPA HUB-4 program evaluates NE systems on speech recognizer output in SNOR (Standard Normalized Orthographic Representation) that is case insensitive and has no punctuations [Chincor et al. 1998"
W03-1211,A00-1044,0,0.0287536,"Missing"
W03-1211,A00-1023,1,0.89417,"ssue here is how to minimize the performance degradation by adopting some strategy for the system adaptation. For search engines, the case information is often ignored in keyword indexing and retrieval for the sake of efficiency and robustness/recall. However, QA requires fine-grained text processing beyond keyword indexing since, instead of a list of documents or URLs, a list of candidate answers at phrase level or sentence level is expected to be returned in response to a query. Typically QA is supported by Natural Language Processing (NLP) and IE [Chinchor & Marsh 1998] [Hovy et al. 2001] [Srihari & Li 2000]. Examples of using NLP and IE in Question Answering include shallow parsing [Kupiec 1993] [Srihari & Li 2000], deep parsing [Li et al. 2002] [Litkowski 1999] [Voorhees 1999], and IE [Abney et al. 2000] [Srihari & Li 2000]. Almost all state-of-the-art QA systems rely on NE in searching for candidate answers. For a system based on language models, a feature exclusion approach is used to re-train the models, excluding features related to the case information [Kubala et al. 1998] [Miller et al. 2000] [Palmer et al. 2000]. In particular, the DARPA HUB-4 program evaluates NE systems on speech reco"
W03-1211,voorhees-tice-2000-trec,0,0.0628041,"Missing"
W03-1211,W03-0808,1,\N,Missing
W03-1211,M98-1004,0,\N,Missing
W04-0846,P95-1026,0,0.187905,"k lies in the fact that context features and the corresponding statistical distribution are different for each individual word. Traditionally, WSD involves modeling the contexts for each word. [Gale et al. 1992] uses the Naïve Bayes method for context modeling which requires a manually truthed corpus for each ambiguous word. This causes a serious Knowledge Bottleneck. The situation is worse when considering the domain dependency of word senses. To avoid the Knowledge Bottleneck, unsupervised or weakly supervised learning approaches have been proposed. These include the bootstrapping approach [Yarowsky 1995] and the context clustering approach [Schutze 1998]. Although the above unsupervised or weakly supervised learning approaches are less subject to the Knowledge Bottleneck, some weakness exists: i) for each individual keyword, the sense number has to be provided and in the bootstrapping case, seeds for each sense are also required; ii) the modeling usually assumes some form of evidence independency, e.g. the vector space model used in [Schutze 1998] and [Niu et al. 2003]: this limits the performance and its potential enhancement; iii) most WSD systems either use selectional restriction in pars"
W04-0846,W03-0808,1,0.791453,"Missing"
W05-0605,P04-1018,0,0.0203028,"Missing"
W05-0605,P95-1026,0,0.0855239,"ding statistical distribution are different for each individual word. Traditionally, WSD involves training the context classification models for each ambiguous word. (Gale et al. 1992) uses the Naïve Bayes method for context classification which requires a manually annotated corpus for each ambiguous word. This causes a serious Knowledge Bottleneck. The bottleneck is particularly serious when considering the domain dependency of word senses. To overcome the Knowledge Bottleneck, unsupervised or weakly supervised learning approaches have been proposed. These include the bootstrapping approach (Yarowsky 1995) and the context clustering approach (Schütze 1998). The above unsupervised or weakly supervised learning approaches are less subject to the Knowledge Bottleneck. For example, (Yarowsky 1995) only requires sense number and a few seeds for each sense of an ambiguous word (hereafter called keyword). (Schütze 1998) may only need minimal annotation to map the resulting context clusters onto external thesaurus for benchmarking and application-related purposes. Both methods are based on trigger words only. This paper presents a novel approach based on learning word-independent context pair classific"
W05-0605,W04-0704,0,0.036577,"Missing"
W05-0605,J98-1004,0,\N,Missing
